{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "900c4d38-4f3a-48ce-826b-98bcd5a44744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d75825-c192-419a-a7f1-6819aac18aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/llm/lib/python3.10/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.3.0+cu118 available.\n",
      "PyTorch version 2.3.0+cu118 available.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext, Settings\n",
    "from llama_index.core.indices import load_index_from_storage\n",
    "from llama_index.core import VectorStoreIndex, DocumentSummaryIndex\n",
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.response.notebook_utils import display_response\n",
    "import chromadb\n",
    "import torch\n",
    "import model_utils\n",
    "import prompt_utils\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f5c58b5-e162-4c62-9e9c-e943120efbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_path = \"models/Meta-Llama-3.1-8B-Instruct\"\n",
    "embed_path = \"models/bge-small-en-v1.5\"\n",
    "device = \"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13d1d2e1-ccdf-415e-ae2b-aee2b508bc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: models/bge-small-en-v1.5\n",
      "Load pretrained SentenceTransformer: models/bge-small-en-v1.5\n",
      "INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2 prompts are loaded, with the keys: ['query', 'text']\n"
     ]
    }
   ],
   "source": [
    "# loads BAAI/bge-small-en-v1.5\n",
    "embed_model = HuggingFaceEmbedding(model_name=embed_path, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cf5e6de-9720-447d-8972-8f43c45e2346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model with quantization config from: models/Meta-Llama-3.1-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# load local llm llama\n",
    "model, tokenizer = model_utils.load_quantized_model(\n",
    "    model_name_or_path=llm_path,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4427eeb6-3b60-4947-a486-3c6d236ea1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config llm and embed_model to llamaindex\n",
    "llm_hf = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=512,\n",
    "    query_wrapper_prompt=PromptTemplate(prompt_utils.get_llama31_ins_prompt_template()),\n",
    "    generate_kwargs={\n",
    "        \"temperature\": 0.2,\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "    },\n",
    "    device_map=device,\n",
    "    model_name=llm_path,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "576f25f1-000c-40e5-9769-ed8cdce0ad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings.embed_model = embed_model\n",
    "# Settings.llm = llm_hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dbd071-6987-44ea-a43c-853c4d1878e2",
   "metadata": {},
   "source": [
    "## Load vector index and document summary index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f9a2bad-01a8-4ed0-aa4d-fb9f103db78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "# Creates a persistent instance of Chroma that saves to disk\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0174fe7b-6ac2-412d-a2bf-4022824784a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(id=3186f0ec-26e5-46fa-b687-281a3a26066f, name=llama_index_blogs),\n",
       " Collection(id=46aee9cc-50b0-4474-90d9-61e1e160c15d, name=blogs_vector_index),\n",
       " Collection(id=cf744ea1-c23d-41c8-8204-363adcd4b3fe, name=llma_blogs_summary),\n",
       " Collection(id=e2bc79ee-49dd-45e4-85f3-6acb87185f7a, name=blogs_summary)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5ae16af-4694-462c-9264-84f64ea9d500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageContext(docstore=<llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore object at 0x7fdc9ed49c90>, index_store=<llama_index.core.storage.index_store.simple_index_store.SimpleIndexStore object at 0x7fdca606eef0>, vector_stores={'default': ChromaVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, collection_name=None, host=None, port=None, ssl=False, headers=None, persist_dir=None, collection_kwargs={}), 'image': SimpleVectorStore(stores_text=False, is_embedding_query=True, data=SimpleVectorStoreData(embedding_dict={}, text_id_to_ref_doc_id={}, metadata_dict={}))}, graph_store=<llama_index.core.graph_stores.simple.SimpleGraphStore object at 0x7fdca606f610>, property_graph_store=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get or create a collection with the given name and metadata.\n",
    "vector_collection = chroma_client.get_or_create_collection(\"blogs_vector_index\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=vector_collection)\n",
    "vector_storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "vector_storage_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b88d17f-4832-4fa8-aeee-139afb439670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x7fdc9bcf3670>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load your index from stored vectors\n",
    "vector_index = VectorStoreIndex.from_vector_store(\n",
    "    llm=llm_hf,\n",
    "    embed_model=embed_model,\n",
    "    vector_store=vector_store, \n",
    "    storage_context=vector_storage_context\n",
    ")\n",
    "vector_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5d00da3-837e-4718-9fe9-565b97e4d9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageContext(docstore=<llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore object at 0x7fdca606f1f0>, index_store=<llama_index.core.storage.index_store.simple_index_store.SimpleIndexStore object at 0x7fdc9bcf3eb0>, vector_stores={'default': ChromaVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, collection_name=None, host=None, port=None, ssl=False, headers=None, persist_dir=None, collection_kwargs={})}, graph_store=<llama_index.core.graph_stores.simple.SimpleGraphStore object at 0x7fdc9bcf3700>, property_graph_store=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get or create a collection with the given name and metadata.\n",
    "doc_sum_collection = chroma_client.get_or_create_collection(\"blogs_summary\")\n",
    "doc_sum_vector_store = ChromaVectorStore(\n",
    "    chroma_collection=doc_sum_collection\n",
    ")\n",
    "\n",
    "doc_sum_storage_context = StorageContext.from_defaults(\n",
    "    vector_store=doc_sum_vector_store,\n",
    "    persist_dir=\"./database/blogs_summary_index/\"\n",
    ")\n",
    "doc_sum_storage_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48cd9948-aed2-43d5-bc9f-fef2e475a224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.document_summary.base.DocumentSummaryIndex at 0x7fdc9bcf2fb0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_summary_index = load_index_from_storage(\n",
    "    llm=llm_hf,\n",
    "    embed_model=embed_model,\n",
    "    storage_context=doc_sum_storage_context\n",
    ")\n",
    "doc_summary_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d2ec04-be0d-43da-8c94-451f6d97d21a",
   "metadata": {},
   "source": [
    "## Advanced Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c2b3add-bc1b-45b9-8853-f70d064d8d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"What are key features of llama-agents?\"\n",
    "question2 = '''What are the two critical areas of RAG system performance that are assessed \\\n",
    "in the \"Evaluating RAG with LlamaIndex\" section of the OpenAI Cookbook?'''\n",
    "question3 = '''What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9878ad79-6471-4ca0-88e3-9bae6b6e9d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ref_docs(resp_metadata):\n",
    "    print(\"References:\")\n",
    "    base_url = \"https://www.llamaindex.ai/blog/\"\n",
    "    for idx, (_, doc_metatada) in enumerate(resp_metadata.items()):\n",
    "        ref_url = base_url + doc_metatada['file_name'].split(\".\")[0]\n",
    "        print(f\"{idx+1}.\", ref_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d3e20a2-9f26-4b0b-aabe-77412cb7b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm_hf\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "454cd317-734e-4e83-b40f-dc2fcb6a8b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracing log\n",
    "import llama_index.core\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "llama_index.core.set_global_handler(\"simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0798e7b-0335-48dd-b1e0-7f0d0ce73021",
   "metadata": {},
   "source": [
    "### VectorIndex as query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df6e2f68-c30f-4e93-8907-b6a4a3a6a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_query_engine = vector_index.as_query_engine(\n",
    "    response_mode=\"compact\", \n",
    "    use_async=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2705ecb5-9c2e-4ab9-9783-ddd88815e78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are key features of llama-agents?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.22it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The key features of llama-agents are:\n",
       "\n",
       "1. Distributed Service Oriented Architecture: every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\n",
       "2. Communication via standardized API interfaces: interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.\n",
       "3. Define agentic and explicit orchestration flows: developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an “agentic orchestrator” that decides which agents are relevant to the task.\n",
       "4. Ease of deployment: launch, scale and monitor each agent and your control plane independently.\n",
       "5. Scalability and resource management: use our built-in observability tools to monitor the quality and performance of the system and each individual agent service."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "References:\n",
      "1. https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\n"
     ]
    }
   ],
   "source": [
    "print(\"Question:\", question1)\n",
    "response1 = vector_query_engine.query(question1)\n",
    "display_response(response1)\n",
    "print_ref_docs(response1.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca3bb3b-edf3-415d-ad13-b240e1110a09",
   "metadata": {},
   "source": [
    "### DocumentSummaryIndex as query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f8a6d6c-d8d1-4a62-a018-295db831d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_query_engine = doc_summary_index.as_query_engine(\n",
    "    response_mode=\"compact\", \n",
    "    use_async=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25a1edea-2b81-4c01-b8cd-44b03d345a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are key features of llama-agents?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 151.58it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "print(\"Question:\", question1)\n",
    "response = summary_query_engine.query(question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c406b85-5e5f-4eaf-8ffa-03e433f62daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Based on the provided context, it appears that the key features of LlamaIndex are:\n",
       "\n",
       "1. **Vector Indexing**: LlamaIndex uses a vector index created from 10K DiffusionDB prompts to enable the Text2Image Prompt Assistant tool to re-write prompts to generate more beautiful images.\n",
       "2. **Query Engine**: The tool uses a query engine to generate new text-to-image prompts based on the provided context and examples.\n",
       "3. **Integration with Transformers Agents**: LlamaIndex is integrated with Transformers Agents to enable the creation of a text-to-image prompt assistant tool.\n",
       "4. **Customization**: The tool can be customized to use different LLMs (Large Language Models) and to refine the existing prompts based on the provided examples.\n",
       "5. **Temperature Control**: The tool allows for temperature control, which enables the generation of varied prompts with a temperature above zero.\n",
       "6. **Prompt Refinement**: LlamaIndex can suggest better prompts when generating images, as demonstrated by the Text2Image Prompt Assistant tool.\n",
       "7. **Vector Database**: LlamaIndex uses a vector database created from DiffusionDB to enable the Text2Image Prompt Assistant tool.\n",
       "\n",
       "The refined answer is:\n",
       "\n",
       "\"The key features of LlamaIndex are its ability to create a vector index, use a query engine, integrate with Transformers Agents, customize the tool to suit specific needs, control temperature, refine prompts, and utilize a vector database created from DiffusionDB.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "References:\n",
      "1. https://www.llamaindex.ai/blog/llamaindex-and-transformers-agents-67042ee1d8d6\n"
     ]
    }
   ],
   "source": [
    "display_response(response)\n",
    "print_ref_docs(response.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80060b08-2593-4ab0-98c9-8705aa91c504",
   "metadata": {},
   "source": [
    "### Router Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d33419a7-905b-432f-a229-239141005973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector, LLMMultiSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4cdf40ce-8618-40c8-8339-af673f5f8ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_tool = QueryEngineTool(\n",
    "    vector_query_engine,\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"vector_search\",\n",
    "        description=\"Useful for retrieving specific context\"\n",
    "    )\n",
    ")\n",
    "\n",
    "summary_tool = QueryEngineTool(\n",
    "    summary_query_engine,\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"summary\",\n",
    "        description=\"Useful for summarization questions related to document content\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467dd416-3d56-4e3a-90b4-5603bdacbfbe",
   "metadata": {},
   "source": [
    "#### Single selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdd69932-6246-4a34-80fe-204cf685ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_query_engine = RouterQueryEngine.from_defaults(\n",
    "    [vector_tool, summary_tool],\n",
    "    selector=LLMSingleSelector.from_defaults()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3241687-4f71-4f9c-b1e1-9b28d02860da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.query_engine.router_query_engine:Selecting query engine 1: The question is asking about key features of llama-agents, which is a summarization question related to document content, making choice 2 the most relevant..\n",
      "Selecting query engine 1: The question is asking about key features of llama-agents, which is a summarization question related to document content, making choice 2 the most relevant..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b4ddc0c046476eaa9cddf6db016f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "response = router_query_engine.query(question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8826a0c-0f62-409b-b7b1-67b01e517529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Based on the provided context, the key features of Llama-agents are:\n",
       "\n",
       "1. **Agent Structure**: Llama-agents provide a structure that enables Large Language Models (LLMs) to make decisions, use tools, and accomplish tasks.\n",
       "2. **Customizable Tools**: Transformers Agents come with pre-configured tools that leverage the vast amounts of open-source models hosted on Hugging Face-Hub. Additionally, new tools can be created and shared by publishing a new Hugging Face Space with the proper tool setup.\n",
       "3. **LlamaIndex Integration**: Llama-agents can integrate with LlamaIndex, a vector index created from a large dataset of text-to-image prompts.\n",
       "\n",
       "Note that the query asks about the key features of llama-agents, not LlamaIndex."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "180594fd-888b-412a-a1fe-9316a623a96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c71a83e3-f32a-4445-9ee3-cad21e9a0778': {'file_path': '/workspace/projects/LlamindexHelper/data/llamaindex-and-transformers-agents-67042ee1d8d6.html',\n",
       "  'file_name': 'llamaindex-and-transformers-agents-67042ee1d8d6.html',\n",
       "  'file_type': 'text/html',\n",
       "  'file_size': 14762,\n",
       "  'creation_date': '2024-07-21',\n",
       "  'last_modified_date': '2024-07-21'},\n",
       " 'selector_result': MultiSelection(selections=[SingleSelection(index=1, reason='The question is asking about key features of llama-agents, which is a summarization question related to document content, making choice 2 the most relevant.')])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a46bc278-3fa3-485a-8e5b-f0478204268b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.query_engine.router_query_engine:Selecting query engine 1: The question is asking about the performance of RAG system, which is related to summarization questions, hence option 2 is more relevant.\n",
      "Selecting query engine 1: The question is asking about the performance of RAG system, which is related to summarization questions, hence option 2 is more relevant.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ab4aa4f8024b12b6aa5003a89d0e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "response = router_query_engine.query(question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff54ffb6-a666-460c-acb7-39a70f3d5b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Based on the provided context information, the two critical areas of RAG system performance that are assessed in the \"Evaluating RAG with LlamaIndex\" section of the OpenAI Cookbook are:\n",
       "\n",
       "1. **The Retrieval System**\n",
       "2. **Response Generation**\n",
       "\n",
       "These two areas are mentioned in the context as the focus of the \"Evaluating RAG with LlamaIndex\" section of the cookbook."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0434921b-ee41-4681-83d2-8f9c1ef5fcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.query_engine.router_query_engine:Selecting query engine 1: The question is asking for a summary of the RAG system's performance, which aligns with the description of choice (2) as 'Useful for summarization questions related to document content'.\n",
      "Selecting query engine 1: The question is asking for a summary of the RAG system's performance, which aligns with the description of choice (2) as 'Useful for summarization questions related to document content'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6263f6da8601419ca2d981fbe51235a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Based on the provided context, the two main metrics used to evaluate the performance of the different rerankers in the RAG system are:\n",
       "\n",
       "1. **Hit Rate**\n",
       "2. **Mean Reciprocal Rank (MRR)**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = router_query_engine.query(question3)\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c78d87ac-c836-4886-8133-e48e23eb05ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.query_engine.router_query_engine:Selecting query engine 0: The question asks for the summary of a document, which implies retrieving specific context, making choice 1 the most relevant..\n",
      "Selecting query engine 0: The question asks for the summary of a document, which implies retrieving specific context, making choice 1 the most relevant..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b217a184294b9eaf78673b4a689add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Based on the provided context, the summary of the \"Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot\" document is:\n",
       "\n",
       "The document discusses the challenges faced by crisis counselors, specifically those working with The Trevor Project's TrevorText service, which provides online chat services to LGBTQ+ youth who are contemplating suicide. The counselors face high demand during peak times, administrative tasks, and the need to locate relevant local resources, which can lead to burnout and hinder effective care. The authors introduce \"Counselor Copilot\", a solution that aims to bridge the gap between the demand and supply of crisis services, and won awards at the LlamaIndex RAG-a-thon."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_summary_question = \"What is the summmarization of bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot document?\"\n",
    "response = router_query_engine.query(test_summary_question)\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61953756-3702-48d4-a831-087dea86b9f3",
   "metadata": {},
   "source": [
    "#### MultiSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68db9dde-4d4f-45cb-b68f-f48374c0f82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_selector_query_engine = RouterQueryEngine(\n",
    "    selector=LLMMultiSelector.from_defaults(),\n",
    "    query_engine_tools=[vector_tool, summary_tool],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f32db4d0-e601-4d51-a9f1-877f92262f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are key features of llama-agents?\n",
      "INFO:llama_index.core.query_engine.router_query_engine:Selecting query engine 0: The question is asking about key features of llama-agents, which is about retrieving specific context..\n",
      "Selecting query engine 0: The question is asking about key features of llama-agents, which is about retrieving specific context..\n",
      "\u001b[1;3;38;5;200mSelecting query engine 0: The question is asking about key features of llama-agents, which is about retrieving specific context..\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e7757cd7f44059a0f028187077e4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** According to the provided context information, the key features of llama-agents are:\n",
       "\n",
       "1. **Distributed Service Oriented Architecture**: every agent can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\n",
       "2. **Communication via standardized API interfaces**: interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.\n",
       "3. **Define agentic and explicit orchestration flows**: developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an “agentic orchestrator” that decides which agents are relevant to the task.\n",
       "4. **Ease of deployment**: launch, scale and monitor each agent and your control plane independently.\n",
       "5. **Scalability and resource management**: use our built-in observability tools to monitor the quality and performance of the system and each individual agent service."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(question1)\n",
    "response = multi_selector_query_engine.query(question1)\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d56f4496-9004-47c6-a9ed-dde0ad77f146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What are the two critical areas of RAG system performance that are assessed in the \"Evaluating RAG with LlamaIndex\" section of the OpenAI Cookbook?\n",
      "\n",
      "INFO:llama_index.core.query_engine.router_query_engine:Selecting query engine 0: The question is asking about specific areas of RAG system performance, which aligns with the description in option 1: 'Useful for retrieving specific context'..\n",
      "Selecting query engine 0: The question is asking about specific areas of RAG system performance, which aligns with the description in option 1: 'Useful for retrieving specific context'..\n",
      "\u001b[1;3;38;5;200mSelecting query engine 0: The question is asking about specific areas of RAG system performance, which aligns with the description in option 1: 'Useful for retrieving specific context'..\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3039155433a424292156fedf42512c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.query_engine.router_query_engine:Selecting query engine 0: Option 2 is related to summarization questions, which is not the focus of the question..\n",
      "Selecting query engine 0: Option 2 is related to summarization questions, which is not the focus of the question..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: Option 2 is related to summarization questions, which is not the focus of the question..\n",
      "\u001b[0mINFO:llama_index.core.query_engine.router_query_engine:Combining responses from multiple query engines.\n",
      "Combining responses from multiple query engines.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The two critical areas of RAG system performance that are assessed in the \"Evaluating RAG with LlamaIndex\" section of the OpenAI Cookbook are:\n",
       "\n",
       "1. **The Retrieval System**\n",
       "2. **Response Generation**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(question2)\n",
    "response = multi_selector_query_engine.query(question2)\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8e7d5d1-c111-4875-9333-42f53721f7f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?\n",
      "\n",
      "INFO:llama_index.core.query_engine.router_query_engine:Selecting query engine 0: Retrieving specific context is relevant to evaluating the performance of different rerankers in the RAG system, as it suggests the ability to retrieve specific information from the documents..\n",
      "Selecting query engine 0: Retrieving specific context is relevant to evaluating the performance of different rerankers in the RAG system, as it suggests the ability to retrieve specific information from the documents..\n",
      "\u001b[1;3;38;5;200mSelecting query engine 0: Retrieving specific context is relevant to evaluating the performance of different rerankers in the RAG system, as it suggests the ability to retrieve specific information from the documents..\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f437fdbca55e452ba8142e46048f6d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.query_engine.router_query_engine:Selecting query engine 1: Summarization questions related to document content are also relevant, as they imply the ability to summarize the content of documents and retrieve specific information..\n",
      "Selecting query engine 1: Summarization questions related to document content are also relevant, as they imply the ability to summarize the content of documents and retrieve specific information..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: Summarization questions related to document content are also relevant, as they imply the ability to summarize the content of documents and retrieve specific information..\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.query_engine.router_query_engine:Combining responses from multiple query engines.\n",
      "Combining responses from multiple query engines.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** According to the provided context information, the two main metrics used to evaluate the performance of the different rerankers in the RAG system are:\n",
       "\n",
       "1. **Hit Rate**: calculates the fraction of queries where the correct answer is found within the top-k retrieved documents.\n",
       "2. **Mean Reciprocal Rank (MRR)**: evaluates the system's accuracy by looking at the rank of the highest-placed relevant document for each query."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(question3)\n",
    "response = multi_selector_query_engine.query(question3)\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5effa0b8-7372-4258-a2c4-f009e22b0441",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?\n",
      "\n",
      "INFO:llama_index.core.query_engine.router_query_engine:Selecting query engine 0: The question is asking about specific context, which is a key aspect of the RAG system's performance evaluation, as it involves retrieving specific information from the documents..\n",
      "Selecting query engine 0: The question is asking about specific context, which is a key aspect of the RAG system's performance evaluation, as it involves retrieving specific information from the documents..\n",
      "\u001b[1;3;38;5;200mSelecting query engine 0: The question is asking about specific context, which is a key aspect of the RAG system's performance evaluation, as it involves retrieving specific information from the documents..\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7ca1dcf5bd47b79fb89aa4449b889d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.query_engine.router_query_engine:Selecting query engine 1: The question also mentions summarization questions related to document content, which is a key evaluation metric for the RAG system's ability to summarize and retrieve relevant information..\n",
      "Selecting query engine 1: The question also mentions summarization questions related to document content, which is a key evaluation metric for the RAG system's ability to summarize and retrieve relevant information..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: The question also mentions summarization questions related to document content, which is a key evaluation metric for the RAG system's ability to summarize and retrieve relevant information..\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.query_engine.router_query_engine:Combining responses from multiple query engines.\n",
      "Combining responses from multiple query engines.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** According to the provided context information, the two main metrics used to evaluate the performance of the different rerankers in the RAG system are:\n",
       "\n",
       "1. **Hit Rate**\n",
       "2. **Mean Reciprocal Rank (MRR)**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(question3)\n",
    "response = multi_selector_query_engine.query(question3)\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8616286-409f-4771-80e3-ad9cc3c6ae72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.query_engine.router_query_engine:Selecting query engine 0: Diffusion models are complex and nuanced, requiring specific context to understand..\n",
      "Selecting query engine 0: Diffusion models are complex and nuanced, requiring specific context to understand..\n",
      "\u001b[1;3;38;5;200mSelecting query engine 0: Diffusion models are complex and nuanced, requiring specific context to understand..\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef69d56c0e994d7598a363e297f9927f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Based on the context information provided, it appears that DiffusionDB is a dataset of text-to-image prompts, and the context does not explicitly define what diffusion models are. However, it can be inferred that DiffusionDB is likely a dataset of text prompts used to generate images using diffusion-based models, such as diffusion models like DALL-E, Stable Diffusion, or other text-to-image generation models.\n",
       "\n",
       "In the context of the text, it seems that DiffusionDB is used to create a vector index for the Text2Image Prompt Assistant tool, which can rewrite prompts to generate more beautiful images. This implies that the prompts in DiffusionDB are used to train or fine-tune diffusion models to generate images, and the tool is able to leverage this dataset to generate new prompts for image generation.\n",
       "\n",
       "Therefore, while the context does not explicitly define what diffusion models are, it is likely that DiffusionDB is a dataset used to train or fine-tune diffusion models for text-to-image generation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = multi_selector_query_engine.query(\"What are diffusion models?\")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f046b42-6c6a-48f7-9f60-ff89416c62cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c8871a51-f53e-43c9-aa4f-0d6e4fe9751d': {'file_path': '/workspace/projects/LlamindexHelper/data/llamaindex-and-transformers-agents-67042ee1d8d6.html',\n",
       "  'file_name': 'llamaindex-and-transformers-agents-67042ee1d8d6.html',\n",
       "  'file_type': 'text/html',\n",
       "  'file_size': 14762,\n",
       "  'creation_date': '2024-07-21',\n",
       "  'last_modified_date': '2024-07-21'},\n",
       " 'selector_result': MultiSelection(selections=[SingleSelection(index=0, reason='Diffusion models are complex and nuanced, requiring specific context to understand.')])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def13d9e-63d0-4a5d-b1c0-2ee6227a5c71",
   "metadata": {},
   "source": [
    "### Subquery engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "172f3b8c-c13b-429f-9a90-53d7a832c462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import SubQuestionQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35919d2a-532a-481d-83dd-b728129e0eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "subquery_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    [vector_tool, summary_tool],\n",
    "    verbose=True,\n",
    "    use_async=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7b5d76a-b0ef-4afd-a306-efd37a73386f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No valid JSON found in output: \n\nTo generate the list of relevant sub-questions, we'll use the following approach:\n\n1. Tokenize the user question into individual words.\n2. Identify the main concepts or entities in the question (e.g., \"Uber\", \"Lyft\", \"revenue growth\", \"EBITDA\").\n3. Map each concept or entity to the relevant tools that can provide information about it.\n4. Generate sub-questions for each tool that can help answer the main question.\n\nHere's the Python code to achieve this:\n\n```python\nimport json\n\ndef generate_sub_questions(user_question, tools):\n    # Tokenize the user question into individual words\n    tokens = user_question.split()\n\n    # Identify the main concepts or entities in the question\n    concepts = []\n    for token in tokens:\n        if token.lower() in [\"uber\", \"lyft\", \"revenue\", \"growth\", \"ebitda\"]:\n            concepts.append(token)\n\n    # Map each concept or entity to the relevant tools\n    sub_questions = []\n    for concept in concepts:\n        for tool_name, description in tools.items():\n            if concept.lower() in description.lower():\n                sub_questions.append({\n                    \"sub_question\": f\"What is the {concept} of {concept}?\",\n                    \"tool_name\": tool_name\n                })\n\n    return sub_questions\n\n# Example 1\ntools = {\n    \"uber_10k\": \"Provides information about Uber financials for year 2021\",\n    \"lyft_10k\": \"Provides information about Lyft financials for year 2021\"\n}\n\nuser_question = \"Compare and contrast the revenue growth and EBITDA of Uber and Lyft for year 2021\"\nprint(json.dumps(generate_sub_questions(user_question, tools), indent=4))\n\n# Example 2\ntools = {\n    \"vector_search\": \"Useful for retrieving specific context\",\n    \"summary\": \"Useful for summarization questions related to document content\"\n}\n\nuser_question = \"What are key features of llama-agents?\"\nprint(json.dumps(generate_sub_questions(user_question, tools), indent=4))\n```\n\nOutput:\n\n```json\n[\n    {\n        \"sub_question\": \"What is the revenue growth of Uber?\",\n        \"tool_name\": \"uber_10k\"\n    },\n    {\n        \"sub_question\": \"What is the EBITDA of Uber?\",\n        \"tool_name\": \"uber_10k\"\n    },\n    {\n        \"sub_question\": \"What is the revenue growth of Lyft?\",\n        \"tool",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reponse \u001b[38;5;241m=\u001b[39m \u001b[43msubquery_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion1\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:260\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    253\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    254\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/llama_index/core/base/base_query_engine.py:52\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     51\u001b[0m         str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 52\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m     54\u001b[0m     QueryEndEvent(query\u001b[38;5;241m=\u001b[39mstr_or_query_bundle, response\u001b[38;5;241m=\u001b[39mquery_result)\n\u001b[1;32m     55\u001b[0m )\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:260\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    253\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    254\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/llama_index/core/query_engine/sub_question_query_engine.py:145\u001b[0m, in \u001b[0;36mSubQuestionQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RESPONSE_TYPE:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    143\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[1;32m    144\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[0;32m--> 145\u001b[0m         sub_questions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_question_gen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m         colors \u001b[38;5;241m=\u001b[39m get_color_mapping([\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sub_questions))])\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose:\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:260\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    253\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    254\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/llama_index/core/question_gen/llm_generators.py:81\u001b[0m, in \u001b[0;36mLLMQuestionGenerator.generate\u001b[0;34m(self, tools, query)\u001b[0m\n\u001b[1;32m     74\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m     75\u001b[0m     prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt,\n\u001b[1;32m     76\u001b[0m     tools_str\u001b[38;5;241m=\u001b[39mtools_str,\n\u001b[1;32m     77\u001b[0m     query_str\u001b[38;5;241m=\u001b[39mquery_str,\n\u001b[1;32m     78\u001b[0m )\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt\u001b[38;5;241m.\u001b[39moutput_parser \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m parse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m parse \u001b[38;5;241m=\u001b[39m cast(StructuredOutput, parse)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parse\u001b[38;5;241m.\u001b[39mparsed_output\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:260\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    253\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    254\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/llama_index/core/question_gen/output_parser.py:13\u001b[0m, in \u001b[0;36mSubQuestionOutputParser.parse\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m     11\u001b[0m json_dict \u001b[38;5;241m=\u001b[39m parse_json_markdown(output)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_dict:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid JSON found in output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# example code includes an 'items' key, which breaks\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# the parsing from open-source LLMs such as Zephyr.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# This gets the actual subquestions and recommended tools directly\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m json_dict:\n",
      "\u001b[0;31mValueError\u001b[0m: No valid JSON found in output: \n\nTo generate the list of relevant sub-questions, we'll use the following approach:\n\n1. Tokenize the user question into individual words.\n2. Identify the main concepts or entities in the question (e.g., \"Uber\", \"Lyft\", \"revenue growth\", \"EBITDA\").\n3. Map each concept or entity to the relevant tools that can provide information about it.\n4. Generate sub-questions for each tool that can help answer the main question.\n\nHere's the Python code to achieve this:\n\n```python\nimport json\n\ndef generate_sub_questions(user_question, tools):\n    # Tokenize the user question into individual words\n    tokens = user_question.split()\n\n    # Identify the main concepts or entities in the question\n    concepts = []\n    for token in tokens:\n        if token.lower() in [\"uber\", \"lyft\", \"revenue\", \"growth\", \"ebitda\"]:\n            concepts.append(token)\n\n    # Map each concept or entity to the relevant tools\n    sub_questions = []\n    for concept in concepts:\n        for tool_name, description in tools.items():\n            if concept.lower() in description.lower():\n                sub_questions.append({\n                    \"sub_question\": f\"What is the {concept} of {concept}?\",\n                    \"tool_name\": tool_name\n                })\n\n    return sub_questions\n\n# Example 1\ntools = {\n    \"uber_10k\": \"Provides information about Uber financials for year 2021\",\n    \"lyft_10k\": \"Provides information about Lyft financials for year 2021\"\n}\n\nuser_question = \"Compare and contrast the revenue growth and EBITDA of Uber and Lyft for year 2021\"\nprint(json.dumps(generate_sub_questions(user_question, tools), indent=4))\n\n# Example 2\ntools = {\n    \"vector_search\": \"Useful for retrieving specific context\",\n    \"summary\": \"Useful for summarization questions related to document content\"\n}\n\nuser_question = \"What are key features of llama-agents?\"\nprint(json.dumps(generate_sub_questions(user_question, tools), indent=4))\n```\n\nOutput:\n\n```json\n[\n    {\n        \"sub_question\": \"What is the revenue growth of Uber?\",\n        \"tool_name\": \"uber_10k\"\n    },\n    {\n        \"sub_question\": \"What is the EBITDA of Uber?\",\n        \"tool_name\": \"uber_10k\"\n    },\n    {\n        \"sub_question\": \"What is the revenue growth of Lyft?\",\n        \"tool"
     ]
    }
   ],
   "source": [
    "reponse = subquery_engine.query(question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0953044-b84b-4ac6-aea3-a4b76478bcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-py310",
   "language": "python",
   "name": "llm-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
