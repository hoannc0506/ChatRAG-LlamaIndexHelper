{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "286022ad-cae2-4777-94c8-1872ae06f77d",
   "metadata": {},
   "source": [
    "## Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225e7ebe-4a4c-49e5-b19d-cf1c7e5d5723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    Document,\n",
    "    Settings,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    SummaryIndex\n",
    ")\n",
    "from llama_index.readers.file import HTMLTagReader, FlatReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import HTMLNodeParser, MarkdownNodeParser, SentenceSplitter\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from IPython.display import display, HTML, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8cc4464-7185-425a-bb3b-8e3868705e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_metadata = json.load(open(\"./data/llama_blogs_metadata.json\", \"r\"))\n",
    "len(docs_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df52367-e4fb-4a40-8b47-7421ba15c8e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Load html documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78f9156d-3774-44c0-a024-7edf3ac13145",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_loader = SimpleDirectoryReader(\n",
    "    input_dir=\"./data/llama-blogs-html/\",\n",
    "    filename_as_id=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3707e0bf-a32e-44fd-ab05-d41d245a4c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_documents = html_loader.load_data(show_progress=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5783b64-2efb-4a80-be00-f79d670ec590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_doc = html_documents[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd07a31f-6d4d-4110-94d8-964ee8e2ab5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_path': '/workspace/projects/LlamindexHelper/data/llama-blogs-html/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.html',\n",
       " 'file_name': 'bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.html',\n",
       " 'file_type': 'text/html',\n",
       " 'file_size': 11333,\n",
       " 'creation_date': '2024-08-08',\n",
       " 'last_modified_date': '2024-08-08'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_doc.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "904f4ac8-ba7e-4525-aac8-01a621167809",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"BlogPost_htmlPost__Z5oDL\">\n",
       " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "  <em>\n",
       "   Co-authored by:\n",
       "   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/u/e84f937083e3?source=post_page-----db42e26ab4f3--------------------------------\" rel=\"noreferrer noopener\">\n",
       "    Riya Jagetia\n",
       "   </a>\n",
       "   ,\n",
       "  </em>\n",
       "  <em>\n",
       "   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/u/7d279643046e?source=post_page-----db42e26ab4f3--------------------------------\" rel=\"noreferrer noopener\">\n",
       "    Tarun Malik\n",
       "   </a>\n",
       "   ,\n",
       "  </em>\n",
       "  <em>\n",
       "   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/u/6fe4060f2a53?source=post_page-----db42e26ab4f3--------------------------------\" rel=\"noreferrer noopener\">\n",
       "    Divija N\n",
       "   </a>\n",
       "   ,\n",
       "  </em>\n",
       "  <em>\n",
       "   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/u/361137e928c3?source=post_page-----db42e26ab4f3--------------------------------\" rel=\"noreferrer noopener\">\n",
       "    Sharon Tan\n",
       "   </a>\n",
       "   ,\n",
       "  </em>\n",
       "  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/u/7bdf7b817eec?source=post_page-----db42e26ab4f3--------------------------------\" rel=\"noreferrer noopener\">\n",
       "   <em>\n",
       "    Zehra Rizvi\n",
       "   </em>\n",
       "  </a>\n",
       "  ,\n",
       "  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/u/b75312a598d6?source=post_page-----db42e26ab4f3--------------------------------\" rel=\"noreferrer noopener\">\n",
       "   <em>\n",
       "    Amanda Piyapanee\n",
       "   </em>\n",
       "  </a>\n",
       " </p>\n",
       " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "  At the recent LlamaIndex RAG-a-thon [1], our team’s\n",
       "  <strong>\n",
       "   “Counselor Copilot”\n",
       "  </strong>\n",
       "  won 2nd prize in the Traditional track and 1st prize in the Datastax/AstraDB category. More details can be found on our\n",
       "  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://devpost.com/software/counselor-copilot\" rel=\"noreferrer noopener\">\n",
       "   DevPost\n",
       "  </a>\n",
       "  [2] writeup.\n",
       " </p>\n",
       " <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n",
       "  Introduction\n",
       " </h2>\n",
       " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "  Against the backdrop of growing strain on mental health services [3, 4], non-profit organizations like The Trevor Project [5] are a critical part of the care ecosystem. Focusing on helping LGBTQ+ youth who are contemplating suicide, The Trevor Project provides accessible crisis services including via TrevorText, an online chat service with trained volunteer counselors.\n",
       " </p>\n",
       " <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n",
       "  Problem: The Dual Challenge Faced by Crisis Counselors\n",
       " </h2>\n",
       " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "  However, TrevorText counselors face significant challenges. Not only is there high demand for counselors during busy times like holidays and night shifts, but also, counselors have to juggle a number of administrative tasks such as sifting through forms, responding to messages across multiple chats, and locating relevant local resources. This not only increases the risk of counselors burning out but also hampers their ability to provide timely and effective care.\n",
       " </p>\n",
       " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "  In light of these challenges, there’s a pressing need for innovative solutions to bridge the gap between the demand and supply of crisis services.\n",
       " </p>\n",
       " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "  While our hackathon project focused on augmenting TrevorText, our product can be easily extended to general crisis chat alternatives as well.\n",
       " </p>\n",
       " <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n",
       "  The Winning Solution: An AI Copilot for Crisis Counselors\n",
       " </h2>\n",
       " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "  Counselor Copilot is a real-time assistant for crisis counselors that takes into account contact context and chat history to suggest replies so that counselors can focus on what they do best: providing care.\n",
       "  <strong>\n",
       "   There is no prompting that is needed from counselors; the copilot works seamlessly in the background.\n",
       "  </strong>\n",
       " </p>\n",
       " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "  Further, the copilot never directly replies to contacts; instead, replies are suggested and can be edited.\n",
       " </p>\n",
       " <figure>\n",
       "  <figcaption>\n",
       "   Counselor copilot takes into account contact context and chat history to provide real-time reply suggestions to the counselors\n",
       "  </figcaption>\n",
       " </figure>\n",
       " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "  Specifically, the copilot automates counselor tasks that include but are not limited to:\n",
       " </p>\n",
       " <ol>\n",
       "  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "   Retrieving and synthesizing contact data from complex PDFs in real-time. This also provides counselors context on their contacts when conversations are initiated.\n",
       "  </li>\n",
       "  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "   Assessing from the chat context if emergency intervention is required. If so, suggesting escalation to a supervisor.\n",
       "  </li>\n",
       "  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "   Using existing resources and guidelines from the organization to suggest appropriate replies.\n",
       "  </li>\n",
       " </ol>\n",
       " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "  4. Searching for location-specific resources for contacts, and quickly sharing those resources via email.\n",
       " </p>\n",
       " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "  5. Completing case forms in a CRM for contacts, including summarizing the interaction.\n",
       " </p>\n",
       " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "  While these tasks are important and necessary, they pull attention away from conversations with youth in crisis and take up precious time.\n",
       " </p>\n",
       " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "  With Counselor copilot, these tasks are completed when they are required and without any prompting from counselors, providing more bandwidth for counselors and ultimately leading to higher-quality conversations with patients.\n",
       " </p>\n",
       " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "  Below is a demo of our solution:\n",
       " </p>\n",
       " <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n",
       "  How we built it\n",
       " </h2>\n",
       " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "  When the chat is initiated, the Counselor Copilot gets the contact’s data from the CRM, which is stored in complex PDFs. We used LlamaParse to extract relevant contact data in real-time and then provide a summary of that data to counselors as context at the beginning of each conversation.\n",
       " </p>\n",
       " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "  Further, we used a LlamaIndex ReAct Agent to monitor the conversation and — based on the chat history and contact context — deploy the right tool. Tools at the ReAct Agent’s disposal include:\n",
       " </p>\n",
       " <ol>\n",
       "  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "   Escalating the conversation to a supervisor\n",
       "  </li>\n",
       "  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "   Suggesting a response and related resources based on The Trevor Project’s guidelines\n",
       "  </li>\n",
       "  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "   Searching the web for location-specific resources and sending the resources to the contact\n",
       "  </li>\n",
       " </ol>\n",
       " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "  For tool #2, we created a vector database that contains The Trevor Project’s documents, which highlight key guidelines for counselors based on different scenarios and situations that they may face. We used RAG to retrieve resources relevant to the conversation, and GPT4 to draft a response for the counselor based on those resources, both of which are essential due to the sensitive nature of the conversation.\n",
       " </p>\n",
       " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "  Lastly, we used the conversation content to fill out a form with key Salesforce fields (e.g. name, age, city, state), as well as to summarize the conversation.\n",
       " </p>\n",
       " <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n",
       "  Possible Extensions\n",
       " </h2>\n",
       " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "  We’re excited by the potential for others to build on our work [6] and extend Counselor Copilot further. Some ideas include:\n",
       " </p>\n",
       " <ol>\n",
       "  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "   Reduce costs and improve quality of suggested responses: Fine-tune a state-of-the-art open-source LLM on extracts of chat conversations conducted by counselors\n",
       "  </li>\n",
       "  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "   More targeted conversation management: Add a tool for the agent to assess the stage of the conversation, given that there are recommended styles and questions for each stage (e.g. establishing rapport, risk assessment)\n",
       "  </li>\n",
       "  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "   Closed-loop feedback cycle: Allow counselors to thumbs-up or thumbs-down selected responses, as a natural way to collect human feedback which can be used for further model or agent training\n",
       "  </li>\n",
       " </ol>\n",
       " <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n",
       "  Conclusion: A Step Toward Efficient and Effective Crisis Care\n",
       " </h2>\n",
       " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "  Our AI copilot for crisis counselors represents a significant step toward more efficient and effective crisis care. By automating administrative tasks, it frees up counselors to focus on their core mission of providing youth in crisis a safe place to talk. This not only enhances the quality of care provided but also addresses the pressing issue of counselor shortage by maximizing the impact of existing resources. As we continue to refine and expand this technology, we envision a future where crisis counseling is more accessible, responsive, and impactful for all those in need​​.\n",
       " </p>\n",
       " <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n",
       "  References\n",
       " </h2>\n",
       " <ol>\n",
       "  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://rag-a-thon.devpost.com/\" rel=\"noreferrer noopener\">\n",
       "    https://rag-a-thon.devpost.com/\n",
       "   </a>\n",
       "  </li>\n",
       "  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://devpost.com/software/counselor-copilot\" rel=\"noreferrer noopener\">\n",
       "    https://devpost.com/software/counselor-copilot\n",
       "   </a>\n",
       "  </li>\n",
       "  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.mhanational.org/issues/state-mental-health-america\" rel=\"noreferrer noopener\">\n",
       "    https://www.mhanational.org/issues/state-mental-health-america\n",
       "   </a>\n",
       "  </li>\n",
       "  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.aamc.org/news/growing-psychiatrist-shortage-enormous-demand-mental-health-services\" rel=\"noreferrer noopener\">\n",
       "    https://www.aamc.org/news/growing-psychiatrist-shortage-enormous-demand-mental-health-services\n",
       "   </a>\n",
       "  </li>\n",
       "  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.thetrevorproject.org/\" rel=\"noreferrer noopener\">\n",
       "    https://www.thetrevorproject.org/\n",
       "   </a>\n",
       "  </li>\n",
       "  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
       "   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/zrizvi93/trevorhack\" rel=\"noreferrer noopener\">\n",
       "    https://github.com/zrizvi93/trevorhack\n",
       "   </a>\n",
       "  </li>\n",
       " </ol>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(test_doc.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6ca086-5bfc-4400-a5c4-05f8f2b36d2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Preview markdown document loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fed4ee10-db0e-4e9e-8b2e-454adfec8d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_reader = FlatReader()\n",
    "parser = MarkdownNodeParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f45c6d67-d262-4525-bc1b-645e0a10d1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Towards Long Context RAG',\n",
       " 'date': 'Mar 1, 2024',\n",
       " 'url': 'https://www.llamaindex.ai/blog/towards-long-context-rag'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_doc_metadata = docs_metadata[50]\n",
    "test_doc_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71fa9249-dfd0-43c3-8df5-1b807f119034",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_dir = \"./data/llama-blogs-md/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8ac70b4-474f-4cb9-bba4-4ba2659ceef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/llama-blogs-md/towards-long-context-rag.md'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.path.join(md_dir, test_doc_metadata[\"url\"].split(\"/\")[-1] + \".md\")\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84b939b8-0187-4083-8f23-8503ecc065c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_docs = md_reader.load_data(Path(file_path), extra_info=doc0_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14031a1d-042e-490a-b427-a94036987222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': 'towards-long-context-rag.md',\n",
       " 'extension': '.md',\n",
       " 'title': 'Towards Long Context RAG',\n",
       " 'date': 'Mar 1, 2024',\n",
       " 'url': 'https://www.llamaindex.ai/blog/towards-long-context-rag'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa39c45a-bd80-4912-8947-70634cabc429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 400.49it/s]\n"
     ]
    }
   ],
   "source": [
    "nodes = parser.get_nodes_from_documents(md_docs, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c293666-13ff-461c-a182-b5b0a09bbb65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f91d67d9-d3a0-4e8a-b21d-72ebd831f578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'towards-long-context-rag.md', 'extension': '.md', 'title': 'Towards Long Context RAG', 'date': 'Mar 1, 2024', 'url': 'https://www.llamaindex.ai/blog/towards-long-context-rag'}\n",
      "{'Header_2': ' Our Mission Goes Beyond RAG', 'filename': 'towards-long-context-rag.md', 'extension': '.md', 'title': 'Towards Long Context RAG', 'date': 'Mar 1, 2024', 'url': 'https://www.llamaindex.ai/blog/towards-long-context-rag'}\n",
      "{'Header_2': ' Initial Gemini 1.5 Pro Observations', 'filename': 'towards-long-context-rag.md', 'extension': '.md', 'title': 'Towards Long Context RAG', 'date': 'Mar 1, 2024', 'url': 'https://www.llamaindex.ai/blog/towards-long-context-rag'}\n",
      "{'Header_2': ' Long Contexts Resolve Some Pain Points, but some Challenges Remain', 'filename': 'towards-long-context-rag.md', 'extension': '.md', 'title': 'Towards Long Context RAG', 'date': 'Mar 1, 2024', 'url': 'https://www.llamaindex.ai/blog/towards-long-context-rag'}\n",
      "{'Header_2': ' Towards New RAG Architectures', 'filename': 'towards-long-context-rag.md', 'extension': '.md', 'title': 'Towards Long Context RAG', 'date': 'Mar 1, 2024', 'url': 'https://www.llamaindex.ai/blog/towards-long-context-rag'}\n",
      "{'Header_2': ' Towards New RAG Architectures', 'Header_3': ' 1\\\\. Small to Big Retrieval over Documents', 'filename': 'towards-long-context-rag.md', 'extension': '.md', 'title': 'Towards Long Context RAG', 'date': 'Mar 1, 2024', 'url': 'https://www.llamaindex.ai/blog/towards-long-context-rag'}\n",
      "{'Header_2': ' Towards New RAG Architectures', 'Header_3': ' 2\\\\. Intelligent Routing for Latency/Cost Tradeoffs', 'filename': 'towards-long-context-rag.md', 'extension': '.md', 'title': 'Towards Long Context RAG', 'date': 'Mar 1, 2024', 'url': 'https://www.llamaindex.ai/blog/towards-long-context-rag'}\n",
      "{'Header_2': ' Towards New RAG Architectures', 'Header_3': ' 3\\\\. Retrieval Augmented KV Caching', 'filename': 'towards-long-context-rag.md', 'extension': '.md', 'title': 'Towards Long Context RAG', 'date': 'Mar 1, 2024', 'url': 'https://www.llamaindex.ai/blog/towards-long-context-rag'}\n",
      "{'Header_2': ' Towards New RAG Architectures', 'Header_3': ' What’s Next', 'filename': 'towards-long-context-rag.md', 'extension': '.md', 'title': 'Towards Long Context RAG', 'date': 'Mar 1, 2024', 'url': 'https://www.llamaindex.ai/blog/towards-long-context-rag'}\n"
     ]
    }
   ],
   "source": [
    "for node in nodes:\n",
    "    print(node.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff71b0f-535a-4235-a501-224a119c9174",
   "metadata": {},
   "source": [
    "### Markdown documents injestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71ad6a55-145f-4f18-83ec-a18a4afbc5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_reader = FlatReader()\n",
    "parser = MarkdownNodeParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a20fe026-c371-4d61-a188-6f12b88c4ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_dir = \"./data/llama-blogs-md/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d08fa854-c664-443e-95c3-b4dec9501ff8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing documents: 100%|██████████| 166/166 [00:00<00:00, 11530.83it/s]\n"
     ]
    }
   ],
   "source": [
    "loaded_documents = []\n",
    "for doc_metadata in tqdm(docs_metadata, desc=\"Parsing documents\"):\n",
    "    file_path = os.path.join(md_dir, doc_metadata[\"url\"].split(\"/\")[-1] + \".md\")\n",
    "    md_docs = md_reader.load_data(Path(file_path), extra_info=doc_metadata)\n",
    "    loaded_documents = loaded_documents + md_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ffa08d3-8a27-444d-b5b6-de9856eaf4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loaded_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4a143f1-98f1-45d0-9a7d-c873be5c7929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store documents\n",
    "md_docs_store = SimpleDocumentStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e549d20-1968-407a-b873-12cd6f13a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_docs_store.add_documents(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f063fae-dee4-40e4-8d96-a880fe870d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_docs_store.persist(\"database/llama-blogs-nodes-md-parser.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379942c6-b415-4bff-86bd-74df91c61011",
   "metadata": {},
   "source": [
    "### Split documents to nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b667061-e611-4942-9bae-0030d601ef1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 166/166 [00:00<00:00, 1001.43it/s]\n"
     ]
    }
   ],
   "source": [
    "nodes = parser.get_nodes_from_documents(loaded_documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca327869-d7a1-4502-ac14-f08998a1d658",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1221"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53e2031f-4a59-4f1a-bb04-cca1836ac172",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Greetings, Llama Lovers!\n",
       "\n",
       "Welcome to this week’s edition of the LlamaIndex newsletter! We’re excited to\n",
       "share our latest updates including dynamic features like LlamaIndex Workflows\n",
       "and retrieval capabilities in LlamaCloud. Check out our in-depth guides,\n",
       "tutorials, and the upcoming webinars that will help you make the most of these\n",
       "new developments."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'llamaindex-newsletter-2024-08-06.md', 'extension': '.md', 'title': 'LlamaIndex Newsletter 2024-08-06', 'date': 'Aug 6, 2024', 'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-08-06'}\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**The highlights:**\n",
       "\n",
       "  1. **LlamaIndex Workflows Launched:** LlamaIndex Workflows, a new event-driven architecture for building multi-agent applications, supports batching, async operations, and streaming. Agents subscribe to and emit events for complex, readable, Pythonic orchestration. [ Blogpost ](https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex) , [ Tweet ](https://x.com/llama_index/status/1819048068798616058) . \n",
       "  2. **Dynamic Retrieval Feature in LlamaCloud:** A new feature in LlamaCloud now supports dynamic retrieval for QA assistants, enabling both chunk-level and file-level document retrieval based on query similarity to intelligently route queries. [ Blogpost ](https://www.llamaindex.ai/blog/dynamic-retrieval-with-llamacloud) , [ Notebook ](https://github.com/run-llama/llamacloud-demo/blob/main/examples/10k_apple_tesla/demo_file_retrieval.ipynb) , [ Tweet ](https://x.com/llama_index/status/1818337133746360623) . \n",
       "  3. **LongRAG LlamaPack:** LongRAG is now available as a LlamaPack in LlamaIndex, utilizing larger document chunks and long-context LLMs for more effective synthesis. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-longrag/examples/longrag.ipynb) , [ Tweet ](https://x.com/llama_index/status/1818802688274100578) ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Header_2': ' **The highlights:**', 'filename': 'llamaindex-newsletter-2024-08-06.md', 'extension': '.md', 'title': 'LlamaIndex Newsletter 2024-08-06', 'date': 'Aug 6, 2024', 'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-08-06'}\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Feature Releases and Enhancements:**\n",
       "\n",
       "  * We have launched LlamaIndex Workflows, a new event-driven way to build multi-agent applications where each agent acts as a component that subscribes to and emits events, allowing for complex, readable, and Pythonic orchestration with enhanced support for batching, async operations, and streaming. [ Blogpost ](https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex) , [ Tweet ](https://x.com/llama_index/status/1819048068798616058) . \n",
       "  * We have introduced a new feature in LlamaCloud to improve your QA assistant with our latest capability for dynamic retrieval, allowing both chunk-level and file-level retrieval. This feature enables the retrieval of entire documents based on query similarity, which supports building agents that can intelligently route queries based on their content. [ Blogpost ](https://www.llamaindex.ai/blog/dynamic-retrieval-with-llamacloud) , [ Notebook ](https://github.com/run-llama/llamacloud-demo/blob/main/examples/10k_apple_tesla/demo_file_retrieval.ipynb) , [ Tweet ](https://x.com/llama_index/status/1818337133746360623) . \n",
       "  * We have launched LongRAG as a LlamaPack in LlamaIndex. LongRAG simplifies retrieval by using larger document chunks and leveraging long-context LLMs for synthesis. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-longrag/examples/longrag.ipynb) , [ Tweet ](https://x.com/llama_index/status/1818802688274100578) . \n",
       "\n",
       "**Guides:**\n",
       "\n",
       "  * [ Guide ](https://docs.llamaindex.ai/en/latest/examples/workflow/react_agent/) to building a ReAct agent from scratch using LlamaIndex workflows. \n",
       "  * [ Guide ](https://docs.llamaindex.ai/en/latest/examples/workflow/rag/) to Building an Event-Driven RAG Pipeline with LlamaIndex, featuring distinct event-driven steps for retrieval, reranking, and synthesis, enhanced with graph tracing and async processing. \n",
       "  * [ Guide ](https://docs.llamaindex.ai/en/latest/module_guides/observability/#mlflow) to MLflow in LlamaIndex to manage, deploy, and monitor your genAI applications with MLflow's tracking, packaging, evaluation, and tracing capabilities. \n",
       "\n",
       "**Tutorials:**\n",
       "\n",
       "  * [ Pavan Kumar’s ](https://x.com/pavan_mantha1) [ tutorial ](https://blog.gopenai.com/building-smarter-agents-using-llamaindex-agents-and-qdrants-hybrid-search-50c0ecbbfb0d) on Building Smarter Agents using LlamaIndex Agents and Qdrant’s Hybrid Search. \n",
       "  * [ Farzad Sunavala’s ](https://www.linkedin.com/in/farzadsunavala) [ tutorial ](https://farzzy.hashnode.dev/rag-observability-and-evaluation-with-azure-ai-search-azure-openai-llamaindex-and-arize-phoenix) on RAG Observability and Evaluation with Azure AI Search, Azure OpenAI, LlamaIndex, and Arize Phoenix. \n",
       "  * [ Composio’s ](https://x.com/composiohq) [ tutorial ](https://github.com/ComposioHQ/composio/tree/master/python/examples/pr_agent/pr_agent_llama_index) on building a PR review agent using Composio's GitHub/Slack tools and LlamaIndex agent abstractions. \n",
       "  * [ Benito Martin’s ](https://medium.com/@benitomartin) [ tutorial ](https://medium.com/@benitomartin/find-your-code-scaling-a-llamaindex-and-qdrant-application-with-google-kubernetes-engine-2db126f16344) on Scaling a LlamaIndex and Qdrant Application with Google Kubernetes Engine. \n",
       "  * [ Chew Loong Nian’s ](https://medium.com/@chewloongnian) [ tutorial ](https://pub.towardsai.net/introducing-llamaextract-beta-transforming-metadata-extraction-for-enhanced-rag-queries-de3d74d34cd7) on Transforming Metadata Extraction for Enhanced RAG Queries using LlamaExtract. \n",
       "  * [ Pavan Kumar’s ](https://x.com/pavan_mantha1) [ tutorial ](https://medium.com/@manthapavankumar11/practical-implementation-of-agentic-rag-workflows-with-llama-index-and-qdrant-3b6622cd3124) on Practical Implementation of Agentic RAG Workflows with Llama-Index and Qdrant. \n",
       "  * AI21 Labs [ tutorial ](https://www.llamaindex.ai/blog/jamba-instruct-s-256k-context-window-on-llamaindex) on using Jamba-Instruct Model with LlamaIndex. \n",
       "\n",
       "**Webinars And Hackathons:**\n",
       "\n",
       "  * [ Join us ](https://lu.ma/ka5xtyqo) for a webinar on August 8th with [ Dedy Kredo ](https://x.com/DedyKredo) from [ CodiumAI ](https://x.com/CodiumAI) on using RAG with LlamaIndex to help build a code generation solution that’s contextually aware of the right elements of source code. \n",
       "  * [ Join us ](https://lu.ma/p13pkknm?tk=SsniSt) on RAG Hack Night at GitHub with [ Weaviate ](https://x.com/weaviate_io) , [ Neosync ](https://x.com/neosynccloud) , [ Arize AI ](https://x.com/arizeai) on August 13th."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Header_2': ' **Feature Releases and Enhancements:**', 'filename': 'llamaindex-newsletter-2024-08-06.md', 'extension': '.md', 'title': 'LlamaIndex Newsletter 2024-08-06', 'date': 'Aug 6, 2024', 'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-08-06'}\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "We’re pleased to be introducing a brand-new beta feature of LlamaIndex:\n",
       "workflows, a mechanism for orchestrating actions in the increasingly-complex\n",
       "AI application we see our users building.\n",
       "\n",
       "What started as a trend with the advent of LLMs is now a de-facto standard: AI\n",
       "applications are made of multiple tasks implemented by different components.\n",
       "Open source frameworks in the market strive to make the life of AI engineers\n",
       "easier by providing easy-to-use abstractions for foundational components like\n",
       "data loaders, LLMs, vector databases, and rerankers, all the way up to\n",
       "external services. Meanwhile, all of those frameworks are also on a quest to\n",
       "find what’s the best abstraction to orchestrate such components, researching\n",
       "what’s most intuitive and efficient for an AI developer in order to implement\n",
       "the logic that keeps together a compound AI system.\n",
       "\n",
       "Two of those potential orchestration patterns are chains and pipelines, both\n",
       "of which are implementations of the same Directed Acyclic Graph (DAG)\n",
       "abstraction. We took a stab at this with our [ Query Pipelines\n",
       "](https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537)\n",
       "release at the beginning of the year - it was a declarative API that let you\n",
       "orchestrate simple-to-advanced query workflows over your data for different\n",
       "use cases, like QA, structured extraction, and agentic automation. But as we\n",
       "tried to build upon it and experimented with adding cycles to better support\n",
       "more complex workflows, we noticed several issues, causing us to reflect on\n",
       "why a DAG may not be the right fit for an agentic landscape, and what\n",
       "alternatives we could introduce in the framework."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex.md', 'extension': '.md', 'title': 'Introducing workflows beta: a new way to create complex AI applications with LlamaIndex', 'date': 'Aug 1, 2024', 'url': 'https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex'}\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Limitations of a Graph-based UX\n",
       "\n",
       "A fundamental aspect of DAGs is the “A” in DAGs: they are acyclic, meaning\n",
       "there are no loops. But in a world that’s more and more agentic, the inability\n",
       "to perform loops in an AI application’s logic is simply unacceptable. For\n",
       "example, if one component provides bad results, an AI developer should have a\n",
       "way to tell the system to self-correct and try again.\n",
       "\n",
       "Even without adding cycles and loops to a DAG, the query pipeline suffered\n",
       "from a few noticeable issues:\n",
       "\n",
       "  * hard to debug when things go wrong \n",
       "  * they obscure how components and modules are being executed \n",
       "  * our pipeline orchestrator became increasingly extremely complex and had to handle a ton of different edge cases \n",
       "  * they were hard to read for complex pipelines \n",
       "\n",
       "Once we added cycles to query pipelines, these developer UX issues around\n",
       "graphs were amplified. We experienced first-hand developer pain in areas like:\n",
       "\n",
       "  * A lot of core orchestration logic like ` if-else ` statements and ` while ` loops get baked into the edges of the graph. Defining these edges becomes cumbersome and verbose. \n",
       "  * It became hard to handle edge cases around optional and default values. It was hard for us as a framework to figure out whether a parameter would get passed from upstream nodes. \n",
       "  * Defining graphs with cycles didn’t always feel as natural to developers building agents. An agent encapsulates a general LLM-powered entity that can take in observations and generate responses. Here the graph UX enforced that “agent” node had the incoming edges and outgoing edges explicitly defined, forcing users to define verbose communication patterns with other nodes. \n",
       "\n",
       "We asked: are graphs really the only abstraction we can use to orchestrate\n",
       "components in a compound AI system?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Header_2': ' Limitations of a Graph-based UX', 'filename': 'introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex.md', 'extension': '.md', 'title': 'Introducing workflows beta: a new way to create complex AI applications with LlamaIndex', 'date': 'Aug 1, 2024', 'url': 'https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex'}\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "From Graphs to EDA: go event-driven\n",
       "\n",
       "A compound AI system can be implemented with a LlamaIndex _workflow_ . The\n",
       "workflow dispatches events back and forth through a collection of Python\n",
       "functions called _steps_ . Each step can be seen as one component of your\n",
       "system: one to process a query, one to talk with an LLM, one to load data from\n",
       "a vector database and so on. Every step receives one or more events to process\n",
       "and can optionally send back events that will be relayed to other components\n",
       "if needed.\n",
       "\n",
       "Moving to an event-driven architecture causes a fundamental shift in design.\n",
       "In many graph implementations the graph traversal algorithm is responsible for\n",
       "determining what component should run next and what data should be passed. In\n",
       "an event-driven architecture, the component subscribes to a certain types of\n",
       "events and it’s ultimately responsible for deciding what to do based on the\n",
       "data it received.\n",
       "\n",
       "In an event-driven system, concepts like optionality of inputs and default\n",
       "values are sorted out at the component level, dramatically simplifying the\n",
       "orchestration code."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Header_2': ' From Graphs to EDA: go event-driven', 'filename': 'introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex.md', 'extension': '.md', 'title': 'Introducing workflows beta: a new way to create complex AI applications with LlamaIndex', 'date': 'Aug 1, 2024', 'url': 'https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex'}\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "A workflow primer\n",
       "\n",
       "To help clarify this idea, let’s look at an example. A minimal LlamaIndex\n",
       "workflow looks like this:\n",
       "\n",
       "    \n",
       "    \n",
       "    from llama_index.core.workflow import (\n",
       "        StartEvent,\n",
       "        StopEvent,\n",
       "        Workflow,\n",
       "        step,\n",
       "    )\n",
       "    \n",
       "    from llama_index.llms.openai import OpenAI\n",
       "    \n",
       "    class OpenAIGenerator(Workflow):\n",
       "        @step()\n",
       "        async def generate(self, ev: StartEvent) -> StopEvent:\n",
       "            query = ev.get(\"query\")\n",
       "            llm = OpenAI()\n",
       "            response = await llm.acomplete(query)\n",
       "            return StopEvent(result=str(response))\n",
       "    \n",
       "    w = OpenAIGenerator(timeout=10, verbose=False)\n",
       "    result = await w.run(query=\"What's LlamaIndex?\")\n",
       "    print(result)\n",
       "\n",
       "The ` generate ` function is marked as a workflow step using the ` @step `\n",
       "decorator and it declares which events it wants to receive and which events it\n",
       "will send back using the method signature with proper typing annotations. In\n",
       "order to run a workflow, we create an instance of the ` OpenAIGenerator `\n",
       "class passing some configuration parameters like the desired timeout and we\n",
       "then call the ` run ` method. Any keyword argument passed to ` run ` will be\n",
       "packed into a special event of type ` StartEvent ` that will be relayed to the\n",
       "steps that requested it (in this case, only the ` generate ` step). The `\n",
       "generate ` step returns a special event of type ` StopEvent ` that will signal\n",
       "the workflow to gracefully halt its execution. A ` StopEvent ` carries any\n",
       "data that we want to return to the caller as the workflow result, in this case\n",
       "the LLM response."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Header_2': ' A workflow primer', 'filename': 'introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex.md', 'extension': '.md', 'title': 'Introducing workflows beta: a new way to create complex AI applications with LlamaIndex', 'date': 'Aug 1, 2024', 'url': 'https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex'}\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Workflows can loop\n",
       "\n",
       "In event-driven architectures, loops have to do with communication rather than\n",
       "topology. Any step can decide to call another step multiple times by crafting\n",
       "and sending the proper event. Let’s see a self-correction loop for example\n",
       "(check the [ notebook\n",
       "](https://docs.llamaindex.ai/en/latest/examples/workflow/reflection/) for the\n",
       "full code):\n",
       "\n",
       "    \n",
       "    \n",
       "    class ExtractionDone(Event):\n",
       "        output: str\n",
       "        passage: str\n",
       "    \n",
       "    \n",
       "    class ValidationErrorEvent(Event):\n",
       "        error: str\n",
       "        wrong_output: str\n",
       "        passage: str\n",
       "        \n",
       "        \n",
       "    class ReflectionWorkflow(Workflow):\n",
       "        @step()\n",
       "        async def extract(\n",
       "            self, ev: StartEvent | ValidationErrorEvent\n",
       "        ) -> StopEvent | ExtractionDone:\n",
       "            if isinstance(ev, StartEvent):\n",
       "                passage = ev.get(\"passage\")\n",
       "                if not passage:\n",
       "                    return StopEvent(result=\"Please provide some text in input\")\n",
       "                reflection_prompt = \"\"\n",
       "            elif isinstance(ev, ValidationErrorEvent):\n",
       "                passage = ev.passage\n",
       "                reflection_prompt = REFLECTION_PROMPT.format(\n",
       "                    wrong_answer=ev.wrong_output, error=ev.error\n",
       "                )\n",
       "    \n",
       "            llm = Ollama(model=\"llama3\", request_timeout=30)\n",
       "            prompt = EXTRACTION_PROMPT.format(\n",
       "                passage=passage, schema=CarCollection.schema_json()\n",
       "            )\n",
       "            if reflection_prompt:\n",
       "                prompt += reflection_prompt\n",
       "    \n",
       "            output = await llm.acomplete(prompt)\n",
       "    \n",
       "            return ExtractionDone(output=str(output), passage=passage)\n",
       "    \n",
       "        @step()\n",
       "        async def validate(\n",
       "            self, ev: ExtractionDone\n",
       "        ) -> StopEvent | ValidationErrorEvent:\n",
       "            try:\n",
       "                json.loads(ev.output)\n",
       "            except Exception as e:\n",
       "                print(\"Validation failed, retrying...\")\n",
       "                return ValidationErrorEvent(\n",
       "                    error=str(e), wrong_output=ev.output, passage=ev.passage\n",
       "                )\n",
       "    \n",
       "            return StopEvent(result=ev.output)\n",
       "    \n",
       "    w = ReflectionWorkflow(timeout=60, verbose=True)\n",
       "    result = await w.run(\n",
       "        passage=\"There are two cars available: a Fiat Panda with 45Hp and a Honda Civic with 330Hp.\"\n",
       "    )\n",
       "    print(result)\n",
       "\n",
       "In this example, the ` validate ` step receives the result of the tentative\n",
       "schema extraction as an event and it can decide to try again by returning a `\n",
       "ValidationErrorEvent ` that will be eventually delivered to the ` extract `\n",
       "step which will perform another attempt. Note that in this example the\n",
       "workflow might time out if this extract/validate loop keeps providing poor\n",
       "results for too long, but another strategy might be giving up after a precise\n",
       "number of attempts, just to give an example."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Header_2': ' A workflow primer', 'Header_3': ' Workflows can loop', 'filename': 'introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex.md', 'extension': '.md', 'title': 'Introducing workflows beta: a new way to create complex AI applications with LlamaIndex', 'date': 'Aug 1, 2024', 'url': 'https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex'}\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Workflows keep state\n",
       "\n",
       "Workflows keep a global state during the execution, and this state can be\n",
       "shared and propagated to its steps upon request. This shared state is\n",
       "implemented as a ` Context ` object and can be used by steps to store data in\n",
       "between iterations but also as an alternative form of communication among\n",
       "different steps. Let’s see an excerpt from a more complex RAG example as an\n",
       "example showing how to use the global context (check [ notebook\n",
       "](https://docs.llamaindex.ai/en/latest/examples/workflow/rag/) for full code):\n",
       "\n",
       "    \n",
       "    \n",
       "    class RAGWorkflow(Workflow):\n",
       "        @step(pass_context=True)\n",
       "        async def ingest(self, ctx: Context, ev: StartEvent) -> Optional[StopEvent]:\n",
       "            dataset_name = ev.get(\"dataset\")\n",
       "            _, documents = download_llama_dataset(dsname, \"./data\")\n",
       "            ctx.data[\"INDEX\"] = VectorStoreIndex.from_documents(documents=documents)\n",
       "            return StopEvent(result=f\"Indexed {len(documents)} documents.\")\n",
       "            \n",
       "        ...\n",
       "\n",
       "In this case the ` ingest ` step creates an index, and it wants to make it\n",
       "available to any other step that might needed it later during workflow\n",
       "execution. The idiomatic way of doing that in a LlamaIndex workflow is to\n",
       "declare the step requires an instance of the global context ( `\n",
       "@step(pass_context=True) ` does the trick) and store the index in the context\n",
       "itself with a predefined key that other steps might access later."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Header_2': ' A workflow primer', 'Header_3': ' Workflows keep state', 'filename': 'introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex.md', 'extension': '.md', 'title': 'Introducing workflows beta: a new way to create complex AI applications with LlamaIndex', 'date': 'Aug 1, 2024', 'url': 'https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex'}\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Workflows can be customized\n",
       "\n",
       "Alongside Workflows, we’ll be releasing a set of predefined workflows so that\n",
       "the most common use cases can be implemented with a single line of code. Using\n",
       "these predefined flows, users still might want to just _slightly_ change a\n",
       "predefined workflow to introduce some custom behavior without having to\n",
       "rewrite a whole workflow from scratch. Let’s say you want to customize a RAG\n",
       "workflow and use a custom re-ranking step, all you would need to do is\n",
       "subclass a hypothetical built-in ` RAGWorkflow ` class and override the `\n",
       "rerank ` step like this:\n",
       "\n",
       "    \n",
       "    \n",
       "    class MyWorkflow(RAGWorkflow):\n",
       "        @step(pass_context=True)\n",
       "        def rerank(\n",
       "            self, ctx: Context, ev: Union[RetrieverEvent, StartEvent]\n",
       "        ) -> Optional[QueryResult]:\n",
       "            # my custom reranking logic here\n",
       "            \n",
       "     \n",
       "    w = MyWorkflow(timeout=60, verbose=True)\n",
       "    result = await w.run(query=\"Who is Paul Graham?\")"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Header_2': ' A workflow primer', 'Header_3': ' Workflows can be customized', 'filename': 'introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex.md', 'extension': '.md', 'title': 'Introducing workflows beta: a new way to create complex AI applications with LlamaIndex', 'date': 'Aug 1, 2024', 'url': 'https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex'}\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for node in nodes[:10]:\n",
    "    display(Markdown(node.text))\n",
    "    print(node.metadata)\n",
    "    print(\"==\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e4fdfd-47b2-4bd4-8cf2-86d87bec8778",
   "metadata": {},
   "source": [
    "## Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e87a5dd6-e9b1-4a65-9b24-cacb383306f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import (\n",
    "    SentenceSplitter,\n",
    "    SemanticSplitterNodeParser,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a439b50e-79b3-4240-a902-5ea6d81820ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"models/bge-base-en-v1.5\",\n",
    "    device=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9196d6b8-0a5e-4245-92e6-f06176f747bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1, breakpoint_percentile_threshold=95, embed_model=Settings.embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d19d180b-85ce-44e5-be64-406acfaec627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes:   0%|          | 0/166 [00:00<?, ?it/s]\n",
      "Generating embeddings:   0%|          | 0/34 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  29%|██▉       | 10/34 [00:00<00:00, 28.39it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 34/34 [00:00<00:00, 66.55it/s]\u001b[A\n",
      "Parsing nodes:   1%|          | 1/166 [00:00<01:27,  1.90it/s]\n",
      "Generating embeddings:   0%|          | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  58%|█████▊    | 30/52 [00:00<00:00, 191.79it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 52/52 [00:00<00:00, 128.37it/s]\u001b[A\n",
      "Parsing nodes:   1%|          | 2/166 [00:00<01:15,  2.16it/s]\n",
      "Generating embeddings:   0%|          | 0/42 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  48%|████▊     | 20/42 [00:00<00:00, 138.20it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 42/42 [00:00<00:00, 97.05it/s] \u001b[A\n",
      "Parsing nodes:   2%|▏         | 3/166 [00:01<01:14,  2.20it/s]\n",
      "Generating embeddings:   0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 35/35 [00:00<00:00, 187.87it/s]\u001b[A\n",
      "Parsing nodes:   2%|▏         | 4/166 [00:01<00:57,  2.83it/s]\n",
      "Generating embeddings:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 33/33 [00:00<00:00, 202.82it/s]\u001b[A\n",
      "Parsing nodes:   3%|▎         | 5/166 [00:01<00:46,  3.46it/s]\n",
      "Generating embeddings:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  50%|█████     | 20/40 [00:00<00:00, 187.59it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 40/40 [00:00<00:00, 163.08it/s]\u001b[A\n",
      "Parsing nodes:   4%|▎         | 6/166 [00:02<00:44,  3.59it/s]\n",
      "Generating embeddings:   0%|          | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  38%|███▊      | 20/52 [00:00<00:00, 197.49it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 52/52 [00:00<00:00, 178.99it/s]\u001b[A\n",
      "Parsing nodes:   4%|▍         | 7/166 [00:02<00:45,  3.47it/s]\n",
      "Generating embeddings:   0%|          | 0/105 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  29%|██▊       | 30/105 [00:00<00:00, 226.80it/s]\u001b[A\n",
      "Generating embeddings:  67%|██████▋   | 70/105 [00:00<00:00, 220.16it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 105/105 [00:00<00:00, 218.52it/s]\u001b[A\n",
      "Parsing nodes:   5%|▍         | 8/166 [00:02<00:57,  2.76it/s]\n",
      "Generating embeddings:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 22/22 [00:00<00:00, 174.57it/s]\u001b[A\n",
      "Parsing nodes:   5%|▌         | 9/166 [00:02<00:45,  3.44it/s]\n",
      "Generating embeddings:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 24/24 [00:00<00:00, 186.96it/s]\u001b[A\n",
      "Parsing nodes:   6%|▌         | 10/166 [00:03<00:37,  4.11it/s]\n",
      "Generating embeddings:   0%|          | 0/96 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  31%|███▏      | 30/96 [00:00<00:00, 264.92it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 96/96 [00:00<00:00, 217.45it/s]\u001b[A\n",
      "Parsing nodes:   7%|▋         | 11/166 [00:03<00:47,  3.23it/s]\n",
      "Generating embeddings:   0%|          | 0/41 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 41/41 [00:00<00:00, 188.81it/s]\u001b[A\n",
      "Parsing nodes:   7%|▋         | 12/166 [00:03<00:44,  3.49it/s]\n",
      "Generating embeddings: 100%|██████████| 14/14 [00:00<00:00, 212.39it/s]\n",
      "\n",
      "Generating embeddings:   0%|          | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 39/39 [00:00<00:00, 196.81it/s]\u001b[A\n",
      "Parsing nodes:   8%|▊         | 14/166 [00:04<00:33,  4.55it/s]\n",
      "Generating embeddings:   0%|          | 0/43 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 43/43 [00:00<00:00, 151.37it/s]\u001b[A\n",
      "Parsing nodes:   9%|▉         | 15/166 [00:04<00:36,  4.19it/s]\n",
      "Generating embeddings:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  50%|█████     | 30/60 [00:00<00:00, 297.05it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 60/60 [00:00<00:00, 266.88it/s]\u001b[A\n",
      "Parsing nodes:  10%|▉         | 16/166 [00:04<00:35,  4.21it/s]\n",
      "Generating embeddings:   0%|          | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 32/32 [00:00<00:00, 165.15it/s]\u001b[A\n",
      "Parsing nodes:  10%|█         | 17/166 [00:04<00:34,  4.38it/s]\n",
      "Generating embeddings:   0%|          | 0/45 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 45/45 [00:00<00:00, 123.51it/s]\u001b[A\n",
      "Parsing nodes:  11%|█         | 18/166 [00:05<00:40,  3.69it/s]\n",
      "Generating embeddings:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 25/25 [00:00<00:00, 162.12it/s]\u001b[A\n",
      "Parsing nodes:  11%|█▏        | 19/166 [00:05<00:35,  4.11it/s]\n",
      "Generating embeddings:   0%|          | 0/41 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  49%|████▉     | 20/41 [00:00<00:00, 166.49it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 41/41 [00:00<00:00, 145.07it/s]\u001b[A\n",
      "Parsing nodes:  12%|█▏        | 20/166 [00:05<00:37,  3.87it/s]\n",
      "Generating embeddings:   0%|          | 0/89 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  34%|███▎      | 30/89 [00:00<00:00, 210.85it/s]\u001b[A\n",
      "Generating embeddings:  58%|█████▊    | 52/89 [00:00<00:00, 163.26it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 89/89 [00:00<00:00, 170.98it/s]\u001b[A\n",
      "Parsing nodes:  13%|█▎        | 21/166 [00:06<00:49,  2.94it/s]\n",
      "Generating embeddings:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 30/30 [00:00<00:00, 202.24it/s]\u001b[A\n",
      "Parsing nodes:  13%|█▎        | 22/166 [00:06<00:41,  3.50it/s]\n",
      "Generating embeddings:   0%|          | 0/42 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 42/42 [00:00<00:00, 175.23it/s]\u001b[A\n",
      "Parsing nodes:  14%|█▍        | 23/166 [00:06<00:39,  3.63it/s]\n",
      "Generating embeddings:   0%|          | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 39/39 [00:00<00:00, 145.93it/s]\u001b[A\n",
      "Parsing nodes:  14%|█▍        | 24/166 [00:06<00:39,  3.62it/s]\n",
      "Generating embeddings:   0%|          | 0/38 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 38/38 [00:00<00:00, 204.93it/s]\u001b[A\n",
      "Parsing nodes:  15%|█▌        | 25/166 [00:07<00:35,  3.98it/s]\n",
      "Generating embeddings:   0%|          | 0/41 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 41/41 [00:00<00:00, 185.99it/s]\u001b[A\n",
      "Parsing nodes:  16%|█▌        | 26/166 [00:07<00:34,  4.06it/s]\n",
      "Generating embeddings:   0%|          | 0/41 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  49%|████▉     | 20/41 [00:00<00:00, 98.46it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 41/41 [00:00<00:00, 123.23it/s]\u001b[A\n",
      "Parsing nodes:  16%|█▋        | 27/166 [00:07<00:38,  3.63it/s]\n",
      "Generating embeddings:   0%|          | 0/84 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  36%|███▌      | 30/84 [00:00<00:00, 206.18it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 84/84 [00:00<00:00, 219.35it/s]\u001b[A\n",
      "Parsing nodes:  17%|█▋        | 28/166 [00:08<00:43,  3.20it/s]\n",
      "Generating embeddings:   0%|          | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  51%|█████▏    | 20/39 [00:00<00:00, 186.93it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 39/39 [00:00<00:00, 158.15it/s]\u001b[A\n",
      "Parsing nodes:  17%|█▋        | 29/166 [00:08<00:40,  3.38it/s]\n",
      "Generating embeddings:   0%|          | 0/45 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  22%|██▏       | 10/45 [00:00<00:00, 71.42it/s]\u001b[A\n",
      "Generating embeddings:  67%|██████▋   | 30/45 [00:00<00:00, 113.10it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 45/45 [00:00<00:00, 117.01it/s]\u001b[A\n",
      "Parsing nodes:  18%|█▊        | 30/166 [00:08<00:44,  3.07it/s]\n",
      "Generating embeddings:   0%|          | 0/78 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  38%|███▊      | 30/78 [00:00<00:00, 232.80it/s]\u001b[A\n",
      "Generating embeddings:  69%|██████▉   | 54/78 [00:00<00:00, 233.62it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 78/78 [00:00<00:00, 171.93it/s]\u001b[A\n",
      "Parsing nodes:  19%|█▊        | 31/166 [00:09<00:49,  2.72it/s]\n",
      "Generating embeddings:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 29/29 [00:00<00:00, 176.41it/s]\u001b[A\n",
      "Parsing nodes:  19%|█▉        | 32/166 [00:09<00:41,  3.22it/s]\n",
      "Generating embeddings:   0%|          | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 27/27 [00:00<00:00, 151.93it/s]\u001b[A\n",
      "Parsing nodes:  20%|█▉        | 33/166 [00:09<00:36,  3.67it/s]\n",
      "Generating embeddings:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 31/31 [00:00<00:00, 195.28it/s]\u001b[A\n",
      "Parsing nodes:  20%|██        | 34/166 [00:09<00:31,  4.15it/s]\n",
      "Generating embeddings:   0%|          | 0/70 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  43%|████▎     | 30/70 [00:00<00:00, 182.49it/s]\u001b[A\n",
      "Generating embeddings:  71%|███████▏  | 50/70 [00:00<00:00, 174.33it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 70/70 [00:00<00:00, 163.09it/s]\u001b[A\n",
      "Parsing nodes:  21%|██        | 35/166 [00:10<00:39,  3.32it/s]\n",
      "Generating embeddings:   0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 35/35 [00:00<00:00, 170.13it/s]\u001b[A\n",
      "Parsing nodes:  22%|██▏       | 36/166 [00:10<00:35,  3.63it/s]\n",
      "Generating embeddings:   0%|          | 0/46 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 46/46 [00:00<00:00, 231.95it/s]\u001b[A\n",
      "Parsing nodes:  22%|██▏       | 37/166 [00:10<00:33,  3.90it/s]\n",
      "Generating embeddings:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 30/30 [00:00<00:00, 195.42it/s]\u001b[A\n",
      "Parsing nodes:  23%|██▎       | 38/166 [00:10<00:29,  4.37it/s]\n",
      "Generating embeddings:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  50%|█████     | 20/40 [00:00<00:00, 116.56it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 40/40 [00:00<00:00, 105.24it/s]\u001b[A\n",
      "Parsing nodes:  23%|██▎       | 39/166 [00:11<00:35,  3.61it/s]\n",
      "Generating embeddings:   0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 35/35 [00:00<00:00, 162.52it/s]\u001b[A\n",
      "Parsing nodes:  24%|██▍       | 40/166 [00:11<00:32,  3.82it/s]\n",
      "Generating embeddings:   0%|          | 0/26 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 26/26 [00:00<00:00, 166.35it/s]\u001b[A\n",
      "Parsing nodes:  25%|██▍       | 41/166 [00:11<00:29,  4.30it/s]\n",
      "Generating embeddings:   0%|          | 0/88 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  45%|████▌     | 40/88 [00:00<00:00, 230.66it/s]\u001b[A\n",
      "Generating embeddings:  73%|███████▎  | 64/88 [00:00<00:00, 183.33it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 88/88 [00:00<00:00, 162.64it/s]\u001b[A\n",
      "Parsing nodes:  25%|██▌       | 42/166 [00:12<00:41,  3.02it/s]\n",
      "Generating embeddings:   0%|          | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  27%|██▋       | 20/75 [00:00<00:00, 167.46it/s]\u001b[A\n",
      "Generating embeddings:  67%|██████▋   | 50/75 [00:00<00:00, 219.10it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 75/75 [00:00<00:00, 203.35it/s]\u001b[A\n",
      "Parsing nodes:  26%|██▌       | 43/166 [00:12<00:42,  2.87it/s]\n",
      "Generating embeddings:   0%|          | 0/36 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 36/36 [00:00<00:00, 212.02it/s]\u001b[A\n",
      "Parsing nodes:  27%|██▋       | 44/166 [00:12<00:36,  3.35it/s]\n",
      "Generating embeddings:   0%|          | 0/41 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  49%|████▉     | 20/41 [00:00<00:00, 196.87it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 41/41 [00:00<00:00, 158.63it/s]\u001b[A\n",
      "Parsing nodes:  27%|██▋       | 45/166 [00:12<00:35,  3.45it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:00<00:00, 229.24it/s]\n",
      "\n",
      "Generating embeddings:   0%|          | 0/97 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  41%|████      | 40/97 [00:00<00:00, 349.96it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 97/97 [00:00<00:00, 200.87it/s]\u001b[A\n",
      "Parsing nodes:  28%|██▊       | 47/166 [00:13<00:33,  3.51it/s]\n",
      "Generating embeddings:   0%|          | 0/34 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 34/34 [00:00<00:00, 158.64it/s]\u001b[A\n",
      "Parsing nodes:  29%|██▉       | 48/166 [00:13<00:32,  3.66it/s]\n",
      "Generating embeddings:   0%|          | 0/38 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 38/38 [00:00<00:00, 174.77it/s]\u001b[A\n",
      "Parsing nodes:  30%|██▉       | 49/166 [00:13<00:30,  3.82it/s]\n",
      "Generating embeddings:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  34%|███▍      | 10/29 [00:00<00:00, 92.85it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 29/29 [00:00<00:00, 114.70it/s]\u001b[A\n",
      "Parsing nodes:  30%|███       | 50/166 [00:14<00:30,  3.83it/s]\n",
      "Generating embeddings:   0%|          | 0/125 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  24%|██▍       | 30/125 [00:00<00:00, 278.38it/s]\u001b[A\n",
      "Generating embeddings:  48%|████▊     | 60/125 [00:00<00:00, 268.63it/s]\u001b[A\n",
      "Generating embeddings:  72%|███████▏  | 90/125 [00:00<00:00, 246.76it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 125/125 [00:00<00:00, 231.82it/s]\u001b[A\n",
      "Parsing nodes:  31%|███       | 51/166 [00:14<00:39,  2.89it/s]\n",
      "Generating embeddings:   0%|          | 0/56 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 56/56 [00:00<00:00, 233.09it/s]\u001b[A\n",
      "Parsing nodes:  31%|███▏      | 52/166 [00:15<00:36,  3.12it/s]\n",
      "Generating embeddings:   0%|          | 0/53 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 53/53 [00:00<00:00, 203.41it/s]\u001b[A\n",
      "Parsing nodes:  32%|███▏      | 53/166 [00:15<00:34,  3.24it/s]\n",
      "Generating embeddings:   0%|          | 0/44 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  45%|████▌     | 20/44 [00:00<00:00, 152.90it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 44/44 [00:00<00:00, 167.31it/s]\u001b[A\n",
      "Parsing nodes:  33%|███▎      | 54/166 [00:15<00:33,  3.34it/s]\n",
      "Generating embeddings:   0%|          | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  38%|███▊      | 20/52 [00:00<00:00, 147.06it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 52/52 [00:00<00:00, 196.31it/s]\u001b[A\n",
      "Parsing nodes:  33%|███▎      | 55/166 [00:15<00:32,  3.42it/s]\n",
      "Generating embeddings:   0%|          | 0/97 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  31%|███       | 30/97 [00:00<00:00, 239.81it/s]\u001b[A\n",
      "Generating embeddings:  62%|██████▏   | 60/97 [00:00<00:00, 247.37it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 97/97 [00:00<00:00, 181.91it/s]\u001b[A\n",
      "Parsing nodes:  34%|███▎      | 56/166 [00:16<00:40,  2.69it/s]\n",
      "Generating embeddings:   0%|          | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  51%|█████▏    | 20/39 [00:00<00:00, 179.69it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 39/39 [00:00<00:00, 167.35it/s]\u001b[A\n",
      "Parsing nodes:  34%|███▍      | 57/166 [00:16<00:36,  2.98it/s]\n",
      "Generating embeddings:   0%|          | 0/73 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  41%|████      | 30/73 [00:00<00:00, 264.29it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 73/73 [00:00<00:00, 182.61it/s]\u001b[A\n",
      "Parsing nodes:  35%|███▍      | 58/166 [00:17<00:38,  2.79it/s]\n",
      "Generating embeddings:   0%|          | 0/51 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 51/51 [00:00<00:00, 167.89it/s]\u001b[A\n",
      "Parsing nodes:  36%|███▌      | 59/166 [00:17<00:37,  2.88it/s]\n",
      "Generating embeddings:   0%|          | 0/56 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 56/56 [00:00<00:00, 211.86it/s]\u001b[A\n",
      "Parsing nodes:  36%|███▌      | 60/166 [00:17<00:34,  3.07it/s]\n",
      "Generating embeddings:   0%|          | 0/101 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  20%|█▉        | 20/101 [00:00<00:00, 170.49it/s]\u001b[A\n",
      "Generating embeddings:  40%|███▉      | 40/101 [00:00<00:00, 158.98it/s]\u001b[A\n",
      "Generating embeddings:  59%|█████▉    | 60/101 [00:00<00:00, 155.18it/s]\u001b[A\n",
      "Generating embeddings:  79%|███████▉  | 80/101 [00:00<00:00, 167.03it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 101/101 [00:00<00:00, 151.77it/s]\u001b[A\n",
      "Parsing nodes:  37%|███▋      | 61/166 [00:18<00:45,  2.30it/s]\n",
      "Generating embeddings:   0%|          | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 27/27 [00:00<00:00, 121.20it/s]\u001b[A\n",
      "Parsing nodes:  37%|███▋      | 62/166 [00:18<00:39,  2.67it/s]\n",
      "Generating embeddings:   0%|          | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 39/39 [00:00<00:00, 162.02it/s]\u001b[A\n",
      "Parsing nodes:  38%|███▊      | 63/166 [00:18<00:34,  2.95it/s]\n",
      "Generating embeddings:   0%|          | 0/38 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  53%|█████▎    | 20/38 [00:00<00:00, 116.55it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 38/38 [00:00<00:00, 107.88it/s]\u001b[A\n",
      "Parsing nodes:  39%|███▊      | 64/166 [00:19<00:35,  2.89it/s]\n",
      "Generating embeddings:   0%|          | 0/135 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  22%|██▏       | 30/135 [00:00<00:00, 247.78it/s]\u001b[A\n",
      "Generating embeddings:  52%|█████▏    | 70/135 [00:00<00:00, 301.95it/s]\u001b[A\n",
      "Generating embeddings:  75%|███████▍  | 101/135 [00:00<00:00, 191.35it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 135/135 [00:00<00:00, 212.12it/s]\u001b[A\n",
      "Parsing nodes:  39%|███▉      | 65/166 [00:19<00:44,  2.26it/s]\n",
      "Generating embeddings:   0%|          | 0/48 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 48/48 [00:00<00:00, 226.41it/s]\u001b[A\n",
      "Parsing nodes:  40%|███▉      | 66/166 [00:20<00:37,  2.63it/s]\n",
      "Generating embeddings:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 37/37 [00:00<00:00, 165.89it/s]\u001b[A\n",
      "Parsing nodes:  40%|████      | 67/166 [00:20<00:33,  2.97it/s]\n",
      "Generating embeddings:   0%|          | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  58%|█████▊    | 30/52 [00:00<00:00, 210.24it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 52/52 [00:00<00:00, 200.01it/s]\u001b[A\n",
      "Parsing nodes:  41%|████      | 68/166 [00:20<00:31,  3.14it/s]\n",
      "Generating embeddings:   0%|          | 0/89 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  45%|████▍     | 40/89 [00:00<00:00, 279.10it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 89/89 [00:00<00:00, 207.46it/s]\u001b[A\n",
      "Parsing nodes:  42%|████▏     | 69/166 [00:21<00:34,  2.77it/s]\n",
      "Generating embeddings:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 22/22 [00:00<00:00, 194.24it/s]\u001b[A\n",
      "Parsing nodes:  42%|████▏     | 70/166 [00:21<00:27,  3.46it/s]\n",
      "Generating embeddings:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 40/40 [00:00<00:00, 220.58it/s]\u001b[A\n",
      "Parsing nodes:  43%|████▎     | 71/166 [00:21<00:24,  3.84it/s]\n",
      "Generating embeddings:   0%|          | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  25%|██▌       | 30/118 [00:00<00:00, 257.29it/s]\u001b[A\n",
      "Generating embeddings:  47%|████▋     | 56/118 [00:00<00:00, 249.67it/s]\u001b[A\n",
      "Generating embeddings:  69%|██████▊   | 81/118 [00:00<00:00, 200.59it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 118/118 [00:00<00:00, 200.20it/s]\u001b[A\n",
      "Parsing nodes:  43%|████▎     | 72/166 [00:22<00:34,  2.74it/s]\n",
      "Generating embeddings:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 50/50 [00:00<00:00, 240.45it/s]\u001b[A\n",
      "Parsing nodes:  44%|████▍     | 73/166 [00:22<00:29,  3.12it/s]\n",
      "Generating embeddings:   0%|          | 0/53 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 53/53 [00:00<00:00, 209.23it/s]\u001b[A\n",
      "Parsing nodes:  45%|████▍     | 74/166 [00:22<00:28,  3.28it/s]\n",
      "Generating embeddings:   0%|          | 0/46 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 46/46 [00:00<00:00, 228.29it/s]\u001b[A\n",
      "Parsing nodes:  45%|████▌     | 75/166 [00:22<00:25,  3.54it/s]\n",
      "Generating embeddings:   0%|          | 0/59 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  51%|█████     | 30/59 [00:00<00:00, 216.34it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 59/59 [00:00<00:00, 226.45it/s]\u001b[A\n",
      "Parsing nodes:  46%|████▌     | 76/166 [00:23<00:25,  3.58it/s]\n",
      "Generating embeddings:   0%|          | 0/36 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 36/36 [00:00<00:00, 257.07it/s]\u001b[A\n",
      "Parsing nodes:  46%|████▋     | 77/166 [00:23<00:21,  4.12it/s]\n",
      "Generating embeddings:   0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  48%|████▊     | 30/63 [00:00<00:00, 227.09it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 63/63 [00:00<00:00, 229.45it/s]\u001b[A\n",
      "Parsing nodes:  47%|████▋     | 78/166 [00:23<00:23,  3.82it/s]\n",
      "Generating embeddings:   0%|          | 0/78 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  38%|███▊      | 30/78 [00:00<00:00, 220.45it/s]\u001b[A\n",
      "Generating embeddings:  68%|██████▊   | 53/78 [00:00<00:00, 208.71it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 78/78 [00:00<00:00, 219.72it/s]\u001b[A\n",
      "Parsing nodes:  48%|████▊     | 79/166 [00:23<00:25,  3.40it/s]\n",
      "Generating embeddings:   0%|          | 0/58 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  52%|█████▏    | 30/58 [00:00<00:00, 177.94it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 58/58 [00:00<00:00, 161.33it/s]\u001b[A\n",
      "Parsing nodes:  48%|████▊     | 80/166 [00:24<00:27,  3.15it/s]\n",
      "Generating embeddings:   0%|          | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  49%|████▉     | 40/81 [00:00<00:00, 295.42it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 81/81 [00:00<00:00, 251.38it/s]\u001b[A\n",
      "Parsing nodes:  49%|████▉     | 81/166 [00:24<00:27,  3.07it/s]\n",
      "Generating embeddings:   0%|          | 0/84 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  36%|███▌      | 30/84 [00:00<00:00, 247.49it/s]\u001b[A\n",
      "Generating embeddings:  65%|██████▌   | 55/84 [00:00<00:00, 212.68it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 84/84 [00:00<00:00, 169.14it/s]\u001b[A\n",
      "Parsing nodes:  49%|████▉     | 82/166 [00:25<00:32,  2.62it/s]\n",
      "Generating embeddings:   0%|          | 0/77 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  39%|███▉      | 30/77 [00:00<00:00, 278.41it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 77/77 [00:00<00:00, 236.22it/s]\u001b[A\n",
      "Parsing nodes:  50%|█████     | 83/166 [00:25<00:30,  2.71it/s]\n",
      "Generating embeddings:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  50%|█████     | 20/40 [00:00<00:00, 190.51it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 40/40 [00:00<00:00, 168.34it/s]\u001b[A\n",
      "Parsing nodes:  51%|█████     | 84/166 [00:25<00:27,  3.00it/s]\n",
      "Generating embeddings:   0%|          | 0/77 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  39%|███▉      | 30/77 [00:00<00:00, 274.52it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 77/77 [00:00<00:00, 232.86it/s]\u001b[A\n",
      "Parsing nodes:  51%|█████     | 85/166 [00:26<00:27,  2.95it/s]\n",
      "Generating embeddings:   0%|          | 0/62 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  48%|████▊     | 30/62 [00:00<00:00, 252.60it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 62/62 [00:00<00:00, 222.62it/s]\u001b[A\n",
      "Parsing nodes:  52%|█████▏    | 86/166 [00:26<00:26,  3.06it/s]\n",
      "Generating embeddings:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 50/50 [00:00<00:00, 215.10it/s]\u001b[A\n",
      "Parsing nodes:  52%|█████▏    | 87/166 [00:26<00:23,  3.31it/s]\n",
      "Generating embeddings:   0%|          | 0/48 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 48/48 [00:00<00:00, 241.36it/s]\u001b[A\n",
      "Parsing nodes:  53%|█████▎    | 88/166 [00:26<00:21,  3.65it/s]\n",
      "Generating embeddings:   0%|          | 0/203 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  20%|█▉        | 40/203 [00:00<00:00, 291.29it/s]\u001b[A\n",
      "Generating embeddings:  34%|███▍      | 70/203 [00:00<00:00, 280.50it/s]\u001b[A\n",
      "Generating embeddings:  54%|█████▍    | 110/203 [00:00<00:00, 294.73it/s]\u001b[A\n",
      "Generating embeddings:  69%|██████▉   | 140/203 [00:00<00:00, 229.42it/s]\u001b[A\n",
      "Generating embeddings:  81%|████████▏ | 165/203 [00:00<00:00, 227.82it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 203/203 [00:00<00:00, 249.98it/s]\u001b[A\n",
      "Parsing nodes:  54%|█████▎    | 89/166 [00:27<00:34,  2.25it/s]\n",
      "Generating embeddings:   0%|          | 0/46 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 46/46 [00:00<00:00, 226.07it/s]\u001b[A\n",
      "Parsing nodes:  54%|█████▍    | 90/166 [00:27<00:28,  2.66it/s]\n",
      "Generating embeddings:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  50%|█████     | 30/60 [00:00<00:00, 219.60it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 60/60 [00:00<00:00, 190.57it/s]\u001b[A\n",
      "Parsing nodes:  55%|█████▍    | 91/166 [00:28<00:27,  2.76it/s]\n",
      "Generating embeddings:   0%|          | 0/68 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  44%|████▍     | 30/68 [00:00<00:00, 242.04it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 68/68 [00:00<00:00, 221.95it/s]\u001b[A\n",
      "Parsing nodes:  55%|█████▌    | 92/166 [00:28<00:25,  2.86it/s]\n",
      "Generating embeddings:   0%|          | 0/47 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 47/47 [00:00<00:00, 248.72it/s]\u001b[A\n",
      "Parsing nodes:  56%|█████▌    | 93/166 [00:28<00:22,  3.28it/s]\n",
      "Generating embeddings:   0%|          | 0/80 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  38%|███▊      | 30/80 [00:00<00:00, 273.50it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 80/80 [00:00<00:00, 207.03it/s]\u001b[A\n",
      "Parsing nodes:  57%|█████▋    | 94/166 [00:29<00:24,  3.00it/s]\n",
      "Generating embeddings:   0%|          | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  47%|████▋     | 30/64 [00:00<00:00, 266.59it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 64/64 [00:00<00:00, 213.18it/s]\u001b[A\n",
      "Parsing nodes:  57%|█████▋    | 95/166 [00:29<00:23,  3.03it/s]\n",
      "Generating embeddings:   0%|          | 0/58 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  52%|█████▏    | 30/58 [00:00<00:00, 176.93it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 58/58 [00:00<00:00, 182.40it/s]\u001b[A\n",
      "Parsing nodes:  58%|█████▊    | 96/166 [00:29<00:23,  3.02it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 183.99it/s]\n",
      "\n",
      "Generating embeddings:   0%|          | 0/45 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 45/45 [00:00<00:00, 138.70it/s]\u001b[A\n",
      "Parsing nodes:  59%|█████▉    | 98/166 [00:30<00:18,  3.70it/s]\n",
      "Generating embeddings:   0%|          | 0/77 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  39%|███▉      | 30/77 [00:00<00:00, 250.14it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 77/77 [00:00<00:00, 222.70it/s]\u001b[A\n",
      "Parsing nodes:  60%|█████▉    | 99/166 [00:30<00:19,  3.42it/s]\n",
      "Generating embeddings:   0%|          | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 52/52 [00:00<00:00, 201.70it/s]\u001b[A\n",
      "Parsing nodes:  60%|██████    | 100/166 [00:30<00:18,  3.48it/s]\n",
      "Generating embeddings:   0%|          | 0/53 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 53/53 [00:00<00:00, 292.59it/s]\u001b[A\n",
      "Parsing nodes:  61%|██████    | 101/166 [00:30<00:17,  3.82it/s]\n",
      "Generating embeddings:   0%|          | 0/42 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 42/42 [00:00<00:00, 218.88it/s]\u001b[A\n",
      "Parsing nodes:  61%|██████▏   | 102/166 [00:31<00:15,  4.08it/s]\n",
      "Generating embeddings:   0%|          | 0/59 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  51%|█████     | 30/59 [00:00<00:00, 194.96it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 59/59 [00:00<00:00, 168.58it/s]\u001b[A\n",
      "Parsing nodes:  62%|██████▏   | 103/166 [00:31<00:17,  3.57it/s]\n",
      "Generating embeddings:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 29/29 [00:00<00:00, 249.84it/s]\u001b[A\n",
      "Parsing nodes:  63%|██████▎   | 104/166 [00:31<00:14,  4.26it/s]\n",
      "Generating embeddings:   0%|          | 0/179 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  17%|█▋        | 30/179 [00:00<00:00, 291.19it/s]\u001b[A\n",
      "Generating embeddings:  34%|███▎      | 60/179 [00:00<00:00, 243.26it/s]\u001b[A\n",
      "Generating embeddings:  50%|█████     | 90/179 [00:00<00:00, 245.71it/s]\u001b[A\n",
      "Generating embeddings:  73%|███████▎  | 130/179 [00:00<00:00, 265.82it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 179/179 [00:00<00:00, 251.52it/s]\u001b[A\n",
      "Parsing nodes:  63%|██████▎   | 105/166 [00:32<00:23,  2.61it/s]\n",
      "Generating embeddings:   0%|          | 0/48 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 48/48 [00:00<00:00, 243.24it/s]\u001b[A\n",
      "Parsing nodes:  64%|██████▍   | 106/166 [00:32<00:19,  3.02it/s]\n",
      "Generating embeddings:   0%|          | 0/80 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  25%|██▌       | 20/80 [00:00<00:00, 177.64it/s]\u001b[A\n",
      "Generating embeddings:  50%|█████     | 40/80 [00:00<00:00, 184.62it/s]\u001b[A\n",
      "Generating embeddings:  75%|███████▌  | 60/80 [00:00<00:00, 144.11it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 80/80 [00:00<00:00, 141.11it/s]\u001b[A\n",
      "Parsing nodes:  64%|██████▍   | 107/166 [00:33<00:23,  2.47it/s]\n",
      "Generating embeddings:   0%|          | 0/46 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  43%|████▎     | 20/46 [00:00<00:00, 146.71it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 46/46 [00:00<00:00, 153.39it/s]\u001b[A\n",
      "Parsing nodes:  65%|██████▌   | 108/166 [00:33<00:21,  2.64it/s]\n",
      "Generating embeddings:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 24/24 [00:00<00:00, 228.90it/s]\u001b[A\n",
      "Parsing nodes:  66%|██████▌   | 109/166 [00:33<00:17,  3.33it/s]\n",
      "Generating embeddings:   0%|          | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  38%|███▊      | 20/52 [00:00<00:00, 165.36it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 52/52 [00:00<00:00, 175.59it/s]\u001b[A\n",
      "Parsing nodes:  66%|██████▋   | 110/166 [00:33<00:16,  3.31it/s]\n",
      "Generating embeddings:   0%|          | 0/36 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 36/36 [00:00<00:00, 184.20it/s]\u001b[A\n",
      "Parsing nodes:  67%|██████▋   | 111/166 [00:34<00:15,  3.66it/s]\n",
      "Generating embeddings:   0%|          | 0/53 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  57%|█████▋    | 30/53 [00:00<00:00, 184.46it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 53/53 [00:00<00:00, 180.40it/s]\u001b[A\n",
      "Parsing nodes:  67%|██████▋   | 112/166 [00:34<00:15,  3.52it/s]\n",
      "Generating embeddings:   0%|          | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 52/52 [00:00<00:00, 226.50it/s]\u001b[A\n",
      "Parsing nodes:  68%|██████▊   | 113/166 [00:34<00:14,  3.66it/s]\n",
      "Generating embeddings:   0%|          | 0/46 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 46/46 [00:00<00:00, 216.08it/s]\u001b[A\n",
      "Parsing nodes:  69%|██████▊   | 114/166 [00:34<00:13,  3.87it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 142.25it/s]\n",
      "\n",
      "Generating embeddings:   0%|          | 0/85 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  35%|███▌      | 30/85 [00:00<00:00, 291.59it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 85/85 [00:00<00:00, 275.13it/s]\u001b[A\n",
      "Parsing nodes:  70%|██████▉   | 116/166 [00:35<00:11,  4.28it/s]\n",
      "Generating embeddings:   0%|          | 0/79 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  38%|███▊      | 30/79 [00:00<00:00, 242.32it/s]\u001b[A\n",
      "Generating embeddings:  70%|██████▉   | 55/79 [00:00<00:00, 184.53it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 79/79 [00:00<00:00, 179.04it/s]\u001b[A\n",
      "Parsing nodes:  70%|███████   | 117/166 [00:35<00:14,  3.44it/s]\n",
      "Generating embeddings:   0%|          | 0/56 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  36%|███▌      | 20/56 [00:00<00:00, 199.48it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 56/56 [00:00<00:00, 155.27it/s]\u001b[A\n",
      "Parsing nodes:  71%|███████   | 118/166 [00:36<00:14,  3.20it/s]\n",
      "Generating embeddings:   0%|          | 0/61 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  49%|████▉     | 30/61 [00:00<00:00, 230.01it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 61/61 [00:00<00:00, 175.50it/s]\u001b[A\n",
      "Parsing nodes:  72%|███████▏  | 119/166 [00:36<00:15,  3.07it/s]\n",
      "Generating embeddings:   0%|          | 0/48 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 48/48 [00:00<00:00, 196.87it/s]\u001b[A\n",
      "Parsing nodes:  72%|███████▏  | 120/166 [00:36<00:14,  3.26it/s]\n",
      "Generating embeddings:   0%|          | 0/69 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  29%|██▉       | 20/69 [00:00<00:00, 166.92it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 69/69 [00:00<00:00, 208.38it/s]\u001b[A\n",
      "Parsing nodes:  73%|███████▎  | 121/166 [00:37<00:14,  3.14it/s]\n",
      "Generating embeddings:   0%|          | 0/72 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  42%|████▏     | 30/72 [00:00<00:00, 237.85it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 72/72 [00:00<00:00, 221.83it/s]\u001b[A\n",
      "Parsing nodes:  73%|███████▎  | 122/166 [00:37<00:14,  3.07it/s]\n",
      "Generating embeddings:   0%|          | 0/49 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 49/49 [00:00<00:00, 263.89it/s]\u001b[A\n",
      "Parsing nodes:  74%|███████▍  | 123/166 [00:37<00:12,  3.47it/s]\n",
      "Generating embeddings:   0%|          | 0/79 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  38%|███▊      | 30/79 [00:00<00:00, 205.89it/s]\u001b[A\n",
      "Generating embeddings:  65%|██████▍   | 51/79 [00:00<00:00, 198.43it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 79/79 [00:00<00:00, 170.57it/s]\u001b[A\n",
      "Parsing nodes:  75%|███████▍  | 124/166 [00:38<00:14,  2.90it/s]\n",
      "Generating embeddings:   0%|          | 0/87 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  11%|█▏        | 10/87 [00:00<00:00, 97.62it/s]\u001b[A\n",
      "Generating embeddings:  34%|███▍      | 30/87 [00:00<00:00, 102.71it/s]\u001b[A\n",
      "Generating embeddings:  57%|█████▋    | 50/87 [00:00<00:00, 115.64it/s]\u001b[A\n",
      "Generating embeddings:  80%|████████  | 70/87 [00:00<00:00, 140.78it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 87/87 [00:00<00:00, 134.14it/s]\u001b[A\n",
      "Parsing nodes:  75%|███████▌  | 125/166 [00:38<00:18,  2.27it/s]\n",
      "Generating embeddings:   0%|          | 0/124 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  24%|██▍       | 30/124 [00:00<00:00, 196.90it/s]\u001b[A\n",
      "Generating embeddings:  48%|████▊     | 60/124 [00:00<00:00, 178.73it/s]\u001b[A\n",
      "Generating embeddings:  65%|██████▍   | 80/124 [00:00<00:00, 173.73it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 124/124 [00:00<00:00, 185.47it/s]\u001b[A\n",
      "Parsing nodes:  76%|███████▌  | 126/166 [00:39<00:20,  1.92it/s]\n",
      "Generating embeddings:   0%|          | 0/58 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  34%|███▍      | 20/58 [00:00<00:00, 132.71it/s]\u001b[A\n",
      "Generating embeddings:  69%|██████▉   | 40/58 [00:00<00:00, 143.95it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 58/58 [00:00<00:00, 127.85it/s]\u001b[A\n",
      "Parsing nodes:  77%|███████▋  | 127/166 [00:39<00:19,  1.98it/s]\n",
      "Generating embeddings:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 33/33 [00:00<00:00, 302.34it/s]\u001b[A\n",
      "Parsing nodes:  77%|███████▋  | 128/166 [00:40<00:14,  2.57it/s]\n",
      "Generating embeddings:   0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  19%|█▉        | 20/106 [00:00<00:00, 183.17it/s]\u001b[A\n",
      "Generating embeddings:  38%|███▊      | 40/106 [00:00<00:00, 180.19it/s]\u001b[A\n",
      "Generating embeddings:  57%|█████▋    | 60/106 [00:00<00:00, 151.13it/s]\u001b[A\n",
      "Generating embeddings:  75%|███████▌  | 80/106 [00:00<00:00, 128.49it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 106/106 [00:00<00:00, 142.89it/s]\u001b[A\n",
      "Parsing nodes:  78%|███████▊  | 129/166 [00:40<00:18,  2.00it/s]\n",
      "Generating embeddings:   0%|          | 0/97 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  21%|██        | 20/97 [00:00<00:00, 176.08it/s]\u001b[A\n",
      "Generating embeddings:  52%|█████▏    | 50/97 [00:00<00:00, 176.20it/s]\u001b[A\n",
      "Generating embeddings:  72%|███████▏  | 70/97 [00:00<00:00, 172.70it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 97/97 [00:00<00:00, 185.36it/s]\u001b[A\n",
      "Parsing nodes:  78%|███████▊  | 130/166 [00:41<00:18,  1.95it/s]\n",
      "Generating embeddings:   0%|          | 0/61 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  33%|███▎      | 20/61 [00:00<00:00, 182.19it/s]\u001b[A\n",
      "Generating embeddings:  66%|██████▌   | 40/61 [00:00<00:00, 184.29it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 61/61 [00:00<00:00, 172.10it/s]\u001b[A\n",
      "Parsing nodes:  79%|███████▉  | 131/166 [00:41<00:16,  2.13it/s]\n",
      "Generating embeddings:   0%|          | 0/262 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  11%|█▏        | 30/262 [00:00<00:01, 214.96it/s]\u001b[A\n",
      "Generating embeddings:  23%|██▎       | 60/262 [00:00<00:00, 224.21it/s]\u001b[A\n",
      "Generating embeddings:  32%|███▏      | 83/262 [00:00<00:00, 215.98it/s]\u001b[A\n",
      "Generating embeddings:  40%|████      | 105/262 [00:00<00:00, 192.66it/s]\u001b[A\n",
      "Generating embeddings:  48%|████▊     | 125/262 [00:00<00:00, 178.96it/s]\u001b[A\n",
      "Generating embeddings:  57%|█████▋    | 150/262 [00:00<00:00, 190.99it/s]\u001b[A\n",
      "Generating embeddings:  73%|███████▎  | 190/262 [00:00<00:00, 232.07it/s]\u001b[A\n",
      "Generating embeddings:  84%|████████▍ | 220/262 [00:00<00:00, 249.13it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 262/262 [00:01<00:00, 225.43it/s]\u001b[A\n",
      "Parsing nodes:  80%|███████▉  | 132/166 [00:42<00:23,  1.45it/s]\n",
      "Generating embeddings:   0%|          | 0/58 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  34%|███▍      | 20/58 [00:00<00:00, 142.23it/s]\u001b[A\n",
      "Generating embeddings:  69%|██████▉   | 40/58 [00:00<00:00, 117.90it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 58/58 [00:00<00:00, 118.83it/s]\u001b[A\n",
      "Parsing nodes:  80%|████████  | 133/166 [00:43<00:20,  1.57it/s]\n",
      "Generating embeddings:   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 28/28 [00:00<00:00, 144.68it/s]\u001b[A\n",
      "Parsing nodes:  81%|████████  | 134/166 [00:43<00:16,  1.97it/s]\n",
      "Generating embeddings:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  42%|████▏     | 10/24 [00:00<00:00, 89.50it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 24/24 [00:00<00:00, 78.41it/s]\u001b[A\n",
      "Parsing nodes:  81%|████████▏ | 135/166 [00:44<00:13,  2.23it/s]\n",
      "Generating embeddings:   0%|          | 0/77 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  39%|███▉      | 30/77 [00:00<00:00, 242.98it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 77/77 [00:00<00:00, 250.17it/s]\u001b[A\n",
      "Parsing nodes:  82%|████████▏ | 136/166 [00:44<00:12,  2.43it/s]\n",
      "Generating embeddings:   0%|          | 0/69 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  29%|██▉       | 20/69 [00:00<00:00, 153.37it/s]\u001b[A\n",
      "Generating embeddings:  58%|█████▊    | 40/69 [00:00<00:00, 148.61it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 69/69 [00:00<00:00, 146.65it/s]\u001b[A\n",
      "Parsing nodes:  83%|████████▎ | 137/166 [00:44<00:12,  2.31it/s]\n",
      "Generating embeddings:   0%|          | 0/96 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  21%|██        | 20/96 [00:00<00:00, 146.91it/s]\u001b[A\n",
      "Generating embeddings:  52%|█████▏    | 50/96 [00:00<00:00, 168.17it/s]\u001b[A\n",
      "Generating embeddings:  73%|███████▎  | 70/96 [00:00<00:00, 160.83it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 96/96 [00:00<00:00, 162.49it/s]\u001b[A\n",
      "Parsing nodes:  83%|████████▎ | 138/166 [00:45<00:13,  2.05it/s]\n",
      "Generating embeddings:   0%|          | 0/68 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  44%|████▍     | 30/68 [00:00<00:00, 187.19it/s]\u001b[A\n",
      "Generating embeddings:  74%|███████▎  | 50/68 [00:00<00:00, 156.63it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 68/68 [00:00<00:00, 129.92it/s]\u001b[A\n",
      "Parsing nodes:  84%|████████▎ | 139/166 [00:45<00:13,  1.99it/s]\n",
      "Generating embeddings:   0%|          | 0/92 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  22%|██▏       | 20/92 [00:00<00:00, 178.16it/s]\u001b[A\n",
      "Generating embeddings:  43%|████▎     | 40/92 [00:00<00:00, 134.96it/s]\u001b[A\n",
      "Generating embeddings:  76%|███████▌  | 70/92 [00:00<00:00, 145.46it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 92/92 [00:00<00:00, 134.35it/s]\u001b[A\n",
      "Parsing nodes:  84%|████████▍ | 140/166 [00:46<00:14,  1.77it/s]\n",
      "Generating embeddings:   0%|          | 0/34 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  59%|█████▉    | 20/34 [00:00<00:00, 139.20it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 34/34 [00:00<00:00, 129.68it/s]\u001b[A\n",
      "Parsing nodes:  85%|████████▍ | 141/166 [00:46<00:11,  2.09it/s]\n",
      "Generating embeddings:   0%|          | 0/115 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  26%|██▌       | 30/115 [00:00<00:00, 246.60it/s]\u001b[A\n",
      "Generating embeddings:  48%|████▊     | 55/115 [00:00<00:00, 206.97it/s]\u001b[A\n",
      "Generating embeddings:  70%|██████▉   | 80/115 [00:00<00:00, 185.14it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 115/115 [00:00<00:00, 189.04it/s]\u001b[A\n",
      "Parsing nodes:  86%|████████▌ | 142/166 [00:47<00:12,  1.91it/s]\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:00<00:00, 151.67it/s]\n",
      "\n",
      "Generating embeddings: 100%|██████████| 30/30 [00:00<00:00, 314.27it/s]\n",
      "Parsing nodes:  87%|████████▋ | 144/166 [00:47<00:06,  3.19it/s]\n",
      "Generating embeddings:   0%|          | 0/102 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  29%|██▉       | 30/102 [00:00<00:00, 194.91it/s]\u001b[A\n",
      "Generating embeddings:  49%|████▉     | 50/102 [00:00<00:00, 193.10it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 102/102 [00:00<00:00, 191.93it/s][A\n",
      "Parsing nodes:  87%|████████▋ | 145/166 [00:48<00:07,  2.69it/s]\n",
      "Generating embeddings:   0%|          | 0/160 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  12%|█▎        | 20/160 [00:00<00:00, 163.75it/s]\u001b[A\n",
      "Generating embeddings:  25%|██▌       | 40/160 [00:00<00:00, 142.54it/s]\u001b[A\n",
      "Generating embeddings:  38%|███▊      | 60/160 [00:00<00:00, 149.99it/s]\u001b[A\n",
      "Generating embeddings:  50%|█████     | 80/160 [00:00<00:00, 154.09it/s]\u001b[A\n",
      "Generating embeddings:  62%|██████▎   | 100/160 [00:00<00:00, 144.65it/s]\u001b[A\n",
      "Generating embeddings:  81%|████████▏ | 130/160 [00:00<00:00, 165.37it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 160/160 [00:01<00:00, 158.34it/s]\u001b[A\n",
      "Parsing nodes:  88%|████████▊ | 146/166 [00:49<00:10,  1.83it/s]\n",
      "Generating embeddings:   0%|          | 0/76 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  39%|███▉      | 30/76 [00:00<00:00, 230.74it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 76/76 [00:00<00:00, 184.41it/s]\u001b[A\n",
      "Parsing nodes:  89%|████████▊ | 147/166 [00:49<00:09,  1.95it/s]\n",
      "Generating embeddings:   0%|          | 0/68 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  44%|████▍     | 30/68 [00:00<00:00, 162.56it/s]\u001b[A\n",
      "Generating embeddings:  74%|███████▎  | 50/68 [00:00<00:00, 123.38it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 68/68 [00:00<00:00, 121.21it/s]\u001b[A\n",
      "Parsing nodes:  89%|████████▉ | 148/166 [00:50<00:09,  1.88it/s]\n",
      "Generating embeddings:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  27%|██▋       | 30/110 [00:00<00:00, 269.52it/s]\u001b[A\n",
      "Generating embeddings:  55%|█████▍    | 60/110 [00:00<00:00, 263.53it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 110/110 [00:00<00:00, 265.94it/s][A\n",
      "Parsing nodes:  90%|████████▉ | 149/166 [00:50<00:08,  1.98it/s]\n",
      "Generating embeddings:   0%|          | 0/133 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:   8%|▊         | 10/133 [00:00<00:01, 94.09it/s]\u001b[A\n",
      "Generating embeddings:  15%|█▌        | 20/133 [00:00<00:01, 83.09it/s]\u001b[A\n",
      "Generating embeddings:  30%|███       | 40/133 [00:00<00:00, 95.71it/s]\u001b[A\n",
      "Generating embeddings:  45%|████▌     | 60/133 [00:00<00:00, 106.97it/s]\u001b[A\n",
      "Generating embeddings:  60%|██████    | 80/133 [00:00<00:00, 104.66it/s]\u001b[A\n",
      "Generating embeddings:  68%|██████▊   | 91/133 [00:00<00:00, 97.80it/s] \u001b[A\n",
      "Generating embeddings:  90%|█████████ | 120/133 [00:01<00:00, 124.96it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 133/133 [00:01<00:00, 110.84it/s]\u001b[A\n",
      "Parsing nodes:  90%|█████████ | 150/166 [00:52<00:11,  1.39it/s]\n",
      "Generating embeddings:   0%|          | 0/129 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  16%|█▌        | 20/129 [00:00<00:00, 195.09it/s]\u001b[A\n",
      "Generating embeddings:  31%|███       | 40/129 [00:00<00:00, 185.41it/s]\u001b[A\n",
      "Generating embeddings:  47%|████▋     | 60/129 [00:00<00:00, 160.47it/s]\u001b[A\n",
      "Generating embeddings:  62%|██████▏   | 80/129 [00:00<00:00, 155.99it/s]\u001b[A\n",
      "Generating embeddings:  78%|███████▊  | 100/129 [00:00<00:00, 147.34it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 129/129 [00:00<00:00, 163.31it/s]\u001b[A\n",
      "Parsing nodes:  91%|█████████ | 151/166 [00:52<00:11,  1.34it/s]\n",
      "Generating embeddings:   0%|          | 0/114 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  26%|██▋       | 30/114 [00:00<00:00, 282.49it/s]\u001b[A\n",
      "Generating embeddings:  53%|█████▎    | 60/114 [00:00<00:00, 261.15it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 114/114 [00:00<00:00, 264.12it/s][A\n",
      "Parsing nodes:  92%|█████████▏| 152/166 [00:53<00:09,  1.52it/s]\n",
      "Generating embeddings:   0%|          | 0/49 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 49/49 [00:00<00:00, 277.31it/s]\u001b[A\n",
      "Parsing nodes:  92%|█████████▏| 153/166 [00:53<00:06,  1.93it/s]\n",
      "Generating embeddings:   0%|          | 0/51 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 51/51 [00:00<00:00, 235.46it/s]\u001b[A\n",
      "Parsing nodes:  93%|█████████▎| 154/166 [00:53<00:05,  2.31it/s]\n",
      "Generating embeddings:   0%|          | 0/49 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 49/49 [00:00<00:00, 259.14it/s]\u001b[A\n",
      "Parsing nodes:  93%|█████████▎| 155/166 [00:53<00:04,  2.75it/s]\n",
      "Generating embeddings:   0%|          | 0/38 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  53%|█████▎    | 20/38 [00:00<00:00, 159.57it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 38/38 [00:00<00:00, 151.68it/s]\u001b[A\n",
      "Parsing nodes:  94%|█████████▍| 156/166 [00:54<00:03,  3.00it/s]\n",
      "Generating embeddings:   0%|          | 0/71 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  42%|████▏     | 30/71 [00:00<00:00, 218.92it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 71/71 [00:00<00:00, 210.16it/s]\u001b[A\n",
      "Parsing nodes:  95%|█████████▍| 157/166 [00:54<00:03,  2.95it/s]\n",
      "Generating embeddings:   0%|          | 0/48 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 48/48 [00:00<00:00, 237.24it/s]\u001b[A\n",
      "Parsing nodes:  95%|█████████▌| 158/166 [00:54<00:02,  3.32it/s]\n",
      "Generating embeddings:   0%|          | 0/103 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  29%|██▉       | 30/103 [00:00<00:00, 273.07it/s]\u001b[A\n",
      "Generating embeddings:  58%|█████▊    | 60/103 [00:00<00:00, 263.95it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 103/103 [00:00<00:00, 241.55it/s][A\n",
      "Parsing nodes:  96%|█████████▌| 159/166 [00:55<00:02,  2.90it/s]\n",
      "Generating embeddings:   0%|          | 0/164 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  24%|██▍       | 40/164 [00:00<00:00, 325.61it/s]\u001b[A\n",
      "Generating embeddings:  45%|████▍     | 73/164 [00:00<00:00, 233.82it/s]\u001b[A\n",
      "Generating embeddings:  60%|█████▉    | 98/164 [00:00<00:00, 210.53it/s]\u001b[A\n",
      "Generating embeddings:  73%|███████▎  | 120/164 [00:00<00:00, 201.52it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 164/164 [00:00<00:00, 224.67it/s]\u001b[A\n",
      "Parsing nodes:  96%|█████████▋| 160/166 [00:55<00:02,  2.13it/s]\n",
      "Generating embeddings:   0%|          | 0/72 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  42%|████▏     | 30/72 [00:00<00:00, 272.10it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 72/72 [00:00<00:00, 262.89it/s]\u001b[A\n",
      "Parsing nodes:  97%|█████████▋| 161/166 [00:56<00:02,  2.41it/s]\n",
      "Generating embeddings:   0%|          | 0/90 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  33%|███▎      | 30/90 [00:00<00:00, 279.24it/s]\u001b[A\n",
      "Generating embeddings:  64%|██████▍   | 58/90 [00:00<00:00, 248.72it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 90/90 [00:00<00:00, 217.07it/s]\u001b[A\n",
      "Parsing nodes:  98%|█████████▊| 162/166 [00:56<00:01,  2.38it/s]\n",
      "Generating embeddings:   0%|          | 0/172 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  17%|█▋        | 30/172 [00:00<00:00, 219.29it/s]\u001b[A\n",
      "Generating embeddings:  35%|███▍      | 60/172 [00:00<00:00, 234.92it/s]\u001b[A\n",
      "Generating embeddings:  58%|█████▊    | 100/172 [00:00<00:00, 290.24it/s]\u001b[A\n",
      "Generating embeddings:  76%|███████▌  | 130/172 [00:00<00:00, 289.97it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 172/172 [00:00<00:00, 285.64it/s]\u001b[A\n",
      "Parsing nodes:  98%|█████████▊| 163/166 [00:57<00:01,  2.07it/s]\n",
      "Generating embeddings:   0%|          | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 64/64 [00:00<00:00, 283.44it/s]\u001b[A\n",
      "Parsing nodes:  99%|█████████▉| 164/166 [00:57<00:00,  2.44it/s]\n",
      "Generating embeddings:   0%|          | 0/61 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 61/61 [00:00<00:00, 263.77it/s]\u001b[A\n",
      "Parsing nodes:  99%|█████████▉| 165/166 [00:57<00:00,  2.75it/s]\n",
      "Generating embeddings:   0%|          | 0/46 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  43%|████▎     | 20/46 [00:00<00:00, 183.63it/s]\u001b[A\n",
      "Generating embeddings: 100%|██████████| 46/46 [00:00<00:00, 191.50it/s]\u001b[A\n",
      "Parsing nodes: 100%|██████████| 166/166 [00:58<00:00,  2.86it/s]\n"
     ]
    }
   ],
   "source": [
    "semantic_nodes = splitter.get_nodes_from_documents(loaded_documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4be7336b-3499-493e-b2e7-b5fb858abbdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Greetings, Llama Lovers!\n",
       "\n",
       "Welcome to this week’s edition of the LlamaIndex newsletter! We’re excited to\n",
       "share our latest updates including dynamic features like LlamaIndex Workflows\n",
       "and retrieval capabilities in LlamaCloud. Check out our in-depth guides,\n",
       "tutorials, and the upcoming webinars that will help you make the most of these\n",
       "new developments.\n",
       "\n",
       "##  **The highlights:**\n",
       "\n",
       "  1. **LlamaIndex Workflows Launched:** LlamaIndex Workflows, a new event-driven architecture for building multi-agent applications, supports batching, async operations, and streaming. Agents subscribe to and emit events for complex, readable, Pythonic orchestration. [ Blogpost ](https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex) , [ Tweet ](https://x.com/llama_index/status/1819048068798616058) . \n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'llamaindex-newsletter-2024-08-06.md', 'extension': '.md', 'title': 'LlamaIndex Newsletter 2024-08-06', 'date': 'Aug 6, 2024', 'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-08-06'}\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "2. **Dynamic Retrieval Feature in LlamaCloud:** A new feature in LlamaCloud now supports dynamic retrieval for QA assistants, enabling both chunk-level and file-level document retrieval based on query similarity to intelligently route queries. [ Blogpost ](https://www.llamaindex.ai/blog/dynamic-retrieval-with-llamacloud) , [ Notebook ](https://github.com/run-llama/llamacloud-demo/blob/main/examples/10k_apple_tesla/demo_file_retrieval.ipynb) , [ Tweet ](https://x.com/llama_index/status/1818337133746360623) . \n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'llamaindex-newsletter-2024-08-06.md', 'extension': '.md', 'title': 'LlamaIndex Newsletter 2024-08-06', 'date': 'Aug 6, 2024', 'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-08-06'}\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "3. **LongRAG LlamaPack:** LongRAG is now available as a LlamaPack in LlamaIndex, utilizing larger document chunks and long-context LLMs for more effective synthesis. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-longrag/examples/longrag.ipynb) , [ Tweet ](https://x.com/llama_index/status/1818802688274100578) . \n",
       "\n",
       "##  **Feature Releases and Enhancements:**\n",
       "\n",
       "  * We have launched LlamaIndex Workflows, a new event-driven way to build multi-agent applications where each agent acts as a component that subscribes to and emits events, allowing for complex, readable, and Pythonic orchestration with enhanced support for batching, async operations, and streaming. [ Blogpost ](https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex) , [ Tweet ](https://x.com/llama_index/status/1819048068798616058) . \n",
       "  * We have introduced a new feature in LlamaCloud to improve your QA assistant with our latest capability for dynamic retrieval, allowing both chunk-level and file-level retrieval. This feature enables the retrieval of entire documents based on query similarity, which supports building agents that can intelligently route queries based on their content. [ Blogpost ](https://www.llamaindex.ai/blog/dynamic-retrieval-with-llamacloud) , [ Notebook ](https://github.com/run-llama/llamacloud-demo/blob/main/examples/10k_apple_tesla/demo_file_retrieval.ipynb) , [ Tweet ](https://x.com/llama_index/status/1818337133746360623) . \n",
       "  * We have launched LongRAG as a LlamaPack in LlamaIndex. LongRAG simplifies retrieval by using larger document chunks and leveraging long-context LLMs for synthesis. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-longrag/examples/longrag.ipynb) , [ Tweet ](https://x.com/llama_index/status/1818802688274100578) . \n",
       "\n",
       "**Guides:**\n",
       "\n",
       "  * [ Guide ](https://docs.llamaindex.ai/en/latest/examples/workflow/react_agent/) to building a ReAct agent from scratch using LlamaIndex workflows. \n",
       "  * [ Guide ](https://docs.llamaindex.ai/en/latest/examples/workflow/rag/) to Building an Event-Driven RAG Pipeline with LlamaIndex, featuring distinct event-driven steps for retrieval, reranking, and synthesis, enhanced with graph tracing and async processing. \n",
       "  * [ Guide ](https://docs.llamaindex.ai/en/latest/module_guides/observability/#mlflow) to MLflow in LlamaIndex to manage, deploy, and monitor your genAI applications with MLflow's tracking, packaging, evaluation, and tracing capabilities. \n",
       "\n",
       "**Tutorials:**\n",
       "\n",
       "  * [ Pavan Kumar’s ](https://x.com/pavan_mantha1) [ tutorial ](https://blog.gopenai.com/building-smarter-agents-using-llamaindex-agents-and-qdrants-hybrid-search-50c0ecbbfb0d) on Building Smarter Agents using LlamaIndex Agents and Qdrant’s Hybrid Search. \n",
       "  * [ Farzad Sunavala’s ](https://www.linkedin.com/in/farzadsunavala) [ tutorial ](https://farzzy.hashnode.dev/rag-observability-and-evaluation-with-azure-ai-search-azure-openai-llamaindex-and-arize-phoenix) on RAG Observability and Evaluation with Azure AI Search, Azure OpenAI, LlamaIndex, and Arize Phoenix. \n",
       "  * [ Composio’s ](https://x.com/composiohq) [ tutorial ](https://github.com/ComposioHQ/composio/tree/master/python/examples/pr_agent/pr_agent_llama_index) on building a PR review agent using Composio's GitHub/Slack tools and LlamaIndex agent abstractions. \n",
       "  * [ Benito Martin’s ](https://medium.com/@benitomartin) [ tutorial ](https://medium.com/@benitomartin/find-your-code-scaling-a-llamaindex-and-qdrant-application-with-google-kubernetes-engine-2db126f16344) on Scaling a LlamaIndex and Qdrant Application with Google Kubernetes Engine. \n",
       "  * [ Chew Loong Nian’s ](https://medium.com/@chewloongnian) [ tutorial ](https://pub.towardsai.net/introducing-llamaextract-beta-transforming-metadata-extraction-for-enhanced-rag-queries-de3d74d34cd7) on Transforming Metadata Extraction for Enhanced RAG Queries using LlamaExtract. \n",
       "  * [ Pavan Kumar’s ](https://x.com/pavan_mantha1) [ tutorial ](https://medium.com/@manthapavankumar11/practical-implementation-of-agentic-rag-workflows-with-llama-index-and-qdrant-3b6622cd3124) on Practical Implementation of Agentic RAG Workflows with Llama-Index and Qdrant. \n",
       "  * AI21 Labs [ tutorial ](https://www.llamaindex.ai/blog/jamba-instruct-s-256k-context-window-on-llamaindex) on using Jamba-Instruct Model with LlamaIndex. \n",
       "\n",
       "**Webinars And Hackathons:**\n",
       "\n",
       "  * [ Join us ](https://lu.ma/ka5xtyqo) for a webinar on August 8th with [ Dedy Kredo ](https://x.com/DedyKredo) from [ CodiumAI ](https://x.com/CodiumAI) on using RAG with LlamaIndex to help build a code generation solution that’s contextually aware of the right elements of source code. \n",
       "  * [ Join us ](https://lu.ma/p13pkknm?tk=SsniSt) on RAG Hack Night at GitHub with [ Weaviate ](https://x.com/weaviate_io) , [ Neosync ](https://x.com/neosynccloud) , [ Arize AI ](https://x.com/arizeai) on August 13th. \n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'llamaindex-newsletter-2024-08-06.md', 'extension': '.md', 'title': 'LlamaIndex Newsletter 2024-08-06', 'date': 'Aug 6, 2024', 'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-08-06'}\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "We’re pleased to be introducing a brand-new beta feature of LlamaIndex:\n",
       "workflows, a mechanism for orchestrating actions in the increasingly-complex\n",
       "AI application we see our users building.\n",
       "\n",
       "What started as a trend with the advent of LLMs is now a de-facto standard: AI\n",
       "applications are made of multiple tasks implemented by different components.\n",
       "Open source frameworks in the market strive to make the life of AI engineers\n",
       "easier by providing easy-to-use abstractions for foundational components like\n",
       "data loaders, LLMs, vector databases, and rerankers, all the way up to\n",
       "external services. Meanwhile, all of those frameworks are also on a quest to\n",
       "find what’s the best abstraction to orchestrate such components, researching\n",
       "what’s most intuitive and efficient for an AI developer in order to implement\n",
       "the logic that keeps together a compound AI system.\n",
       "\n",
       "Two of those potential orchestration patterns are chains and pipelines, both\n",
       "of which are implementations of the same Directed Acyclic Graph (DAG)\n",
       "abstraction. We took a stab at this with our [ Query Pipelines\n",
       "](https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537)\n",
       "release at the beginning of the year - it was a declarative API that let you\n",
       "orchestrate simple-to-advanced query workflows over your data for different\n",
       "use cases, like QA, structured extraction, and agentic automation. But as we\n",
       "tried to build upon it and experimented with adding cycles to better support\n",
       "more complex workflows, we noticed several issues, causing us to reflect on\n",
       "why a DAG may not be the right fit for an agentic landscape, and what\n",
       "alternatives we could introduce in the framework.\n",
       "\n",
       "##  Limitations of a Graph-based UX\n",
       "\n",
       "A fundamental aspect of DAGs is the “A” in DAGs: they are acyclic, meaning\n",
       "there are no loops. But in a world that’s more and more agentic, the inability\n",
       "to perform loops in an AI application’s logic is simply unacceptable. For\n",
       "example, if one component provides bad results, an AI developer should have a\n",
       "way to tell the system to self-correct and try again.\n",
       "\n",
       "Even without adding cycles and loops to a DAG, the query pipeline suffered\n",
       "from a few noticeable issues:\n",
       "\n",
       "  * hard to debug when things go wrong \n",
       "  * they obscure how components and modules are being executed \n",
       "  * our pipeline orchestrator became increasingly extremely complex and had to handle a ton of different edge cases \n",
       "  * they were hard to read for complex pipelines \n",
       "\n",
       "Once we added cycles to query pipelines, these developer UX issues around\n",
       "graphs were amplified. We experienced first-hand developer pain in areas like:\n",
       "\n",
       "  * A lot of core orchestration logic like ` if-else ` statements and ` while ` loops get baked into the edges of the graph. Defining these edges becomes cumbersome and verbose. \n",
       "  * It became hard to handle edge cases around optional and default values. It was hard for us as a framework to figure out whether a parameter would get passed from upstream nodes. \n",
       "  * Defining graphs with cycles didn’t always feel as natural to developers building agents. An agent encapsulates a general LLM-powered entity that can take in observations and generate responses. Here the graph UX enforced that “agent” node had the incoming edges and outgoing edges explicitly defined, forcing users to define verbose communication patterns with other nodes. \n",
       "\n",
       "We asked: are graphs really the only abstraction we can use to orchestrate\n",
       "components in a compound AI system?\n",
       "\n",
       "##  From Graphs to EDA: go event-driven\n",
       "\n",
       "A compound AI system can be implemented with a LlamaIndex _workflow_ . The\n",
       "workflow dispatches events back and forth through a collection of Python\n",
       "functions called _steps_ . Each step can be seen as one component of your\n",
       "system: one to process a query, one to talk with an LLM, one to load data from\n",
       "a vector database and so on. Every step receives one or more events to process\n",
       "and can optionally send back events that will be relayed to other components\n",
       "if needed.\n",
       "\n",
       "Moving to an event-driven architecture causes a fundamental shift in design.\n",
       "In many graph implementations the graph traversal algorithm is responsible for\n",
       "determining what component should run next and what data should be passed. In\n",
       "an event-driven architecture, the component subscribes to a certain types of\n",
       "events and it’s ultimately responsible for deciding what to do based on the\n",
       "data it received.\n",
       "\n",
       "In an event-driven system, concepts like optionality of inputs and default\n",
       "values are sorted out at the component level, dramatically simplifying the\n",
       "orchestration code.\n",
       "\n",
       "##  A workflow primer\n",
       "\n",
       "To help clarify this idea, let’s look at an example. A minimal LlamaIndex\n",
       "workflow looks like this:\n",
       "\n",
       "    \n",
       "    \n",
       "    from llama_index.core.workflow import (\n",
       "        StartEvent,\n",
       "        StopEvent,\n",
       "        Workflow,\n",
       "        step,\n",
       "    )\n",
       "    \n",
       "    from llama_index.llms.openai import OpenAI\n",
       "    \n",
       "    class OpenAIGenerator(Workflow):\n",
       "        @step()\n",
       "        async def generate(self, ev: StartEvent) -> StopEvent:\n",
       "            query = ev.get(\"query\")\n",
       "            llm = OpenAI()\n",
       "            response = await llm.acomplete(query)\n",
       "            return StopEvent(result=str(response))\n",
       "    \n",
       "    w = OpenAIGenerator(timeout=10, verbose=False)\n",
       "    result = await w.run(query=\"What's LlamaIndex?\")\n",
       "    print(result)\n",
       "\n",
       "The ` generate ` function is marked as a workflow step using the ` @step `\n",
       "decorator and it declares which events it wants to receive and which events it\n",
       "will send back using the method signature with proper typing annotations. In\n",
       "order to run a workflow, we create an instance of the ` OpenAIGenerator `\n",
       "class passing some configuration parameters like the desired timeout and we\n",
       "then call the ` run ` method. Any keyword argument passed to ` run ` will be\n",
       "packed into a special event of type ` StartEvent ` that will be relayed to the\n",
       "steps that requested it (in this case, only the ` generate ` step). The `\n",
       "generate ` step returns a special event of type ` StopEvent ` that will signal\n",
       "the workflow to gracefully halt its execution. A ` StopEvent ` carries any\n",
       "data that we want to return to the caller as the workflow result, in this case\n",
       "the LLM response.\n",
       "\n",
       "###  Workflows can loop\n",
       "\n",
       "In event-driven architectures, loops have to do with communication rather than\n",
       "topology. Any step can decide to call another step multiple times by crafting\n",
       "and sending the proper event. Let’s see a self-correction loop for example\n",
       "(check the [ notebook\n",
       "](https://docs.llamaindex.ai/en/latest/examples/workflow/reflection/) for the\n",
       "full code):\n",
       "\n",
       "    \n",
       "    \n",
       "    class ExtractionDone(Event):\n",
       "        output: str\n",
       "        passage: str\n",
       "    \n",
       "    \n",
       "    class ValidationErrorEvent(Event):\n",
       "        error: str\n",
       "        wrong_output: str\n",
       "        passage: str\n",
       "        \n",
       "        \n",
       "    class ReflectionWorkflow(Workflow):\n",
       "        @step()\n",
       "        async def extract(\n",
       "            self, ev: StartEvent | ValidationErrorEvent\n",
       "        ) -> StopEvent | ExtractionDone:\n",
       "            if isinstance(ev, StartEvent):\n",
       "                passage = ev.get(\"passage\")\n",
       "                if not passage:\n",
       "                    return StopEvent(result=\"Please provide some text in input\")\n",
       "                reflection_prompt = \"\"\n",
       "            elif isinstance(ev, ValidationErrorEvent):\n",
       "                passage = ev.passage\n",
       "                reflection_prompt = REFLECTION_PROMPT.format(\n",
       "                    wrong_answer=ev.wrong_output, error=ev.error\n",
       "                )\n",
       "    \n",
       "            llm = Ollama(model=\"llama3\", request_timeout=30)\n",
       "            prompt = EXTRACTION_PROMPT.format(\n",
       "                passage=passage, schema=CarCollection.schema_json()\n",
       "            )\n",
       "            if reflection_prompt:\n",
       "                prompt += reflection_prompt\n",
       "    \n",
       "            output = await llm.acomplete(prompt)\n",
       "    \n",
       "            return ExtractionDone(output=str(output), passage=passage)\n",
       "    \n",
       "        @step()\n",
       "        async def validate(\n",
       "            self, ev: ExtractionDone\n",
       "        ) -> StopEvent | ValidationErrorEvent:\n",
       "            try:\n",
       "                json.loads(ev.output)\n",
       "            except Exception as e:\n",
       "                print(\"Validation failed, retrying...\")\n",
       "                return ValidationErrorEvent(\n",
       "                    error=str(e), wrong_output=ev.output, passage=ev.passage\n",
       "                )\n",
       "    \n",
       "            return StopEvent(result=ev.output)\n",
       "    \n",
       "    w = ReflectionWorkflow(timeout=60, verbose=True)\n",
       "    result = await w.run(\n",
       "        passage=\"There are two cars available: a Fiat Panda with 45Hp and a Honda Civic with 330Hp.\"\n",
       "    )\n",
       "    print(result)\n",
       "\n",
       "In this example, the ` validate ` step receives the result of the tentative\n",
       "schema extraction as an event and it can decide to try again by returning a `\n",
       "ValidationErrorEvent ` that will be eventually delivered to the ` extract `\n",
       "step which will perform another attempt. Note that in this example the\n",
       "workflow might time out if this extract/validate loop keeps providing poor\n",
       "results for too long, but another strategy might be giving up after a precise\n",
       "number of attempts, just to give an example.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex.md', 'extension': '.md', 'title': 'Introducing workflows beta: a new way to create complex AI applications with LlamaIndex', 'date': 'Aug 1, 2024', 'url': 'https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex'}\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "###  Workflows keep state\n",
       "\n",
       "Workflows keep a global state during the execution, and this state can be\n",
       "shared and propagated to its steps upon request. This shared state is\n",
       "implemented as a ` Context ` object and can be used by steps to store data in\n",
       "between iterations but also as an alternative form of communication among\n",
       "different steps. Let’s see an excerpt from a more complex RAG example as an\n",
       "example showing how to use the global context (check [ notebook\n",
       "](https://docs.llamaindex.ai/en/latest/examples/workflow/rag/) for full code):\n",
       "\n",
       "    \n",
       "    \n",
       "    class RAGWorkflow(Workflow):\n",
       "        @step(pass_context=True)\n",
       "        async def ingest(self, ctx: Context, ev: StartEvent) -> Optional[StopEvent]:\n",
       "            dataset_name = ev.get(\"dataset\")\n",
       "            _, documents = download_llama_dataset(dsname, \"./data\")\n",
       "            ctx.data[\"INDEX\"] = VectorStoreIndex.from_documents(documents=documents)\n",
       "            return StopEvent(result=f\"Indexed {len(documents)} documents.\")\n",
       "            \n",
       "        ...\n",
       "\n",
       "In this case the ` ingest ` step creates an index, and it wants to make it\n",
       "available to any other step that might needed it later during workflow\n",
       "execution. The idiomatic way of doing that in a LlamaIndex workflow is to\n",
       "declare the step requires an instance of the global context ( `\n",
       "@step(pass_context=True) ` does the trick) and store the index in the context\n",
       "itself with a predefined key that other steps might access later.\n",
       "\n",
       "###  Workflows can be customized\n",
       "\n",
       "Alongside Workflows, we’ll be releasing a set of predefined workflows so that\n",
       "the most common use cases can be implemented with a single line of code. "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex.md', 'extension': '.md', 'title': 'Introducing workflows beta: a new way to create complex AI applications with LlamaIndex', 'date': 'Aug 1, 2024', 'url': 'https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex'}\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Using\n",
       "these predefined flows, users still might want to just _slightly_ change a\n",
       "predefined workflow to introduce some custom behavior without having to\n",
       "rewrite a whole workflow from scratch. Let’s say you want to customize a RAG\n",
       "workflow and use a custom re-ranking step, all you would need to do is\n",
       "subclass a hypothetical built-in ` RAGWorkflow ` class and override the `\n",
       "rerank ` step like this:\n",
       "\n",
       "    \n",
       "    \n",
       "    class MyWorkflow(RAGWorkflow):\n",
       "        @step(pass_context=True)\n",
       "        def rerank(\n",
       "            self, ctx: Context, ev: Union[RetrieverEvent, StartEvent]\n",
       "        ) -> Optional[QueryResult]:\n",
       "            # my custom reranking logic here\n",
       "            \n",
       "     \n",
       "    w = MyWorkflow(timeout=60, verbose=True)\n",
       "    result = await w.run(query=\"Who is Paul Graham?\")\n",
       "\n",
       "###  Workflows can be debugged\n",
       "\n",
       "The complexity of your workflows will grow with the complexity of your\n",
       "application logic, and sometimes it can be hard to understand how events will\n",
       "flow during execution by just looking at the Python code. "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex.md', 'extension': '.md', 'title': 'Introducing workflows beta: a new way to create complex AI applications with LlamaIndex', 'date': 'Aug 1, 2024', 'url': 'https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex'}\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "To ease the\n",
       "understanding of complex workflows and to support the debugging of workflow\n",
       "executions, LlamaIndex provides two functions:\n",
       "\n",
       "  * ` draw_all_possible_flows ` produces a picture showing all the steps in a workflow and how events will possibly flow \n",
       "  * ` draw_most_recent_execution ` produces a similar picture, showing only the events that were actually sent during the last workflow execution \n",
       "\n",
       "On top of that, workflows can be executed manually, by calling ` run_step() `\n",
       "multiple times until all the steps have completed. After each ` run_step `\n",
       "call, the workflow can be inspected, examining any intermediate results or\n",
       "debug logs.\n",
       "\n",
       "##  Why you should use workflows today\n",
       "\n",
       "Despite being at an early stage of development, LlamaIndex workflows already\n",
       "represent a step forward compared to query pipelines, extending their\n",
       "functionalities and adding more flexibility. On top of that, workflows come\n",
       "with a set of features that you would normally expect from a much more mature\n",
       "software:\n",
       "\n",
       "  * Fully async with streaming support \n",
       "  * Instrumented by default, providing one-click observability with the supported integrations \n",
       "  * Step-by-step execution for easier debugging \n",
       "  * Validation and visualization of the event-driven dependencies \n",
       "  * Events are implemented as pydantic models to ease customization and further developments of new features \n",
       "\n",
       "##  Resources\n",
       "\n",
       "Check out our [ workflow documentation\n",
       "](https://docs.llamaindex.ai/en/latest/module_guides/workflow/) and our [\n",
       "examples ](https://github.com/run-\n",
       "llama/llama_index/tree/main/docs/docs/examples/workflow) including:\n",
       "\n",
       "  * [ RAG ](https://docs.llamaindex.ai/en/latest/examples/workflow/rag/)\n",
       "  * [ Reflection ](https://docs.llamaindex.ai/en/latest/examples/workflow/reflection/)\n",
       "  * [ Function calling ](https://docs.llamaindex.ai/en/latest/examples/workflow/function_calling_agent/)\n",
       "  * [ ReAct agent ](https://docs.llamaindex.ai/en/latest/examples/workflow/react_agent/)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex.md', 'extension': '.md', 'title': 'Introducing workflows beta: a new way to create complex AI applications with LlamaIndex', 'date': 'Aug 1, 2024', 'url': 'https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex'}\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Build state-of-the-art RAG applications for the enterprise by leveraging\n",
       "LlamaIndex’s market-leading RAG strategies with AI21 Labs’ long context\n",
       "Foundation Model, Jamba-Instruct.\n",
       "\n",
       "We at AI21 Labs are excited to announce that our groundbreaking Jamba-Instruct\n",
       "foundation model is now available through leading data framework LlamaIndex.\n",
       "With this integration, developers can now build powerful RAG enterprise\n",
       "applications with enhanced accuracy and cost-efficiency due to Jamba-\n",
       "Instruct’s impressive 256K context window and LlamaIndex’s sophisticated end-\n",
       "to-end offerings for RAG.\n",
       "\n",
       "While many models declare long context windows, researchers at NVIDIA found\n",
       "that [ most falter under evaluation ](https://arxiv.org/pdf/2404.06654) ,\n",
       "revealing a discrepancy between their claimed and effective context window\n",
       "lengths. Jamba-Instruct is one of the few models on the market to not only\n",
       "achieve parity between its declared and effective lengths, but to do so with a\n",
       "much longer context window length than any other model in its size class.\n",
       "\n",
       "By offering a context window of 256K—roughly equivalent to 800 pages of\n",
       "text—Jamba-Instruct increases the number of retrieved chunks and can vastly\n",
       "improve the entire RAG system, rather than trying to improve the search\n",
       "mechanism or incorporating an additional reranking component. Using a long\n",
       "context foundation model like Jamba-Instruct makes querying private enterprise\n",
       "data with RAG both more reliable and easier.\n",
       "\n",
       "In the following notebook ( [ also available directly on colab\n",
       "](https://colab.research.google.com/drive/1ycpC1pfCty9bqCmHdrgvAtqQwP1o0lPg)\n",
       "), we’ll walk through an example of querying a collection of financial\n",
       "documents, showing how Jamba-Instruct’s 256K context window allows the RAG\n",
       "pipeline to retrieve more chunks at once in order to deliver an accurate\n",
       "answer.\n",
       "\n",
       "###  RAG Q&A on financial documents\n",
       "\n",
       "To get started, these are the packages you need to install. You will also need\n",
       "API keys to set up OpenAI for embeddings and AI21 for Jamba-Instruct.\n",
       "\n",
       "    \n",
       "    \n",
       "    !pip install llama-index\n",
       "    !pip install -U ai21\n",
       "    !pip install llama-index-llms-ai21\n",
       "    \n",
       "    import os\n",
       "    from llama_index.core.llama_dataset import download_llama_dataset\n",
       "    from llama_index.core.llama_pack import download_llama_pack\n",
       "    from llama_index.core import VectorStoreIndex\n",
       "    from llama_index.core import SimpleDirectoryReader\n",
       "    from llama_index.llms.ai21 import AI21\n",
       "    \n",
       "    os.environ['OPENAI_API_KEY'] = 'YOUR_OPENAI_API_KEY' # For embeddings\n",
       "    os.environ['AI21_API_KEY'] = 'YOUR_AI21_API_KEY' # For the generation\n",
       "    \n",
       "    # Setup jamba instruct as the llm\n",
       "    llm = AI21(\n",
       "        model='jamba-instruct',\n",
       "        temperature=0,\n",
       "        max_tokens=2000\n",
       "    )\n",
       "\n",
       "Next, download 5 10-K forms from Amazon from [ Amazon’s Investor Relations\n",
       "page. ](https://ir.aboutamazon.com/sec-filings/default.aspx)\n",
       "\n",
       "    \n",
       "    \n",
       "    # Get the data - download 10k forms from AMZN from the last five years\n",
       "    os.mkdir(\"data\")\n",
       "    !wget 'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/c7c14359-36fa-40c3-b3ca-5bf7f3fa0b96.pdf' -O 'data/amazon_2023.pdf'\n",
       "    !wget 'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/d2fde7ee-05f7-419d-9ce8-186de4c96e25.pdf' -O 'data/amazon_2022.pdf'\n",
       "    !wget 'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/f965e5c3-fded-45d3-bbdb-f750f156dcc9.pdf' -O 'data/amazon_2021.pdf'\n",
       "    !wget 'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/336d8745-ea82-40a5-9acc-1a89df23d0f3.pdf' -O 'data/amazon_2020.pdf'\n",
       "    !wget 'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/4d39f579-19d8-4119-b087-ee618abf82d6.pdf' -O 'data/amazon_2019.pdf'\n",
       "\n",
       "Set up your index and query engine to create the retrieval and generation\n",
       "components of your RAG system.\n",
       "\n",
       "    \n",
       "    \n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'jamba-instruct-s-256k-context-window-on-llamaindex.md', 'extension': '.md', 'title': \"Jamba-Instruct's 256k context window on LlamaIndex\", 'date': 'Jul 31, 2024', 'url': 'https://www.llamaindex.ai/blog/jamba-instruct-s-256k-context-window-on-llamaindex'}\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Setup the index\n",
       "    file_list = [os.path.join(\"data\", f) for f in os.listdir(\"data\")]\n",
       "    \n",
       "    amzn_10k_docs = SimpleDirectoryReader(input_files=file_list).load_data()\n",
       "    index = VectorStoreIndex.from_documents(documents=amzn_10k_docs)\n",
       "    \n",
       "    # Build a query engine\n",
       "    default_query_engine = index.as_query_engine(llm)\n",
       "\n",
       "Let’s enter a query to make sure our RAG system is working.\n",
       "\n",
       "    \n",
       "    \n",
       "    answer = default_query_engine.query(\"What was the company's revenue in 2021?\")\n",
       "    print(answer.response)\n",
       "    \n",
       "    \n",
       "    The company's revenue in 2021 was $469,822 million.\n",
       "\n",
       "Great! "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'jamba-instruct-s-256k-context-window-on-llamaindex.md', 'extension': '.md', 'title': \"Jamba-Instruct's 256k context window on LlamaIndex\", 'date': 'Jul 31, 2024', 'url': 'https://www.llamaindex.ai/blog/jamba-instruct-s-256k-context-window-on-llamaindex'}\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "It works. Now let’s try a similar query to continue validating.\n",
       "\n",
       "    \n",
       "    \n",
       "    answer = default_query_engine.query(\"What was the company's revenue in 2023?\")\n",
       "    print(answer.response)\n",
       "    \n",
       "    \n",
       "    The company's revenue in 2023 was not explicitly mentioned in the provided context. However, it is mentioned that the company's operating income increased to $36.9 billion in 2023, compared to $12.2 billion in 2022.\n",
       "\n",
       "We can see there’s a problem—we know that the answer to our question is most\n",
       "definitely included in our documents, yet our RAG system is claiming that it\n",
       "cannot find the answer. That’s because the default amount of retrieved chunks\n",
       "is rather small (a few chunks). This makes the whole system prone to errors\n",
       "and failing to capture information that is indeed located in the documents.\n",
       "\n",
       "However, with Jamba-Instruct, a model which handles a 256K context window\n",
       "effectively, we can increase the number of retrieved chunks from just a few\n",
       "(default value) to 100 and vastly improve the entire RAG system.\n",
       "\n",
       "Let’s build a new query engine on top of our existing index and try the query\n",
       "that failed before.\n",
       "\n",
       "    \n",
       "    \n",
       "    # Large amount of chunks in the retrieval process\n",
       "    extended_query_engine = index.as_query_engine(llm,\n",
       "                                                  similarity_top_k=100)\n",
       "    \n",
       "    answer = extended_query_engine.query(\"What was the company's revenue in 2023?\")\n",
       "    print(answer.response)\n",
       "    \n",
       "    \n",
       "    The company's revenue in 2023 was $574.785 million.\n",
       "\n",
       "We see that the RAG system, with the help of Jamba-Instruct’s 256K context\n",
       "window, is now able to produce the accurate answer.\n",
       "\n",
       "Let’s try one more answer to validate our new RAG system.\n",
       "\n",
       "    \n",
       "    \n",
       "    answer = default_query_engine.query(\"Was there a stock split in the last five years?\")\n",
       "    print(answer.response)\n",
       "    \n",
       "    \n",
       "    No, there was no stock split in the last five years.\n",
       "    \n",
       "    \n",
       "    answer = extended_query_engine.query(\"Was there a stock split in the last five years?\")\n",
       "    print(answer.response)\n",
       "    \n",
       "    \n",
       "    Yes, there was a stock split in the last five years. On May 27, 2022, Amazon.com, Inc. effected a 20-for-1 stock split of its common stock.\n",
       "\n",
       "###  Context is king\n",
       "\n",
       "Often, the debate is framed as “RAG vs. long context.” We at AI21 Labs believe\n",
       "that’s the wrong way to look at it. Rather, it’s long context _plus_ RAG. When\n",
       "paired together in an AI system, a long context model enhances the quality and\n",
       "accuracy of a RAG system, especially useful in enterprise contexts that\n",
       "involve lengthy documents or vast databases of information.\n",
       "\n",
       "Going forward, as RAG systems continue to scale, the number of documents and\n",
       "lengths of chunks will drastically increase. Only a long context model—whose\n",
       "context length truly delivers—can handle this amount of text.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'jamba-instruct-s-256k-context-window-on-llamaindex.md', 'extension': '.md', 'title': \"Jamba-Instruct's 256k context window on LlamaIndex\", 'date': 'Jul 31, 2024', 'url': 'https://www.llamaindex.ai/blog/jamba-instruct-s-256k-context-window-on-llamaindex'}\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for node in semantic_nodes[:10]:\n",
    "    display(Markdown(node.text))\n",
    "    print(node.metadata)\n",
    "    print(\"==\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0ce6512-b710-4842-959d-26c872604e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_docstore = SimpleDocumentStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e4bb2e3-d6e7-4fe1-b3c6-f6c191ab553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_docstore.add_documents(docs=semantic_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57c1e16a-c8d8-46d1-9efd-13b6e8177f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_docstore.persist(\"database/llama-blogs-nodes-semantic-parser.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aae002-8532-4ec6-933b-e179f6e33dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-py310",
   "language": "python",
   "name": "llm-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
