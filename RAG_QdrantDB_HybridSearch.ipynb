{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0a3748d-7cd0-4434-b832-266ee0b51b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/llm/lib/python3.10/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import qdrant_client\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "# from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.response.notebook_utils import display_response\n",
    "# from fastembed import TextEmbedding\n",
    "import model_utils\n",
    "import prompt_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1d48fbd-f93d-4523-a5a6-d4374a50a851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama index ascyncio config\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# logging config\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7c6f555-91bb-4b02-8294-be90b2999e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_name = \"models/Meta-Llama-3.1-8B-Instruct\"\n",
    "embed_model_name = \"models/bge-small-en-v1.5\"\n",
    "device_map = \"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87988429-cfa2-4497-b2d6-9c78f0557f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: models/bge-small-en-v1.5\n",
      "Load pretrained SentenceTransformer: models/bge-small-en-v1.5\n",
      "INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2 prompts are loaded, with the keys: ['query', 'text']\n"
     ]
    }
   ],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=embed_model_name, device=device_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e233b9ee-9a55-44c7-9ebf-830e505c21ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model with quantization config from: models/Meta-Llama-3.1-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "# load local llm llama\n",
    "model, tokenizer = model_utils.load_quantized_model(\n",
    "    model_name_or_path=llm_name,\n",
    "    device=device_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce7be653-7e3f-4d74-80aa-f81b9b9749e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config llm and embed_model to llamaindex\n",
    "llm_hf = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    query_wrapper_prompt=PromptTemplate(prompt_utils.get_llama31_prompt_template()),\n",
    "    generate_kwargs={\n",
    "        \"temperature\": 0.5,\n",
    "        \"do_sample\": True\n",
    "    },\n",
    "    device_map=\"cuda\",\n",
    "    model_name=llm_name,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "Settings.llm = llm_hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8472df0e-2967-42de-8af4-8be4228aa085",
   "metadata": {},
   "source": [
    "### Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f3a370e-53f6-4079-b36f-dd33a22faea6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 159 documents\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    input_dir=\"./data\",\n",
    "    filename_as_id=True,\n",
    ").load_data()\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cb50d4-9701-489e-9e1f-db3564ad1d53",
   "metadata": {},
   "source": [
    "### Build/Load the VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a67ddbd-cd5d-479b-88f6-ba4a6dbb9947",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = qdrant_client.QdrantClient(\n",
    "    path=\"./qdrant_db/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb7c0d4a-cc69-4d72-98e2-f87b8384a1d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 94965.37it/s]\n",
      "\u001b[0;93m2024-08-07 15:19:36.602350110 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-08-07 15:19:36.602368583 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 89877.94it/s]\n",
      "\u001b[0;93m2024-08-07 15:19:37.046097667 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-08-07 15:19:37.046115130 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "vector_store = QdrantVectorStore(\n",
    "    client=client, \n",
    "    collection_name=\"llamaindex-blogs-hybrid-search\",\n",
    "    enable_hybrid=True,\n",
    "    fastembed_sparse_model=\"Qdrant/bm42-all-minilm-l6-v2-attentions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bef4d65-f167-411d-b1db-3840f10f4546",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageContext(docstore=<llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore object at 0x7f53ec9a6c20>, index_store=<llama_index.core.storage.index_store.simple_index_store.SimpleIndexStore object at 0x7f53eca61f90>, vector_stores={'default': QdrantVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=False, collection_name='llamaindex-blogs-hybrid-search', url=None, api_key=None, batch_size=64, parallel=1, max_retries=3, client_kwargs={}, enable_hybrid=True, index_doc_id=True, fastembed_sparse_model='Qdrant/bm42-all-minilm-l6-v2-attentions'), 'image': SimpleVectorStore(stores_text=False, is_embedding_query=True, data=SimpleVectorStoreData(embedding_dict={}, text_id_to_ref_doc_id={}, metadata_dict={}))}, graph_store=<llama_index.core.graph_stores.simple.SimpleGraphStore object at 0x7f53eca62860>, property_graph_store=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "storage_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c3e24b2-52d2-4fd6-b075-7ae1c333f3a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 159/159 [00:01<00:00, 143.84it/s]\n",
      "Generating embeddings:   0%|          | 0/874 [00:00<?, ?it/s]\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.22it/s]\u001b[A\n",
      "Generating embeddings:   1%|          | 10/874 [00:00<00:23, 36.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.91it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.05it/s]\n",
      "Generating embeddings:   3%|▎         | 30/874 [00:00<00:10, 81.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.41it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.56it/s]\n",
      "Generating embeddings:   6%|▌         | 50/874 [00:00<00:07, 111.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.48it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.08it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.46it/s]\n",
      "Generating embeddings:   9%|▉         | 80/874 [00:00<00:05, 148.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.44it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.44it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.58it/s]\n",
      "Generating embeddings:  13%|█▎        | 110/874 [00:00<00:04, 174.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.88it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.27it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.80it/s]\n",
      "Generating embeddings:  16%|█▌        | 140/874 [00:00<00:03, 193.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.05it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.44it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.09it/s]\n",
      "Generating embeddings:  19%|█▉        | 170/874 [00:01<00:03, 210.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.71it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.49it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.74it/s]\n",
      "Generating embeddings:  23%|██▎       | 200/874 [00:01<00:03, 222.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.23it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.77it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.25it/s]\n",
      "Generating embeddings:  26%|██▋       | 230/874 [00:01<00:02, 225.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.24it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.26it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.81it/s]\n",
      "Generating embeddings:  30%|██▉       | 260/874 [00:01<00:02, 230.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.06it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.06it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.16it/s]\n",
      "Generating embeddings:  33%|███▎      | 290/874 [00:01<00:02, 228.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.17it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.36it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.56it/s]\n",
      "Generating embeddings:  37%|███▋      | 320/874 [00:01<00:02, 233.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.67it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.43it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.97it/s]\n",
      "Generating embeddings:  40%|████      | 350/874 [00:01<00:02, 236.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.79it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.28it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.91it/s]\n",
      "Generating embeddings:  43%|████▎     | 380/874 [00:01<00:02, 238.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.16it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.11it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.84it/s]\n",
      "Generating embeddings:  47%|████▋     | 410/874 [00:02<00:01, 235.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.71it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.17it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.06it/s]\n",
      "Generating embeddings:  50%|█████     | 440/874 [00:02<00:01, 237.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.92it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.36it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.47it/s]\n",
      "Generating embeddings:  54%|█████▍    | 470/874 [00:02<00:01, 239.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.59it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.85it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.56it/s]\n",
      "Generating embeddings:  57%|█████▋    | 500/874 [00:02<00:01, 239.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.39it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.70it/s]\n",
      "Generating embeddings:  60%|█████▉    | 524/874 [00:02<00:01, 238.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.40it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.82it/s]\n",
      "Generating embeddings:  63%|██████▎   | 548/874 [00:02<00:01, 228.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.94it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.25it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.45it/s]\n",
      "Generating embeddings:  65%|██████▌   | 571/874 [00:02<00:01, 217.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.60it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.89it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.27it/s]\n",
      "Generating embeddings:  69%|██████▊   | 600/874 [00:02<00:01, 220.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.73it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.30it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.37it/s]\n",
      "Generating embeddings:  72%|███████▏  | 630/874 [00:03<00:01, 223.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.47it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.30it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.37it/s]\n",
      "Generating embeddings:  76%|███████▌  | 660/874 [00:03<00:00, 221.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.86it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.22it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.21it/s]\n",
      "Generating embeddings:  79%|███████▉  | 690/874 [00:03<00:00, 225.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.85it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.20it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.08it/s]\n",
      "Generating embeddings:  82%|████████▏ | 720/874 [00:03<00:00, 228.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.05it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.50it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.53it/s]\n",
      "Generating embeddings:  86%|████████▌ | 750/874 [00:03<00:00, 230.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.51it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.30it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.21it/s]\n",
      "Generating embeddings:  89%|████████▉ | 780/874 [00:03<00:00, 234.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.35it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.06it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.24it/s]\n",
      "Generating embeddings:  93%|█████████▎| 810/874 [00:03<00:00, 235.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.09it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.19it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.12it/s]\n",
      "Generating embeddings:  96%|█████████▌| 840/874 [00:03<00:00, 231.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.90it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.99it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.75it/s]\n",
      "Generating embeddings: 100%|█████████▉| 870/874 [00:04<00:00, 232.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 61.62it/s]\n",
      "Generating embeddings: 100%|██████████| 874/874 [00:04<00:00, 214.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:root:Payload indexes have no effect in the local Qdrant. Please use server Qdrant if you need payload indexes.\n",
      "Payload indexes have no effect in the local Qdrant. Please use server Qdrant if you need payload indexes.\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "adf3c01e-7a11-470b-a0dc-ba1bea0475e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = VectorStoreIndex.from_vector_store(\n",
    "#     vector_store=vector_store,\n",
    "#     storage_context=storage_context,\n",
    "#     show_progress=True,\n",
    "# )\n",
    "# index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33afca6-1c96-4e5f-a0fb-d6eb0a5686a5",
   "metadata": {},
   "source": [
    "### Query Index¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cee0f780-95be-44c2-bd92-2502999d0f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the two critical areas of RAG system performance that are assessed in the \"Evaluating RAG with LlamaIndex\" section of the OpenAI Cookbook?\n"
     ]
    }
   ],
   "source": [
    "question = '''What are the two critical areas of RAG system performance that are assessed \\\n",
    "in the \"Evaluating RAG with LlamaIndex\" section of the OpenAI Cookbook?'''\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3886cf3c-203f-43ab-a46c-2ebe536497b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 75.71it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = index.as_query_engine(use_async=True, response_mode=\"refine\")\n",
    "response = query_engine.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c39a82c5-61f4-406d-9128-8ae7d306fd83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The two critical areas of RAG system performance that are assessed in the \"Evaluating RAG with LlamaIndex\" section of the OpenAI Cookbook are:\n",
       "\n",
       "1. The Retrieval System\n",
       "2. Response Generation\n",
       "\n",
       "The context provided does not offer any additional information that would alter the original answer. The provided text discusses various guides, demos, and tutorials related to RAG (Retrieval-Augmented Generation) and its applications, but it does not provide information on the specific areas of RAG system performance that are assessed in the \"Evaluating RAG with LlamaIndex\" section. Therefore, the refined answer remains the same as the original one."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4501b38-be4d-42dc-bbda-f03db93263fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a2742526-5479-4dad-b533-1c5c04d33912': {'file_path': '/workspace/projects/LlamindexHelper/data/openai-cookbook-evaluating-rag-systems-fe393c61fb93.html',\n",
       "  'file_name': 'openai-cookbook-evaluating-rag-systems-fe393c61fb93.html',\n",
       "  'file_type': 'text/html',\n",
       "  'file_size': 2220,\n",
       "  'creation_date': '2024-07-21',\n",
       "  'last_modified_date': '2024-07-21'},\n",
       " '2b66388c-fb02-4509-92f6-1c6b0a69cf08': {'file_path': '/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-11.html',\n",
       "  'file_name': 'llamaindex-newsletter-2024-06-11.html',\n",
       "  'file_type': 'text/html',\n",
       "  'file_size': 11257,\n",
       "  'creation_date': '2024-07-21',\n",
       "  'last_modified_date': '2024-07-21'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5684b65-fcff-458f-a548-cd5f4cd1f95a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Combine DocumentSummaryIndex and VectorIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2511082-d27c-4afd-afe7-92165cb0d2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageContext(docstore=<llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore object at 0x7fe41e188a30>, index_store=<llama_index.core.storage.index_store.simple_index_store.SimpleIndexStore object at 0x7fe41e188730>, vector_stores={'default': QdrantVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=False, collection_name='llamaindex-blogs', url=None, api_key=None, batch_size=64, parallel=1, max_retries=3, client_kwargs={}, enable_hybrid=False, index_doc_id=True, fastembed_sparse_model=None), 'image': SimpleVectorStore(stores_text=False, is_embedding_query=True, data=SimpleVectorStoreData(embedding_dict={}, text_id_to_ref_doc_id={}, metadata_dict={}))}, graph_store=<llama_index.core.graph_stores.simple.SimpleGraphStore object at 0x7fe41e188190>, property_graph_store=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40492a41-f468-472a-b729-d1da8e67ff06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QdrantVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=False, collection_name='llamaindex-blogs', url=None, api_key=None, batch_size=64, parallel=1, max_retries=3, client_kwargs={}, enable_hybrid=False, index_doc_id=True, fastembed_sparse_model=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b31a674e-de44-4d57-9017-cf9db925ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load document index\n",
    "idex_mapping = storage_context.index_store.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "918863f8-5f2a-41c8-9753-b552f30729f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6089ce48-849d-4b4f-95e9-d84b8ac98b18'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.index_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a5bc5d-1f55-4797-ad16-8a27bc0d05d8",
   "metadata": {},
   "source": [
    "### Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "265eb550-6e62-4072-becb-adefe08ee1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core import QueryBundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "074abf23-6f5b-424f-8cd2-c29d69b4db52",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is latest LlamaIndex Newsletter?\"\n",
    "query_bundle = QueryBundle(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f959cb28-f8a1-4347-847d-92ce83508111",
   "metadata": {},
   "source": [
    "#### Normal Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd4e0b6f-4a04-41db-9638-68411d292fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.94it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(use_async=True, response_mode=\"refine\")\n",
    "response = query_engine.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8cde883d-b2a3-4114-b0d1-7d841ff912be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The latest LlamaIndex newsletter is the one from October 31, 2023."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3683413c-161e-4164-bc35-ad8c32bece2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7741100641613752 {'file_path': '/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-02-f349db8c1842.html', 'file_name': 'llamaindex-newsletter-2024-01-02-f349db8c1842.html', 'file_type': 'text/html', 'file_size': 17293, 'creation_date': '2024-07-21', 'last_modified_date': '2024-07-21'}\n",
      "0.7739661334334822 {'file_path': '/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-31-36244e2b3f0c.html', 'file_name': 'llamaindex-newsletter-2023-10-31-36244e2b3f0c.html', 'file_type': 'text/html', 'file_size': 11836, 'creation_date': '2024-07-21', 'last_modified_date': '2024-07-21'}\n"
     ]
    }
   ],
   "source": [
    "for node in response.source_nodes:\n",
    "    print(node.score, node.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcbffca-ea95-40e2-abfa-73c1b3276efc",
   "metadata": {},
   "source": [
    "\n",
    "#### Hybrid Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "534f9730-a54d-4504-be47-e296a85d6f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 162.52it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "hybrid_query_engine = index.as_query_engine(\n",
    "    use_async=True, \n",
    "    response_mode=\"refine\", \n",
    "    vector_store_query_mode=\"hybrid\",\n",
    "    similarity_top_k=2, sparse_top_k=12\n",
    ")\n",
    "hybrid_response = hybrid_query_engine.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e9bc0594-d800-497d-ac96-1f1f3f8e46c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The latest LlamaIndex Newsletter is the special edition for the last two weeks of 2023, which is packed with updates on the latest features, community demos, courses, insightful tutorials, guides, and webinars curated by LlamaIndex.\n",
       "\n",
       "However, based on the provided context, it seems that the latest newsletter is not the special edition for the last two weeks of 2023, but rather the one available on June 18, 2024. This newsletter includes updates on the following topics:\n",
       "\n",
       "- A tutorial by Arkiti on building a dynamic text-to-SQL solution using Llama 3 and GroqInc, highlighting the scalable and fast capabilities of SingleStoreDB Helios for multi-cloud deployments.\n",
       "- A tutorial by Kingzzm on Advanced RAG Patterns detailing effective strategies for handling documents with embedded tables, utilizing tools like LlamaParse and Nougat for enhanced QA performance.\n",
       "- A webinar on The Future of Web Agents with MultiOn, where Div Garg provided a full demo walkthrough and discussed the agentification of the internet.\n",
       "\n",
       "The newsletter is available at the following path: /workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-18.html."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(hybrid_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1c0ea55c-ca25-40ab-82c6-c321c3128a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 {'file_path': '/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-02-f349db8c1842.html', 'file_name': 'llamaindex-newsletter-2024-01-02-f349db8c1842.html', 'file_type': 'text/html', 'file_size': 17293, 'creation_date': '2024-07-21', 'last_modified_date': '2024-07-21'}\n",
      "0.5 {'file_path': '/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-18.html', 'file_name': 'llamaindex-newsletter-2024-06-18.html', 'file_type': 'text/html', 'file_size': 12216, 'creation_date': '2024-07-21', 'last_modified_date': '2024-07-21'}\n"
     ]
    }
   ],
   "source": [
    "for node in hybrid_response.source_nodes:\n",
    "    print(node.score, node.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d6c5f0-6af7-4adc-b4dd-478adb221f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-py310",
   "language": "python",
   "name": "llm-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
