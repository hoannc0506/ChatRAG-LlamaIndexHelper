{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac3ec62e-aeae-4737-9a03-d161fce5eea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c91eb40-3758-4ae4-894d-9d6cccb3eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c512ed12-3df7-4717-bd4f-fbeb2ba4b713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/dev/lib/python3.10/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.response.notebook_utils import display_response, display_metadata\n",
    "import model_utils, prompt_utils, db_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8df810e0-48a5-4199-88f3-03a33b4412fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageContext(docstore=<llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore object at 0x7fdc3cc31690>, index_store=<llama_index.core.storage.index_store.simple_index_store.SimpleIndexStore object at 0x7fdc3cc6b580>, vector_stores={'default': QdrantVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=False, collection_name='llamaindex-blogs', url=None, api_key=None, batch_size=64, parallel=1, max_retries=3, client_kwargs={}, enable_hybrid=False, index_doc_id=True, fastembed_sparse_model=None), 'image': SimpleVectorStore(stores_text=False, is_embedding_query=True, data=SimpleVectorStoreData(embedding_dict={}, text_id_to_ref_doc_id={}, metadata_dict={}))}, graph_store=<llama_index.core.graph_stores.simple.SimpleGraphStore object at 0x7fdc3cc6b640>, property_graph_store=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load vector index\n",
    "vector_store, storage_context = db_utils.load_qdrant_db(\n",
    "    local_path=\"./qdrant_db\",\n",
    "    coll_name=\"llamaindex-blogs\"\n",
    ")\n",
    "storage_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abc3d592-da73-4cd6-b0cb-07d845fa0074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: models/bge-small-en-v1.5/\n",
      "Load pretrained SentenceTransformer: models/bge-small-en-v1.5/\n",
      "Load pretrained SentenceTransformer: models/bge-small-en-v1.5/\n",
      "INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2 prompts are loaded, with the keys: ['query', 'text']\n"
     ]
    }
   ],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"models/bge-small-en-v1.5/\",\n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0005a230-fb4c-44f8-819d-5bd67d9ddd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    storage_context=storage_context,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1d7266c-2e19-4a81-9f8b-eef67322cd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model with quantization config from: models/Meta-Llama-3-8B-Instruct\n",
      "DEBUG:bitsandbytes.cextension:Loading bitsandbytes native library from: /opt/conda/envs/dev/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "Loading bitsandbytes native library from: /opt/conda/envs/dev/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "Loading bitsandbytes native library from: /opt/conda/envs/dev/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711c8b45928542d68f52f8b0f27f41a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load model\n",
    "model_name = \"models/Meta-Llama-3-8B-Instruct\"\n",
    "model, tokenizer = model_utils.load_quantized_model(\n",
    "    model_name_or_path=model_name,\n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "442ee4fa-2b4c-402b-897b-424e45523a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config llm and embed_model to llamaindex\n",
    "llm_hf = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=512,\n",
    "    query_wrapper_prompt=PromptTemplate(prompt_utils.get_llama3_prompt_template()),\n",
    "    generate_kwargs={\n",
    "        \"temperature\": 0.7,\n",
    "        \"do_sample\": True\n",
    "    },\n",
    "    device_map=\"cuda\",\n",
    "    model_name=model_name,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "Settings.llm = llm_hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82231a7b-efb9-4ec8-a3ca-901774afa45c",
   "metadata": {},
   "source": [
    "## Basic Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cf3681b-d653-49ac-8630-2fdec5993982",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"What are key features of llama-agents?\"\n",
    "question2 = '''What are the two critical areas of RAG system performance that are assessed \\\n",
    "in the \"Evaluating RAG with LlamaIndex\" section of the OpenAI Cookbook?'''\n",
    "question3 = '''What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be8595e8-e229-41ae-a651-c0b8a7c5c614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are key features of llama-agents?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6219eaafe14312944c7141115a33df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.core.indices.utils:> Top 2 nodes:\n",
      "> [Node ccb2800f-1560-404a-ae19-fd4e729ab440] [Similarity score:             0.763726] <div class=\"BlogPost_htmlPost__Z5oDL\">\n",
      " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
      "  W...\n",
      "> [Node 067e700a-84e7-482f-a7aa-d573faeb1e27] [Similarity score:             0.720235] \"\"\"</span>\n",
      "    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"The secret fac...\n",
      "> Top 2 nodes:\n",
      "> [Node ccb2800f-1560-404a-ae19-fd4e729ab440] [Similarity score:             0.763726] <div class=\"BlogPost_htmlPost__Z5oDL\">\n",
      " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
      "  W...\n",
      "> [Node 067e700a-84e7-482f-a7aa-d573faeb1e27] [Similarity score:             0.720235] \"\"\"</span>\n",
      "    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"The secret fac...\n",
      "> Top 2 nodes:\n",
      "> [Node ccb2800f-1560-404a-ae19-fd4e729ab440] [Similarity score:             0.763726] <div class=\"BlogPost_htmlPost__Z5oDL\">\n",
      " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
      "  W...\n",
      "> [Node 067e700a-84e7-482f-a7aa-d573faeb1e27] [Similarity score:             0.720235] \"\"\"</span>\n",
      "    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"The secret fac...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "print(\"Question:\", question1)\n",
    "query_engine = index.as_query_engine(use_async=True)\n",
    "response = query_engine.query(question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cba6c079-0c27-4950-be82-0bb9a2c71e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** According to the provided context information, the key features of llama-agents are:\n",
       "\n",
       "1. **Distributed Service Oriented Architecture**: every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\n",
       "2. **Communication via standardized API interfaces**: interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.\n",
       "3. **Define agentic and explicit orchestration flows**: developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an “agentic orchestrator” that decides which agents are relevant to the task.\n",
       "4. **Ease of deployment**: launch, scale and monitor each agent and your control plane independently.\n",
       "5. **Scalability and resource management**: use our built-in observability tools to monitor the quality and performance of the system and each individual agent service."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a39893f-a331-4028-9fa3-a100e233abb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ccb2800f-1560-404a-ae19-fd4e729ab440': {'file_path': '/workspace/projects/LlamindexHelper/data/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html',\n",
       "  'file_name': 'introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html',\n",
       "  'file_type': 'text/html',\n",
       "  'file_size': 18790,\n",
       "  'creation_date': '2024-07-21',\n",
       "  'last_modified_date': '2024-07-21'},\n",
       " '067e700a-84e7-482f-a7aa-d573faeb1e27': {'file_path': '/workspace/projects/LlamindexHelper/data/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html',\n",
       "  'file_name': 'introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html',\n",
       "  'file_type': 'text/html',\n",
       "  'file_size': 18790,\n",
       "  'creation_date': '2024-07-21',\n",
       "  'last_modified_date': '2024-07-21'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_metadata(response.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6771685b-1792-4a8c-88c0-2dc56bdcbd47",
   "metadata": {},
   "source": [
    "## Two-stage query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266fb7cb-f9f8-4c56-82ef-6bca1bee9e1c",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dc92b0a-97ca-45c5-9e10-11ff49a72d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core import QueryBundle\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fdf74ee-56cc-4278-b27d-ecc968220a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/dev/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "rerank_postprocessor = SentenceTransformerRerank(\n",
    "    model='models/mxbai-rerank-xsmall-v1',\n",
    "    top_n=2, # number of nodes after re-ranking,\n",
    "    keep_retrieval_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c42471-9a66-485d-9435-90f1a5d62a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 100)\n",
    "\n",
    "def get_retrieved_nodes(\n",
    "    query_str, vector_top_k=10, reranker_top_n=3, with_reranker=False\n",
    "):\n",
    "    query_bundle = QueryBundle(query_str)\n",
    "    # configure retriever\n",
    "    retriever = VectorIndexRetriever(\n",
    "        index=indreex,\n",
    "        similarity_top_k=vector_top_k,\n",
    "    )\n",
    "    retrieved_nodes = retriever.retrieve(query_bundle)\n",
    "\n",
    "    if with_reranker:\n",
    "        # configure reranker\n",
    "        reranker = LLMRerank(\n",
    "            choice_batch_size=5,\n",
    "            top_n=reranker_top_n,\n",
    "        )\n",
    "        retrieved_nodes = reranker.postprocess_nodes(\n",
    "            retrieved_nodes, query_bundle\n",
    "        )\n",
    "\n",
    "    return retrieved_nodes\n",
    "\n",
    "def pretty_print(df):\n",
    "    return display(HTML(df.to_html().replace(\"\\\\n\", \"<br>\")))\n",
    "\n",
    "\n",
    "def visualize_retrieved_nodes(nodes) -> None:\n",
    "    result_dicts = []\n",
    "    for node in nodes:\n",
    "        result_dict = {\"Score\": node.score, \"Text\": node.node.get_text()}\n",
    "        result_dicts.append(result_dict)\n",
    "\n",
    "    pretty_print(pd.DataFrame(result_dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34b6405b-3bc5-4a33-9184-c2fb0becff42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d320a1fbe5b4c7ba763362cc5c9a908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.core.indices.utils:> Top 3 nodes:\n",
      "> [Node ccb2800f-1560-404a-ae19-fd4e729ab440] [Similarity score:             0.763726] <div class=\"BlogPost_htmlPost__Z5oDL\">\n",
      " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
      "  W...\n",
      "> [Node 067e700a-84e7-482f-a7aa-d573faeb1e27] [Similarity score:             0.720235] \"\"\"</span>\n",
      "    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"The secret fac...\n",
      "> [Node 0544da4e-b27c-40ae-b51d-f576d370d26c] [Similarity score:             0.718015] <div class=\"BlogPost_htmlPost__Z5oDL\">\n",
      " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
      "  H...\n",
      "> Top 3 nodes:\n",
      "> [Node ccb2800f-1560-404a-ae19-fd4e729ab440] [Similarity score:             0.763726] <div class=\"BlogPost_htmlPost__Z5oDL\">\n",
      " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
      "  W...\n",
      "> [Node 067e700a-84e7-482f-a7aa-d573faeb1e27] [Similarity score:             0.720235] \"\"\"</span>\n",
      "    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"The secret fac...\n",
      "> [Node 0544da4e-b27c-40ae-b51d-f576d370d26c] [Similarity score:             0.718015] <div class=\"BlogPost_htmlPost__Z5oDL\">\n",
      " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
      "  H...\n",
      "> Top 3 nodes:\n",
      "> [Node ccb2800f-1560-404a-ae19-fd4e729ab440] [Similarity score:             0.763726] <div class=\"BlogPost_htmlPost__Z5oDL\">\n",
      " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
      "  W...\n",
      "> [Node 067e700a-84e7-482f-a7aa-d573faeb1e27] [Similarity score:             0.720235] \"\"\"</span>\n",
      "    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"The secret fac...\n",
      "> [Node 0544da4e-b27c-40ae-b51d-f576d370d26c] [Similarity score:             0.718015] <div class=\"BlogPost_htmlPost__Z5oDL\">\n",
      " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
      "  H...\n"
     ]
    }
   ],
   "source": [
    "new_nodes = get_retrieved_nodes(\n",
    "    question1,\n",
    "    vector_top_k=3,\n",
    "    with_reranker=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d221283-85b2-4f6e-95d3-7d10533f5b4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.763726</td>\n",
       "      <td>&lt;div class=\"BlogPost_htmlPost__Z5oDL\"&gt;<br> &lt;p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\"&gt;<br>  We're excited to announce the alpha release of<br>  &lt;code class=\"SanityPortableText_inlineCode__cI85z\"&gt;<br>   llama-agents<br>  &lt;/code&gt;<br>  , a new open-source framework designed to simplify the process of building, iterating, and deploying multi-agent AI systems and turn your agents into production microservices. Whether you're working on complex question-answering systems, collaborative AI assistants, or distributed AI workflows, llama-agents provides the tools and structure you need to bring your ideas to life.<br> &lt;/p&gt;<br> &lt;h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\"&gt;<br>  Key Features of llama-agents<br> &lt;/h2&gt;<br> &lt;ol&gt;<br>  &lt;li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\"&gt;<br>   &lt;strong&gt;<br>    Distributed Service Oriented Architecture:<br>   &lt;/strong&gt;<br>   every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.<br>  &lt;/li&gt;<br>  &lt;li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\"&gt;<br>   &lt;strong&gt;<br>    Communication via standardized API interfaces:<br>   &lt;/strong&gt;<br>   interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.<br>  &lt;/li&gt;<br>  &lt;li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\"&gt;<br>   &lt;strong&gt;<br>    Define agentic and explicit orchestration flows:<br>   &lt;/strong&gt;<br>   developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an “agentic orchestrator” that decides which agents are relevant to the task.<br>  &lt;/li&gt;<br>  &lt;li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\"&gt;<br>   &lt;strong&gt;<br>    Ease of deployment:<br>   &lt;/strong&gt;<br>   launch, scale and monitor each agent and your control plane independently.<br>  &lt;/li&gt;<br>  &lt;li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\"&gt;<br>   &lt;strong&gt;<br>    Scalability and resource management:<br>   &lt;/strong&gt;<br>   use our built-in observability tools to monitor the quality and performance of the system and each individual agent service<br>  &lt;/li&gt;<br> &lt;/ol&gt;<br> &lt;p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\"&gt;<br>  Let's dive into how you can start using llama-agents to build your own multi-agent systems.<br> &lt;/p&gt;<br> &lt;h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\"&gt;<br>  Getting Started with llama-agents<br> &lt;/h2&gt;<br> &lt;p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\"&gt;<br>  First, install the framework using pip:<br> &lt;/p&gt;<br> &lt;pre&gt;&lt;code&gt;pip install llama-agents llama-index-agent-openai&lt;/code&gt;&lt;/pre&gt;<br> &lt;h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\"&gt;<br>  Basic System Setup<br> &lt;/h3&gt;<br> &lt;p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\"&gt;<br>  Here's a simple example of how to set up a basic multi-agent system using llama-agents.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.720235</td>\n",
       "      <td>\"\"\"&lt;/span&gt;<br>    &lt;span class=\"hljs-keyword\"&gt;return&lt;/span&gt; &lt;span class=\"hljs-string\"&gt;\"The secret fact is: A baby llama is called a 'Cria'.\"&lt;/span&gt;<br><br>tool = FunctionTool.from_defaults(fn=get_the_secret_fact)<br><br>&lt;span class=\"hljs-comment\"&gt;# create our agents&lt;/span&gt;<br>worker1 = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())<br>worker2 = FunctionCallingAgentWorker.from_tools([], llm=OpenAI())<br>agent1 = worker1.as_agent()<br>agent2 = worker2.as_agent()&lt;/code&gt;&lt;/pre&gt;<br> &lt;p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\"&gt;<br>  We turn those agents into services:<br> &lt;/p&gt;<br> &lt;pre&gt;&lt;code&gt;agent_server_1 = AgentService(<br>    agent=agent1,<br>    message_queue=message_queue,<br>    description=&lt;span class=\"hljs-string\"&gt;\"Useful for getting the secret fact.\"&lt;/span&gt;,<br>    service_name=&lt;span class=\"hljs-string\"&gt;\"secret_fact_agent\"&lt;/span&gt;,<br>    host=&lt;span class=\"hljs-string\"&gt;\"localhost\"&lt;/span&gt;,<br>    port=&lt;span class=\"hljs-number\"&gt;8003&lt;/span&gt;<br>)<br>agent_server_2 = AgentService(<br>    agent=agent2,<br>    message_queue=message_queue,<br>    description=&lt;span class=\"hljs-string\"&gt;\"Useful for getting random dumb facts.\"&lt;/span&gt;,<br>    service_name=&lt;span class=\"hljs-string\"&gt;\"dumb_fact_agent\"&lt;/span&gt;,<br>    host=&lt;span class=\"hljs-string\"&gt;\"localhost\"&lt;/span&gt;,<br>    port=&lt;span class=\"hljs-number\"&gt;8004&lt;/span&gt;<br>)&lt;/code&gt;&lt;/pre&gt;<br> &lt;p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\"&gt;<br>  And finally we launch each service as an independent agent. Here we’re doing them all from a single script, but each of these could be a totally separate service, launched and scaled independently:<br> &lt;/p&gt;<br> &lt;pre&gt;&lt;code&gt;&lt;span class=\"hljs-keyword\"&gt;from&lt;/span&gt; llama_agents &lt;span class=\"hljs-keyword\"&gt;import&lt;/span&gt; ServerLauncher, CallableMessageConsumer<br><br>&lt;span class=\"hljs-comment\"&gt;# Additional human consumer&lt;/span&gt;<br>&lt;span class=\"hljs-keyword\"&gt;def&lt;/span&gt; &lt;span class=\"hljs-title function_\"&gt;handle_result&lt;/span&gt;(&lt;span class=\"hljs-params\"&gt;message&lt;/span&gt;) -&amp;gt; &lt;span class=\"hljs-literal\"&gt;None&lt;/span&gt;:<br>    &lt;span class=\"hljs-built_in\"&gt;print&lt;/span&gt;(&lt;span class=\"hljs-string\"&gt;f\"Got result:\"&lt;/span&gt;, message.data)<br><br><br>&lt;span class=\"hljs-comment\"&gt;# the final result is published to a \"human\" consumer&lt;/span&gt;<br>&lt;span class=\"hljs-comment\"&gt;# so we define one to handle it!&lt;/span&gt;<br>human_consumer = CallableMessageConsumer(<br>    handler=handle_result, message_type=&lt;span class=\"hljs-string\"&gt;\"human\"&lt;/span&gt;<br>)<br><br>&lt;span class=\"hljs-comment\"&gt;# Define Launcher&lt;/span&gt;<br>launcher = ServerLauncher(<br>    [agent_server_1, agent_server_2],<br>    control_plane,<br>    message_queue,<br>    additional_consumers=[human_consumer]<br>)<br><br>launcher.launch_servers()&lt;/code&gt;&lt;/pre&gt;<br> &lt;h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\"&gt;<br>  Real-time monitoring<br> &lt;/h2&gt;<br> &lt;p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\"&gt;<br>  One of the coolest debugging features of our multi-agent system is our agent monitor, which is built right in. You launch it like this:<br> &lt;/p&gt;<br> &lt;pre&gt;&lt;code&gt;llama-agents monitor --control-plane-url http://127.0.0.1:8000&lt;/code&gt;&lt;/pre&gt;<br> &lt;p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\"&gt;<br>  Once launched, you get an intuitive, point-and-click terminal application. You can see both of the agents running, and at the bottom you can inject a task like the query “What is the secret fact?” You’ll get a job ID which you can then click on to see your results:<br> &lt;/p&gt;<br> &lt;h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\"&gt;<br>  Building a Query Rewriting RAG System<br> &lt;/h2&gt;<br> &lt;p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\"&gt;<br>  Let's look at a more complex example: a Query Rewriting RAG system. This system will rewrite user queries to improve retrieval, then use the rewritten query to perform RAG over a document.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.718015</td>\n",
       "      <td>&lt;div class=\"BlogPost_htmlPost__Z5oDL\"&gt;<br> &lt;p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\"&gt;<br>  Hello, Llama enthusiasts! 🦙<br> &lt;/p&gt;<br> &lt;p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\"&gt;<br>  Welcome to this week’s edition of the LlamaIndex newsletter! In this issue, we’re excited to bring you exciting updates about<br>  &lt;code class=\"SanityPortableText_inlineCode__cI85z\"&gt;<br>   llama-agents<br>  &lt;/code&gt;<br>  , live demos, extensive guides, and in-depth tutorials to enhance your understanding of our tools.<br> &lt;/p&gt;<br> &lt;p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\"&gt;<br>  Before moving into our newsletter, we have an exciting update on our enterprise offerings. We are thrilled to announce the waitlist release of LlamaCloud, our fully-managed ingestion service.<br>  &lt;a class=\"SanityPortableText_link__QA4Ze\" href=\"http://bit.ly/llamacloud\" rel=\"noreferrer noopener\"&gt;<br>   Sign up<br>  &lt;/a&gt;<br>  now if you’re eager to collaborate and build LLM applications with LlamaCloud.<br> &lt;/p&gt;<br> &lt;h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\"&gt;<br>  🤩<br>  &lt;strong&gt;<br>   The highlights:<br>  &lt;/strong&gt;<br> &lt;/h2&gt;<br> &lt;ul&gt;<br>  &lt;li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\"&gt;<br>   &lt;strong&gt;<br>    Launched Llama-Agents Framework:<br>   &lt;/strong&gt;<br>   Our new alpha-release, llama-agents, enables multi-agent AI systems for production with a distributed architecture, HTTP API communication, and agentic orchestration. It’s designed for easy deployment, scalability, and observability.<br>   &lt;a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\" rel=\"noreferrer noopener\"&gt;<br>    Blogpost<br>   &lt;/a&gt;<br>   ,<br>   &lt;a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1806116419995844947\" rel=\"noreferrer noopener\"&gt;<br>    Tweet<br>   &lt;/a&gt;<br>   .<br>  &lt;/li&gt;<br>  &lt;li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\"&gt;<br>   &lt;strong&gt;<br>    &lt;code class=\"SanityPortableText_inlineCode__cI85z\"&gt;<br>     create-llama<br>    &lt;/code&gt;<br>    Integrated with LlamaCloud:<br>   &lt;/strong&gt;<br>   Streamline your LLM application data pipelines with create-llama, now integrated with LlamaCloud for faster setup and efficient system maintenance.<br>   &lt;a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/MarcusSchiesser/status/1806960577299767767\" rel=\"noreferrer noopener\"&gt;<br>    Tweet<br>   &lt;/a&gt;<br>   .<br>  &lt;/li&gt;<br> &lt;/ul&gt;<br> &lt;h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\"&gt;<br>  &lt;strong&gt;<br>   ✨ Feature Releases and Enhancements:<br>  &lt;/strong&gt;<br> &lt;/h2&gt;<br> &lt;ol&gt;<br>  &lt;li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\"&gt;<br>   We have launched llama-agents - new alpha-release framework that enables multi-agent AI systems to go into production. It features a distributed, service-oriented architecture, communication through standard HTTP APIs, agentic orchestration of flows, and is designed for easy deployment, scalability, and observability.<br>   &lt;a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\" rel=\"noreferrer noopener\"&gt;<br>    Blogpost<br>   &lt;/a&gt;<br>   ,<br>   &lt;a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1806116419995844947\" rel=\"noreferrer noopener\"&gt;<br>    Tweet<br>   &lt;/a&gt;<br>   .<br>  &lt;/li&gt;<br>  &lt;li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\"&gt;<br>   create-llama is now integrated with LlamaCloud to streamline the setup and management of data pipelines for LLM applications, providing a fast and efficient way to deploy and maintain these systems.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_retrieved_nodes(new_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46ee1fe9-27f9-4ecd-a88c-c349f276ab2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a470ffac8d246ac8f7f22415ffb02d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.core.indices.utils:> Top 10 nodes:\n",
      "> [Node ccb2800f-1560-404a-ae19-fd4e729ab440] [Similarity score:             0.763726] <div class=\"BlogPost_htmlPost__Z5oDL\">\n",
      " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
      "  W...\n",
      "> [Node 067e700a-84e7-482f-a7aa-d573faeb1e27] [Similarity score:             0.720235] \"\"\"</span>\n",
      "    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"The secret fac...\n",
      "> [Node 0544da4e-b27c-40ae-b51d-f576d370d26c] [Similarity score:             0.718015] <div class=\"BlogPost_htmlPost__Z5oDL\">\n",
      " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
      "  H...\n",
      "> [Node f481b7c0-d747-4981-a0d6-305f91ed89f9] [Similarity score:             0.717277] <div class=\"BlogPost_htmlPost__Z5oDL\">\n",
      " <p>\n",
      "  Agents are autonomous systems that can execute end-...\n",
      "> [Node 7237c7e9-ca0c-4222-b4d4-f807fb80a71f] [Similarity score:             0.714119] </li>\n",
      "  <li>\n",
      "   <strong>\n",
      "    New lower-level agent API:\n",
      "   </strong>\n",
      "   For enhanced transparency...\n",
      "> [Node ba9e5373-5bef-44e3-bdac-40b60c3c7e40] [Similarity score:             0.71394] </p>\n",
      " <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n",
      "  Getting Started with llama-agents\n",
      "...\n",
      "> [Node 6346b08b-1600-4973-96c4-46f12d3e4a1d] [Similarity score:             0.708287] launch_single(<span class=\"hljs-string\">\"Tell me about rabbits\"</span>)\n",
      "<span class=\"hljs-built_i...\n",
      "> [Node 2fc9cff9-79ac-432a-8cec-637e01ec91c5] [Similarity score:             0.706781] </li>\n",
      "  <li>\n",
      "   We have launched full-stack agent servers with a single CLI command using\n",
      "   <cod...\n",
      "> [Node 275bc773-58de-4cb7-a814-f75d9ec48a6a] [Similarity score:             0.696942] </p>\n",
      " <p>\n",
      "  LlamaIndex has evolved into a broad toolkit containing hundreds of integrations:\n",
      " </p...\n",
      "> [Node a1f25db5-97aa-4909-8105-36646a4e4a9b] [Similarity score:             0.69693] <div class=\"BlogPost_htmlPost__Z5oDL\">\n",
      " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
      "  H...\n",
      "> Top 10 nodes:\n",
      "> [Node ccb2800f-1560-404a-ae19-fd4e729ab440] [Similarity score:             0.763726] <div class=\"BlogPost_htmlPost__Z5oDL\">\n",
      " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
      "  W...\n",
      "> [Node 067e700a-84e7-482f-a7aa-d573faeb1e27] [Similarity score:             0.720235] \"\"\"</span>\n",
      "    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"The secret fac...\n",
      "> [Node 0544da4e-b27c-40ae-b51d-f576d370d26c] [Similarity score:             0.718015] <div class=\"BlogPost_htmlPost__Z5oDL\">\n",
      " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
      "  H...\n",
      "> [Node f481b7c0-d747-4981-a0d6-305f91ed89f9] [Similarity score:             0.717277] <div class=\"BlogPost_htmlPost__Z5oDL\">\n",
      " <p>\n",
      "  Agents are autonomous systems that can execute end-...\n",
      "> [Node 7237c7e9-ca0c-4222-b4d4-f807fb80a71f] [Similarity score:             0.714119] </li>\n",
      "  <li>\n",
      "   <strong>\n",
      "    New lower-level agent API:\n",
      "   </strong>\n",
      "   For enhanced transparency...\n",
      "> [Node ba9e5373-5bef-44e3-bdac-40b60c3c7e40] [Similarity score:             0.71394] </p>\n",
      " <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n",
      "  Getting Started with llama-agents\n",
      "...\n",
      "> [Node 6346b08b-1600-4973-96c4-46f12d3e4a1d] [Similarity score:             0.708287] launch_single(<span class=\"hljs-string\">\"Tell me about rabbits\"</span>)\n",
      "<span class=\"hljs-built_i...\n",
      "> [Node 2fc9cff9-79ac-432a-8cec-637e01ec91c5] [Similarity score:             0.706781] </li>\n",
      "  <li>\n",
      "   We have launched full-stack agent servers with a single CLI command using\n",
      "   <cod...\n",
      "> [Node 275bc773-58de-4cb7-a814-f75d9ec48a6a] [Similarity score:             0.696942] </p>\n",
      " <p>\n",
      "  LlamaIndex has evolved into a broad toolkit containing hundreds of integrations:\n",
      " </p...\n",
      "> [Node a1f25db5-97aa-4909-8105-36646a4e4a9b] [Similarity score:             0.69693] <div class=\"BlogPost_htmlPost__Z5oDL\">\n",
      " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
      "  H...\n",
      "> Top 10 nodes:\n",
      "> [Node ccb2800f-1560-404a-ae19-fd4e729ab440] [Similarity score:             0.763726] <div class=\"BlogPost_htmlPost__Z5oDL\">\n",
      " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
      "  W...\n",
      "> [Node 067e700a-84e7-482f-a7aa-d573faeb1e27] [Similarity score:             0.720235] \"\"\"</span>\n",
      "    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"The secret fac...\n",
      "> [Node 0544da4e-b27c-40ae-b51d-f576d370d26c] [Similarity score:             0.718015] <div class=\"BlogPost_htmlPost__Z5oDL\">\n",
      " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
      "  H...\n",
      "> [Node f481b7c0-d747-4981-a0d6-305f91ed89f9] [Similarity score:             0.717277] <div class=\"BlogPost_htmlPost__Z5oDL\">\n",
      " <p>\n",
      "  Agents are autonomous systems that can execute end-...\n",
      "> [Node 7237c7e9-ca0c-4222-b4d4-f807fb80a71f] [Similarity score:             0.714119] </li>\n",
      "  <li>\n",
      "   <strong>\n",
      "    New lower-level agent API:\n",
      "   </strong>\n",
      "   For enhanced transparency...\n",
      "> [Node ba9e5373-5bef-44e3-bdac-40b60c3c7e40] [Similarity score:             0.71394] </p>\n",
      " <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n",
      "  Getting Started with llama-agents\n",
      "...\n",
      "> [Node 6346b08b-1600-4973-96c4-46f12d3e4a1d] [Similarity score:             0.708287] launch_single(<span class=\"hljs-string\">\"Tell me about rabbits\"</span>)\n",
      "<span class=\"hljs-built_i...\n",
      "> [Node 2fc9cff9-79ac-432a-8cec-637e01ec91c5] [Similarity score:             0.706781] </li>\n",
      "  <li>\n",
      "   We have launched full-stack agent servers with a single CLI command using\n",
      "   <cod...\n",
      "> [Node 275bc773-58de-4cb7-a814-f75d9ec48a6a] [Similarity score:             0.696942] </p>\n",
      " <p>\n",
      "  LlamaIndex has evolved into a broad toolkit containing hundreds of integrations:\n",
      " </p...\n",
      "> [Node a1f25db5-97aa-4909-8105-36646a4e4a9b] [Similarity score:             0.69693] <div class=\"BlogPost_htmlPost__Z5oDL\">\n",
      " <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n",
      "  H...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m new_nodes \u001b[38;5;241m=\u001b[39m \u001b[43mget_retrieved_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvector_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreranker_top_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_reranker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m visualize_retrieved_nodes(new_nodes)\n",
      "Cell \u001b[0;32mIn[25], line 27\u001b[0m, in \u001b[0;36mget_retrieved_nodes\u001b[0;34m(query_str, vector_top_k, reranker_top_n, with_reranker)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_reranker:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# configure reranker\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     reranker \u001b[38;5;241m=\u001b[39m LLMRerank(\n\u001b[1;32m     24\u001b[0m         choice_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     25\u001b[0m         top_n\u001b[38;5;241m=\u001b[39mreranker_top_n,\n\u001b[1;32m     26\u001b[0m     )\n\u001b[0;32m---> 27\u001b[0m     retrieved_nodes \u001b[38;5;241m=\u001b[39m \u001b[43mreranker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretrieved_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_bundle\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retrieved_nodes\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.10/site-packages/llama_index/core/postprocessor/types.py:56\u001b[0m, in \u001b[0;36mBaseNodePostprocessor.postprocess_nodes\u001b[0;34m(self, nodes, query_bundle, query_str)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_postprocess_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    228\u001b[0m )\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.10/site-packages/llama_index/core/postprocessor/llm_rerank.py:99\u001b[0m, in \u001b[0;36mLLMRerank._postprocess_nodes\u001b[0;34m(self, nodes, query_bundle)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# call each batch independently\u001b[39;00m\n\u001b[1;32m     93\u001b[0m raw_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchoice_select_prompt,\n\u001b[1;32m     95\u001b[0m     context_str\u001b[38;5;241m=\u001b[39mfmt_batch_str,\n\u001b[1;32m     96\u001b[0m     query_str\u001b[38;5;241m=\u001b[39mquery_str,\n\u001b[1;32m     97\u001b[0m )\n\u001b[0;32m---> 99\u001b[0m raw_choices, relevances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_choice_select_answer_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnodes_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m choice_idxs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(choice) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m choice \u001b[38;5;129;01min\u001b[39;00m raw_choices]\n\u001b[1;32m    103\u001b[0m choice_nodes \u001b[38;5;241m=\u001b[39m [nodes_batch[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m choice_idxs]\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.10/site-packages/llama_index/core/indices/utils.py:104\u001b[0m, in \u001b[0;36mdefault_parse_choice_select_answer_fn\u001b[0;34m(answer, num_choices, raise_error)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    100\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid answer line: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer_line\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer line must be of the form: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer_num: <int>, answer_relevance: <float>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    103\u001b[0m         )\n\u001b[0;32m--> 104\u001b[0m answer_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mline_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer_num \u001b[38;5;241m>\u001b[39m num_choices:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "new_nodes = get_retrieved_nodes(\n",
    "    question1,\n",
    "    vector_top_k=10,\n",
    "    reranker_top_n=3,\n",
    "    with_reranker=True,\n",
    ")\n",
    "visualize_retrieved_nodes(new_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d370361-083d-45c1-97e0-8b26588bd918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-py310",
   "language": "python",
   "name": "llm-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
