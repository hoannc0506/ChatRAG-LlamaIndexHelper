{"docstore/data": {"832e7ccd-c4f6-4c41-8eec-b8aa8fa9b599": {"__data__": {"id_": "832e7ccd-c4f6-4c41-8eec-b8aa8fa9b599", "embedding": null, "metadata": {"Header_1": " Introduction", "filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc7a0042-7c68-4dfc-a9b5-9af172e63a70", "node_type": "4", "metadata": {"filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "51210ae3bef64a717cf6af26f81baec17f059a2f72450316fa6c6612126bc69d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f7a35bb-585b-4e70-a5f2-5f84a6c5239b", "node_type": "1", "metadata": {"Header_1": " Features"}, "hash": "86d3d2a531d483196b73bf4a1d8f04d4afcf76432aefa401f0de3e1d96e510de", "class_name": "RelatedNodeInfo"}}, "text": "Introduction\n\nThe C3 Voice Assistant is my latest project aimed at making Large Language\nModel (LLM) and Retrieval-Augmented Generation (RAG) applications more\naccessible. This voice-activated assistant caters to a broad audience,\nincluding those facing typing challenges or accessibility issues.", "mimetype": "text/plain", "start_char_idx": 3, "end_char_idx": 299, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f7a35bb-585b-4e70-a5f2-5f84a6c5239b": {"__data__": {"id_": "8f7a35bb-585b-4e70-a5f2-5f84a6c5239b", "embedding": null, "metadata": {"Header_1": " Features", "filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc7a0042-7c68-4dfc-a9b5-9af172e63a70", "node_type": "4", "metadata": {"filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "51210ae3bef64a717cf6af26f81baec17f059a2f72450316fa6c6612126bc69d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "832e7ccd-c4f6-4c41-8eec-b8aa8fa9b599", "node_type": "1", "metadata": {"Header_1": " Introduction", "filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "5b0568fb4578e914850d755c66b5a21fbc3a154845b324f408fd1616cf1326d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6bf48242-04f1-4c9c-87d3-8c348b66b843", "node_type": "1", "metadata": {"Header_1": " The Tech Stack"}, "hash": "428c81a1d3300edacd1a3d7f12b33c56a3b44277730bf88de71acf7da66fc08d", "class_name": "RelatedNodeInfo"}}, "text": "Features\n\n  * **Voice Activation:** Initiated by saying \u201cC3.\u201d Alternatively, users can click the blue ring to activate the listening mode of the app. The wake word \u201cC3\u201d is configurable and you can choose any other word. \n  * **Universal Accessibility:** Ideal for users preferring voice commands or facing typing challenges. \n  * **LLM Integration:** Capable of general queries and document-specific inquiries (e.g., Nvidia\u2019s FY 2023 10K report). \n  * **User-Friendly Interface:** The interface of the AI voice assistant is designed for simplicity and ease of use, focusing on voice chat interactions. It features a minimalistic and user-friendly React.js layout. Additionally, there is a convenient sidebar that displays the entire chat history in text format, allowing users to review and reflect on their interactions with the AI.", "mimetype": "text/plain", "start_char_idx": 304, "end_char_idx": 1137, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6bf48242-04f1-4c9c-87d3-8c348b66b843": {"__data__": {"id_": "6bf48242-04f1-4c9c-87d3-8c348b66b843", "embedding": null, "metadata": {"Header_1": " The Tech Stack", "filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc7a0042-7c68-4dfc-a9b5-9af172e63a70", "node_type": "4", "metadata": {"filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "51210ae3bef64a717cf6af26f81baec17f059a2f72450316fa6c6612126bc69d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f7a35bb-585b-4e70-a5f2-5f84a6c5239b", "node_type": "1", "metadata": {"Header_1": " Features", "filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "53760df8cce10796767d7a8ab72f36380ac0dff673b667423d6f6633c81734f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a304de9-03eb-4094-85d5-0ec2da711b09", "node_type": "1", "metadata": {"Header_1": " Building the Frontend"}, "hash": "509c72a767f7283b03989e177bc7efd9e0d3b1b34570837cfe70a774269a5851", "class_name": "RelatedNodeInfo"}}, "text": "The Tech Stack\n\nThe app is built on a robust and flexible tech stack that ensures a smooth,\nreliable, and efficient user experience. Here\u2019s an overview:\n\n  * **Frontend:** The user interface is a custom application developed using React.js. It\u2019s designed to be minimalistic yet highly functional, prioritizing ease of use and accessibility. \n  * **Backend:** The server-side operations are powered by Python Flask. I\u2019ve utilized the innovative \u2018create-llama\u2019 feature from LlamaIndex, which significantly streamlines the development process. \n  * **Hosting:** For a seamless performance, the frontend of the C3 Voice Assistant is hosted on Vercel. The backend, on the other hand, is deployed on Render, ensuring efficient management and operation of server-side tasks.", "mimetype": "text/plain", "start_char_idx": 1143, "end_char_idx": 1910, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a304de9-03eb-4094-85d5-0ec2da711b09": {"__data__": {"id_": "4a304de9-03eb-4094-85d5-0ec2da711b09", "embedding": null, "metadata": {"Header_1": " Building the Frontend", "filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc7a0042-7c68-4dfc-a9b5-9af172e63a70", "node_type": "4", "metadata": {"filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "51210ae3bef64a717cf6af26f81baec17f059a2f72450316fa6c6612126bc69d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6bf48242-04f1-4c9c-87d3-8c348b66b843", "node_type": "1", "metadata": {"Header_1": " The Tech Stack", "filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "9f9efb97a768eee0811672793b4f3adb86ee7284502db22d54cadae3de07fa8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3479674b-4de3-42d5-b398-88cebd8635ce", "node_type": "1", "metadata": {"Header_1": " Building the Frontend", "Header_2": " 1\\. Component and State Initialization"}, "hash": "a201d6b85eb1cd2a75364842dc04d1fbbaf23f049d944912038182b2af779667", "class_name": "RelatedNodeInfo"}}, "text": "Building the Frontend\n\nThe frontend, built with React.js, focuses on user interaction and\naccessibility. The ` App.js ` script incorporates features like wake word\nrecognition, speech-to-text conversion, state management, and dynamic UI\nelements like speech bubbles and spinners.", "mimetype": "text/plain", "start_char_idx": 1916, "end_char_idx": 2195, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3479674b-4de3-42d5-b398-88cebd8635ce": {"__data__": {"id_": "3479674b-4de3-42d5-b398-88cebd8635ce", "embedding": null, "metadata": {"Header_1": " Building the Frontend", "Header_2": " 1\\. Component and State Initialization", "filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc7a0042-7c68-4dfc-a9b5-9af172e63a70", "node_type": "4", "metadata": {"filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "51210ae3bef64a717cf6af26f81baec17f059a2f72450316fa6c6612126bc69d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a304de9-03eb-4094-85d5-0ec2da711b09", "node_type": "1", "metadata": {"Header_1": " Building the Frontend", "filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "3fa4d445d394c7b5d8c7521a842cf6a37e13c628f6e79c00be8657a4a3e16b33", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3fe2ec90-a336-4799-8bae-0a5a378109bd", "node_type": "1", "metadata": {"Header_1": " Building the Frontend", "Header_2": " 2\\. Speech Recognition Setup"}, "hash": "765b333f6657e4587a51ba9ea90b3010791a8e2a64792d40d8a9cbc3f032c826", "class_name": "RelatedNodeInfo"}}, "text": "1\\. Component and State Initialization\n\nThis section sets up the React component and initializes various states, such\nas ` appState ` to track the current state of the app (idle, listening,\nspeaking), and ` transcript ` to store the text transcribed from user speech.\n\n    \n    \n    import React, { useState, useRef, useEffect } from \"react\";\n    import \"./App.css\";\n    \n    const App = () =&gt; {\n      const [appState, setAppState] = useState(\"idle\");\n      const [transcript, setTranscript] = useState(\"\");\n      // Additional state and ref declarations...\n    };", "mimetype": "text/plain", "start_char_idx": 2201, "end_char_idx": 2768, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3fe2ec90-a336-4799-8bae-0a5a378109bd": {"__data__": {"id_": "3fe2ec90-a336-4799-8bae-0a5a378109bd", "embedding": null, "metadata": {"Header_1": " Building the Frontend", "Header_2": " 2\\. Speech Recognition Setup", "filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc7a0042-7c68-4dfc-a9b5-9af172e63a70", "node_type": "4", "metadata": {"filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "51210ae3bef64a717cf6af26f81baec17f059a2f72450316fa6c6612126bc69d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3479674b-4de3-42d5-b398-88cebd8635ce", "node_type": "1", "metadata": {"Header_1": " Building the Frontend", "Header_2": " 1\\. Component and State Initialization", "filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "e5bc0af482d4638698dbc7815f321a04f996155021b7d6fd85717a2031d11dcc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de702dea-ba93-4d63-bef8-d32e9c09c251", "node_type": "1", "metadata": {"Header_1": " Building the Frontend", "Header_2": " 3\\. Handling User Speech and Response"}, "hash": "dee4dc55514fbb0e66a3790dd04234cf47d567234e3da6833b84f923fd6624d1", "class_name": "RelatedNodeInfo"}}, "text": "2\\. Speech Recognition Setup\n\nIn this useEffect hook, two speech recognition instances are initialized: one\nfor detecting the wake word \u201cC3\u201d and another for the main speech recognition.\nThis setup ensures that the app starts listening for commands when \u201cC3\u201d is\nmentioned.\n\nYou can easily swap \u201cC3\u201d with any other wake word of your choice.\n\n    \n    \n      useEffect(() =&gt; {\n        // Wake word listener setup\n        const WakeWordSpeechRecognition =\n          window.SpeechRecognition || window.webkitSpeechRecognition;\n        if (WakeWordSpeechRecognition &amp;&amp; !wakeWordRecognitionRef.current) {\n          wakeWordRecognitionRef.current = new WakeWordSpeechRecognition();\n          wakeWordRecognitionRef.current.continuous = true;\n          wakeWordRecognitionRef.current.interimResults = false;\n    \n          wakeWordRecognitionRef.current.onresult = (event) =&gt; {\n            const transcript = event.results[event.results.length - 1][0].transcript\n              .trim()\n              .toLowerCase();\n            if (transcript.includes(\"c3\")) {\n              toggleRecording(); // Start the main speech recognition process\n            }\n          };\n    \n          wakeWordRecognitionRef.current.start();\n        }\n    \n        // Main speech recognition setup\n        const SpeechRecognition =\n          window.SpeechRecognition || window.webkitSpeechRecognition;\n        if (SpeechRecognition &amp;&amp; !recognitionRef.current) {\n          recognitionRef.current = new SpeechRecognition();\n          recognitionRef.current.continuous = false;\n          recognitionRef.current.interimResults = false;\n    \n          recognitionRef.current.onresult = (event) =&gt; {\n            const lastResultIndex = event.results.length - 1;\n            const transcriptResult = event.results[lastResultIndex][0].transcript;\n            setTranscript(transcriptResult);\n            setAppState(\"playing\");\n            setShowSpeechBubble(true);\n            setTimeout(() =&gt; setShowSpeechBubble(false), speechBubbleTimeout);\n            fetchResponseFromLLM(transcriptResult);\n          };\n    \n          recognitionRef.current.onend = () =&gt; {\n            setShowSpinner(true);\n          };\n        }\n      }, []);", "mimetype": "text/plain", "start_char_idx": 2774, "end_char_idx": 5001, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de702dea-ba93-4d63-bef8-d32e9c09c251": {"__data__": {"id_": "de702dea-ba93-4d63-bef8-d32e9c09c251", "embedding": null, "metadata": {"Header_1": " Building the Frontend", "Header_2": " 3\\. Handling User Speech and Response", "filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc7a0042-7c68-4dfc-a9b5-9af172e63a70", "node_type": "4", "metadata": {"filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "51210ae3bef64a717cf6af26f81baec17f059a2f72450316fa6c6612126bc69d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3fe2ec90-a336-4799-8bae-0a5a378109bd", "node_type": "1", "metadata": {"Header_1": " Building the Frontend", "Header_2": " 2\\. Speech Recognition Setup", "filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "7f3b73381eb16be424211f918807136ff503789e33a6df8ee971679c631467b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0dff0803-7d8a-4bf2-b4d1-24812ad4ea52", "node_type": "1", "metadata": {"Header_1": " Building the Frontend", "Header_2": " 4\\. Speech Synthesis"}, "hash": "b71f17972ed446c78b7b30b6d371ec6569ebebaa63448f9a3461df21b4fba880", "class_name": "RelatedNodeInfo"}}, "text": "3\\. Handling User Speech and Response\n\n` toggleRecording ` controls the speech recognition process, while `\nfetchResponseFromLLM ` sends the user's speech to the LLM backend and handles\nthe response. This response is then spoken out via speech synthesis and also\nused to update the chat history displayed on the UI.\n\n    \n    \n     const toggleRecording = () =&gt; {\n        try {\n          if (appState === \"idle\") {\n            recognitionRef.current.start();\n            setAppState(\"listening\");\n          } else if (appState === \"listening\") {\n            recognitionRef.current.stop();\n          }\n        } catch (error) {\n        }\n      };\n    \n    \n      const fetchResponseFromLLM = async (text) =&gt; {\n        try {\n          const response = await fetch(\n            `https://c3-python-nostream.onrender.com/api/chat`,\n            {\n              method: \"POST\",\n              headers: { \"Content-Type\": \"application/json\" },\n              body: JSON.stringify({\n                messages: [\n                  {\n                    role: \"user\",\n                    content:\n                      \"You are an AI voice assistant called C3. You can provide any general information as well as answer basic questions about the Nvidia 10k report for year ended Jan 2023\" +\n                      text,\n                  },\n                ],\n              }),\n            }\n          );\n          const data = await response.json();\n    \n          setChatHistory((prevHistory) =&gt; [\n            ...prevHistory,\n            { query: text, response: data.result.content },\n          ]);\n          speak(data.result.content);\n        } catch (error) {\n          console.error(\"Error communicating with LLM:\", error);\n        }\n      };", "mimetype": "text/plain", "start_char_idx": 5007, "end_char_idx": 6748, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0dff0803-7d8a-4bf2-b4d1-24812ad4ea52": {"__data__": {"id_": "0dff0803-7d8a-4bf2-b4d1-24812ad4ea52", "embedding": null, "metadata": {"Header_1": " Building the Frontend", "Header_2": " 4\\. Speech Synthesis", "filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc7a0042-7c68-4dfc-a9b5-9af172e63a70", "node_type": "4", "metadata": {"filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "51210ae3bef64a717cf6af26f81baec17f059a2f72450316fa6c6612126bc69d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de702dea-ba93-4d63-bef8-d32e9c09c251", "node_type": "1", "metadata": {"Header_1": " Building the Frontend", "Header_2": " 3\\. Handling User Speech and Response", "filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "7e6ac0c9520cc0717c31ff218c23888d5d8e991fea0978806298a73df52597a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "261f5227-5018-4bc3-8636-8dd4a1a2166c", "node_type": "1", "metadata": {"Header_1": " Building the Frontend", "Header_2": " 5\\. UI Rendering"}, "hash": "e86a91c6c8869b47ed891dd0f5b5d513c7c4864eb7e278aa4ba4f7cecb2d87a3", "class_name": "RelatedNodeInfo"}}, "text": "4\\. Speech Synthesis\n\nThe ` speak ` function takes the text response from the LLM and uses the\nSpeechSynthesis API to read it aloud, providing an interactive experience for\nthe user.\n\n    \n    \n      const speak = (text) =&gt; {\n        if (synthRef.current &amp;&amp; text) {\n          const utterance = new SpeechSynthesisUtterance(text);\n    \n          const voices = window.speechSynthesis.getVoices();\n          if (voices.length &gt; 0) {\n            utterance.voice = voices[3]; // You can change this to select different voices\n          }\n    \n          utterance.onstart = () =&gt; {\n            console.log(\"TTS starts speaking\");\n            setShowSpinner(false);\n          };\n    \n          utterance.onend = () =&gt; {\n            setAppState(\"idle\");\n            if (wakeWordRecognitionRef.current) {\n              wakeWordRecognitionRef.current.start(); // Restart wake word listener after speaking\n            }\n          };\n          synthRef.current.speak(utterance);\n        }", "mimetype": "text/plain", "start_char_idx": 6754, "end_char_idx": 7751, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "261f5227-5018-4bc3-8636-8dd4a1a2166c": {"__data__": {"id_": "261f5227-5018-4bc3-8636-8dd4a1a2166c", "embedding": null, "metadata": {"Header_1": " Building the Frontend", "Header_2": " 5\\. UI Rendering", "filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc7a0042-7c68-4dfc-a9b5-9af172e63a70", "node_type": "4", "metadata": {"filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "51210ae3bef64a717cf6af26f81baec17f059a2f72450316fa6c6612126bc69d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0dff0803-7d8a-4bf2-b4d1-24812ad4ea52", "node_type": "1", "metadata": {"Header_1": " Building the Frontend", "Header_2": " 4\\. Speech Synthesis", "filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "dc68cf4c0bacb5fe58ca6570ff17bf59eac2e171a50cd2562a0ee694b2823f41", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0fb3997c-39aa-4b8f-967f-8e1d333ed49b", "node_type": "1", "metadata": {"Header_1": " Backend Server Setup"}, "hash": "287e393e467f9ab6a16ace1557239608708e85e49bf8ebbf2b8a69be8d333bbb", "class_name": "RelatedNodeInfo"}}, "text": "5\\. UI Rendering\n\nThe return statement of the ` App ` function contains the JSX code for\nrendering the app's UI. This includes buttons for starting/stopping the voice\ninteraction, a display area for the transcript, and a chat sidebar showing the\nhistory of interactions.\n\nBy combining voice recognition, LLM integration, and speech synthesis, this\nfrontend component provides a comprehensive and accessible interface for\ninteracting with the C3 Voice Assistant.", "mimetype": "text/plain", "start_char_idx": 7757, "end_char_idx": 8218, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0fb3997c-39aa-4b8f-967f-8e1d333ed49b": {"__data__": {"id_": "0fb3997c-39aa-4b8f-967f-8e1d333ed49b", "embedding": null, "metadata": {"Header_1": " Backend Server Setup", "filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc7a0042-7c68-4dfc-a9b5-9af172e63a70", "node_type": "4", "metadata": {"filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "51210ae3bef64a717cf6af26f81baec17f059a2f72450316fa6c6612126bc69d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "261f5227-5018-4bc3-8636-8dd4a1a2166c", "node_type": "1", "metadata": {"Header_1": " Building the Frontend", "Header_2": " 5\\. UI Rendering", "filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "e885e5d702c1a0b1963a5ee8f9926f308bd163b291ae5df85b207c3740fe7eac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e23fea9a-f5a4-4d8d-bfa9-2f6c74089e74", "node_type": "1", "metadata": {"Header_1": " Integration"}, "hash": "4a809c3320e31bbe00cd8c185d473908ca26cc019e7bc55fe872896663d76931", "class_name": "RelatedNodeInfo"}}, "text": "Backend Server Setup\n\n  1. Initialize Create-Llama: Run ` npx create-llama@latest ` in your terminal. \n  2. Follow the prompts to set up a Python FastAPI backend, which we can be integrated with our frontend. \n  3. Use ` poetry install ` and ` poetry shell ` to prepare the environment. \n  4. Create a ` .env ` file with ` OPENAI_API_KEY=<openai_api_key> ` . \n  5. Generate Embeddings (optional): If a ` ./data ` directory exists, run ` python app/engine/generate.py ` . \n  6. Execute ` python main.py ` to start the server. \n  7. Test the API: Use ` curl --location 'localhost:8000/api/chat' --header 'Content-Type: application/json' --data '{ \"messages\": [{ \"role\": \"user\", \"content\": \"Hello\" }] }' ` to test. \n  8. Modify API behavior in ` app/api/routers/chat.py ` . The server supports CORS for all origins, alterable with the ` ENVIRONMENT=prod ` setting.", "mimetype": "text/plain", "start_char_idx": 8223, "end_char_idx": 9084, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e23fea9a-f5a4-4d8d-bfa9-2f6c74089e74": {"__data__": {"id_": "e23fea9a-f5a4-4d8d-bfa9-2f6c74089e74", "embedding": null, "metadata": {"Header_1": " Integration", "filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc7a0042-7c68-4dfc-a9b5-9af172e63a70", "node_type": "4", "metadata": {"filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "51210ae3bef64a717cf6af26f81baec17f059a2f72450316fa6c6612126bc69d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0fb3997c-39aa-4b8f-967f-8e1d333ed49b", "node_type": "1", "metadata": {"Header_1": " Backend Server Setup", "filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "f5a7ffb34951edf35fe9a38149d086ad5befc5c37ec35c1a7166457a0d56cbc0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8994e8d-d8c9-496c-808b-1042aaaad23f", "node_type": "1", "metadata": {"Header_1": " Final Thoughts"}, "hash": "a8217cbd7912902a8e5808a31e93ffed1fadb20f51091e320211e22dcfadd1dd", "class_name": "RelatedNodeInfo"}}, "text": "Integration\n\nOnce the backend server is set up, integrating it with the frontend is\nstraightforward. Simply update the ` fetchResponseFromLLM ` function in your\nfrontend's ` App.js ` to call the backend server URL. This change ensures that\nwhen the frontend makes a request, it communicates with your newly configured\nbackend, thus effectively integrating the two components.", "mimetype": "text/plain", "start_char_idx": 9090, "end_char_idx": 9465, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8994e8d-d8c9-496c-808b-1042aaaad23f": {"__data__": {"id_": "a8994e8d-d8c9-496c-808b-1042aaaad23f", "embedding": null, "metadata": {"Header_1": " Final Thoughts", "filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc7a0042-7c68-4dfc-a9b5-9af172e63a70", "node_type": "4", "metadata": {"filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "51210ae3bef64a717cf6af26f81baec17f059a2f72450316fa6c6612126bc69d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e23fea9a-f5a4-4d8d-bfa9-2f6c74089e74", "node_type": "1", "metadata": {"Header_1": " Integration", "filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "1ccf572cc79d6c3e18323c9c08df155835ac476fac298fb00b10c446e7a9f34f", "class_name": "RelatedNodeInfo"}}, "text": "Final Thoughts\n\nWrapping up, the C3 Voice Assistant isn\u2019t just a tech showcase; it\u2019s a stride\ntowards democratizing AI. It\u2019s about making powerful AI tools, like LLMs and\nRAG, accessible and user-friendly. This project is more than lines of code \u2014\nit\u2019s a push to break down tech barriers and empower everyone.\n\nYour thoughts and feedback are invaluable \u2014 let\u2019s make AI more accessible\ntogether!\n\nLink to Github Repo: [ Frontend ](https://github.com/AI-ANK/C3-Voice-\nAssistant-UI) and [ Backend ](https://github.com/AI-ANK/c3-python-nostream)\n\n[ Connect with Me on LinkedIn\n](https://www.linkedin.com/in/harshadsuryawanshi/)\n\n[ Linkedin Post ](https://www.linkedin.com/posts/harshadsuryawanshi_ai-\nllamaindex-\ngpt3-activity-7149796976442740736-1lXj?utm_source=share&utm_medium=member_desktop)", "mimetype": "text/plain", "start_char_idx": 9470, "end_char_idx": 10261, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"832e7ccd-c4f6-4c41-8eec-b8aa8fa9b599": {"doc_hash": "5b0568fb4578e914850d755c66b5a21fbc3a154845b324f408fd1616cf1326d1", "ref_doc_id": "dc7a0042-7c68-4dfc-a9b5-9af172e63a70"}, "8f7a35bb-585b-4e70-a5f2-5f84a6c5239b": {"doc_hash": "53760df8cce10796767d7a8ab72f36380ac0dff673b667423d6f6633c81734f3", "ref_doc_id": "dc7a0042-7c68-4dfc-a9b5-9af172e63a70"}, "6bf48242-04f1-4c9c-87d3-8c348b66b843": {"doc_hash": "9f9efb97a768eee0811672793b4f3adb86ee7284502db22d54cadae3de07fa8f", "ref_doc_id": "dc7a0042-7c68-4dfc-a9b5-9af172e63a70"}, "4a304de9-03eb-4094-85d5-0ec2da711b09": {"doc_hash": "3fa4d445d394c7b5d8c7521a842cf6a37e13c628f6e79c00be8657a4a3e16b33", "ref_doc_id": "dc7a0042-7c68-4dfc-a9b5-9af172e63a70"}, "3479674b-4de3-42d5-b398-88cebd8635ce": {"doc_hash": "e5bc0af482d4638698dbc7815f321a04f996155021b7d6fd85717a2031d11dcc", "ref_doc_id": "dc7a0042-7c68-4dfc-a9b5-9af172e63a70"}, "3fe2ec90-a336-4799-8bae-0a5a378109bd": {"doc_hash": "7f3b73381eb16be424211f918807136ff503789e33a6df8ee971679c631467b4", "ref_doc_id": "dc7a0042-7c68-4dfc-a9b5-9af172e63a70"}, "de702dea-ba93-4d63-bef8-d32e9c09c251": {"doc_hash": "7e6ac0c9520cc0717c31ff218c23888d5d8e991fea0978806298a73df52597a8", "ref_doc_id": "dc7a0042-7c68-4dfc-a9b5-9af172e63a70"}, "0dff0803-7d8a-4bf2-b4d1-24812ad4ea52": {"doc_hash": "dc68cf4c0bacb5fe58ca6570ff17bf59eac2e171a50cd2562a0ee694b2823f41", "ref_doc_id": "dc7a0042-7c68-4dfc-a9b5-9af172e63a70"}, "261f5227-5018-4bc3-8636-8dd4a1a2166c": {"doc_hash": "e885e5d702c1a0b1963a5ee8f9926f308bd163b291ae5df85b207c3740fe7eac", "ref_doc_id": "dc7a0042-7c68-4dfc-a9b5-9af172e63a70"}, "0fb3997c-39aa-4b8f-967f-8e1d333ed49b": {"doc_hash": "f5a7ffb34951edf35fe9a38149d086ad5befc5c37ec35c1a7166457a0d56cbc0", "ref_doc_id": "dc7a0042-7c68-4dfc-a9b5-9af172e63a70"}, "e23fea9a-f5a4-4d8d-bfa9-2f6c74089e74": {"doc_hash": "1ccf572cc79d6c3e18323c9c08df155835ac476fac298fb00b10c446e7a9f34f", "ref_doc_id": "dc7a0042-7c68-4dfc-a9b5-9af172e63a70"}, "a8994e8d-d8c9-496c-808b-1042aaaad23f": {"doc_hash": "d666953c4dc5bee53826148483a9c32c5494b9e20ba5dca5d2c0563c8d4da184", "ref_doc_id": "dc7a0042-7c68-4dfc-a9b5-9af172e63a70"}}, "docstore/ref_doc_info": {"dc7a0042-7c68-4dfc-a9b5-9af172e63a70": {"node_ids": ["832e7ccd-c4f6-4c41-8eec-b8aa8fa9b599", "8f7a35bb-585b-4e70-a5f2-5f84a6c5239b", "6bf48242-04f1-4c9c-87d3-8c348b66b843", "4a304de9-03eb-4094-85d5-0ec2da711b09", "3479674b-4de3-42d5-b398-88cebd8635ce", "3fe2ec90-a336-4799-8bae-0a5a378109bd", "de702dea-ba93-4d63-bef8-d32e9c09c251", "0dff0803-7d8a-4bf2-b4d1-24812ad4ea52", "261f5227-5018-4bc3-8636-8dd4a1a2166c", "0fb3997c-39aa-4b8f-967f-8e1d333ed49b", "e23fea9a-f5a4-4d8d-bfa9-2f6c74089e74", "a8994e8d-d8c9-496c-808b-1042aaaad23f"], "metadata": {"Header_1": " Introduction", "filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}}}}