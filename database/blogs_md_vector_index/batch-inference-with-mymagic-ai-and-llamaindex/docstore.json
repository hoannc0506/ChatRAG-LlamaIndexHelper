{"docstore/data": {"91a67b0c-d6a4-4ae1-9e5d-f1fe93b23d66": {"__data__": {"id_": "91a67b0c-d6a4-4ae1-9e5d-f1fe93b23d66", "embedding": null, "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3fc617da-f983-4ea8-8a49-b8ff6c3f14d3", "node_type": "4", "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "9fa6c05cb14bac9d0e24f5480a646a38f7ad05cdc4f0df72b5ae1195bf9f11a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7aa4f7ce-4b00-460a-802c-941942010fdd", "node_type": "1", "metadata": {"Header_2": " Part 1: batch inference"}, "hash": "56a0c0edb4011ca38827ce0f26c28143a4156c449eb204838146011eefbfde74", "class_name": "RelatedNodeInfo"}}, "text": "_This is a guest post from MyMagic AI._\n\n[ MyMagic AI ](https://mymagic.ai/) allows processing and analyzing large\ndatasets with AI. MyMagic AI offers a powerful API for _batch_ inference (also\nknown as _offline_ or _delayed_ inference) that brings various open-source\nLarge Language Models (LLMs) such as Llama 70B, Mistral 7B, Mixtral 8x7B,\nCodeLlama70b, and advanced Embedding models to its users. Our framework is\ndesigned to perform data extraction, summarization, categorization, sentiment\nanalysis, training data generation, and embedding, to name a few. And now it's\nintegrated directly into LlamaIndex!", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 611, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7aa4f7ce-4b00-460a-802c-941942010fdd": {"__data__": {"id_": "7aa4f7ce-4b00-460a-802c-941942010fdd", "embedding": null, "metadata": {"Header_2": " Part 1: batch inference", "filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3fc617da-f983-4ea8-8a49-b8ff6c3f14d3", "node_type": "4", "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "9fa6c05cb14bac9d0e24f5480a646a38f7ad05cdc4f0df72b5ae1195bf9f11a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "91a67b0c-d6a4-4ae1-9e5d-f1fe93b23d66", "node_type": "1", "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "047ddef1b4c32d27462f7042d57b96abf73dba0c246990e2f16f4fdeb994fb9d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e20c8dcd-c122-4e58-87d2-09577289c745", "node_type": "1", "metadata": {"Header_2": " Part 1: batch inference", "Header_3": " How It Works:"}, "hash": "e4ef1cc04fd191e4c201554cda9ede1e16b366504036dd98c7edd33bc6656ed9", "class_name": "RelatedNodeInfo"}}, "text": "Part 1: batch inference", "mimetype": "text/plain", "start_char_idx": 617, "end_char_idx": 640, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e20c8dcd-c122-4e58-87d2-09577289c745": {"__data__": {"id_": "e20c8dcd-c122-4e58-87d2-09577289c745", "embedding": null, "metadata": {"Header_2": " Part 1: batch inference", "Header_3": " How It Works:", "filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3fc617da-f983-4ea8-8a49-b8ff6c3f14d3", "node_type": "4", "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "9fa6c05cb14bac9d0e24f5480a646a38f7ad05cdc4f0df72b5ae1195bf9f11a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7aa4f7ce-4b00-460a-802c-941942010fdd", "node_type": "1", "metadata": {"Header_2": " Part 1: batch inference", "filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "bb944eda71ed7bb0f46ca28e7024b502c92cf6821d67e1de5b1d894db838a562", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65dc8fd6-8a8b-4abb-a2ac-c1267da61afa", "node_type": "1", "metadata": {"Header_2": " Part 1: batch inference", "Header_3": " Use Cases"}, "hash": "9e564043d19636eb86ed68cce8c625ffce71f08b64fcd76554d7722052572c91", "class_name": "RelatedNodeInfo"}}, "text": "How It Works:\n\n**1\\. Setup** :\n\n  1. Organize Your Data in an AWS S3 or GCS Bucket: \n    1. Create a folder using your user ID assigned to you upon registration. \n    2. Inside that folder, create another folder (called a \"session\") to store all the files you need for your tasks. \n  2. Purpose of the 'Session' Folder: \n    1. This \"Session\" folder keeps your files separate from others, making sure that your tasks run on the right set of files. You can name your session subfolder anything you like. \n  3. Granting Access to MyMagic AI: \n    1. To allow MyMagic AI to securely access your files in the cloud, follow the setup instructions provided in the [ MyMagic AI documentation ](https://docs.mymagic.ai/) . \n\n**2\\. Install** : Install both MyMagic AI\u2019s API integration and LlamaIndex\nlibrary:\n\n    \n    \n    pip install llama-index\n    pip install llama-index-llms-mymagic\n\n**3\\. API Request:** The llamaIndex library is a wrapper around MyMagic AI\u2019s\nAPI. What it does under the hood is simple: it sends a POST request to the\nMyMagic AI API while specifying the model, storage provider, bucket name,\nsession name, and other necessary details.\n\n    \n    \n    import asyncio\n    from llama_index.llms.mymagic import MyMagicAI\n    \n    llm = MyMagicAI(\n        api_key=\"user_...\", # provided by MyMagic AI upon sign-up\n        storage_provider=\"s3\",\n        bucket_name=\"batch-bucket\", # you may name anything\n        session=\"my-session\",\n        role_arn=\"arn:aws:iam::<your account id>:role/mymagic-role\",\n        system_prompt=\"You are an AI assistant that helps to summarize the documents without essential loss of information\", # default prompt at https://docs.mymagic.ai/api-reference/endpoint/create\n        region=\"eu-west-2\",\n    )\n\nWe have designed the integration to allow the user to set up the bucket and\ndata together with the system prompt when instantiating the llm object. Other\ninputs, e.g. question (i.e. your prompt), model and max_tokens are dynamic\nrequirements when submitting complete and acomplete requests.\n\n    \n    \n    resp = llm.complete(\n        question=\"Summarise this in one sentence.\",\n        model=\"mixtral8x7\", \n        max_tokens=20,  # default is 10\n    )\n    print(resp)\n    async def main():\n        aresp = await llm.acomplete(\n            question=\"Summarize this in one sentence.\",\n            model=\"llama7b\",\n            max_tokens=20,\n        )\n        print(aresp)\n    \n    asyncio.run(main())\n\nThis dynamic entry allows developers to experiment with different prompts and\nmodels in their workflow while also controlling for model output to cap their\nspending limit. MyMagic AI\u2019s backend supports both synchronous requests\n(complete) and asynchronous requests (acomplete). It is advisable, however, to\nuse our async endpoints as much as possible as batch jobs are inherently\nasynchronous with potentially long processing times (depending on the size of\nyour data).\n\nCurrently, we do not support chat or achat methods as our API is not designed\nfor real-time interactive experience. However, we are planning to add those\nmethods in the future that will function in a \u201cbatch way\u201d. The user queries\nwill be aggregated and appended as one prompt (to give the chat context) and\nsent to all files at once.", "mimetype": "text/plain", "start_char_idx": 647, "end_char_idx": 3901, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65dc8fd6-8a8b-4abb-a2ac-c1267da61afa": {"__data__": {"id_": "65dc8fd6-8a8b-4abb-a2ac-c1267da61afa", "embedding": null, "metadata": {"Header_2": " Part 1: batch inference", "Header_3": " Use Cases", "filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3fc617da-f983-4ea8-8a49-b8ff6c3f14d3", "node_type": "4", "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "9fa6c05cb14bac9d0e24f5480a646a38f7ad05cdc4f0df72b5ae1195bf9f11a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e20c8dcd-c122-4e58-87d2-09577289c745", "node_type": "1", "metadata": {"Header_2": " Part 1: batch inference", "Header_3": " How It Works:", "filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "cda0779003da05c1a62d5ba689a868b18c1cce858b08aa497eca1267254d413f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64042a75-0c31-4a4f-97fb-555de740b411", "node_type": "1", "metadata": {"Header_2": " Part 1: batch inference", "Header_3": " Use Cases", "Header_4": " 1\\. Extraction"}, "hash": "b7ffe2c710aadbee44e6db4d42c6f3bd4b1dda9717c4770e96967adf1446cfcb", "class_name": "RelatedNodeInfo"}}, "text": "Use Cases\n\nWhile there are myriads of use cases, here we provide a few to help motivate\nour users. Feel free to embed our API in your workflows that are good fit for\nbatch processing.", "mimetype": "text/plain", "start_char_idx": 3908, "end_char_idx": 4091, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64042a75-0c31-4a4f-97fb-555de740b411": {"__data__": {"id_": "64042a75-0c31-4a4f-97fb-555de740b411", "embedding": null, "metadata": {"Header_2": " Part 1: batch inference", "Header_3": " Use Cases", "Header_4": " 1\\. Extraction", "filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3fc617da-f983-4ea8-8a49-b8ff6c3f14d3", "node_type": "4", "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "9fa6c05cb14bac9d0e24f5480a646a38f7ad05cdc4f0df72b5ae1195bf9f11a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65dc8fd6-8a8b-4abb-a2ac-c1267da61afa", "node_type": "1", "metadata": {"Header_2": " Part 1: batch inference", "Header_3": " Use Cases", "filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "f6d817d34dc024671e85858e7a0c09a1e630036801e9638b1b79b3ba4cc5beb9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9caff2fa-963a-4ee4-a29f-4026c4bbf2be", "node_type": "1", "metadata": {"Header_2": " Part 1: batch inference", "Header_3": " Use Cases", "Header_4": " 2\\. Classification"}, "hash": "ccbcfbcad1031f16735bfb2e2e1bbded5da4f32f2d4c7bc042e41d00f765a84b", "class_name": "RelatedNodeInfo"}}, "text": "1\\. Extraction\n\nImagine needing to extract specific information from millions of files stored\nin a bucket. Information from all files will be extracted with one API call\ninstead of a million sequential ones.", "mimetype": "text/plain", "start_char_idx": 4099, "end_char_idx": 4306, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9caff2fa-963a-4ee4-a29f-4026c4bbf2be": {"__data__": {"id_": "9caff2fa-963a-4ee4-a29f-4026c4bbf2be", "embedding": null, "metadata": {"Header_2": " Part 1: batch inference", "Header_3": " Use Cases", "Header_4": " 2\\. Classification", "filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3fc617da-f983-4ea8-8a49-b8ff6c3f14d3", "node_type": "4", "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "9fa6c05cb14bac9d0e24f5480a646a38f7ad05cdc4f0df72b5ae1195bf9f11a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64042a75-0c31-4a4f-97fb-555de740b411", "node_type": "1", "metadata": {"Header_2": " Part 1: batch inference", "Header_3": " Use Cases", "Header_4": " 1\\. Extraction", "filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "3540b181686c138601d8592449d191c3d46380540054471f6e1c83785337439f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05a60d5e-6120-4984-a4a4-5d065a5c1de3", "node_type": "1", "metadata": {"Header_2": " Part 1: batch inference", "Header_3": " Use Cases", "Header_4": " 3\\. Embedding"}, "hash": "721659a5eb922cf7e0db09764c2d06321539e4a1b1b4d1d53fd24a7f32a204ea", "class_name": "RelatedNodeInfo"}}, "text": "2\\. Classification\n\nFor businesses looking to classify customer reviews such as positive, neutral,\nand negative. With one request you can start processing the requests over the\nweekend and get them ready by Monday morning.", "mimetype": "text/plain", "start_char_idx": 4314, "end_char_idx": 4536, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05a60d5e-6120-4984-a4a4-5d065a5c1de3": {"__data__": {"id_": "05a60d5e-6120-4984-a4a4-5d065a5c1de3", "embedding": null, "metadata": {"Header_2": " Part 1: batch inference", "Header_3": " Use Cases", "Header_4": " 3\\. Embedding", "filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3fc617da-f983-4ea8-8a49-b8ff6c3f14d3", "node_type": "4", "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "9fa6c05cb14bac9d0e24f5480a646a38f7ad05cdc4f0df72b5ae1195bf9f11a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9caff2fa-963a-4ee4-a29f-4026c4bbf2be", "node_type": "1", "metadata": {"Header_2": " Part 1: batch inference", "Header_3": " Use Cases", "Header_4": " 2\\. Classification", "filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "bb6aa512c8282aaec09484ade47ea2e262fc78f458bbeb154c9531a053b15a4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a029da55-b944-46d6-bb4c-51d22d9b9b37", "node_type": "1", "metadata": {"Header_2": " Part 1: batch inference", "Header_3": " Use Cases", "Header_4": " 4\\. Training (Fine-tuning) Data Generation"}, "hash": "319c8bc766ac61818317507103784829171c956529715436bb09f8d3246fed0e", "class_name": "RelatedNodeInfo"}}, "text": "3\\. Embedding\n\nEmbedding text files for further machine learning applications is another\npowerful use case of MyMagic AI's API. You will be ready for your vector db in\na matter of days not weeks.", "mimetype": "text/plain", "start_char_idx": 4544, "end_char_idx": 4739, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a029da55-b944-46d6-bb4c-51d22d9b9b37": {"__data__": {"id_": "a029da55-b944-46d6-bb4c-51d22d9b9b37", "embedding": null, "metadata": {"Header_2": " Part 1: batch inference", "Header_3": " Use Cases", "Header_4": " 4\\. Training (Fine-tuning) Data Generation", "filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3fc617da-f983-4ea8-8a49-b8ff6c3f14d3", "node_type": "4", "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "9fa6c05cb14bac9d0e24f5480a646a38f7ad05cdc4f0df72b5ae1195bf9f11a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05a60d5e-6120-4984-a4a4-5d065a5c1de3", "node_type": "1", "metadata": {"Header_2": " Part 1: batch inference", "Header_3": " Use Cases", "Header_4": " 3\\. Embedding", "filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "734f3b2393e6df42d6b8aed04a63c4ffab9c3f756b5188d5bf4f8d8e2c837a7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a694031-9c34-4165-a5d9-5b76e330ccd7", "node_type": "1", "metadata": {"Header_2": " Part 1: batch inference", "Header_3": " Use Cases", "Header_4": " 5\\. Transcription"}, "hash": "f25cfa9de4d9310d8e8e64754387ad0b1f4c3c48ff9e82bef0ec1cc412476f78", "class_name": "RelatedNodeInfo"}}, "text": "4\\. Training (Fine-tuning) Data Generation\n\nImagine generating thousands of synthetic data for your fine-tuning tasks.\nWith MyMagic AI\u2019s API, you can reduce the generation time by a factor of 5-10x\ncompared to GPT-3.5.", "mimetype": "text/plain", "start_char_idx": 4747, "end_char_idx": 4965, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a694031-9c34-4165-a5d9-5b76e330ccd7": {"__data__": {"id_": "5a694031-9c34-4165-a5d9-5b76e330ccd7", "embedding": null, "metadata": {"Header_2": " Part 1: batch inference", "Header_3": " Use Cases", "Header_4": " 5\\. Transcription", "filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3fc617da-f983-4ea8-8a49-b8ff6c3f14d3", "node_type": "4", "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "9fa6c05cb14bac9d0e24f5480a646a38f7ad05cdc4f0df72b5ae1195bf9f11a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a029da55-b944-46d6-bb4c-51d22d9b9b37", "node_type": "1", "metadata": {"Header_2": " Part 1: batch inference", "Header_3": " Use Cases", "Header_4": " 4\\. Training (Fine-tuning) Data Generation", "filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "6ba5df8f1e1885b8b584ed7c8bb623063c7bb5a65d70826c61e8f01c69a6ba9b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ec7d96c-4047-4396-8fb4-ff47ad7867a1", "node_type": "1", "metadata": {"Header_2": " Part 2: Integration with LlamaIndex\u2019s RAG Pipeline"}, "hash": "62a22dd81a97125e4f1f72eb1136f76110a04d0a3d5bf208bdb118117368d85b", "class_name": "RelatedNodeInfo"}}, "text": "5\\. Transcription\n\nMyMagic AI\u2019s API supports different types of files, so it is also easy to\nbatch transcribe many mp3 or mp4 files in your bucket.", "mimetype": "text/plain", "start_char_idx": 4973, "end_char_idx": 5120, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ec7d96c-4047-4396-8fb4-ff47ad7867a1": {"__data__": {"id_": "7ec7d96c-4047-4396-8fb4-ff47ad7867a1", "embedding": null, "metadata": {"Header_2": " Part 2: Integration with LlamaIndex\u2019s RAG Pipeline", "filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3fc617da-f983-4ea8-8a49-b8ff6c3f14d3", "node_type": "4", "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "9fa6c05cb14bac9d0e24f5480a646a38f7ad05cdc4f0df72b5ae1195bf9f11a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a694031-9c34-4165-a5d9-5b76e330ccd7", "node_type": "1", "metadata": {"Header_2": " Part 1: batch inference", "Header_3": " Use Cases", "Header_4": " 5\\. Transcription", "filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "cca42176f258df615f9d92a1a95aba02e8093ab312d16ddec6f1ea0e81df261a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4efaa25e-1308-4321-8fca-876165898b8a", "node_type": "1", "metadata": {"Header_2": " Next Steps"}, "hash": "a6bce630b538d9fbe98b1c4e43e11bbb2681a7584a9c44737997aa55277e178f", "class_name": "RelatedNodeInfo"}}, "text": "Part 2: Integration with LlamaIndex\u2019s RAG Pipeline\n\nThe output from batch inference processes, often voluminous, can seamlessly\nintegrate into LlamaIndex's RAG pipeline for effective data storage and\nretrieval.\n\nThis section demonstrates how to use the Llama3 model from the Ollama library\ncoupled with BGE embedding to manage information storage and execute queries.\nPlease ensure the following prerequisites are installed and Llama3 model is\npulled:\n\n    \n    \n    pip install llama-index-embeddings-huggingface\n    curl -fsSL https://ollama.com/install.sh | sh\n    ollama pull llama3\n\nFor this demo, we have run a batch summarization job on 5 Amazon reviews (but\nthis might be millions in some real scenarios) and saved the results as\nreviews_1_5.json:\n\n    \n    \n    {\n      \"id_review1\": {\n        \"query\": \"Summarize the document!\",\n        \"output\": \"The document describes a family with a young boy who believes there is a zombie in his closet, while his parents are constantly fighting. The movie is criticized for its inconsistent genre, described as a slow-paced drama with occasional thriller elements. The review praises the well-playing parents and the decent dialogs but criticizes the lack of a boogeyman-like horror element. The overall rating is 3 out of 10.\"\n      },\n      \"id_review2\": {\n        \"query\": \"Summarize the document!\",\n        \"output\": \"The document is a positive review of a light-hearted Woody Allen comedy. The reviewer praises the witty dialogue, likable characters, and Woody Allen's control over his signature style. The film is noted for making the reviewer laugh more than any recent Woody Allen comedy and praises Scarlett Johansson's performance. It concludes by calling the film a great comedy to watch with friends.\"\n      },\n      \"id_review3\": {\n        \"query\": \"Summarize the document!\",\n        \"output\": \"The document describes a well-made film about one of the great masters of comedy, filmed in an old-time BBC fashion that adds realism. The actors, including Michael Sheen, are well-chosen and convincing. The production is masterful, showcasing realistic details like the fantasy of the guard and the meticulously crafted sets of Orton and Halliwell's flat. Overall, it is a terrific and well-written piece.\"\n      },\n      \"id_review4\": {\n        \"query\": \"Summarize the document!\",\n        \"output\": \"Petter Mattei's 'Love in the Time of Money' is a visually appealing film set in New York, exploring human relations in the context of money, power, and success. The characters, played by a talented cast including Steve Buscemi and Rosario Dawson, are connected in various ways but often unaware of their shared links. The film showcases the different stages of loneliness experienced by individuals in a big city. Mattei successfully portrays the world of these characters, creating a luxurious and sophisticated look. The film is a modern adaptation of Arthur Schnitzler's play on the same theme. Mattei's work is appreciated, and viewers look forward to his future projects.\"\n      },\n      \"id_review5\": {\n        \"query\": \"Summarize the document!\",\n        \"output\": \"The document describes the TV show 'Oz', set in the Oswald Maximum Security State Penitentiary. Known for its brutality, violence, and lack of privacy, it features an experimental section of the prison called Em City, where all the cells have glass fronts and face inwards. The show goes where others wouldn't dare, featuring graphic violence, injustice, and the harsh realities of prison life. The viewer may become comfortable with uncomfortable viewing if they can embrace their darker side.\"\n      },\n      \"token_count\": 3391\n    }\n    \n\nNow let\u2019s embed and store this document and ask questions using LlamaIndex\u2019s\nquery engine. Bring in our dependencies:\n\n    \n    \n    import os\n    \n    from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n    from llama_index.core.indices.vector_store import VectorStoreIndex\n    from llama_index.core.settings import Settings\n    from llama_index.core.readers import SimpleDirectoryReader\n    from llama_index.llms.ollama import Ollama\n\nConfigure the embedding model and Llama3 model\n\n    \n    \n    embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n    llm = Ollama(model=\"llama3\", request_timeout=300.0)\n\nUpdate settings for the indexing pipeline:\n\n    \n    \n    Settings.llm = llm\n    Settings.embed_model = embed_model\n    Settings.chunk_size = 512 # This parameter defines the size of text chunks for embedding\n    \n    documents = SimpleDirectoryReader(\"reviews_1_5.json\").load_data() #Modify path for your case\n\nNow create our index, our query engine and run a query:\n\n    \n    \n    index = VectorStoreIndex.from_documents(documents, show_progress=True)\n    \n    query_engine = index.as_query_engine(similarity_top_k=3)\n    \n    response = query_engine.query(\"What is the least favourite movie?\")\n    print(response)\n\nOutput:\n\n    \n    \n    Based on query results, the least favourite movie is: review 1 with a rating of 3 out of 10.\n\nNow we know that the review 1 is the least favorite movie among these reviews.", "mimetype": "text/plain", "start_char_idx": 5126, "end_char_idx": 10257, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4efaa25e-1308-4321-8fca-876165898b8a": {"__data__": {"id_": "4efaa25e-1308-4321-8fca-876165898b8a", "embedding": null, "metadata": {"Header_2": " Next Steps", "filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3fc617da-f983-4ea8-8a49-b8ff6c3f14d3", "node_type": "4", "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "9fa6c05cb14bac9d0e24f5480a646a38f7ad05cdc4f0df72b5ae1195bf9f11a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ec7d96c-4047-4396-8fb4-ff47ad7867a1", "node_type": "1", "metadata": {"Header_2": " Part 2: Integration with LlamaIndex\u2019s RAG Pipeline", "filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "e676d6de0186cb149c3e7d13b7638980194ae0bee46e87b5ffdab6b4ace33f4c", "class_name": "RelatedNodeInfo"}}, "text": "Next Steps\n\nThis shows how batch inference combined with real-time inference can be a\npowerful tool for analyzing, storing and retrieving information from massive\namounts of data. [ Get started with MyMagic AI\u2019s API ](https://vivacious-\nriver-18.authkit.app/sign-up?redirect_uri=https://api.mymagic.ai/workspace)\ntoday!", "mimetype": "text/plain", "start_char_idx": 10263, "end_char_idx": 10582, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"91a67b0c-d6a4-4ae1-9e5d-f1fe93b23d66": {"doc_hash": "047ddef1b4c32d27462f7042d57b96abf73dba0c246990e2f16f4fdeb994fb9d", "ref_doc_id": "3fc617da-f983-4ea8-8a49-b8ff6c3f14d3"}, "7aa4f7ce-4b00-460a-802c-941942010fdd": {"doc_hash": "bb944eda71ed7bb0f46ca28e7024b502c92cf6821d67e1de5b1d894db838a562", "ref_doc_id": "3fc617da-f983-4ea8-8a49-b8ff6c3f14d3"}, "e20c8dcd-c122-4e58-87d2-09577289c745": {"doc_hash": "cda0779003da05c1a62d5ba689a868b18c1cce858b08aa497eca1267254d413f", "ref_doc_id": "3fc617da-f983-4ea8-8a49-b8ff6c3f14d3"}, "65dc8fd6-8a8b-4abb-a2ac-c1267da61afa": {"doc_hash": "f6d817d34dc024671e85858e7a0c09a1e630036801e9638b1b79b3ba4cc5beb9", "ref_doc_id": "3fc617da-f983-4ea8-8a49-b8ff6c3f14d3"}, "64042a75-0c31-4a4f-97fb-555de740b411": {"doc_hash": "3540b181686c138601d8592449d191c3d46380540054471f6e1c83785337439f", "ref_doc_id": "3fc617da-f983-4ea8-8a49-b8ff6c3f14d3"}, "9caff2fa-963a-4ee4-a29f-4026c4bbf2be": {"doc_hash": "bb6aa512c8282aaec09484ade47ea2e262fc78f458bbeb154c9531a053b15a4c", "ref_doc_id": "3fc617da-f983-4ea8-8a49-b8ff6c3f14d3"}, "05a60d5e-6120-4984-a4a4-5d065a5c1de3": {"doc_hash": "734f3b2393e6df42d6b8aed04a63c4ffab9c3f756b5188d5bf4f8d8e2c837a7a", "ref_doc_id": "3fc617da-f983-4ea8-8a49-b8ff6c3f14d3"}, "a029da55-b944-46d6-bb4c-51d22d9b9b37": {"doc_hash": "6ba5df8f1e1885b8b584ed7c8bb623063c7bb5a65d70826c61e8f01c69a6ba9b", "ref_doc_id": "3fc617da-f983-4ea8-8a49-b8ff6c3f14d3"}, "5a694031-9c34-4165-a5d9-5b76e330ccd7": {"doc_hash": "cca42176f258df615f9d92a1a95aba02e8093ab312d16ddec6f1ea0e81df261a", "ref_doc_id": "3fc617da-f983-4ea8-8a49-b8ff6c3f14d3"}, "7ec7d96c-4047-4396-8fb4-ff47ad7867a1": {"doc_hash": "e676d6de0186cb149c3e7d13b7638980194ae0bee46e87b5ffdab6b4ace33f4c", "ref_doc_id": "3fc617da-f983-4ea8-8a49-b8ff6c3f14d3"}, "4efaa25e-1308-4321-8fca-876165898b8a": {"doc_hash": "69097bffbbb51f190848366c72d27a8dd8692fe0384371eb5942ac514e670ac2", "ref_doc_id": "3fc617da-f983-4ea8-8a49-b8ff6c3f14d3"}}, "docstore/ref_doc_info": {"3fc617da-f983-4ea8-8a49-b8ff6c3f14d3": {"node_ids": ["91a67b0c-d6a4-4ae1-9e5d-f1fe93b23d66", "7aa4f7ce-4b00-460a-802c-941942010fdd", "e20c8dcd-c122-4e58-87d2-09577289c745", "65dc8fd6-8a8b-4abb-a2ac-c1267da61afa", "64042a75-0c31-4a4f-97fb-555de740b411", "9caff2fa-963a-4ee4-a29f-4026c4bbf2be", "05a60d5e-6120-4984-a4a4-5d065a5c1de3", "a029da55-b944-46d6-bb4c-51d22d9b9b37", "5a694031-9c34-4165-a5d9-5b76e330ccd7", "7ec7d96c-4047-4396-8fb4-ff47ad7867a1", "4efaa25e-1308-4321-8fca-876165898b8a"], "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}}}}