{"docstore/data": {"38d07735-cff6-4aef-b129-68ddb644165f": {"__data__": {"id_": "38d07735-cff6-4aef-b129-68ddb644165f", "embedding": null, "metadata": {"Header_2": " **Introduction**", "filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ef14a4e-7e93-41ae-aada-058b036e40d7", "node_type": "4", "metadata": {"filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}, "hash": "adff9486539f8783cb7bd0696d17b40f22b60ff1d1acf2eaa43443d41078c908", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1436cdae-add0-48b9-a6b5-abbea8f8e2dd", "node_type": "1", "metadata": {"Header_2": " **The OCR Dilemma: Obstacles and Constraints Optical Character**"}, "hash": "145ca12d2eb38cccfab09e8b9f5fe0d12137c5970ebd969bbeb2e4a0c72f1337", "class_name": "RelatedNodeInfo"}}, "text": "**Introduction**\n\nIn the domain of document handling, accurately extracting crucial information\nfrom images has posed an enduring obstacle. Despite Optical Character\nRecognition (OCR) advancements in converting images to editable text, it faces\nnumerous intricacies with diverse document formats and quality. Here enters\nZephyr 7b LLM, a pioneering remedy that, coupled with LlamaIndex, directly\naddresses these hurdles, heralding a transformative era in image-based\ndocument extraction.\n\nSource: [ Zephyr-llama-index ](https://corca.substack.com/p/top-llm-papers-of-\nthe-week-95e?utm_source=profile&utm_medium=reader2)", "mimetype": "text/plain", "start_char_idx": 4, "end_char_idx": 623, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1436cdae-add0-48b9-a6b5-abbea8f8e2dd": {"__data__": {"id_": "1436cdae-add0-48b9-a6b5-abbea8f8e2dd", "embedding": null, "metadata": {"Header_2": " **The OCR Dilemma: Obstacles and Constraints Optical Character**", "filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ef14a4e-7e93-41ae-aada-058b036e40d7", "node_type": "4", "metadata": {"filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}, "hash": "adff9486539f8783cb7bd0696d17b40f22b60ff1d1acf2eaa43443d41078c908", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38d07735-cff6-4aef-b129-68ddb644165f", "node_type": "1", "metadata": {"Header_2": " **Introduction**", "filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}, "hash": "3fae0c0b8340a778482d2d26c03ec7c3dfc6f57b12b1a2c32ffa9dce925bf852", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66ae644c-8487-45d6-ae0b-79af7ab62cc5", "node_type": "1", "metadata": {"Header_2": " **Zephyr 7b LLM: Narrowing the Divide**"}, "hash": "657548ec16f117ac3e2cfd75805734a8aaa784fe0c8b045294b3fb77fb91a76c", "class_name": "RelatedNodeInfo"}}, "text": "**The OCR Dilemma: Obstacles and Constraints Optical Character**\n\nRecognition (OCR), though potent, faces impediments such as:\n\n  1. **Diverse Document Formats** : Documents exhibit intricate layouts, fonts, and structures, posing challenges for traditional OCR to precisely interpret and extract information. \n  2. **Quality and Clarity** : Images with low resolution, blurriness, or skewed angles hinder OCR\u2019s accuracy in deciphering text. \n  3. **Handwritten and Cursive Content** : OCR often struggles with handwritten text or cursive fonts, resulting in errors or incomplete extraction. \n  4. **Multilingual Complexity** : Processing documents in multiple languages poses a challenge for OCR systems lacking proficiency in recognizing and extracting varied linguistic content. \n\nSource: Created by Author using MidJourney", "mimetype": "text/plain", "start_char_idx": 629, "end_char_idx": 1455, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66ae644c-8487-45d6-ae0b-79af7ab62cc5": {"__data__": {"id_": "66ae644c-8487-45d6-ae0b-79af7ab62cc5", "embedding": null, "metadata": {"Header_2": " **Zephyr 7b LLM: Narrowing the Divide**", "filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ef14a4e-7e93-41ae-aada-058b036e40d7", "node_type": "4", "metadata": {"filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}, "hash": "adff9486539f8783cb7bd0696d17b40f22b60ff1d1acf2eaa43443d41078c908", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1436cdae-add0-48b9-a6b5-abbea8f8e2dd", "node_type": "1", "metadata": {"Header_2": " **The OCR Dilemma: Obstacles and Constraints Optical Character**", "filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}, "hash": "343a2a7ed379dc611d7cf8b2839f25be35772acd495a92524e1a2ff8d8d12de6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "540f71a9-0dd6-4e69-87fd-e6245685fe80", "node_type": "1", "metadata": {"Header_2": " **Implementation of Code**"}, "hash": "6c48c2056402ef9e6ece9a8546e7825a41f22a451de291b653bfa7d24e0236b3", "class_name": "RelatedNodeInfo"}}, "text": "**Zephyr 7b LLM: Narrowing the Divide**\n\nZephyr 7b LLM revolutionizes the landscape by tackling these inherent\nconstraints of OCR technology:\n\n  1. **Advanced Machine Learning Algorithms:**\n\nEmploying state-of-the-art machine learning algorithms, Zephyr 7b LLM\nundergoes extensive training with diverse document formats and languages. This\nequips it to adapt and learn from various document structures, resulting in\nheightened accuracy and robust extraction capabilities.\n\n**2\\. Contextual Comprehension:**\n\nDiverging from conventional OCR, Zephyr 7b LLM doesn\u2019t merely identify\nindividual characters; it comprehends the context in which these characters\nexist. This contextual understanding significantly reduces errors, ensuring\nprecise extraction even from intricate document layouts.\n\n**3\\. Adaptive Image Processing:**\n\nThe fusion with LlamaIndex amplifies Zephyr 7b LLM\u2019s ability to handle images\nof varying resolutions or qualities. Leveraging adaptive image processing\ntechniques, it rectifies distortions, enhances clarity, and optimizes images\nfor meticulous OCR analysis.\n\n**4\\. Multilingual Proficiency:**\n\nZephyr 7b LLM surpasses language barriers. Its multilingual proficiency\nfacilitates seamless content extraction from documents in various languages,\nextending global accessibility for businesses dealing with multilingual\ndocumentation.\n\nSource: Created by Author using MidJourney", "mimetype": "text/plain", "start_char_idx": 1461, "end_char_idx": 2859, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "540f71a9-0dd6-4e69-87fd-e6245685fe80": {"__data__": {"id_": "540f71a9-0dd6-4e69-87fd-e6245685fe80", "embedding": null, "metadata": {"Header_2": " **Implementation of Code**", "filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ef14a4e-7e93-41ae-aada-058b036e40d7", "node_type": "4", "metadata": {"filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}, "hash": "adff9486539f8783cb7bd0696d17b40f22b60ff1d1acf2eaa43443d41078c908", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66ae644c-8487-45d6-ae0b-79af7ab62cc5", "node_type": "1", "metadata": {"Header_2": " **Zephyr 7b LLM: Narrowing the Divide**", "filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}, "hash": "f741523304827a6dd439b213537daad3bdb5ffe03f2579c2ed29057c1dbbad11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b161277-3a79-4ad1-9a1c-b1058e6fcecf", "node_type": "1", "metadata": {"Header_2": " Conclusion"}, "hash": "fa0c0f6fdc942a8a867c07b3be8382b63a3d39855dba6e151608262795f2013b", "class_name": "RelatedNodeInfo"}}, "text": "**Implementation of Code**\n\nThe collaboration between Zephyr 7b LLM and LlamaIndex signifies a pivotal\ntransformation in document extraction. By merging Zephyr\u2019s advanced OCR\ncapabilities with LlamaIndex\u2019s image enhancement and data organization\nfeatures, this integration presents a comprehensive solution:\n\n  1. **Augmented Precision** : The fusion of Zephyr\u2019s machine learning expertise and LlamaIndex\u2019s image enhancement markedly heightens the accuracy of extracted data, diminishing errors and enhancing overall efficiency. \n  2. **Efficient Workflow** : Users experience an optimized workflow, enabling swift extraction and conversion of image-based documents into structured, actionable data, facilitating expedited decision-making processes. \n  3. **Adaptability Across Document Varieties** : This integration empowers users to handle diverse document formats and languages effortlessly, granting access to previously challenging document types for extraction and analysis. \n\nSource: Image created by Author using MidJourney\n\n**Step 1: Install and Import Libraries**\n\n    \n    \n    !pip install llama-index transformers accelerate sentencepiece bitsandbytes -q\n\n**Step 2: Load the Model**\n\n    \n    \n    import torchfrom transformers import BitsAndBytesConfigfrom llama_index.prompts import PromptTemplatefrom llama_index.llms import HuggingFaceLLMquantization_config = BitsAndBytesConfig(    load_in_4bit=True,    bnb_4bit_compute_dtype=torch.float16,    bnb_4bit_quant_type=\"nf4\",    bnb_4bit_use_double_quant=True,)def messages_to_prompt(messages):  prompt = \"\"  for message in messages:    if message.role == 'system':      prompt += f\"<|system|>\\n{message.content}</s>\\n\"    elif message.role == 'user':      prompt += f\"<|user|>\\n{message.content}</s>\\n\"    elif message.role == 'assistant':      prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"  # ensure we start with a system prompt, insert blank if needed  if not prompt.startswith(\"<|system|>\\n\"):    prompt = \"<|system|>\\n</s>\\n\" + prompt  # add final assistant prompt  prompt = prompt + \"<|assistant|>\\n\"  return promptllm = HuggingFaceLLM(    model_name=\"HuggingFaceH4/zephyr-7b-alpha\",    tokenizer_name=\"HuggingFaceH4/zephyr-7b-alpha\",    query_wrapper_prompt=PromptTemplate(\"<|system|>\\n</s>\\n<|user|>\\n{query_str}</s>\\n<|assistant|>\\n\"),    context_window=3900,    max_new_tokens=2000,    model_kwargs={\"quantization_config\": quantization_config},    # tokenizer_kwargs={},    generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},    messages_to_prompt=messages_to_prompt,    device_map=\"auto\",)\n    \n    \n    from llama_index import ServiceContext, set_global_service_contextservice_context = ServiceContext.from_defaults(llm=llm, embed_model=\"local:BAAI/bge-small-en-v1.5\")\n    \n    \n    set_global_service_context(service_context)\n\n**Step 3: Storing your index**\n\n    \n    \n    from llama_index import SimpleDirectoryReader, VectorStoreIndexfrom llama_index.readers.file.base import (    DEFAULT_FILE_READER_CLS,    ImageReader,)from llama_index.response.notebook_utils import (    display_response,    display_image,)from llama_index.indices.query.query_transform.base import (    ImageOutputQueryTransform,)filename_fn = lambda filename: {\"file_name\": filename}llama_reader = SimpleDirectoryReader(    input_dir=\"/content/llama\",    file_metadata=filename_fn,)llama_documents = llama_reader.load_data()llama_index = VectorStoreIndex.from_documents(llama_documents)\n\n**Step 4: Query** [ **Transformations**\n](https://github.com/andysingal/CV_public/tree/main/zephyr-7b-alpha)\n\n    \n    \n    from llama_index.query_engine import TransformQueryEnginequery_engine = llama_index.as_query_engine(similarity_top_k=2)query_engine = TransformQueryEngine(    query_engine, query_transform=ImageOutputQueryTransform(width=400))llama_response = query_engine.query(    \"Show an image to illustrate how tree index works and explain briefly\",)display_response(llama_response)#OutputFinal Response: I am not capable of displaying images. however, i can provide you with an explanation of how tree index works.tree index is a data structure that organizes data in a hierarchical manner, similar to a tree. it is commonly used in databases to improve query performance.when querying a tree index, the process involves traversing from the root node down to the leaf nodes. the number of child nodes chosen per parent node is determined by the child_branch_factor parameter.for example, if child_branch_factor=1, a query will choose one child node given a parent node. if child_branch_factor=2, a query will choose two child nodes per parent.the following image illustrates how a tree index works:! Tree Index Examplein this example, the tree index is built from a set of nodes (which become leaf nodes in this tree). when querying this index, the process involves traversing from the root node down to the leaf nodes. for instance, if we want to find a specific node with the value \"x\", we would start at the root node and follow the left branch (since \"x\" is less than \"a\") to the next level. we would then follow the left branch again to reach the leaf node with the value \"x\".i hope this helps clarify how tree index works!\n\n**Step 5: Lets read the** [ **receipts**\n](https://github.com/andysingal/CV_public/tree/main/zephyr-7b-alpha)\n\n    \n    \n    from llama_index.readers.file.base import DEFAULT_FILE_READER_CLSfrom llama_index.readers.file.image_reader import ImageReaderimage_parser =ImageReader(    keep_image=True,    parse_text=True    )file_extractor = DEFAULT_FILE_READER_CLSfile_extractor.update({    \".jpg\": image_parser,    \".png\": image_parser,    \".jpeg\": image_parser,    })receipt_reader = SimpleDirectoryReader(    input_dir=\"/content/data\",    file_metadata=filename_fn,    file_extractor=file_extractor,)receipt_documents = receipt_reader.load_data()print(len(receipt_documents))#Output3\n    \n    \n    receipts_index = VectorStoreIndex.from_documents(receipt_documents)from llama_index.query_engine import TransformQueryEnginequery_engine = receipts_index.as_query_engine()receipts_response = query_engine.query(    \"When was the last time I went to RESTAURANT and how much did I spend? this data is in your latest vector index.\",)display_response(receipts_response)# Output Final Response: Based on the given context information, the last time the querying individual went to RESTAURANT was on July 5, 2019, and they spent $164.00.", "mimetype": "text/plain", "start_char_idx": 2865, "end_char_idx": 9298, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b161277-3a79-4ad1-9a1c-b1058e6fcecf": {"__data__": {"id_": "1b161277-3a79-4ad1-9a1c-b1058e6fcecf", "embedding": null, "metadata": {"Header_2": " Conclusion", "filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ef14a4e-7e93-41ae-aada-058b036e40d7", "node_type": "4", "metadata": {"filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}, "hash": "adff9486539f8783cb7bd0696d17b40f22b60ff1d1acf2eaa43443d41078c908", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "540f71a9-0dd6-4e69-87fd-e6245685fe80", "node_type": "1", "metadata": {"Header_2": " **Implementation of Code**", "filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}, "hash": "3ebc42ec57a2bfd70fdf8a65670d399fd3d3adf39b12d9c2fc6e140afadcf001", "class_name": "RelatedNodeInfo"}}, "text": "Conclusion\n\nIn summary, the fusion of Zephyr 7b LLM and LlamaIndex initiates a new chapter\nin image-based document extraction. Beyond addressing OCR\u2019s inherent\nchallenges, it enhances the precision and efficiency of data extraction from\nimages, fostering improved productivity and decision-making in document-\nfocused workflows.\n\n\u201cStay connected and support my work through various platforms:\n\n  * [ GitHub ](https://medium.com/u/8df3bf3c40ae) : For all my open-source projects and Notebooks, you can visit my GitHub profile at [ https://github.com/andysingal ](https://github.com/andysingal) . If you find my content valuable, don\u2019t hesitate to leave a star. \n  * Patreon: If you\u2019d like to provide additional support, you can consider becoming a patron on my Patreon page at [ https://www.patreon.com/AndyShanu ](https://www.patreon.com/AndyShanu) . \n  * [ Medium ](https://medium.com/u/504c7870fdb6) : You can read my latest articles and insights on Medium at [ https://medium.com/@andysingal ](https://medium.com/@andysingal) . \n  * [ The Kaggle ](https://medium.com/u/29b47aa8cce3) : Check out my Kaggle profile for data science and machine learning projects at [ https://www.kaggle.com/alphasingal ](https://www.kaggle.com/alphasingal) . \n  * [ Hugging Face ](https://medium.com/u/b1574f0c6c5e) : For natural language processing and AI-related projects, you can explore my Huggingface profile at [ https://huggingface.co/Andyrasika ](https://huggingface.co/Andyrasika) . \n  * YouTube: To watch my video content, visit my YouTube channel at [ https://www.youtube.com/@andy111007 ](https://www.youtube.com/@andy111007) . \n  * LinkedIn: To stay updated on my latest projects and posts, you can follow me on LinkedIn. Here is the link to my profile: [ https://www.linkedin.com/in/ankushsingal/.\" ](https://www.linkedin.com/in/ankushsingal/.%22)\n\nRequests and questions: If you have a project in mind that you\u2019d like me to\nwork on or if you have any questions about the concepts I\u2019ve explained, don\u2019t\nhesitate to let me know. I\u2019m always looking for new ideas for future Notebooks\nand I love helping to resolve any doubts you might have.\n\nRemember, each \u201cLike\u201d, \u201cShare\u201d, and \u201cStar\u201d greatly contributes to my work and\nmotivates me to continue producing more quality content. Thank you for your\nsupport!\n\nIf you enjoyed this story, feel free [ to subscribe\n](https://medium.com/@andysingal) to Medium, and you will get notifications\nwhen my new articles will be published, as well as full access to thousands of\nstories from other authors.\n\nResource:\n\n  * [ Data used for above code ](https://github.com/andysingal/CV_public/tree/main/zephyr-7b-alpha)\n  * [ llama-index ](https://gpt-index.readthedocs.io/en/stable/)", "mimetype": "text/plain", "start_char_idx": 9304, "end_char_idx": 12017, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"38d07735-cff6-4aef-b129-68ddb644165f": {"doc_hash": "3fae0c0b8340a778482d2d26c03ec7c3dfc6f57b12b1a2c32ffa9dce925bf852", "ref_doc_id": "5ef14a4e-7e93-41ae-aada-058b036e40d7"}, "1436cdae-add0-48b9-a6b5-abbea8f8e2dd": {"doc_hash": "343a2a7ed379dc611d7cf8b2839f25be35772acd495a92524e1a2ff8d8d12de6", "ref_doc_id": "5ef14a4e-7e93-41ae-aada-058b036e40d7"}, "66ae644c-8487-45d6-ae0b-79af7ab62cc5": {"doc_hash": "f741523304827a6dd439b213537daad3bdb5ffe03f2579c2ed29057c1dbbad11", "ref_doc_id": "5ef14a4e-7e93-41ae-aada-058b036e40d7"}, "540f71a9-0dd6-4e69-87fd-e6245685fe80": {"doc_hash": "3ebc42ec57a2bfd70fdf8a65670d399fd3d3adf39b12d9c2fc6e140afadcf001", "ref_doc_id": "5ef14a4e-7e93-41ae-aada-058b036e40d7"}, "1b161277-3a79-4ad1-9a1c-b1058e6fcecf": {"doc_hash": "491a7f07d84e4b0cb3c3bb987b00527a4deebd908621e4ef4de7bce11c867210", "ref_doc_id": "5ef14a4e-7e93-41ae-aada-058b036e40d7"}}, "docstore/ref_doc_info": {"5ef14a4e-7e93-41ae-aada-058b036e40d7": {"node_ids": ["38d07735-cff6-4aef-b129-68ddb644165f", "1436cdae-add0-48b9-a6b5-abbea8f8e2dd", "66ae644c-8487-45d6-ae0b-79af7ab62cc5", "540f71a9-0dd6-4e69-87fd-e6245685fe80", "1b161277-3a79-4ad1-9a1c-b1058e6fcecf"], "metadata": {"Header_2": " **Introduction**", "filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}}}}