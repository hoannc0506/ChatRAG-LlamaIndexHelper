{"docstore/data": {"6c365e23-9b93-478c-82fc-fd026116679a": {"__data__": {"id_": "6c365e23-9b93-478c-82fc-fd026116679a", "embedding": null, "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db886fea-56cf-4e10-a649-5a3b9fda691b", "node_type": "4", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "7681e64ffab777e24150ac9a12ce2f5605fd961b8c339fe201dda2d5a3c2e1e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4db723e6-d9b8-4f6b-b666-3cf912921a1c", "node_type": "1", "metadata": {"Header_1": " Introduction"}, "hash": "696c1bfff7c475c7693e550d28157cde0e83d20cd84f14abaf6568646db548ca", "class_name": "RelatedNodeInfo"}}, "text": "Co-authors: Jerry Liu (CEO at LlamaIndex), Amog Kamsetty (Software Engineer at\nAnyscale)\n\n( **note:** this is cross-posted from the original blog post on Anyscale\u2019s\nwebsite. [ Check it out here ](https://www.anyscale.com/blog/build-and-scale-\na-powerful-query-engine-with-llamaindex-ray) !)\n\nIn this blog, we showcase how you can use LlamaIndex and Ray to build a query\nengine to answer questions and generate insights about Ray itself, given its\ndocumentation and blog posts.\n\nWe\u2019ll give a quick introduction of LlamaIndex + Ray, and then walk through a\nstep-by-step tutorial on building and deploying this query engine. We make use\nof both Ray Datasets to parallelize building indices as well as Ray Serve to\nbuild deployments.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 729, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4db723e6-d9b8-4f6b-b666-3cf912921a1c": {"__data__": {"id_": "4db723e6-d9b8-4f6b-b666-3cf912921a1c", "embedding": null, "metadata": {"Header_1": " Introduction", "filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db886fea-56cf-4e10-a649-5a3b9fda691b", "node_type": "4", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "7681e64ffab777e24150ac9a12ce2f5605fd961b8c339fe201dda2d5a3c2e1e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c365e23-9b93-478c-82fc-fd026116679a", "node_type": "1", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "316cb5f885f678437ef53f3c6ca701b3ec65fe2fd8e1cb65ed5548c4efe6d16e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82c4d045-6cd6-41b4-ba72-2765332096eb", "node_type": "1", "metadata": {"Header_1": " Data Ingestion and Embedding Pipeline"}, "hash": "505a6cc634e68954b6860dc59e57c847b6a0e43f675d3d85ae508ef7ebe171c1", "class_name": "RelatedNodeInfo"}}, "text": "Introduction\n\nLarge Language Models (LLMs) offer the promise of allowing users to extract\ncomplex insights from their unstructured text data. Retrieval-augmented\ngeneration pipelines have emerged as a common pattern for developing LLM\napplications allowing users to effectively perform semantic search over a\ncollection of documents.\n\n_Example of retrieval augmented generation. Relevant context is pulled from a\nset of documents and included in the LLM input prompt._\n\nHowever, when productionizing these applications over many different data\nsources, there are a few challenges:\n\n  1. Tooling for indexing data from many different data sources \n  2. Handling complex queries over different data sources \n  3. Scaling indexing to thousands or millions of documents \n  4. Deploying a scalable LLM application into production \n\nHere, we showcase how [ LlamaIndex ](https://gpt-\nindex.readthedocs.io/en/latest/) and [ Ray ](https://docs.ray.io/en/latest/)\nare the perfect setup for this task.\n\nLlamaIndex is a data framework for building LLM applications, and solves\nChallenges #1 and #2. It also provides a comprehensive toolkit allowing users\nto connect their private data with a language model. It offers a variety of\ntools to help users first ingest and index their data \u2014 convert different\nformats of unstructured and structured data into a format that the language\nmodel can use, and query their private data.\n\nRay is a powerful framework for scalable AI that solves Challenges #3 and #4.\nWe can use it to dramatically accelerate ingest, inference, pretraining, and\nalso effortlessly deploy and scale the query capabilities of LlamaIndex into\nthe cloud.\n\nMore specifically, we showcase a very relevant use case \u2014 highlighting Ray\nfeatures that are present in both the documentation as well as the Ray blog\nposts!", "mimetype": "text/plain", "start_char_idx": 734, "end_char_idx": 2550, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82c4d045-6cd6-41b4-ba72-2765332096eb": {"__data__": {"id_": "82c4d045-6cd6-41b4-ba72-2765332096eb", "embedding": null, "metadata": {"Header_1": " Data Ingestion and Embedding Pipeline", "filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db886fea-56cf-4e10-a649-5a3b9fda691b", "node_type": "4", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "7681e64ffab777e24150ac9a12ce2f5605fd961b8c339fe201dda2d5a3c2e1e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4db723e6-d9b8-4f6b-b666-3cf912921a1c", "node_type": "1", "metadata": {"Header_1": " Introduction", "filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "43cf94a4085cca9850e419371364c210c67b5b53ce7b325cd2fbc6b6db519c35", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f67a9ee1-e416-4566-83e0-a2f096630488", "node_type": "1", "metadata": {"Header_1": " Load Data"}, "hash": "5b045e254f1d3228d0b0798f8a9981edbaf5e1b5c98eb0cf7347f4d5da43e16a", "class_name": "RelatedNodeInfo"}}, "text": "Data Ingestion and Embedding Pipeline\n\nWe use LlamaIndex + Ray to ingest, parse, embed and store Ray docs and blog\nposts in a parallel fashion. For the most part, these steps are duplicated\nacross the two data sources, so we show the steps for just the documentation\nbelow.\n\nCode for this part of the blog is [ available here\n](https://github.com/amogkam/llama_index_ray/blob/main/create_vector_index.py)\n.\n\n_Sequential pipeline with \u201cingest\u201d, \u201cparse\u201d and \u201cembed\u201d stages. Files are\nprocessed sequentially resulting in poor hardware utilization and long\ncomputation time._ _Parallel pipeline. Thanks to Ray we can process multiple\ninput files simultaneously. Parallel processing has much better performance,\nbecause hardware is better utilized._", "mimetype": "text/plain", "start_char_idx": 2555, "end_char_idx": 3299, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f67a9ee1-e416-4566-83e0-a2f096630488": {"__data__": {"id_": "f67a9ee1-e416-4566-83e0-a2f096630488", "embedding": null, "metadata": {"Header_1": " Load Data", "filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db886fea-56cf-4e10-a649-5a3b9fda691b", "node_type": "4", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "7681e64ffab777e24150ac9a12ce2f5605fd961b8c339fe201dda2d5a3c2e1e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82c4d045-6cd6-41b4-ba72-2765332096eb", "node_type": "1", "metadata": {"Header_1": " Data Ingestion and Embedding Pipeline", "filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "218784aa4c161b6013eef6a7392a322132a498367f075eaff3101afdf4ba8a61", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bde5588b-ab43-44be-8861-26cde97797f7", "node_type": "1", "metadata": {"Header_1": " Load Data", "Header_2": " Scaling Data Ingest"}, "hash": "dc9af344602a9ccdb3d1e0ab7d1f5c37b0c6f1a8b6ac33cdf2be6e7c26f78e98", "class_name": "RelatedNodeInfo"}}, "text": "Load Data\n\nWe start by ingesting these two sources of data. We first fetch both data\nsources and download the HTML files.\n\nWe then need to load and parse these files. We can do this with the help of\nLlamaHub, our community-driven repository of 100+ data loaders from various\nAPI\u2019s, file formats (.pdf, .html, .docx), and databases. We use an HTML data\nloader offered by [ Unstructured ](https://github.com/Unstructured-\nIO/unstructured) .\n\n    \n    \n    from typing import Dict, List\n    from pathlib import Path\n    \n    from llama_index import download_loader\n    from llama_index import Document\n    \n    # Step 1: Logic for loading and parsing the files into llama_index documents.\n    UnstructuredReader = download_loader(\"UnstructuredReader\")\n    loader = UnstructuredReader()\n    \n    def load_and_parse_files(file_row: Dict[str, Path]) -&gt; Dict[str, Document]:\n        documents = []\n        file = file_row[\"path\"]\n        if file.is_dir():\n            return []\n        # Skip all non-html files like png, jpg, etc.\n        if file.suffix.lower() == \".html\":\n            loaded_doc = loader.load_data(file=file, split_documents=False)\n            loaded_doc[0].extra_info = {\"path\": str(file)}\n            documents.extend(loaded_doc)\n        return [{\"doc\": doc} for doc in documents]\n\nUnstructured offers a robust suite of parsing tools on top of various files.\nIt is able to help sanitize HTML documents by stripping out information like\ntags and formatting the text accordingly.", "mimetype": "text/plain", "start_char_idx": 3304, "end_char_idx": 4798, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bde5588b-ab43-44be-8861-26cde97797f7": {"__data__": {"id_": "bde5588b-ab43-44be-8861-26cde97797f7", "embedding": null, "metadata": {"Header_1": " Load Data", "Header_2": " Scaling Data Ingest", "filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db886fea-56cf-4e10-a649-5a3b9fda691b", "node_type": "4", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "7681e64ffab777e24150ac9a12ce2f5605fd961b8c339fe201dda2d5a3c2e1e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f67a9ee1-e416-4566-83e0-a2f096630488", "node_type": "1", "metadata": {"Header_1": " Load Data", "filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "28d6c62b3354c9951b8c8aced860e7f818581c5ca103cfde3eeeca710c65a524", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a78bcf8-8a41-4c1e-9329-b911ad37189d", "node_type": "1", "metadata": {"Header_1": " Parse Files"}, "hash": "87ca37a562cbcf389cfd22d718edf8250be1138191cd64906bff99b4277d025a", "class_name": "RelatedNodeInfo"}}, "text": "Scaling Data Ingest\n\nSince we have many HTML documents to process, loading/processing each one\nserially is inefficient and slow. This is an opportunity to use Ray and\ndistribute execution of the `load_and_parse_files` method across multiple CPUs\nor GPUs.\n\n    \n    \n    import ray\n    \n    # Get the paths for the locally downloaded documentation.\n    all_docs_gen = Path(\"./docs.ray.io/\").rglob(\"*\")\n    all_docs = [{\"path\": doc.resolve()} for doc in all_docs_gen]\n    \n    # Create the Ray Dataset pipeline\n    ds = ray.data.from_items(all_docs)\n    \n    # Use `flat_map` since there is a 1:N relationship.\n    # Each filepath returns multiple documents.\n    loaded_docs = ds.flat_map(load_and_parse_files)", "mimetype": "text/plain", "start_char_idx": 4804, "end_char_idx": 5512, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a78bcf8-8a41-4c1e-9329-b911ad37189d": {"__data__": {"id_": "7a78bcf8-8a41-4c1e-9329-b911ad37189d", "embedding": null, "metadata": {"Header_1": " Parse Files", "filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db886fea-56cf-4e10-a649-5a3b9fda691b", "node_type": "4", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "7681e64ffab777e24150ac9a12ce2f5605fd961b8c339fe201dda2d5a3c2e1e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bde5588b-ab43-44be-8861-26cde97797f7", "node_type": "1", "metadata": {"Header_1": " Load Data", "Header_2": " Scaling Data Ingest", "filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "d8bb126992073c90f5f3762c88165f063fb77bcd8a54fdbf671c4b8f32058d09", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d96292b8-5474-4de7-ac39-e6656f7b99a1", "node_type": "1", "metadata": {"Header_1": " Parse Files", "Header_2": " Run Parsing in Parallel"}, "hash": "d27fb83b91f82be8c0f10585f342a8e49f2e628ffa2b6aabc4653894daa577a7", "class_name": "RelatedNodeInfo"}}, "text": "Parse Files\n\nNow that we\u2019ve loaded the documents, the next step is to parse them into Node\nobjects \u2014 a \u201cNode\u201d object represents a more granular chunk of text, derived\nfrom the source documents. Node objects can be used in the input prompt as\ncontext; by setting a small enough chunk size, we can make sure that inserting\nNode objects do not overflow the context limits.\n\nWe define a function called `convert_documents_into_nodes` which converts\ndocuments into nodes using a simple text splitting strategy.\n\n    \n    \n    # Step 2: Convert the loaded documents into llama_index Nodes. This will split the documents into chunks.\n    from llama_index.node_parser import SimpleNodeParser\n    from llama_index.data_structs import Node\n    \n    def convert_documents_into_nodes(documents: Dict[str, Document]) -&gt; Dict[str, Node]:\n        parser = SimpleNodeParser()\n        document = documents[\"doc\"]\n        nodes = parser.get_nodes_from_documents([document]) \n        return [{\"node\": node} for node in nodes]", "mimetype": "text/plain", "start_char_idx": 5517, "end_char_idx": 6526, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d96292b8-5474-4de7-ac39-e6656f7b99a1": {"__data__": {"id_": "d96292b8-5474-4de7-ac39-e6656f7b99a1", "embedding": null, "metadata": {"Header_1": " Parse Files", "Header_2": " Run Parsing in Parallel", "filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db886fea-56cf-4e10-a649-5a3b9fda691b", "node_type": "4", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "7681e64ffab777e24150ac9a12ce2f5605fd961b8c339fe201dda2d5a3c2e1e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a78bcf8-8a41-4c1e-9329-b911ad37189d", "node_type": "1", "metadata": {"Header_1": " Parse Files", "filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "e56acc264d3696cd2cda596311490d1a9a45a150be931f3ed176fd45a4481c1b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35696564-db3c-4bcf-9e7c-2f5bab651e83", "node_type": "1", "metadata": {"Header_1": " Generate Embeddings"}, "hash": "b17e3b06d1790dd68875f941eef803832b0916f2e4b601b62f371e8898b814f6", "class_name": "RelatedNodeInfo"}}, "text": "Run Parsing in Parallel\n\nSince we have many documents, processing each document into nodes serially is\ninefficient and slow. We use Ray `flat_map` method to process documents into\nnodes in parallel:\n\n    \n    \n    # Use `flat_map` since there is a 1:N relationship. Each document returns multiple nodes.\n    nodes = loaded_docs.flat_map(convert_documents_into_nodes)", "mimetype": "text/plain", "start_char_idx": 6532, "end_char_idx": 6898, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35696564-db3c-4bcf-9e7c-2f5bab651e83": {"__data__": {"id_": "35696564-db3c-4bcf-9e7c-2f5bab651e83", "embedding": null, "metadata": {"Header_1": " Generate Embeddings", "filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db886fea-56cf-4e10-a649-5a3b9fda691b", "node_type": "4", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "7681e64ffab777e24150ac9a12ce2f5605fd961b8c339fe201dda2d5a3c2e1e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d96292b8-5474-4de7-ac39-e6656f7b99a1", "node_type": "1", "metadata": {"Header_1": " Parse Files", "Header_2": " Run Parsing in Parallel", "filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "3e75dbd9479c395a641f9dc19bb071c286c5d87b29884d3753e54961cc536af6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a88aa73-7ac1-4095-ae3e-d66496394a8b", "node_type": "1", "metadata": {"Header_1": " Data Indexing"}, "hash": "76e0957b14ccd53df7951473f4e2517897764b48f0348f75072414f03ad805b8", "class_name": "RelatedNodeInfo"}}, "text": "Generate Embeddings\n\nWe then generate embeddings for each Node using a Hugging Face Sentence\nTransformers model. We can do this with the help of LangChain\u2019s embedding\nabstraction.\n\nSimilar to document loading/parsing, embedding generation can similarly be\nparallelized with Ray. We wrap these embedding operations into a helper class,\ncalled `EmbedNodes`, to take advantage of Ray abstractions.\n\n    \n    \n    # Step 3: Embed each node using a local embedding model.\n    from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n    \n    class EmbedNodes:\n        def __init__(self):\n            self.embedding_model = HuggingFaceEmbeddings(\n                # Use all-mpnet-base-v2 Sentence_transformer.\n                # This is the default embedding model for LlamaIndex/Langchain.\n                model_name=\"sentence-transformers/all-mpnet-base-v2\", \n                model_kwargs={\"device\": \"cuda\"},\n                # Use GPU for embedding and specify a large enough batch size to maximize GPU utilization.\n                # Remove the \"device\": \"cuda\" to use CPU instead.\n                encode_kwargs={\"device\": \"cuda\", \"batch_size\": 100}\n                )\n    \n        def __call__(self, node_batch: Dict[str, List[Node]]) -&gt; Dict[str, List[Node]]:\n            nodes = node_batch[\"node\"]\n            text = [node.text for node in nodes]\n            embeddings = self.embedding_model.embed_documents(text)\n            assert len(nodes) == len(embeddings)\n    \n            for node, embedding in zip(nodes, embeddings):\n                node.embedding = embedding\n            return {\"embedded_nodes\": nodes}\n\nAfterwards, generating an embedding for each node is as simple as calling the\nfollowing operation in Ray:\n\n    \n    \n    # Use `map_batches` to specify a batch size to maximize GPU utilization.\n    # We define `EmbedNodes` as a class instead of a function so we only initialize the embedding model once. \n    \n    # This state can be reused for multiple batches.\n    embedded_nodes = nodes.map_batches(\n        EmbedNodes, \n        batch_size=100, \n        # Use 1 GPU per actor.\n        num_gpus=1,\n        # There are 4 GPUs in the cluster. Each actor uses 1 GPU. So we want 4 total actors.\n        compute=ActorPoolStrategy(size=4))\n    \n    # Step 5: Trigger execution and collect all the embedded nodes.\n    ray_docs_nodes = []\n    for row in embedded_nodes.iter_rows():\n        node = row[\"embedded_nodes\"]\n        assert node.embedding is not None\n        ray_docs_nodes.append(node)", "mimetype": "text/plain", "start_char_idx": 6903, "end_char_idx": 9419, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a88aa73-7ac1-4095-ae3e-d66496394a8b": {"__data__": {"id_": "0a88aa73-7ac1-4095-ae3e-d66496394a8b", "embedding": null, "metadata": {"Header_1": " Data Indexing", "filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db886fea-56cf-4e10-a649-5a3b9fda691b", "node_type": "4", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "7681e64ffab777e24150ac9a12ce2f5605fd961b8c339fe201dda2d5a3c2e1e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35696564-db3c-4bcf-9e7c-2f5bab651e83", "node_type": "1", "metadata": {"Header_1": " Generate Embeddings", "filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "c5602b12e5a2c60ef7d78246da107069825352bce324426997154df0468bcce8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b9ebeb9-4819-4136-942e-b17fa8b825b1", "node_type": "1", "metadata": {"Header_1": " Data Querying"}, "hash": "d9e110457e7d2a7006e1d14e6adb2f3a7945814fc2a7f5d36be4c72b5cc966d6", "class_name": "RelatedNodeInfo"}}, "text": "Data Indexing\n\nThe next step is to store these nodes within an \u201cindex\u201d in LlamaIndex. An\nindex is a core abstraction in LlamaIndex to \u201cstructure\u201d your data in a\ncertain way \u2014 this structure can then be used for downstream LLM retrieval +\nquerying. An index can interface with a storage or vector store abstraction.\n\nThe most commonly used index abstraction within LlamaIndex is our vector\nindex, where each node is stored along with an embedding. In this example, we\nuse a simple in-memory vector store, but you can also choose to specify any\none of LlamaIndex\u2019s 10+ vector store integrations as the storage provider\n(e.g. Pinecone, Weaviate, Chroma).\n\nWe build two vector indices: one over the documentation nodes, and another\nover the blog post nodes and persist them to disk. Code is [ available here\n](https://github.com/amogkam/llama_index_ray/blob/main/create_vector_index.py#L102:L131)\n.\n\n    \n    \n    from llama_index import GPTVectorStoreIndex\n    \n    # Store Ray Documentation embeddings\n    ray_docs_index = GPTVectorStoreIndex(nodes=ray_docs_nodes)\n    ray_docs_index.storage_context.persist(persist_dir=\"/tmp/ray_docs_index\")\n    \n    # Store Anyscale blog post embeddings\n    ray_blogs_index = GPTVectorStoreIndex(nodes=ray_blogs_nodes)\n    ray_blogs_index.storage_context.persist(persist_dir=\"/tmp/ray_blogs_index\")\n\n**That\u2019s it in terms of building a data pipeline using LlamaIndex + Ray Data**\n!\n\nYour data is now ready to be used within your LLM application. Check out our\nnext section for how to use advanced LlamaIndex query capabilities on top of\nyour data.", "mimetype": "text/plain", "start_char_idx": 9424, "end_char_idx": 11004, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b9ebeb9-4819-4136-942e-b17fa8b825b1": {"__data__": {"id_": "4b9ebeb9-4819-4136-942e-b17fa8b825b1", "embedding": null, "metadata": {"Header_1": " Data Querying", "filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db886fea-56cf-4e10-a649-5a3b9fda691b", "node_type": "4", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "7681e64ffab777e24150ac9a12ce2f5605fd961b8c339fe201dda2d5a3c2e1e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a88aa73-7ac1-4095-ae3e-d66496394a8b", "node_type": "1", "metadata": {"Header_1": " Data Indexing", "filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "14730a25dff8686a2b322ba025431270d1eae60a0cd8c0ea5a8a3000df3e114b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8446edd8-88a0-424d-a835-3e7f56eef755", "node_type": "1", "metadata": {"Header_1": " Deploying with Ray Serve"}, "hash": "543d2193f7b809085d416f4fbb1ed2ef12355a9644b9054c5d426cc30935f7c3", "class_name": "RelatedNodeInfo"}}, "text": "Data Querying\n\nLlamaIndex provides both simple and advanced query capabilities on top of your\ndata + indices. The central abstraction within LlamaIndex is called a \u201cquery\nengine.\u201d A query engine takes in a natural language query input and returns a\nnatural language \u201coutput\u201d. Each index has a \u201cdefault\u201d corresponding query\nengine. For instance, the default query engine for a vector index first\nperforms top-k retrieval over the vector store to fetch the most relevant\ndocuments.\n\nThese query engines can be easily derived from each index:\n\n    \n    \n    ray_docs_engine = ray_docs_index.as_query_engine(similarity_top_k=5, service_context=service_context)\n    \n    ray_blogs_engine = ray_blogs_index.as_query_engine(similarity_top_k=5, service_context=service_context)\n\nLlamaIndex also provides more advanced query engines for multi-document use\ncases \u2014 for instance, we may want to ask how a given feature in Ray is\nhighlighted in both the documentation and blog. `SubQuestionQueryEngine` can\ntake in other query engines as input. Given an existing question, it can\ndecide to break down the question into simpler questions over any subset of\nquery engines; it will execute the simpler questions and combine results at\nthe top-level.\n\nThis abstraction is quite powerful; it can perform semantic search over one\ndocument, or combine results across multiple documents.\n\nFor instance, given the following question \u201cWhat is Ray?\u201d, we can break this\ninto sub-questions \u201cWhat is Ray according to the documentation\u201d, and \u201cWhat is\nRay according to the blog posts\u201d over the document query engine and blog query\nengine respectively.\n\n    \n    \n    # Define a sub-question query engine, that can use the individual query engines as tools.\n            query_engine_tools = [\n                QueryEngineTool(\n                    query_engine=self.ray_docs_engine,\n                    metadata=ToolMetadata(name=\"ray_docs_engine\", description=\"Provides information about the Ray documentation\")\n                ),\n                QueryEngineTool(\n                    query_engine=self.ray_blogs_engine, \n                    metadata=ToolMetadata(name=\"ray_blogs_engine\", description=\"Provides information about Ray blog posts\")\n                ),\n            ]\n    \n    sub_query_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools, service_context=service_context, use_async=False)\n\nHave a look at [ deploy_app.py\n](https://github.com/amogkam/llama_index_ray/blob/main/deploy_app.py#L22:L56)\nto review the full implementation.", "mimetype": "text/plain", "start_char_idx": 11009, "end_char_idx": 13555, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8446edd8-88a0-424d-a835-3e7f56eef755": {"__data__": {"id_": "8446edd8-88a0-424d-a835-3e7f56eef755", "embedding": null, "metadata": {"Header_1": " Deploying with Ray Serve", "filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db886fea-56cf-4e10-a649-5a3b9fda691b", "node_type": "4", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "7681e64ffab777e24150ac9a12ce2f5605fd961b8c339fe201dda2d5a3c2e1e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b9ebeb9-4819-4136-942e-b17fa8b825b1", "node_type": "1", "metadata": {"Header_1": " Data Querying", "filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "726032b958e6cca08493a399bd1f479df7b359c8d6ee791b4a3bd90f77c67d30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05f6ae0a-2982-4f6e-91f0-0ffcf6316d24", "node_type": "1", "metadata": {"Header_1": " Example Queries"}, "hash": "7ff85005f3164ca0e2692d99cf126957c111df57ca6b4f8dfd930e3ba3b9382e", "class_name": "RelatedNodeInfo"}}, "text": "Deploying with Ray Serve\n\nWe\u2019ve now created an incredibly powerful query module over your data. As a\nnext step, what if we could seamlessly deploy this function to production and\nserve users? Ray Serve makes this incredibly easy to do. Ray Serve is a\nscalable compute layer for serving ML models and LLMs that enables serving\nindividual models or creating composite model pipelines where you can\nindependently deploy, update, and scale individual components.\n\nTo do this, you just need to do the following steps:\n\n  1. Define an outer class that can \u201cwrap\u201d a query engine, and expose a \u201cquery\u201d endpoint \n  2. Add a `@ray.serve.deployment` decorator on this class \n  3. Deploy the Ray Serve application \n\nIt will look something like the following:\n\n    \n    \n    from ray import serve\n    \n    @serve.deployment\n    class QADeployment:\n        def __init__(self):\n     self.query_engine = ...\n    \n        def query(self, query: str):\n                response =  self.query_engine.query(query)\n                source_nodes = response.source_nodes\n                source_str = \"\"\n                for i in range(len(source_nodes)):\n                    node = source_nodes[i]\n                    source_str += f\"Sub-question {i+1}:\\n\"\n                    source_str += node.node.text\n                    source_str += \"\\n\\n\"\n                return f\"Response: {str(response)} \\n\\n\\n {source_str}\\n\"\n    \n        async def __call__(self, request: Request):\n            query = request.query_params[\"query\"]\n            return str(self.query(query))\n    \n    # Deploy the Ray Serve application.\n    deployment = QADeployment.bind()\n\nHave a look at the [ deploy_app.py\n](https://github.com/amogkam/llama_index_ray/blob/main/deploy_app.py) for full\nimplementation.", "mimetype": "text/plain", "start_char_idx": 13560, "end_char_idx": 15316, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05f6ae0a-2982-4f6e-91f0-0ffcf6316d24": {"__data__": {"id_": "05f6ae0a-2982-4f6e-91f0-0ffcf6316d24", "embedding": null, "metadata": {"Header_1": " Example Queries", "filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db886fea-56cf-4e10-a649-5a3b9fda691b", "node_type": "4", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "7681e64ffab777e24150ac9a12ce2f5605fd961b8c339fe201dda2d5a3c2e1e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8446edd8-88a0-424d-a835-3e7f56eef755", "node_type": "1", "metadata": {"Header_1": " Deploying with Ray Serve", "filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "a9eb97b74d8d851131472deddc47e548044b3e0da1ab78338fd05acad8e0ceef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94d02d68-917f-44ac-b79e-69a781520c73", "node_type": "1", "metadata": {"Header_1": " Conclusion"}, "hash": "6d4fff116e0e4b0f3fb6e388a7e301d9084ab43bb2d20333ede07454f7867d3c", "class_name": "RelatedNodeInfo"}}, "text": "Example Queries\n\nOnce we\u2019ve deployed the application, we can query it with questions about Ray.\n\nWe can query just one of the data sources:\n\n    \n    \n    Q: \"What is Ray Serve?\"\n    \n    Ray Serve is a system for deploying and managing applications on a Ray\n    cluster. It provides APIs for deploying applications, managing replicas, and\n    making requests to applications. It also provides a command line interface\n    (CLI) for managing applications and a dashboard for monitoring applications.\n\nBut, we can also provide complex queries that require synthesis across both\nthe documentation and the blog posts. These complex queries are easily handled\nby the subquestion-query engine that we defined.\n\n    \n    \n    Q: \"Compare and contrast how the Ray docs and the Ray blogs present Ray Serve\"\n    \n    Response: \n    The Ray docs and the Ray blogs both present Ray Serve as a web interface\n    that provides metrics, charts, and other features to help Ray users\n    understand and debug Ray applications. However, the Ray docs provide more\n    detailed information, such as a Quick Start guide, user guide, production\n    guide, performance tuning guide, development workflow guide, API reference,\n    experimental Java API, and experimental gRPC support. Additionally, the Ray\n    docs provide a guide for migrating from 1.x to 2.x. On the other hand, the\n    Ray blogs provide a Quick Start guide, a User Guide, and Advanced Guides to\n    help users get started and understand the features of Ray Serve.\n    Additionally, the Ray blogs provide examples and use cases to help users\n    understand how to use Ray Serve in their own projects.\n    \n    ---\n    \n    Sub-question 1\n    \n    Sub question: How does the Ray docs present Ray Serve\n    \n    Response: \n    The Ray docs present Ray Serve as a web interface that provides metrics,\n    charts, and other features to help Ray users understand and debug Ray\n    applications. It provides a Quick Start guide, user guide, production guide,\n    performance tuning guide, and development workflow guide. It also provides\n    an API reference, experimental Java API, and experimental gRPC support.\n    Finally, it provides a guide for migrating from 1.x to 2.x.\n    \n    ---\n    \n    Sub-question 2\n    \n    Sub question: How does the Ray blogs present Ray Serve\n    \n    Response: \n    The Ray blog presents Ray Serve as a framework for distributed applications\n    that enables users to handle HTTP requests, scale and allocate resources,\n    compose models, and more. It provides a Quick Start guide, a User Guide, and\n    Advanced Guides to help users get started and understand the features of Ray\n    Serve. Additionally, it provides examples and use cases to help users\n    understand how to use Ray Serve in their own projects.", "mimetype": "text/plain", "start_char_idx": 15321, "end_char_idx": 18113, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94d02d68-917f-44ac-b79e-69a781520c73": {"__data__": {"id_": "94d02d68-917f-44ac-b79e-69a781520c73", "embedding": null, "metadata": {"Header_1": " Conclusion", "filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db886fea-56cf-4e10-a649-5a3b9fda691b", "node_type": "4", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "7681e64ffab777e24150ac9a12ce2f5605fd961b8c339fe201dda2d5a3c2e1e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05f6ae0a-2982-4f6e-91f0-0ffcf6316d24", "node_type": "1", "metadata": {"Header_1": " Example Queries", "filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "70e300e8b77048917842b466a3c871b2c1c562b2f8e5b9db01d8fec299e62674", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2279d8cc-966c-4fc1-b5bd-947000235352", "node_type": "1", "metadata": {"Header_1": " What\u2019s next?"}, "hash": "960e7e75daf49ad8459f131555695348be5f42443895bbe12f4d4a29d5a849d6", "class_name": "RelatedNodeInfo"}}, "text": "Conclusion\n\nIn this example, we showed how you can build a scalable data pipeline and a\npowerful query engine using LlamaIndex + Ray. We also demonstrated how to\ndeploy LlamaIndex applications using Ray Serve. This allows you to\neffortlessly ask questions and synthesize insights about Ray across disparate\ndata sources!\n\nWe used LlamaIndex \u2014 a data framework for building LLM applications \u2014 to load,\nparse, embed and index the data. We ensured efficient and fast parallel\nexecution by using Ray. Then, we used LlamaIndex querying capabilities to\nperform semantic search over a single document, or combine results across\nmultiple documents. Finally, we used Ray Serve to package the application for\nproduction use.\n\nImplementation in open source, code is available on GitHub: [ LlamaIndex-Ray-\napp ](https://github.com/amogkam/llama_index_ray)", "mimetype": "text/plain", "start_char_idx": 18118, "end_char_idx": 18961, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2279d8cc-966c-4fc1-b5bd-947000235352": {"__data__": {"id_": "2279d8cc-966c-4fc1-b5bd-947000235352", "embedding": null, "metadata": {"Header_1": " What\u2019s next?", "filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db886fea-56cf-4e10-a649-5a3b9fda691b", "node_type": "4", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "7681e64ffab777e24150ac9a12ce2f5605fd961b8c339fe201dda2d5a3c2e1e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94d02d68-917f-44ac-b79e-69a781520c73", "node_type": "1", "metadata": {"Header_1": " Conclusion", "filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "de7d4f1e32bb600efd5cfadf457f85637401744e7d5df2829d70bc528eb7aa34", "class_name": "RelatedNodeInfo"}}, "text": "What\u2019s next?\n\nVisit LlamaIndex [ site ](https://www.llamaindex.ai/) and [ docs\n](https://gpt-index.readthedocs.io/en/latest/) to learn more about this data\nframework for building LLM applications.\n\nVisit [ Ray docs ](https://docs.ray.io/en/latest/ray-overview/use-\ncases.html#llms-and-gen-ai) to learn more about how to build and deploy\nscalable LLM apps.\n\nJoin our communities!\n\n  * [ Join Ray community ](https://forms.gle/9TSdDYUgxYs8SA9e8) on Slack and Ray #LLM channel. \n  * You can also join the LlamaIndex [ community on discord ](https://discord.gg/UB58qbeq) . \n\nWe have our [ Ray Summit 2023 ](https://raysummit.anyscale.com/) early-bird\nregistration open until 6/30. Secure your spot, save some money, savor the\ncommunity camaraderie at the summit.", "mimetype": "text/plain", "start_char_idx": 18966, "end_char_idx": 19724, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"6c365e23-9b93-478c-82fc-fd026116679a": {"doc_hash": "316cb5f885f678437ef53f3c6ca701b3ec65fe2fd8e1cb65ed5548c4efe6d16e", "ref_doc_id": "db886fea-56cf-4e10-a649-5a3b9fda691b"}, "4db723e6-d9b8-4f6b-b666-3cf912921a1c": {"doc_hash": "43cf94a4085cca9850e419371364c210c67b5b53ce7b325cd2fbc6b6db519c35", "ref_doc_id": "db886fea-56cf-4e10-a649-5a3b9fda691b"}, "82c4d045-6cd6-41b4-ba72-2765332096eb": {"doc_hash": "218784aa4c161b6013eef6a7392a322132a498367f075eaff3101afdf4ba8a61", "ref_doc_id": "db886fea-56cf-4e10-a649-5a3b9fda691b"}, "f67a9ee1-e416-4566-83e0-a2f096630488": {"doc_hash": "28d6c62b3354c9951b8c8aced860e7f818581c5ca103cfde3eeeca710c65a524", "ref_doc_id": "db886fea-56cf-4e10-a649-5a3b9fda691b"}, "bde5588b-ab43-44be-8861-26cde97797f7": {"doc_hash": "d8bb126992073c90f5f3762c88165f063fb77bcd8a54fdbf671c4b8f32058d09", "ref_doc_id": "db886fea-56cf-4e10-a649-5a3b9fda691b"}, "7a78bcf8-8a41-4c1e-9329-b911ad37189d": {"doc_hash": "e56acc264d3696cd2cda596311490d1a9a45a150be931f3ed176fd45a4481c1b", "ref_doc_id": "db886fea-56cf-4e10-a649-5a3b9fda691b"}, "d96292b8-5474-4de7-ac39-e6656f7b99a1": {"doc_hash": "3e75dbd9479c395a641f9dc19bb071c286c5d87b29884d3753e54961cc536af6", "ref_doc_id": "db886fea-56cf-4e10-a649-5a3b9fda691b"}, "35696564-db3c-4bcf-9e7c-2f5bab651e83": {"doc_hash": "c5602b12e5a2c60ef7d78246da107069825352bce324426997154df0468bcce8", "ref_doc_id": "db886fea-56cf-4e10-a649-5a3b9fda691b"}, "0a88aa73-7ac1-4095-ae3e-d66496394a8b": {"doc_hash": "14730a25dff8686a2b322ba025431270d1eae60a0cd8c0ea5a8a3000df3e114b", "ref_doc_id": "db886fea-56cf-4e10-a649-5a3b9fda691b"}, "4b9ebeb9-4819-4136-942e-b17fa8b825b1": {"doc_hash": "726032b958e6cca08493a399bd1f479df7b359c8d6ee791b4a3bd90f77c67d30", "ref_doc_id": "db886fea-56cf-4e10-a649-5a3b9fda691b"}, "8446edd8-88a0-424d-a835-3e7f56eef755": {"doc_hash": "a9eb97b74d8d851131472deddc47e548044b3e0da1ab78338fd05acad8e0ceef", "ref_doc_id": "db886fea-56cf-4e10-a649-5a3b9fda691b"}, "05f6ae0a-2982-4f6e-91f0-0ffcf6316d24": {"doc_hash": "70e300e8b77048917842b466a3c871b2c1c562b2f8e5b9db01d8fec299e62674", "ref_doc_id": "db886fea-56cf-4e10-a649-5a3b9fda691b"}, "94d02d68-917f-44ac-b79e-69a781520c73": {"doc_hash": "de7d4f1e32bb600efd5cfadf457f85637401744e7d5df2829d70bc528eb7aa34", "ref_doc_id": "db886fea-56cf-4e10-a649-5a3b9fda691b"}, "2279d8cc-966c-4fc1-b5bd-947000235352": {"doc_hash": "9bb22d1334f5714a60b3921644dc3c9fa8ca3ac241c09c7c3e775e233ddbb407", "ref_doc_id": "db886fea-56cf-4e10-a649-5a3b9fda691b"}}, "docstore/ref_doc_info": {"db886fea-56cf-4e10-a649-5a3b9fda691b": {"node_ids": ["6c365e23-9b93-478c-82fc-fd026116679a", "4db723e6-d9b8-4f6b-b666-3cf912921a1c", "82c4d045-6cd6-41b4-ba72-2765332096eb", "f67a9ee1-e416-4566-83e0-a2f096630488", "bde5588b-ab43-44be-8861-26cde97797f7", "7a78bcf8-8a41-4c1e-9329-b911ad37189d", "d96292b8-5474-4de7-ac39-e6656f7b99a1", "35696564-db3c-4bcf-9e7c-2f5bab651e83", "0a88aa73-7ac1-4095-ae3e-d66496394a8b", "4b9ebeb9-4819-4136-942e-b17fa8b825b1", "8446edd8-88a0-424d-a835-3e7f56eef755", "05f6ae0a-2982-4f6e-91f0-0ffcf6316d24", "94d02d68-917f-44ac-b79e-69a781520c73", "2279d8cc-966c-4fc1-b5bd-947000235352"], "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}}}}