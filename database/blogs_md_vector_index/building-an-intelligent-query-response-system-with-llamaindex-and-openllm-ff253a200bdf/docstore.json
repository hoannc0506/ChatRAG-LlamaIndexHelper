{"docstore/data": {"a720dbbc-269c-476c-856b-d8dd0a603a38": {"__data__": {"id_": "a720dbbc-269c-476c-856b-d8dd0a603a38", "embedding": null, "metadata": {"filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "32940c64-9a6d-4478-947a-e29fb85ab016", "node_type": "4", "metadata": {"filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "hash": "cc550ed7a5ae4885396bdc46e84eb7813dc58af8619e23e1ca18efac283d7302", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ccc8e7e-cd94-4606-a7ea-55060bd080dc", "node_type": "1", "metadata": {"Header_1": " Setting up the environment"}, "hash": "f7792840d60784c450d1898f1384212525419e766deff5e817a0dda0e0de7dad", "class_name": "RelatedNodeInfo"}}, "text": "Over the past year, Large Language Models (LLMs) like GPT-4 have not only\ntransformed how we interact with machines but also have redefined the\npossibilities within the realm of natural language processing (NLP). A notable\ntrend in this evolution is the increasing popularity of open-source LLMs like\nLlama 2, Falcon, OPT and Yi. Some may prefer them over their commercial\ncounterparts in terms of accessibility, data security and privacy,\ncustomization potential, cost, and vendor dependency. Among the tools gaining\nincreasing traction in the LLM space are OpenLLM and LlamaIndex \u2014 two powerful\nplatforms that, when combined, unlock new use cases for building AI-driven\napplications.\n\n[ OpenLLM ](https://github.com/bentoml/OpenLLM) is an open-source platform for\ndeploying and operating any open-source LLMs in production. Its flexibility\nand ease of use make it an ideal choice for AI application developers seeking\nto harness the power of LLMs. You can easily fine-tune, serve, deploy, and\nmonitor LLMs in a wide range of creative and practical applications.\n\n[ LlamaIndex ](https://github.com/run-llama/llama_index) provides a\ncomprehensive framework for managing and retrieving private and domain-\nspecific data. It acts as a bridge between the extensive knowledge of LLMs and\nthe unique, contextual data needs of specific applications.\n\nOpenLLM\u2019s support for a diverse range of open-source LLMs and LlamaIndex\u2019s\nability to seamlessly integrate custom data sources provide great\ncustomization for developers in both communities. This combination allows them\nto create AI solutions that are both highly intelligent and properly tailored\nto specific data contexts, which is very important for query-response systems.\n\nIn this blog post, I will explain how you can leverage the combined strengths\nof OpenLLM and LlamaIndex to build an intelligent query-response system. This\nsystem can understand, process, and respond to queries by tapping into a\ncustom corpus.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1966, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ccc8e7e-cd94-4606-a7ea-55060bd080dc": {"__data__": {"id_": "4ccc8e7e-cd94-4606-a7ea-55060bd080dc", "embedding": null, "metadata": {"Header_1": " Setting up the environment", "filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "32940c64-9a6d-4478-947a-e29fb85ab016", "node_type": "4", "metadata": {"filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "hash": "cc550ed7a5ae4885396bdc46e84eb7813dc58af8619e23e1ca18efac283d7302", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a720dbbc-269c-476c-856b-d8dd0a603a38", "node_type": "1", "metadata": {"filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "hash": "8ad4474bd9c887bdb63af6019cb8fabee5b4084982786d8f10b4a57c1e982732", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "117c1c67-1809-4380-8211-7333277af27f", "node_type": "1", "metadata": {"Header_1": " v1: Creating a simple completion service"}, "hash": "d465ffe86ed3844758797c17a37e81ce5600bb727904cb49d927db2a40b00897", "class_name": "RelatedNodeInfo"}}, "text": "Setting up the environment\n\nThe first step is to create a virtual environment in your machine, which helps\nprevent conflicts with other Python projects you might be working on. Let\u2019s\njust call it ` llamaindex-openllm ` and activate it.\n\n    \n    \n    python -m venv llamaindex-openllm\n    source llamaindex-openllm/bin/activate\n\nInstall the required packages. This command installs OpenLLM with the optional\n` vllm ` component (I will explain it later).\n\n    \n    \n    pip install \"openllm[vllm]\" llama-index llama-index-llms-openllm llama-index-embeddings-huggingface\n\nFor handling requests, you need to have an LLM server. Here, I use the\nfollowing command to start a Llama 2 7B local server at [\nhttp://localhost:3000 ](http://localhost:3000) . Feel free to choose any model\nthat fits your needs. If you already have a remote LLM server, you can skip\nthis step.\n\n    \n    \n    openllm start meta-llama/Llama-2-7b-chat-hf --backend vllm\n\nOpenLLM automatically selects the most suitable runtime implementation for the\nmodel. For models with vLLM support, OpenLLM uses vLLM by default. Otherwise,\nit falls back to PyTorch. vLLM is a high-throughput and memory-efficient\ninference and serving engine for LLMs. According to [ this report\n](https://www.anyscale.com/blog/continuous-batching-llm-inference) , you can\nachieve 23x LLM inference throughput while reducing P50 latency using vLLM.\n\n> **_Note_ ** _: To use the vLLM backend, you need a GPU with at least the\n> Ampere architecture (or newer) and CUDA version 11.8. This demo uses a\n> machine with an Ampere A100\u201380G GPU. If your machine has a compatible GPU,\n> you can also choose vLLM. Otherwise, simply install the standard OpenLLM\n> package (_ ` _pip install openllm_ ` _) in the previous command._", "mimetype": "text/plain", "start_char_idx": 1971, "end_char_idx": 3728, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "117c1c67-1809-4380-8211-7333277af27f": {"__data__": {"id_": "117c1c67-1809-4380-8211-7333277af27f", "embedding": null, "metadata": {"Header_1": " v1: Creating a simple completion service", "filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "32940c64-9a6d-4478-947a-e29fb85ab016", "node_type": "4", "metadata": {"filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "hash": "cc550ed7a5ae4885396bdc46e84eb7813dc58af8619e23e1ca18efac283d7302", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ccc8e7e-cd94-4606-a7ea-55060bd080dc", "node_type": "1", "metadata": {"Header_1": " Setting up the environment", "filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "hash": "b81e4448551cc7c0c2c7b8dc98f13b75e70d561eece7efc5f2bea3ffd72ab03b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29dacc6b-8562-4e45-b3e0-a49221bc7476", "node_type": "1", "metadata": {"Header_1": " v2: Enhancing with a query-response system"}, "hash": "d683bc48ab3a1827b32dc21d4edec374423465a96497c6664e5f37bd5b092da6", "class_name": "RelatedNodeInfo"}}, "text": "v1: Creating a simple completion service\n\nBefore building a query-response system, let\u2019s get familiar with the\nintegration of OpenLLM and LlamaIndex and use it to create a simple completion\nservice.\n\nThe integration offers two APIs for interactions with LLMs:\n\n1\\. ` OpenLLM ` : This can be used to initiate a local LLM server directly\nwithout the need for starting a separate one using commands like ` openllm\nstart ` . Here\u2019s how you can use it:\n\n    \n    \n    from llama_index.llms.openllm import OpenLLM\n    llm = OpenLLM('meta-llama/Llama-2-7b-chat-hf')\n\n2\\. ` OpenLLMAPI ` : This can be used to interact with a server hosted\nelsewhere, like the Llama 2 7B model I started previously.\n\nLet\u2019s try the ` complete ` endpoint and see if the Llama 2 7B model is able to\ntell what OpenLLM is by completing the sentence \u201cOpenLLM is an open source\ntool for\u201d.\n\n    \n    \n    from llama_index.llms.openllm import OpenLLMAPI\n    \n    remote_llm = OpenLLMAPI(address=\"http://localhost:3000\")\n    \n    completion_response = remote_llm.complete(\"OpenLLM is an open source tool for\", max_new_tokens=1024)\n    print(completion_response)\n\nRun this script and here is the output:\n\n    \n    \n    learning lifelong learning models. It is designed to be easy to use, even for those without extensive knowledge of machine learning. OpenLLM allows users to train, evaluate, and deploy lifelong learning models using a variety of datasets and algorithms.\n    \n    OpenLLM provides a number of features that make it useful for learning lifelong learning models. Some of these features include:\n    \n    1. Easy-to-use interface: OpenLLM provides an easy-to-use interface that makes it simple to train, evaluate, and deploy lifelong learning models.\n    2. Support for a variety of datasets: OpenLLM supports a variety of datasets, including images, text, and time-series data.\n    3. Support for a variety of algorithms: OpenLLM supports a variety of algorithms for lifelong learning, including neural networks, decision trees, and support vector machines.\n    4. Evaluation tools: OpenLLM provides a number of evaluation tools that allow users to assess the performance of their lifelong learning models.\n    5. Deployment tools: OpenLLM provides a number of deployment tools that allow users to deploy their lifelong learning models in a variety of environments.\n    \n    OpenLLM is written in Python and is available under an open source license. It is designed to be used in a variety of settings, including research, education, and industry.\n    \n    Some potential use cases for OpenLLM include:\n    \n    1. Training lifelong learning models for image classification: OpenLLM could be used to train a lifelong learning model to classify images based on their content.\n    2. Training lifelong learning models for natural language processing: OpenLLM could be used to train a lifelong learning model to process and analyze natural language text.\n    3. Training lifelong learning models for time-series data: OpenLLM could be used to train a lifelong learning model to predict future values in a time-series dataset.\n    4. Deploying lifelong learning models in a production environment: OpenLLM could be used to deploy a lifelong learning model in a production environment, such as a recommendation system or a fraud detection system.\n    \n    Overall, OpenLLM is a powerful tool for learning lifelong learning models. Its ease of use, flexibility, and support for a variety of datasets and algorithms make it a valuable resource for researchers and practitioners in a variety of fields.\n\nObviously, the model couldn\u2019t correctly explain OpenLLM with some\nhallucinations . Nevertheless, the code works well as the server outputs a\nresponse for the request. This is a good start as we proceed with building our\nsystem.", "mimetype": "text/plain", "start_char_idx": 3733, "end_char_idx": 7535, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29dacc6b-8562-4e45-b3e0-a49221bc7476": {"__data__": {"id_": "29dacc6b-8562-4e45-b3e0-a49221bc7476", "embedding": null, "metadata": {"Header_1": " v2: Enhancing with a query-response system", "filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "32940c64-9a6d-4478-947a-e29fb85ab016", "node_type": "4", "metadata": {"filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "hash": "cc550ed7a5ae4885396bdc46e84eb7813dc58af8619e23e1ca18efac283d7302", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "117c1c67-1809-4380-8211-7333277af27f", "node_type": "1", "metadata": {"Header_1": " v1: Creating a simple completion service", "filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "hash": "b24a133e561fd9288ca202ccfd77e4f77e43fe8dc2b0b2e58399b4a6f484b812", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7df20803-2522-488d-8905-7d9d4fc2d87e", "node_type": "1", "metadata": {"Header_1": " Conclusion"}, "hash": "042af03fefc28dd92a991eb7b77356d847b13a47ba86143b5d600fd07210a47a", "class_name": "RelatedNodeInfo"}}, "text": "v2: Enhancing with a query-response system\n\nThe initial version revealed a key limitation: the model\u2019s lack of specific\nknowledge about OpenLLM. One solution is to feed the model with domain-\nspecific information, allowing it to learn and respond according to topic-\nspecific queries. This is where LlamaIndex comes into play, enabling you to\nbuild a local knowledge base with pertinent information. Specifically, you\ncreate a directory (for example, ` data ` ) and build an index for all the\ndocuments in the folder.\n\nCreate a folder and let\u2019s import the GitHub README file of OpenLLM into the\nfolder:\n\n    \n    \n    mkdir data\n    cd data\n    wget https://github.com/bentoml/OpenLLM/blob/main/README.md\n\nGo back to the previous directory and create a script called ` starter.py `\nlike the following:\n\n    \n    \n    from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n    from llama_index.llms.openllm import OpenLLMAPI\n    from llama_index.core.node_parser import SentenceSplitter\n    \n    # Change the address to your OpenLLM server\n    llm = OpenLLMAPI(address=\"http://localhost:3000\")\n    \n    # Break down the document into manageable chunks (each of size 1024 characters, with a 20-character overlap)\n    text_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n    \n    # Create a ServiceContext with the custom model and all the configurations\n    service_context = ServiceContext.from_defaults(\n        llm=llm,\n        embed_model=\"local\",\n        text_splitter=text_splitter,\n        context_window=8192,\n        num_output=4096,\n    )\n    \n    # Load documents from the data directory\n    documents = SimpleDirectoryReader(\"data\").load_data()\n    \n    # Build an index over the documents using the customized LLM in the ServiceContext\n    index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n    \n    # Query your data using the built index\n    query_engine = index.as_query_engine()\n    response = query_engine.query(\"What is OpenLLM?\")\n    print(response)\n\nTo improve the quality of your response, I recommend you define a `\nSentenceSplitter ` to provide finer control over the input processing, leading\nto better output quality.\n\nIn addition, you can set ` streaming=True ` to stream your response:\n\n    \n    \n    query_engine = index.as_query_engine(streaming=True)\n    response = query_engine.query(\"What is OpenLLM?\")\n    response.print_response_stream()\n\nYour directory structure should look like this now:\n\n    \n    \n     starter.py\n     data\n         README.md\n\nRun ` starter.py ` to test the query-response system. The output should be\nconsistent with the content of the OpenLLM README. Here is the response I\nreceived:\n\n> OpenLLM is an open-source platform for deploying and managing large language\n> models (LLMs) in a variety of environments, including on-premises, cloud,\n> and edge devices. It provides a comprehensive suite of tools and features\n> for fine-tuning, serving, deploying, and monitoring LLMs, simplifying the\n> end-to-end deployment workflow for LLMs.", "mimetype": "text/plain", "start_char_idx": 7540, "end_char_idx": 10608, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7df20803-2522-488d-8905-7d9d4fc2d87e": {"__data__": {"id_": "7df20803-2522-488d-8905-7d9d4fc2d87e", "embedding": null, "metadata": {"Header_1": " Conclusion", "filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "32940c64-9a6d-4478-947a-e29fb85ab016", "node_type": "4", "metadata": {"filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "hash": "cc550ed7a5ae4885396bdc46e84eb7813dc58af8619e23e1ca18efac283d7302", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29dacc6b-8562-4e45-b3e0-a49221bc7476", "node_type": "1", "metadata": {"Header_1": " v2: Enhancing with a query-response system", "filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "hash": "f44274df342c09b8c89cb99bf0788edbd8dd69932eb2ba798290b3255a985455", "class_name": "RelatedNodeInfo"}}, "text": "Conclusion\n\nThe exploration in this article underscores the importance of customizing AI\ntools to fit specific needs. By using OpenLLM for flexible deployment of LLMs\nand LlamaIndex for data management, I have demonstrated how to create an AI-\npowered system. It not only understands and processes queries but also\ndelivers responses based on a unique knowledge base. I hope this blog post has\ninspired you to explore more capabilities and use cases of OpenLLM and\nLlamaIndex. Happy coding! \u2328", "mimetype": "text/plain", "start_char_idx": 10613, "end_char_idx": 11105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"a720dbbc-269c-476c-856b-d8dd0a603a38": {"doc_hash": "8ad4474bd9c887bdb63af6019cb8fabee5b4084982786d8f10b4a57c1e982732", "ref_doc_id": "32940c64-9a6d-4478-947a-e29fb85ab016"}, "4ccc8e7e-cd94-4606-a7ea-55060bd080dc": {"doc_hash": "b81e4448551cc7c0c2c7b8dc98f13b75e70d561eece7efc5f2bea3ffd72ab03b", "ref_doc_id": "32940c64-9a6d-4478-947a-e29fb85ab016"}, "117c1c67-1809-4380-8211-7333277af27f": {"doc_hash": "b24a133e561fd9288ca202ccfd77e4f77e43fe8dc2b0b2e58399b4a6f484b812", "ref_doc_id": "32940c64-9a6d-4478-947a-e29fb85ab016"}, "29dacc6b-8562-4e45-b3e0-a49221bc7476": {"doc_hash": "f44274df342c09b8c89cb99bf0788edbd8dd69932eb2ba798290b3255a985455", "ref_doc_id": "32940c64-9a6d-4478-947a-e29fb85ab016"}, "7df20803-2522-488d-8905-7d9d4fc2d87e": {"doc_hash": "66681779d9cf2fb85b6b0a2d9853a95d2238c4451df55de7f79f6ed7faee799c", "ref_doc_id": "32940c64-9a6d-4478-947a-e29fb85ab016"}}, "docstore/ref_doc_info": {"32940c64-9a6d-4478-947a-e29fb85ab016": {"node_ids": ["a720dbbc-269c-476c-856b-d8dd0a603a38", "4ccc8e7e-cd94-4606-a7ea-55060bd080dc", "117c1c67-1809-4380-8211-7333277af27f", "29dacc6b-8562-4e45-b3e0-a49221bc7476", "7df20803-2522-488d-8905-7d9d4fc2d87e"], "metadata": {"filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}}}}