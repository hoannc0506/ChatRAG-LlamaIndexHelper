{"docstore/data": {"69172e2f-80a1-4de7-859d-cccb4e550102": {"__data__": {"id_": "69172e2f-80a1-4de7-859d-cccb4e550102", "embedding": null, "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5995cb91-e571-446a-94c3-284b83e02c3e", "node_type": "4", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "1beba26d53a7fbe5d5daca8c63bb5f086f199996b63573ec2ef86e96a8574283", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b68d8e6a-ae4d-418c-ab41-638bc7e90c3a", "node_type": "1", "metadata": {"Header_1": " Context"}, "hash": "80a470914a9e4cd1fa5fb53a01d6dcb3eebd77d45517f039d28782430513397f", "class_name": "RelatedNodeInfo"}}, "text": "Today we\u2019re incredibly excited to announce the launch of a big new capability\nwithin LlamaIndex: **Data Agents** .\n\nData Agents are LLM-powered knowledge workers that can intelligently perform\nvarious tasks over your data, in both a \u201cread\u201d and \u201cwrite\u201d function. They are\ncapable of the following:\n\n  * Perform automated search and retrieval over different types of data \u2014 unstructured, semi-structured, and structured. \n  * Calling any external service API in a structured fashion. They can either process the response immediately, or index/cache this data for future use. \n  * Storing conversation history. \n  * Using all of the above to fulfill both simple and complex data tasks. \n\nWe\u2019ve worked hard to provide abstractions, services, and guides on both the\nagents side and tools side in order to build data agents. Today\u2019s launch\nconsists of the following key components:\n\n  * [ **General Agent/Tool Abstractions** ](https://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/agents/root.html) **:** a set of abstractions to build agent loops, and to have those loops interact with tools according to a structured API definition. \n  * [ **LlamaHub Tool Repository** ](https://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/tools/root.html) **:** A [ brand-new section within LlamaHub ](https://llamahub.ai/) that consists of 15+ Tools (e.g. Google Calendar, Notion, SQL, OpenAPI) that can be connected. Opening to [ community contributions ](https://github.com/emptycrown/llama-hub) ! \n\nSee below for full details. **We show you how to build a Gmail agent that\u2019s\nable to automatically create/send emails in <10 lines of code! **", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1658, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b68d8e6a-ae4d-418c-ab41-638bc7e90c3a": {"__data__": {"id_": "b68d8e6a-ae4d-418c-ab41-638bc7e90c3a", "embedding": null, "metadata": {"Header_1": " Context", "filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5995cb91-e571-446a-94c3-284b83e02c3e", "node_type": "4", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "1beba26d53a7fbe5d5daca8c63bb5f086f199996b63573ec2ef86e96a8574283", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69172e2f-80a1-4de7-859d-cccb4e550102", "node_type": "1", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "17cc8fa87c8d6d9eacbbe4453b47571ffb7f48f45ddd5394c340a6c8d706585c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "099f4473-af94-4330-8896-ac0a98561637", "node_type": "1", "metadata": {"Header_1": " Core Components of Data Agents"}, "hash": "0fe7897dfbb2cbe588d191a57cb448f431808b9d69c8098193d6784473063df8", "class_name": "RelatedNodeInfo"}}, "text": "Context\n\nOur core mission at LlamaIndex is to unlock the full capabilities of LLMs over\nyour external sources of data. It provides a set of tools to both define\n\u201cstate\u201d (how to parse/structure your data), and \u201ccompute\u201d (how to query your\ndata). Up until now, our framework has primarily focused on search and\nretrieval use case. We have an incredible suite of tools and capabilities that\nnot only allow you to create the basic RAG stack around a vector database +\ntop-k retrieval, but also offer much greater functionality [ beyond that\n](https://gpt-\nindex.readthedocs.io/en/latest/core_modules/query_modules/query_engine/root.html)\n.\n\nA lot of that technology used to lie in our query engines. Our goal was to\nincrease the capability of query engines to answer a wide range of different\nqueries. In order to do this, we had to improve the \u201creasoning\u201d capabilities\nof these query engines. As a result some of our existing query capabilities\ncontain \u201cagent-like\u201d components: we have query engines capable of chain-of-\nthought reasoning, query decomposition, and routing. In the process, users had\nthe option of choosing from a spectrum of query engines that had more\nconstrained reasoning capabilities to less constrained capabilities.\n\nBut there was a huge opportunity for LLMs to have an even richer set of\ninteractions with data; they should be capable of general reasoning over any\nset of tools, whether from a database or an API. They should also be capable\nof both \u201cread\u201d and \u201cwrite\u201d capabilities \u2014 the ability to not only understand\nstate but also modify it. As a result they should be able to do more than\nsearch and retrieval from a static knowledge source.\n\nSome existing [ services ](https://openai.com/blog/chatgpt-plugins) , [\ntoolkits ](https://python.langchain.com/docs/modules/agents/) , and [ research\n](https://arxiv.org/abs/2302.04761) [ papers\n](https://arxiv.org/abs/2210.03629) have already demonstrated the\npossibilities of LLM-powered \u201cagents\u201d that can interact with the external\nenvironment. Using these existing approaches as inspiration, we saw an\nopportunity to build a principled series of abstractions enabling anyone to\nbuild knowledge workers over their data.", "mimetype": "text/plain", "start_char_idx": 1663, "end_char_idx": 3854, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "099f4473-af94-4330-8896-ac0a98561637": {"__data__": {"id_": "099f4473-af94-4330-8896-ac0a98561637", "embedding": null, "metadata": {"Header_1": " Core Components of Data Agents", "filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5995cb91-e571-446a-94c3-284b83e02c3e", "node_type": "4", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "1beba26d53a7fbe5d5daca8c63bb5f086f199996b63573ec2ef86e96a8574283", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b68d8e6a-ae4d-418c-ab41-638bc7e90c3a", "node_type": "1", "metadata": {"Header_1": " Context", "filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "96588b68202d89f7c4a87643de4f285bff0f4237c70e601d2b029dca67cf18e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0bd01400-8b0d-47e9-9b66-5bbae7f44654", "node_type": "1", "metadata": {"Header_1": " Core Components of Data Agents", "Header_2": " Agent Abstraction + Reasoning Loop"}, "hash": "7cdef39b0486fb726ddbf92cc9c93942704c038ac197e8553d6a4ec66186c340", "class_name": "RelatedNodeInfo"}}, "text": "Core Components of Data Agents\n\nBuilding a data agent requires the following core components:\n\n  * A reasoning loop \n  * Tool abstractions \n\nAt a high-level, a data agent is provided with a set of APIs, or Tools, to\ninteract with. These APIs can return information about the world, or perform\nan action that modifies state. Each Tool exposes a request/response interface.\nThe request is a set of structured parameters, and the response can be any\nformat (at least conceptually, in most cases the response here is a text\nstring of some form).\n\nGiven an input task, the data agent uses a **reasoning loop** to decide which\ntools to use, in which sequence, and the parameters to call each tool. The\n\u201cloop\u201d can conceptually be very simple (a one-step tool selection process), or\ncomplex (a multi-step selection process, where a multitude of tools are picked\nat each step).\n\nThese components are described in more detail below.", "mimetype": "text/plain", "start_char_idx": 3859, "end_char_idx": 4781, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0bd01400-8b0d-47e9-9b66-5bbae7f44654": {"__data__": {"id_": "0bd01400-8b0d-47e9-9b66-5bbae7f44654", "embedding": null, "metadata": {"Header_1": " Core Components of Data Agents", "Header_2": " Agent Abstraction + Reasoning Loop", "filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5995cb91-e571-446a-94c3-284b83e02c3e", "node_type": "4", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "1beba26d53a7fbe5d5daca8c63bb5f086f199996b63573ec2ef86e96a8574283", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "099f4473-af94-4330-8896-ac0a98561637", "node_type": "1", "metadata": {"Header_1": " Core Components of Data Agents", "filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "8665f3b6a6f824348235b0888fccba1555a64eb497e0621e88c332f7b91ad28a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6ba1c03-602d-47c8-8025-627afc736075", "node_type": "1", "metadata": {"Header_1": " Core Components of Data Agents", "Header_2": " Tool Abstractions"}, "hash": "40e761f797dc5806f0b64eeef1bdc3ee214726ed53e4fe42ee95853277eb2486", "class_name": "RelatedNodeInfo"}}, "text": "Agent Abstraction + Reasoning Loop\n\nWe have support for the following agents:\n\n  * OpenAI Function agent (built on top of the OpenAI Function API) \n  * a ReAct agent (which works across any chat/text completion endpoint). \n\nYou can use them as the following:\n\n    \n    \n    from llama_index.agent import OpenAIAgent, ReActAgent\n    from llama_index.llms import OpenAI\n    \n    # import and define tools\n    ...\n    # initialize llm\n    llm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n    # initialize openai agent\n    agent = OpenAIAgent.from_tools(tools, llm=llm, verbose=True)\n    # initialize ReAct agent\n    agent = ReActAgent.from_tools(tools, llm=llm, verbose=True)\n    # use agent\n    response = agent.chat(\"What is (121 * 3) + 42?\")\n\nEach agent takes in a set of Tools. The details behind our tool abstractions\nare provided below. Each agent also supports two main methods for taking in an\ninput task \u2014 ` chat ` and ` query ` . Note that these are the core methods\nused in our ` ChatEngine ` and ` QueryEngine ` respectively. In fact that our\nbase agent class ( ` BaseAgent ` ) simply inherits from ` BaseChatEngine ` and\n` BaseQueryEngine ` . ` chat ` allows the agent to utilize previously stored\nconversation history, whereas ` query ` is a stateless call - history/state is\nnot preserved over time.\n\nThe reasoning loop depends on the type of agent. The OpenAI agent calls the\nOpenAI function API in a while loop, since the tool decision logic is baked\ninto the function API. Given an input prompt and previous chat history (which\nincludes previous function calls), the function API will decide whether to\nmake another function call (pick a Tool), or return an assistant message. If\nthe API returns a function call, then we are responsible for executing the\nfunction and passing in a function message in the chat history. If the API\nreturns an assistant message, then the loop is complete (we assume the task is\nsolved).\n\nThe ReAct agent uses general text completion endpoints, so it can be used with\nany LLM. A text completion endpoint has a simple input str \u2192 output str\nformat, which means that the reasoning logic must be encoded in the prompt.\nThe ReAct agent uses an input prompt inspired by the ReAct paper (and adapted\ninto other versions), in order to decide which tool to pick. It looks\nsomething like this:\n\n    \n    \n    ...\n    You have access to the following tools:\n    {tool_desc}\n    \n    To answer the question, please use the following format.\n    \n    ```\n    Thought: I need to use a tool to help me answer the question.\n    Action: tool name (one of {tool_names})\n    Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"text\": \"hello world\", \"num_beams\": 5}})\n    ```\n    Please use a valid JSON format for the action input. Do NOT do this {{'text': 'hello world', 'num_beams': 5}}.\n    \n    If this format is used, you will receive a response in the following format:\n    \n    ```\n    Observation: tool response\n    ```\n    ...\n\nWe implement ReAct natively over chat prompts; the reasoning loop is\nimplemented as an alternating series of assistant and user messages. The\nThought/Action/Action Input section is represented as an assistant message,\nand the Observation section is implemented as a user message.\n\n**Note:** the ReAct prompt expects not only the name of the tool to pick, but\nalso the parameters to fill in the tool in a JSON format. This makes the\noutput not dissimilar from the output of the OpenAI Function API \u2014 the main\ndifference is that in the case of the function API, the tool-picking logic is\nbaked into the API itself (through a finetuned model), whereas here it is\nelicited through explicit prompting.", "mimetype": "text/plain", "start_char_idx": 4787, "end_char_idx": 8474, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6ba1c03-602d-47c8-8025-627afc736075": {"__data__": {"id_": "f6ba1c03-602d-47c8-8025-627afc736075", "embedding": null, "metadata": {"Header_1": " Core Components of Data Agents", "Header_2": " Tool Abstractions", "filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5995cb91-e571-446a-94c3-284b83e02c3e", "node_type": "4", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "1beba26d53a7fbe5d5daca8c63bb5f086f199996b63573ec2ef86e96a8574283", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0bd01400-8b0d-47e9-9b66-5bbae7f44654", "node_type": "1", "metadata": {"Header_1": " Core Components of Data Agents", "Header_2": " Agent Abstraction + Reasoning Loop", "filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "04c874b103796f051daa1dd2c9b0ea47883219c9f2cdd63e04a70874466dd1d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e59cfae3-46ba-46dd-b4b3-aa5b5660f03d", "node_type": "1", "metadata": {"Header_1": " LlamaHub Tool Repository"}, "hash": "2bc2df2e5f9192e11d1333bf79d9b3053c54107d0ee751850426c09dddd8f291", "class_name": "RelatedNodeInfo"}}, "text": "Tool Abstractions\n\nHaving proper tool abstractions is at the core of building data agents.\nDefining a set of Tools is similar to defining any API interface, with the\nexception that these Tools are meant for agent rather than human use. We allow\nusers to define both a single Tool as well as a \u201cToolSpec\u201d containing a series\nof functions under the hood.\n\nWe describe the base tool abstraction, as well as how you can easily define\ntools over existing query engines, other tools.\n\n**Base Tool Abstraction**\n\nThe base tool defines a very generic interface. The ` __call__ ` function can\ntake in any series of arguments, and return a generic ` ToolOutput ` container\nthat can capture any response. A tool also has metadata containing its name,\ndescription, and function schema.\n\n    \n    \n    @dataclass\n    class ToolMetadata:\n        description: str\n        name: Optional[str] = None\n        fn_schema: Optional[Type[BaseModel]] = DefaultToolFnSchema\n    \n    class BaseTool:\n        @property\n        @abstractmethod\n        def metadata(self) -&gt; ToolMetadata:\n            pass\n        @abstractmethod\n        def __call__(self, input: Any) -&gt; ToolOutput:\n            pass\n\n**Function Tool**\n\nA function tool allows users to easily convert any function into a Tool. It\ntakes in a user-defined function (that can take in any inputs/outputs), and\nwraps it into a tool interface. It can also \u201cauto-infer\u201d the function schema\nif it isn\u2019t specified beforehand.\n\nOur ` ToolSpec ` classes make use of this ` FunctionTool ` abstraction to\nconvert functions defined in the tool spec into a set of agent tools (see\nbelow).\n\nHere\u2019s a trivial example of defining a FunctionTool.\n\n    \n    \n    from llama_index.tools.function_tool import FunctionTool\n    \n    def multiply(a: int, b: int) -&gt; int:\n        \"\"\"Multiple two integers and returns the result integer\"\"\"\n        return a * b\n    multiply_tool = FunctionTool.from_defaults(fn=multiply)\n\n**QueryEngineTool**\n\nOf course, we also provide Tool abstractions to wrap our existing query\nengines. This provides a seamless transition from working on query engines to\nworking on agents. Our query engines can be thought of \u201cconstrained\u201d agents\nmeant for the read/write setting and centered around retrieval purposes. These\nquery engines can be used in an overall agent setting.\n\n    \n    \n    from llama_index.tools import QueryEngineTool\n    \n    query_engine_tools = [\n        QueryEngineTool(\n            query_engine=query_engine, \n            metadata=ToolMetadata(\n                name='&lt;tool_name&gt;', \n                description=\"Queries over X data source.\"\n            )\n        ),\n     ...\n    ]\n\n**Tool Specs**\n\nA **tool spec** is a Python class that represents a full API specification\nthat an agent can interact with, and a tool spec can be converted into a list\nof tools that an agent can be initialized with.\n\nThis class allows users to define entire services, not just single tools that\nperform individual tasks. Each tool spec may contain read/write endpoints that\nallow an agent to interact with a service in meaningful ways. For instance, a\nSlack tool spec could allow the user to both read existing messages and\nchannels ( ` load_data ` , ` fetch_channels ` ) as well as write messages ( `\nsend_message ` ). It would be roughly defined as the following:\n\n    \n    \n    class SlackToolSpec(BaseToolSpec):\n        \"\"\"Slack tool spec.\"\"\"\n        spec_functions = [\"load_data\", \"send_message\", \"fetch_channels\"]\n    \n        def load_data(\n              self,\n              channel_ids: List[str],\n              reverse_chronological: bool = True,\n          ) -&gt; List[Document]:\n              \"\"\"Load data from the input directory.\"\"\"\n              ...\n          def send_message(\n              self,\n              channel_id: str,\n              message: str,\n          ) -&gt; None:\n              \"\"\"Send a message to a channel given the channel ID.\"\"\"\n              ...\n          def fetch_channels(\n              self,\n          ) -&gt; List[str]:\n              \"\"\"Fetch a list of relevant channels.\"\"\"\n              ...\n\nIf a tool spec is initialized, it can be converted into a list of tools that\ncan be fed into an agent with ` to_tool_list ` . For instance,\n\n    \n    \n    tool_spec = SlackToolSpec()\n    # initialize openai agent\n    agent = OpenAIAgent.from_tools(tool_spec.to_tool_list(), llm=llm, verbose=True)\n\nDefining a tool spec is not that different than defining a Python class. Each\nfunction becomes converted into a tool, and by default the docstring for each\nfunction gets used as the tool description (though you can customize\nnames/description in ` to_tool_list(func_to_metadata_mapping=...) ` .\n\nWe also made the intentional choice that the input arguments and return types\ncan be anything. The primary reason is to preserve the generality of the tool\ninterface for subsequent iterations of agents. Even if current iterations of\nagents expect tool outputs to be in string format, that may change in the\nfuture, and we didn\u2019t want to arbitrarily restrict the types of tool\ninterface.", "mimetype": "text/plain", "start_char_idx": 8480, "end_char_idx": 13557, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e59cfae3-46ba-46dd-b4b3-aa5b5660f03d": {"__data__": {"id_": "e59cfae3-46ba-46dd-b4b3-aa5b5660f03d", "embedding": null, "metadata": {"Header_1": " LlamaHub Tool Repository", "filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5995cb91-e571-446a-94c3-284b83e02c3e", "node_type": "4", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "1beba26d53a7fbe5d5daca8c63bb5f086f199996b63573ec2ef86e96a8574283", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f6ba1c03-602d-47c8-8025-627afc736075", "node_type": "1", "metadata": {"Header_1": " Core Components of Data Agents", "Header_2": " Tool Abstractions", "filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "8f83dfe9927fe53dc943f1321396a4bef6945c9e13176081371b6be90cb19186", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db9e9669-edbf-4c65-be24-f8e885e5c443", "node_type": "1", "metadata": {"Header_1": " LlamaHub Tool Repository", "Header_2": " Example Walkthrough"}, "hash": "fdaedf63aa717fa737972d8081b87beb9f2f8868204ad23efceb7b73534cd03e", "class_name": "RelatedNodeInfo"}}, "text": "LlamaHub Tool Repository\n\nA huge component of our launch is a brand-new addition to [ LlamaHub\n](https://llamahub.ai/) : a Tool Repository. The Tool Repository consists of\n**15+ Tool Specs** that an agent can use. These tool specs represent an\ninitial curated list of services that an agent can interact with and enrich\nits capability to perform different actions.\n\nAmong others, they include the following specs:\n\n  * Gmail Spec \n  * Zapier Spec \n  * Google Calendar Spec \n  * OpenAPI Spec \n  * SQL + Vector Database Spec \n\nWe also provide a list of **utility tools** that help to abstract away pain\npoints when designing agents to interact with different API services that\nreturn large amounts of data.\n\nFor instance, our Gmail Tool Spec allows an agent to search existing emails,\ncreate drafts, update drafts, and send emails. Our Zapier Spec allows an agent\nto perform any natural language query to Zapier through their [ Natural\nLanguage Actions ](https://nla.zapier.com/start/) interface.\n\nBest of all, you don\u2019t need to spend a lot of time figuring out how to use\nthese tools \u2014 we have [ **10+ notebooks**\n](https://github.com/emptycrown/llama-hub/tree/main/llama_hub/tools/notebooks)\nshowing how you can build agents for each service, or even build agents that\nuse a combination of services (e.g. Gmail, Google Calendar, and Search).", "mimetype": "text/plain", "start_char_idx": 13562, "end_char_idx": 14903, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db9e9669-edbf-4c65-be24-f8e885e5c443": {"__data__": {"id_": "db9e9669-edbf-4c65-be24-f8e885e5c443", "embedding": null, "metadata": {"Header_1": " LlamaHub Tool Repository", "Header_2": " Example Walkthrough", "filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5995cb91-e571-446a-94c3-284b83e02c3e", "node_type": "4", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "1beba26d53a7fbe5d5daca8c63bb5f086f199996b63573ec2ef86e96a8574283", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e59cfae3-46ba-46dd-b4b3-aa5b5660f03d", "node_type": "1", "metadata": {"Header_1": " LlamaHub Tool Repository", "filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "419ce0e5db47f4b00ff7ed2aa436bfc82d13838b42b163a334780425ac4ac1b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd7a4e25-7f4a-4947-880f-219bfc567ce3", "node_type": "1", "metadata": {"Header_1": " LlamaHub Tool Repository", "Header_2": " Utility Tools"}, "hash": "c196a4bdb26c74e6dfd6ba944562910480b1124154c022fe53125cbabc3e247f", "class_name": "RelatedNodeInfo"}}, "text": "Example Walkthrough\n\nLet\u2019s take a look at a few examples! We initialize an OpenAIAgent with the\nGmail Spec. As mentioned above, the spec consists of tools to search emails,\ncreate/update drafts, and send emails.\n\nNow let\u2019s give the agent a sequence of commands so that it can create an email\ndraft, make a few edits to it, and then send it off.\n\nFirst, let\u2019s create an initial email draft. Note that the agent chooses the `\ncreate_draft ` tool, which takes in the \u201cto\u201d, \u201csubject\u201d, and \u201cmessage\u201d\nparameters. The agent is able to infer the parameters simultaneously while\npicking the tool.\n\nNext, let\u2019s update the draft with a slight modification:\n\nNext, let\u2019s show the current state of the draft.\n\nFinally, let\u2019s send the email!\n\nThis is a good start, but this is just the beginning. We are actively working\non contributing more tools to this repository, and we\u2019re also opening this up\nto community contributions. If you\u2019re interested in contributing a Tool to\nLlamaHub, please feel free to open a PR in this repo.", "mimetype": "text/plain", "start_char_idx": 14909, "end_char_idx": 15922, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd7a4e25-7f4a-4947-880f-219bfc567ce3": {"__data__": {"id_": "cd7a4e25-7f4a-4947-880f-219bfc567ce3", "embedding": null, "metadata": {"Header_1": " LlamaHub Tool Repository", "Header_2": " Utility Tools", "filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5995cb91-e571-446a-94c3-284b83e02c3e", "node_type": "4", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "1beba26d53a7fbe5d5daca8c63bb5f086f199996b63573ec2ef86e96a8574283", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db9e9669-edbf-4c65-be24-f8e885e5c443", "node_type": "1", "metadata": {"Header_1": " LlamaHub Tool Repository", "Header_2": " Example Walkthrough", "filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "d9d42da29f6319ab027a6bec5ee7e508029d0e6e3d4eb859e5ac74a355398785", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "376e058b-81f6-44d1-ac3e-bb9eded74db5", "node_type": "1", "metadata": {"Header_1": " FAQ"}, "hash": "13e3be58965dae358cd0192d9153cfc01bcb979b36b6eca820fad63811b3c3f4", "class_name": "RelatedNodeInfo"}}, "text": "Utility Tools\n\nOftentimes, directly querying an API can return a massive volume of data,\nwhich on its own may overflow the context window of the LLM (or at the very\nleast unnecessarily increase the number of tokens that you are using).\n\nTo tackle this, we\u2019ve provided an initial set of \u201cutility tools\u201d in the core\nLlamaIndex repo \u2014 utility tools are not conceptually tied to a given service\n(e.g. Gmail, Notion), but rather can augment the capabilities of existing\nTools. In this particular case, utility tools help to abstract away common\npatterns of needing to cache/index and query data that\u2019s returned from any API\nrequest.\n\nLet\u2019s walk through our two main utility tools below.\n\n**OnDemandLoaderTool**\n\nThis tool turns any existing LlamaIndex data loader ( ` BaseReader ` class)\ninto a tool that an agent can use. The tool can be called with all the\nparameters needed to trigger ` load_data ` from the data loader, along with a\nnatural language query string. During execution, we first load data from the\ndata loader, index it (for instance with a vector store), and then query it\n\u201con-demand\u201d. All three of these steps happen in a single tool call.\n\nOftentimes this can be preferable to figuring out how to load and index API\ndata yourself. While this may allow for data reusability, oftentimes users\njust need an ad-hoc index to abstract away prompt window limitations for any\nAPI call.\n\nA usage example is given below:\n\n    \n    \n    from llama_hub.wikipedia.base import WikipediaReader\n    from llama_index.tools.on_demand_loader_tool import OnDemandLoaderTool\n    \n    tool = OnDemandLoaderTool.from_defaults(\n     reader,\n     name=\"Wikipedia Tool\",\n     description=\"A tool for loading data and querying articles from Wikipedia\"\n    )\n\n**LoadAndSearchToolSpec**\n\nThe LoadAndSearchToolSpec takes in any existing Tool as input. As a tool spec,\nit implements ` to_tool_list ` , and when that function is called, two tools\nare returned: a ` load ` tool and then a ` search ` tool.\n\nThe ` load ` Tool execution would call the underlying Tool, and the index the\noutput (by default with a vector index). The ` search ` Tool execution would\ntake in a query string as input and call the underlying index.\n\nThis is helpful for any API endpoint that will by default return large volumes\nof data \u2014 for instance our WikipediaToolSpec will by default return entire\nWikipedia pages, which will easily overflow most LLM context windows.\n\nExample usage is shown below:\n\n    \n    \n    from llama_hub.tools.wikipedia.base import WikipediaToolSpec\n    from llama_index.tools.tool_spec.load_and_search.base import LoadAndSearchToolSpec\n    \n    wiki_spec = WikipediaToolSpec()\n    # Get the search wikipedia tool\n    tool = wiki_spec.to_tool_list()[1]\n    # Create the Agent with load/search tools\n    agent = OpenAIAgent.from_tools(\n     LoadAndSearchToolSpec.from_defaults(\n        tool\n     ).to_tool_list(), verbose=True\n    )\n\nThis is the output when we run an input prompt\n\n    \n    \n    agent.chat('what is the capital of poland')\n\nOutput:\n\n    \n    \n    === Calling Function ===\n    Calling function: search_data with args: {\n      \"query\": \"capital of Poland\"\n    }\n    Got output: Content loaded! You can now search the information using read_search_data\n    ========================\n    === Calling Function ===\n    Calling function: read_search_data with args: {\n      \"query\": \"What is the capital of Poland?\"\n    }\n    Got output: \n    The capital of Poland is Warsaw.\n    ========================\n    AgentChatResponse(response='The capital of Poland is Warsaw.', sources=[])\n\nNote that the agent figures out that it first needs to first call the \u201cload\u201d\ntool (denoted by the original name of the tool, \u201csearch_data\u201d). This load tool\nwill load the Wikipedia page and index under the hood. The output just\nmentions that the \u201ccontent is loaded, and tells the agent that the next step\nis to use ` read_search_data ` . The agent then reasons that it needs to call\nthe ` read_search_data ` tool, which will query the index for the right\nanswer.", "mimetype": "text/plain", "start_char_idx": 15928, "end_char_idx": 19970, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "376e058b-81f6-44d1-ac3e-bb9eded74db5": {"__data__": {"id_": "376e058b-81f6-44d1-ac3e-bb9eded74db5", "embedding": null, "metadata": {"Header_1": " FAQ", "filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5995cb91-e571-446a-94c3-284b83e02c3e", "node_type": "4", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "1beba26d53a7fbe5d5daca8c63bb5f086f199996b63573ec2ef86e96a8574283", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd7a4e25-7f4a-4947-880f-219bfc567ce3", "node_type": "1", "metadata": {"Header_1": " LlamaHub Tool Repository", "Header_2": " Utility Tools", "filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "eed1cf8a6b23bd7a6aa437d33c12ff036d32e75b28ca2528b21ca0316e0bca6d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "260f1db7-762a-4124-be10-2a986eeed782", "node_type": "1", "metadata": {"Header_1": " Conclusion"}, "hash": "12fb7f6a31d2af87f1eafdfb80609bde713d639ffe5b9e5e2c6d8c59f2341a77", "class_name": "RelatedNodeInfo"}}, "text": "FAQ\n\n**Should I use Data Agents for search and retrieval, or continue to use Query\nEngines?**\n\nShort answer: both are possible. Query engines give you the ability to define\nyour own workflows over your data, in both a constrained reasoning fashion as\nwell as unconstrained fashion. For instance, you may want to define a specific\nworkflow over text-to-SQL with our ` NLStructStoreQueryEngine ` (constrained),\nor a router module to decide between semantic search or summarization (less\nconstrained), or use our ` SubQuestionQueryEngine ` to decompose a question\namong sub-documents (even less constrained).\n\nBy default, agent loops are unconstrained, and can theoretically reason over\nany set of tools that you give them. This means that you can get out-of-the-\nbox advanced search/retrieval capabilities \u2014 for instance, in our OpenAI\ncookbook we show that you can get joint text-to-SQL capabilities by simply\nproviding a SQL query engine and Vector Store Query engine as tools. But on\nthe other hand, agents built in this fashion can be quite unreliable (see our\nblog post for more insights). If you are using agents for search/retrieval, be\nmindful of the 1) LLM you pick, and the 2) set of tools you pick too.\n\n**How are LlamaIndex data agents different than existing agent frameworks\n(LangChain, Hugging Face, etc.)?**\n\nMost of these core concepts are not new. Our overall design has taken\ninspiration from popular tools and frameworks for building agents. But in our\n\u201cdata agents\u201d design, we\u2019ve tried our best to answer the following key\nquestions well:\n\n  * How do we effectively index/query and retrieve data beforehand? \n  * How do we effectively index/query and retrieve data on the fly? \n  * How do we design API interfaces for read/writes that are simultaneously rich (can take in structured inputs), but also easy for agents to understand? \n  * How do we properly get sources in citations? \n\nOur goal with data agents is to create automated knowledge workers that can\nreason over and interact with data. Our core toolkit provides the foundations\nfor properly indexing, retrieving, and querying data \u2014 these can be easily\nintegrated as tools. We provide some additional tool abstractions to handle\nthe cases where you want to \u201ccache\u201d API outputs on the fly (see above).\nFinally, we provide principled tool abstractions and design principles so that\nagents can interface with external services in a structured manner.\n\n**Can I use Tools with LangChain agents?** You can easily use any of our tools\nwith LangChain agents as well.\n\n    \n    \n    tools = tool_spec.to_tool_list()\n    langchain_tools = [t.to_langchain_tool() for t in tools]\n\nSee our [ tools usage guide ](https://gpt-\nindex.readthedocs.io/en/latest/core_modules/agent_modules/tools/usage_pattern.html#using-\nwith-langchain) for more details!", "mimetype": "text/plain", "start_char_idx": 19975, "end_char_idx": 22789, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "260f1db7-762a-4124-be10-2a986eeed782": {"__data__": {"id_": "260f1db7-762a-4124-be10-2a986eeed782", "embedding": null, "metadata": {"Header_1": " Conclusion", "filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5995cb91-e571-446a-94c3-284b83e02c3e", "node_type": "4", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "1beba26d53a7fbe5d5daca8c63bb5f086f199996b63573ec2ef86e96a8574283", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "376e058b-81f6-44d1-ac3e-bb9eded74db5", "node_type": "1", "metadata": {"Header_1": " FAQ", "filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "55a99eddad8164dd981647a70bad2a8b2385b8b33e4d6fe52a4a737094706ddd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78b0083c-16a0-4508-9397-388827d070ca", "node_type": "1", "metadata": {"Header_1": " Conclusion", "Header_2": " Resources"}, "hash": "90a0355a0d75a3dadaf24e06bb6495b36b7c9a8daa758b5345bbb7f019839f05", "class_name": "RelatedNodeInfo"}}, "text": "Conclusion\n\nIn summary, today we launched two key items: Data Agent components (incl.\nagent reasoning loop and tool abstractions) and the LlamaHub Tool repository.", "mimetype": "text/plain", "start_char_idx": 22794, "end_char_idx": 22957, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78b0083c-16a0-4508-9397-388827d070ca": {"__data__": {"id_": "78b0083c-16a0-4508-9397-388827d070ca", "embedding": null, "metadata": {"Header_1": " Conclusion", "Header_2": " Resources", "filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5995cb91-e571-446a-94c3-284b83e02c3e", "node_type": "4", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "1beba26d53a7fbe5d5daca8c63bb5f086f199996b63573ec2ef86e96a8574283", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "260f1db7-762a-4124-be10-2a986eeed782", "node_type": "1", "metadata": {"Header_1": " Conclusion", "filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "5c11e4c1939688e871c779f85b5eb089e9785c8167d81a778810172d7d701bca", "class_name": "RelatedNodeInfo"}}, "text": "Resources\n\nWe\u2019ve written a comprehensive section in the docs \u2014 take a look here: [\nhttps://gpt-\nindex.readthedocs.io/en/latest/core_modules/agent_modules/agents/root.html\n](https://gpt-\nindex.readthedocs.io/en/latest/core_modules/agent_modules/agents/root.html)\n\nTake a look at our LlamaHub Tools section: [ https://llamahub.ai/\n](https://llamahub.ai/)\n\nNotebook Tutorials for LlamaHub Tools: [ https://github.com/emptycrown/llama-\nhub/tree/main/llama_hub/tools/notebooks ](https://github.com/emptycrown/llama-\nhub/tree/main/llama_hub/tools/notebooks)\n\nIf you have questions, please hop on our Discord: [\nhttps://discord.gg/dGcwcsnxhU ](https://discord.gg/dGcwcsnxhU)", "mimetype": "text/plain", "start_char_idx": 22963, "end_char_idx": 23630, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"69172e2f-80a1-4de7-859d-cccb4e550102": {"doc_hash": "17cc8fa87c8d6d9eacbbe4453b47571ffb7f48f45ddd5394c340a6c8d706585c", "ref_doc_id": "5995cb91-e571-446a-94c3-284b83e02c3e"}, "b68d8e6a-ae4d-418c-ab41-638bc7e90c3a": {"doc_hash": "96588b68202d89f7c4a87643de4f285bff0f4237c70e601d2b029dca67cf18e6", "ref_doc_id": "5995cb91-e571-446a-94c3-284b83e02c3e"}, "099f4473-af94-4330-8896-ac0a98561637": {"doc_hash": "8665f3b6a6f824348235b0888fccba1555a64eb497e0621e88c332f7b91ad28a", "ref_doc_id": "5995cb91-e571-446a-94c3-284b83e02c3e"}, "0bd01400-8b0d-47e9-9b66-5bbae7f44654": {"doc_hash": "04c874b103796f051daa1dd2c9b0ea47883219c9f2cdd63e04a70874466dd1d4", "ref_doc_id": "5995cb91-e571-446a-94c3-284b83e02c3e"}, "f6ba1c03-602d-47c8-8025-627afc736075": {"doc_hash": "8f83dfe9927fe53dc943f1321396a4bef6945c9e13176081371b6be90cb19186", "ref_doc_id": "5995cb91-e571-446a-94c3-284b83e02c3e"}, "e59cfae3-46ba-46dd-b4b3-aa5b5660f03d": {"doc_hash": "419ce0e5db47f4b00ff7ed2aa436bfc82d13838b42b163a334780425ac4ac1b3", "ref_doc_id": "5995cb91-e571-446a-94c3-284b83e02c3e"}, "db9e9669-edbf-4c65-be24-f8e885e5c443": {"doc_hash": "d9d42da29f6319ab027a6bec5ee7e508029d0e6e3d4eb859e5ac74a355398785", "ref_doc_id": "5995cb91-e571-446a-94c3-284b83e02c3e"}, "cd7a4e25-7f4a-4947-880f-219bfc567ce3": {"doc_hash": "eed1cf8a6b23bd7a6aa437d33c12ff036d32e75b28ca2528b21ca0316e0bca6d", "ref_doc_id": "5995cb91-e571-446a-94c3-284b83e02c3e"}, "376e058b-81f6-44d1-ac3e-bb9eded74db5": {"doc_hash": "55a99eddad8164dd981647a70bad2a8b2385b8b33e4d6fe52a4a737094706ddd", "ref_doc_id": "5995cb91-e571-446a-94c3-284b83e02c3e"}, "260f1db7-762a-4124-be10-2a986eeed782": {"doc_hash": "5c11e4c1939688e871c779f85b5eb089e9785c8167d81a778810172d7d701bca", "ref_doc_id": "5995cb91-e571-446a-94c3-284b83e02c3e"}, "78b0083c-16a0-4508-9397-388827d070ca": {"doc_hash": "56bf2d6a2f62d84eb565646b398b945f84047cd8b5e97ac922aaf65d88cf03c5", "ref_doc_id": "5995cb91-e571-446a-94c3-284b83e02c3e"}}, "docstore/ref_doc_info": {"5995cb91-e571-446a-94c3-284b83e02c3e": {"node_ids": ["69172e2f-80a1-4de7-859d-cccb4e550102", "b68d8e6a-ae4d-418c-ab41-638bc7e90c3a", "099f4473-af94-4330-8896-ac0a98561637", "0bd01400-8b0d-47e9-9b66-5bbae7f44654", "f6ba1c03-602d-47c8-8025-627afc736075", "e59cfae3-46ba-46dd-b4b3-aa5b5660f03d", "db9e9669-edbf-4c65-be24-f8e885e5c443", "cd7a4e25-7f4a-4947-880f-219bfc567ce3", "376e058b-81f6-44d1-ac3e-bb9eded74db5", "260f1db7-762a-4124-be10-2a986eeed782", "78b0083c-16a0-4508-9397-388827d070ca"], "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}}}}