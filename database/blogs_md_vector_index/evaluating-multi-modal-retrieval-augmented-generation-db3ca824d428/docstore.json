{"docstore/data": {"b1df07a3-4d25-48ad-ad55-a2c0314d0826": {"__data__": {"id_": "b1df07a3-4d25-48ad-ad55-a2c0314d0826", "embedding": null, "metadata": {"filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d32d4ea-6419-481a-8422-8f9fb38c3caa", "node_type": "4", "metadata": {"filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "hash": "d6772b63975af35a2a7cc00ea18a7f02900a2dfb9c17e6b3713f476752c70b28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c4e1857-eb03-4054-877f-5e607b1bb78a", "node_type": "1", "metadata": {"Header_1": " Primer: Multi-Modal RAG vs Text-Only RAG"}, "hash": "2953a08bcff06765125eb4e4e0ff91254f2711747de9fcb4882777b521d9ed4b", "class_name": "RelatedNodeInfo"}}, "text": "A few days ago, we published a blog on [ Multi-Modal RAG ](/multi-modal-\nrag-621de7525fea) (Retrieval-Augmented Generation) and our latest (still in\nbeta) abstractions to help enable and simplify building them. In this post, we\nnow go over the important topic of how one can sensibly evaluate Multi-Modal\nRAG systems.\n\nA natural starting point is to consider how evaluation was done in\ntraditional, text-only RAG and then ask ourselves how this ought to be\nmodified to suit the multi-modal scenario (e.g., in text-only RAG, we use an\nLLM, but in multi-modal RAG we require a Large Multi-Modal Model or LMM for\nshort). This is exactly what we\u2019ll do next and as you\u2019ll see, the overarching\nevaluation framework stays the same as it was in the text-only RAG, requiring\nonly a few additions and modifications in order to make it more multi-modal\nappropriate.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 854, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c4e1857-eb03-4054-877f-5e607b1bb78a": {"__data__": {"id_": "6c4e1857-eb03-4054-877f-5e607b1bb78a", "embedding": null, "metadata": {"Header_1": " Primer: Multi-Modal RAG vs Text-Only RAG", "filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d32d4ea-6419-481a-8422-8f9fb38c3caa", "node_type": "4", "metadata": {"filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "hash": "d6772b63975af35a2a7cc00ea18a7f02900a2dfb9c17e6b3713f476752c70b28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b1df07a3-4d25-48ad-ad55-a2c0314d0826", "node_type": "1", "metadata": {"filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "hash": "5638be3e2abba0891f0353d7c045e0eda54b1436c37542b43901f25c6d93f2bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81e01632-66a7-4ef3-acac-262903fde3cf", "node_type": "1", "metadata": {"Header_1": " Evaluation Of Text-Only RAG"}, "hash": "d0f0fb344043afa6cfb5b0bc82768928777f5c21b146318d3a89cd2a93e52d43", "class_name": "RelatedNodeInfo"}}, "text": "Primer: Multi-Modal RAG vs Text-Only RAG\n\nIllustration of text-only RAG versus multi-modal RAG. In multi-modal RAG,\nimages modality can show up in the user query, the retrieved context, as well\nas the final answer.\n\nLet\u2019s consider the main differences between multi-modal and text-only RAG.\nBelow are two tables that describe the RAG build considerations as well as\nquery-time pipeline and compares and contrasts multi-modal and text-only cases\nagainst them.\n\nTable 1: Build considerations for RAG systems and how they differ text-only\nversus multi-modal scenarios.  Table 2: The pipeline for querying a RAG and\nhow they differ text-only versus multi-modal scenarios.", "mimetype": "text/plain", "start_char_idx": 859, "end_char_idx": 1526, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81e01632-66a7-4ef3-acac-262903fde3cf": {"__data__": {"id_": "81e01632-66a7-4ef3-acac-262903fde3cf", "embedding": null, "metadata": {"Header_1": " Evaluation Of Text-Only RAG", "filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d32d4ea-6419-481a-8422-8f9fb38c3caa", "node_type": "4", "metadata": {"filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "hash": "d6772b63975af35a2a7cc00ea18a7f02900a2dfb9c17e6b3713f476752c70b28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c4e1857-eb03-4054-877f-5e607b1bb78a", "node_type": "1", "metadata": {"Header_1": " Primer: Multi-Modal RAG vs Text-Only RAG", "filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "hash": "7a77fc71c5b84fa3d0d9016e4d7f2b7c18c81527bc60e917811f681fa45a7a6d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f5660fd9-2e72-4c98-99ae-ccc4252f4582", "node_type": "1", "metadata": {"Header_1": " Evaluation Of Multi-Modal RAG"}, "hash": "39fb595fa99cd4b22bc602cb38e1218578ea23dcfa03275cf102f9464b56767d", "class_name": "RelatedNodeInfo"}}, "text": "Evaluation Of Text-Only RAG\n\nFor text-only RAG, the standard approach is to separately consider the\nevaluation of two stages: Retrieval and Generation.\n\n**Retriever Evaluation:** are the retrieved documents relevant to the user\nquery?\n\nSome of the more popular metrics for retrieval evaluation include **recall,\nhit rate** , **mean reciprocal rank, mean average precision, and normalized\ndiscounted cumulative gain.** The first two of these metrics recall and hit\nrate, don\u2019t consider the position (or ranking) of the relevant documents,\nwhereas all the others do in their own respective ways.\n\n**Generator Evaluation:** does the response use the retrieved documents to\nsufficiently answer the user query?\n\nIn abstractive question-answering systems, like the kinds we\u2019re talking about\nin this blog, measuring the generated response is made more tricky due to the\nfact that there isn\u2019t just one way to sufficiently answer a query in written\nlanguage \u2014 there\u2019s plenty!\n\nSo, in this case, our measurement relies on subjective judgement, which can be\nperformed by humans, though this is costly and unscalable. An alternative\napproach, is to use an LLM judge to measure things like _relevancy_ and\n_faithfulness_ .\n\n  * Relevancy: considers textual context and evaluates how much the generated response matches the query. \n  * Faithfulness: evaluates how much the generated response matches the retrieved textual context. \n\nFor both of these, the retrieved context as well as the query and generated\nresponse are passed to the LLM judge. (This pattern of using an LLM to judge\nthe responses has been termed by some researchers of the space as LLM-As-A-\nJudge ( [ Zheng et al., 2023 ](https://arxiv.org/abs/2306.05685) ).)\n\nCurrently, the ` llama-index (v0.9.2) ` library supports hit-rate and mean\nreciprocal rank for retrieval evaluation, as well as relevancy, faithfulness\nand a few others for generator evaluation. (Check out our Evaluation guides in\nour [ docs\n](https://docs.llamaindex.ai/en/stable/optimizing/evaluation/evaluation.html)\n!).", "mimetype": "text/plain", "start_char_idx": 1531, "end_char_idx": 3572, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5660fd9-2e72-4c98-99ae-ccc4252f4582": {"__data__": {"id_": "f5660fd9-2e72-4c98-99ae-ccc4252f4582", "embedding": null, "metadata": {"Header_1": " Evaluation Of Multi-Modal RAG", "filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d32d4ea-6419-481a-8422-8f9fb38c3caa", "node_type": "4", "metadata": {"filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "hash": "d6772b63975af35a2a7cc00ea18a7f02900a2dfb9c17e6b3713f476752c70b28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81e01632-66a7-4ef3-acac-262903fde3cf", "node_type": "1", "metadata": {"Header_1": " Evaluation Of Text-Only RAG", "filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "hash": "a367a16820081258aad3c17bedc30080a985e580db98c687f7a715f22242d91e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e7718eb-3803-4dc3-bbf4-d57ac7908676", "node_type": "1", "metadata": {"Header_1": " A Few Important Remarks"}, "hash": "61292aa862d74122b5af15c39035807f243d30a0fae9ce7310a920b03af5d3d2", "class_name": "RelatedNodeInfo"}}, "text": "Evaluation Of Multi-Modal RAG\n\nFor the multi-modal case, the evaluation can (and should) still be carried out\nwith respect to the different stages of retrieval and generation.\n\n**Separating Out Retrieval Evaluation For Text and Image Modalities**\n\nNow that retrieved documents can come in two forms, it would seem most\nsensible to consider computing the usual retrieval evaluation metrics\nseparately for images and text. In this way, you have more knowledge as to\nwhich aspect of the multi-modal retriever is working well and what isn\u2019t. One\ncan then apply a desired weighting scheme to establish a single aggregated\nretrieval score per metric.\n\nHit Rate Mean Reciprocal Rank Text 0.95 0.88 Images 0.88 0.75\n\nTable 3: Retrieval evaluation in multi-modal scenario.\n\n**Using Multi-Modal LLMs For Generator Evaluations (LMM-As-A-Judge)**\n\nMulti-modal models (i.e., LMMs) like OpenAI\u2019s GPT-4V or open-source\nalternatives like LLaVA are able to take in both input and image context to\nproduce an answer the user query. As in text-only RAG, we are also concerned\nabout the \u201crelevancy\u201d and \u201cfaithfulness\u201d of these generated answers. But in\norder to be able to compute such metrics in the multi-modal case, we would\nneed a judge model that is also able to take in the context images and text\ndata. Thus, in the multi-modal case, we adopt the LMM-As-A-Judge pattern in\norder to compute relevancy and faithfulness as well as other related metrics!\n\n  * Relevancy (multi-modal): considers **textual and visual context** and evaluates how much the generated response matches the query. \n  * Faithfulness (multi-modal): evaluates how much the generated response matches the retrieved **textual and visual context** . \n\nIf you want to test these out, then you\u2019re in luck as we\u2019ve recently released\nour beta Multi-Modal Evaluator abstractions! See the code snippet below for\nhow one can use these abstractions to perform their respective evaluations on\na generated response to a given query.\n\n    \n    \n    from llama_index.evaluation.multi_modal import (\n    \tMultiModalRelevancyEvaluator,\n    \tMultiModalFaithfulnessEvaluator\n    )\n    from llama_index.multi_modal_llm import OpenAIMultiModal\n    \n    relevancy_judge = MultiModalRelevancyEvaluator(\n        multi_modal_llm=OpenAIMultiModal(\n            model=\"gpt-4-vision-preview\",\n            max_new_tokens=300,\n        )\n    )\n    \n    faithfulness_judge = MultiModalRelevancyEvaluator(\n        multi_modal_llm=OpenAIMultiModal(\n            model=\"gpt-4-vision-preview\",\n            max_new_tokens=300,\n        )\n    )\n    \n    # Generated response to a query and its retrieved context information\n    query = ...\n    response = ...\n    contexts = ...  # retrieved text contexts\n    image_paths = ...  # retrieved image contexts\n    \n    # Evaluations\n    relevancy_eval = relevancy_judge.evaluate(\n     query=query,\n     response=response,\n     contexts=contexts,\n     image_paths=image_paths\n    )\n    \n    faithfulness_eval = faithfulness_judge.evaluate(\n     query=query,\n     response=response,\n     contexts=contexts,\n     image_paths=image_paths\n    )", "mimetype": "text/plain", "start_char_idx": 3577, "end_char_idx": 6677, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e7718eb-3803-4dc3-bbf4-d57ac7908676": {"__data__": {"id_": "9e7718eb-3803-4dc3-bbf4-d57ac7908676", "embedding": null, "metadata": {"Header_1": " A Few Important Remarks", "filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d32d4ea-6419-481a-8422-8f9fb38c3caa", "node_type": "4", "metadata": {"filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "hash": "d6772b63975af35a2a7cc00ea18a7f02900a2dfb9c17e6b3713f476752c70b28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5660fd9-2e72-4c98-99ae-ccc4252f4582", "node_type": "1", "metadata": {"Header_1": " Evaluation Of Multi-Modal RAG", "filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "hash": "bd671f5f4ee4d2049ee27d444b70383a7e314684b340cf66236cb40513bb8a5c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f19749f5-b9df-4bf9-b01a-5ec0a4839429", "node_type": "1", "metadata": {"Header_1": " Go forth and evaluate"}, "hash": "b3c01548f6b9423fadb2ba22130630e3197aff041237f996a3235b9ffa9ed3dd", "class_name": "RelatedNodeInfo"}}, "text": "A Few Important Remarks\n\nFirst, it is worth mentioning that using LLMs or LMMs to judge generated\nresponses has its drawbacks. These judges are generative models themselves and\ncan suffer from hallucinations and other inconsistencies. Though studies have\nshown that strong LLMs can align to human judgments at a relatively high rate\n( [ Zheng et al., 2023 ](https://arxiv.org/abs/2306.05685) ), using them in\nproduction systems should be handled with higher standards of care. At time of\nwriting, there has no been study to show that strong LMMs can also align well\nto human judgements.\n\nSecondly, the evaluation of a generator touches mostly on the evaluation of\nits knowledge and reasoning capabilities. There are other important dimensions\non which to evaluate LLMs and LMMs, including Alignment and Safety \u2014 see [\nEvaluating LMMs: A Comprehensive Survey\n](https://arxiv.org/pdf/2310.19736.pdf) for more information.", "mimetype": "text/plain", "start_char_idx": 6682, "end_char_idx": 7601, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f19749f5-b9df-4bf9-b01a-5ec0a4839429": {"__data__": {"id_": "f19749f5-b9df-4bf9-b01a-5ec0a4839429", "embedding": null, "metadata": {"Header_1": " Go forth and evaluate", "filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d32d4ea-6419-481a-8422-8f9fb38c3caa", "node_type": "4", "metadata": {"filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "hash": "d6772b63975af35a2a7cc00ea18a7f02900a2dfb9c17e6b3713f476752c70b28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e7718eb-3803-4dc3-bbf4-d57ac7908676", "node_type": "1", "metadata": {"Header_1": " A Few Important Remarks", "filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "hash": "16ee78a3f9927f05be81234f9f4d9e3cea582b0ddd05e777578868f9204b4c85", "class_name": "RelatedNodeInfo"}}, "text": "Go forth and evaluate\n\nIn this post, we covered how evaluation can be performed on multi-modal RAG\nsystems. We believe that separating out the retrieval evaluations per\nmodalities for increased visibility as well as the LMM-As-A-Judge represent a\nsensible extension of the evaluation framework for text-only RAG. We encourage\nyou to check out our practical notebook guides as well as docs for more\ninformation on how you can not only build Multi-Modal RAGs but also adequately\nevaluate them!\n\n  * [ Notebook guide for evaluating Multi-Modal RAG systems with LlamaIndex ](https://docs.llamaindex.ai/en/stable/examples/evaluation/multi_modal/multi_modal_rag_evaluation.html)\n  * [ Intro to Multi-Modal RAG ](/multi-modal-rag-621de7525fea)\n  * [ Docs/guides on Multi-Modal Abstractions ](https://docs.llamaindex.ai/en/stable/module_guides/models/multi_modal.html)", "mimetype": "text/plain", "start_char_idx": 7606, "end_char_idx": 8466, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"b1df07a3-4d25-48ad-ad55-a2c0314d0826": {"doc_hash": "5638be3e2abba0891f0353d7c045e0eda54b1436c37542b43901f25c6d93f2bd", "ref_doc_id": "9d32d4ea-6419-481a-8422-8f9fb38c3caa"}, "6c4e1857-eb03-4054-877f-5e607b1bb78a": {"doc_hash": "7a77fc71c5b84fa3d0d9016e4d7f2b7c18c81527bc60e917811f681fa45a7a6d", "ref_doc_id": "9d32d4ea-6419-481a-8422-8f9fb38c3caa"}, "81e01632-66a7-4ef3-acac-262903fde3cf": {"doc_hash": "a367a16820081258aad3c17bedc30080a985e580db98c687f7a715f22242d91e", "ref_doc_id": "9d32d4ea-6419-481a-8422-8f9fb38c3caa"}, "f5660fd9-2e72-4c98-99ae-ccc4252f4582": {"doc_hash": "bd671f5f4ee4d2049ee27d444b70383a7e314684b340cf66236cb40513bb8a5c", "ref_doc_id": "9d32d4ea-6419-481a-8422-8f9fb38c3caa"}, "9e7718eb-3803-4dc3-bbf4-d57ac7908676": {"doc_hash": "16ee78a3f9927f05be81234f9f4d9e3cea582b0ddd05e777578868f9204b4c85", "ref_doc_id": "9d32d4ea-6419-481a-8422-8f9fb38c3caa"}, "f19749f5-b9df-4bf9-b01a-5ec0a4839429": {"doc_hash": "23bd3d1dae7f94e428d698eda202edafc3fb128da6d41002f88d6f3e8ae4eae5", "ref_doc_id": "9d32d4ea-6419-481a-8422-8f9fb38c3caa"}}, "docstore/ref_doc_info": {"9d32d4ea-6419-481a-8422-8f9fb38c3caa": {"node_ids": ["b1df07a3-4d25-48ad-ad55-a2c0314d0826", "6c4e1857-eb03-4054-877f-5e607b1bb78a", "81e01632-66a7-4ef3-acac-262903fde3cf", "f5660fd9-2e72-4c98-99ae-ccc4252f4582", "9e7718eb-3803-4dc3-bbf4-d57ac7908676", "f19749f5-b9df-4bf9-b01a-5ec0a4839429"], "metadata": {"filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}}}}