{"docstore/data": {"5f81332a-ea96-4b8c-8632-6ad9752f8453": {"__data__": {"id_": "5f81332a-ea96-4b8c-8632-6ad9752f8453", "embedding": null, "metadata": {"Header_1": " Introduction", "filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ecc12aef-43d2-4aef-90e1-b601f2356e96", "node_type": "4", "metadata": {"filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "5f1ead524c7a5ea5c2b8bb1f5ae83cf915628d1032ab2cd4bc701b369b8b0b57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "55d61afb-186c-43ad-8d07-4023bd26ee1d", "node_type": "1", "metadata": {"Header_1": " Why Chunk Size Matters"}, "hash": "c166ddb0c2c148caff61e6ab1f22209a460f4a7065676baae891f6e12e9f82a0", "class_name": "RelatedNodeInfo"}}, "text": "Introduction\n\nRetrieval-augmented generation (RAG) has introduced an innovative approach\nthat fuses the extensive retrieval capabilities of search systems with the\nLLM. When implementing a RAG system, one critical parameter that governs the\nsystem\u2019s efficiency and performance is the ` chunk_size ` . How does one\ndiscern the optimal chunk size for seamless retrieval? This is where\nLlamaIndex ` Response Evaluation ` comes in handy. In this blog post, we'll\nguide you through the steps to determine the best ` chunk size ` using\nLlamaIndex\u2019s ` Response Evaluation ` module. If you're unfamiliar with the `\nResponse ` Evaluation module, we recommend reviewing its [ documentation\n](https://docs.llamaindex.ai/en/latest/core_modules/supporting_modules/evaluation/modules.html)\nbefore proceeding.", "mimetype": "text/plain", "start_char_idx": 3, "end_char_idx": 797, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "55d61afb-186c-43ad-8d07-4023bd26ee1d": {"__data__": {"id_": "55d61afb-186c-43ad-8d07-4023bd26ee1d", "embedding": null, "metadata": {"Header_1": " Why Chunk Size Matters", "filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ecc12aef-43d2-4aef-90e1-b601f2356e96", "node_type": "4", "metadata": {"filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "5f1ead524c7a5ea5c2b8bb1f5ae83cf915628d1032ab2cd4bc701b369b8b0b57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f81332a-ea96-4b8c-8632-6ad9752f8453", "node_type": "1", "metadata": {"Header_1": " Introduction", "filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "5fb8589007f258dbd09c3f3867f18eeb0cbda2008ce529389f58029089eae80e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6e4260a-f559-4810-a36a-9192927a28cd", "node_type": "1", "metadata": {"Header_1": " Setup"}, "hash": "2ffb753b4afbbc9db83a0ad44ef60a8e7ff824b0a5582be70527553a5f1ac15e", "class_name": "RelatedNodeInfo"}}, "text": "Why Chunk Size Matters\n\nChoosing the right ` chunk_size ` is a critical decision that can influence\nthe efficiency and accuracy of a RAG system in several ways:\n\n  1. **Relevance and Granularity** : A small ` chunk_size ` , like 128, yields more granular chunks. This granularity, however, presents a risk: vital information might not be among the top retrieved chunks, especially if the ` similarity_top_k ` setting is as restrictive as 2. Conversely, a chunk size of 512 is likely to encompass all necessary information within the top chunks, ensuring that answers to queries are readily available. To navigate this, we employ the Faithfulness and Relevancy metrics. These measure the absence of \u2018hallucinations\u2019 and the \u2018relevancy\u2019 of responses based on the query and the retrieved contexts respectively. \n  2. **Response Generation Time** : As the ` chunk_size ` increases, so does the volume of information directed into the LLM to generate an answer. While this can ensure a more comprehensive context, it might also slow down the system. Ensuring that the added depth doesn't compromise the system's responsiveness is crucial. \n\nIn essence, determining the optimal ` chunk_size ` is about striking a\nbalance: capturing all essential information without sacrificing speed. It's\nvital to undergo thorough testing with various sizes to find a configuration\nthat suits the specific use case and dataset.\n\nFor a practical evaluation in choosing the right ` chunk_size ` , you can\naccess and run the following setup on this [ **Google Colab Notebook**\n](https://colab.research.google.com/drive/1LPvJyEON6btMpubYdwySfNs0FuNR9nza?usp=sharing)\n.", "mimetype": "text/plain", "start_char_idx": 802, "end_char_idx": 2445, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6e4260a-f559-4810-a36a-9192927a28cd": {"__data__": {"id_": "e6e4260a-f559-4810-a36a-9192927a28cd", "embedding": null, "metadata": {"Header_1": " Setup", "filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ecc12aef-43d2-4aef-90e1-b601f2356e96", "node_type": "4", "metadata": {"filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "5f1ead524c7a5ea5c2b8bb1f5ae83cf915628d1032ab2cd4bc701b369b8b0b57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "55d61afb-186c-43ad-8d07-4023bd26ee1d", "node_type": "1", "metadata": {"Header_1": " Why Chunk Size Matters", "filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "007a001e6bbc59f1ef10807be6d462a5c2d1d6ed5c8cea45f89fa22bb3b08225", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4598ae24-afee-4ce9-b048-66ce3ab7631b", "node_type": "1", "metadata": {"Header_1": " Download Data"}, "hash": "32096171d766d649086214cc814a29a26cc679b2b0db4951a3b8dfa0fd281148", "class_name": "RelatedNodeInfo"}}, "text": "Setup\n\nBefore embarking on the experiment, we need to ensure all requisite modules\nare imported:\n\n    \n    \n    import nest_asyncio\n    \n    nest_asyncio.apply()\n    \n    from llama_index import (\n        SimpleDirectoryReader,\n        VectorStoreIndex,\n        ServiceContext,\n    )\n    from llama_index.evaluation import (\n        DatasetGenerator,\n        FaithfulnessEvaluator,\n        RelevancyEvaluator\n    )\n    from llama_index.llms import OpenAI\n    \n    import openai\n    import time\n    openai.api_key = 'OPENAI-API-KEY'", "mimetype": "text/plain", "start_char_idx": 2450, "end_char_idx": 2981, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4598ae24-afee-4ce9-b048-66ce3ab7631b": {"__data__": {"id_": "4598ae24-afee-4ce9-b048-66ce3ab7631b", "embedding": null, "metadata": {"Header_1": " Download Data", "filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ecc12aef-43d2-4aef-90e1-b601f2356e96", "node_type": "4", "metadata": {"filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "5f1ead524c7a5ea5c2b8bb1f5ae83cf915628d1032ab2cd4bc701b369b8b0b57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6e4260a-f559-4810-a36a-9192927a28cd", "node_type": "1", "metadata": {"Header_1": " Setup", "filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "df41b4356f5e12c7f7297fd2220794afe50c93a54bd1d5988efb09778fa2e965", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56a8ca6f-910f-4d98-9e2e-89441013f273", "node_type": "1", "metadata": {"Header_1": " Load Data"}, "hash": "0824eb9b04f858f8c4ccca365abafd06d22494efac328562e25ce399150f606b", "class_name": "RelatedNodeInfo"}}, "text": "Download Data\n\nWe\u2019ll be using the Uber 10K SEC Filings for 2021 for this experiment.\n\n    \n    \n    !mkdir -p 'data/10k/'\n    !wget 'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'", "mimetype": "text/plain", "start_char_idx": 2986, "end_char_idx": 3245, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56a8ca6f-910f-4d98-9e2e-89441013f273": {"__data__": {"id_": "56a8ca6f-910f-4d98-9e2e-89441013f273", "embedding": null, "metadata": {"Header_1": " Load Data", "filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ecc12aef-43d2-4aef-90e1-b601f2356e96", "node_type": "4", "metadata": {"filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "5f1ead524c7a5ea5c2b8bb1f5ae83cf915628d1032ab2cd4bc701b369b8b0b57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4598ae24-afee-4ce9-b048-66ce3ab7631b", "node_type": "1", "metadata": {"Header_1": " Download Data", "filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "89ec9668580b395fe3bc436402351256080cd4e21ce76c4ccaa679e88e5d0838", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad826995-55c0-4c92-a7fa-c9d16559c83e", "node_type": "1", "metadata": {"Header_1": " Question Generation"}, "hash": "d9b9d9ea2f8238da9d9c317f1853700eaa954d57f730d658f9e6e09c35f75775", "class_name": "RelatedNodeInfo"}}, "text": "Load Data\n\nLet\u2019s load our document.\n\n    \n    \n    documents = SimpleDirectoryReader(\"./data/10k/\").load_data()", "mimetype": "text/plain", "start_char_idx": 3250, "end_char_idx": 3361, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad826995-55c0-4c92-a7fa-c9d16559c83e": {"__data__": {"id_": "ad826995-55c0-4c92-a7fa-c9d16559c83e", "embedding": null, "metadata": {"Header_1": " Question Generation", "filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ecc12aef-43d2-4aef-90e1-b601f2356e96", "node_type": "4", "metadata": {"filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "5f1ead524c7a5ea5c2b8bb1f5ae83cf915628d1032ab2cd4bc701b369b8b0b57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56a8ca6f-910f-4d98-9e2e-89441013f273", "node_type": "1", "metadata": {"Header_1": " Load Data", "filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "95ef7dea12cf32ab38a1495bd1019f355e8f795ae80d9f5e87fe347c16ba8b28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc11947d-6585-4138-9222-14ce0f188b75", "node_type": "1", "metadata": {"Header_1": " Setting Up Evaluators"}, "hash": "77568b95ada7fd72007c2c64bbf4c967ad68f8924ecb3b4341d36b2d9066ee02", "class_name": "RelatedNodeInfo"}}, "text": "Question Generation\n\nTo select the right ` chunk_size ` , we'll compute metrics like Average\nResponse time, Faithfulness, and Relevancy for various ` chunk_sizes ` . The `\nDatasetGenerator ` will help us generate questions from the documents.\n\n    \n    \n    data_generator = DatasetGenerator.from_documents(documents)\n    eval_questions = data_generator.generate_questions_from_nodes()", "mimetype": "text/plain", "start_char_idx": 3366, "end_char_idx": 3751, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc11947d-6585-4138-9222-14ce0f188b75": {"__data__": {"id_": "bc11947d-6585-4138-9222-14ce0f188b75", "embedding": null, "metadata": {"Header_1": " Setting Up Evaluators", "filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ecc12aef-43d2-4aef-90e1-b601f2356e96", "node_type": "4", "metadata": {"filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "5f1ead524c7a5ea5c2b8bb1f5ae83cf915628d1032ab2cd4bc701b369b8b0b57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad826995-55c0-4c92-a7fa-c9d16559c83e", "node_type": "1", "metadata": {"Header_1": " Question Generation", "filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "4279084319f7aa2d3f1bfe627a5006fc083c1f72ad5a0f1a484407b776a348ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89b4021b-fe2b-43dd-91e2-87a4bfb3bf3e", "node_type": "1", "metadata": {"Header_1": " Response Evaluation For A Chunk Size"}, "hash": "b9a20603bc0bcd31c00bf4ec648c30d1713ad69dc4f5c5bdd50832240b58e9a9", "class_name": "RelatedNodeInfo"}}, "text": "Setting Up Evaluators\n\nWe are setting up the GPT-4 model to serve as the backbone for evaluating the\nresponses generated during the experiment. Two evaluators, `\nFaithfulnessEvaluator ` and ` RelevancyEvaluator ` , are initialised with the\n` service_context ` .\n\n  1. **Faithfulness Evaluator** \u2014 It is useful for measuring if the response was hallucinated and measures if the response from a query engine matches any source nodes. \n  2. **Relevancy Evaluator** \u2014 It is useful for measuring if the query was actually answered by the response and measures if the response + source nodes match the query. \n\n    \n    \n    # We will use GPT-4 for evaluating the responses\n    gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n    \n    # Define service context for GPT-4 for evaluation\n    service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n    \n    # Define Faithfulness and Relevancy Evaluators which are based on GPT-4\n    faithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\n    relevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4)", "mimetype": "text/plain", "start_char_idx": 3756, "end_char_idx": 4839, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89b4021b-fe2b-43dd-91e2-87a4bfb3bf3e": {"__data__": {"id_": "89b4021b-fe2b-43dd-91e2-87a4bfb3bf3e", "embedding": null, "metadata": {"Header_1": " Response Evaluation For A Chunk Size", "filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ecc12aef-43d2-4aef-90e1-b601f2356e96", "node_type": "4", "metadata": {"filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "5f1ead524c7a5ea5c2b8bb1f5ae83cf915628d1032ab2cd4bc701b369b8b0b57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc11947d-6585-4138-9222-14ce0f188b75", "node_type": "1", "metadata": {"Header_1": " Setting Up Evaluators", "filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "1ee8ac63e977934b73f13f2cce691a722d07dac7a30605d9d197c679831b27be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa0612de-0873-4534-9abe-0757df552e71", "node_type": "1", "metadata": {"Header_1": " Testing Across Different Chunk Sizes"}, "hash": "722b94de6025300382ea3b9e7dd8bdd98f07b2dbaf8a58818cf0e572d8b57c28", "class_name": "RelatedNodeInfo"}}, "text": "Response Evaluation For A Chunk Size\n\nWe evaluate each chunk_size based on 3 metrics.\n\n  1. Average Response Time. \n  2. Average Faithfulness. \n  3. Average Relevancy. \n\nHere\u2019s a function, ` evaluate_response_time_and_accuracy ` , that does just\nthat which has:\n\n  1. VectorIndex Creation. \n  2. Building the Query Engine. \n  3. Metrics Calculation. \n\n    \n    \n    # Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size\n    # We use GPT-3.5-Turbo to generate response and GPT-4 to evaluate it.\n    def evaluate_response_time_and_accuracy(chunk_size, eval_questions):\n        \"\"\"\n        Evaluate the average response time, faithfulness, and relevancy of responses generated by GPT-3.5-turbo for a given chunk size.\n        \n        Parameters:\n        chunk_size (int): The size of data chunks being processed.\n        \n        Returns:\n        tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.\n        \"\"\"\n    \n        total_response_time = 0\n        total_faithfulness = 0\n        total_relevancy = 0\n    \n        # create vector index\n        llm = OpenAI(model=\"gpt-3.5-turbo\")\n        service_context = ServiceContext.from_defaults(llm=llm, chunk_size=chunk_size)\n        vector_index = VectorStoreIndex.from_documents(\n            eval_documents, service_context=service_context\n        )\n        # build query engine\n        query_engine = vector_index.as_query_engine()\n        num_questions = len(eval_questions)\n    \n        # Iterate over each question in eval_questions to compute metrics.\n        # While BatchEvalRunner can be used for faster evaluations (see: https://docs.llamaindex.ai/en/latest/examples/evaluation/batch_eval.html),\n        # we're using a loop here to specifically measure response time for different chunk sizes.\n        for question in eval_questions:\n            start_time = time.time()\n            response_vector = query_engine.query(question)\n            elapsed_time = time.time() - start_time\n            \n            faithfulness_result = faithfulness_gpt4.evaluate_response(\n                response=response_vector\n            ).passing\n            \n            relevancy_result = relevancy_gpt4.evaluate_response(\n                query=question, response=response_vector\n            ).passing\n    \n            total_response_time += elapsed_time\n            total_faithfulness += faithfulness_result\n            total_relevancy += relevancy_result\n    \n        average_response_time = total_response_time / num_questions\n        average_faithfulness = total_faithfulness / num_questions\n        average_relevancy = total_relevancy / num_questions\n    \n        return average_response_time, average_faithfulness, average_relevancy", "mimetype": "text/plain", "start_char_idx": 4844, "end_char_idx": 7633, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fa0612de-0873-4534-9abe-0757df552e71": {"__data__": {"id_": "fa0612de-0873-4534-9abe-0757df552e71", "embedding": null, "metadata": {"Header_1": " Testing Across Different Chunk Sizes", "filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ecc12aef-43d2-4aef-90e1-b601f2356e96", "node_type": "4", "metadata": {"filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "5f1ead524c7a5ea5c2b8bb1f5ae83cf915628d1032ab2cd4bc701b369b8b0b57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89b4021b-fe2b-43dd-91e2-87a4bfb3bf3e", "node_type": "1", "metadata": {"Header_1": " Response Evaluation For A Chunk Size", "filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "009550b154f8698e8cd1ce128e9a722570a8e2f7359dd1f65fc0913cc2052a36", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d519d218-8040-4015-a7a3-3aecf82022bc", "node_type": "1", "metadata": {"Header_1": " Bringing It All Together"}, "hash": "08b959c057d61fa60fc8735fd81ac44746d96a7d1544a994a07cb7a22af94fee", "class_name": "RelatedNodeInfo"}}, "text": "Testing Across Different Chunk Sizes\n\nWe\u2019ll evaluate a range of chunk sizes to identify which offers the most\npromising metrics.\n\n    \n    \n    chunk_sizes = [128, 256, 512, 1024, 2048]\n    \n    for chunk_size in chunk_sizes:\n      avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size, eval_questions)\n      print(f\"Chunk size {chunk_size} - Average Response time: {avg_response_time:.2f}s, Average Faithfulness: {avg_faithfulness:.2f}, Average Relevancy: {avg_relevancy:.2f}\")", "mimetype": "text/plain", "start_char_idx": 7638, "end_char_idx": 8162, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d519d218-8040-4015-a7a3-3aecf82022bc": {"__data__": {"id_": "d519d218-8040-4015-a7a3-3aecf82022bc", "embedding": null, "metadata": {"Header_1": " Bringing It All Together", "filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ecc12aef-43d2-4aef-90e1-b601f2356e96", "node_type": "4", "metadata": {"filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "5f1ead524c7a5ea5c2b8bb1f5ae83cf915628d1032ab2cd4bc701b369b8b0b57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa0612de-0873-4534-9abe-0757df552e71", "node_type": "1", "metadata": {"Header_1": " Testing Across Different Chunk Sizes", "filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "96cb41d6a30873c5a00854d4f37530df4aae7857ff93470ea994903d125e6c6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1074a87e-69c4-4c17-814f-249e0b5e3621", "node_type": "1", "metadata": {"Header_1": " Result"}, "hash": "a583fd077ae35f255f4a53be2404a443cfdebb721ada6ad61a2c6ebf5f1acf72", "class_name": "RelatedNodeInfo"}}, "text": "Bringing It All Together\n\nLet\u2019s compile the processes:\n\n    \n    \n    import nest_asyncio\n    \n    nest_asyncio.apply()\n    \n    from llama_index import (\n        SimpleDirectoryReader,\n        VectorStoreIndex,\n        ServiceContext,\n    )\n    from llama_index.evaluation import (\n        DatasetGenerator,\n        FaithfulnessEvaluator,\n        RelevancyEvaluator\n    )\n    from llama_index.llms import OpenAI\n    \n    import openai\n    import time\n    \n    openai.api_key = 'OPENAI-API-KEY'\n    \n    # Download Data\n    !mkdir -p 'data/10k/'\n    !wget 'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'\n    \n    # Load Data\n    reader = SimpleDirectoryReader(\"./data/10k/\")\n    documents = reader.load_data()\n    \n    # To evaluate for each chunk size, we will first generate a set of 40 questions from first 20 pages.\n    eval_documents = documents[:20]\n    data_generator = DatasetGenerator.from_documents()\n    eval_questions = data_generator.generate_questions_from_nodes(num = 20)\n    \n    # We will use GPT-4 for evaluating the responses\n    gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n    \n    # Define service context for GPT-4 for evaluation\n    service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n    \n    # Define Faithfulness and Relevancy Evaluators which are based on GPT-4\n    faithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\n    relevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4)\n    \n    # Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size\n    def evaluate_response_time_and_accuracy(chunk_size):\n        total_response_time = 0\n        total_faithfulness = 0\n        total_relevancy = 0\n    \n        # create vector index\n        llm = OpenAI(model=\"gpt-3.5-turbo\")\n        service_context = ServiceContext.from_defaults(llm=llm, chunk_size=chunk_size)\n        vector_index = VectorStoreIndex.from_documents(\n            eval_documents, service_context=service_context\n        )\n    \n        query_engine = vector_index.as_query_engine()\n        num_questions = len(eval_questions)\n    \n        for question in eval_questions:\n            start_time = time.time()\n            response_vector = query_engine.query(question)\n            elapsed_time = time.time() - start_time\n            \n            faithfulness_result = faithfulness_gpt4.evaluate_response(\n                response=response_vector\n            ).passing\n            \n            relevancy_result = relevancy_gpt4.evaluate_response(\n                query=question, response=response_vector\n            ).passing\n    \n            total_response_time += elapsed_time\n            total_faithfulness += faithfulness_result\n            total_relevancy += relevancy_result\n    \n        average_response_time = total_response_time / num_questions\n        average_faithfulness = total_faithfulness / num_questions\n        average_relevancy = total_relevancy / num_questions\n    \n        return average_response_time, average_faithfulness, average_relevancy\n    \n    # Iterate over different chunk sizes to evaluate the metrics to help fix the chunk size.\n    for chunk_size in [128, 256, 512, 1024, 2048]\n      avg_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size)\n      print(f\"Chunk size {chunk_size} - Average Response time: {avg_time:.2f}s, Average Faithfulness: {avg_faithfulness:.2f}, Average Relevancy: {avg_relevancy:.2f}\")", "mimetype": "text/plain", "start_char_idx": 8167, "end_char_idx": 11732, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1074a87e-69c4-4c17-814f-249e0b5e3621": {"__data__": {"id_": "1074a87e-69c4-4c17-814f-249e0b5e3621", "embedding": null, "metadata": {"Header_1": " Result", "filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ecc12aef-43d2-4aef-90e1-b601f2356e96", "node_type": "4", "metadata": {"filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "5f1ead524c7a5ea5c2b8bb1f5ae83cf915628d1032ab2cd4bc701b369b8b0b57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d519d218-8040-4015-a7a3-3aecf82022bc", "node_type": "1", "metadata": {"Header_1": " Bringing It All Together", "filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "4756d5c6b848b10d1877a509e2250c62c0fce8cf85f8b61b546ca946f1b11a81", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5fd80c6e-e379-4999-a08b-ddffe0a166b5", "node_type": "1", "metadata": {"Header_1": " Conclusion"}, "hash": "346e1ae58f5f44b48420add05a2ed7cf6c2d1fa618dd09718cf93d4b05628b5d", "class_name": "RelatedNodeInfo"}}, "text": "Result\n\nThe above table illustrates that as the chunk size increases, there is a minor\nuptick in the Average Response Time. Interestingly, the Average Faithfulness\nseems to reach its zenith at ` chunk_size ` of 1024, whereas Average Relevancy\nshows a consistent improvement with larger chunk sizes, also peaking at 1024.\nThis suggests that a chunk size of 1024 might strike an optimal balance\nbetween response time and the quality of the responses, measured in terms of\nfaithfulness and relevancy.", "mimetype": "text/plain", "start_char_idx": 11737, "end_char_idx": 12234, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5fd80c6e-e379-4999-a08b-ddffe0a166b5": {"__data__": {"id_": "5fd80c6e-e379-4999-a08b-ddffe0a166b5", "embedding": null, "metadata": {"Header_1": " Conclusion", "filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ecc12aef-43d2-4aef-90e1-b601f2356e96", "node_type": "4", "metadata": {"filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "5f1ead524c7a5ea5c2b8bb1f5ae83cf915628d1032ab2cd4bc701b369b8b0b57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1074a87e-69c4-4c17-814f-249e0b5e3621", "node_type": "1", "metadata": {"Header_1": " Result", "filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "93df9c8073fe27b2c42d99505a255a933d7ad449cfefa9314627ad48191e1164", "class_name": "RelatedNodeInfo"}}, "text": "Conclusion\n\nIdentifying the best chunk size for a RAG system is as much about intuition as\nit is empirical evidence. With LlamaIndex\u2019s ` Response Evaluation ` module,\nyou can experiment with various sizes and base your decisions on concrete\ndata. When building a RAG system, always remember that ` chunk_size ` is a\npivotal parameter. Invest the time to meticulously evaluate and adjust your\nchunk size for unmatched results.", "mimetype": "text/plain", "start_char_idx": 12239, "end_char_idx": 12664, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"5f81332a-ea96-4b8c-8632-6ad9752f8453": {"doc_hash": "5fb8589007f258dbd09c3f3867f18eeb0cbda2008ce529389f58029089eae80e", "ref_doc_id": "ecc12aef-43d2-4aef-90e1-b601f2356e96"}, "55d61afb-186c-43ad-8d07-4023bd26ee1d": {"doc_hash": "007a001e6bbc59f1ef10807be6d462a5c2d1d6ed5c8cea45f89fa22bb3b08225", "ref_doc_id": "ecc12aef-43d2-4aef-90e1-b601f2356e96"}, "e6e4260a-f559-4810-a36a-9192927a28cd": {"doc_hash": "df41b4356f5e12c7f7297fd2220794afe50c93a54bd1d5988efb09778fa2e965", "ref_doc_id": "ecc12aef-43d2-4aef-90e1-b601f2356e96"}, "4598ae24-afee-4ce9-b048-66ce3ab7631b": {"doc_hash": "89ec9668580b395fe3bc436402351256080cd4e21ce76c4ccaa679e88e5d0838", "ref_doc_id": "ecc12aef-43d2-4aef-90e1-b601f2356e96"}, "56a8ca6f-910f-4d98-9e2e-89441013f273": {"doc_hash": "95ef7dea12cf32ab38a1495bd1019f355e8f795ae80d9f5e87fe347c16ba8b28", "ref_doc_id": "ecc12aef-43d2-4aef-90e1-b601f2356e96"}, "ad826995-55c0-4c92-a7fa-c9d16559c83e": {"doc_hash": "4279084319f7aa2d3f1bfe627a5006fc083c1f72ad5a0f1a484407b776a348ca", "ref_doc_id": "ecc12aef-43d2-4aef-90e1-b601f2356e96"}, "bc11947d-6585-4138-9222-14ce0f188b75": {"doc_hash": "1ee8ac63e977934b73f13f2cce691a722d07dac7a30605d9d197c679831b27be", "ref_doc_id": "ecc12aef-43d2-4aef-90e1-b601f2356e96"}, "89b4021b-fe2b-43dd-91e2-87a4bfb3bf3e": {"doc_hash": "009550b154f8698e8cd1ce128e9a722570a8e2f7359dd1f65fc0913cc2052a36", "ref_doc_id": "ecc12aef-43d2-4aef-90e1-b601f2356e96"}, "fa0612de-0873-4534-9abe-0757df552e71": {"doc_hash": "96cb41d6a30873c5a00854d4f37530df4aae7857ff93470ea994903d125e6c6f", "ref_doc_id": "ecc12aef-43d2-4aef-90e1-b601f2356e96"}, "d519d218-8040-4015-a7a3-3aecf82022bc": {"doc_hash": "4756d5c6b848b10d1877a509e2250c62c0fce8cf85f8b61b546ca946f1b11a81", "ref_doc_id": "ecc12aef-43d2-4aef-90e1-b601f2356e96"}, "1074a87e-69c4-4c17-814f-249e0b5e3621": {"doc_hash": "93df9c8073fe27b2c42d99505a255a933d7ad449cfefa9314627ad48191e1164", "ref_doc_id": "ecc12aef-43d2-4aef-90e1-b601f2356e96"}, "5fd80c6e-e379-4999-a08b-ddffe0a166b5": {"doc_hash": "98fffb8b08c57b47e8ecc5e06026d60e4f2142b746f4a9d94b8200fee637d730", "ref_doc_id": "ecc12aef-43d2-4aef-90e1-b601f2356e96"}}, "docstore/ref_doc_info": {"ecc12aef-43d2-4aef-90e1-b601f2356e96": {"node_ids": ["5f81332a-ea96-4b8c-8632-6ad9752f8453", "55d61afb-186c-43ad-8d07-4023bd26ee1d", "e6e4260a-f559-4810-a36a-9192927a28cd", "4598ae24-afee-4ce9-b048-66ce3ab7631b", "56a8ca6f-910f-4d98-9e2e-89441013f273", "ad826995-55c0-4c92-a7fa-c9d16559c83e", "bc11947d-6585-4138-9222-14ce0f188b75", "89b4021b-fe2b-43dd-91e2-87a4bfb3bf3e", "fa0612de-0873-4534-9abe-0757df552e71", "d519d218-8040-4015-a7a3-3aecf82022bc", "1074a87e-69c4-4c17-814f-249e0b5e3621", "5fd80c6e-e379-4999-a08b-ddffe0a166b5"], "metadata": {"Header_1": " Introduction", "filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}}}}