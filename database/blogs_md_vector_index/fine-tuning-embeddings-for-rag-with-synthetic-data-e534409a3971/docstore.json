{"docstore/data": {"369b3c92-b779-4e9a-a8a8-bac6c0154daa": {"__data__": {"id_": "369b3c92-b779-4e9a-a8a8-bac6c0154daa", "embedding": null, "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "505add2d-b6fd-4ef2-ad63-908d45a8de33", "node_type": "4", "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "f733d11d3ca9cc9e059982e27f234bfd7c3adc087650da7b664397e36dd130ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c2fd7fe-bd55-4650-96b3-7f2a069dfecd", "node_type": "1", "metadata": {"Header_1": " Background/Context"}, "hash": "df98d5d59161e17724c1622c7c88f28fb1d65b9fc618d3d3b8977fabdcf11b78", "class_name": "RelatedNodeInfo"}}, "text": "**UPDATE 9/10/2023:** We\u2019ve included embedding finetuning abstractions into\nthe LlamaIndex repo, so this repo is technically outdated! Please check out\nour [ embedding fine-tuning guides ](https://gpt-\nindex.readthedocs.io/en/latest/end_to_end_tutorials/finetuning.html#finetuning-\nembeddings-for-better-retrieval-performance) in the core documentation.\n\nWe\u2019ve created a [ comprehensive, end-to-end guide ](https://github.com/run-\nllama/finetune-embedding) showing you how to fine-tune an embedding model to\nimprove performance of Retrieval Augmented Generation (RAG) systems over any\nunstructured text corpus (no labels required!).\n\nThe result is a [ **5\u201310% performance increase in retrieval evaluation\nmetrics** ](https://github.com/run-llama/finetune-\nembedding/blob/main/evaluate.ipynb) \u2014 our finetuned ` bge ` model almost\nreaches ` text-embedding-ada-002 ` levels of retrieval performance in terms of\nhit rate. This enables more accurate retrieval which leads to better RAG\nsystems as a whole.\n\nThis tutorial is helpful to _anyone_ building RAG systems:\n\n  * If you\u2019re new to finetuning, no problem! We have [ step by step notebooks ](https://github.com/run-llama/finetune-embedding#steps-for-running) walking through the key steps. Simply substitute the file links for your own data, and just run every cell. \n  * Finetuning embedding models is lightweight and doesn\u2019t require a GPU. These notebooks were tested on an M2 Macbook Pro. \n\n**Resources**\n\n  * Repo: [ https://github.com/run-llama/finetune-embedding ](https://github.com/run-llama/finetune-embedding)\n  * Notebooks: [ Dataset Generation ](https://github.com/run-llama/finetune-embedding/blob/main/generate_dataset.ipynb) , [ Finetuning ](https://github.com/run-llama/finetune-embedding/blob/main/finetune.ipynb) , [ Evaluation ](https://github.com/run-llama/finetune-embedding/blob/main/evaluate.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1871, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c2fd7fe-bd55-4650-96b3-7f2a069dfecd": {"__data__": {"id_": "1c2fd7fe-bd55-4650-96b3-7f2a069dfecd", "embedding": null, "metadata": {"Header_1": " Background/Context", "filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "505add2d-b6fd-4ef2-ad63-908d45a8de33", "node_type": "4", "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "f733d11d3ca9cc9e059982e27f234bfd7c3adc087650da7b664397e36dd130ee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "369b3c92-b779-4e9a-a8a8-bac6c0154daa", "node_type": "1", "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "63a7827a9ee77def208986f9f20c4f48ec9917fc4218827bf553ffdfe41a84ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb80d134-911d-483a-b1d7-0f15dc0e7654", "node_type": "1", "metadata": {"Header_1": " Background/Context", "Header_2": " **The Current RAG Stack**"}, "hash": "d6890e898253787ae7791887036a902a47968eb43ddf3e46e03d3aa3b14855cd", "class_name": "RelatedNodeInfo"}}, "text": "Background/Context", "mimetype": "text/plain", "start_char_idx": 1876, "end_char_idx": 1894, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb80d134-911d-483a-b1d7-0f15dc0e7654": {"__data__": {"id_": "eb80d134-911d-483a-b1d7-0f15dc0e7654", "embedding": null, "metadata": {"Header_1": " Background/Context", "Header_2": " **The Current RAG Stack**", "filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "505add2d-b6fd-4ef2-ad63-908d45a8de33", "node_type": "4", "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "f733d11d3ca9cc9e059982e27f234bfd7c3adc087650da7b664397e36dd130ee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c2fd7fe-bd55-4650-96b3-7f2a069dfecd", "node_type": "1", "metadata": {"Header_1": " Background/Context", "filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "91f5c200ed3a43f575fbd4ed5c173c5c9f84705fd5528ead766057b9de2e4c42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28faaf64-de02-4d60-8a22-4d3f50f69a6b", "node_type": "1", "metadata": {"Header_1": " Background/Context", "Header_2": " **How Can We Make Retrieval Better?**"}, "hash": "ce9503202074ab85a960ac87be39a9f6851d5deb70c7fcf29323e24e1597e154", "class_name": "RelatedNodeInfo"}}, "text": "**The Current RAG Stack**\n\nRAG is a popular paradigm for connecting Large Language Models (LLMs) with an\nexternal source of data that was not present in its training corpus. It pairs\na **retrieval model** over a knowledge bank with the LLM through its input\nprompt space. RAG stacks typically look like the following:\n\n  * **Indexing** : Prepare a corpus of unstructured text, parse/chunk it. Then _embed_ each chunk and put in a vector database. \n  * **Query-time:** _Retrieve_ context from the vector db using top-k embedding similarity lookup, and stuff context into the LLM input space. \n\n(Of course RAG can be much more advanced than this, and LlamaIndex provides\ntools for both [ simple and advanced RAG ](https://gpt-\nindex.readthedocs.io/en/latest/getting_started/concepts.html) )\n\nUnfortunately RAG is easy to prototype by cobbling together the different\ncomponents, but hard to productionize. The simple stack has many failure modes\nand oftentimes the issue lies with bad retrieval \u2014 if the returned context is\nirrelevant to the query, then the capability of the LLM is irrelevant; the\nanswer will always be bad.", "mimetype": "text/plain", "start_char_idx": 1900, "end_char_idx": 3022, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28faaf64-de02-4d60-8a22-4d3f50f69a6b": {"__data__": {"id_": "28faaf64-de02-4d60-8a22-4d3f50f69a6b", "embedding": null, "metadata": {"Header_1": " Background/Context", "Header_2": " **How Can We Make Retrieval Better?**", "filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "505add2d-b6fd-4ef2-ad63-908d45a8de33", "node_type": "4", "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "f733d11d3ca9cc9e059982e27f234bfd7c3adc087650da7b664397e36dd130ee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb80d134-911d-483a-b1d7-0f15dc0e7654", "node_type": "1", "metadata": {"Header_1": " Background/Context", "Header_2": " **The Current RAG Stack**", "filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "99c914889747a3573ec9875130617f7280b932bd658dbea051e005d5a9ca410d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e17d9e4b-9f97-4ef5-bd9d-e138d8cbe3f5", "node_type": "1", "metadata": {"Header_1": " Background/Context", "Header_2": " Challenges/Considerations"}, "hash": "46725005f77ae0f22f54208dcfb8ab29fc7f5fd24cbbe11d544fb2a7666b7df0", "class_name": "RelatedNodeInfo"}}, "text": "**How Can We Make Retrieval Better?**\n\nWe can try more sophisticated retrieval algorithms (e.g. hybrid search,\nreranking).\n\nAn [ insight ](https://twitter.com/jerryjliu0/status/1692931028963221929?s=20)\nfrom our recent [ production RAG webinar\n](https://www.youtube.com/watch?v=Zj5RCweUHIk) , however, is that the\nembeddings themselves may not live in an optimal latent space for your data.\nEmbeddings generated by pre-trained models may be close/far from each other\nbased on the pre-training objective, but may not completely align with your\nown retrieval objective. For instance, if you\u2019re building search over ML ArXiv\npapers, you may want the embeddings to align semantically with specific ML\nconcepts (e.g. \u201cLLMs\u201d, \u201cNLP\u201d) and not filler words \u201cThis paper is\u2026\u201d).\n\nFinetuning is a way to solve that. The concept of finetuning has become\nincreasingly popular in the LLM space, with [ technological advancements\n](https://github.com/artidoro/qlora) as well as [ easy-to-use services\n](https://platform.openai.com/docs/guides/fine-tuning) .\n\nIn this tutorial, we focus on **finetuning the embedding model.** We show how\nfinetuning the embedding model can lead to better retrieval performance.", "mimetype": "text/plain", "start_char_idx": 3028, "end_char_idx": 4220, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e17d9e4b-9f97-4ef5-bd9d-e138d8cbe3f5": {"__data__": {"id_": "e17d9e4b-9f97-4ef5-bd9d-e138d8cbe3f5", "embedding": null, "metadata": {"Header_1": " Background/Context", "Header_2": " Challenges/Considerations", "filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "505add2d-b6fd-4ef2-ad63-908d45a8de33", "node_type": "4", "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "f733d11d3ca9cc9e059982e27f234bfd7c3adc087650da7b664397e36dd130ee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28faaf64-de02-4d60-8a22-4d3f50f69a6b", "node_type": "1", "metadata": {"Header_1": " Background/Context", "Header_2": " **How Can We Make Retrieval Better?**", "filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "d05e4020ab46ac4efb08029a9519d5b84e6ff2f46067fbb22b6270f3497e51de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5411a3be-f96f-4065-8a57-6624cbd85782", "node_type": "1", "metadata": {"Header_1": " Walkthrough"}, "hash": "a5566773e7cf41666928b8d3ab55d227962789c7ffb5c18bb5e49a1dbb973e2d", "class_name": "RelatedNodeInfo"}}, "text": "Challenges/Considerations\n\nWhen you finetune embeddings, you need training examples. In the case of\nembeddings, this typically means that you have both \u201cpositive\u201d and \u201cnegative\u201d\nexamples \u2014 pairs of texts that should be close to each other and far from each\nother.\n\nAn issue is that we don\u2019t have these positive or negative examples apriori.\nGiven a dataset of unstructured text, is it possible to **automatically**\ngenerate these example pairs?\n\nWith LlamaIndex you can! We use LlamaIndex modules to automatically generate a\nset of questions from unstructured text chunks. These (question, chunk) pairs\nare then used as positive examples as training signals for the model (negative\nexamples are randomly sampled across other chunks).\n\nThe next section shows a full walkthrough across all of our modules.", "mimetype": "text/plain", "start_char_idx": 4226, "end_char_idx": 5029, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5411a3be-f96f-4065-8a57-6624cbd85782": {"__data__": {"id_": "5411a3be-f96f-4065-8a57-6624cbd85782", "embedding": null, "metadata": {"Header_1": " Walkthrough", "filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "505add2d-b6fd-4ef2-ad63-908d45a8de33", "node_type": "4", "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "f733d11d3ca9cc9e059982e27f234bfd7c3adc087650da7b664397e36dd130ee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e17d9e4b-9f97-4ef5-bd9d-e138d8cbe3f5", "node_type": "1", "metadata": {"Header_1": " Background/Context", "Header_2": " Challenges/Considerations", "filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "99e9371721184aad3244bb094350c4ed63b73c1b70c3bb8fc25c6b4ee866f132", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4fba390e-0010-48a4-89df-f7e73fb7c341", "node_type": "1", "metadata": {"Header_1": " Walkthrough", "Header_2": " Generating synthetic dataset for training and evaluation"}, "hash": "6071da64c467de965979cdc5f3d6f9cfd2ddb9110c4af5b7dc9b4cd91b16b584", "class_name": "RelatedNodeInfo"}}, "text": "Walkthrough\n\nAt a high-level, we do the following:\n\n  1. Generating synthetic dataset for training and evaluation ( [ Notebook ](https://github.com/run-llama/finetune-embedding/blob/main/generate_dataset.ipynb) ) \n  2. Finetuning an opensource embedding model ( [ Notebook ](https://github.com/run-llama/finetune-embedding/blob/main/finetune.ipynb) ) \n  3. Evaluating the embedding model ( [ Notebook ](https://github.com/run-llama/finetune-embedding/blob/main/evaluate.ipynb) )", "mimetype": "text/plain", "start_char_idx": 5034, "end_char_idx": 5512, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4fba390e-0010-48a4-89df-f7e73fb7c341": {"__data__": {"id_": "4fba390e-0010-48a4-89df-f7e73fb7c341", "embedding": null, "metadata": {"Header_1": " Walkthrough", "Header_2": " Generating synthetic dataset for training and evaluation", "filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "505add2d-b6fd-4ef2-ad63-908d45a8de33", "node_type": "4", "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "f733d11d3ca9cc9e059982e27f234bfd7c3adc087650da7b664397e36dd130ee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5411a3be-f96f-4065-8a57-6624cbd85782", "node_type": "1", "metadata": {"Header_1": " Walkthrough", "filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "b9d76e47e4c8eebcec6f822a3ab95e260a25d003153ac21936859c9c4769b5df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a75e8b41-4a8d-4792-bbe8-34671ca648d4", "node_type": "1", "metadata": {"Header_1": " Walkthrough", "Header_2": " Finetuning an opensource embedding model"}, "hash": "a047be816ba63a6aea39d46d7fce737ed59acd75351fb5b2cf27b09bac5abfd0", "class_name": "RelatedNodeInfo"}}, "text": "Generating synthetic dataset for training and evaluation\n\nThe key idea here is that we can leverage an LLM to generate hypothetical\nquestions that are best answered by a given piece of context. This allows us\nto generate synthetic positive pairs of (query, relevant documents) in a\nscalable way without requiring human labellers.\n\nMore concretely, we first process the given documents into a corpus of text\nchunks. We do this with the ` SimpleNodeParser ` module in LlamaIndex:\n\n    \n    \n    parser = SimpleNodeParser()\n    nodes = parser.get_nodes_from_documents(docs, show_progress=verbose)\n    corpus = {\n      node.node_id: node.get_content(metadata_mode=MetadataMode.NONE) \n      for node in nodes\n    }\n\nThen for each text chunk, we use LLM to generate a few hypothetical questions\nthat can be answered with information form that text chunk. The example prompt\nis shown below as well.\n\n    \n    \n    prompt_template = prompt_template or \"\"\"\\\n      Context information is below.\n      \n      ---------------------\n      {context_str}\n      ---------------------\n      \n      Given the context information and not prior knowledge.\n      generate only questions based on the below query.\n      \n      You are a Teacher/ Professor. Your task is to setup \\\n      {num_questions_per_chunk} questions for an upcoming \\\n      quiz/examination. The questions should be diverse in nature \\\n      across the document. Restrict the questions to the \\\n      context information provided.\"\n      \"\"\"\n    \n    # for a given node, extract questions (do this over all nodes in outer loop)\n    query = prompt_template.format(context_str=text, num_questions_per_chunk=num_questions_per_chunk)\n    response = llm.complete(query)\n    \n    result = str(response).strip().split(\"\\n\")\n    questions = [\n        re.sub(r\"^\\d+[\\).\\s]\", \"\", question).strip() for question in result\n    ]\n    questions = [question for question in questions if len(question) &gt; 0]\n    \n\nFinally, we collect all pairs of questions and text chunks as the dataset.\nExample query, chunk, and mapping is shown below.\n\n    \n    \n    # example query\n    f331640a-b407-4028-8db8-4b8db691dd34: \"What is the market value of Lyft's common stock held by non-affiliates as of June 30, 2021, based on the closing sales price of the Class A common stock on that date?\"\n    \n    # example corpus\n    d5554f3e-cdaf-41d7-ac49-8f0ffe3f5759:\"UNITED STATESSECURITIES AND...\"\n    \n    # example mapping\n    f331640a-b407-4028-8db8-4b8db691dd34: d5554f3e-cdaf-41d7-ac49-8f0ffe3f5759", "mimetype": "text/plain", "start_char_idx": 5519, "end_char_idx": 8042, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a75e8b41-4a8d-4792-bbe8-34671ca648d4": {"__data__": {"id_": "a75e8b41-4a8d-4792-bbe8-34671ca648d4", "embedding": null, "metadata": {"Header_1": " Walkthrough", "Header_2": " Finetuning an opensource embedding model", "filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "505add2d-b6fd-4ef2-ad63-908d45a8de33", "node_type": "4", "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "f733d11d3ca9cc9e059982e27f234bfd7c3adc087650da7b664397e36dd130ee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4fba390e-0010-48a4-89df-f7e73fb7c341", "node_type": "1", "metadata": {"Header_1": " Walkthrough", "Header_2": " Generating synthetic dataset for training and evaluation", "filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "5107a20ec7cccfc3054e4068d9280f3089ebc6699f9090fa477a48dfbd12e11c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b7e3059-306e-4a58-862a-a053c2e7418b", "node_type": "1", "metadata": {"Header_1": " Walkthrough", "Header_2": " Evaluating the embedding model"}, "hash": "c5fe44c1265d1632d7ac38e1c67b4a545bfc225b12a26c1a0fb49d7994adf389", "class_name": "RelatedNodeInfo"}}, "text": "Finetuning an opensource embedding model\n\nWe leverage the high-level model fitting API from ` sentencetransformers ` to\nvery easily setup a training process.\n\nWe use ` MultipleNegativesRankingLoss ` as the training object and `\nInformationRetrievalEvaluator ` as the evaluator during training. Also, we use\n` [ BAAI/bge-small-en ](https://huggingface.co/BAAI/bge-small-en) ` on Hugging\nFace as the base model and train for a small number of epochs.\n\n    \n    \n    # define model\n    model_id = \"BAAI/bge-small-en\"\n    model = SentenceTransformer(model_id)\n    \n    ...\n    \n    # define loss\n    from sentence_transformers import losses\n    loss = losses.MultipleNegativesRankingLoss(model)\n    \n    # define evaluator\n    from sentence_transformers.evaluation import InformationRetrievalEvaluator\n    # define over validation dataset\n    ...\n    evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs)\n    \n    # run training\n    ...\n    model.fit(\n        train_objectives=[(loader, loss)],\n        epochs=EPOCHS,\n        warmup_steps=warmup_steps,\n        output_path='exp_finetune',\n        show_progress_bar=True,\n        evaluator=evaluator, \n        evaluation_steps=50,\n    )", "mimetype": "text/plain", "start_char_idx": 8048, "end_char_idx": 9249, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b7e3059-306e-4a58-862a-a053c2e7418b": {"__data__": {"id_": "7b7e3059-306e-4a58-862a-a053c2e7418b", "embedding": null, "metadata": {"Header_1": " Walkthrough", "Header_2": " Evaluating the embedding model", "filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "505add2d-b6fd-4ef2-ad63-908d45a8de33", "node_type": "4", "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "f733d11d3ca9cc9e059982e27f234bfd7c3adc087650da7b664397e36dd130ee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a75e8b41-4a8d-4792-bbe8-34671ca648d4", "node_type": "1", "metadata": {"Header_1": " Walkthrough", "Header_2": " Finetuning an opensource embedding model", "filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "c9a9d58e14bfa97648302302c33669dad4c79cc1b0e81f54bcd06b33aa079f16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64b330e8-1d2e-46f3-8a3c-1a7f558775cf", "node_type": "1", "metadata": {"Header_1": " Conclusion"}, "hash": "085c45889b5ef94eef40f7faeffd5fb3e38a741730f57325793ba29538d690bc", "class_name": "RelatedNodeInfo"}}, "text": "Evaluating the embedding model\n\nWe compare the finetuned model against the base model, as well as the OpenAI\nembedding model ` text-embedding-ada-002 ` .\n\nWe evaluate with two main metrics:\n\n  * **Hit-rate metric:** For each (query, relevant_doc) pair, we retrieve the top-k documents with the query. It\u2019s a _hit_ if the results contain relevant_doc. \n  * ` InformationRetrievalEvaluator ` from sentence_transformers. This provides a comprehensive suite of metrics such as cosine similarity accuracy, precision, recall at different top-k values. \n\n**Results**\n\nIn terms of hit-rate metric, the base model gets 78% hit-rate on the\nvalidation dataset, and the fine-tuned model gets 84%. ` text-embedding-\nada-002 ` gets 87%, which means that our fine-tuned model is only 3% off!\n\nHit-rate for `text-embedding-ada-002`, base model, finetuned model\n\nThe InformationRetrievalEvaluator shows a similar improvement across an entire\nsuite of metrics. The fine-tuned model increases evaluation metrics by 5\u201310%\ncompared to the base-model.\n\nEvaluation suite from `InformationRetrievalEvaluator`", "mimetype": "text/plain", "start_char_idx": 9255, "end_char_idx": 10339, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64b330e8-1d2e-46f3-8a3c-1a7f558775cf": {"__data__": {"id_": "64b330e8-1d2e-46f3-8a3c-1a7f558775cf", "embedding": null, "metadata": {"Header_1": " Conclusion", "filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "505add2d-b6fd-4ef2-ad63-908d45a8de33", "node_type": "4", "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "f733d11d3ca9cc9e059982e27f234bfd7c3adc087650da7b664397e36dd130ee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b7e3059-306e-4a58-862a-a053c2e7418b", "node_type": "1", "metadata": {"Header_1": " Walkthrough", "Header_2": " Evaluating the embedding model", "filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "f73969ae63e2370ff1ac4a3b6d8360685fd4676dd36b7a6ca6e258f503167563", "class_name": "RelatedNodeInfo"}}, "text": "Conclusion\n\nWe successfully finetuned an embedding model over unlabeled, unstructured data\nto give better retrieval performance for downstream RAG systems. We show a\n5\u201310% improvement across all metrics!\n\n**Resources**\n\n(copied from intro)\n\n  * Repo: [ https://github.com/run-llama/finetune-embedding ](https://github.com/run-llama/finetune-embedding)\n  * Notebooks: [ Dataset Generation ](https://github.com/run-llama/finetune-embedding/blob/main/generate_dataset.ipynb) , [ Finetuning ](https://github.com/run-llama/finetune-embedding/blob/main/finetune.ipynb) , [ Evaluation ](https://github.com/run-llama/finetune-embedding/blob/main/evaluate.ipynb)", "mimetype": "text/plain", "start_char_idx": 10344, "end_char_idx": 10997, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"369b3c92-b779-4e9a-a8a8-bac6c0154daa": {"doc_hash": "63a7827a9ee77def208986f9f20c4f48ec9917fc4218827bf553ffdfe41a84ab", "ref_doc_id": "505add2d-b6fd-4ef2-ad63-908d45a8de33"}, "1c2fd7fe-bd55-4650-96b3-7f2a069dfecd": {"doc_hash": "91f5c200ed3a43f575fbd4ed5c173c5c9f84705fd5528ead766057b9de2e4c42", "ref_doc_id": "505add2d-b6fd-4ef2-ad63-908d45a8de33"}, "eb80d134-911d-483a-b1d7-0f15dc0e7654": {"doc_hash": "99c914889747a3573ec9875130617f7280b932bd658dbea051e005d5a9ca410d", "ref_doc_id": "505add2d-b6fd-4ef2-ad63-908d45a8de33"}, "28faaf64-de02-4d60-8a22-4d3f50f69a6b": {"doc_hash": "d05e4020ab46ac4efb08029a9519d5b84e6ff2f46067fbb22b6270f3497e51de", "ref_doc_id": "505add2d-b6fd-4ef2-ad63-908d45a8de33"}, "e17d9e4b-9f97-4ef5-bd9d-e138d8cbe3f5": {"doc_hash": "99e9371721184aad3244bb094350c4ed63b73c1b70c3bb8fc25c6b4ee866f132", "ref_doc_id": "505add2d-b6fd-4ef2-ad63-908d45a8de33"}, "5411a3be-f96f-4065-8a57-6624cbd85782": {"doc_hash": "b9d76e47e4c8eebcec6f822a3ab95e260a25d003153ac21936859c9c4769b5df", "ref_doc_id": "505add2d-b6fd-4ef2-ad63-908d45a8de33"}, "4fba390e-0010-48a4-89df-f7e73fb7c341": {"doc_hash": "5107a20ec7cccfc3054e4068d9280f3089ebc6699f9090fa477a48dfbd12e11c", "ref_doc_id": "505add2d-b6fd-4ef2-ad63-908d45a8de33"}, "a75e8b41-4a8d-4792-bbe8-34671ca648d4": {"doc_hash": "c9a9d58e14bfa97648302302c33669dad4c79cc1b0e81f54bcd06b33aa079f16", "ref_doc_id": "505add2d-b6fd-4ef2-ad63-908d45a8de33"}, "7b7e3059-306e-4a58-862a-a053c2e7418b": {"doc_hash": "f73969ae63e2370ff1ac4a3b6d8360685fd4676dd36b7a6ca6e258f503167563", "ref_doc_id": "505add2d-b6fd-4ef2-ad63-908d45a8de33"}, "64b330e8-1d2e-46f3-8a3c-1a7f558775cf": {"doc_hash": "781ed0735b7d0462ace841ca9ec04752b83354a5434c5a9cb044eaf1c2e7a948", "ref_doc_id": "505add2d-b6fd-4ef2-ad63-908d45a8de33"}}, "docstore/ref_doc_info": {"505add2d-b6fd-4ef2-ad63-908d45a8de33": {"node_ids": ["369b3c92-b779-4e9a-a8a8-bac6c0154daa", "1c2fd7fe-bd55-4650-96b3-7f2a069dfecd", "eb80d134-911d-483a-b1d7-0f15dc0e7654", "28faaf64-de02-4d60-8a22-4d3f50f69a6b", "e17d9e4b-9f97-4ef5-bd9d-e138d8cbe3f5", "5411a3be-f96f-4065-8a57-6624cbd85782", "4fba390e-0010-48a4-89df-f7e73fb7c341", "a75e8b41-4a8d-4792-bbe8-34671ca648d4", "7b7e3059-306e-4a58-862a-a053c2e7418b", "64b330e8-1d2e-46f3-8a3c-1a7f558775cf"], "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}}}}