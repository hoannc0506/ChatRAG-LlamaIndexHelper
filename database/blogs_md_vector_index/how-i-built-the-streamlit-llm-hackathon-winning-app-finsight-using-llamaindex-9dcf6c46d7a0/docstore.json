{"docstore/data": {"2004b5bd-2c8d-490b-9928-350d717e16c4": {"__data__": {"id_": "2004b5bd-2c8d-490b-9928-350d717e16c4", "embedding": null, "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4891d519-9c09-4fa8-9942-3a550a6538d9", "node_type": "4", "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "d6379eb20ccdbee08bc4bc9291fd9103b7bce96980f9770d4c8746ebcbf3816f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e0ab54d-3412-45ac-ac6f-56ed7a456890", "node_type": "1", "metadata": {"Header_1": " Introduction"}, "hash": "9e0ad6b3b1453b86623b064b0503bb674decb39decbbbb21551b0347284bf5cf", "class_name": "RelatedNodeInfo"}}, "text": "In this article, we\u2019ll dive deep into the world of LLM app development and\ntake a closer look at my journey of building the Streamlit LLM hackathon-\nwinning app [ FinSight \u2014 Financial Insights At Your Fingertips\n](https://finsight-report.streamlit.app/) . This article covers the entire\nprocess from ideation to execution, along with code snippets and snapshots.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 362, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e0ab54d-3412-45ac-ac6f-56ed7a456890": {"__data__": {"id_": "3e0ab54d-3412-45ac-ac6f-56ed7a456890", "embedding": null, "metadata": {"Header_1": " Introduction", "filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4891d519-9c09-4fa8-9942-3a550a6538d9", "node_type": "4", "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "d6379eb20ccdbee08bc4bc9291fd9103b7bce96980f9770d4c8746ebcbf3816f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2004b5bd-2c8d-490b-9928-350d717e16c4", "node_type": "1", "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "82ed449e6c47eff667c34cd1eb2d7bbcdef44ec69c3dae11d9578c8b71cda01a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "606dbaf1-93c7-4092-ac1d-f32e74f7b432", "node_type": "1", "metadata": {"Header_1": " Introduction", "Header_2": " A use case for LLMs in finance"}, "hash": "1ef08799b9cabc82cfe61857d4dee410782953033be752a6fc97ea42eea2766e", "class_name": "RelatedNodeInfo"}}, "text": "Introduction", "mimetype": "text/plain", "start_char_idx": 367, "end_char_idx": 379, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "606dbaf1-93c7-4092-ac1d-f32e74f7b432": {"__data__": {"id_": "606dbaf1-93c7-4092-ac1d-f32e74f7b432", "embedding": null, "metadata": {"Header_1": " Introduction", "Header_2": " A use case for LLMs in finance", "filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4891d519-9c09-4fa8-9942-3a550a6538d9", "node_type": "4", "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "d6379eb20ccdbee08bc4bc9291fd9103b7bce96980f9770d4c8746ebcbf3816f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e0ab54d-3412-45ac-ac6f-56ed7a456890", "node_type": "1", "metadata": {"Header_1": " Introduction", "filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "b9a936e604bff49e426ffa61a0c3b30d4bfcaaa9f96f002004e576f87532dfd5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c3bc88f1-4953-422d-aa47-977ec442342e", "node_type": "1", "metadata": {"Header_1": " Introduction", "Header_2": " How does FinSight work?"}, "hash": "2ed1448a1324ba413786bc344f61a45c47eecbbe67509d8822c2a3e576ce5eb5", "class_name": "RelatedNodeInfo"}}, "text": "A use case for LLMs in finance\n\nOne fascinating use case for LLMs in finance is to use them on company annual\nreports (10-K form). These reports are publicly available information that\npretty much every portfolio manager, financial analyst, and shareholder uses\nregularly to make informed decisions.\n\nHowever reading, understanding, and assessing these reports, especially for\nmultiple companies can be tedious and time-consuming. Hence, using LLMs on\nannual reports to extract insights and summarize would solve a lot of problems\nand save valuable time.\n\nWhen the [ Streamlit LLM Hackathon\n](https://www.linkedin.com/posts/vishwasgowda217_llm-hackathon-streamlit-\nactivity-7115398433573666816-1y72?utm_source=share&utm_medium=member_desktop)\nwas, announced I thought this was the best time to explore this idea. And\nthat\u2019s how [ FinSight ](https://finsight-report.streamlit.app/) came into\nexistence.", "mimetype": "text/plain", "start_char_idx": 385, "end_char_idx": 1286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c3bc88f1-4953-422d-aa47-977ec442342e": {"__data__": {"id_": "c3bc88f1-4953-422d-aa47-977ec442342e", "embedding": null, "metadata": {"Header_1": " Introduction", "Header_2": " How does FinSight work?", "filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4891d519-9c09-4fa8-9942-3a550a6538d9", "node_type": "4", "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "d6379eb20ccdbee08bc4bc9291fd9103b7bce96980f9770d4c8746ebcbf3816f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "606dbaf1-93c7-4092-ac1d-f32e74f7b432", "node_type": "1", "metadata": {"Header_1": " Introduction", "Header_2": " A use case for LLMs in finance", "filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "2b4a83fc38efae43c021bb39272ec8e1259a6f98cc500293eef76fd3ac3ebd2d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43e01405-5658-41b6-b7e9-7af1622b3c88", "node_type": "1", "metadata": {"Header_1": " Setup"}, "hash": "e1037a018f25e8cfa7e757b6bfe07b0def691a5ffb63876a7ceb6eece0d5fd23", "class_name": "RelatedNodeInfo"}}, "text": "How does FinSight work?\n\nA small Demonstration\n\nFinSight has two main features called Annual Report Analyzer and Finance\nMetric Review, but for this blog post, we will be concentrating on the former.\n\nAnnual Report Analyzer is a RAG(Retrieval Augmented Generation) based feature,\nwhich means that the LLM will be generating insights based on the information\nin a knowledge base (which in this case is a company\u2019s annual report). Here\u2019s\nhow it works behind the scenes:\n\nRAG pipeline for Annual Report Analyzer\n\nWhile this is a basic representation of the architecture, we will be doing a\ndeep dive into the importance of each of these components and how they work.", "mimetype": "text/plain", "start_char_idx": 1292, "end_char_idx": 1955, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43e01405-5658-41b6-b7e9-7af1622b3c88": {"__data__": {"id_": "43e01405-5658-41b6-b7e9-7af1622b3c88", "embedding": null, "metadata": {"Header_1": " Setup", "filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4891d519-9c09-4fa8-9942-3a550a6538d9", "node_type": "4", "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "d6379eb20ccdbee08bc4bc9291fd9103b7bce96980f9770d4c8746ebcbf3816f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c3bc88f1-4953-422d-aa47-977ec442342e", "node_type": "1", "metadata": {"Header_1": " Introduction", "Header_2": " How does FinSight work?", "filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "278b6421eab6440ceedad236a43f881c3187e44fc2708ced711bef1e39b1c1c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "59121e3d-1250-47fc-ab04-a9623f05f511", "node_type": "1", "metadata": {"Header_1": " Document Loading, Indexing, and Storage"}, "hash": "8606749636a12bf412872cbbd333f3e621ed82302bd1ede48643446b3f7b1fb3", "class_name": "RelatedNodeInfo"}}, "text": "Setup\n\nIn case you want to refer the code to the app: [ Repo\n](https://github.com/vishwasg217/finsight)\n\nWe will use [ LlamaIndex ](https://www.llamaindex.ai/) to build the knowledge\nbase and to query it using an LLM (gpt-4 is the best suited). LlamaIndex is a\nsimple, flexible data framework for connecting custom data sources to large\nlanguage models.\n\nFor the front end, [ Streamlit ](https://streamlit.io/) is the most convenient\ntool to build and share web apps.\n\n  1. Clone Repository \n\n    \n    \n    git clone https://github.com/vishwasg217/finsight.gitcd finsight\n\n2\\. Setup Virtual Environment\n\n    \n    \n    # For macOS and Linux:python3 -m venv venv# For Windows:python -m venv venv\n\n3\\. Activate Virtual Environment\n\n    \n    \n    # For macOS and Linux:source venv/bin/activate# For Windows:.\\venv\\Scripts\\activate\n\n4\\. Install Required Dependencies:\n\n    \n    \n    pip install -r requirements.txt\n\n5\\. Set up the Environment Variables:\n\n    \n    \n    # create directorymkdir .streamlit# create toml filetouch .streamlit/secrets.toml\n\nYou can get your API keys here: [ AlphaVantage\n](https://www.alphavantage.co/support/#api-key) , [ OpenAI\n](https://openai.com/blog/openai-api) ,\n\n    \n    \n    # Add the following API keysav_api_key = \"ALPHA_VANTAGE API KEY\"openai_api_key = \"OPEN AI API KEY\"", "mimetype": "text/plain", "start_char_idx": 1960, "end_char_idx": 3266, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59121e3d-1250-47fc-ab04-a9623f05f511": {"__data__": {"id_": "59121e3d-1250-47fc-ab04-a9623f05f511", "embedding": null, "metadata": {"Header_1": " Document Loading, Indexing, and Storage", "filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4891d519-9c09-4fa8-9942-3a550a6538d9", "node_type": "4", "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "d6379eb20ccdbee08bc4bc9291fd9103b7bce96980f9770d4c8746ebcbf3816f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "43e01405-5658-41b6-b7e9-7af1622b3c88", "node_type": "1", "metadata": {"Header_1": " Setup", "filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "ec85387c19bbb3d41901c6a4d5e84708435ee8b59d918a56d1edb5c68042ad92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "075c3e90-6de0-4a82-b3c9-1a697941b950", "node_type": "1", "metadata": {"Header_1": " Query Tools and Engines"}, "hash": "c43a958b4f6ff0e5ebc6670c74e78a1574996909e7118a8f1b92e3b201f0d62f", "class_name": "RelatedNodeInfo"}}, "text": "Document Loading, Indexing, and Storage\n\nAlthough LlamaIndex has its own set of data connectors to read PDFs, we still\nneed to write a small function ` process_pdf() ` to load the PDFs since we are\ndoing it through Streamlit.\n\n    \n    \n    from pypdf import PdfReaderfrom llama_index.schema import Documentdef process_pdf(pdf):    file = PdfReader(pdf)    text = \"\"    for page in file.pages:        text += str(page.extract_text())            doc = Document(text=text)    return [doc]\n\nThe next step is to ingest, index, and store this document in a vector\ndatabase. In this case, we will use FAISS DB, as we require in an in-memory\nvector database. FAISS is also very convenient to use. Hence, we write a\nfunction called ` get_vector_index() ` to do exactly that.\n\nIn case you\u2019re interested in checking out other vector DB options, you read\ncan [ this ](https://gpt-\nindex.readthedocs.io/en/stable/core_modules/data_modules/storage/vector_stores.html)\n.\n\n    \n    \n    from llama_index.llms import OpenAIfrom llama_index import VectorStoreIndex, ServiceContext, StorageContextfrom llama_index.vector_stores import FaissVectorStoredef get_vector_index(documents):    llm = OpenAI(OPENAI_API_KEY)    faiss_index = faiss.IndexFlatL2(d)    vector_store = FaissVectorStore(faiss_index=faiss_index)    storage_context = StorageContext.from_defaults(vector_store=vector_store)    service_context = ServiceContext.from_defaults(llm=llm)     index = VectorStoreIndex.from_documents(documents,         service_context=service_context,        storage_context=storage_context    )       return index\n\n` ServiceContext() ` and ` StorageContext() ` are used to set the\nconfigurations for the vector store. Using ` VectorStoreIndex.from_documents()\n` we ingest, index, and store the document as vector embeddings in the FAISS\nDB.\n\n    \n    \n    # Calling the functions through streamlit frontendimport streamlit as stif \"index\" not in st.session_state:  st.session_state.index = Noneif \"process_doc\" not in st.session_state:        st.session_state.process_doc = Falseif st.sidebar.button(\"Process Document\"):        with st.spinner(\"Processing Document...\"):            documents = process_pdf(pdfs)            st.session_state.index = get_vector_index(documents)            st.session_state.process_doc = True  st.toast(\"Document Processsed!\")", "mimetype": "text/plain", "start_char_idx": 3271, "end_char_idx": 5604, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "075c3e90-6de0-4a82-b3c9-1a697941b950": {"__data__": {"id_": "075c3e90-6de0-4a82-b3c9-1a697941b950", "embedding": null, "metadata": {"Header_1": " Query Tools and Engines", "filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4891d519-9c09-4fa8-9942-3a550a6538d9", "node_type": "4", "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "d6379eb20ccdbee08bc4bc9291fd9103b7bce96980f9770d4c8746ebcbf3816f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59121e3d-1250-47fc-ab04-a9623f05f511", "node_type": "1", "metadata": {"Header_1": " Document Loading, Indexing, and Storage", "filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "227faffcc21d48f55d7e20b170d4a3c53fbc73e8127703b30426791d6d356d8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2cccbf26-a786-421d-af86-ee40fb16bc7f", "node_type": "1", "metadata": {"Header_1": " Prompt Engineering"}, "hash": "06a7a17f71c239580f2d867f19fd423c000064576896bad3b9d506dbf7dcb74d", "class_name": "RelatedNodeInfo"}}, "text": "Query Tools and Engines\n\nNow that we have our knowledge base ready, it\u2019s time to build a mechanism to\nquery it.\n\n    \n    \n    index = get_vector_index(documents)engine = index.as_query_engine()query = \"How has Microsoft performed in this fiscal year?\"response = engine(query)\n\nIdeally, the above code should have been enough to query and synthesize a\nresponse from the information in the vector DB. However, the response wouldn't\nbe comprehensive and detailed enough, especially for such open-ended\nquestions. We need to develop a better mechanism that allows us to break down\na query into more detailed questions and retrieve context from multiple parts\nof the vector DB.\n\n    \n    \n    def get_query_engine(engine):    query_engine_tools = [        QueryEngineTool(            query_engine=engine,            metadata=ToolMetadata(                name=\"Annual Report\",                description=f\"Provides information about the company from its annual report.\",            ),        ),    ]    s_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools)    return s_engineindex = get_vector_index(documents)engine = index.as_query_engine()s_engine = get_query_engine(engine)\n\nLet\u2019s break the above function down. The ` QueryEngineTool ` module wraps\naround the ` engine ` and helps provide context and metadata to the engine.\nThis is especially useful when you have more than one engine and you want to\nprovide context to the LLM as to which one to use for a given query.\n\nHere\u2019s what that would look like:\n\n    \n    \n    # example for multiple query engine toolsquery_engine_tools = [    QueryEngineTool(        query_engine=sept_engine,        metadata=ToolMetadata(            name=\"sept_22\",            description=\"Provides information about Uber quarterly financials ending September 2022\",        ),    ),    QueryEngineTool(        query_engine=june_engine,        metadata=ToolMetadata(            name=\"june_22\",            description=\"Provides information about Uber quarterly financials ending June 2022\",        ),    )]\n\nYou can read more about the tools available in LlamaIndex [ here\n](https://docs.llamaindex.ai/en/stable/core_modules/agent_modules/tools/root.html)\n.\n\nHowever, we\u2019re currently sticking to just one QueryEnginerTool for now.\n\nThe ` SubQuestionQueryEngine ` module breaks down a complex query into many\nsub-questions and their target query engine for execution. After executing all\nsub-questions, all responses are gathered and sent to a response synthesizer\nto produce the final response. Using this module is essential because\ngenerating insights from annual reports requires complex queries that need to\nretrieve information from multiple nodes within the vector DB.\n\nSubQuestionQueryEngine at work", "mimetype": "text/plain", "start_char_idx": 5609, "end_char_idx": 8374, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2cccbf26-a786-421d-af86-ee40fb16bc7f": {"__data__": {"id_": "2cccbf26-a786-421d-af86-ee40fb16bc7f", "embedding": null, "metadata": {"Header_1": " Prompt Engineering", "filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4891d519-9c09-4fa8-9942-3a550a6538d9", "node_type": "4", "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "d6379eb20ccdbee08bc4bc9291fd9103b7bce96980f9770d4c8746ebcbf3816f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "075c3e90-6de0-4a82-b3c9-1a697941b950", "node_type": "1", "metadata": {"Header_1": " Query Tools and Engines", "filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "e769c97395cadc3fb29fa00a30b7116a8ed83a0adfeb126b724b93be5540f65b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4601271f-71c4-4548-a5fb-4513d8913853", "node_type": "1", "metadata": {"Header_1": " Upcoming Features"}, "hash": "5e72c3d671508a02b0f55b72be3d70ab4811f55c00e9fea6320d468bacf16618", "class_name": "RelatedNodeInfo"}}, "text": "Prompt Engineering\n\nPrompt engineering is essential to the entire process mainly for two reasons:\n\n  1. To provide clarity to the agent as to what it needs to retrieve from the vector DB by writing precise and relevant queries \n  2. And then control the quality of the output generated from the retrieved context by providing a structure and description for the output to be generated. \n\nBoth these points are handled by using ` PromptTemplate ` and `\nPydanticOutputParser ` module in ` langchain ` .\n\nUsing the ` PydanticOutputParser ` we write the description for the different\nsections of the insights to be generated. After having a few conversations\nwith finance experts, I concluded generating insights for these 4 sections:\ndifferent sections: Fiscal Year Highlights, Strategic Outlook and Future\nDirection, Risk Management, Innovation and R&D. Now let\u2019s write the ` pydantic\n` class for these sections:\n\n    \n    \n    from pydantic import BaseModel, Fieldclass FiscalYearHighlights(BaseModel):    performance_highlights: str = Field(..., description=\"Key performance metrics and financial stats over the fiscal year.\")    major_events: str = Field(..., description=\"Highlight of significant events, acquisitions, or strategic shifts that occurred during the year.\")    challenges_encountered: str = Field(..., description=\"Challenges the company faced during the year and, if and how they managed or overcame them.\")class StrategyOutlookFutureDirection(BaseModel):    strategic_initiatives: str = Field(..., description=\"The company's primary objectives and growth strategies for the upcoming years.\")    market_outlook: str = Field(..., description=\"Insights into the broader market, competitive landscape, and industry trends the company anticipates.\")class RiskManagement(BaseModel):    risk_factors: str = Field(..., description=\"Primary risks the company acknowledges.\")    risk_mitigation: str = Field(..., description=\"Strategies for managing these risks.\")class InnovationRnD(BaseModel):    r_and_d_activities: str = Field(..., description=\"Overview of the company's focus on research and development, major achievements, or breakthroughs.\")    innovation_focus: str = Field(..., description=\"Mention of new technologies, patents, or areas of research the company is diving into.\")\n\n**Note: These sections and their description are for generic use cases. They\ncan be changed to suit your particular needs.**\n\nThese pydantic classes will provide the format and description for each\nsection to the prompt. So let\u2019s write a function that allows us to plug in any\npydantic class to a prompt:\n\n    \n    \n    from langchain.prompts import PromptTemplatefrom langchain.output_parsers import PydanticOutputParserprompt_template = \"\"\"You are given the task of generating insights for {section} from the annual report of the company. Given below is the output format, which has the subsections.Must use bullet points.Always use $ symbol for money values, and round it off to millions or billions accordinglyIncase you don't have enough info you can just write: No information available---{output_format}---\"\"\"def report_insights(engine, section, pydantic_model):    parser = PydanticOutputParser(pydantic_object=pydantic_model)    prompt_template = PromptTemplate(        template=prompt_template,        input_variables=[\"section\"],        partial_variables={\"output_format\": parser.get_format_instructions()}    )    formatted_input = prompt_template.format(section=section)    response = engine.query(formatted_input)    parsed_response = parser.parse(response.response)    return parsed_response\n\n` PromptTemplate ` plugs in all the values such as ` section ` and `\noutput_format ` into the prompt template. ` PydanticOutputParser ` converts\nthe pydantic class into a format that is readable to the LLM. The response\ngenerated will be in string format, hence we use the ` parser.parse() `\nfunction to parse the response and get a structured output.\n\n    \n    \n    # calling the function in streamlit frontendif st.session_state.process_doc:    if st.button(\"Analyze Report\"):        engine = get_query_engine(st.session_state.index.as_query_engine(similarity_top_k=3))        with st.status(\"**Analyzing Report...**\"):            st.write(\"Fiscal Year Highlights...\")            st.session_state.fiscal_year_highlights = report_insights(engine, \"Fiscal Year Highlights\", FiscalYearHighlights)            st.write(\"Strategy Outlook and Future Direction...\")            st.session_state.strategy_outlook_future_direction = report_insights(engine, \"Strategy Outlook and Future Direction\", StrategyOutlookFutureDirection)            st.write(\"Risk Management...\")            st.session_state.risk_management = report_insights(engine, \"Risk Management\", RiskManagement)                        st.write(\"Innovation and R&D...\")            st.session_state.innovation_and_rd = report_insights(engine, \"Innovation and R&D\", InnovationRnD)# displaying the generated insights  if st.session_state.fiscal_year_highlights:                with tab1:            st.write(\"## Fiscal Year Highlights\")            st.write(\"### Performance Highlights\")            st.write(st.session_state.fiscal_year_highlights.performance_highlights)            st.write(\"### Major Events\")            st.write(st.session_state.fiscal_year_highlights.major_events)            st.write(\"### Challenges Encountered\")            st.write(st.session_state.fiscal_year_highlights.challenges_encountered)            st.write(\"### Milestone Achievements\")            st.write(str(st.session_state.fiscal_year_highlights.milestone_achievements))    if st.session_state.strategy_outlook_future_direction:        with tab2:            st.write(\"## Strategy Outlook and Future Direction\")            st.write(\"### Strategic Initiatives\")            st.write(st.session_state.strategy_outlook_future_direction.strategic_initiatives)            st.write(\"### Market Outlook\")            st.write(st.session_state.strategy_outlook_future_direction.market_outlook)            st.write(\"### Product Roadmap\")            st.write(st.session_state.strategy_outlook_future_direction.product_roadmap)    if st.session_state.risk_management:        with tab3:            st.write(\"## Risk Management\")            st.write(\"### Risk Factors\")            st.write(st.session_state.risk_management.risk_factors)            st.write(\"### Risk Mitigation\")            st.write(st.session_state.risk_management.risk_mitigation)    if st.session_state.innovation_and_rd:        with tab4:            st.write(\"## Innovation and R&D\")            st.write(\"### R&D Activities\")            st.write(st.session_state.innovation_and_rd.r_and_d_activities)            st.write(\"### Innovation Focus\")            st.write(st.session_state.innovation_and_rd.innovation_focus)\n\nYou can find the complete code Annual Report Analyzer [ here\n](https://github.com/vishwasg217/finsight/blob/main/src/pages/2_%F0%9F%97%82%EF%B8%8F_Annual_Report_Analyzer.py)", "mimetype": "text/plain", "start_char_idx": 8379, "end_char_idx": 15372, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4601271f-71c4-4548-a5fb-4513d8913853": {"__data__": {"id_": "4601271f-71c4-4548-a5fb-4513d8913853", "embedding": null, "metadata": {"Header_1": " Upcoming Features", "filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4891d519-9c09-4fa8-9942-3a550a6538d9", "node_type": "4", "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "d6379eb20ccdbee08bc4bc9291fd9103b7bce96980f9770d4c8746ebcbf3816f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2cccbf26-a786-421d-af86-ee40fb16bc7f", "node_type": "1", "metadata": {"Header_1": " Prompt Engineering", "filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "0ec5cd6a730059d5d5e05a0a0c4023a7a262df0883ce8d33a7c083e4b6d17e6b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76f1f635-470f-4fb5-87dc-fde875293f9d", "node_type": "1", "metadata": {"Header_1": " Conclusion"}, "hash": "3156b2a6df9b295f604769332be755d6fa7782883f3b549ecd20819749b21372", "class_name": "RelatedNodeInfo"}}, "text": "Upcoming Features\n\n  1. Select and Store Insights: I\u2019ve been working on a feature that allows the user to select any insight needed and also save it into the user\u2019s account \n  2. Adding more profession-specific insights: Currently, the insight works well for generic purposes. However, different professions use annual reports differently, so naturally I need to create a different set of insights based on the user\u2019s use case. \n  3. ` PandasQueryEngine ` Module for querying financial statements: Using this module, the LLM will be able to extract better insights from financial statements which are typically in a structured format.", "mimetype": "text/plain", "start_char_idx": 15377, "end_char_idx": 16011, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76f1f635-470f-4fb5-87dc-fde875293f9d": {"__data__": {"id_": "76f1f635-470f-4fb5-87dc-fde875293f9d", "embedding": null, "metadata": {"Header_1": " Conclusion", "filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4891d519-9c09-4fa8-9942-3a550a6538d9", "node_type": "4", "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "d6379eb20ccdbee08bc4bc9291fd9103b7bce96980f9770d4c8746ebcbf3816f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4601271f-71c4-4548-a5fb-4513d8913853", "node_type": "1", "metadata": {"Header_1": " Upcoming Features", "filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "d941ff38b010a1fb437ba5c1b06f1400ca05b82101265939a82c886ac8c0ec55", "class_name": "RelatedNodeInfo"}}, "text": "Conclusion\n\nIn summary, FinSight\u2019s Annual Report Analyzer makes financial analysis easier\nand more insightful by harnessing the power of LLMs. It\u2019s a valuable tool for\nportfolio managers, financial analysts, and shareholders, saving time and\nimproving decision-making. While the core pipeline remains consistent, note\nthat our deployed app code might evolve to incorporate upgrades and enhanced\nfeatures, ensuring ongoing improvements.\n\nBig thanks to [ LlamaIndex ](https://www.llamaindex.ai/) for helping me make\nFinSight a reality. No other framework is as advanced in making RAG-based\ntools.\n\nIf you like what you\u2019ve read, please do leave a clap for me, and also show\nsome love to [ FinSight ](https://finsight-report.streamlit.app/) . You can\ncheck out the GitHub repo [ here ](https://github.com/vishwasg217/finsight) .\n\nYou connect with me on [ LinkedIn ](https://www.linkedin.com/feed/) and [\nTwitter ](https://twitter.com/VishwasAiTech)", "mimetype": "text/plain", "start_char_idx": 16017, "end_char_idx": 16961, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"2004b5bd-2c8d-490b-9928-350d717e16c4": {"doc_hash": "82ed449e6c47eff667c34cd1eb2d7bbcdef44ec69c3dae11d9578c8b71cda01a", "ref_doc_id": "4891d519-9c09-4fa8-9942-3a550a6538d9"}, "3e0ab54d-3412-45ac-ac6f-56ed7a456890": {"doc_hash": "b9a936e604bff49e426ffa61a0c3b30d4bfcaaa9f96f002004e576f87532dfd5", "ref_doc_id": "4891d519-9c09-4fa8-9942-3a550a6538d9"}, "606dbaf1-93c7-4092-ac1d-f32e74f7b432": {"doc_hash": "2b4a83fc38efae43c021bb39272ec8e1259a6f98cc500293eef76fd3ac3ebd2d", "ref_doc_id": "4891d519-9c09-4fa8-9942-3a550a6538d9"}, "c3bc88f1-4953-422d-aa47-977ec442342e": {"doc_hash": "278b6421eab6440ceedad236a43f881c3187e44fc2708ced711bef1e39b1c1c4", "ref_doc_id": "4891d519-9c09-4fa8-9942-3a550a6538d9"}, "43e01405-5658-41b6-b7e9-7af1622b3c88": {"doc_hash": "ec85387c19bbb3d41901c6a4d5e84708435ee8b59d918a56d1edb5c68042ad92", "ref_doc_id": "4891d519-9c09-4fa8-9942-3a550a6538d9"}, "59121e3d-1250-47fc-ab04-a9623f05f511": {"doc_hash": "227faffcc21d48f55d7e20b170d4a3c53fbc73e8127703b30426791d6d356d8f", "ref_doc_id": "4891d519-9c09-4fa8-9942-3a550a6538d9"}, "075c3e90-6de0-4a82-b3c9-1a697941b950": {"doc_hash": "e769c97395cadc3fb29fa00a30b7116a8ed83a0adfeb126b724b93be5540f65b", "ref_doc_id": "4891d519-9c09-4fa8-9942-3a550a6538d9"}, "2cccbf26-a786-421d-af86-ee40fb16bc7f": {"doc_hash": "0ec5cd6a730059d5d5e05a0a0c4023a7a262df0883ce8d33a7c083e4b6d17e6b", "ref_doc_id": "4891d519-9c09-4fa8-9942-3a550a6538d9"}, "4601271f-71c4-4548-a5fb-4513d8913853": {"doc_hash": "d941ff38b010a1fb437ba5c1b06f1400ca05b82101265939a82c886ac8c0ec55", "ref_doc_id": "4891d519-9c09-4fa8-9942-3a550a6538d9"}, "76f1f635-470f-4fb5-87dc-fde875293f9d": {"doc_hash": "5f651217a167e94e6ca9f7b00b60c4ec13d150ddccd5b6fb0a4546e8e3931e07", "ref_doc_id": "4891d519-9c09-4fa8-9942-3a550a6538d9"}}, "docstore/ref_doc_info": {"4891d519-9c09-4fa8-9942-3a550a6538d9": {"node_ids": ["2004b5bd-2c8d-490b-9928-350d717e16c4", "3e0ab54d-3412-45ac-ac6f-56ed7a456890", "606dbaf1-93c7-4092-ac1d-f32e74f7b432", "c3bc88f1-4953-422d-aa47-977ec442342e", "43e01405-5658-41b6-b7e9-7af1622b3c88", "59121e3d-1250-47fc-ab04-a9623f05f511", "075c3e90-6de0-4a82-b3c9-1a697941b950", "2cccbf26-a786-421d-af86-ee40fb16bc7f", "4601271f-71c4-4548-a5fb-4513d8913853", "76f1f635-470f-4fb5-87dc-fde875293f9d"], "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}}}}