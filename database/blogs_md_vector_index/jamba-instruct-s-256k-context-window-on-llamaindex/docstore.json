{"docstore/data": {"f90f0e2f-a3dc-4785-8f91-a3c45a02b7e1": {"__data__": {"id_": "f90f0e2f-a3dc-4785-8f91-a3c45a02b7e1", "embedding": null, "metadata": {"filename": "jamba-instruct-s-256k-context-window-on-llamaindex.md", "extension": ".md", "title": "Jamba-Instruct's 256k context window on LlamaIndex", "date": "Jul 31, 2024", "url": "https://www.llamaindex.ai/blog/jamba-instruct-s-256k-context-window-on-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe7416b0-35d5-48ed-a9c7-1387cdab434c", "node_type": "4", "metadata": {"filename": "jamba-instruct-s-256k-context-window-on-llamaindex.md", "extension": ".md", "title": "Jamba-Instruct's 256k context window on LlamaIndex", "date": "Jul 31, 2024", "url": "https://www.llamaindex.ai/blog/jamba-instruct-s-256k-context-window-on-llamaindex"}, "hash": "85f47676dd585ef6f69680f12e9ec734a66fd4f734864c01b001a01ce5068324", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "561fd2da-d9e1-4e22-83d1-3cfa3a4fc63a", "node_type": "1", "metadata": {"Header_3": " RAG Q&A on financial documents"}, "hash": "f22cac002e131f4ac743b98969dd5ee42525ab4f6f27d002bf2efe6a02c0dbd9", "class_name": "RelatedNodeInfo"}}, "text": "Build state-of-the-art RAG applications for the enterprise by leveraging\nLlamaIndex\u2019s market-leading RAG strategies with AI21 Labs\u2019 long context\nFoundation Model, Jamba-Instruct.\n\nWe at AI21 Labs are excited to announce that our groundbreaking Jamba-Instruct\nfoundation model is now available through leading data framework LlamaIndex.\nWith this integration, developers can now build powerful RAG enterprise\napplications with enhanced accuracy and cost-efficiency due to Jamba-\nInstruct\u2019s impressive 256K context window and LlamaIndex\u2019s sophisticated end-\nto-end offerings for RAG.\n\nWhile many models declare long context windows, researchers at NVIDIA found\nthat [ most falter under evaluation ](https://arxiv.org/pdf/2404.06654) ,\nrevealing a discrepancy between their claimed and effective context window\nlengths. Jamba-Instruct is one of the few models on the market to not only\nachieve parity between its declared and effective lengths, but to do so with a\nmuch longer context window length than any other model in its size class.\n\nBy offering a context window of 256K\u2014roughly equivalent to 800 pages of\ntext\u2014Jamba-Instruct increases the number of retrieved chunks and can vastly\nimprove the entire RAG system, rather than trying to improve the search\nmechanism or incorporating an additional reranking component. Using a long\ncontext foundation model like Jamba-Instruct makes querying private enterprise\ndata with RAG both more reliable and easier.\n\nIn the following notebook ( [ also available directly on colab\n](https://colab.research.google.com/drive/1ycpC1pfCty9bqCmHdrgvAtqQwP1o0lPg)\n), we\u2019ll walk through an example of querying a collection of financial\ndocuments, showing how Jamba-Instruct\u2019s 256K context window allows the RAG\npipeline to retrieve more chunks at once in order to deliver an accurate\nanswer.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1823, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "561fd2da-d9e1-4e22-83d1-3cfa3a4fc63a": {"__data__": {"id_": "561fd2da-d9e1-4e22-83d1-3cfa3a4fc63a", "embedding": null, "metadata": {"Header_3": " RAG Q&A on financial documents", "filename": "jamba-instruct-s-256k-context-window-on-llamaindex.md", "extension": ".md", "title": "Jamba-Instruct's 256k context window on LlamaIndex", "date": "Jul 31, 2024", "url": "https://www.llamaindex.ai/blog/jamba-instruct-s-256k-context-window-on-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe7416b0-35d5-48ed-a9c7-1387cdab434c", "node_type": "4", "metadata": {"filename": "jamba-instruct-s-256k-context-window-on-llamaindex.md", "extension": ".md", "title": "Jamba-Instruct's 256k context window on LlamaIndex", "date": "Jul 31, 2024", "url": "https://www.llamaindex.ai/blog/jamba-instruct-s-256k-context-window-on-llamaindex"}, "hash": "85f47676dd585ef6f69680f12e9ec734a66fd4f734864c01b001a01ce5068324", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f90f0e2f-a3dc-4785-8f91-a3c45a02b7e1", "node_type": "1", "metadata": {"filename": "jamba-instruct-s-256k-context-window-on-llamaindex.md", "extension": ".md", "title": "Jamba-Instruct's 256k context window on LlamaIndex", "date": "Jul 31, 2024", "url": "https://www.llamaindex.ai/blog/jamba-instruct-s-256k-context-window-on-llamaindex"}, "hash": "0607b1e735b544c9a3e9b24c1a0847913afa3a449157aaa47c35e2bc34ad9c7c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1bbb26d-1755-4aa5-a3e7-bdf473967760", "node_type": "1", "metadata": {"Header_3": " Context is king"}, "hash": "cb0f3dbd477aab418417e13af159a7c087f98b0875a463047bfaec60456d6f6d", "class_name": "RelatedNodeInfo"}}, "text": "RAG Q&A on financial documents\n\nTo get started, these are the packages you need to install. You will also need\nAPI keys to set up OpenAI for embeddings and AI21 for Jamba-Instruct.\n\n    \n    \n    !pip install llama-index\n    !pip install -U ai21\n    !pip install llama-index-llms-ai21\n    \n    import os\n    from llama_index.core.llama_dataset import download_llama_dataset\n    from llama_index.core.llama_pack import download_llama_pack\n    from llama_index.core import VectorStoreIndex\n    from llama_index.core import SimpleDirectoryReader\n    from llama_index.llms.ai21 import AI21\n    \n    os.environ['OPENAI_API_KEY'] = 'YOUR_OPENAI_API_KEY' # For embeddings\n    os.environ['AI21_API_KEY'] = 'YOUR_AI21_API_KEY' # For the generation\n    \n    # Setup jamba instruct as the llm\n    llm = AI21(\n        model='jamba-instruct',\n        temperature=0,\n        max_tokens=2000\n    )\n\nNext, download 5 10-K forms from Amazon from [ Amazon\u2019s Investor Relations\npage. ](https://ir.aboutamazon.com/sec-filings/default.aspx)\n\n    \n    \n    # Get the data - download 10k forms from AMZN from the last five years\n    os.mkdir(\"data\")\n    !wget 'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/c7c14359-36fa-40c3-b3ca-5bf7f3fa0b96.pdf' -O 'data/amazon_2023.pdf'\n    !wget 'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/d2fde7ee-05f7-419d-9ce8-186de4c96e25.pdf' -O 'data/amazon_2022.pdf'\n    !wget 'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/f965e5c3-fded-45d3-bbdb-f750f156dcc9.pdf' -O 'data/amazon_2021.pdf'\n    !wget 'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/336d8745-ea82-40a5-9acc-1a89df23d0f3.pdf' -O 'data/amazon_2020.pdf'\n    !wget 'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/4d39f579-19d8-4119-b087-ee618abf82d6.pdf' -O 'data/amazon_2019.pdf'\n\nSet up your index and query engine to create the retrieval and generation\ncomponents of your RAG system.\n\n    \n    \n    # Setup the index\n    file_list = [os.path.join(\"data\", f) for f in os.listdir(\"data\")]\n    \n    amzn_10k_docs = SimpleDirectoryReader(input_files=file_list).load_data()\n    index = VectorStoreIndex.from_documents(documents=amzn_10k_docs)\n    \n    # Build a query engine\n    default_query_engine = index.as_query_engine(llm)\n\nLet\u2019s enter a query to make sure our RAG system is working.\n\n    \n    \n    answer = default_query_engine.query(\"What was the company's revenue in 2021?\")\n    print(answer.response)\n    \n    \n    The company's revenue in 2021 was $469,822 million.\n\nGreat! It works. Now let\u2019s try a similar query to continue validating.\n\n    \n    \n    answer = default_query_engine.query(\"What was the company's revenue in 2023?\")\n    print(answer.response)\n    \n    \n    The company's revenue in 2023 was not explicitly mentioned in the provided context. However, it is mentioned that the company's operating income increased to $36.9 billion in 2023, compared to $12.2 billion in 2022.\n\nWe can see there\u2019s a problem\u2014we know that the answer to our question is most\ndefinitely included in our documents, yet our RAG system is claiming that it\ncannot find the answer. That\u2019s because the default amount of retrieved chunks\nis rather small (a few chunks). This makes the whole system prone to errors\nand failing to capture information that is indeed located in the documents.\n\nHowever, with Jamba-Instruct, a model which handles a 256K context window\neffectively, we can increase the number of retrieved chunks from just a few\n(default value) to 100 and vastly improve the entire RAG system.\n\nLet\u2019s build a new query engine on top of our existing index and try the query\nthat failed before.\n\n    \n    \n    # Large amount of chunks in the retrieval process\n    extended_query_engine = index.as_query_engine(llm,\n                                                  similarity_top_k=100)\n    \n    answer = extended_query_engine.query(\"What was the company's revenue in 2023?\")\n    print(answer.response)\n    \n    \n    The company's revenue in 2023 was $574.785 million.\n\nWe see that the RAG system, with the help of Jamba-Instruct\u2019s 256K context\nwindow, is now able to produce the accurate answer.\n\nLet\u2019s try one more answer to validate our new RAG system.\n\n    \n    \n    answer = default_query_engine.query(\"Was there a stock split in the last five years?\")\n    print(answer.response)\n    \n    \n    No, there was no stock split in the last five years.\n    \n    \n    answer = extended_query_engine.query(\"Was there a stock split in the last five years?\")\n    print(answer.response)\n    \n    \n    Yes, there was a stock split in the last five years. On May 27, 2022, Amazon.com, Inc. effected a 20-for-1 stock split of its common stock.", "mimetype": "text/plain", "start_char_idx": 1830, "end_char_idx": 6473, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1bbb26d-1755-4aa5-a3e7-bdf473967760": {"__data__": {"id_": "d1bbb26d-1755-4aa5-a3e7-bdf473967760", "embedding": null, "metadata": {"Header_3": " Context is king", "filename": "jamba-instruct-s-256k-context-window-on-llamaindex.md", "extension": ".md", "title": "Jamba-Instruct's 256k context window on LlamaIndex", "date": "Jul 31, 2024", "url": "https://www.llamaindex.ai/blog/jamba-instruct-s-256k-context-window-on-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe7416b0-35d5-48ed-a9c7-1387cdab434c", "node_type": "4", "metadata": {"filename": "jamba-instruct-s-256k-context-window-on-llamaindex.md", "extension": ".md", "title": "Jamba-Instruct's 256k context window on LlamaIndex", "date": "Jul 31, 2024", "url": "https://www.llamaindex.ai/blog/jamba-instruct-s-256k-context-window-on-llamaindex"}, "hash": "85f47676dd585ef6f69680f12e9ec734a66fd4f734864c01b001a01ce5068324", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "561fd2da-d9e1-4e22-83d1-3cfa3a4fc63a", "node_type": "1", "metadata": {"Header_3": " RAG Q&A on financial documents", "filename": "jamba-instruct-s-256k-context-window-on-llamaindex.md", "extension": ".md", "title": "Jamba-Instruct's 256k context window on LlamaIndex", "date": "Jul 31, 2024", "url": "https://www.llamaindex.ai/blog/jamba-instruct-s-256k-context-window-on-llamaindex"}, "hash": "df68bfe060d680fc9ee2945a4f0e56a1e48b747d8b8dce17edc344b184367d88", "class_name": "RelatedNodeInfo"}}, "text": "Context is king\n\nOften, the debate is framed as \u201cRAG vs. long context.\u201d We at AI21 Labs believe\nthat\u2019s the wrong way to look at it. Rather, it\u2019s long context _plus_ RAG. When\npaired together in an AI system, a long context model enhances the quality and\naccuracy of a RAG system, especially useful in enterprise contexts that\ninvolve lengthy documents or vast databases of information.\n\nGoing forward, as RAG systems continue to scale, the number of documents and\nlengths of chunks will drastically increase. Only a long context model\u2014whose\ncontext length truly delivers\u2014can handle this amount of text.", "mimetype": "text/plain", "start_char_idx": 6480, "end_char_idx": 7082, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"f90f0e2f-a3dc-4785-8f91-a3c45a02b7e1": {"doc_hash": "0607b1e735b544c9a3e9b24c1a0847913afa3a449157aaa47c35e2bc34ad9c7c", "ref_doc_id": "fe7416b0-35d5-48ed-a9c7-1387cdab434c"}, "561fd2da-d9e1-4e22-83d1-3cfa3a4fc63a": {"doc_hash": "df68bfe060d680fc9ee2945a4f0e56a1e48b747d8b8dce17edc344b184367d88", "ref_doc_id": "fe7416b0-35d5-48ed-a9c7-1387cdab434c"}, "d1bbb26d-1755-4aa5-a3e7-bdf473967760": {"doc_hash": "68c8e243731d51d712d2592bab6d46364a2d8f1ff24fc6a4fed9496063e84426", "ref_doc_id": "fe7416b0-35d5-48ed-a9c7-1387cdab434c"}}, "docstore/ref_doc_info": {"fe7416b0-35d5-48ed-a9c7-1387cdab434c": {"node_ids": ["f90f0e2f-a3dc-4785-8f91-a3c45a02b7e1", "561fd2da-d9e1-4e22-83d1-3cfa3a4fc63a", "d1bbb26d-1755-4aa5-a3e7-bdf473967760"], "metadata": {"filename": "jamba-instruct-s-256k-context-window-on-llamaindex.md", "extension": ".md", "title": "Jamba-Instruct's 256k context window on LlamaIndex", "date": "Jul 31, 2024", "url": "https://www.llamaindex.ai/blog/jamba-instruct-s-256k-context-window-on-llamaindex"}}}}