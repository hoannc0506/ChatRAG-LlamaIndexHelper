{"docstore/data": {"8fc8d06d-3cd7-4fbd-93bf-79483ed05364": {"__data__": {"id_": "8fc8d06d-3cd7-4fbd-93bf-79483ed05364", "embedding": null, "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d99e4905-d1e3-4f39-8612-085066c2cab2", "node_type": "4", "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "0f7a5685c5883fbac0e3d3d6f269286645e361b8584df00b881cee2e53504eed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26c06b92-f546-4d58-8fb7-ec6a7ceb1907", "node_type": "1", "metadata": {"Header_1": " Standalone LLM Abstractions"}, "hash": "01a736cab1e2e6f8e43f41b4f59a375de7b26604ae4c02dee02b83764681ced8", "class_name": "RelatedNodeInfo"}}, "text": "A few months ago, we launched LlamaIndex 0.6.0, which included a massive\nrewrite of our codebase to make our library more modular, customizable, and\naccessible to both beginner and advanced users:\n\n  * We created modular storage abstractions (data, indices), and compute abstractions (retrievers, query engines). \n  * We created a lower-level API where users could use our modules (retrievers, query engines) independently and customize it as part of a larger system. \n\nToday, we\u2019re excited to launch LlamaIndex 0.7.0. Our latest release continues\nthe theme of improving modularity/customizability at the lower level to enable\n**bottoms-up development of LLM applications over your data.** You now have\neven more control over using key abstractions: the LLM, our response\nsynthesizer, and our Document and Node objects.\n\n  * We\u2019ve created **standalone LLM abstractions** (OpenAI, HuggingFace, PaLM). \n  * We\u2019ve made our **response synthesis module an independent module** you can use completely independently of the rest of our abstractions \u2014 get rid of the prompt boilerplate of trying to figure out how to fit context within a context window. \n  * We\u2019ve added **extensive metadata management capabilities** to our Document/Node objects \u2014 now you have complete control over context you decide to inject into your documents. \n\nBelow, we describe each section more in detail. We also outline a full list of\nbreaking changes at the bottom.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1437, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26c06b92-f546-4d58-8fb7-ec6a7ceb1907": {"__data__": {"id_": "26c06b92-f546-4d58-8fb7-ec6a7ceb1907", "embedding": null, "metadata": {"Header_1": " Standalone LLM Abstractions", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d99e4905-d1e3-4f39-8612-085066c2cab2", "node_type": "4", "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "0f7a5685c5883fbac0e3d3d6f269286645e361b8584df00b881cee2e53504eed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8fc8d06d-3cd7-4fbd-93bf-79483ed05364", "node_type": "1", "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "6e2267ca9d93459df0df866085e465d8de8e8b6a5e2b9bf371510bfa935a2acc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6026233-b9d6-4eb5-a70c-97c3bfccc261", "node_type": "1", "metadata": {"Header_1": " Standalone LLM Abstractions", "Header_2": " High-level Motivation"}, "hash": "44c582f46ab40b72699946d41c9372baf8a7427ea322185a47ad60449e376a2a", "class_name": "RelatedNodeInfo"}}, "text": "Standalone LLM Abstractions\n\nWe\u2019ve created standalone LLM abstractions for OpenAI, HuggingFace, and PaLM.\nThese abstractions can be used on their own, or as part of an existing\nLlamaIndex system (query engines, retrievers).", "mimetype": "text/plain", "start_char_idx": 1442, "end_char_idx": 1665, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6026233-b9d6-4eb5-a70c-97c3bfccc261": {"__data__": {"id_": "e6026233-b9d6-4eb5-a70c-97c3bfccc261", "embedding": null, "metadata": {"Header_1": " Standalone LLM Abstractions", "Header_2": " High-level Motivation", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d99e4905-d1e3-4f39-8612-085066c2cab2", "node_type": "4", "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "0f7a5685c5883fbac0e3d3d6f269286645e361b8584df00b881cee2e53504eed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26c06b92-f546-4d58-8fb7-ec6a7ceb1907", "node_type": "1", "metadata": {"Header_1": " Standalone LLM Abstractions", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "1c19f349a5ac01275ffc9f7707def193fdefa99a7c87f914758cd2ddc7ee453f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c8850f7-da4f-47ab-8ae7-b2bd04f7d906", "node_type": "1", "metadata": {"Header_1": " Standalone LLM Abstractions", "Header_2": " **Using on their own**"}, "hash": "7ff0d886fd0fb6d2d04c97201018590799e2aecb17eaf22fa8f7e36c8f220e75", "class_name": "RelatedNodeInfo"}}, "text": "High-level Motivation\n\nWe did this for multiple reasons:\n\n  * Cleaner abstractions in the codebase. Before, our ` LLMPredictor ` class had a ton of leaky abstractions with the underlying LangChain LLM class. This made our LLM abstractions hard to reason about, and hard to customize. \n  * Slightly cleaner dev UX. Before, if you wanted to customize the default LLM (for instance, use \u201ctext-davinci-003\u201d, you had to import the correct LangChain class, wrap it in our LLMPredictor, and then pass it to ServiceContext. Now it\u2019s easy to just import our LLM abstraction (which is natively documented with our docs) and plug it into ServiceContext. Of course, you can still use LangChain\u2019s LLMs if you wish. \n  * Conducive to bottoms-up development: it makes sense to play around with these LLM modules independently before plugging them in as part of a larger system. It\u2019s reflective of our bigger push in 0.7.0 to let users compose their own workflows.", "mimetype": "text/plain", "start_char_idx": 1671, "end_char_idx": 2619, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c8850f7-da4f-47ab-8ae7-b2bd04f7d906": {"__data__": {"id_": "3c8850f7-da4f-47ab-8ae7-b2bd04f7d906", "embedding": null, "metadata": {"Header_1": " Standalone LLM Abstractions", "Header_2": " **Using on their own**", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d99e4905-d1e3-4f39-8612-085066c2cab2", "node_type": "4", "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "0f7a5685c5883fbac0e3d3d6f269286645e361b8584df00b881cee2e53504eed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6026233-b9d6-4eb5-a70c-97c3bfccc261", "node_type": "1", "metadata": {"Header_1": " Standalone LLM Abstractions", "Header_2": " High-level Motivation", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "4c521418f06d7e2f8a81817ce6716cedecacfb020fc8241143d3699228cb5671", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e910915-491c-4ce2-b329-bf90e6c9bdd8", "node_type": "1", "metadata": {"Header_1": " Standalone LLM Abstractions", "Header_2": " **Resources**"}, "hash": "07e2ecb86c22b649c51e512137b5312ddcf263716edde4d4557381627df4a202", "class_name": "RelatedNodeInfo"}}, "text": "**Using on their own**\n\nOur LLM abstractions support both ` complete ` and ` chat ` endpoints. The\nmain difference is that ` complete ` is designed to take in a simple string\ninput, and output a ` CompletionResponse ` (containing text output +\nadditional fields). ` chat ` takes in a ` ChatMessage ` and outputs a `\nChatResponse ` (containing a chat message + additional fields).\n\nThese LLM endpoints also natively support streaming via ` stream_complete `\nand ` stream_chat ` .\n\nHere\u2019s on how you can use the LLM abstractions on their own:\n\n    \n    \n    from llama_index.llms import OpenAI\n    \n    # using complete endpoint\n    resp = OpenAI().complete('Paul Graham is ')\n    print(resp)\n    # get raw object\n    resp_raw = resp.raw\n    # using chat endpoint\n    from llama_index.llms import ChatMessage, OpenAI\n    messages = [\n        ChatMessage(role='system', content='You are a pirate with a colorful personality'),\n        ChatMessage(role='user', content='What is your name')\n    ]\n    resp = OpenAI().chat(messages)\n    print(resp)\n    # get raw object\n    resp_raw = resp.raw\n    # using streaming endpoint\n    from llama_index.llms import OpenAI\n    llm = OpenAI()\n    resp = llm.stream_complete('Paul Graham is ')\n    for delta in resp:\n        print(delta, end='')\n\nHere\u2019s how you can use the LLM abstractions as part of an overall LlamaIndex\nsystem.\n\n    \n    \n    from llama_index.llms import OpenAI\n    from llama_index.indices.service_context import ServiceContext\n    from llama_index import VectorStoreIndex\n    \n    llm = OpenAI(model='gpt-3.5-turbo', temperature=0)\n    service_context = ServiceContext.from_defaults(llm=llm)\n    index = VectorStoreIndex.from_documents(docs, service_context=service_context)\n    response = index.as_query_engine().query(\"&lt;question&gt;\")\n\nNote: Our top-level ` LLMPredictor ` still exists but is less user-facing (and\nwe might deprecate in the future). Also, you can still use LangChain LLMs\nthrough our ` LangChainLLM ` class.", "mimetype": "text/plain", "start_char_idx": 2626, "end_char_idx": 4612, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e910915-491c-4ce2-b329-bf90e6c9bdd8": {"__data__": {"id_": "5e910915-491c-4ce2-b329-bf90e6c9bdd8", "embedding": null, "metadata": {"Header_1": " Standalone LLM Abstractions", "Header_2": " **Resources**", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d99e4905-d1e3-4f39-8612-085066c2cab2", "node_type": "4", "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "0f7a5685c5883fbac0e3d3d6f269286645e361b8584df00b881cee2e53504eed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c8850f7-da4f-47ab-8ae7-b2bd04f7d906", "node_type": "1", "metadata": {"Header_1": " Standalone LLM Abstractions", "Header_2": " **Using on their own**", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "2aaa8b671aecfc2a1fd0873e03fc07c196e77af764a42d2c614f1276b3572cdf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e18418f0-7e8b-43df-afcf-a1c6d264a37d", "node_type": "1", "metadata": {"Header_1": " Standalone Response Synthesis Modules"}, "hash": "0de945a1414dcffb8941dc77d53db8651853a38dc2ac6bd74328b7d8fa2a0157", "class_name": "RelatedNodeInfo"}}, "text": "**Resources**\n\nAll of our notebooks have by default been updated to use our native OpenAI LLM\nintegration. Here\u2019s some resources to show both the LLM abstraction on its own\nas well as how it can be used in the overall system:\n\n  * [ OpenAI LLM ](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/llm/openai.ipynb)\n  * [ Using LLM in LLMPredictor ](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/llm/llm_predictor.ipynb)\n  * [ Changing LLM within Index/Query Engine ](https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html#example-changing-the-underlying-llm)\n  * [ Defining a custom LLM Model ](https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html#example-using-a-custom-llm-model-advanced)", "mimetype": "text/plain", "start_char_idx": 4618, "end_char_idx": 5395, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e18418f0-7e8b-43df-afcf-a1c6d264a37d": {"__data__": {"id_": "e18418f0-7e8b-43df-afcf-a1c6d264a37d", "embedding": null, "metadata": {"Header_1": " Standalone Response Synthesis Modules", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d99e4905-d1e3-4f39-8612-085066c2cab2", "node_type": "4", "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "0f7a5685c5883fbac0e3d3d6f269286645e361b8584df00b881cee2e53504eed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e910915-491c-4ce2-b329-bf90e6c9bdd8", "node_type": "1", "metadata": {"Header_1": " Standalone LLM Abstractions", "Header_2": " **Resources**", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "9f0b6c35aae556e43885dcf1d6f04a81150eb4e663da9fe22ef019748ea5f679", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ecb9b5b5-7d63-429b-9adf-b1d5ac942df6", "node_type": "1", "metadata": {"Header_1": " Standalone Response Synthesis Modules", "Header_2": " **Context**"}, "hash": "2cd687c2484f5f0e61e5fcfe5ee88724dc6efdf1315fd67a02cb5c2152db335b", "class_name": "RelatedNodeInfo"}}, "text": "Standalone Response Synthesis Modules", "mimetype": "text/plain", "start_char_idx": 5400, "end_char_idx": 5437, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ecb9b5b5-7d63-429b-9adf-b1d5ac942df6": {"__data__": {"id_": "ecb9b5b5-7d63-429b-9adf-b1d5ac942df6", "embedding": null, "metadata": {"Header_1": " Standalone Response Synthesis Modules", "Header_2": " **Context**", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d99e4905-d1e3-4f39-8612-085066c2cab2", "node_type": "4", "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "0f7a5685c5883fbac0e3d3d6f269286645e361b8584df00b881cee2e53504eed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e18418f0-7e8b-43df-afcf-a1c6d264a37d", "node_type": "1", "metadata": {"Header_1": " Standalone Response Synthesis Modules", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "db1f12a9a9ab62d6589aca24f92ed8312c74a59a651490e367a71266f20bfe2e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b79e2f88-2fc7-422b-88a1-930437b2935b", "node_type": "1", "metadata": {"Header_1": " Standalone Response Synthesis Modules", "Header_2": " **Usage**"}, "hash": "d8a9708556ed381d62af34165a76fe9cd7902b0b288489470e699178cfdeadb9", "class_name": "RelatedNodeInfo"}}, "text": "**Context**\n\nIn any RAG system, there is retrieval and there is synthesis. The\nresponsibility of the synthesis component is to take in incoming context as\ninput, and synthesize a response using the LLM.\n\nFundamentally, the synthesis module needs to synthesize a response over\n**any** context list, regardless of how long that context list is. This is\nessentially \u201cboilerplate\u201d that an LLM developer / [ \u201cAI engineer\u201d\n](https://www.latent.space/p/ai-engineer) must write.\n\nWe had this as an internal abstraction in LlamaIndex before (as a `\nResponseSynthesizer ` ), but the external-facing UX was unfriendly to users.\nThe actual piece that gathered responses (the ` ResponseBuilder ` ) was hard\nto customize, and the ` ResponseSynthesizer ` itself was adding an extra\nunnecessary layer.\n\nNow we have a set of standalone modules that you can easily import.\nPreviously, when you set the ` response_mode ` in the query engine, these were\nbeing setup for you. Now they are more directly available and user-facing.\n\nHere\u2019s a list of all the new ` Response Synthesiszer ` modules available from\n` llama_index.response_synthesizer ` :\n\n  * ` Refine ` \\- Query an LLM, sending each text chunk individually. After the first LLM call, the existing answer is also sent to the LLM for updating and refinement using the next text chunk. \n  * ` Accumulate ` \\- Query an LLM with the same prompt across multiple text chunks, and return a formatted list of responses \n  * ` Compact ` \\- The same as ` Refine ` , but puts as much text as possible into each LLM call \n  * ` CompactAndAccumulate ` \\- The same as ` Accumulate ` , but puts as much text as possible \n  * ` TreeSummarize ` \\- Create a bottom-up summary from the provided text chunks, and return the root summary \n  * ` SimpleSummarize ` \\- Combine and truncate all text chunks, and summarize in a single LLM call", "mimetype": "text/plain", "start_char_idx": 5443, "end_char_idx": 7299, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b79e2f88-2fc7-422b-88a1-930437b2935b": {"__data__": {"id_": "b79e2f88-2fc7-422b-88a1-930437b2935b", "embedding": null, "metadata": {"Header_1": " Standalone Response Synthesis Modules", "Header_2": " **Usage**", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d99e4905-d1e3-4f39-8612-085066c2cab2", "node_type": "4", "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "0f7a5685c5883fbac0e3d3d6f269286645e361b8584df00b881cee2e53504eed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ecb9b5b5-7d63-429b-9adf-b1d5ac942df6", "node_type": "1", "metadata": {"Header_1": " Standalone Response Synthesis Modules", "Header_2": " **Context**", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "78de9d995854f71a991ada4f023d0363379fac15c170604dec785c900583d56e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f544c1de-47bb-4f23-b841-7fec15ba7f08", "node_type": "1", "metadata": {"Header_1": " Standalone Response Synthesis Modules", "Header_2": " Resources"}, "hash": "be0a66b9795d70bfeddfabb3802817d10d7a6232fd3c525aec5fdea552ba4c3d", "class_name": "RelatedNodeInfo"}}, "text": "**Usage**\n\nAs detailed above, you can directly set a response synthesizer in a query\nengine, or let the ` response_mode ` fetch the relevant response synthesizer.\n\nFurthermore though, you can directly call and use these synthesizers as low\nlevel modules. Here\u2019s a small example:\n\n    \n    \n    from llama_index import ServiceContext\n    from llama_index.response_synthesizers import CompactAndRefine\n    \n    # you can also configure the text_qa_template, refine_template, \n    # and streaming toggle from here\n    response_synthesizer = CompactAndRefine(\n      service_context=service_context.from_defaults()\n    )\n    response = response_synthesizer.get_response(\n     \"What skills does Bob have?\",\n      text_chunks=[\" ...\"]  # here would be text, hopefully about Bob's skills\n    )", "mimetype": "text/plain", "start_char_idx": 7306, "end_char_idx": 8091, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f544c1de-47bb-4f23-b841-7fec15ba7f08": {"__data__": {"id_": "f544c1de-47bb-4f23-b841-7fec15ba7f08", "embedding": null, "metadata": {"Header_1": " Standalone Response Synthesis Modules", "Header_2": " Resources", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d99e4905-d1e3-4f39-8612-085066c2cab2", "node_type": "4", "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "0f7a5685c5883fbac0e3d3d6f269286645e361b8584df00b881cee2e53504eed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b79e2f88-2fc7-422b-88a1-930437b2935b", "node_type": "1", "metadata": {"Header_1": " Standalone Response Synthesis Modules", "Header_2": " **Usage**", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "4ce86a67e08dc4fdca65830758e5d30c03a7fbdac8aa87fb59c7d6f939f0eef4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd3cd8b0-caab-4d1c-948a-5aca52876d42", "node_type": "1", "metadata": {"Header_1": " Metadata Management Capabilities"}, "hash": "16441546ef92f9702c05c76ec601fbff4d36e47b95be82ef70c3aef80d80928a", "class_name": "RelatedNodeInfo"}}, "text": "Resources\n\nHere are some additional notebooks showing how to use `\nget_response_synthesizer ` :\n\n  * [ Low-level API Usage Pattern ](https://gpt-index.readthedocs.io/en/latest/guides/primer/usage_pattern.html#low-level-api)\n  * [ Custom Retrievers ](https://gpt-index.readthedocs.io/en/latest/examples/query_engine/CustomRetrievers.html#plugin-retriever-into-query-engine)", "mimetype": "text/plain", "start_char_idx": 8097, "end_char_idx": 8469, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd3cd8b0-caab-4d1c-948a-5aca52876d42": {"__data__": {"id_": "fd3cd8b0-caab-4d1c-948a-5aca52876d42", "embedding": null, "metadata": {"Header_1": " Metadata Management Capabilities", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d99e4905-d1e3-4f39-8612-085066c2cab2", "node_type": "4", "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "0f7a5685c5883fbac0e3d3d6f269286645e361b8584df00b881cee2e53504eed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f544c1de-47bb-4f23-b841-7fec15ba7f08", "node_type": "1", "metadata": {"Header_1": " Standalone Response Synthesis Modules", "Header_2": " Resources", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "09da8c35e84eeb4fa8029277a65707cb2f9f42b50f660a42d7469045be36986a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75c04a00-25a7-415f-bb7b-1dd6841a314c", "node_type": "1", "metadata": {"Header_1": " Full List of Breaking Changes"}, "hash": "a9dd700d419928094bfd522381bc8be4b2586d5fb87d1ae81f9e2e7a6173631b", "class_name": "RelatedNodeInfo"}}, "text": "Metadata Management Capabilities\n\nIf you want to have good performance in any LLM application over your data\n(including a RAG pipeline), you need to make sure that your documents actually\ncontain relevant context for the query. One way to do this is to add proper\nmetadata, both at the document-level and after the documents have been parsed\ninto text chunks (into Nodes).\n\nWe allow you to define metadata fields within a Document, customize the ID,\nand also customize the metadata text/format for LLM usage and embedding usage.\n\n**Defining Metadata Fields**\n\n    \n    \n    document = Document(\n        text='text', \n        metadata={\n            'filename': '&lt;doc_file_name&gt;', \n            'category': '&lt;category&gt;'\n        }\n    )\n\n**Customizing the ID**\n\nThe ID of each document can be set multiple ways\n\n  * Within the constructor: ` document = Document(text=\"text\", doc_id_=\"id\") `\n  * After constructing the object: ` document.doc_id = \"id\" `\n  * Automatically using the ` SimpleDirectoryReader ` : ` SimpleDirectoryReader(filename_as_id=True).load_data() `\n\n**Customizing the Metadata Text for LLMs and Embeddings**\n\nAs seen above, you can set metadata containing useful information. By default,\nall the metadata will be seen by the embedding model and the LLM. However,\nsometimes you may want to only include data to bias embeddings, or only\ninclude data as extra information for the LLM!\n\nWith the new ` Document ` objects, you can configure what each metadata field\nis used for:\n\n    \n    \n    document = Document(\n        text='text', \n        metadata={\n            'filename': '&lt;doc_file_name&gt;', \n            'category': '&lt;category&gt;'\n        },\n        excluded_llm_metadata_keys=['filename', 'category'],\n        excluded_embed_metadata_keys=['filename']\n    )\n\n**Customizing the Metadata Format Template**\n\nWhen the metadata is inserted into the text, it follows a very specific\nformat. This format is configurable at multiple levels:\n\n    \n    \n    from llama_index.schema import MetadataMode\n    \n    document = Document(\n      text='text',\n      metadata={\"key\": \"val\"},\n      metadata_seperator=\"::\",\n        metadata_template=\"{key}=&gt;{value}\",\n        text_template=\"Metadata: {metadata_str}\\\\n-----\\\\nContent: {content}\"\n    )\n    # available modes are ALL, NONE, LLM, and EMBED\n    print(document.get_content(metadata_mode=MetadataMode.ALL))\n    # output:\n    # Metadata: key=&gt;val\n    # -----\n    # text\n\nPlease check out this guide for more [ details ](https://gpt-\nindex.readthedocs.io/en/latest/how_to/customization/custom_documents.html) !", "mimetype": "text/plain", "start_char_idx": 8474, "end_char_idx": 11069, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75c04a00-25a7-415f-bb7b-1dd6841a314c": {"__data__": {"id_": "75c04a00-25a7-415f-bb7b-1dd6841a314c", "embedding": null, "metadata": {"Header_1": " Full List of Breaking Changes", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d99e4905-d1e3-4f39-8612-085066c2cab2", "node_type": "4", "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "0f7a5685c5883fbac0e3d3d6f269286645e361b8584df00b881cee2e53504eed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd3cd8b0-caab-4d1c-948a-5aca52876d42", "node_type": "1", "metadata": {"Header_1": " Metadata Management Capabilities", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "70764317c391d2636d9f1e09c54b460d9c504a5c3a12dc1c583f8d16ba562d3f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b7fa1f3-f048-48c4-ab14-323aa54a37aa", "node_type": "1", "metadata": {"Header_1": " Full List of Breaking Changes", "Header_2": " Response Synthesis + Node Postprocessors"}, "hash": "a2dc8e29b445bacf43d41d8b631998ce0819d7fd734b3dc26b5d86d97002bdd4", "class_name": "RelatedNodeInfo"}}, "text": "Full List of Breaking Changes", "mimetype": "text/plain", "start_char_idx": 11074, "end_char_idx": 11103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b7fa1f3-f048-48c4-ab14-323aa54a37aa": {"__data__": {"id_": "4b7fa1f3-f048-48c4-ab14-323aa54a37aa", "embedding": null, "metadata": {"Header_1": " Full List of Breaking Changes", "Header_2": " Response Synthesis + Node Postprocessors", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d99e4905-d1e3-4f39-8612-085066c2cab2", "node_type": "4", "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "0f7a5685c5883fbac0e3d3d6f269286645e361b8584df00b881cee2e53504eed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75c04a00-25a7-415f-bb7b-1dd6841a314c", "node_type": "1", "metadata": {"Header_1": " Full List of Breaking Changes", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "70da124ed527645153c982eca0ccc12778bc328c649068359c738df234911bed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "473c2d07-10e2-48ba-ba3b-071db99b7490", "node_type": "1", "metadata": {"Header_1": " Full List of Breaking Changes", "Header_2": " LLM Predictor"}, "hash": "c95dec89e64ab2c38f34e0a9ed429f061bbde54d508eedfcd1ff675886509274", "class_name": "RelatedNodeInfo"}}, "text": "Response Synthesis + Node Postprocessors\n\nThe ` ResponseSynthesizer ` object class has been removed, and replaced with `\nget_response_synthesizer ` . In addition to this, node post processors are now\nhandled by the query engine directly, and the old ` SentenceEmbeddingOptimizer\n` has been switched to become a node post processor instance itself.\n\nHere is an example of the required migration to use all moved features.\n\n**Old**\n\n    \n    \n    from llama_index import (\n        VectorStoreIndex,\n        ResponseSynthesizer,\n    )\n    from llama_index.indices.postprocessor import SimilarityPostprocessor\n    from llama_index.optimizers import SentenceEmbeddingOptimizer\n    from llama_index.query_engine import RetrieverQueryEngine\n    \n    documents = ...\n    # build index\n    index = VectorStoreIndex.from_documents(documents)\n    # configure retriever\n    retriever = index.as_retriever(\n       similarity_top_k=3\n    )\n    # configure response synthesizer\n    response_synthesizer = ResponseSynthesizer.from_args(\n       response_mode=\"tree_summarize\",\n        node_postprocessors=[\n            SimilarityPostprocessor(similarity_cutoff=0.7),\n            SentenceEmbeddingOptimizer(percentile_cutoff=0.5)\n        ]\n    )\n    # assemble query engine\n    query_engine = RetrieverQueryEngine(\n        retriever=retriever,\n        response_synthesizer=response_synthesizer,\n    )\n\n**New**\n\n    \n    \n    from llama_index import (\n        VectorStoreIndex,\n        get_response_synthesizer,\n    )\n    from llama_index.indices.postprocessor import (\n        SimilarityPostprocessor,\n        SentenceEmbeddingOptimizer\n    )\n    \n    documents = ...\n    # build index\n    index = VectorStoreIndex.from_documents(documents)\n    # configure response synthesizer\n    response_synthesizer = get_response_synthesizer(\n       response_mode=\"tree_summarize\",\n    )\n    # assemble query engine\n    query_engine = index.as_query_engine(\n      similarity_top_k=3,\n        response_synthesizer=response_synthesizer,\n        node_postprocessors=[\n            SimilarityPostprocessor(similarity_cutoff=0.7),\n            SentenceEmbeddingOptimizer(percentile_cutoff=0.5)\n        ]\n    )", "mimetype": "text/plain", "start_char_idx": 11109, "end_char_idx": 13281, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "473c2d07-10e2-48ba-ba3b-071db99b7490": {"__data__": {"id_": "473c2d07-10e2-48ba-ba3b-071db99b7490", "embedding": null, "metadata": {"Header_1": " Full List of Breaking Changes", "Header_2": " LLM Predictor", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d99e4905-d1e3-4f39-8612-085066c2cab2", "node_type": "4", "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "0f7a5685c5883fbac0e3d3d6f269286645e361b8584df00b881cee2e53504eed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b7fa1f3-f048-48c4-ab14-323aa54a37aa", "node_type": "1", "metadata": {"Header_1": " Full List of Breaking Changes", "Header_2": " Response Synthesis + Node Postprocessors", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "89f2d5deb35d2c4d785dad6e35c941c7523815ef0045fa070b77b43cd26ec9e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d7015e1-01ad-445a-adf5-681e99ee4269", "node_type": "1", "metadata": {"Header_1": " Full List of Breaking Changes", "Header_2": " Chat Engine"}, "hash": "60f6859f060eb7ae142e7167a80f9a11a63bd1a50a798c500924adbffdd430ad", "class_name": "RelatedNodeInfo"}}, "text": "LLM Predictor\n\nWhile introducing a new LLM abstraction, we cleaned up the LLM Predictor and\nremoved several deprecated functionalities:\n\n  1. Remove ` ChatGPTLLMPredictor ` and ` HuggingFaceLLMPredictor ` (use ` OpenAI ` and ` HuggingFaceLLM ` instead, see [ migration guide ](https://gpt-index.readthedocs.io/en/latest/how_to/customization/llms_migration_guide.html) ) \n  2. Remove support for setting ` cache ` via ` LLMPredictor ` constructor. \n  3. Removed ` llama_index.token_counter.token_counter ` module (see [ migration guide ](https://gpt-index.readthedocs.io/en/latest/how_to/callbacks/token_counting_migration.html) ). \n\nNow, the LLM Predictor class is mostly a lightweight wrapper on top of the `\nLLM ` abstraction that handles:\n\n  * conversion of prompts to the string or chat message input format expected by the LLM \n  * logging of prompts and responses to a callback manager \n\nWe advice users to configure the ` llm ` argument in ` ServiceContext `\ndirectly (instead of creating LLM Predictor).", "mimetype": "text/plain", "start_char_idx": 13287, "end_char_idx": 14298, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d7015e1-01ad-445a-adf5-681e99ee4269": {"__data__": {"id_": "4d7015e1-01ad-445a-adf5-681e99ee4269", "embedding": null, "metadata": {"Header_1": " Full List of Breaking Changes", "Header_2": " Chat Engine", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d99e4905-d1e3-4f39-8612-085066c2cab2", "node_type": "4", "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "0f7a5685c5883fbac0e3d3d6f269286645e361b8584df00b881cee2e53504eed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "473c2d07-10e2-48ba-ba3b-071db99b7490", "node_type": "1", "metadata": {"Header_1": " Full List of Breaking Changes", "Header_2": " LLM Predictor", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "afb81090598dfbf19e631a69e7b55e645acd37b778007ea4ffa883f90895c1e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94c6ee9f-4e04-4c42-98dd-b2d061bb205e", "node_type": "1", "metadata": {"Header_1": " Full List of Breaking Changes", "Header_2": " Prompt Helper"}, "hash": "92349daedf7bbf919293e41e6e94b4f19df852d4b3e706c0a8aa21b0124a3280", "class_name": "RelatedNodeInfo"}}, "text": "Chat Engine\n\nWe updated the ` BaseChatEngine ` interface to take in a ` List[ChatMessage]]\n` for the ` chat_history ` instead of tuple of strings. This makes the data\nmodel consistent with the input/output of the ` LLM ` , also more flexibility\nto specify consecutive messages with the same role.\n\n**Old**\n\n    \n    \n    engine = SimpleChatEngine.from_defaults(\n    \tchat_history=[(\"human message\", \"assistant message\")],\n    )\n    response = engine.chat(\"new human message\")\n\n**New**\n\n    \n    \n    engine = SimpleChatEngine.from_defaults(\n        service_context=mock_service_context,\n        chat_history=[\n            ChatMessage(role=MessageRole.USER, content=\"human message\"),\n            ChatMessage(role=MessageRole.ASSISTANT, content=\"assistant message\"),\n        ],\n    )\n    response = engine.chat(\"new human message\")\n\nWe also exposed ` chat_history ` state as a property and supported overriding\n` chat_history ` in ` chat ` and ` achat ` endpoints.", "mimetype": "text/plain", "start_char_idx": 14304, "end_char_idx": 15266, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94c6ee9f-4e04-4c42-98dd-b2d061bb205e": {"__data__": {"id_": "94c6ee9f-4e04-4c42-98dd-b2d061bb205e", "embedding": null, "metadata": {"Header_1": " Full List of Breaking Changes", "Header_2": " Prompt Helper", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d99e4905-d1e3-4f39-8612-085066c2cab2", "node_type": "4", "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "0f7a5685c5883fbac0e3d3d6f269286645e361b8584df00b881cee2e53504eed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d7015e1-01ad-445a-adf5-681e99ee4269", "node_type": "1", "metadata": {"Header_1": " Full List of Breaking Changes", "Header_2": " Chat Engine", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "7f17fbf2241505af485ec0b90fbdeb29932383034609a03d0abe6db2a401810c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9a94d43-bb26-4ba7-aafb-36b72e3a4e64", "node_type": "1", "metadata": {"Header_1": " Conclusion"}, "hash": "98f36b4da1c97acda66a6a4a3e40418871a1ed6e644ef56332ae9cd1c20a0845", "class_name": "RelatedNodeInfo"}}, "text": "Prompt Helper\n\nWe removed some previously deprecated arguments: ` max_input_size ` , `\nembedding_limit ` , ` max_chunk_overlap `", "mimetype": "text/plain", "start_char_idx": 15272, "end_char_idx": 15400, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9a94d43-bb26-4ba7-aafb-36b72e3a4e64": {"__data__": {"id_": "b9a94d43-bb26-4ba7-aafb-36b72e3a4e64", "embedding": null, "metadata": {"Header_1": " Conclusion", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d99e4905-d1e3-4f39-8612-085066c2cab2", "node_type": "4", "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "0f7a5685c5883fbac0e3d3d6f269286645e361b8584df00b881cee2e53504eed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94c6ee9f-4e04-4c42-98dd-b2d061bb205e", "node_type": "1", "metadata": {"Header_1": " Full List of Breaking Changes", "Header_2": " Prompt Helper", "filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "38fa2000386e201c3cffd4314e59bc477ec0ef03f7ef7a08f21b09614cbba828", "class_name": "RelatedNodeInfo"}}, "text": "Conclusion\n\nAt a high-level, we hope that these changes continue to enable bottoms-up\ndevelopment of LLM applications over your data. We first encourage you to play\naround with our new modules on their own to get a sense what they do and where\nthey can be used. Once you\u2019re ready to use them in more advanced workflows,\nthen you can figure out how to use our outer components to setup a\nsophisticated RAG pipeline.\n\nAs always, our [ repo ](https://github.com/jerryjliu/llama_index) is here and\nour [ docs ](https://gpt-index.readthedocs.io/en/latest/) are here. If you\nhave thoughts/comments, don\u2019t hesitate to hop in our [ Discord\n](https://discord.gg/dGcwcsnxhU) !", "mimetype": "text/plain", "start_char_idx": 15405, "end_char_idx": 16071, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"8fc8d06d-3cd7-4fbd-93bf-79483ed05364": {"doc_hash": "6e2267ca9d93459df0df866085e465d8de8e8b6a5e2b9bf371510bfa935a2acc", "ref_doc_id": "d99e4905-d1e3-4f39-8612-085066c2cab2"}, "26c06b92-f546-4d58-8fb7-ec6a7ceb1907": {"doc_hash": "1c19f349a5ac01275ffc9f7707def193fdefa99a7c87f914758cd2ddc7ee453f", "ref_doc_id": "d99e4905-d1e3-4f39-8612-085066c2cab2"}, "e6026233-b9d6-4eb5-a70c-97c3bfccc261": {"doc_hash": "4c521418f06d7e2f8a81817ce6716cedecacfb020fc8241143d3699228cb5671", "ref_doc_id": "d99e4905-d1e3-4f39-8612-085066c2cab2"}, "3c8850f7-da4f-47ab-8ae7-b2bd04f7d906": {"doc_hash": "2aaa8b671aecfc2a1fd0873e03fc07c196e77af764a42d2c614f1276b3572cdf", "ref_doc_id": "d99e4905-d1e3-4f39-8612-085066c2cab2"}, "5e910915-491c-4ce2-b329-bf90e6c9bdd8": {"doc_hash": "9f0b6c35aae556e43885dcf1d6f04a81150eb4e663da9fe22ef019748ea5f679", "ref_doc_id": "d99e4905-d1e3-4f39-8612-085066c2cab2"}, "e18418f0-7e8b-43df-afcf-a1c6d264a37d": {"doc_hash": "db1f12a9a9ab62d6589aca24f92ed8312c74a59a651490e367a71266f20bfe2e", "ref_doc_id": "d99e4905-d1e3-4f39-8612-085066c2cab2"}, "ecb9b5b5-7d63-429b-9adf-b1d5ac942df6": {"doc_hash": "78de9d995854f71a991ada4f023d0363379fac15c170604dec785c900583d56e", "ref_doc_id": "d99e4905-d1e3-4f39-8612-085066c2cab2"}, "b79e2f88-2fc7-422b-88a1-930437b2935b": {"doc_hash": "4ce86a67e08dc4fdca65830758e5d30c03a7fbdac8aa87fb59c7d6f939f0eef4", "ref_doc_id": "d99e4905-d1e3-4f39-8612-085066c2cab2"}, "f544c1de-47bb-4f23-b841-7fec15ba7f08": {"doc_hash": "09da8c35e84eeb4fa8029277a65707cb2f9f42b50f660a42d7469045be36986a", "ref_doc_id": "d99e4905-d1e3-4f39-8612-085066c2cab2"}, "fd3cd8b0-caab-4d1c-948a-5aca52876d42": {"doc_hash": "70764317c391d2636d9f1e09c54b460d9c504a5c3a12dc1c583f8d16ba562d3f", "ref_doc_id": "d99e4905-d1e3-4f39-8612-085066c2cab2"}, "75c04a00-25a7-415f-bb7b-1dd6841a314c": {"doc_hash": "70da124ed527645153c982eca0ccc12778bc328c649068359c738df234911bed", "ref_doc_id": "d99e4905-d1e3-4f39-8612-085066c2cab2"}, "4b7fa1f3-f048-48c4-ab14-323aa54a37aa": {"doc_hash": "89f2d5deb35d2c4d785dad6e35c941c7523815ef0045fa070b77b43cd26ec9e5", "ref_doc_id": "d99e4905-d1e3-4f39-8612-085066c2cab2"}, "473c2d07-10e2-48ba-ba3b-071db99b7490": {"doc_hash": "afb81090598dfbf19e631a69e7b55e645acd37b778007ea4ffa883f90895c1e5", "ref_doc_id": "d99e4905-d1e3-4f39-8612-085066c2cab2"}, "4d7015e1-01ad-445a-adf5-681e99ee4269": {"doc_hash": "7f17fbf2241505af485ec0b90fbdeb29932383034609a03d0abe6db2a401810c", "ref_doc_id": "d99e4905-d1e3-4f39-8612-085066c2cab2"}, "94c6ee9f-4e04-4c42-98dd-b2d061bb205e": {"doc_hash": "38fa2000386e201c3cffd4314e59bc477ec0ef03f7ef7a08f21b09614cbba828", "ref_doc_id": "d99e4905-d1e3-4f39-8612-085066c2cab2"}, "b9a94d43-bb26-4ba7-aafb-36b72e3a4e64": {"doc_hash": "a4ed2eddb798814ff39f9984962468ba9218269a096a3d56619f88e637b5413d", "ref_doc_id": "d99e4905-d1e3-4f39-8612-085066c2cab2"}}, "docstore/ref_doc_info": {"d99e4905-d1e3-4f39-8612-085066c2cab2": {"node_ids": ["8fc8d06d-3cd7-4fbd-93bf-79483ed05364", "26c06b92-f546-4d58-8fb7-ec6a7ceb1907", "e6026233-b9d6-4eb5-a70c-97c3bfccc261", "3c8850f7-da4f-47ab-8ae7-b2bd04f7d906", "5e910915-491c-4ce2-b329-bf90e6c9bdd8", "e18418f0-7e8b-43df-afcf-a1c6d264a37d", "ecb9b5b5-7d63-429b-9adf-b1d5ac942df6", "b79e2f88-2fc7-422b-88a1-930437b2935b", "f544c1de-47bb-4f23-b841-7fec15ba7f08", "fd3cd8b0-caab-4d1c-948a-5aca52876d42", "75c04a00-25a7-415f-bb7b-1dd6841a314c", "4b7fa1f3-f048-48c4-ab14-323aa54a37aa", "473c2d07-10e2-48ba-ba3b-071db99b7490", "4d7015e1-01ad-445a-adf5-681e99ee4269", "94c6ee9f-4e04-4c42-98dd-b2d061bb205e", "b9a94d43-bb26-4ba7-aafb-36b72e3a4e64"], "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}}}}