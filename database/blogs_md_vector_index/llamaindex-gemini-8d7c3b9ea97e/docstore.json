{"docstore/data": {"902c38a7-bdf7-4f5c-9a1b-a15218a245a8": {"__data__": {"id_": "902c38a7-bdf7-4f5c-9a1b-a15218a245a8", "embedding": null, "metadata": {"filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "731f758c-bc9b-40e0-821d-a913e73150e8", "node_type": "4", "metadata": {"filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "hash": "ab296a557358963296718282d5289068d8d29711128dd40ebd31fd83489f4bf4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a1b686e-8f6d-4598-b004-aadf0decb88f", "node_type": "1", "metadata": {"Header_1": " Gemini Release and Support"}, "hash": "5b0f7bf509c4fe986f01f7c2c34ca5c866dac8899cd79580c69408f5328b06f2", "class_name": "RelatedNodeInfo"}}, "text": "(co-authored by Jerry Liu, Haotian Zhang, Logan Markewich, and Laurie Voss @\nLlamaIndex)\n\nToday is Google\u2019s [ public release ](https://blog.google/technology/ai/gemini-\napi-developers-cloud/) of its latest AI model, Gemini. We\u2019re excited to be a\nday 1 launch partner for Gemini, with support immediately available in\nLlamaIndex today!\n\nAs of 0.9.15, LlamaIndex offers full support for all currently **released and\nupcoming Gemini models** (Gemini Pro, Gemini Ultra). We support both a \u201ctext-\nonly\u201d Gemini variant with a text-in/text-out format as well as a multimodal\nvariant that takes in both text and images as input, and outputs text. We\u2019ve\nmade some fundamental multi-modal abstraction changes to support the Gemini\nmulti-modal interface, which allows users to input multiple images along with\ntext. Our Gemini integrations are also **feature-complete:** they support\n(non-streaming, streaming), (sync, async), and (text completion, chat message)\nformats \u2014 8 combinations in total.\n\nIn addition, we also support the brand-new **Semantic Retriever API,** which\nbundles storage, embedding models, retrieval, and LLM in a RAG pipeline. We\nshow you how it can be used on its own, or decomposed+bundled with LlamaIndex\ncomponents to create advanced RAG pipelines.\n\nHuge shoutout to the Google Labs and Semantic Retriever teams for helping us\nget setup with early access.\n\n  * **Google Labs:** Mark McDonald, Josh Gordon, Arthur Soroken \n  * **Semantic Retriever:** Lawrence Tsang, Cher Hu \n\nThe below sections contain a detailed walkthrough of both our brand-new Gemini\nand Semantic Retriever abstractions in LlamaIndex. If you don\u2019t want to read\nthat now, make sure you bookmark our detailed notebook guides below!\n\n  * [ Gemini (text-only) Guide ](https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/gemini.ipynb)\n  * [ Gemini (multi-modal) Guide ](https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/gemini.ipynb)\n  * [ Semantic Retriever Guide ](https://github.com/run-llama/llama_index/blob/main/docs/examples/managed/GoogleDemo.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2082, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a1b686e-8f6d-4598-b004-aadf0decb88f": {"__data__": {"id_": "0a1b686e-8f6d-4598-b004-aadf0decb88f", "embedding": null, "metadata": {"Header_1": " Gemini Release and Support", "filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "731f758c-bc9b-40e0-821d-a913e73150e8", "node_type": "4", "metadata": {"filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "hash": "ab296a557358963296718282d5289068d8d29711128dd40ebd31fd83489f4bf4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "902c38a7-bdf7-4f5c-9a1b-a15218a245a8", "node_type": "1", "metadata": {"filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "hash": "dee310dd15048863d0a05d8b4392bbbb47a6624067043e9c45d442f8d92cc9a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9005072-4060-44d8-a003-4a45e4d13a57", "node_type": "1", "metadata": {"Header_1": " Text Model"}, "hash": "1e919e9e518e5b5c31fbf8e05f2976c596b41157179eb823a36a42aa466bfe88", "class_name": "RelatedNodeInfo"}}, "text": "Gemini Release and Support\n\nThere\u2019s been a ton of press around Gemini, which boasts [ impressive\nperformance ](https://blog.google/technology/ai/google-gemini-ai/#performance)\nat a variety of benchmarks. The Ultra variants (which are not yet publicly\navailable) outperform GPT-4 on benchmarks from MMLU to Big-Bench Hard to math\nand coding tasks. Their [ multimodal demos\n](https://www.youtube.com/watch?v=K4pX1VAxaAI) demonstrate joint image/text\nunderstanding from domains like scientific paper understanding to literature\nreview.\n\nLet\u2019s walk through examples of using Gemini in LlamaIndex. We walk through\nboth the text model ( ` from llama_index.llms import Gemini ` ) as well as the\nmulti-modal model ( ` from llama_index.multi_modal_llms.gemini import\nGeminiMultiModal ` )", "mimetype": "text/plain", "start_char_idx": 2087, "end_char_idx": 2865, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e9005072-4060-44d8-a003-4a45e4d13a57": {"__data__": {"id_": "e9005072-4060-44d8-a003-4a45e4d13a57", "embedding": null, "metadata": {"Header_1": " Text Model", "filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "731f758c-bc9b-40e0-821d-a913e73150e8", "node_type": "4", "metadata": {"filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "hash": "ab296a557358963296718282d5289068d8d29711128dd40ebd31fd83489f4bf4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a1b686e-8f6d-4598-b004-aadf0decb88f", "node_type": "1", "metadata": {"Header_1": " Gemini Release and Support", "filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "hash": "4a44610ea244808128a746f9554d27fe018ff7a8c85f7be45f5289c7773b8555", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b600aa9e-f34e-4ed7-9832-4b52e5de886b", "node_type": "1", "metadata": {"Header_1": " Multi-modal Model"}, "hash": "3b7467bfb60517e4974163387db56682986cc3283ef455547f10e22532714b00", "class_name": "RelatedNodeInfo"}}, "text": "Text Model\n\n[ Full Notebook Guide Here ](https://github.com/run-\nllama/llama_index/blob/main/docs/examples/llm/gemini.ipynb)\n\nWe start with the text model. In the code snippet below, we show a bunch of\ndifferent configurations, from completion to chat to streaming to async.\n\n    \n    \n    from llama_index.llms import Gemini\n    \n    # completion\n    resp = Gemini().complete(\"Write a poem about a magic backpack\")\n    # chat\n    messages = [\n        ChatMessage(role=\"user\", content=\"Hello friend!\"),\n        ChatMessage(role=\"assistant\", content=\"Yarr what is shakin' matey?\"),\n        ChatMessage(\n            role=\"user\", content=\"Help me decide what to have for dinner.\"\n        ),\n    ]\n    resp = Gemini().chat(messages)\n    # streaming (completion)\n    llm = Gemini()\n    resp = llm.stream_complete(\n        \"The story of Sourcrust, the bread creature, is really interesting. It all started when...\"\n    )\n    # streaming (chat)\n    llm = Gemini()\n    messages = [\n        ChatMessage(role=\"user\", content=\"Hello friend!\"),\n        ChatMessage(role=\"assistant\", content=\"Yarr what is shakin' matey?\"),\n        ChatMessage(\n            role=\"user\", content=\"Help me decide what to have for dinner.\"\n        ),\n    ]\n    resp = llm.stream_chat(messages)\n    # async completion\n    resp = await llm.acomplete(\"Llamas are famous for \")\n    print(resp)\n    # async streaming (completion)\n    resp = await llm.astream_complete(\"Llamas are famous for \")\n    async for chunk in resp:\n        print(chunk.text, end=\"\")\n\nThe ` Gemini ` class of course has parameters that can be set. This includes `\nmodel_name ` , ` temperature ` , ` max_tokens ` , and ` generate_kwargs ` .\n\nAs an example, you can do:\n\n    \n    \n    llm = Gemini(model=\"models/gemini-ultra\")", "mimetype": "text/plain", "start_char_idx": 2870, "end_char_idx": 4629, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b600aa9e-f34e-4ed7-9832-4b52e5de886b": {"__data__": {"id_": "b600aa9e-f34e-4ed7-9832-4b52e5de886b", "embedding": null, "metadata": {"Header_1": " Multi-modal Model", "filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "731f758c-bc9b-40e0-821d-a913e73150e8", "node_type": "4", "metadata": {"filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "hash": "ab296a557358963296718282d5289068d8d29711128dd40ebd31fd83489f4bf4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9005072-4060-44d8-a003-4a45e4d13a57", "node_type": "1", "metadata": {"Header_1": " Text Model", "filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "hash": "9f26f69e2305035f9cda7b4b684806ef9c29689c6311b33e0ab37b87a6c47153", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f34bcfe2-dd91-4078-9c27-4d26fbcabbe3", "node_type": "1", "metadata": {"Header_1": " Multi-Modal Use Cases (Structured Outputs, RAG)"}, "hash": "1c2ce6dcb7c827810767cbb21a94fa455b741fada61423e6e710804228cf234a", "class_name": "RelatedNodeInfo"}}, "text": "Multi-modal Model\n\n[ Full Notebook Guide Here ](https://github.com/run-\nllama/llama_index/blob/main/docs/examples/multi_modal/gemini.ipynb)\n\nIn this notebook, we test out the ` gemini-pro-vision ` variant that features\n**multi-modal inputs.** It contains the following features:\n\n  * supports both ` complete ` and ` chat ` capabilities \n  * supports streaming and async \n  * Supports feeding in **multiple images** in addition to text in the completion endpoint \n  * Future work: multi-turn chat interleaving text and images is supported within our abstraction, but is not yet enabled for gemini-pro-vision. \n\nLet\u2019s walk through a concrete example. Let\u2019s say we are given a picture of the\n[ following scene ](https://storage.googleapis.com/generativeai-\ndownloads/data/scene.jpg) :\n\nScene from a street in New York City\n\nWe can then initialize our Gemini Vision model, and ask it a question:\n\u201cIdentify the city where this photo was taken\u201d:\n\n    \n    \n    from llama_index.multi_modal_llms.gemini import GeminiMultiModal\n    from llama_index.multi_modal_llms.generic_utils import (\n        load_image_urls,\n    )\n    \n    image_urls = [\n        \"&lt;https://storage.googleapis.com/generativeai-downloads/data/scene.jpg&gt;\",\n        # Add yours here!\n    ]\n    image_documents = load_image_urls(image_urls)\n    gemini_pro = GeminiMultiModal(model=\"models/gemini-pro\")\n    complete_response = gemini_pro.complete(\n        prompt=\"Identify the city where this photo was taken.\",\n        image_documents=image_documents,\n    )\n\nOur response is the following:\n\n    \n    \n    New York City\n\nWe can insert multiple images too. Here\u2019s an example with an image of Messi\nand the Colosseum.\n\n    \n    \n    image_urls = [\n        \"&lt;https://www.sportsnet.ca/wp-content/uploads/2023/11/CP1688996471-1040x572.jpg&gt;\",\n        \"&lt;https://res.cloudinary.com/hello-tickets/image/upload/c_limit,f_auto,q_auto,w_1920/v1640835927/o3pfl41q7m5bj8jardk0.jpg&gt;\",\n    ]\n    image_documents_1 = load_image_urls(image_urls)\n    response_multi = gemini_pro.complete(\n        prompt=\"is there any relationship between those images?\",\n        image_documents=image_documents_1,\n    )\n    print(response_multi)", "mimetype": "text/plain", "start_char_idx": 4634, "end_char_idx": 6821, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f34bcfe2-dd91-4078-9c27-4d26fbcabbe3": {"__data__": {"id_": "f34bcfe2-dd91-4078-9c27-4d26fbcabbe3", "embedding": null, "metadata": {"Header_1": " Multi-Modal Use Cases (Structured Outputs, RAG)", "filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "731f758c-bc9b-40e0-821d-a913e73150e8", "node_type": "4", "metadata": {"filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "hash": "ab296a557358963296718282d5289068d8d29711128dd40ebd31fd83489f4bf4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b600aa9e-f34e-4ed7-9832-4b52e5de886b", "node_type": "1", "metadata": {"Header_1": " Multi-modal Model", "filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "hash": "927c99f4070d2a03ba2fc6f22979470974b1d4ad287a6e009853f9e85c30d314", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4780e318-06b3-4978-8bba-984e133e9386", "node_type": "1", "metadata": {"Header_1": " Semantic Retriever"}, "hash": "bc234945eed1220ba914546e8bb4f0a64a859d371b159e4ab76639b9df288e45", "class_name": "RelatedNodeInfo"}}, "text": "Multi-Modal Use Cases (Structured Outputs, RAG)\n\n[ Full Notebook Guide Here ](https://github.com/run-\nllama/llama_index/blob/main/docs/examples/multi_modal/gemini.ipynb)\n\nWe\u2019ve created extensive resources about different [ multi-modal use cases\n](https://docs.llamaindex.ai/en/latest/use_cases/multimodal.html) , from\nstructured output extraction to RAG.\n\nThanks to Haotian Zhang, we have examples for **_both_ ** these use cases with\nGemini. Please see our extensive notebook guides for more details. In the\nmeantime here\u2019s the final results!\n\n**Structured Data Extraction with Gemini Pro Vision**\n\nScreenshot of a Google Maps Restaurant Listing\n\nOutput:\n\n    \n    \n    ('restaurant', 'La Mar by Gaston Acurio')\n    ('food', 'South American')\n    ('location', '500 Brickell Key Dr, Miami, FL 33131')\n    ('category', 'Restaurant')\n    ('hours', 'Open \u22c5 Closes 11 PM')\n    ('price', 4.0)\n    ('rating', 4)\n    ('review', '4.4 (2,104)')\n    ('description', 'Chic waterfront find offering Peruvian & fusion fare, plus bars for cocktails, ceviche & anticucho.')\n    ('nearby_tourist_places', 'Brickell Key Park')\n\n**Multi-Modal RAG**\n\nWe run our structured output extractor on multiple restaurant images, index\nthese nodes, and then ask a question \u201cRecommend a Orlando restaurant for me\nand its nearby tourist places\u201d\n\n    \n    \n    I recommend Mythos Restaurant in Orlando. It is an American restaurant located at 6000 Universal Blvd, Orlando, FL 32819, United States. It has a rating of 4 and a review score of 4.3 based on 2,115 reviews. The restaurant offers a mythic underwater-themed dining experience with a view of Universal Studios' Inland Sea. It is located near popular tourist places such as Universal's Islands of Adventure, Skull Island: Reign of Kong, The Wizarding World of Harry Potter, Jurassic Park River Adventure, Hollywood Rip Ride Rockit, and Universal Studios Florida.", "mimetype": "text/plain", "start_char_idx": 6826, "end_char_idx": 8715, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4780e318-06b3-4978-8bba-984e133e9386": {"__data__": {"id_": "4780e318-06b3-4978-8bba-984e133e9386", "embedding": null, "metadata": {"Header_1": " Semantic Retriever", "filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "731f758c-bc9b-40e0-821d-a913e73150e8", "node_type": "4", "metadata": {"filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "hash": "ab296a557358963296718282d5289068d8d29711128dd40ebd31fd83489f4bf4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f34bcfe2-dd91-4078-9c27-4d26fbcabbe3", "node_type": "1", "metadata": {"Header_1": " Multi-Modal Use Cases (Structured Outputs, RAG)", "filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "hash": "6765e74c5ef9c4e10b2efa7cf5ee2b7b71385f1d92c7580ea88df0ac02a5037e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39290a0d-4fd4-4b84-8a22-4ff66ff6a4d9", "node_type": "1", "metadata": {"Header_1": " Out of the Box Configuration"}, "hash": "33e20243fc0fee458eebc9a655a3eaba09ef3900a45bf0b9bb0b7bcc74787a13", "class_name": "RelatedNodeInfo"}}, "text": "Semantic Retriever\n\nThe Generative Language Semantic Retriever offers specialized embedding models\nfor high-quality retrieval, and a tuned LLM for producing grounded-output with\nsafety settings.\n\nIt can be used out of the box (with our ` GoogleIndex ` ) or decomposed into\ndifferent components ( ` GoogleVectorStore ` and ` GoogleTextSynthesizer ` )\nand combined with LlamaIndex abstractions!\n\nOur full [ semantic retriever notebook guide is here.\n](https://github.com/run-\nllama/llama_index/blob/main/docs/examples/managed/GoogleDemo.ipynb)", "mimetype": "text/plain", "start_char_idx": 8720, "end_char_idx": 9261, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "39290a0d-4fd4-4b84-8a22-4ff66ff6a4d9": {"__data__": {"id_": "39290a0d-4fd4-4b84-8a22-4ff66ff6a4d9", "embedding": null, "metadata": {"Header_1": " Out of the Box Configuration", "filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "731f758c-bc9b-40e0-821d-a913e73150e8", "node_type": "4", "metadata": {"filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "hash": "ab296a557358963296718282d5289068d8d29711128dd40ebd31fd83489f4bf4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4780e318-06b3-4978-8bba-984e133e9386", "node_type": "1", "metadata": {"Header_1": " Semantic Retriever", "filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "hash": "28d9c677b51b242b14538a567d254e07221e03625d5d38265a50268ec1f81ed0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "54eb1bd1-a1ff-4a7b-88db-abee493da227", "node_type": "1", "metadata": {"Header_1": " Decomposing into Different Components"}, "hash": "854304482f85239c855d12c7a0b908528efecba113d0e654e561819367b6b65b", "class_name": "RelatedNodeInfo"}}, "text": "Out of the Box Configuration\n\nYou can use it out of the box with very few lines of setup. Simply define the\nindex, insert nodes, and then get a query engine:\n\n    \n    \n    from llama_index.indices.managed.google.generativeai import GoogleIndex\n    \n    index = GoogleIndex.from_corpus(corpus_id=\"&lt;corpus_id&gt;\")\n    index.insert_documents(nodes)\n    query_engine = index.as_query_engine(...)\n    response = query_engine.query(\"&lt;query&gt;\")\n\nA cool feature here is that Google\u2019s query engine supports different\n**answering styles** as well as **safety settings** .\n\n**Answering Styles:**\n\n  * ABSTRACTIVE (succinct but abstract) \n  * EXTRACTIVE (brief and extractive) \n  * VERBOSE (extra details) \n\n**Safety Settings**\n\nYou can specify safety settings in the query engine, which let you define\nguardrails on whether the answer is explicit in different settings. See the `\n[ generative-ai-python ](https://github.com/google/generative-ai-python) `\nlibrary for more information.", "mimetype": "text/plain", "start_char_idx": 9266, "end_char_idx": 10249, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "54eb1bd1-a1ff-4a7b-88db-abee493da227": {"__data__": {"id_": "54eb1bd1-a1ff-4a7b-88db-abee493da227", "embedding": null, "metadata": {"Header_1": " Decomposing into Different Components", "filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "731f758c-bc9b-40e0-821d-a913e73150e8", "node_type": "4", "metadata": {"filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "hash": "ab296a557358963296718282d5289068d8d29711128dd40ebd31fd83489f4bf4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "39290a0d-4fd4-4b84-8a22-4ff66ff6a4d9", "node_type": "1", "metadata": {"Header_1": " Out of the Box Configuration", "filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "hash": "ae911c5267d7ca7d3901aa50c29073ade85de3322be2df01a520bb7b98adc78c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51945cf6-0985-4877-bbfa-5505fc0010c5", "node_type": "1", "metadata": {"Header_1": " Conclusion"}, "hash": "8da1e0e9633579aa53c24898bc743ae8e46b48375406e730c2ab9b2eb99365e2", "class_name": "RelatedNodeInfo"}}, "text": "Decomposing into Different Components\n\nThe ` GoogleIndex ` is built upon two components: a vector store ( `\nGoogleVectorStore ` ) and the response synthesizer ( ` GoogleTextSynthesizer `\n). You can use these as modular components in conjunction with LlamaIndex\nabstractions to create **advanced RAG** .\n\nThe notebook guide highlights three **advanced RAG use cases** :\n\n  * **Google Retriever + Reranking:** Use the Semantic Retriever to return relevant results, but then use our r [ eranking modules ](https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/root.html) to process/filter results before feeding it to response synthesis. \n  * **Multi-Query + Google Retriever:** Use our multi-query capabilities, like our ` MultiStepQueryEngine ` to break a complex question into multiple steps, and execute each step against the semantic retriever. \n  * **HyDE + Google Retriever:** HyDE is a popular query transformation technique that hallucinates an answer from a query, and uses the hallucinated answer for embedding lookup. Use that as a step before the retrieval step from the Semantic Retriever.", "mimetype": "text/plain", "start_char_idx": 10254, "end_char_idx": 11380, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51945cf6-0985-4877-bbfa-5505fc0010c5": {"__data__": {"id_": "51945cf6-0985-4877-bbfa-5505fc0010c5", "embedding": null, "metadata": {"Header_1": " Conclusion", "filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "731f758c-bc9b-40e0-821d-a913e73150e8", "node_type": "4", "metadata": {"filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "hash": "ab296a557358963296718282d5289068d8d29711128dd40ebd31fd83489f4bf4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54eb1bd1-a1ff-4a7b-88db-abee493da227", "node_type": "1", "metadata": {"Header_1": " Decomposing into Different Components", "filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "hash": "efb97c3e946b324c3952dbc9c77d534e956e7f4a147739725c858ad9ab649dc8", "class_name": "RelatedNodeInfo"}}, "text": "Conclusion\n\nThere\u2019s a **lot** in here, and even then the blog post doesn\u2019t even cover half\nof what we\u2019ve released today.\n\nPlease please make sure to check out our extensive notebook guides! Linking\nthe resources again below:\n\n  * [ Gemini (text-only) Guide ](https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/gemini.ipynb)\n  * [ Gemini (multi-modal) Guide ](https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/gemini.ipynb)\n  * [ Semantic Retriever Guide ](https://github.com/run-llama/llama_index/blob/main/docs/examples/managed/GoogleDemo.ipynb)\n\nAgain, huge shoutout to the Google teams and Haotian Zhang, Logan Markewich\nfrom the LlamaIndex team for putting together everything for this release.", "mimetype": "text/plain", "start_char_idx": 11386, "end_char_idx": 12129, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"902c38a7-bdf7-4f5c-9a1b-a15218a245a8": {"doc_hash": "dee310dd15048863d0a05d8b4392bbbb47a6624067043e9c45d442f8d92cc9a1", "ref_doc_id": "731f758c-bc9b-40e0-821d-a913e73150e8"}, "0a1b686e-8f6d-4598-b004-aadf0decb88f": {"doc_hash": "4a44610ea244808128a746f9554d27fe018ff7a8c85f7be45f5289c7773b8555", "ref_doc_id": "731f758c-bc9b-40e0-821d-a913e73150e8"}, "e9005072-4060-44d8-a003-4a45e4d13a57": {"doc_hash": "9f26f69e2305035f9cda7b4b684806ef9c29689c6311b33e0ab37b87a6c47153", "ref_doc_id": "731f758c-bc9b-40e0-821d-a913e73150e8"}, "b600aa9e-f34e-4ed7-9832-4b52e5de886b": {"doc_hash": "927c99f4070d2a03ba2fc6f22979470974b1d4ad287a6e009853f9e85c30d314", "ref_doc_id": "731f758c-bc9b-40e0-821d-a913e73150e8"}, "f34bcfe2-dd91-4078-9c27-4d26fbcabbe3": {"doc_hash": "6765e74c5ef9c4e10b2efa7cf5ee2b7b71385f1d92c7580ea88df0ac02a5037e", "ref_doc_id": "731f758c-bc9b-40e0-821d-a913e73150e8"}, "4780e318-06b3-4978-8bba-984e133e9386": {"doc_hash": "28d9c677b51b242b14538a567d254e07221e03625d5d38265a50268ec1f81ed0", "ref_doc_id": "731f758c-bc9b-40e0-821d-a913e73150e8"}, "39290a0d-4fd4-4b84-8a22-4ff66ff6a4d9": {"doc_hash": "ae911c5267d7ca7d3901aa50c29073ade85de3322be2df01a520bb7b98adc78c", "ref_doc_id": "731f758c-bc9b-40e0-821d-a913e73150e8"}, "54eb1bd1-a1ff-4a7b-88db-abee493da227": {"doc_hash": "efb97c3e946b324c3952dbc9c77d534e956e7f4a147739725c858ad9ab649dc8", "ref_doc_id": "731f758c-bc9b-40e0-821d-a913e73150e8"}, "51945cf6-0985-4877-bbfa-5505fc0010c5": {"doc_hash": "29932b37b4d289931409b8d2a4521402f5a6426d942f91340a5fe8c9368c7e50", "ref_doc_id": "731f758c-bc9b-40e0-821d-a913e73150e8"}}, "docstore/ref_doc_info": {"731f758c-bc9b-40e0-821d-a913e73150e8": {"node_ids": ["902c38a7-bdf7-4f5c-9a1b-a15218a245a8", "0a1b686e-8f6d-4598-b004-aadf0decb88f", "e9005072-4060-44d8-a003-4a45e4d13a57", "b600aa9e-f34e-4ed7-9832-4b52e5de886b", "f34bcfe2-dd91-4078-9c27-4d26fbcabbe3", "4780e318-06b3-4978-8bba-984e133e9386", "39290a0d-4fd4-4b84-8a22-4ff66ff6a4d9", "54eb1bd1-a1ff-4a7b-88db-abee493da227", "51945cf6-0985-4877-bbfa-5505fc0010c5"], "metadata": {"filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}}}}