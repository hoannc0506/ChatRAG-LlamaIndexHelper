{"docstore/data": {"e231b57b-2508-40e7-b07a-dd528af8481c": {"__data__": {"id_": "e231b57b-2508-40e7-b07a-dd528af8481c", "embedding": null, "metadata": {"filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46b26a21-dd92-4535-be8d-38f38503b5d1", "node_type": "4", "metadata": {"filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "hash": "d0187bf2ee948a24b388233c78e9ba256d6c7274030e1d4f63b92de761860d7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17da85bd-3a4f-4c15-bdca-866c0026a136", "node_type": "1", "metadata": {"Header_1": " Introduction"}, "hash": "65f4e3a26865f40ec028a878008ff9961b10768f2c9f1e3df8918281fbc26c65", "class_name": "RelatedNodeInfo"}}, "text": "(co-authored by Ofer Mendelevitch, head of Developer Relations at Vectara, and\nLogan Markewich, founding engineer at LlamaIndex)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 128, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17da85bd-3a4f-4c15-bdca-866c0026a136": {"__data__": {"id_": "17da85bd-3a4f-4c15-bdca-866c0026a136", "embedding": null, "metadata": {"Header_1": " Introduction", "filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46b26a21-dd92-4535-be8d-38f38503b5d1", "node_type": "4", "metadata": {"filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "hash": "d0187bf2ee948a24b388233c78e9ba256d6c7274030e1d4f63b92de761860d7a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e231b57b-2508-40e7-b07a-dd528af8481c", "node_type": "1", "metadata": {"filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "hash": "9b5035834219839d7df498d2de5def81b27c10887f20dfc17a2858563279fa04", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1cb9eb0-e452-4223-8d26-3a7f38c88f9b", "node_type": "1", "metadata": {"Header_1": " What is Vectara?"}, "hash": "dce1595f0bbeb180348db5d316af198e625efabb8d02004c4713574af69fbd5e", "class_name": "RelatedNodeInfo"}}, "text": "Introduction\n\n[ Vectara ](https://vectara.com) is a trusted GenAI platform. Exposing a set\nof easy to use [ APIs ](https://docs.vectara.com/docs/) , Vectara\u2019s platform\nreduces the complexity involved in developing [ Grounded Generation\n](https://vectara.com/grounded-generation-making-generative-ai-safe-\ntrustworthy-more-relevant/) (aka retrieval-augmented-generation) applications,\nand managing the LLM infrastructure that\u2019s required to deploy them at scale in\nproduction.\n\nToday we\u2019re happy to announce Vectara\u2019s integration with [ LlamaIndex\n](https://github.com/jerryjliu/llama_index) via a new type of Index: the\n_Managed Index_ . In this blog post, we\u2019ll dig deeper into how a [\nManagedIndex ](https://gpt-\nindex.readthedocs.io/en/stable/community/integrations/managed_indices.html)\nworks, and show examples of using Vectara as a Managed Index.", "mimetype": "text/plain", "start_char_idx": 133, "end_char_idx": 984, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1cb9eb0-e452-4223-8d26-3a7f38c88f9b": {"__data__": {"id_": "f1cb9eb0-e452-4223-8d26-3a7f38c88f9b", "embedding": null, "metadata": {"Header_1": " What is Vectara?", "filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46b26a21-dd92-4535-be8d-38f38503b5d1", "node_type": "4", "metadata": {"filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "hash": "d0187bf2ee948a24b388233c78e9ba256d6c7274030e1d4f63b92de761860d7a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17da85bd-3a4f-4c15-bdca-866c0026a136", "node_type": "1", "metadata": {"Header_1": " Introduction", "filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "hash": "50c2fd86336269f42f4b7396103abebe8ba60fe4ab72de5b7b16ce848fb6c470", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58023a41-5f56-40cb-ac0a-217df0accde4", "node_type": "1", "metadata": {"Header_1": " From VectorStoreIndex to ManagedIndex"}, "hash": "757457a60a99e348f6f708dafdfafc496ff6cb86d4e495797bb558af041deb1b", "class_name": "RelatedNodeInfo"}}, "text": "What is Vectara?\n\nVectara is an end-to-end platform that offers powerful generative AI\ncapabilities for developers, including:\n\n**Data processing.** Vectara supports various file types for ingestion\nincluding markdown, PDF, PPT, DOC, HTML and many others. At ingestion time,\nthe text is automatically extracted from the files, and chunked into\nsentences. Then a vector embedding is computed for each chunk, so you don\u2019t\nneed to call any additional service for that.\n\n**Vector and text storage.** Vectara hosts and manages the vector store (where\nthe document embeddings are stored) as well as the associated text. Developers\ndon\u2019t need to go through a long and expensive process of evaluation and choice\nof vector databases. Nor do they have to worry about setting up that Vector\ndatabase, managing it in their production environment, re-indexing, and many\nother DevOps considerations that become important when you scale your\napplication beyond a simple prototype.\n\n**Query flow.** When issuing a query, calculating the embedding vector for\nthat query and retrieving the resulting text segments (based on similarity\nmatch) is fully managed by Vectara. Vectara also provides a robust\nimplementation of hybrid search and re-ranking out of the box, which together\nwith a state of the art embedding model ensures the most relevant text\nsegments are returned in the retrieval step.\n\n**Security and Privacy.** Vectara\u2019s API is fully encrypted in transit and at\nrest, and supports customer-managed-keys (CMK). We never train on your data,\nso you can be sure your data is safe from privacy leaks.\n\n**Figure 1:** Vectara\u2019s API platform for \u201cGrounded Generation\u201d\n\nThe nice thing is that all this complexity is fully managed by Vectara, taking\na lot of the heavy lifting off of the developer\u2019s shoulders, so that they\ndon\u2019t have to specialize in the constantly evolving skills of large language\nmodels, embedding models, vector stores and MLOps.", "mimetype": "text/plain", "start_char_idx": 989, "end_char_idx": 2924, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58023a41-5f56-40cb-ac0a-217df0accde4": {"__data__": {"id_": "58023a41-5f56-40cb-ac0a-217df0accde4", "embedding": null, "metadata": {"Header_1": " From VectorStoreIndex to ManagedIndex", "filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46b26a21-dd92-4535-be8d-38f38503b5d1", "node_type": "4", "metadata": {"filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "hash": "d0187bf2ee948a24b388233c78e9ba256d6c7274030e1d4f63b92de761860d7a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1cb9eb0-e452-4223-8d26-3a7f38c88f9b", "node_type": "1", "metadata": {"Header_1": " What is Vectara?", "filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "hash": "9803f923e251d6d2f3d46cb07081106d3d56e89504ca4dad989b7e9f6175d925", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e176518-859e-4ce3-a460-c06ca1625516", "node_type": "1", "metadata": {"Header_1": " How does the VectaraIndex work?"}, "hash": "6ca3c1a4fbe07d25909270a8eda62990d6b08fcdbf5a3f9a0ff2c865460d2530", "class_name": "RelatedNodeInfo"}}, "text": "From VectorStoreIndex to ManagedIndex\n\nLlamaIndex is a data framework for building LLM applications. It provides a\nset of composable modules for users to define a data pipeline for their\napplication. This consists of data loaders, text splitters, metadata\nextractors, and vector store integrations.\n\nA popular abstraction that users use is the VectorStoreIndex, providing\nintegrations with different vector databases.\n\nHowever, a challenge here is that users still need to carefully define how to\nload data, parse it, as well as choose an embedding model and a vector DB to\nuse. Since Vectara abstracts away this complexity, the Vectara and LlamaIndex\nteams jointly came up with a new abstraction: The ManagedIndex.\n\nAs shown in figure 2, when ingesting data into a VectorStoreIndex, data is\nprocessed locally taking advantage of multiple components like Data Connectors\nand Node parsers.\n\n**Figure 2:** typical flow of document processing in LlamaIndex for a\nVectorStoreIndex\n\nWith Vectara (figure 3), this whole flow is replaced by a single \u201cindexing\u201d\nAPI call , and all this processing is instead performed in the backend by the\nVectara platform.\n\n**Figure 3:** pre-processing with the VectaraIndex simplifies the complex\ningest flow to a single step.", "mimetype": "text/plain", "start_char_idx": 2929, "end_char_idx": 4183, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e176518-859e-4ce3-a460-c06ca1625516": {"__data__": {"id_": "3e176518-859e-4ce3-a460-c06ca1625516", "embedding": null, "metadata": {"Header_1": " How does the VectaraIndex work?", "filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46b26a21-dd92-4535-be8d-38f38503b5d1", "node_type": "4", "metadata": {"filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "hash": "d0187bf2ee948a24b388233c78e9ba256d6c7274030e1d4f63b92de761860d7a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58023a41-5f56-40cb-ac0a-217df0accde4", "node_type": "1", "metadata": {"Header_1": " From VectorStoreIndex to ManagedIndex", "filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "hash": "29e0b0a3c6aa27031c4db24572661b5d4c52879bcdcf4ec6235eb962ba543116", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ed09e67-a62e-408e-b60c-2564dac0c9ff", "node_type": "1", "metadata": {"Header_1": " Why Use VectaraIndex with LlamaIndex?"}, "hash": "09aa983d1ca3d03aabd2dfbf008f6d53ebdd55acd5ffdd9029aa91eaf845cf60", "class_name": "RelatedNodeInfo"}}, "text": "How does the VectaraIndex work?\n\nLet\u2019s take a look at a simple question-answering example using VectaraIndex,\nin this case asking questions from one of Paul Graham\u2019s Essays.\n\n**Step 1: Setup your Vectara account and Index**\n\nTo get started, follow our [ quickstart\n](https://docs.vectara.com/docs/quickstart) guide: [ signup\n](https://console.vectara.com/signup) for a free Vectara account, create a\ncorpus (index), and generate your API key.\n\nThen setup your Vectara customer_id, corpus_id and api_key as environment\nvariables, so that the VectaraIndex can access those easily, for example:\n\n    \n    \n    VECTARA_CUSTOMER_ID=<YOUR_CUSTOMER_ID>\n    VECTARA_CORPUS_ID=<YOUR_CORPUS_ID>\n    VECTARA_API_KEY=\"zwt_RbZfGT\u2026\"\n\n**Step 2: Create a VectaraIndex instance with LlamaIndex**\n\nBuilding the Vectara Index is extremely simple:\n\n    \n    \n    from llama_index import SimpleDirectoryReader\n    from llama_index.indices import VectaraIndex\n    From pprint Import pprint\n    \n    documents = SimpleDirectoryReader(\"paul_graham\").load_data()\n    index = VectaraIndex.from_documents(documents)\n\nHere we load Paul Graham\u2019s Essay using LlamaIndex\u2019s SimpleDirectoryReader into\na single document. The from_documents() constructor is then used to generate\nthe VectaraIndex instance.\n\nUnlike the common flow that uses LlamaIndex tools like data connectors,\nparsers and embedding models to process the input data, with VectaraIndex the\ndocuments are sent directly to Vectara via the [ Indexing API\n](https://docs.vectara.com/docs/api-reference/indexing-apis/indexing) .\nVectara\u2019s platform then processes, chunks, encodes and stores the text and\nembeddings into a Vectara corpus, making it available instantly for querying.\n\n**Step 3: Query**\n\nAfter the data is fully ingested, you can take advantage of the rich set of\nquery constructs built into LlamaIndex. For example let\u2019s use the index to\nretrieve the top-k most relevant nodes:\n\n    \n    \n    retriever = index.as_retriever(similarity_top_k=7)\n    # docs should contain the 7 most relevant documents for the query\n    docs = retriever.retrieve(\u201cWhat is the IBM 1401?\u201d)\n    pprint(docs[0].node.text)\n\n> (\u2018My stories were awful. They had hardly any plot, just characters with\n> strong feelings, which I imagined made them deep. The first programs I tried\n> writing were on the IBM 1401 that our school district used for what was then\n> called \u201cdata processing.\u201d This was in 9th grade, so I was 13 or 14. The\n> school district\u2019s 1401 happened to be in the basement of our junior high\n> school, and my friend Rich Draves and I got permission to use it.\u2019)\n\nHere we printed out the top matching Node given the query \u201cwhat is the IBM\n1401?\u201d This in turn results in a call to Vectara\u2019s [ Search API\n](https://docs.vectara.com/docs/api-reference/search-apis/search) that returns\nthe top-k matching document segments.\n\nThose are transformed into NodeWithScore objects and thus can be used as usual\nwith the rest of the LlamaIndex querying tools. For example we can use\nLlamaIndex\u2019s query_engine() to convert the retrieved matching document\nsegments (nodes) into a comprehensive response to our question:\n\n    \n    \n    # Get an answer to the query based on the content of the essay\n    response = index.as_query_engine().query(\"What can the 1401 do?\")\n    print(response)\n\n> The 1401 was used for \u201cdata processing\u201d and could load programs into memory\n> and run them. It had a card reader, printer, CPU, disk drives, and used an\n> early version of Fortran as the programming language. The only form of input\n> to programs was data stored on punched cards.", "mimetype": "text/plain", "start_char_idx": 4188, "end_char_idx": 7776, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ed09e67-a62e-408e-b60c-2564dac0c9ff": {"__data__": {"id_": "9ed09e67-a62e-408e-b60c-2564dac0c9ff", "embedding": null, "metadata": {"Header_1": " Why Use VectaraIndex with LlamaIndex?", "filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46b26a21-dd92-4535-be8d-38f38503b5d1", "node_type": "4", "metadata": {"filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "hash": "d0187bf2ee948a24b388233c78e9ba256d6c7274030e1d4f63b92de761860d7a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e176518-859e-4ce3-a460-c06ca1625516", "node_type": "1", "metadata": {"Header_1": " How does the VectaraIndex work?", "filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "hash": "064e0fd77fcf2fac1e6dfd0ea4f825645fd34f2a35fe392e88dcc079d79e357c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7ee8854-902b-4c0c-8b6a-8df10f91e9f3", "node_type": "1", "metadata": {"Header_1": " Summary"}, "hash": "879af79ff68880ec0976a15d5d2d161af5a9e416a61430d97bef5099693f3b9c", "class_name": "RelatedNodeInfo"}}, "text": "Why Use VectaraIndex with LlamaIndex?\n\nBy adding the concept of a \u201cManaged Index\u201d and the VectaraIndex to LlamaIndex,\nusers can continue to take advantage of the tools and capabilities offered by\nthe LlamaIndex library while integrating with a generative AI platform like\nVectara.\n\nRetrievers and Query Engines are just the tip of the iceberg. Using a managed\nindex with Vectara, developers have full access to advanced utilities like\nrouters, advanced query engines, data agents, chat engines, and more! Being\nable to retrieve context using Vectara empowers developers to build these\ncomplex applications using LlamaIndex components.\n\nFor example, in the following code we use the chat engine in LlamaIndex to\nquickly create a chat interaction using our VectaraIndex:\n\n    \n    \n    chat = index.as_chat_engine(chat_mode='context')\n    res = chat.chat(\"When did the author learn Lisp?\")\n    print(res.response)\n\n> \u201cThe author learned Lisp in college.\u201d\n\nA follow up question retains the chat history for context, as you might\nexpect:\n\n    \n    \n    chat.chat(\"and was it helpful for projects?\").response\n\n> \u201cYes, learning Lisp was helpful for the author\u2019s projects. They used Lisp in\n> both Viaweb and Y Combinator, indicating its usefulness in their work.\u201d\n    \n    \n    chat.chat(\"what was a distinctive characteristic of that programming language?\").response\n\n> \u201cA distinctive characteristic of Lisp is that its core is a language defined\n> by writing an interpreter in itself. It was originally designed as a formal\n> model of computation and an alternative to the Turing machine. This self-\n> referential nature of Lisp sets it apart from other programming languages.\u201d\n\nFor more information on how to use chat-engines, check out the [ documentation\n](https://gpt-\nindex.readthedocs.io/en/latest/core_modules/query_modules/chat_engines/usage_pattern.html)\n, and for more information on other query capabilities with LlamaIndex, check\nout the full documentation [ here ](https://gpt-\nindex.readthedocs.io/en/latest/index.html) .", "mimetype": "text/plain", "start_char_idx": 7781, "end_char_idx": 9812, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7ee8854-902b-4c0c-8b6a-8df10f91e9f3": {"__data__": {"id_": "c7ee8854-902b-4c0c-8b6a-8df10f91e9f3", "embedding": null, "metadata": {"Header_1": " Summary", "filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46b26a21-dd92-4535-be8d-38f38503b5d1", "node_type": "4", "metadata": {"filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "hash": "d0187bf2ee948a24b388233c78e9ba256d6c7274030e1d4f63b92de761860d7a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ed09e67-a62e-408e-b60c-2564dac0c9ff", "node_type": "1", "metadata": {"Header_1": " Why Use VectaraIndex with LlamaIndex?", "filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "hash": "c6b8f4445e2113a722829d347fda3c36af72d1c4cc14daa6ed88fc3b2e796a30", "class_name": "RelatedNodeInfo"}}, "text": "Summary\n\nLlamaIndex makes it super easy to populate VectaraIndex with content from any\ndocument or data source, while utilizing the Vectara service for managing the\ndocument processing, chunking, embedding and making all of this data available\nfor advanced retrieval in query time using the LlamaIndex library.\n\nVectaraIndex is based on the new LlamaIndex Managed Index abstraction, which\nbetter supports GenAI platforms like Vectara, and enables additional vendors\nwho also provide end-to-end platforms to join in.\n\nTo get started with Vectara and LlamaIndex you can follow the Vectara\nquickstart [ guide ](https://docs.vectara.com/docs/quickstart) to setup your\naccount, and the examples above with your own data.", "mimetype": "text/plain", "start_char_idx": 9817, "end_char_idx": 10532, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"e231b57b-2508-40e7-b07a-dd528af8481c": {"doc_hash": "9b5035834219839d7df498d2de5def81b27c10887f20dfc17a2858563279fa04", "ref_doc_id": "46b26a21-dd92-4535-be8d-38f38503b5d1"}, "17da85bd-3a4f-4c15-bdca-866c0026a136": {"doc_hash": "50c2fd86336269f42f4b7396103abebe8ba60fe4ab72de5b7b16ce848fb6c470", "ref_doc_id": "46b26a21-dd92-4535-be8d-38f38503b5d1"}, "f1cb9eb0-e452-4223-8d26-3a7f38c88f9b": {"doc_hash": "9803f923e251d6d2f3d46cb07081106d3d56e89504ca4dad989b7e9f6175d925", "ref_doc_id": "46b26a21-dd92-4535-be8d-38f38503b5d1"}, "58023a41-5f56-40cb-ac0a-217df0accde4": {"doc_hash": "29e0b0a3c6aa27031c4db24572661b5d4c52879bcdcf4ec6235eb962ba543116", "ref_doc_id": "46b26a21-dd92-4535-be8d-38f38503b5d1"}, "3e176518-859e-4ce3-a460-c06ca1625516": {"doc_hash": "064e0fd77fcf2fac1e6dfd0ea4f825645fd34f2a35fe392e88dcc079d79e357c", "ref_doc_id": "46b26a21-dd92-4535-be8d-38f38503b5d1"}, "9ed09e67-a62e-408e-b60c-2564dac0c9ff": {"doc_hash": "c6b8f4445e2113a722829d347fda3c36af72d1c4cc14daa6ed88fc3b2e796a30", "ref_doc_id": "46b26a21-dd92-4535-be8d-38f38503b5d1"}, "c7ee8854-902b-4c0c-8b6a-8df10f91e9f3": {"doc_hash": "e9986139b0346a69fa99cdd35be35b0882d52f6b0d0067f6df86bbb545c7768c", "ref_doc_id": "46b26a21-dd92-4535-be8d-38f38503b5d1"}}, "docstore/ref_doc_info": {"46b26a21-dd92-4535-be8d-38f38503b5d1": {"node_ids": ["e231b57b-2508-40e7-b07a-dd528af8481c", "17da85bd-3a4f-4c15-bdca-866c0026a136", "f1cb9eb0-e452-4223-8d26-3a7f38c88f9b", "58023a41-5f56-40cb-ac0a-217df0accde4", "3e176518-859e-4ce3-a460-c06ca1625516", "9ed09e67-a62e-408e-b60c-2564dac0c9ff", "c7ee8854-902b-4c0c-8b6a-8df10f91e9f3"], "metadata": {"filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}}}}