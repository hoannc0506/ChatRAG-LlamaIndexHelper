{"docstore/data": {"a9048e95-98b1-4f7e-a0a7-7f045a634bd0": {"__data__": {"id_": "a9048e95-98b1-4f7e-a0a7-7f045a634bd0", "embedding": null, "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc0b986c-3d16-4c1e-a7f7-2d4b7f122cd3", "node_type": "4", "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "a859d48892dc6e360c88b587077ec9d9f4e21bc29f58b3da751982d8adfd78d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0828224-9afa-4591-8c46-5b878995759e", "node_type": "1", "metadata": {"Header_1": " Re-ranking is an intuitive concept."}, "hash": "4b918e4aa642612d6ba9c91c821f8dbaee13f233af878c135a5427162d88fc26", "class_name": "RelatedNodeInfo"}}, "text": "In  **_the RAG, after the retrieval phase, it\u2019s necessary to perform Re-\nranking + Fine-Grained Prompt Compression + Subsequence Recovery to enhance\nLLM\u2019s perception of key information, which is LongLLMLingua._ **\n\n> TL;DR: While Retrieval-Augmented Generation (RAG) is highly effective in\n> various scenarios, it still has drawbacks such as 1) Performance drop, like\n> the \u201cLost in the middle\u201d issue, 2) High costs, both financially and in terms\n> of latency, and 3) Context windows limitation. LongLLMLingua offers a\n> solution to these problems in RAG or Long Context scenarios via prompt\n> compression. It can boost accuracy by as much as 21.4% while only using \u00bc of\n> the tokens. In long context situations, it can save $28 for every 1000\n> examples.\n\nSee real-world cases on the [ **project page** ](https://llmlingua.com/) .\n\nWe previously wrote a [ blog post ](https://wyydsb.xin/NLP/LLMLingua_en.html)\nintroducing the design of **LLMLingua** , which started from the perspective\nof **designing a special language for LLMs** . This time, our focus will be on\nthe scenarios involving RAG.\n\nRetrieval-Augmented Generation is currently the most reliable and proven\ntechnique for creating AI-agents that are grounded on any specific collection\nof text. Frameworks like **LlamaIndex** provide comprehensive RAG solutions to\nhelp users utilize specialized data in LLMs more conveniently.\n\nA common misunderstanding is that retrieving as many relevant documents as\npossible during the RAG process and stitching them together to form a long\nretrieved prompt is beneficial, especially as more and more LLMs support\nlonger context windows. However, this method can introduce more **noise** into\nthe prompt and **weaken** **the LLM\u2019s perception of key information** ,\nleading to issues such as \u2018 **lost in the middle** \u2019[1].\n\nThese issues become more apparent in real-world scenarios involving RAG.\nBetter retrieval mechanisms can introduce higher quality noise documents,\nwhich can more easily lead to a drop in performance.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2022, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0828224-9afa-4591-8c46-5b878995759e": {"__data__": {"id_": "c0828224-9afa-4591-8c46-5b878995759e", "embedding": null, "metadata": {"Header_1": " Re-ranking is an intuitive concept.", "filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc0b986c-3d16-4c1e-a7f7-2d4b7f122cd3", "node_type": "4", "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "a859d48892dc6e360c88b587077ec9d9f4e21bc29f58b3da751982d8adfd78d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a9048e95-98b1-4f7e-a0a7-7f045a634bd0", "node_type": "1", "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "6b8ec13c1e85ff2d54169df303bacf9187283bb0c694974db95a2dbf8056b39c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee1b0be9-fe1e-4862-8c29-6ec73db54309", "node_type": "1", "metadata": {"Header_1": " Compress unrelated and unimportant information"}, "hash": "9912d8488835bd51c7adb8a41ccd5a81d9e653b4d1b2936458a0ff3885e7cfab", "class_name": "RelatedNodeInfo"}}, "text": "Re-ranking is an intuitive concept.\n\nOne intuitive idea is to reposition the most relevant information to the sides\nof the prompt through re-ranking. This concept of re-ranking has already been\nimplemented in frameworks such as [ **LlamaIndex** ](https://github.com/run-\nllama/llama_index/blob/main/llama_index/indices/postprocessor/sbert_rerank.py)\nand [ **LangChain**\n](https://python.langchain.com/docs/modules/data_connection/document_transformers/post_retrieval/long_context_reorder)\n.\n\nHowever, according to our experiments, it\u2019s difficult for an embedding model\nto serve as a \u2018good\u2019 re-ranker. The underlying reason is the lack of an\ninteraction process between the query and the document. The dual-tower\nstructure of embeddings is not suitable for re-ranking in general scenarios,\nalthough it may be effective after fine-tuning.\n\nUsing LLMs directly as a re-ranker may also lead to misjudgments due to\n**hallucinations** . Recently, some re-ranking models have been extended from\nembedding models, such as [ **bge-rerank** ](https://huggingface.co/BAAI/bge-\nreranker-large) . However, such re-ranking models generally have context\nwindow limitations.\n\nTo address the above issues, we propose a Question-aware Coarse-Grained prompt\ncompression method. This method evaluates the relevance between the context\nand the question based on the perplexity corresponding to the question.\n\nTo mitigate the hallucination problem in smaller LLMs, we append a restrictive\nstatement, specifically \u201c _We can get the answer to this question in the given\ndocuments_ \u201d, after the question to limit the latent space caused by related\nhallucinations.\n\n**Figure 1.** The accuracy of different methods for ranking documents from\nMulti-documemnt QA dataset, which increases from top to bottom in terms of\nRecall@1. Different colors represent different types of methods. Among them,\nyellow represents traditional relevance methods, green signifies embedding-\nbased methods, and red denotes rerank-based methods. You can find the script\nin [ this link\n](https://github.com/microsoft/LLMLingua/blob/main/examples/Retrieval.ipynb) .\n\nResults show that this approach significantly outperforms both embedding\nmodels and re-ranking models. We\u2019ve added some recently released embedding and\nreranking models. As you can see, the performance of **bge-rerank-large** is\nvery close to that of **LongLLMLingua** . Reranking models generally perform\nbetter than embedding models. Currently, **Jina** is the best performing\nmethod among the embedding models.", "mimetype": "text/plain", "start_char_idx": 2027, "end_char_idx": 4555, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee1b0be9-fe1e-4862-8c29-6ec73db54309": {"__data__": {"id_": "ee1b0be9-fe1e-4862-8c29-6ec73db54309", "embedding": null, "metadata": {"Header_1": " Compress unrelated and unimportant information", "filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc0b986c-3d16-4c1e-a7f7-2d4b7f122cd3", "node_type": "4", "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "a859d48892dc6e360c88b587077ec9d9f4e21bc29f58b3da751982d8adfd78d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0828224-9afa-4591-8c46-5b878995759e", "node_type": "1", "metadata": {"Header_1": " Re-ranking is an intuitive concept.", "filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "5a379c588d5b5e0269ff16279f45c95a1087c849384ab8626dde5de804f9e2c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c8a3f08-bd9b-419f-9ddf-79c7101fb95d", "node_type": "1", "metadata": {"Header_1": " Question-aware Fine-grained Prompt Compression"}, "hash": "a28fc9a2b25ab1eb668dd8323c0b74cd34f4ce72e2bec37386d7fca557bbb696", "class_name": "RelatedNodeInfo"}}, "text": "Compress unrelated and unimportant information\n\nBesides recalling as many relevant documents as possible, another approach is\nto **compress** irrelevant or unimportant contexts as much as possible.\n\nPrevious work on long context has focused on how to extend LLMs to support\nlonger context windows. However, almost no work has explored whether this can\nactually improve the performance of downstream tasks. Some previous studies\nhave shown that the presence of more noise in the prompt, as well as the\nposition of key information in the prompt, can affect the performance of LLMs.\n\nFrom the perspective of prompt compression, Selective Context[2] and\nLLMLingua[3] estimate the importance of elements by using a small language\nmodel to calculate the mutual information or perplexity of the prompt.\nHowever, in scenarios like RAG or long context scenarios, this method can\neasily lose key information because it cannot perceive the question\ninformation.\n\nIn recent submissions to ICLR\u201924, there have been some similar practices. For\nexample, Recomp[4] reduces the use of tokens in RAG scenarios by jointly\ntraining compressors of two different granularities. RAG in Long Context[5]\ndecomposes the long context into a series of chunks and uses retrieval methods\nfor compression, which is actually the retrieval-based method implemented in\nthe LongLLMLingua paper. In addition, Walking Down the Memory Maze[6] also\ndesigned a hierarchical summarization tree to enhance the LLM\u2019s perception of\nkey information.", "mimetype": "text/plain", "start_char_idx": 4560, "end_char_idx": 6064, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c8a3f08-bd9b-419f-9ddf-79c7101fb95d": {"__data__": {"id_": "6c8a3f08-bd9b-419f-9ddf-79c7101fb95d", "embedding": null, "metadata": {"Header_1": " Question-aware Fine-grained Prompt Compression", "filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc0b986c-3d16-4c1e-a7f7-2d4b7f122cd3", "node_type": "4", "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "a859d48892dc6e360c88b587077ec9d9f4e21bc29f58b3da751982d8adfd78d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee1b0be9-fe1e-4862-8c29-6ec73db54309", "node_type": "1", "metadata": {"Header_1": " Compress unrelated and unimportant information", "filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "7687cbf824c6f26c0387245796194d0009d9dc884b488cef281889a022121f61", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c247c1cd-97b5-4139-b3d0-da0848cdfc2e", "node_type": "1", "metadata": {"Header_1": " How to reduce the loss in the middle"}, "hash": "0d50a6a01916342e75fac239389e156dfa90dd47f051636d2617fcdfd861d3b4", "class_name": "RelatedNodeInfo"}}, "text": "Question-aware Fine-grained Prompt Compression\n\nIn order to make token-level prompt compression also perceive the information\nof the question, we propose a **contrastive perplexity** , which compares the\ndifference between the perplexity distribution corresponding to the document\nand the perplexity distribution corresponding to the document with the\nquestion.\n\nAn intuitive feeling is that when the question serves as context, the\nperplexity corresponding to the relevant tokens in the document will decrease.\nThis decrease in magnitude represents the importance of the tokens in the\ndocument relative to the question.\n\n**Figure 3.** Comparison between perplexities and contrastive perplexities of\ntokens in the prompt from Multi-documemnt QA dataset. The document with the\nground truth is located on the left side of the dashed line.\n\nFigure 3 shows the distribution difference in extracting key tokens between\nperplexity and contrastive perplexity.", "mimetype": "text/plain", "start_char_idx": 6069, "end_char_idx": 7021, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c247c1cd-97b5-4139-b3d0-da0848cdfc2e": {"__data__": {"id_": "c247c1cd-97b5-4139-b3d0-da0848cdfc2e", "embedding": null, "metadata": {"Header_1": " How to reduce the loss in the middle", "filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc0b986c-3d16-4c1e-a7f7-2d4b7f122cd3", "node_type": "4", "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "a859d48892dc6e360c88b587077ec9d9f4e21bc29f58b3da751982d8adfd78d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c8a3f08-bd9b-419f-9ddf-79c7101fb95d", "node_type": "1", "metadata": {"Header_1": " Question-aware Fine-grained Prompt Compression", "filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "925a6ca77a1cd8c0ed22fbe289406249c716e2e748f1d9df7032fffbfc5c229e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2e7c43c-beb5-495c-9285-456343c57813", "node_type": "1", "metadata": {"Header_1": " How to achieve adaptive granular control during compression?"}, "hash": "4ff2b0ef7e873bfa3a504d7bcfcd63c9dfb9c42c0377cd9dddda6d9c44e875cb", "class_name": "RelatedNodeInfo"}}, "text": "How to reduce the loss in the middle\n\nSince Coarse-grained Prompt compression far exceeds other retrieval methods in\nterms of accuracy, it is a very natural idea to use this ranking information\nto rearrange the documents that are more related to the question to the\nbeginning and end of the prompt. However, through our testing, we found that\nrearranging to the beginning of the prompt is more effective than evenly\ndistributing at both ends. So, we choose to reorder the most related document\nto the beginning of the prompt.", "mimetype": "text/plain", "start_char_idx": 7026, "end_char_idx": 7551, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2e7c43c-beb5-495c-9285-456343c57813": {"__data__": {"id_": "b2e7c43c-beb5-495c-9285-456343c57813", "embedding": null, "metadata": {"Header_1": " How to achieve adaptive granular control during compression?", "filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc0b986c-3d16-4c1e-a7f7-2d4b7f122cd3", "node_type": "4", "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "a859d48892dc6e360c88b587077ec9d9f4e21bc29f58b3da751982d8adfd78d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c247c1cd-97b5-4139-b3d0-da0848cdfc2e", "node_type": "1", "metadata": {"Header_1": " How to reduce the loss in the middle", "filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "8e1b5cc0aa0060813651c1ecd70060ca7f6da33a67283d1ea0dd6b51e70fa139", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8c2a0b7-5957-4181-98e2-0ee0b57de08d", "node_type": "1", "metadata": {"Header_1": " How to improve the integrity of key information?"}, "hash": "7df40f683bf568cb80f37116f5695172e0fcfaa80b8d69dc98f5f9423cb78499", "class_name": "RelatedNodeInfo"}}, "text": "How to achieve adaptive granular control during compression?\n\nIn order to better use the information from the two grained compressions, in\nthe fine-grained prompt compression, we dynamically allocate different\ncompression ratios to different documents based on the rank information\nobtained from the coarse-grained compression, thereby preserving more\nimportant information from important documents.", "mimetype": "text/plain", "start_char_idx": 7556, "end_char_idx": 7955, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8c2a0b7-5957-4181-98e2-0ee0b57de08d": {"__data__": {"id_": "c8c2a0b7-5957-4181-98e2-0ee0b57de08d", "embedding": null, "metadata": {"Header_1": " How to improve the integrity of key information?", "filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc0b986c-3d16-4c1e-a7f7-2d4b7f122cd3", "node_type": "4", "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "a859d48892dc6e360c88b587077ec9d9f4e21bc29f58b3da751982d8adfd78d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2e7c43c-beb5-495c-9285-456343c57813", "node_type": "1", "metadata": {"Header_1": " How to achieve adaptive granular control during compression?", "filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "8cff76efe067e7be5340c9474fe6151aa2bb451b24ca481627ae862b962f0bcd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8450cf4d-3198-4636-a62f-8f4de1461078", "node_type": "1", "metadata": {"Header_1": " Experiments"}, "hash": "bf161df8f40af27867d83ec4d947c843fb027d732bd3d34d59ad960479512c63", "class_name": "RelatedNodeInfo"}}, "text": "How to improve the integrity of key information?\n\nSince LongLLMLingua is a token-level prompt compression, it will inevitably\ndelete some tokens of the word, which may result in some retrieval-related\ntasks not getting complete results. But this can actually be recovered through\na simple subsequence matching method. Specifically, there is a subsequence\nrelationship between the original prompt, compressed prompt and response. By\nestablishing the mapping relationship between the response subsequence that\nappears in the compressed prompt and the subsequence of the original prompt,\nthe original prompt content can be effectively recovered.", "mimetype": "text/plain", "start_char_idx": 7960, "end_char_idx": 8602, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8450cf4d-3198-4636-a62f-8f4de1461078": {"__data__": {"id_": "8450cf4d-3198-4636-a62f-8f4de1461078", "embedding": null, "metadata": {"Header_1": " Experiments", "filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc0b986c-3d16-4c1e-a7f7-2d4b7f122cd3", "node_type": "4", "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "a859d48892dc6e360c88b587077ec9d9f4e21bc29f58b3da751982d8adfd78d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8c2a0b7-5957-4181-98e2-0ee0b57de08d", "node_type": "1", "metadata": {"Header_1": " How to improve the integrity of key information?", "filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "5e2b5a2795ba71123c46b20381cf1fb7710c942698569cafcf2a457c0dc81b4f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b22d1284-e2f3-4354-b697-d7b93ad90b16", "node_type": "1", "metadata": {"Header_1": " Used in LlamaIndex"}, "hash": "57b89882c71421843c71bc3b8c412bd34134673ef1f9e38f65d74043896c05e0", "class_name": "RelatedNodeInfo"}}, "text": "Experiments\n\nTo evaluate the effectiveness of LongLLMLingua, we conducted detailed tests in\nMulti-document QA (RAG) and two long Context benchmarks. Particularly, the\ndataset chosen for Multi-document QA is very close to the actual RAG scenario\n(e.g. Bing Chat), where **Contriever** (one of the state-of-the-art retrieval\nsystems) is used to recall 20 relevant documents including one ground-truth.\nThe original documents have a high semantic relevance with the question.\n\nAs can be seen, compared to Retrieval-based methods and compression-based\nmethods, LongLLMLingua improves performance more in the RAG scenario, and can\nincrease up to 21.4 points at a 4x compression rate, avoiding the original\n\u201clost in the middle\u201d situation.\n\nThe results of the two benchmarks, LongBench and ZeroScrolls, also reached\nsimilar conclusions. LongLLMLingua is better at retaining key information\nrelated to the question in long context scenarios.\n\nBesides, LongLLMLingua is very efficient and can speed up the end-to-end\ninference process.", "mimetype": "text/plain", "start_char_idx": 8607, "end_char_idx": 9633, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b22d1284-e2f3-4354-b697-d7b93ad90b16": {"__data__": {"id_": "b22d1284-e2f3-4354-b697-d7b93ad90b16", "embedding": null, "metadata": {"Header_1": " Used in LlamaIndex", "filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc0b986c-3d16-4c1e-a7f7-2d4b7f122cd3", "node_type": "4", "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "a859d48892dc6e360c88b587077ec9d9f4e21bc29f58b3da751982d8adfd78d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8450cf4d-3198-4636-a62f-8f4de1461078", "node_type": "1", "metadata": {"Header_1": " Experiments", "filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "aff1616c2111fa94efd6477ca1420273461b9282280a485d40806747a8c2b620", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d38ea77-cc9c-49bd-99b9-84e7de65d668", "node_type": "1", "metadata": {"Header_1": " References"}, "hash": "693e2bb71039c83ed03b0deb26e16353d911ab94f5c2b4132f4302352bc10e2a", "class_name": "RelatedNodeInfo"}}, "text": "Used in LlamaIndex\n\nThank [ Jerry Liu ](https://medium.com/u/e76da1c45ef7?source=post_page-----\n54b559b9ddf7--------------------------------) for your help with the\nLongLLMLingua project. Now you can use LongLLMLingua as a\n**NodePostprocessor** in this widely used RAG framework. For specific usage,\nyou can refer to the [ example 1\n](https://github.com/microsoft/LLMLingua/blob/main/examples/RAGLlamaIndex.ipynb)\n, [ example 2 ](https://github.com/run-\nllama/llama_index/blob/main/docs/examples/node_postprocessor/LongLLMLingua.ipynb)\nand the following code.\n\n    \n    \n    from llama_index.query_engine import RetrieverQueryEngine\n    from llama_index.response_synthesizers import CompactAndRefine\n    from llama_index.indices.postprocessor import LongLLMLinguaPostprocessor\n    \n    node_postprocessor = LongLLMLinguaPostprocessor(\n        instruction_str=\"Given the context, please answer the final question\",\n        target_token=300,\n        rank_method=\"longllmlingua\",\n        additional_compress_kwargs={\n            \"condition_compare\": True,\n            \"condition_in_question\": \"after\",\n            \"context_budget\": \"+100\",\n            \"reorder_context\": \"sort\",  # enable document reorder\n            \"dynamic_context_compression_ratio\": 0.4, # enable dynamic compression ratio\n        },\n    )", "mimetype": "text/plain", "start_char_idx": 9638, "end_char_idx": 10946, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d38ea77-cc9c-49bd-99b9-84e7de65d668": {"__data__": {"id_": "3d38ea77-cc9c-49bd-99b9-84e7de65d668", "embedding": null, "metadata": {"Header_1": " References", "filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc0b986c-3d16-4c1e-a7f7-2d4b7f122cd3", "node_type": "4", "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "a859d48892dc6e360c88b587077ec9d9f4e21bc29f58b3da751982d8adfd78d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b22d1284-e2f3-4354-b697-d7b93ad90b16", "node_type": "1", "metadata": {"Header_1": " Used in LlamaIndex", "filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "e20b497af1a1d1ab455dc7727a60e4047b7c4a5d60e60acfb8b9b2488cdd53ef", "class_name": "RelatedNodeInfo"}}, "text": "References\n\n[1] Lost in the Middle: How Language Models Use Long Contexts. Nelson F. Liu\netc. [2] Compressing Context to Enhance Inference Efficiency of Large Language\nModels. Yucheng Li etc. [3] LLMLingua: Compressing Prompts for Accelerated\nInference of Large Language Models. Huiqiang Jiang, Qianhui Wu etc. [4]\nRECOMP: Improving Retrieval-Augmented LMs with Compression and Selective\nAugmentation. Fangyuan Xu etc. [5] Retrieval meets Long Context Large Language\nModels. Peng Xu etc. [6] Walking Down the Memory Maze: Beyond Context Limit\nthrough Interactive Reading. Howard Chen etc.", "mimetype": "text/plain", "start_char_idx": 10951, "end_char_idx": 11539, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"a9048e95-98b1-4f7e-a0a7-7f045a634bd0": {"doc_hash": "6b8ec13c1e85ff2d54169df303bacf9187283bb0c694974db95a2dbf8056b39c", "ref_doc_id": "fc0b986c-3d16-4c1e-a7f7-2d4b7f122cd3"}, "c0828224-9afa-4591-8c46-5b878995759e": {"doc_hash": "5a379c588d5b5e0269ff16279f45c95a1087c849384ab8626dde5de804f9e2c9", "ref_doc_id": "fc0b986c-3d16-4c1e-a7f7-2d4b7f122cd3"}, "ee1b0be9-fe1e-4862-8c29-6ec73db54309": {"doc_hash": "7687cbf824c6f26c0387245796194d0009d9dc884b488cef281889a022121f61", "ref_doc_id": "fc0b986c-3d16-4c1e-a7f7-2d4b7f122cd3"}, "6c8a3f08-bd9b-419f-9ddf-79c7101fb95d": {"doc_hash": "925a6ca77a1cd8c0ed22fbe289406249c716e2e748f1d9df7032fffbfc5c229e", "ref_doc_id": "fc0b986c-3d16-4c1e-a7f7-2d4b7f122cd3"}, "c247c1cd-97b5-4139-b3d0-da0848cdfc2e": {"doc_hash": "8e1b5cc0aa0060813651c1ecd70060ca7f6da33a67283d1ea0dd6b51e70fa139", "ref_doc_id": "fc0b986c-3d16-4c1e-a7f7-2d4b7f122cd3"}, "b2e7c43c-beb5-495c-9285-456343c57813": {"doc_hash": "8cff76efe067e7be5340c9474fe6151aa2bb451b24ca481627ae862b962f0bcd", "ref_doc_id": "fc0b986c-3d16-4c1e-a7f7-2d4b7f122cd3"}, "c8c2a0b7-5957-4181-98e2-0ee0b57de08d": {"doc_hash": "5e2b5a2795ba71123c46b20381cf1fb7710c942698569cafcf2a457c0dc81b4f", "ref_doc_id": "fc0b986c-3d16-4c1e-a7f7-2d4b7f122cd3"}, "8450cf4d-3198-4636-a62f-8f4de1461078": {"doc_hash": "aff1616c2111fa94efd6477ca1420273461b9282280a485d40806747a8c2b620", "ref_doc_id": "fc0b986c-3d16-4c1e-a7f7-2d4b7f122cd3"}, "b22d1284-e2f3-4354-b697-d7b93ad90b16": {"doc_hash": "e20b497af1a1d1ab455dc7727a60e4047b7c4a5d60e60acfb8b9b2488cdd53ef", "ref_doc_id": "fc0b986c-3d16-4c1e-a7f7-2d4b7f122cd3"}, "3d38ea77-cc9c-49bd-99b9-84e7de65d668": {"doc_hash": "b7d7effa351e4c6cad3fdde3924943718809c13d169180d15684a101eacd29aa", "ref_doc_id": "fc0b986c-3d16-4c1e-a7f7-2d4b7f122cd3"}}, "docstore/ref_doc_info": {"fc0b986c-3d16-4c1e-a7f7-2d4b7f122cd3": {"node_ids": ["a9048e95-98b1-4f7e-a0a7-7f045a634bd0", "c0828224-9afa-4591-8c46-5b878995759e", "ee1b0be9-fe1e-4862-8c29-6ec73db54309", "6c8a3f08-bd9b-419f-9ddf-79c7101fb95d", "c247c1cd-97b5-4139-b3d0-da0848cdfc2e", "b2e7c43c-beb5-495c-9285-456343c57813", "c8c2a0b7-5957-4181-98e2-0ee0b57de08d", "8450cf4d-3198-4636-a62f-8f4de1461078", "b22d1284-e2f3-4354-b697-d7b93ad90b16", "3d38ea77-cc9c-49bd-99b9-84e7de65d668"], "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}}}}