{"docstore/data": {"04607c96-63b3-40e8-beda-86631d1d5f72": {"__data__": {"id_": "04607c96-63b3-40e8-beda-86631d1d5f72", "embedding": null, "metadata": {"filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2d0a58eb-f166-4f27-b944-c3cba61befad", "node_type": "4", "metadata": {"filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "hash": "c0a57d501815026a752633916dc58a4891d8ae23da978f92c50cba58ca6960c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab1602cb-c065-44d1-a7e2-3240610c20df", "node_type": "1", "metadata": {"Header_1": " Overview"}, "hash": "e11fe5d0ed72e5d5dc81c00cf78dd2f6651187266870b2c3932c76cbba358aba", "class_name": "RelatedNodeInfo"}}, "text": "(co-authored by Haotian Zhang, Laurie Voss, and Jerry Liu @ LlamaIndex)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 71, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab1602cb-c065-44d1-a7e2-3240610c20df": {"__data__": {"id_": "ab1602cb-c065-44d1-a7e2-3240610c20df", "embedding": null, "metadata": {"Header_1": " Overview", "filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2d0a58eb-f166-4f27-b944-c3cba61befad", "node_type": "4", "metadata": {"filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "hash": "c0a57d501815026a752633916dc58a4891d8ae23da978f92c50cba58ca6960c5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04607c96-63b3-40e8-beda-86631d1d5f72", "node_type": "1", "metadata": {"filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "hash": "5f86eee8bb492e5db93f51b8e99b8bed8b3e0a9744388269186e58940ff218cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd6e6ff8-884c-42ca-b4e9-052852fd88eb", "node_type": "1", "metadata": {"Header_1": " Multi-Modal RAG"}, "hash": "59307988cfcc76e723b818e907642dafe2d2d26d38931b17dd2e08e1d14e735c", "class_name": "RelatedNodeInfo"}}, "text": "Overview\n\nIn this blog we\u2019re excited to present a fundamentally new paradigm: multi-\nmodal Retrieval-Augmented Generation (RAG). We present new abstractions in\nLlamaIndex that now enable the following:\n\n  * Multi-modal LLMs and Embeddings \n  * Multi-modal Indexing and Retrieval (integrates with vector dbs)", "mimetype": "text/plain", "start_char_idx": 76, "end_char_idx": 383, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd6e6ff8-884c-42ca-b4e9-052852fd88eb": {"__data__": {"id_": "fd6e6ff8-884c-42ca-b4e9-052852fd88eb", "embedding": null, "metadata": {"Header_1": " Multi-Modal RAG", "filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2d0a58eb-f166-4f27-b944-c3cba61befad", "node_type": "4", "metadata": {"filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "hash": "c0a57d501815026a752633916dc58a4891d8ae23da978f92c50cba58ca6960c5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab1602cb-c065-44d1-a7e2-3240610c20df", "node_type": "1", "metadata": {"Header_1": " Overview", "filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "hash": "12f401a39c82408137223f84049cb2fd04510816198c4305adcb1fe9b74429cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7be38a09-c844-48be-b97c-65e4260d08e5", "node_type": "1", "metadata": {"Header_1": " Abstractions in LlamaIndex"}, "hash": "1464c8c33463f343397af9fa116c610eaca477632466e9470d34c17c41433b1d", "class_name": "RelatedNodeInfo"}}, "text": "Multi-Modal RAG\n\nOne of the most exciting announcements at OpenAI Dev Day was the release of\nthe [ GPT-4V API ](https://platform.openai.com/docs/guides/vision) . GPT-4V is\na _multi-modal_ model that takes in both text/images, and can output text\nresponses. It\u2019s the latest model in a recent series of advances around multi-\nmodal models: [ LLaVa ](https://github.com/haotian-liu/LLaVA) , and [ Fuyu-8B\n](https://www.adept.ai/blog/fuyu-8b) .\n\nThis extends the capabilities of LLMs in exciting new directions. In the past\nyear, entire application stacks have emerged around the text-in/text-out\nparadigm. One of the most notable examples is Retrieval Augmented Generation\n(RAG) \u2014 combine an LLM with an external text corpus to reason over data that\nthe model isn\u2019t trained on. One of the most significant impacts of RAG for\nend-users was how much it accelerated **time-to-insight** on unstructured text\ndata. By processing an arbitrary document (PDF, web page), loading it into\nstorage, and feeding it into the context window of an LLM, you could extract\nout any insights you wanted from it.\n\nThe introduction of GPT-4V API allows us to extend RAG concepts into the\nhybrid image/text domain, and unlock value from an even greater corpus of data\n(including images).\n\nThink about all the steps in a standard RAG pipeline and how it can be\nextended to a multi-modal setting.\n\n  * **Input:** The input can be text or images. \n  * **Retrieval:** The retrieved context can be text or images. \n  * **Synthesis:** The answer can be synthesized over both text and images. \n  * **Response:** The returned result can be text and/or images. \n\nThis is just a small part of the overall space too. You can have\nchained/sequential calls that interleave between image and text reasoning,\nsuch as [ Retrieval Augmented Image Captioning\n](https://twitter.com/jerryjliu0/status/1717205234269983030) or multi-modal\nagent loops.", "mimetype": "text/plain", "start_char_idx": 389, "end_char_idx": 2293, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7be38a09-c844-48be-b97c-65e4260d08e5": {"__data__": {"id_": "7be38a09-c844-48be-b97c-65e4260d08e5", "embedding": null, "metadata": {"Header_1": " Abstractions in LlamaIndex", "filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2d0a58eb-f166-4f27-b944-c3cba61befad", "node_type": "4", "metadata": {"filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "hash": "c0a57d501815026a752633916dc58a4891d8ae23da978f92c50cba58ca6960c5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd6e6ff8-884c-42ca-b4e9-052852fd88eb", "node_type": "1", "metadata": {"Header_1": " Multi-Modal RAG", "filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "hash": "6292e9b9f5ff7dd8353f6f9372cd71be57a15fbaeec71d0ac5b048f55128883a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1917d2de-01f6-444b-92f8-21fe90c10f4b", "node_type": "1", "metadata": {"Header_1": " Abstractions in LlamaIndex", "Header_2": " Multi-modal LLM"}, "hash": "2e06b701fbbfac4207ade974e7944172ea68f5e8d728adbb001456a5d1f3890b", "class_name": "RelatedNodeInfo"}}, "text": "Abstractions in LlamaIndex\n\nWe\u2019re excited to present new abstractions in LlamaIndex that help make multi-\nmodal RAG possible. For each abstraction, we explicitly note what we\u2019ve done\nso far and what\u2019s still to come.", "mimetype": "text/plain", "start_char_idx": 2298, "end_char_idx": 2513, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1917d2de-01f6-444b-92f8-21fe90c10f4b": {"__data__": {"id_": "1917d2de-01f6-444b-92f8-21fe90c10f4b", "embedding": null, "metadata": {"Header_1": " Abstractions in LlamaIndex", "Header_2": " Multi-modal LLM", "filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2d0a58eb-f166-4f27-b944-c3cba61befad", "node_type": "4", "metadata": {"filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "hash": "c0a57d501815026a752633916dc58a4891d8ae23da978f92c50cba58ca6960c5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7be38a09-c844-48be-b97c-65e4260d08e5", "node_type": "1", "metadata": {"Header_1": " Abstractions in LlamaIndex", "filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "hash": "73b43180a356d3e0244e9db88bf6626ec3e9017fa847c236034805aba975e97e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c98a1cc4-996b-4a3d-b988-b94fbd923445", "node_type": "1", "metadata": {"Header_1": " Abstractions in LlamaIndex", "Header_2": " Multi-Modal Embeddings"}, "hash": "754102274fecb2e9a4368910356a881e8f16abe0d67c9ea9a2f6c01d0a2bf564", "class_name": "RelatedNodeInfo"}}, "text": "Multi-modal LLM\n\nWe have direct support for GPT-4V via our ` OpenAIMultiModal ` class and\nsupport for open-source multi-modal models via our ` ReplicateMultiModal `\nclass (currently in beta, so that name might change). Our `\nSimpleDirectoryReader ` has long been able to ingest audio, images and video,\nbut now you can pass them directly to GPT-4V and ask questions about them,\nlike this:\n\n    \n    \n    from llama_index.multi_modal_llms import OpenAIMultiModal\n    from llama_index import SimpleDirectoryReader\n    \n    image_documents = SimpleDirectoryReader(local_directory).load_data()\n    \n    openai_mm_llm = OpenAIMultiModal(\n        model=\"gpt-4-vision-preview\", api_key=OPENAI_API_TOKEN, max_new_tokens=300\n    )\n    response = openai_mm_llm.complete(\n        prompt=\"what is in the image?\", image_documents=image_documents\n    ) \n\nThis is a new base model abstraction. Unlike our default ` LLM ` class, which\nhas standard completion/chat endpoints, the multi-modal model ( `\nMultiModalLLM ` ) can take in both image and text as input.\n\nThis also unifies the interface between both GPT-4V and open-source models.\n\n**Resources**\n\nWe have initial implementations for both GPT-4V and vision models hosted on\nReplicate. We also have a docs page for multi-modal models:\n\n  * [ Multi-modal docs page ](https://docs.llamaindex.ai/en/latest/module_guides/models/multi_modal.html)\n  * [ GPT-4V ](https://docs.llamaindex.ai/en/latest/examples/multi_modal/openai_multi_modal.html)\n  * [ Replicate ](https://docs.llamaindex.ai/en/latest/examples/multi_modal/replicate_multi_modal.html)\n\nDisplayed image and example output from GPT-4V given text query \u201cDescribe\nimage as alternative text\u201d\n\n**What\u2019s still to come:**\n\n  * More multi-modal LLM integrations \n  * Chat endpoints \n  * Streaming", "mimetype": "text/plain", "start_char_idx": 2519, "end_char_idx": 4304, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c98a1cc4-996b-4a3d-b988-b94fbd923445": {"__data__": {"id_": "c98a1cc4-996b-4a3d-b988-b94fbd923445", "embedding": null, "metadata": {"Header_1": " Abstractions in LlamaIndex", "Header_2": " Multi-Modal Embeddings", "filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2d0a58eb-f166-4f27-b944-c3cba61befad", "node_type": "4", "metadata": {"filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "hash": "c0a57d501815026a752633916dc58a4891d8ae23da978f92c50cba58ca6960c5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1917d2de-01f6-444b-92f8-21fe90c10f4b", "node_type": "1", "metadata": {"Header_1": " Abstractions in LlamaIndex", "Header_2": " Multi-modal LLM", "filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "hash": "4d791f562f8797efae9551716260b3897022165c050ff8db98cdff253d291c30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8bfa86e-706e-4492-97ed-5791cb5ce50c", "node_type": "1", "metadata": {"Header_1": " Abstractions in LlamaIndex", "Header_2": " Multi-Modal Indexing and Retrieval"}, "hash": "1668121e32df53a462bd761303b27ff7961e1bb70d96d874d1321b2ebc8331d7", "class_name": "RelatedNodeInfo"}}, "text": "Multi-Modal Embeddings\n\nWe introduce a new ` MultiModalEmbedding ` base class that can embed both text\nand images. It contains all the methods as our existing embedding models\n(subclasses ` BaseEmbedding ` ) but also exposes ` get_image_embedding ` .\n\nOur primary implementation here is ` ClipEmbedding ` with the CLIP model. See\nbelow for a guide on using this in action.\n\n**What\u2019s still to come**\n\n  * More multi-modal embedding integrations", "mimetype": "text/plain", "start_char_idx": 4311, "end_char_idx": 4754, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8bfa86e-706e-4492-97ed-5791cb5ce50c": {"__data__": {"id_": "e8bfa86e-706e-4492-97ed-5791cb5ce50c", "embedding": null, "metadata": {"Header_1": " Abstractions in LlamaIndex", "Header_2": " Multi-Modal Indexing and Retrieval", "filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2d0a58eb-f166-4f27-b944-c3cba61befad", "node_type": "4", "metadata": {"filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "hash": "c0a57d501815026a752633916dc58a4891d8ae23da978f92c50cba58ca6960c5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c98a1cc4-996b-4a3d-b988-b94fbd923445", "node_type": "1", "metadata": {"Header_1": " Abstractions in LlamaIndex", "Header_2": " Multi-Modal Embeddings", "filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "hash": "4f5f1937e999afd39714f4b2b8f98e5212a27777be240ee2058d6ad6bec9b0d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8a34fae-1ed4-43f8-8d9e-770305b61c40", "node_type": "1", "metadata": {"Header_1": " Notebook Walkthrough"}, "hash": "982969936dc425753fddc1dfbedb6ea31682c09a59e4f165be02c94efc31f2a5", "class_name": "RelatedNodeInfo"}}, "text": "Multi-Modal Indexing and Retrieval\n\nWe create a new index, a ` MultiModalVectorIndex ` that can index both text\nand images into underlying storage systems \u2014 specifically a vector database\nand docstore.\n\nUnlike our existing (most popular) index, the ` VectorStoreIndex ` , this new\nindex can store both text and image documents. Indexing text is unchanged \u2014 it\nis embedded using a text embedding model and stored in a vector database.\nIndexing images involves a separate process:\n\n  1. Embed the image using CLIP \n  2. Represent the image node as a base64 encoding or path, and store it along with its embedding in a vector db (separate collection from text). \n\nWe store images and text separately since we may want to use a text-only\nembedding model for text as opposed to CLIP embeddings (e.g. ada or sbert).\n\nDuring retrieval-time, we do the following:\n\n  1. Retrieve text via vector search on the text embeddings \n  2. Retrieve images via vector search on the image embeddings \n\nBoth text and images are returned as Nodes in the result list. We can then\nsynthesize over these results.\n\n**What\u2019s still to Come**\n\n  * More native ways to store images in a vector store (beyond base64 encoding) \n  * More flexible multi-modal retrieval abstractions (e.g. combining image retrieval with any text retrieval method) \n  * Multi-modal response synthesis abstractions. Currently the way to deal with long text context is to do \u201ccreate-and-refine\u201d or \u201ctree-summarize\u201d over it. It\u2019s unclear what generic response synthesis over multiple images and text looks like.", "mimetype": "text/plain", "start_char_idx": 4761, "end_char_idx": 6317, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8a34fae-1ed4-43f8-8d9e-770305b61c40": {"__data__": {"id_": "b8a34fae-1ed4-43f8-8d9e-770305b61c40", "embedding": null, "metadata": {"Header_1": " Notebook Walkthrough", "filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2d0a58eb-f166-4f27-b944-c3cba61befad", "node_type": "4", "metadata": {"filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "hash": "c0a57d501815026a752633916dc58a4891d8ae23da978f92c50cba58ca6960c5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8bfa86e-706e-4492-97ed-5791cb5ce50c", "node_type": "1", "metadata": {"Header_1": " Abstractions in LlamaIndex", "Header_2": " Multi-Modal Indexing and Retrieval", "filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "hash": "fb3df5f5490a35dbb42a12b7ed3ded8685427767bbd9a4c9a7e4e660eed09cca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de5c7fef-f97e-47c8-a568-1e7af81db66f", "node_type": "1", "metadata": {"Header_1": " Notebook Walkthrough", "Header_2": " Example 1: Retrieval Augmented Captioning"}, "hash": "6c405df7ebc3c15154b4c84cbcbdda2d3f2a1a9407a297b033aa2ac0a83d3d76", "class_name": "RelatedNodeInfo"}}, "text": "Notebook Walkthrough\n\nLet\u2019s walk through a notebook example. Here we go over a use case of querying\nTesla given screenshots of its website/vehicles, SEC fillings, and Wikipedia\npages.\n\nWe load the documents as a mix of text docs and images:\n\n    \n    \n    documents = SimpleDirectoryReader(\"./mixed_wiki/\").load_data()\n\nWe then define two separate vector database collections in Qdrant: a\ncollection for text docs, and a collection for images. We then define a `\nMultiModalVectorStoreIndex ` .\n\n    \n    \n    # Create a local Qdrant vector store\n    client = qdrant_client.QdrantClient(path=\"qdrant_mm_db\")\n    \n    text_store = QdrantVectorStore(\n        client=client, collection_name=\"text_collection\"\n    )\n    image_store = QdrantVectorStore(\n        client=client, collection_name=\"image_collection\"\n    )\n    storage_context = StorageContext.from_defaults(vector_store=text_store)\n    \n    # Create the MultiModal index\n    index = MultiModalVectorStoreIndex.from_documents(\n        documents, storage_context=storage_context, image_vector_store=image_store\n    )\n\nWe can then ask questions over our multi-modal corpus.", "mimetype": "text/plain", "start_char_idx": 6323, "end_char_idx": 7449, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de5c7fef-f97e-47c8-a568-1e7af81db66f": {"__data__": {"id_": "de5c7fef-f97e-47c8-a568-1e7af81db66f", "embedding": null, "metadata": {"Header_1": " Notebook Walkthrough", "Header_2": " Example 1: Retrieval Augmented Captioning", "filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2d0a58eb-f166-4f27-b944-c3cba61befad", "node_type": "4", "metadata": {"filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "hash": "c0a57d501815026a752633916dc58a4891d8ae23da978f92c50cba58ca6960c5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8a34fae-1ed4-43f8-8d9e-770305b61c40", "node_type": "1", "metadata": {"Header_1": " Notebook Walkthrough", "filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "hash": "84182e40d287badfbf44074efb7dd961f315de0bab44e17081b1131bab32d71d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d168438a-eb9e-4e9d-a597-cfd6ebb58369", "node_type": "1", "metadata": {"Header_1": " Notebook Walkthrough", "Header_2": " Example 2: Multi-Modal RAG Querying"}, "hash": "d7129471907852397abb45bce5fef1771a05acc09bfd08490d1523b109743d33", "class_name": "RelatedNodeInfo"}}, "text": "Example 1: Retrieval Augmented Captioning\n\nHere we copy/paste an initial image caption as the input to get a retrieval-\naugmented output:\n\n    \n    \n    retriever_engine = index.as_retriever(\n        similarity_top_k=3, image_similarity_top_k=3\n    )\n    # retrieve more information from the GPT4V response\n    retrieval_results = retriever_engine.retrieve(query_str)\n\nThe retrieved results contain both images and text:\n\nRetrieved Text/Image Results\n\nWe can feed this to GPT-4V to ask a followup question or synthesize a coherent\nresponse:\n\nSynthesized Result", "mimetype": "text/plain", "start_char_idx": 7455, "end_char_idx": 8015, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d168438a-eb9e-4e9d-a597-cfd6ebb58369": {"__data__": {"id_": "d168438a-eb9e-4e9d-a597-cfd6ebb58369", "embedding": null, "metadata": {"Header_1": " Notebook Walkthrough", "Header_2": " Example 2: Multi-Modal RAG Querying", "filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2d0a58eb-f166-4f27-b944-c3cba61befad", "node_type": "4", "metadata": {"filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "hash": "c0a57d501815026a752633916dc58a4891d8ae23da978f92c50cba58ca6960c5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de5c7fef-f97e-47c8-a568-1e7af81db66f", "node_type": "1", "metadata": {"Header_1": " Notebook Walkthrough", "Header_2": " Example 1: Retrieval Augmented Captioning", "filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "hash": "f73a98da65eae5467d1854d102eb6234af4f579c7cee6f23f679de499a1bb2b2", "class_name": "RelatedNodeInfo"}}, "text": "Example 2: Multi-Modal RAG Querying\n\nHere we ask a question and get a response from the entire multi-modal RAG\npipeline. The ` SimpleMultiModalQueryEngine ` first retrieves the set of\nrelevant images/text, and feeds the input to a vision model in order to\nsynthesize a response.\n\n    \n    \n    from llama_index.query_engine import SimpleMultiModalQueryEngine\n    \n    query_engine = index.as_query_engine(\n        multi_modal_llm=openai_mm_llm,\n        text_qa_template=qa_tmpl\n    )\n    \n    query_str = \"Tell me more about the Porsche\"\n    response = query_engine.query(query_str)\n\nThe generated result + sources are shown below:", "mimetype": "text/plain", "start_char_idx": 8021, "end_char_idx": 8652, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"04607c96-63b3-40e8-beda-86631d1d5f72": {"doc_hash": "5f86eee8bb492e5db93f51b8e99b8bed8b3e0a9744388269186e58940ff218cd", "ref_doc_id": "2d0a58eb-f166-4f27-b944-c3cba61befad"}, "ab1602cb-c065-44d1-a7e2-3240610c20df": {"doc_hash": "12f401a39c82408137223f84049cb2fd04510816198c4305adcb1fe9b74429cb", "ref_doc_id": "2d0a58eb-f166-4f27-b944-c3cba61befad"}, "fd6e6ff8-884c-42ca-b4e9-052852fd88eb": {"doc_hash": "6292e9b9f5ff7dd8353f6f9372cd71be57a15fbaeec71d0ac5b048f55128883a", "ref_doc_id": "2d0a58eb-f166-4f27-b944-c3cba61befad"}, "7be38a09-c844-48be-b97c-65e4260d08e5": {"doc_hash": "73b43180a356d3e0244e9db88bf6626ec3e9017fa847c236034805aba975e97e", "ref_doc_id": "2d0a58eb-f166-4f27-b944-c3cba61befad"}, "1917d2de-01f6-444b-92f8-21fe90c10f4b": {"doc_hash": "4d791f562f8797efae9551716260b3897022165c050ff8db98cdff253d291c30", "ref_doc_id": "2d0a58eb-f166-4f27-b944-c3cba61befad"}, "c98a1cc4-996b-4a3d-b988-b94fbd923445": {"doc_hash": "4f5f1937e999afd39714f4b2b8f98e5212a27777be240ee2058d6ad6bec9b0d4", "ref_doc_id": "2d0a58eb-f166-4f27-b944-c3cba61befad"}, "e8bfa86e-706e-4492-97ed-5791cb5ce50c": {"doc_hash": "fb3df5f5490a35dbb42a12b7ed3ded8685427767bbd9a4c9a7e4e660eed09cca", "ref_doc_id": "2d0a58eb-f166-4f27-b944-c3cba61befad"}, "b8a34fae-1ed4-43f8-8d9e-770305b61c40": {"doc_hash": "84182e40d287badfbf44074efb7dd961f315de0bab44e17081b1131bab32d71d", "ref_doc_id": "2d0a58eb-f166-4f27-b944-c3cba61befad"}, "de5c7fef-f97e-47c8-a568-1e7af81db66f": {"doc_hash": "f73a98da65eae5467d1854d102eb6234af4f579c7cee6f23f679de499a1bb2b2", "ref_doc_id": "2d0a58eb-f166-4f27-b944-c3cba61befad"}, "d168438a-eb9e-4e9d-a597-cfd6ebb58369": {"doc_hash": "a8c4f4db2da2788363aa7e040a9b0ea84a132da484e90b7964df4d30bb389b18", "ref_doc_id": "2d0a58eb-f166-4f27-b944-c3cba61befad"}}, "docstore/ref_doc_info": {"2d0a58eb-f166-4f27-b944-c3cba61befad": {"node_ids": ["04607c96-63b3-40e8-beda-86631d1d5f72", "ab1602cb-c065-44d1-a7e2-3240610c20df", "fd6e6ff8-884c-42ca-b4e9-052852fd88eb", "7be38a09-c844-48be-b97c-65e4260d08e5", "1917d2de-01f6-444b-92f8-21fe90c10f4b", "c98a1cc4-996b-4a3d-b988-b94fbd923445", "e8bfa86e-706e-4492-97ed-5791cb5ce50c", "b8a34fae-1ed4-43f8-8d9e-770305b61c40", "de5c7fef-f97e-47c8-a568-1e7af81db66f", "d168438a-eb9e-4e9d-a597-cfd6ebb58369"], "metadata": {"filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}}}}