{"docstore/data": {"be47446f-9aba-46f4-9357-0c3dccba3408": {"__data__": {"id_": "be47446f-9aba-46f4-9357-0c3dccba3408", "embedding": null, "metadata": {"filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e6d653b5-9868-41f4-961b-8c4c09ca0786", "node_type": "4", "metadata": {"filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}, "hash": "04da8dd77b08637b4f37b48c597e93d228ef7a1effb485386553065b23148fea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2592573-50b3-4b5c-913f-251adb328f74", "node_type": "1", "metadata": {"Header_2": " Data preprocessing"}, "hash": "b00bc255651f0b726162077ebab08fb0caeb604eb4287fc21d661005d8260710", "class_name": "RelatedNodeInfo"}}, "text": "The field of AI and large language models is evolving rapidly. One year ago,\nnobody ever used an LLM to enhance their productivity. Today, most of us can\u2019t\nimagine working without or not offloading at least some minor tasks to LLMs.\nDue to much research and interest, LLMs are getting better and wiser every\nday. Not only that, but their comprehension is starting to span across\nmultiple modalities. With the introduction of GPT-4-Vision and other LLMs that\nfollowed it, it seems that LLMs today can tackle and comprehend images very\nwell. Here\u2019s one example of ChatGPT describing what\u2019s in the image.\n\nUsing ChatGPT to describe images.\n\nAs you can observe, ChatGPT is quite good at comprehending and describing\nimages. We can use its ability to understand images in an RAG application,\nwhere instead of relying only on text to generate an accurate and up-to-date\nanswer, we can now combine information from text and pictures to generate more\naccurate answers than ever before. Using LlamaIndex, implementing multimodal\nRAG pipelines is as easy as it gets. Inspired by their [ multimodal cookbook\nexample ](https://github.com/run-\nllama/llama_index/blob/main/docs/examples/multi_modal/gpt4v_multi_modal_retrieval.ipynb)\n, I decided to test if I could implement a multimodal RAG application with\nNeo4j as the database.\n\nTo implement a multimodal RAG pipeline with LlamaIndex, you simply instantiate\ntwo vector stores, one for images and one for text, and then query both of\nthem in order to retrieve relevant information to generate the final answer.\n\nWorkflow diagram for the blog post. Image by author.\n\nArticles are first split into images and text. These elements are then\nconverted into vector representations and indexed separately. For text we will\nuse _ada-002_ text embedding model, while for images we will be using [ dual\nencoder model CLIP ](https://github.com/openai/CLIP) , which can embed both\ntext and images in the same embedding space. When a question is posed by an\nend user, two vector similarity search are performed; one to find relevant\nimages and the other for documents. The results are fed into a multimodal LLM,\nwhich generates an answer for the user, demonstrating an integrated approach\nto processing and utilizing mixed media for information retrieval and response\ngeneration.\n\n_The_ [ _code is available on GitHub_\n](https://github.com/tomasonjo/blogs/blob/master/llm/neo4j_llama_multimodal.ipynb)\n_._", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2431, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2592573-50b3-4b5c-913f-251adb328f74": {"__data__": {"id_": "b2592573-50b3-4b5c-913f-251adb328f74", "embedding": null, "metadata": {"Header_2": " Data preprocessing", "filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e6d653b5-9868-41f4-961b-8c4c09ca0786", "node_type": "4", "metadata": {"filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}, "hash": "04da8dd77b08637b4f37b48c597e93d228ef7a1effb485386553065b23148fea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be47446f-9aba-46f4-9357-0c3dccba3408", "node_type": "1", "metadata": {"filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}, "hash": "8c5c3888421891962a55be1c2b17422ab87e56b00b4194f46c9623e56c87be56", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d9bc4172-93c2-4cd8-84aa-151fc5e9fe9d", "node_type": "1", "metadata": {"Header_2": " Indexing data vectors"}, "hash": "0f309306d49f5c405ce117f2b32e9cae2eeebc75f1d018a0d8cc2898d1779bf6", "class_name": "RelatedNodeInfo"}}, "text": "Data preprocessing\n\nWe will use my Medium articles from 2022 and 2023 as the [ grounding dataset\n](https://github.com/tomasonjo/blog-datasets/blob/main/articles.zip) for an\nRAG application. The articles contain vast information about Neo4j Graph Data\nScience library and combining Neo4j with LLM frameworks. When you download\nyour own articles from Medium, you get them in an HTML format. Therefore, we\nneed to employ a bit of coding to extract text and images separately.\n\n    \n    \n    def process_html_file(file_path):\n        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n            soup = BeautifulSoup(file, \"html.parser\")\n    \n        # Find the required section\n        content_section = soup.find(\"section\", {\"data-field\": \"body\", \"class\": \"e-content\"})\n    \n        if not content_section:\n            return \"Section not found.\"\n    \n        sections = []\n        current_section = {\"header\": \"\", \"content\": \"\", \"source\": file_path.split(\"/\")[-1]}\n        images = []\n        header_found = False\n    \n        for element in content_section.find_all(recursive=True):\n            if element.name in [\"h1\", \"h2\", \"h3\", \"h4\"]:\n                if header_found and (current_section[\"content\"].strip()):\n                    sections.append(current_section)\n                current_section = {\n                    \"header\": element.get_text(),\n                    \"content\": \"\",\n                    \"source\": file_path.split(\"/\")[-1],\n                }\n                header_found = True\n            elif header_found:\n                if element.name == \"pre\":\n                    current_section[\"content\"] += f\"```{element.get_text().strip()}```\\n\"\n                elif element.name == \"img\":\n                    img_src = element.get(\"src\")\n                    img_caption = element.find_next(\"figcaption\")\n                    caption_text = img_caption.get_text().strip() if img_caption else \"\"\n                    images.append(ImageDocument(image_url=img_src))\n                elif element.name in [\"p\", \"span\", \"a\"]:\n                    current_section[\"content\"] += element.get_text().strip() + \"\\n\"\n    \n        if current_section[\"content\"].strip():\n            sections.append(current_section)\n    \n        return images, sections\n\nI won\u2019t go into details for the parsing code, but we split the text based on\nheaders **h1\u2013h4** and extract image links. Then, we simply run all the\narticles through this function to extract all relevant information.\n\n    \n    \n    all_documents = []\n    all_images = []\n    \n    # Directory to search in (current working directory)\n    directory = os.getcwd()\n    \n    # Walking through the directory\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith(\".html\"):\n                # Update the file path to be relative to the current directory\n                images, documents = process_html_file(os.path.join(root, file))\n                all_documents.extend(documents)\n                all_images.extend(images)\n    \n    text_docs = [Document(text=el.pop(\"content\"), metadata=el) for el in all_documents]\n    print(f\"Text document count: {len(text_docs)}\") # Text document count: 252\n    print(f\"Image document count: {len(all_images)}\") # Image document count: 328\n\nWe get a total of 252 text chunks and 328 images. It\u2019s a bit surprising that I\ncreated so many photos, but I know that some are only images of table results.\nWe could use a vision model to filter out irrelevant photos, but I skipped\nthis step here.", "mimetype": "text/plain", "start_char_idx": 2437, "end_char_idx": 5961, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d9bc4172-93c2-4cd8-84aa-151fc5e9fe9d": {"__data__": {"id_": "d9bc4172-93c2-4cd8-84aa-151fc5e9fe9d", "embedding": null, "metadata": {"Header_2": " Indexing data vectors", "filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e6d653b5-9868-41f4-961b-8c4c09ca0786", "node_type": "4", "metadata": {"filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}, "hash": "04da8dd77b08637b4f37b48c597e93d228ef7a1effb485386553065b23148fea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2592573-50b3-4b5c-913f-251adb328f74", "node_type": "1", "metadata": {"Header_2": " Data preprocessing", "filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}, "hash": "5951248fdf8665c66c7a21dc7cef464ef26b1fac62336b098f2610319a0c7f10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d226e32c-bc32-495c-a3b9-2abffcf7b9e5", "node_type": "1", "metadata": {"Header_2": " Multimodal RAG pipeline"}, "hash": "2528e4853f5abfb53f668f4948127b27cbf7f7151b6666a93d1ba0a43d15fe93", "class_name": "RelatedNodeInfo"}}, "text": "Indexing data vectors\n\nAs mentioned, we have to instantiate two vector stores, one for images and the\nother for text. The CLIP embedding model has a dimension of 512, while the\nada-002 has 1536 dimension.\n\n    \n    \n    text_store = Neo4jVectorStore(\n        url=NEO4J_URI,\n        username=NEO4J_USERNAME,\n        password=NEO4J_PASSWORD,\n        index_name=\"text_collection\",\n        node_label=\"Chunk\",\n        embedding_dimension=1536\n    )\n    image_store = Neo4jVectorStore(\n        url=NEO4J_URI,\n        username=NEO4J_USERNAME,\n        password=NEO4J_PASSWORD,\n        index_name=\"image_collection\",\n        node_label=\"Image\",\n        embedding_dimension=512\n    \n    )\n    storage_context = StorageContext.from_defaults(vector_store=text_store)\n\nNow that the vector stores have been initiated, we use the\n**MultiModalVectorStoreIndex** to index both modalities of information we\nhave.\n\n    \n    \n    # Takes 10 min without GPU / 1 min with GPU on Google collab\n    index = MultiModalVectorStoreIndex.from_documents(\n        text_docs + all_images, storage_context=storage_context, image_vector_store=image_store\n    )\n\nUnder the hood, **MultiModalVectorStoreIndex** uses text and image embedding\nmodels to calculate the embeddings and store and index the results in Neo4j.\nOnly the URLs are stored for images, not actual base64 or other\nrepresentations of images.", "mimetype": "text/plain", "start_char_idx": 5967, "end_char_idx": 7341, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d226e32c-bc32-495c-a3b9-2abffcf7b9e5": {"__data__": {"id_": "d226e32c-bc32-495c-a3b9-2abffcf7b9e5", "embedding": null, "metadata": {"Header_2": " Multimodal RAG pipeline", "filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e6d653b5-9868-41f4-961b-8c4c09ca0786", "node_type": "4", "metadata": {"filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}, "hash": "04da8dd77b08637b4f37b48c597e93d228ef7a1effb485386553065b23148fea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d9bc4172-93c2-4cd8-84aa-151fc5e9fe9d", "node_type": "1", "metadata": {"Header_2": " Indexing data vectors", "filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}, "hash": "69c37cf0cda9832f8dd16d405d527067f5b26b88362505ca01d0f925541e3be1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "135cfa4f-88ee-4165-be92-3b94659ee057", "node_type": "1", "metadata": {"Header_2": " Conclusion"}, "hash": "6c001d2ccc6be43ac01b4061a8537fb91e68fc3cdb7c69b569393a75d4c55bc2", "class_name": "RelatedNodeInfo"}}, "text": "Multimodal RAG pipeline\n\nThis piece of code is copied directly from the LlamaIndex multimodal cookbook.\nWe begin by defining a multimodal LLM and the prompt template and then combine\neverything as a query engine.\n\n    \n    \n    openai_mm_llm = OpenAIMultiModal(\n        model=\"gpt-4-vision-preview\", max_new_tokens=1500\n    )\n    \n    qa_tmpl_str = (\n        \"Context information is below.\\n\"\n        \"---------------------\\n\"\n        \"{context_str}\\n\"\n        \"---------------------\\n\"\n        \"Given the context information and not prior knowledge, \"\n        \"answer the query.\\n\"\n        \"Query: {query_str}\\n\"\n        \"Answer: \"\n    )\n    qa_tmpl = PromptTemplate(qa_tmpl_str)\n    \n    query_engine = index.as_query_engine(\n        multi_modal_llm=openai_mm_llm, text_qa_template=qa_tmpl\n    )\n\nNow we can go ahead and test how well it performs.\n\n    \n    \n    query_str = \"How do vector RAG application work?\"\n    response = query_engine.query(query_str)\n    print(response)\n\n_Response_\n\nGenerated response by an LLM.\n\nWe can also visualize which images the retrieval fetched and were used to help\ninform the final answer.\n\nImage input to LLM.\n\nThe LLM got two identical images as input, which just shows that I reuse some\nof my diagrams. However, I am pleasantly surprised by CLIP embeddings as they\nwere able to retrieve he most relevant image out of the collection. In a more\nproduction setting, you might want to clean and deduplicate images, but that\nis beyond the scope of this article.", "mimetype": "text/plain", "start_char_idx": 7347, "end_char_idx": 8844, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "135cfa4f-88ee-4165-be92-3b94659ee057": {"__data__": {"id_": "135cfa4f-88ee-4165-be92-3b94659ee057", "embedding": null, "metadata": {"Header_2": " Conclusion", "filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e6d653b5-9868-41f4-961b-8c4c09ca0786", "node_type": "4", "metadata": {"filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}, "hash": "04da8dd77b08637b4f37b48c597e93d228ef7a1effb485386553065b23148fea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d226e32c-bc32-495c-a3b9-2abffcf7b9e5", "node_type": "1", "metadata": {"Header_2": " Multimodal RAG pipeline", "filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}, "hash": "98be63b638074e49b82cf1640eef51315b30a93d1e78001e7d7a71b3ea1a823f", "class_name": "RelatedNodeInfo"}}, "text": "Conclusion\n\nLLMs are evolving faster than what we are historically used to and are\nspanning across multiple modalities. I firmly believe that by the end of the\nnext year, LLMs will be soon able to comprehend videos, and be therefore able\nto pick up non-verbal cues while talking to you. On the other hand, we can use\nimages as input to RAG pipeline and enhance the variety of information passed\nto an LLM, making responses better and more accurate. The multimodal RAG\npipelines implementation with LlamaIndex and Neo4j is as easy as it gets.\n\nThe code is available on [ GitHub\n](https://github.com/tomasonjo/blogs/blob/master/llm/neo4j_llama_multimodal.ipynb)\n.", "mimetype": "text/plain", "start_char_idx": 8850, "end_char_idx": 9511, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"be47446f-9aba-46f4-9357-0c3dccba3408": {"doc_hash": "8c5c3888421891962a55be1c2b17422ab87e56b00b4194f46c9623e56c87be56", "ref_doc_id": "e6d653b5-9868-41f4-961b-8c4c09ca0786"}, "b2592573-50b3-4b5c-913f-251adb328f74": {"doc_hash": "5951248fdf8665c66c7a21dc7cef464ef26b1fac62336b098f2610319a0c7f10", "ref_doc_id": "e6d653b5-9868-41f4-961b-8c4c09ca0786"}, "d9bc4172-93c2-4cd8-84aa-151fc5e9fe9d": {"doc_hash": "69c37cf0cda9832f8dd16d405d527067f5b26b88362505ca01d0f925541e3be1", "ref_doc_id": "e6d653b5-9868-41f4-961b-8c4c09ca0786"}, "d226e32c-bc32-495c-a3b9-2abffcf7b9e5": {"doc_hash": "98be63b638074e49b82cf1640eef51315b30a93d1e78001e7d7a71b3ea1a823f", "ref_doc_id": "e6d653b5-9868-41f4-961b-8c4c09ca0786"}, "135cfa4f-88ee-4165-be92-3b94659ee057": {"doc_hash": "fe49edf0970b7f438d2c2f06e24d25091e2b619ecae8962121ae24543ace913f", "ref_doc_id": "e6d653b5-9868-41f4-961b-8c4c09ca0786"}}, "docstore/ref_doc_info": {"e6d653b5-9868-41f4-961b-8c4c09ca0786": {"node_ids": ["be47446f-9aba-46f4-9357-0c3dccba3408", "b2592573-50b3-4b5c-913f-251adb328f74", "d9bc4172-93c2-4cd8-84aa-151fc5e9fe9d", "d226e32c-bc32-495c-a3b9-2abffcf7b9e5", "135cfa4f-88ee-4165-be92-3b94659ee057"], "metadata": {"filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}}}}