{"docstore/data": {"869c34b2-dec3-4336-bc51-b13f04de2219": {"__data__": {"id_": "869c34b2-dec3-4336-bc51-b13f04de2219", "embedding": null, "metadata": {"Header_1": " Introduction", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e", "node_type": "4", "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "e2993ef729906ce47697cc74b4c536035ef113ec10650503e6789e1ae38e795e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd8305aa-a331-4021-80be-0e04897bf16a", "node_type": "1", "metadata": {"Header_1": " Introduction", "Header_2": " Why Long Context Matters and How Retrieval Augmentation Steps In:"}, "hash": "61a7c252f29955d57f2629312a7231d94515b5aa0d8031fb2d0d9c7efd47b7ca", "class_name": "RelatedNodeInfo"}}, "text": "Introduction", "mimetype": "text/plain", "start_char_idx": 3, "end_char_idx": 15, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd8305aa-a331-4021-80be-0e04897bf16a": {"__data__": {"id_": "bd8305aa-a331-4021-80be-0e04897bf16a", "embedding": null, "metadata": {"Header_1": " Introduction", "Header_2": " Why Long Context Matters and How Retrieval Augmentation Steps In:", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e", "node_type": "4", "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "e2993ef729906ce47697cc74b4c536035ef113ec10650503e6789e1ae38e795e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "869c34b2-dec3-4336-bc51-b13f04de2219", "node_type": "1", "metadata": {"Header_1": " Introduction", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "4c015de05563cf4569c31dfd23edb7e14d46854530637a05a7ff4e6e20d18ee2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6626b5e7-5e91-4cef-bb74-b0069658f4ca", "node_type": "1", "metadata": {"Header_1": " Introduction", "Header_2": " Prior Research and the NVIDIA Divergence:"}, "hash": "22341b4336f17779d236d4037ddf2015cc82ceae7a8ca7374721c0319a5d25f4", "class_name": "RelatedNodeInfo"}}, "text": "Why Long Context Matters and How Retrieval Augmentation Steps In:\n\nIn the dynamic landscape of LLMs, two methods have gained traction and seem to\nbe taking center stage: expanding the context window of Large Language Models\n(LLMs) and enhancing these models with retrieval capabilities. The continued\nevolution of GPU technology, coupled with breakthroughs in attention\nmechanisms, has given rise to long-context LLMs. Simultaneously, the concept\nof retrieval \u2014 where LLMs pick up only the most relevant context from a\nstandalone retriever \u2014 promises a revolution in efficiency and speed.\n\nIn the midst of these evolving narratives, some interesting questions emerge:\n\n  1. Retrieval-augmentation versus long context window, which one is better for downstream tasks? \n  2. Can both methods be combined to get the best of both worlds? \n\nTo dissect these questions, in this blog post we turn to [ NVIDIA\u2019s recent\nstudy ](https://arxiv.org/pdf/2310.03025v1.pdf) , which harnesses the power of\ntwo powerful LLMs: the proprietary GPT \u2014 43B and LLaMA2\u201370B, the research\nstrives to provide actionable insights for AI practitioners.", "mimetype": "text/plain", "start_char_idx": 21, "end_char_idx": 1145, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6626b5e7-5e91-4cef-bb74-b0069658f4ca": {"__data__": {"id_": "6626b5e7-5e91-4cef-bb74-b0069658f4ca", "embedding": null, "metadata": {"Header_1": " Introduction", "Header_2": " Prior Research and the NVIDIA Divergence:", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e", "node_type": "4", "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "e2993ef729906ce47697cc74b4c536035ef113ec10650503e6789e1ae38e795e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd8305aa-a331-4021-80be-0e04897bf16a", "node_type": "1", "metadata": {"Header_1": " Introduction", "Header_2": " Why Long Context Matters and How Retrieval Augmentation Steps In:", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "6f58d48e83aa02ac607eef4bf499ea8a573d8b7c3b2a8d3c5b1e8a8a0388086d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "801d1658-8c87-40c0-8216-e90c5d06f80a", "node_type": "1", "metadata": {"Header_1": " Models, Datasets, and Evaluation Metrics"}, "hash": "77732775e35d65bf6349c067697c2c0e3fcda8ce5f4f9c0a2f343c60a65b2311", "class_name": "RelatedNodeInfo"}}, "text": "Prior Research and the NVIDIA Divergence:\n\nInterestingly, while NVIDIA\u2019s findings are interesting in many respects,\nAnother recent work by [ Bai et al. (2023) ](https://arxiv.org/abs/2308.14508)\nalso ventured into similar territory, although with differing outcomes.\n\nTheir work explored the impact of retrieval on long context LLMs, evaluating\nmodels like GPT-3.5-Turbo-16k and Llama2\u20137B-chat-4k. However, their findings\ndiverge from NVIDIA\u2019s in crucial ways. [ Bai et al.\n](https://arxiv.org/abs/2308.14508) discerned that retrieval was beneficial\nonly for the Llama2\u20137B-chat-4k with a 4K context window, but not for extended\ncontext models like GPT-3.5-Turbo-16k. One hypothesis for this difference\ncenters on the challenges tied to experiments using black-box APIs and the\nsmaller white-box LLMs they employed, which potentially had limited capability\nto integrate context through retrieval.\n\nNVIDIA\u2019s work distinguishes itself by tapping into much larger LLMs, yielding\nresults that not only match top-tier models like ChatGPT-3.5 but even indicate\nfurther enhancements when incorporating retrieval methods.", "mimetype": "text/plain", "start_char_idx": 1151, "end_char_idx": 2263, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "801d1658-8c87-40c0-8216-e90c5d06f80a": {"__data__": {"id_": "801d1658-8c87-40c0-8216-e90c5d06f80a", "embedding": null, "metadata": {"Header_1": " Models, Datasets, and Evaluation Metrics", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e", "node_type": "4", "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "e2993ef729906ce47697cc74b4c536035ef113ec10650503e6789e1ae38e795e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6626b5e7-5e91-4cef-bb74-b0069658f4ca", "node_type": "1", "metadata": {"Header_1": " Introduction", "Header_2": " Prior Research and the NVIDIA Divergence:", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "56d78a57818b1a83fcedf490350559691e1bf4b788b397707b58eaf961a03168", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea2a0280-57ad-4910-9c55-288d3ec5078b", "node_type": "1", "metadata": {"Header_1": " Models, Datasets, and Evaluation Metrics", "Header_2": " Large Language Models (LLMs) Explored:"}, "hash": "35d9da7d49a1a9fa75c935e272b09028d26a3c0b8e85b5c9efdc72c7cd16809d", "class_name": "RelatedNodeInfo"}}, "text": "Models, Datasets, and Evaluation Metrics", "mimetype": "text/plain", "start_char_idx": 2268, "end_char_idx": 2308, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea2a0280-57ad-4910-9c55-288d3ec5078b": {"__data__": {"id_": "ea2a0280-57ad-4910-9c55-288d3ec5078b", "embedding": null, "metadata": {"Header_1": " Models, Datasets, and Evaluation Metrics", "Header_2": " Large Language Models (LLMs) Explored:", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e", "node_type": "4", "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "e2993ef729906ce47697cc74b4c536035ef113ec10650503e6789e1ae38e795e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "801d1658-8c87-40c0-8216-e90c5d06f80a", "node_type": "1", "metadata": {"Header_1": " Models, Datasets, and Evaluation Metrics", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "4b13d9e5b1b4742c58632e7bf9d3c92f9d557ef4a5b555e97ce9535b2d1fcfa0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c62df18-349c-47a4-9198-2a35a8a80022", "node_type": "1", "metadata": {"Header_1": " Models, Datasets, and Evaluation Metrics", "Header_2": " Context Window Extension:"}, "hash": "12ae3a089d496baefa2dffe753d5e60fd5d730192d73f34885e8b8d495695592", "class_name": "RelatedNodeInfo"}}, "text": "Large Language Models (LLMs) Explored:\n\nThe researchers delved deep into the potential of large language models for\ntasks like generative QA and summarization. Specifically, two models were the\nprimary focus:\n\n  * **Nemo GPT-43B:** A proprietary 43 billion parameter model trained on 1.1T tokens, 70% of which were in English. This model was fed a rich diet of web archives, Wikipedia, Reddit, books, and more. It contains 48 layers and is trained using RoPE embeddings. \n  * **LLaMA2\u201370B:** A publicly available 70B parameter model trained on 2T tokens, primarily in English. It\u2019s structured with 80 layers and also utilizes RoPE embeddings.", "mimetype": "text/plain", "start_char_idx": 2314, "end_char_idx": 2956, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c62df18-349c-47a4-9198-2a35a8a80022": {"__data__": {"id_": "8c62df18-349c-47a4-9198-2a35a8a80022", "embedding": null, "metadata": {"Header_1": " Models, Datasets, and Evaluation Metrics", "Header_2": " Context Window Extension:", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e", "node_type": "4", "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "e2993ef729906ce47697cc74b4c536035ef113ec10650503e6789e1ae38e795e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea2a0280-57ad-4910-9c55-288d3ec5078b", "node_type": "1", "metadata": {"Header_1": " Models, Datasets, and Evaluation Metrics", "Header_2": " Large Language Models (LLMs) Explored:", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "60e5599e45dfca525696598224f866871334f6a07fde655a3407a8eb1dd9f915", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35ebbfe4-3343-40f3-b6f9-e14da3e9b3c4", "node_type": "1", "metadata": {"Header_1": " Models, Datasets, and Evaluation Metrics", "Header_2": " Instruction Tuning:"}, "hash": "8d10abb12519ccfe7c16b73b33b4d2a656ec6df5ab8302a437d407a70cc44ddb", "class_name": "RelatedNodeInfo"}}, "text": "Context Window Extension:\n\nTo enhance the models\u2019 capability to process longer contexts, their initial 4K\ncontext window length was augmented. The GPT-43B was modified to handle 16K,\nwhile the LLaMA2\u201370B was expanded to both 16K and 32K, employing the position\ninterpolation method.", "mimetype": "text/plain", "start_char_idx": 2963, "end_char_idx": 3245, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35ebbfe4-3343-40f3-b6f9-e14da3e9b3c4": {"__data__": {"id_": "35ebbfe4-3343-40f3-b6f9-e14da3e9b3c4", "embedding": null, "metadata": {"Header_1": " Models, Datasets, and Evaluation Metrics", "Header_2": " Instruction Tuning:", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e", "node_type": "4", "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "e2993ef729906ce47697cc74b4c536035ef113ec10650503e6789e1ae38e795e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c62df18-349c-47a4-9198-2a35a8a80022", "node_type": "1", "metadata": {"Header_1": " Models, Datasets, and Evaluation Metrics", "Header_2": " Context Window Extension:", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "b5dabd30154d0057eb13ddef94d21e4755aa42174ee2ff711d2004202e0956a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a96f01c9-a4cb-414f-86da-7740cb761d9e", "node_type": "1", "metadata": {"Header_1": " Models, Datasets, and Evaluation Metrics", "Header_2": " Retrieval Models Tested:"}, "hash": "debbd7eaedde9a14f0456c0b20bd011156ab03204f262e75b421a10abb2f67c5", "class_name": "RelatedNodeInfo"}}, "text": "Instruction Tuning:\n\nTo optimize the LLMs for the tasks at hand, instruction tuning was\nimplemented. A diverse dataset blend, comprising sources like Soda, ELI5,\nFLAN, and others, was created. A consistent format template was adopted for\nmulti-turn dialogue training, and the models were meticulously fine-tuned to\naccentuate the answer segment.", "mimetype": "text/plain", "start_char_idx": 3251, "end_char_idx": 3596, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a96f01c9-a4cb-414f-86da-7740cb761d9e": {"__data__": {"id_": "a96f01c9-a4cb-414f-86da-7740cb761d9e", "embedding": null, "metadata": {"Header_1": " Models, Datasets, and Evaluation Metrics", "Header_2": " Retrieval Models Tested:", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e", "node_type": "4", "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "e2993ef729906ce47697cc74b4c536035ef113ec10650503e6789e1ae38e795e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35ebbfe4-3343-40f3-b6f9-e14da3e9b3c4", "node_type": "1", "metadata": {"Header_1": " Models, Datasets, and Evaluation Metrics", "Header_2": " Instruction Tuning:", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "6c5d5406d104751080e1fa7fdaa7fd22fe547889892de26ab2143978953d825e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b10cf06e-2151-46b7-8061-a158f528c968", "node_type": "1", "metadata": {"Header_1": " Models, Datasets, and Evaluation Metrics", "Header_2": " Datasets Used for Evaluation:"}, "hash": "42f795acc1d4fc0b45f2be2a28bdfd4dd4216ae75719942799befbe3f1872ab0", "class_name": "RelatedNodeInfo"}}, "text": "Retrieval Models Tested:\n\nThree retrieval systems were put to the test:\n\n  * **Dragon:** A state-of-the-art dual encoder model for both supervised and zero-shot information retrieval. \n  * **Contriever:** Utilizes a basic contrastive learning framework and operates unsupervised. \n  * **OpenAI embedding:** The latest version was used, accepting a maximum input of 8,191 tokens. \n\nThe retrieval approach entailed segmenting each document into 300-word\nsections, encoding both questions and these chunks, and then merging the most\npertinent chunks for response generation.", "mimetype": "text/plain", "start_char_idx": 3602, "end_char_idx": 4173, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b10cf06e-2151-46b7-8061-a158f528c968": {"__data__": {"id_": "b10cf06e-2151-46b7-8061-a158f528c968", "embedding": null, "metadata": {"Header_1": " Models, Datasets, and Evaluation Metrics", "Header_2": " Datasets Used for Evaluation:", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e", "node_type": "4", "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "e2993ef729906ce47697cc74b4c536035ef113ec10650503e6789e1ae38e795e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a96f01c9-a4cb-414f-86da-7740cb761d9e", "node_type": "1", "metadata": {"Header_1": " Models, Datasets, and Evaluation Metrics", "Header_2": " Retrieval Models Tested:", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "f07eedb04a6976066bff1c29a17a0afdae63c26784065b214ad846500fdfa698", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b0bd666-50a6-447b-9202-2a8c1666ed34", "node_type": "1", "metadata": {"Header_1": " Models, Datasets, and Evaluation Metrics", "Header_2": " Evaluation Metrics:"}, "hash": "6523b16fed1d0753d9084846ae85b393637fa1d7cbdf663d73fd219d17a2ce21", "class_name": "RelatedNodeInfo"}}, "text": "Datasets Used for Evaluation:\n\nThe study employed seven diverse datasets, sourced from the Scroll benchmark\nand LongBench.\n\nA snapshot of these datasets includes:\n\n  * **QMSum:** A query-based summarization dataset, QMSum consists of transcripts from diverse meetings and their corresponding summaries, built upon contextual queries. \n  * **Qasper:** A question-answering dataset centered on NLP papers, Qasper offers a mix of abstractive, extractive, yes/no, and unanswerable questions from the Semantic Scholar Open Research Corpus. \n  * **NarrativeQA:** Aimed at question-answering over entire books and movie scripts, NarrativeQA provides question-answer pairs created from summaries of these extensive sources. \n  * **QuALITY:** A multiple-choice question answering set based on stories and articles, QuALITY emphasizes thorough reading, with half the questions designed to be challenging and require careful consideration. \n  * **MuSiQue:** Designed for multi-hop reasoning in question answering, MuSiQue creates multi-hop questions from single-hop ones, emphasizing connected reasoning and minimizing shortcuts. \n  * **HotpotQA:** Based on Wikipedia, HotpotQA requires reading multiple supporting documents for reasoning. It features diverse questions and provides sentence-level support for answers. \n  * **MultiFieldQA-en:** Curated to test long-context understanding across fields, MFQA uses sources like legal documents and academic papers, with annotations done by Ph.D. students.", "mimetype": "text/plain", "start_char_idx": 4179, "end_char_idx": 5671, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b0bd666-50a6-447b-9202-2a8c1666ed34": {"__data__": {"id_": "7b0bd666-50a6-447b-9202-2a8c1666ed34", "embedding": null, "metadata": {"Header_1": " Models, Datasets, and Evaluation Metrics", "Header_2": " Evaluation Metrics:", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e", "node_type": "4", "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "e2993ef729906ce47697cc74b4c536035ef113ec10650503e6789e1ae38e795e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b10cf06e-2151-46b7-8061-a158f528c968", "node_type": "1", "metadata": {"Header_1": " Models, Datasets, and Evaluation Metrics", "Header_2": " Datasets Used for Evaluation:", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "78f6ece6ca0e426f4640e4b90290eadcb64f0542e0fb3b8055936679cf4ead8e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af35a9e3-8861-49b1-b64d-b5987753f2cd", "node_type": "1", "metadata": {"Header_1": " Results"}, "hash": "8baea45424d7446be67210b6d29ec418586e830c952db29348f020cc27ea5fbe", "class_name": "RelatedNodeInfo"}}, "text": "Evaluation Metrics:\n\nThe research team used a wide range of metrics suited to each dataset. The\ngeometric mean of ROUGE scores for QM, the exact matching (EM) score for QLTY,\nand F1 scores for others were the primary metrics.", "mimetype": "text/plain", "start_char_idx": 5678, "end_char_idx": 5903, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "af35a9e3-8861-49b1-b64d-b5987753f2cd": {"__data__": {"id_": "af35a9e3-8861-49b1-b64d-b5987753f2cd", "embedding": null, "metadata": {"Header_1": " Results", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e", "node_type": "4", "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "e2993ef729906ce47697cc74b4c536035ef113ec10650503e6789e1ae38e795e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b0bd666-50a6-447b-9202-2a8c1666ed34", "node_type": "1", "metadata": {"Header_1": " Models, Datasets, and Evaluation Metrics", "Header_2": " Evaluation Metrics:", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "dbc84c65321d5ed2bf5d766ad4ffd14397a62024887e68e75a0f03d645ffdd6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c83cbbcd-2d61-4480-9ab3-08e7ac8a2764", "node_type": "1", "metadata": {"Header_1": " Results", "Header_2": " Comparing to OpenAI Models:"}, "hash": "9a29243eb047fdc19a22337f6cb955da9f754fbfb9454612c81c525fc3b1a02d", "class_name": "RelatedNodeInfo"}}, "text": "Results\n\n  * Baseline models without retrieval, having a 4K sequence length, performed poorly since valuable texts get truncated. \n  * With retrieval, performance for 4K models like LLaMA2\u201370B-4K and GPT-43B-4K significantly improved. \n  * HotpotQA, a multi-hop dataset, particularly benefits from longer sequence models. \n  * Models with longer contexts (16K, 32K) outperform their 4K counterparts even when fed the same evidence chunks. \n  * There exists a unique \u201cU-shaped\u201d performance curve for LLMs due to the ` lost in the middle ` phenomenon, making them better at utilizing information at the beginning or end of the input. \n  * The study presents a contrasting perspective to LongBench\u2019s findings, emphasizing that retrieval is beneficial for models regardless of their context window size.", "mimetype": "text/plain", "start_char_idx": 5908, "end_char_idx": 6707, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c83cbbcd-2d61-4480-9ab3-08e7ac8a2764": {"__data__": {"id_": "c83cbbcd-2d61-4480-9ab3-08e7ac8a2764", "embedding": null, "metadata": {"Header_1": " Results", "Header_2": " Comparing to OpenAI Models:", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e", "node_type": "4", "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "e2993ef729906ce47697cc74b4c536035ef113ec10650503e6789e1ae38e795e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af35a9e3-8861-49b1-b64d-b5987753f2cd", "node_type": "1", "metadata": {"Header_1": " Results", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "3fde1bf501b14e014452e6552c441fbdb33259140fd1c32f5c9c325172b7c140", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50684c4c-84c7-49ff-a814-6e2b09a1645b", "node_type": "1", "metadata": {"Header_1": " Results", "Header_2": " Comparison of Different Retrievers:"}, "hash": "0a88ab8f1ace382615da86dd3bde4c2473737eb6e16c09b524ec6acafc1d15f7", "class_name": "RelatedNodeInfo"}}, "text": "Comparing to OpenAI Models:\n\n  * The LLaMA2\u201370B-32k model with retrieval surpasses the performance of GPT-3.5-turbo variants and is competitive with Davinci-003, underscoring its robustness in handling long context tasks.", "mimetype": "text/plain", "start_char_idx": 6714, "end_char_idx": 6935, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50684c4c-84c7-49ff-a814-6e2b09a1645b": {"__data__": {"id_": "50684c4c-84c7-49ff-a814-6e2b09a1645b", "embedding": null, "metadata": {"Header_1": " Results", "Header_2": " Comparison of Different Retrievers:", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e", "node_type": "4", "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "e2993ef729906ce47697cc74b4c536035ef113ec10650503e6789e1ae38e795e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c83cbbcd-2d61-4480-9ab3-08e7ac8a2764", "node_type": "1", "metadata": {"Header_1": " Results", "Header_2": " Comparing to OpenAI Models:", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "cfc1c611a4e4fa5e8a4c188e451217a294f3261367e6c798bff2b042b0e94e60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3dff4aa-dbe3-4a7f-bc73-f0d0b4f2ee87", "node_type": "1", "metadata": {"Header_1": " Results", "Header_2": " Comparing with the number of retrieved chunks:"}, "hash": "7288dc18caa836e874c5e33c902b2feeb63124d72acdac517039cd7b12ab126b", "class_name": "RelatedNodeInfo"}}, "text": "Comparison of Different Retrievers:\n\n  * Retrieval consistently enhances the performance across different retrievers. \n  * Public retrievers outperformed proprietary ones like OpenAI embeddings.", "mimetype": "text/plain", "start_char_idx": 6942, "end_char_idx": 7136, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3dff4aa-dbe3-4a7f-bc73-f0d0b4f2ee87": {"__data__": {"id_": "e3dff4aa-dbe3-4a7f-bc73-f0d0b4f2ee87", "embedding": null, "metadata": {"Header_1": " Results", "Header_2": " Comparing with the number of retrieved chunks:", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e", "node_type": "4", "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "e2993ef729906ce47697cc74b4c536035ef113ec10650503e6789e1ae38e795e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50684c4c-84c7-49ff-a814-6e2b09a1645b", "node_type": "1", "metadata": {"Header_1": " Results", "Header_2": " Comparison of Different Retrievers:", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "5ab88611746d8bfdd9cb950d3755331525a01b0ed9ef6b46d35b016c5273fad8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95eac5ff-d135-463d-9ee1-f97cd1da8271", "node_type": "1", "metadata": {"Header_1": " Conclusion"}, "hash": "8cc2d51bbf8c11bb182c0f96ac7e220ffebd39b46bd74d09dfdcf0a9e4eeafde", "class_name": "RelatedNodeInfo"}}, "text": "Comparing with the number of retrieved chunks:\n\n  * The best performance is achieved by retrieving the top 5 or 10 chunks. Retrieving more, up to 20 chunks, doesn\u2019t offer additional benefits and can even degrade performance. \n  * The deterioration in performance when adding more chunks could be due to the ` lost-in-the-middle ` phenomenon or the model being sidetracked by non-relevant information.", "mimetype": "text/plain", "start_char_idx": 7143, "end_char_idx": 7543, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95eac5ff-d135-463d-9ee1-f97cd1da8271": {"__data__": {"id_": "95eac5ff-d135-463d-9ee1-f97cd1da8271", "embedding": null, "metadata": {"Header_1": " Conclusion", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e", "node_type": "4", "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "e2993ef729906ce47697cc74b4c536035ef113ec10650503e6789e1ae38e795e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3dff4aa-dbe3-4a7f-bc73-f0d0b4f2ee87", "node_type": "1", "metadata": {"Header_1": " Results", "Header_2": " Comparing with the number of retrieved chunks:", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "8a710e0a940b4ab1d53972cfdeb0acb09aa759b0b59cb3b9074fa90f94e89ded", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd082755-2435-417b-b2c8-ea05886cc649", "node_type": "1", "metadata": {"Header_1": " References:"}, "hash": "5009f324a74c6f9965bf29e8217b871b81f6da51a39baf35d00ea61daa3ff695", "class_name": "RelatedNodeInfo"}}, "text": "Conclusion\n\nAs we delved deep into understanding how retrieval augmentation and long-\ncontext extension interact when applied to leading language models fine-tuned\nfor long-context question-answering and summarization tasks. Here are some\nthings to be noted:\n\n  1. **Boost in Performance with Retrieval** : Implementing retrieval techniques significantly enhances the performance of both shorter 4K context language models and their longer 16K/32K context counterparts. \n  2. **Efficiency of 4K Models with Retrieval** : 4K context language models, when combined with retrieval augmentation, can achieve performance levels similar to 16K long context models. Plus, they have the added advantage of being faster during the inference process. \n  3. **Best Model Performance** : After enhancing with both context window extension and retrieval augmentation, the standout model, LLaMA2\u201370B-32k-ret (LLaMA2\u201370B-32k with retrieval), surpasses well-known models like GPT-3.5-turbo-16k and davinci-003.", "mimetype": "text/plain", "start_char_idx": 7549, "end_char_idx": 8543, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd082755-2435-417b-b2c8-ea05886cc649": {"__data__": {"id_": "cd082755-2435-417b-b2c8-ea05886cc649", "embedding": null, "metadata": {"Header_1": " References:", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e", "node_type": "4", "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "e2993ef729906ce47697cc74b4c536035ef113ec10650503e6789e1ae38e795e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95eac5ff-d135-463d-9ee1-f97cd1da8271", "node_type": "1", "metadata": {"Header_1": " Conclusion", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "7cffbc0b83e836ac6a5d9cb52b707a77125d52c255750aa68df71b254c220957", "class_name": "RelatedNodeInfo"}}, "text": "References:\n\n  1. [ Retrieval meets long context, large language models. ](https://arxiv.org/pdf/2310.03025v1.pdf)\n  2. [ Longbench: A bilingual, multitask benchmark for long context understanding. ](https://arxiv.org/abs/2308.14508)\n\nWe trust that this blog post on the review of the paper on retrieval\naugmentation with long-context LLMs has furnished you with meaningful\ninsights. We\u2019re keen to hear if your experiments align with our findings or\npresent new perspectives \u2014 divergent results always make for interesting\ndiscussions and further exploration.", "mimetype": "text/plain", "start_char_idx": 8549, "end_char_idx": 9108, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"869c34b2-dec3-4336-bc51-b13f04de2219": {"doc_hash": "4c015de05563cf4569c31dfd23edb7e14d46854530637a05a7ff4e6e20d18ee2", "ref_doc_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e"}, "bd8305aa-a331-4021-80be-0e04897bf16a": {"doc_hash": "6f58d48e83aa02ac607eef4bf499ea8a573d8b7c3b2a8d3c5b1e8a8a0388086d", "ref_doc_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e"}, "6626b5e7-5e91-4cef-bb74-b0069658f4ca": {"doc_hash": "56d78a57818b1a83fcedf490350559691e1bf4b788b397707b58eaf961a03168", "ref_doc_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e"}, "801d1658-8c87-40c0-8216-e90c5d06f80a": {"doc_hash": "4b13d9e5b1b4742c58632e7bf9d3c92f9d557ef4a5b555e97ce9535b2d1fcfa0", "ref_doc_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e"}, "ea2a0280-57ad-4910-9c55-288d3ec5078b": {"doc_hash": "60e5599e45dfca525696598224f866871334f6a07fde655a3407a8eb1dd9f915", "ref_doc_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e"}, "8c62df18-349c-47a4-9198-2a35a8a80022": {"doc_hash": "b5dabd30154d0057eb13ddef94d21e4755aa42174ee2ff711d2004202e0956a2", "ref_doc_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e"}, "35ebbfe4-3343-40f3-b6f9-e14da3e9b3c4": {"doc_hash": "6c5d5406d104751080e1fa7fdaa7fd22fe547889892de26ab2143978953d825e", "ref_doc_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e"}, "a96f01c9-a4cb-414f-86da-7740cb761d9e": {"doc_hash": "f07eedb04a6976066bff1c29a17a0afdae63c26784065b214ad846500fdfa698", "ref_doc_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e"}, "b10cf06e-2151-46b7-8061-a158f528c968": {"doc_hash": "78f6ece6ca0e426f4640e4b90290eadcb64f0542e0fb3b8055936679cf4ead8e", "ref_doc_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e"}, "7b0bd666-50a6-447b-9202-2a8c1666ed34": {"doc_hash": "dbc84c65321d5ed2bf5d766ad4ffd14397a62024887e68e75a0f03d645ffdd6f", "ref_doc_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e"}, "af35a9e3-8861-49b1-b64d-b5987753f2cd": {"doc_hash": "3fde1bf501b14e014452e6552c441fbdb33259140fd1c32f5c9c325172b7c140", "ref_doc_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e"}, "c83cbbcd-2d61-4480-9ab3-08e7ac8a2764": {"doc_hash": "cfc1c611a4e4fa5e8a4c188e451217a294f3261367e6c798bff2b042b0e94e60", "ref_doc_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e"}, "50684c4c-84c7-49ff-a814-6e2b09a1645b": {"doc_hash": "5ab88611746d8bfdd9cb950d3755331525a01b0ed9ef6b46d35b016c5273fad8", "ref_doc_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e"}, "e3dff4aa-dbe3-4a7f-bc73-f0d0b4f2ee87": {"doc_hash": "8a710e0a940b4ab1d53972cfdeb0acb09aa759b0b59cb3b9074fa90f94e89ded", "ref_doc_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e"}, "95eac5ff-d135-463d-9ee1-f97cd1da8271": {"doc_hash": "7cffbc0b83e836ac6a5d9cb52b707a77125d52c255750aa68df71b254c220957", "ref_doc_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e"}, "cd082755-2435-417b-b2c8-ea05886cc649": {"doc_hash": "a382a577cb4ea9df64d57fcb9125f42c6c116bf0db6ae8ab92c41f857ff71427", "ref_doc_id": "f83c8c4c-a478-41d0-b9fc-c8ad009bd48e"}}, "docstore/ref_doc_info": {"f83c8c4c-a478-41d0-b9fc-c8ad009bd48e": {"node_ids": ["869c34b2-dec3-4336-bc51-b13f04de2219", "bd8305aa-a331-4021-80be-0e04897bf16a", "6626b5e7-5e91-4cef-bb74-b0069658f4ca", "801d1658-8c87-40c0-8216-e90c5d06f80a", "ea2a0280-57ad-4910-9c55-288d3ec5078b", "8c62df18-349c-47a4-9198-2a35a8a80022", "35ebbfe4-3343-40f3-b6f9-e14da3e9b3c4", "a96f01c9-a4cb-414f-86da-7740cb761d9e", "b10cf06e-2151-46b7-8061-a158f528c968", "7b0bd666-50a6-447b-9202-2a8c1666ed34", "af35a9e3-8861-49b1-b64d-b5987753f2cd", "c83cbbcd-2d61-4480-9ab3-08e7ac8a2764", "50684c4c-84c7-49ff-a814-6e2b09a1645b", "e3dff4aa-dbe3-4a7f-bc73-f0d0b4f2ee87", "95eac5ff-d135-463d-9ee1-f97cd1da8271", "cd082755-2435-417b-b2c8-ea05886cc649"], "metadata": {"Header_1": " Introduction", "filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}}}}