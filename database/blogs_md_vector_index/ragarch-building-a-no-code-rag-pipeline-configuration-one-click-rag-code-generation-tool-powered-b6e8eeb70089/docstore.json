{"docstore/data": {"026e504a-138a-4b5c-b4d3-05242d31202a": {"__data__": {"id_": "026e504a-138a-4b5c-b4d3-05242d31202a", "embedding": null, "metadata": {"filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8", "node_type": "4", "metadata": {"filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "0870c90d0dc438893105a56b3911fad5184dfe184befa6d5854a1b23c2fdc79f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "620ae85c-856b-461e-bdc2-60588be37a23", "node_type": "1", "metadata": {"Header_1": " Features"}, "hash": "b64fea257eb37ce2994baf60a462b07f09aac64ca6bda32dbcfc46ef9c7f3ead", "class_name": "RelatedNodeInfo"}}, "text": "Unlocking the power of AI should be as intuitive as using your favorite apps.\nThat\u2019s the philosophy behind RAGArch, my latest creation designed to demystify\nand streamline the process of setting up Retrieval-Augmented Generation (RAG)\npipelines. This tool is born from a simple vision: to provide a\nstraightforward, no-code platform that empowers both seasoned developers and\ncurious explorers in the world of AI to craft, test, and implement RAG\npipelines with confidence and ease.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 482, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "620ae85c-856b-461e-bdc2-60588be37a23": {"__data__": {"id_": "620ae85c-856b-461e-bdc2-60588be37a23", "embedding": null, "metadata": {"Header_1": " Features", "filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8", "node_type": "4", "metadata": {"filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "0870c90d0dc438893105a56b3911fad5184dfe184befa6d5854a1b23c2fdc79f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "026e504a-138a-4b5c-b4d3-05242d31202a", "node_type": "1", "metadata": {"filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "d87e573b46dd48b9f067286e0b28aa9643fbb2d9ed31f86e1c7054586ecffad7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "429f40d2-472b-4a41-899e-5bdb5581e74b", "node_type": "1", "metadata": {"Header_1": " Tools and Technologies"}, "hash": "78c30251db5220498052c05b3d5b906484fb80c3961afe990cdf219daeb5da32", "class_name": "RelatedNodeInfo"}}, "text": "Features\n\nRAGArch leverages LlamaIndex\u2019s powerful LLM orchestration capabilities, to\nprovide a seamless experience and granular control over your RAG pipeline.\n\n  * **Intuitive Interface:** RAGArch\u2019s user-friendly interface, built with Streamlit, allows you to test different RAG pipeline components interactively. \n  * **Custom Configuration** : The app provides a wide range of options to configure Language Models, Embedding Models, Node Parsers, Response Synthesis Methods, and Vector Stores to suit your project\u2019s needs. \n  * **Live Testing:** Instantly test your RAG pipeline with your own data and see how different configurations affect the outcome. \n  * **One-Click Code Generation** : Once you\u2019re satisfied with the configuration, the app can generate the Python code for your custom RAG pipeline, ready to be integrated into your application.", "mimetype": "text/plain", "start_char_idx": 487, "end_char_idx": 1340, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "429f40d2-472b-4a41-899e-5bdb5581e74b": {"__data__": {"id_": "429f40d2-472b-4a41-899e-5bdb5581e74b", "embedding": null, "metadata": {"Header_1": " Tools and Technologies", "filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8", "node_type": "4", "metadata": {"filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "0870c90d0dc438893105a56b3911fad5184dfe184befa6d5854a1b23c2fdc79f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "620ae85c-856b-461e-bdc2-60588be37a23", "node_type": "1", "metadata": {"Header_1": " Features", "filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "79dca5a4833568946178e915a50242bd08f66fd9752295810af078f681a088bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b42aa9eb-da0e-4e05-bdd9-7f95fafe37f1", "node_type": "1", "metadata": {"Header_1": " Deep Dive into the Code"}, "hash": "f0d2d6617024d2ea3e6ccf3dbc9245c524499106bae23308ed78a60bfdfa0e2b", "class_name": "RelatedNodeInfo"}}, "text": "Tools and Technologies\n\nThe creation of RAGArch was made possible by integrating a variety of powerful\ntools and technologies:\n\n  * **UI:** Streamlit \n  * **Hosting:** Hugging Face Spaces \n  * **LLMs:** OpenAI GPT 3.5 and 4, Cohere API, Gemini Pro \n  * **LLM Orchestration:** Llamaindex \n  * **Embedding Models:** \u201cBAAI/bge-small-en-v1.5\u201d, \u201cWhereIsAI/UAE-Large-V1\u201d, \u201cBAAI/bge-large-en-v1.5\u201d, \u201ckhoa-klaytn/bge-small-en-v1.5-angle\u201d, \u201cBAAI/bge-base-en-v1.5\u201d, \u201cllmrails/ember-v1\u201d, \u201cjamesgpt1/sf_model_e5\u201d, \u201cthenlper/gte-large\u201d, \u201cinfgrad/stella-base-en-v2\u201d and \u201cthenlper/gte-base\u201d \n  * **Vector Stores:** Simple (Llamaindex default), Pinecone and Qdrant", "mimetype": "text/plain", "start_char_idx": 1346, "end_char_idx": 1994, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b42aa9eb-da0e-4e05-bdd9-7f95fafe37f1": {"__data__": {"id_": "b42aa9eb-da0e-4e05-bdd9-7f95fafe37f1", "embedding": null, "metadata": {"Header_1": " Deep Dive into the Code", "filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8", "node_type": "4", "metadata": {"filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "0870c90d0dc438893105a56b3911fad5184dfe184befa6d5854a1b23c2fdc79f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "429f40d2-472b-4a41-899e-5bdb5581e74b", "node_type": "1", "metadata": {"Header_1": " Tools and Technologies", "filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "0dcb5986552448d9110842a567dd47abfbe7fd2d43f0e733cbdbc8b1ef49f763", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33b067e2-edba-4f19-8d1f-de220ba4842d", "node_type": "1", "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " ` **upload_file** `"}, "hash": "15f8c831a288a50425bca9cf08dc52829c1700bea86d0f1d6f34ee60498abd83", "class_name": "RelatedNodeInfo"}}, "text": "Deep Dive into the Code\n\nThe ` app.py ` script is the backbone of RAGArch, integrating various\ncomponents to provide a cohesive experience. The following are the key\nfunctions of app.py", "mimetype": "text/plain", "start_char_idx": 2000, "end_char_idx": 2185, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33b067e2-edba-4f19-8d1f-de220ba4842d": {"__data__": {"id_": "33b067e2-edba-4f19-8d1f-de220ba4842d", "embedding": null, "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " ` **upload_file** `", "filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8", "node_type": "4", "metadata": {"filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "0870c90d0dc438893105a56b3911fad5184dfe184befa6d5854a1b23c2fdc79f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b42aa9eb-da0e-4e05-bdd9-7f95fafe37f1", "node_type": "1", "metadata": {"Header_1": " Deep Dive into the Code", "filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "69c9b470aa034e5887730cf43f1f9c1ad2be7465fe1ecc390d1123f2f7a3e2fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0e714be-3371-40bb-bb17-fe8ec80ba830", "node_type": "1", "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " save_uploaded_file"}, "hash": "0f9d85f8a548fe837e1ab3d0a1766887125822a3e7c198cf56a358fdceaa1f3b", "class_name": "RelatedNodeInfo"}}, "text": "` **upload_file** `\n\nThis function manages file uploads and uses Llamaindex's `\nSimpleDirectoryReader ` to load documents into the system. It supports a wide\narray of document types, including PDFs, text files, HTML, JSON files, and\nmore, making it versatile for processing diverse data sources.\n\n    \n    \n    def upload_file():\n        file = st.file_uploader(\"Upload a file\", on_change=reset_pipeline_generated)\n        if file is not None:\n            file_path = save_uploaded_file(file)\n            \n            if file_path:\n                loaded_file = SimpleDirectoryReader(input_files=[file_path]).load_data()\n                print(f\"Total documents: {len(loaded_file)}\")\n    \n                st.success(f\"File uploaded successfully. Total documents loaded: {len(loaded_file)}\")\n            return loaded_file\n        return None", "mimetype": "text/plain", "start_char_idx": 2191, "end_char_idx": 3031, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0e714be-3371-40bb-bb17-fe8ec80ba830": {"__data__": {"id_": "f0e714be-3371-40bb-bb17-fe8ec80ba830", "embedding": null, "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " save_uploaded_file", "filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8", "node_type": "4", "metadata": {"filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "0870c90d0dc438893105a56b3911fad5184dfe184befa6d5854a1b23c2fdc79f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33b067e2-edba-4f19-8d1f-de220ba4842d", "node_type": "1", "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " ` **upload_file** `", "filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "1157b73c8497e390771669c1dd3fbde40c9aaec3415e87ee98fd36e17941af3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e23897e1-8a03-4d70-b257-62156f87f4a2", "node_type": "1", "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " ` **select_llm** `"}, "hash": "dd52e2403d453366d4e1e01ba577e18cded29fb0142bce069cd97ee795a13d5d", "class_name": "RelatedNodeInfo"}}, "text": "save_uploaded_file\n\nThis utility function saves the uploaded file to a temporary location on the\nserver, making it accessible for further processing. It\u2019s a crucial part of\nthe file handling process, ensuring data integrity and availability.\n\n    \n    \n    def save_uploaded_file(uploaded_file):\n        try:\n            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp_file:\n                tmp_file.write(uploaded_file.getvalue())\n                return tmp_file.name\n        except Exception as e:\n            st.error(f\"Error saving file: {e}\")\n            return None", "mimetype": "text/plain", "start_char_idx": 3037, "end_char_idx": 3662, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e23897e1-8a03-4d70-b257-62156f87f4a2": {"__data__": {"id_": "e23897e1-8a03-4d70-b257-62156f87f4a2", "embedding": null, "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " ` **select_llm** `", "filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8", "node_type": "4", "metadata": {"filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "0870c90d0dc438893105a56b3911fad5184dfe184befa6d5854a1b23c2fdc79f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0e714be-3371-40bb-bb17-fe8ec80ba830", "node_type": "1", "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " save_uploaded_file", "filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "54289efa557521bcdc88647e495f35fe14f80f8ee76fe4b487bd510b4a33fd78", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4dffda1-a92f-4b4b-9a00-17c659915f06", "node_type": "1", "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " ` select_embedding_model `"}, "hash": "87e3c1430545aa42d1e1caf797eae887cdd1939552bee5cfa561bdb281ea30c9", "class_name": "RelatedNodeInfo"}}, "text": "` **select_llm** `\n\nAllows users to select a Large Language Model and initializes it for use. You\ncan choose from Google\u2019s Gemini Pro, Cohere, OpenAI\u2019s GPT 3.5 and GPT 4.\n\n    \n    \n    def select_llm():\n        st.header(\"Choose LLM\")\n        llm_choice = st.selectbox(\"Select LLM\", [\"Gemini\", \"Cohere\", \"GPT-3.5\", \"GPT-4\"], on_change=reset_pipeline_generated)\n        \n        if llm_choice == \"GPT-3.5\":\n            llm = OpenAI(temperature=0.1, model=\"gpt-3.5-turbo-1106\")\n            st.write(f\"{llm_choice} selected\")\n        elif llm_choice == \"GPT-4\":\n            llm = OpenAI(temperature=0.1, model=\"gpt-4-1106-preview\")\n            st.write(f\"{llm_choice} selected\")\n        elif llm_choice == \"Gemini\":\n            llm = Gemini(model=\"models/gemini-pro\")\n            st.write(f\"{llm_choice} selected\")\n        elif llm_choice == \"Cohere\":\n            llm = Cohere(model=\"command\", api_key=os.environ['COHERE_API_TOKEN'])\n            st.write(f\"{llm_choice} selected\")\n        return llm, llm_choice", "mimetype": "text/plain", "start_char_idx": 3668, "end_char_idx": 4677, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4dffda1-a92f-4b4b-9a00-17c659915f06": {"__data__": {"id_": "c4dffda1-a92f-4b4b-9a00-17c659915f06", "embedding": null, "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " ` select_embedding_model `", "filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8", "node_type": "4", "metadata": {"filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "0870c90d0dc438893105a56b3911fad5184dfe184befa6d5854a1b23c2fdc79f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e23897e1-8a03-4d70-b257-62156f87f4a2", "node_type": "1", "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " ` **select_llm** `", "filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "83711d54b93bfb6faede3c2a744008be141a7cad9dc44e74050e208775e27202", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e5cb39d-0c2a-434a-a05a-6130ea67ae6e", "node_type": "1", "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " select_node_parser Function"}, "hash": "0e33d41efa3b4fa9346a192d5945e22fa62efed32f90a023e65f198cf3f2f985", "class_name": "RelatedNodeInfo"}}, "text": "` select_embedding_model `\n\nOffers a dropdown for users to select the embedding model of their choice from\na predefined list. I have included some of the top embedding models from\nHugging Face\u2019s MTEB leaderboard. Near the dropdown I have also included a\nhandy link to the leaderboard where users can get more information about the\nembedding models.\n\n    \n    \n    def select_embedding_model():\n        st.header(\"Choose Embedding Model\")\n        col1, col2 = st.columns([2,1])\n        with col2:\n            st.markdown(\"\"\"\n                        [Embedding Models Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n                        \"\"\")\n        model_names = [\n            \"BAAI/bge-small-en-v1.5\",\n            \"WhereIsAI/UAE-Large-V1\",\n            \"BAAI/bge-large-en-v1.5\",\n            \"khoa-klaytn/bge-small-en-v1.5-angle\",\n            \"BAAI/bge-base-en-v1.5\",\n            \"llmrails/ember-v1\",\n            \"jamesgpt1/sf_model_e5\",\n            \"thenlper/gte-large\",\n            \"infgrad/stella-base-en-v2\",\n            \"thenlper/gte-base\"\n        ]\n        selected_model = st.selectbox(\"Select Embedding Model\", model_names,  on_change=reset_pipeline_generated)\n        with st.spinner(\"Please wait\") as status:\n            embed_model = HuggingFaceEmbedding(model_name=selected_model)\n            st.session_state['embed_model'] = embed_model\n            st.markdown(F\"Embedding Model: {embed_model.model_name}\")\n            st.markdown(F\"Embed Batch Size: {embed_model.embed_batch_size}\")\n            st.markdown(F\"Embed Batch Size: {embed_model.max_length}\")\n    \n    \n        return embed_model, selected_model", "mimetype": "text/plain", "start_char_idx": 4683, "end_char_idx": 6318, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e5cb39d-0c2a-434a-a05a-6130ea67ae6e": {"__data__": {"id_": "9e5cb39d-0c2a-434a-a05a-6130ea67ae6e", "embedding": null, "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " select_node_parser Function", "filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8", "node_type": "4", "metadata": {"filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "0870c90d0dc438893105a56b3911fad5184dfe184befa6d5854a1b23c2fdc79f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4dffda1-a92f-4b4b-9a00-17c659915f06", "node_type": "1", "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " ` select_embedding_model `", "filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "eda627819551d78fc42aef85cbd0595aaf030db51d0f2a090606f0fb130b5f5d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c34aca2-8d73-4aca-a1cb-aa8f1afb3403", "node_type": "1", "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " ` select_response_synthesis_method `"}, "hash": "b191415bc81bdc38781615f783f7826cb9aa723b675307d2082baa48533590a9", "class_name": "RelatedNodeInfo"}}, "text": "select_node_parser Function\n\nThis function allows users to choose a node parser, which is instrumental in\nbreaking down documents into manageable chunks or nodes, facilitating better\nhandling and processing. I have included some of the most commonly used node\nparsers supported by Llamaindex, which include SentenceSplitter, CodeSplitter,\nSemanticSplitterNodeParser, TokenTextSplitter, HTMLNodeParser, JSONNodeParser\nand MarkdownNodeParser.\n\n    \n    \n    def select_node_parser():\n        st.header(\"Choose Node Parser\")\n        col1, col2 = st.columns([4,1])\n        with col2:\n            st.markdown(\"\"\"\n                        [More Information](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/root.html)\n                        \"\"\")\n        parser_types = [\"SentenceSplitter\", \"CodeSplitter\", \"SemanticSplitterNodeParser\",\n                        \"TokenTextSplitter\", \"HTMLNodeParser\", \"JSONNodeParser\", \"MarkdownNodeParser\"]\n        parser_type = st.selectbox(\"Select Node Parser\", parser_types, on_change=reset_pipeline_generated)\n        \n        parser_params = {}\n        if parser_type == \"HTMLNodeParser\":\n            tags = st.text_input(\"Enter tags separated by commas\", \"p, h1\")\n            tag_list = tags.split(',')\n            parser = HTMLNodeParser(tags=tag_list)\n            parser_params = {'tags': tag_list}\n            \n        elif parser_type == \"JSONNodeParser\":\n            parser = JSONNodeParser()\n            \n        elif parser_type == \"MarkdownNodeParser\":\n            parser = MarkdownNodeParser()\n            \n        elif parser_type == \"CodeSplitter\":\n            language = st.text_input(\"Language\", \"python\")\n            chunk_lines = st.number_input(\"Chunk Lines\", min_value=1, value=40)\n            chunk_lines_overlap = st.number_input(\"Chunk Lines Overlap\", min_value=0, value=15)\n            max_chars = st.number_input(\"Max Chars\", min_value=1, value=1500)\n            parser = CodeSplitter(language=language, chunk_lines=chunk_lines, chunk_lines_overlap=chunk_lines_overlap, max_chars=max_chars)\n            parser_params = {'language': language, 'chunk_lines': chunk_lines, 'chunk_lines_overlap': chunk_lines_overlap, 'max_chars': max_chars}\n            \n        elif parser_type == \"SentenceSplitter\":\n            chunk_size = st.number_input(\"Chunk Size\", min_value=1, value=1024)\n            chunk_overlap = st.number_input(\"Chunk Overlap\", min_value=0, value=20)\n            parser = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n            parser_params = {'chunk_size': chunk_size, 'chunk_overlap': chunk_overlap}\n            \n        elif parser_type == \"SemanticSplitterNodeParser\":\n            if 'embed_model' not in st.session_state:\n                st.warning(\"Please select an embedding model first.\")\n                return None, None\n            \n            embed_model = st.session_state['embed_model']\n            buffer_size = st.number_input(\"Buffer Size\", min_value=1, value=1)\n            breakpoint_percentile_threshold = st.number_input(\"Breakpoint Percentile Threshold\", min_value=0, max_value=100, value=95)\n            parser = SemanticSplitterNodeParser(buffer_size=buffer_size, breakpoint_percentile_threshold=breakpoint_percentile_threshold, embed_model=embed_model)\n            parser_params = {'buffer_size': buffer_size, 'breakpoint_percentile_threshold': breakpoint_percentile_threshold}\n            \n        elif parser_type == \"TokenTextSplitter\":\n            chunk_size = st.number_input(\"Chunk Size\", min_value=1, value=1024)\n            chunk_overlap = st.number_input(\"Chunk Overlap\", min_value=0, value=20)\n            parser = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n            parser_params = {'chunk_size': chunk_size, 'chunk_overlap': chunk_overlap}\n    \n        # Save the parser type and parameters to the session state\n        st.session_state['node_parser_type'] = parser_type\n        st.session_state['node_parser_params'] = parser_params\n        \n        return parser, parser_type\n\nBelow the node parser selection, I have also included a preview of the first\nnode of the text after splitting/parsing, just to give the users an idea of\nhow the chunking is actually happening based the selected node parser and the\nrelevant parameters.", "mimetype": "text/plain", "start_char_idx": 6324, "end_char_idx": 10637, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c34aca2-8d73-4aca-a1cb-aa8f1afb3403": {"__data__": {"id_": "5c34aca2-8d73-4aca-a1cb-aa8f1afb3403", "embedding": null, "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " ` select_response_synthesis_method `", "filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8", "node_type": "4", "metadata": {"filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "0870c90d0dc438893105a56b3911fad5184dfe184befa6d5854a1b23c2fdc79f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e5cb39d-0c2a-434a-a05a-6130ea67ae6e", "node_type": "1", "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " select_node_parser Function", "filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "d3482f6953d2fc7277aedd0ed988a12d60744af42c5ff04e297d22b895509885", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71edcea7-3a1d-448e-96c8-c2d83b5ba671", "node_type": "1", "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " ` select_vector_store `"}, "hash": "61c80bd79ee37af84e2ccb1ea7e3f674c87393aff9b6dc8143b9800dffc1a7bd", "class_name": "RelatedNodeInfo"}}, "text": "` select_response_synthesis_method `\n\nThis function allows users to choose how the RAG pipeline synthesizes\nresponses. I have included varioud response synthesis methods supported by\nLlamaindex including _refine_ , _tree_summarize_ , _compact_ ,\n_simple_summarize_ , _accumulate_ and _compact_accumulate._\n\nUsers can click on the more information link to get more details about\nresponse synthesis and the different types.\n\n    \n    \n    def select_response_synthesis_method():\n        st.header(\"Choose Response Synthesis Method\")\n        col1, col2 = st.columns([4,1])\n        with col2:\n            st.markdown(\"\"\"\n                        [More Information](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/response_synthesizers.html)\n                        \"\"\")\n        response_modes = [\n            \"refine\",\n            \"tree_summarize\",  \n            \"compact\", \n            \"simple_summarize\", \n            \"accumulate\", \n            \"compact_accumulate\"\n        ]\n        selected_mode = st.selectbox(\"Select Response Mode\", response_modes, on_change=reset_pipeline_generated)\n        response_mode = selected_mode\n        return response_mode, selected_mode", "mimetype": "text/plain", "start_char_idx": 10643, "end_char_idx": 11844, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71edcea7-3a1d-448e-96c8-c2d83b5ba671": {"__data__": {"id_": "71edcea7-3a1d-448e-96c8-c2d83b5ba671", "embedding": null, "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " ` select_vector_store `", "filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8", "node_type": "4", "metadata": {"filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "0870c90d0dc438893105a56b3911fad5184dfe184befa6d5854a1b23c2fdc79f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c34aca2-8d73-4aca-a1cb-aa8f1afb3403", "node_type": "1", "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " ` select_response_synthesis_method `", "filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "4d726d1f4f37b699c9f58316925bf069cf8c05f49379e556201946f3dea636d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6f8bbf4-ca55-4cb5-a3ee-e20f0293f758", "node_type": "1", "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " generate_rag_pipeline Function"}, "hash": "f8470ad34bd974c0837a0c21762ad8f444b631b0ff62a58bface3980324c4c87", "class_name": "RelatedNodeInfo"}}, "text": "` select_vector_store `\n\nEnables users to choose a vector store, which is a critical component for\nstoring and retrieving embeddings in the RAG pipeline. This function supports\nthe selection from multiple vector store options including Simple (Llamaindex\ndefault), Pinecone and Qdrant.\n\n    \n    \n    def select_vector_store():\n        st.header(\"Choose Vector Store\")\n        vector_stores = [\"Simple\", \"Pinecone\", \"Qdrant\"]\n        selected_store = st.selectbox(\"Select Vector Store\", vector_stores, on_change=reset_pipeline_generated)\n    \n        vector_store = None\n        if selected_store == \"Pinecone\":\n            pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\n            index = pc.Index(\"test\")\n            vector_store = PineconeVectorStore(pinecone_index=index)\n        elif selected_store == \"Qdrant\":\n            client = qdrant_client.QdrantClient(location=\":memory:\")\n            vector_store = QdrantVectorStore(client=client, collection_name=\"sampledata\")\n        st.write(selected_store)\n        return vector_store, selected_store", "mimetype": "text/plain", "start_char_idx": 11850, "end_char_idx": 12910, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6f8bbf4-ca55-4cb5-a3ee-e20f0293f758": {"__data__": {"id_": "f6f8bbf4-ca55-4cb5-a3ee-e20f0293f758", "embedding": null, "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " generate_rag_pipeline Function", "filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8", "node_type": "4", "metadata": {"filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "0870c90d0dc438893105a56b3911fad5184dfe184befa6d5854a1b23c2fdc79f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71edcea7-3a1d-448e-96c8-c2d83b5ba671", "node_type": "1", "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " ` select_vector_store `", "filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "a2f6c5c1a15383c0fc6e199ce8fc6e03812819567882785484756fcfb8e5bbe8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f37fc77-a2c2-4d5b-8de1-c1a53ade7574", "node_type": "1", "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " generate_code_snippet Function"}, "hash": "85b4b22ddecbd05fe24efecb0744d46c859f2b6054217d27b5cac6b82e855093", "class_name": "RelatedNodeInfo"}}, "text": "generate_rag_pipeline Function\n\nThis core function ties together the selected components to generate a RAG\npipeline. It initializes the pipeline with the chosen LLM, embedding model,\nnode parser, response synthesis method, and vector store. It is triggered by\npressing the \u2018Generate RAG Pipeline\u2019 button.\n\n    \n    \n    def generate_rag_pipeline(file, llm, embed_model, node_parser, response_mode, vector_store):\n        if vector_store is not None:\n            # Set storage context if vector_store is not None\n            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n        else:\n            storage_context = None\n    \n        # Create the service context\n        service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, node_parser=node_parser)\n    \n        # Create the vector index\n        vector_index = VectorStoreIndex.from_documents(documents=file, storage_context=storage_context, service_context=service_context, show_progress=True)\n        if storage_context:\n            vector_index.storage_context.persist(persist_dir=\"persist_dir\")\n    \n        # Create the query engine\n        query_engine = vector_index.as_query_engine(\n            response_mode=response_mode,\n            verbose=True,\n        )\n    \n        return query_engine", "mimetype": "text/plain", "start_char_idx": 12916, "end_char_idx": 14220, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f37fc77-a2c2-4d5b-8de1-c1a53ade7574": {"__data__": {"id_": "4f37fc77-a2c2-4d5b-8de1-c1a53ade7574", "embedding": null, "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " generate_code_snippet Function", "filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8", "node_type": "4", "metadata": {"filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "0870c90d0dc438893105a56b3911fad5184dfe184befa6d5854a1b23c2fdc79f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f6f8bbf4-ca55-4cb5-a3ee-e20f0293f758", "node_type": "1", "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " generate_rag_pipeline Function", "filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "4b7e8506cbd2fd8343152f34d3e8998a38cc0512d85f28e30cfcc9442cef24a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0554331-1e52-4ba7-8b64-665229544185", "node_type": "1", "metadata": {"Header_1": " Conclusion"}, "hash": "fdd332d6dd86c41a0ef467c27c7188ae932b6cbad9bd05ac7424362adcc82b5d", "class_name": "RelatedNodeInfo"}}, "text": "generate_code_snippet Function\n\nThis function is the culmination of the user\u2019s selections, generating the\nPython code necessary to implement the configured RAG pipeline. It dynamically\nconstructs the code snippet based on the chosen LLM, embedding model, node\nparser, response synthesis method, and vector store, including the parameters\nset for the node parser.\n\n    \n    \n    def generate_code_snippet(llm_choice, embed_model_choice, node_parser_choice, response_mode, vector_store_choice):\n        node_parser_params = st.session_state.get('node_parser_params', {})\n        print(node_parser_params)\n        code_snippet = \"from llama_index.llms import OpenAI, Gemini, Cohere\\n\"\n        code_snippet += \"from llama_index.embeddings import HuggingFaceEmbedding\\n\"\n        code_snippet += \"from llama_index import ServiceContext, VectorStoreIndex, StorageContext\\n\"\n        code_snippet += \"from llama_index.node_parser import SentenceSplitter, CodeSplitter, SemanticSplitterNodeParser, TokenTextSplitter\\n\"\n        code_snippet += \"from llama_index.node_parser.file import HTMLNodeParser, JSONNodeParser, MarkdownNodeParser\\n\"\n        code_snippet += \"from llama_index.vector_stores import MilvusVectorStore, QdrantVectorStore\\n\"\n        code_snippet += \"import qdrant_client\\n\\n\"\n    \n        # LLM initialization\n        if llm_choice == \"GPT-3.5\":\n            code_snippet += \"llm = OpenAI(temperature=0.1, model='gpt-3.5-turbo-1106')\\n\"\n        elif llm_choice == \"GPT-4\":\n            code_snippet += \"llm = OpenAI(temperature=0.1, model='gpt-4-1106-preview')\\n\"\n        elif llm_choice == \"Gemini\":\n            code_snippet += \"llm = Gemini(model='models/gemini-pro')\\n\"\n        elif llm_choice == \"Cohere\":\n            code_snippet += \"llm = Cohere(model='command', api_key='&lt;YOUR_API_KEY&gt;')  # Replace &lt;YOUR_API_KEY&gt; with your actual API key\\n\"\n    \n        # Embedding model initialization\n        code_snippet += f\"embed_model = HuggingFaceEmbedding(model_name='{embed_model_choice}')\\n\\n\"\n    \n        # Node parser initialization\n        node_parsers = {\n            \"SentenceSplitter\": f\"SentenceSplitter(chunk_size={node_parser_params.get('chunk_size', 1024)}, chunk_overlap={node_parser_params.get('chunk_overlap', 20)})\",\n            \"CodeSplitter\": f\"CodeSplitter(language={node_parser_params.get('language', 'python')}, chunk_lines={node_parser_params.get('chunk_lines', 40)}, chunk_lines_overlap={node_parser_params.get('chunk_lines_overlap', 15)}, max_chars={node_parser_params.get('max_chars', 1500)})\",\n            \"SemanticSplitterNodeParser\": f\"SemanticSplitterNodeParser(buffer_size={node_parser_params.get('buffer_size', 1)}, breakpoint_percentile_threshold={node_parser_params.get('breakpoint_percentile_threshold', 95)}, embed_model=embed_model)\",\n            \"TokenTextSplitter\": f\"TokenTextSplitter(chunk_size={node_parser_params.get('chunk_size', 1024)}, chunk_overlap={node_parser_params.get('chunk_overlap', 20)})\",\n            \"HTMLNodeParser\": f\"HTMLNodeParser(tags={node_parser_params.get('tags', ['p', 'h1'])})\",  \n            \"JSONNodeParser\": \"JSONNodeParser()\",\n            \"MarkdownNodeParser\": \"MarkdownNodeParser()\"\n        }\n        code_snippet += f\"node_parser = {node_parsers[node_parser_choice]}\\n\\n\"\n    \n        # Response mode\n        code_snippet += f\"response_mode = '{response_mode}'\\n\\n\"\n    \n        # Vector store initialization\n        if vector_store_choice == \"Pinecone\":\n            code_snippet += \"pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\\n\"\n            code_snippet += \"index = pc.Index('test')\\n\"\n            code_snippet += \"vector_store = PineconeVectorStore(pinecone_index=index)\\n\"\n        elif vector_store_choice == \"Qdrant\":\n            code_snippet += \"client = qdrant_client.QdrantClient(location=':memory:')\\n\"\n            code_snippet += \"vector_store = QdrantVectorStore(client=client, collection_name='sampledata')\\n\"\n        elif vector_store_choice == \"Simple\":\n            code_snippet += \"vector_store = None  # Simple in-memory vector store selected\\n\"\n    \n        code_snippet += \"\\n# Finalizing the RAG pipeline setup\\n\"\n        code_snippet += \"if vector_store is not None:\\n\"\n        code_snippet += \"    storage_context = StorageContext.from_defaults(vector_store=vector_store)\\n\"\n        code_snippet += \"else:\\n\"\n        code_snippet += \"    storage_context = None\\n\\n\"\n    \n        code_snippet += \"service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, node_parser=node_parser)\\n\\n\"\n    \n        code_snippet += \"_file = 'path_to_your_file'  # Replace with the path to your file\\n\"\n        code_snippet += \"vector_index = VectorStoreIndex.from_documents(documents=_file, storage_context=storage_context, service_context=service_context, show_progress=True)\\n\"\n        code_snippet += \"if storage_context:\\n\"\n        code_snippet += \"    vector_index.storage_context.persist(persist_dir='persist_dir')\\n\\n\"\n    \n        code_snippet += \"query_engine = vector_index.as_query_engine(response_mode=response_mode, verbose=True)\\n\"\n    \n        return code_snippet", "mimetype": "text/plain", "start_char_idx": 14226, "end_char_idx": 19329, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0554331-1e52-4ba7-8b64-665229544185": {"__data__": {"id_": "c0554331-1e52-4ba7-8b64-665229544185", "embedding": null, "metadata": {"Header_1": " Conclusion", "filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8", "node_type": "4", "metadata": {"filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "0870c90d0dc438893105a56b3911fad5184dfe184befa6d5854a1b23c2fdc79f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f37fc77-a2c2-4d5b-8de1-c1a53ade7574", "node_type": "1", "metadata": {"Header_1": " Deep Dive into the Code", "Header_2": " generate_code_snippet Function", "filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "e496761fa50786146df9703b4d3109095c6f8139597b4ef907c002e5c216bf27", "class_name": "RelatedNodeInfo"}}, "text": "Conclusion\n\nRAGArch stands at the intersection of innovation and practicality, offering a\nstreamlined no-code approach to RAG pipeline development. It\u2019s designed to\ndemystify the complexities of AI configurations. With RAGArch, both seasoned\ndevelopers and AI enthusiasts can craft custom pipelines with ease,\naccelerating the journey from idea to implementation.\n\nYour insights and contributions are invaluable as I continue to evolve this\ntool. Check out RAGArch on Github and let\u2019s start a conversation on Linkedin.\nI\u2019m always eager to collaborate and share knowledge with fellow tech\nadventurers.\n\n[ GitHub Repo ](https://github.com/AI-ANK/RAGArch)\n\n[ Connect with Me on LinkedIn\n](https://www.linkedin.com/in/harshadsuryawanshi/)\n\n[ Live Demo ](https://huggingface.co/spaces/AI-ANK/RAGArch)", "mimetype": "text/plain", "start_char_idx": 19334, "end_char_idx": 20129, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"026e504a-138a-4b5c-b4d3-05242d31202a": {"doc_hash": "d87e573b46dd48b9f067286e0b28aa9643fbb2d9ed31f86e1c7054586ecffad7", "ref_doc_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8"}, "620ae85c-856b-461e-bdc2-60588be37a23": {"doc_hash": "79dca5a4833568946178e915a50242bd08f66fd9752295810af078f681a088bd", "ref_doc_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8"}, "429f40d2-472b-4a41-899e-5bdb5581e74b": {"doc_hash": "0dcb5986552448d9110842a567dd47abfbe7fd2d43f0e733cbdbc8b1ef49f763", "ref_doc_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8"}, "b42aa9eb-da0e-4e05-bdd9-7f95fafe37f1": {"doc_hash": "69c9b470aa034e5887730cf43f1f9c1ad2be7465fe1ecc390d1123f2f7a3e2fe", "ref_doc_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8"}, "33b067e2-edba-4f19-8d1f-de220ba4842d": {"doc_hash": "1157b73c8497e390771669c1dd3fbde40c9aaec3415e87ee98fd36e17941af3c", "ref_doc_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8"}, "f0e714be-3371-40bb-bb17-fe8ec80ba830": {"doc_hash": "54289efa557521bcdc88647e495f35fe14f80f8ee76fe4b487bd510b4a33fd78", "ref_doc_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8"}, "e23897e1-8a03-4d70-b257-62156f87f4a2": {"doc_hash": "83711d54b93bfb6faede3c2a744008be141a7cad9dc44e74050e208775e27202", "ref_doc_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8"}, "c4dffda1-a92f-4b4b-9a00-17c659915f06": {"doc_hash": "eda627819551d78fc42aef85cbd0595aaf030db51d0f2a090606f0fb130b5f5d", "ref_doc_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8"}, "9e5cb39d-0c2a-434a-a05a-6130ea67ae6e": {"doc_hash": "d3482f6953d2fc7277aedd0ed988a12d60744af42c5ff04e297d22b895509885", "ref_doc_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8"}, "5c34aca2-8d73-4aca-a1cb-aa8f1afb3403": {"doc_hash": "4d726d1f4f37b699c9f58316925bf069cf8c05f49379e556201946f3dea636d5", "ref_doc_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8"}, "71edcea7-3a1d-448e-96c8-c2d83b5ba671": {"doc_hash": "a2f6c5c1a15383c0fc6e199ce8fc6e03812819567882785484756fcfb8e5bbe8", "ref_doc_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8"}, "f6f8bbf4-ca55-4cb5-a3ee-e20f0293f758": {"doc_hash": "4b7e8506cbd2fd8343152f34d3e8998a38cc0512d85f28e30cfcc9442cef24a4", "ref_doc_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8"}, "4f37fc77-a2c2-4d5b-8de1-c1a53ade7574": {"doc_hash": "e496761fa50786146df9703b4d3109095c6f8139597b4ef907c002e5c216bf27", "ref_doc_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8"}, "c0554331-1e52-4ba7-8b64-665229544185": {"doc_hash": "efdd9ade57d01875555773f803f8692d2fbd818a31e4c0bdd10d1781342d438e", "ref_doc_id": "b8ccf1a3-25b9-48f9-8843-7881fbedfad8"}}, "docstore/ref_doc_info": {"b8ccf1a3-25b9-48f9-8843-7881fbedfad8": {"node_ids": ["026e504a-138a-4b5c-b4d3-05242d31202a", "620ae85c-856b-461e-bdc2-60588be37a23", "429f40d2-472b-4a41-899e-5bdb5581e74b", "b42aa9eb-da0e-4e05-bdd9-7f95fafe37f1", "33b067e2-edba-4f19-8d1f-de220ba4842d", "f0e714be-3371-40bb-bb17-fe8ec80ba830", "e23897e1-8a03-4d70-b257-62156f87f4a2", "c4dffda1-a92f-4b4b-9a00-17c659915f06", "9e5cb39d-0c2a-434a-a05a-6130ea67ae6e", "5c34aca2-8d73-4aca-a1cb-aa8f1afb3403", "71edcea7-3a1d-448e-96c8-c2d83b5ba671", "f6f8bbf4-ca55-4cb5-a3ee-e20f0293f758", "4f37fc77-a2c2-4d5b-8de1-c1a53ade7574", "c0554331-1e52-4ba7-8b64-665229544185"], "metadata": {"filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}}}}