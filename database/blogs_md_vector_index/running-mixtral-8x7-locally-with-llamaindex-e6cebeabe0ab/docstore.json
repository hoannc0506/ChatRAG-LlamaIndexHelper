{"docstore/data": {"6a487131-7719-4252-b25b-4246e23dabf3": {"__data__": {"id_": "6a487131-7719-4252-b25b-4246e23dabf3", "embedding": null, "metadata": {"filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "725ac22e-09f4-4365-8e8b-9c2c01cf0359", "node_type": "4", "metadata": {"filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "hash": "68f12f61c892078b4cd4328018a58573934c863f42aba1d1d4bada744c788a9a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "526299ed-8a9a-45aa-9a2d-8ed1984d1818", "node_type": "1", "metadata": {"Header_1": " Step 1: Install Ollama"}, "hash": "99c1e27f1afb3cace552b7ac38a0856632a356b3d0e78ca52640ddfeb01a636f", "class_name": "RelatedNodeInfo"}}, "text": "You may have heard the fuss about the latest release from European AI\npowerhouse [ Mistral AI ](https://mistral.ai/) : it\u2019s called Mixtral 8x7b, a\n\u201cmixture of experts\u201d model \u2014 eight of them, each trained with 7 billion\nparameters, hence the name. Released originally as a [ mic-drop tweet\n](https://twitter.com/MistralAI/status/1733150512395038967) they followed up a\nfew days later with a [ blog post ](https://mistral.ai/news/mixtral-of-\nexperts/) that showed it matching or exceeding GPT-3.5 as well as the much\nlarger Llama2 70b on a number of benchmarks.\n\nHere at LlamaIndex we\u2019re naturally fans of open source software, so open\nmodels with permissive licenses like Mixtral are right up our alley. We\u2019ve had\na few questions about how to get Mixtral working with LlamaIndex, so this post\nis here to get you up and running with a totally local model.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 853, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "526299ed-8a9a-45aa-9a2d-8ed1984d1818": {"__data__": {"id_": "526299ed-8a9a-45aa-9a2d-8ed1984d1818", "embedding": null, "metadata": {"Header_1": " Step 1: Install Ollama", "filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "725ac22e-09f4-4365-8e8b-9c2c01cf0359", "node_type": "4", "metadata": {"filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "hash": "68f12f61c892078b4cd4328018a58573934c863f42aba1d1d4bada744c788a9a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a487131-7719-4252-b25b-4246e23dabf3", "node_type": "1", "metadata": {"filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "hash": "d142e540fba79bf287407abb962a8138540306d9564164c63c391c267e334c43", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66030d39-d4a1-4db4-85bc-e2644d808a6e", "node_type": "1", "metadata": {"Header_1": " Step 2: Install your dependencies"}, "hash": "25a2161ea5f2df02db8bd47e8fda2d6d1ff01217432ade39b4fb04a9b33e05bb", "class_name": "RelatedNodeInfo"}}, "text": "Step 1: Install Ollama\n\nPreviously getting a local model installed and working was a huge pain, but\nwith the release of [ Ollama ](https://ollama.ai/) , it\u2019s suddenly a snap!\nAvailable for MacOS and Linux (and soon on Windows, though you can use it\ntoday on Windows via [ Windows Subsystem For Linux\n](https://learn.microsoft.com/en-us/windows/wsl/install) ), it is itself open\nsource and a [ free download ](https://ollama.ai/download) .\n\nOnce downloaded, you can get Mixtral with a single command:\n\n    \n    \n    ollama run mixtral\n\nThe first time you run this command it will have to download the model, which\ncan take a long time, so go get a snack. Also note that it requires a hefty\n48GB of RAM to run smoothly! If that\u2019s too much for your machine, consider\nusing its smaller but still very capable cousin **Mistral 7b** , which you\ninstall and run the same way:\n\n    \n    \n    ollama run mistral\n\nWe\u2019ll assume you\u2019re using Mixtral for the rest of this tutorial, but Mistral\nwill also work.\n\nOnce the model is running Ollama will automatically let you chat with it.\nThat\u2019s fun, but what\u2019s the point of having a model if it can\u2019t work with your\ndata? That\u2019s where LlamaIndex comes in. The next few steps will take you\nthrough the code line by line, but if you\u2019d prefer to save all the copying and\npasting, all of this code is available in an [ open-source repo\n](https://github.com/run-llama/mixtral_ollama) that you can clone to follow\nalong there.", "mimetype": "text/plain", "start_char_idx": 858, "end_char_idx": 2312, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66030d39-d4a1-4db4-85bc-e2644d808a6e": {"__data__": {"id_": "66030d39-d4a1-4db4-85bc-e2644d808a6e", "embedding": null, "metadata": {"Header_1": " Step 2: Install your dependencies", "filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "725ac22e-09f4-4365-8e8b-9c2c01cf0359", "node_type": "4", "metadata": {"filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "hash": "68f12f61c892078b4cd4328018a58573934c863f42aba1d1d4bada744c788a9a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "526299ed-8a9a-45aa-9a2d-8ed1984d1818", "node_type": "1", "metadata": {"Header_1": " Step 1: Install Ollama", "filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "hash": "ce600b6a46f02b2d32cc6cff87d1c1dc0ce62a565f5c388993fb7b8357b1695a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f91a59cd-fcce-45a7-8a44-2ade5a072f00", "node_type": "1", "metadata": {"Header_1": " Step 3: Smoke test"}, "hash": "ae4ce7720714c748b30c1c91ddb761926af6cc624a03d2012d5f5234f6bb1054", "class_name": "RelatedNodeInfo"}}, "text": "Step 2: Install your dependencies\n\nYou\u2019re going to need LlamaIndex installed, obviously! We\u2019ll also get you going\nwith a handful of other dependencies that are about to come in handy:\n\n    \n    \n    pip install llama-index qdrant_client torch transformers", "mimetype": "text/plain", "start_char_idx": 2317, "end_char_idx": 2572, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f91a59cd-fcce-45a7-8a44-2ade5a072f00": {"__data__": {"id_": "f91a59cd-fcce-45a7-8a44-2ade5a072f00", "embedding": null, "metadata": {"Header_1": " Step 3: Smoke test", "filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "725ac22e-09f4-4365-8e8b-9c2c01cf0359", "node_type": "4", "metadata": {"filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "hash": "68f12f61c892078b4cd4328018a58573934c863f42aba1d1d4bada744c788a9a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66030d39-d4a1-4db4-85bc-e2644d808a6e", "node_type": "1", "metadata": {"Header_1": " Step 2: Install your dependencies", "filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "hash": "1cc7a10ce8faac034568efcc1deb94b0cea09cdc046e573ba63d2430fcc95e8e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45e6871d-552e-46cc-919f-fabfc96eae20", "node_type": "1", "metadata": {"Header_1": " Step 4: Load some data and index it"}, "hash": "820ddd645554107b6e23b360a6c8c9d3dfd2f38fdb8c798baa2a161bee219090", "class_name": "RelatedNodeInfo"}}, "text": "Step 3: Smoke test\n\nIf you\u2019ve got Ollama running and LlamaIndex properly installed, the following\nquick script will make sure everything is in order by asking it a quick \u201csmoke\ntest\u201d question in a script all by itself:\n\n    \n    \n    # Just runs .complete to make sure the LLM is listening\n    from llama_index.llms import Ollama\n    \n    llm = Ollama(model=\"mixtral\")\n    response = llm.complete(\"Who is Laurie Voss?\")\n    print(response)", "mimetype": "text/plain", "start_char_idx": 2577, "end_char_idx": 3016, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45e6871d-552e-46cc-919f-fabfc96eae20": {"__data__": {"id_": "45e6871d-552e-46cc-919f-fabfc96eae20", "embedding": null, "metadata": {"Header_1": " Step 4: Load some data and index it", "filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "725ac22e-09f4-4365-8e8b-9c2c01cf0359", "node_type": "4", "metadata": {"filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "hash": "68f12f61c892078b4cd4328018a58573934c863f42aba1d1d4bada744c788a9a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f91a59cd-fcce-45a7-8a44-2ade5a072f00", "node_type": "1", "metadata": {"Header_1": " Step 3: Smoke test", "filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "hash": "8e989518b89149fc5b9da468232f33e1e23ff73c47eaf72973b6537adc2f96b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7da7c67-b1e1-494a-914b-1a31f4f65fec", "node_type": "1", "metadata": {"Header_1": " Verify our index"}, "hash": "685341e940b4f140166a34294579a96648503332f117ee34dc2d41a0dd807e92", "class_name": "RelatedNodeInfo"}}, "text": "Step 4: Load some data and index it\n\nNow you\u2019re ready to load in some real data! You can use any data you want; in\nthis case I\u2019m using a [ small collection of my own tweets\n](https://www.dropbox.com/scl/fi/6sos49fluvfilj3sqcvoj/tinytweets.json?rlkey=qmxlaqp000kmx8zktvaj4u1vh&dl=0)\nwhich you can download, or use your own! We\u2019re going to be storing our data in\nthe nifty, open source [ Qdrant ](https://github.com/qdrant/qdrant) vector\ndatabase (which is why we got you to install it earlier). Create a new python\nfile, and load in all our dependencies:\n\n    \n    \n    from pathlib import Path\n    import qdrant_client\n    from llama_index import (\n        VectorStoreIndex,\n        ServiceContext,\n        download_loader,\n    )\n    from llama_index.llms import Ollama\n    from llama_index.storage.storage_context import StorageContext\n    from llama_index.vector_stores.qdrant import QdrantVectorStore\n\nThen load our tweets out of our JSON file using a nifty JSONReader from [\nLlamaHub ](https://llamahub.ai/l/file-json?from=all) , our convenient\ncollection of open source data connectors. This will give you a pile of\ndocuments ready to be embedded and indexed:\n\n    \n    \n    JSONReader = download_loader(\"JSONReader\")\n    loader = JSONReader()\n    documents = loader.load_data(Path('./data/tinytweets.json'))\n\nGet Qdrant ready for action by initializing it and passing it into a Storage\nContext we\u2019ll be using later:\n\n    \n    \n    client = qdrant_client.QdrantClient(\n        path=\"./qdrant_data\"\n    )\n    vector_store = QdrantVectorStore(client=client, collection_name=\"tweets\")\n    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n\nNow set up our Service Context. We\u2019ll be passing it Mixtral as the LLM so we\ncan test that things are working once we\u2019ve finished indexing; indexing itself\ndoesn\u2019t need Mixtral. By passing ` embed_model=\"local\" ` we\u2019re specifying that\nLlamaIndex will embed your data locally, which is why you needed ` torch ` and\n` transformers ` .\n\n    \n    \n    llm = Ollama(model=\"mixtral\")\n    service_context = ServiceContext.from_defaults(llm=llm,embed_model=\"local\")\n\nNow bring it all together: build the index from the documents you loaded using\nthe service and storage contexts you already set up, and give it a query:\n\n    \n    \n    index = VectorStoreIndex.from_documents(documents,service_context=service_context,storage_context=storage_context)\n    \n    query_engine = index.as_query_engine()\n    response = query_engine.query(\"What does the author think about Star Trek? Give details.\")\n    print(response)\n\nOllama will need to fire up Mixtral to answer the query, which can take a\nlittle while, so be patient! You should get output something like this (but\nwith more details):\n\n    \n    \n    Based on the provided context information, the author has a mixed opinion about Star Trek.", "mimetype": "text/plain", "start_char_idx": 3021, "end_char_idx": 5868, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7da7c67-b1e1-494a-914b-1a31f4f65fec": {"__data__": {"id_": "e7da7c67-b1e1-494a-914b-1a31f4f65fec", "embedding": null, "metadata": {"Header_1": " Verify our index", "filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "725ac22e-09f4-4365-8e8b-9c2c01cf0359", "node_type": "4", "metadata": {"filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "hash": "68f12f61c892078b4cd4328018a58573934c863f42aba1d1d4bada744c788a9a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45e6871d-552e-46cc-919f-fabfc96eae20", "node_type": "1", "metadata": {"Header_1": " Step 4: Load some data and index it", "filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "hash": "70be1e9e3a3279f1415d7a10a0a82392b573d41c929fb5d2361ab39643d57c42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f9f2bb3-0e18-4904-a3dd-cc047445c8fb", "node_type": "1", "metadata": {"Header_1": " Build a little web service"}, "hash": "607c091388eef0c84d2bdbe9aff3696b7a5c26bde54574c7618e52b706c3e15c", "class_name": "RelatedNodeInfo"}}, "text": "Verify our index\n\nNow to prove it\u2019s not all smoke and mirrors, let\u2019s use our pre-built index.\nStart a new python file and load in dependencies again:\n\n    \n    \n    import qdrant_client\n    from llama_index import (\n        VectorStoreIndex,\n        ServiceContext,\n    )\n    from llama_index.llms import Ollama\n    from llama_index.vector_stores.qdrant import QdrantVectorStore\n\nThis time we won\u2019t need to load the data, that\u2019s already done! We will need\nthe Qdrant client and of course Mixtral again:\n\n    \n    \n    client = qdrant_client.QdrantClient(\n        path=\"./qdrant_data\"\n    )\n    vector_store = QdrantVectorStore(client=client, collection_name=\"tweets\")\n    \n    llm = Ollama(model=\"mixtral\")\n    service_context = ServiceContext.from_defaults(llm=llm,embed_model=\"local\")\n\nThis time instead of creating our index from documents we load it directly\nfrom the vector store using ` from_vector_store ` . We\u2019re also passing `\nsimilarity_top_k=20 ` to the query engine; this will mean it will fetch 20\ntweets at a time (the default is 2) to get more context and better answer the\nquestion.\n\n    \n    \n    index = VectorStoreIndex.from_vector_store(vector_store=vector_store,service_context=service_context)\n    query_engine = index.as_query_engine(similarity_top_k=20)\n    response = query_engine.query(\"Does the author like SQL? Give details.\")\n    print(response)", "mimetype": "text/plain", "start_char_idx": 5873, "end_char_idx": 7247, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f9f2bb3-0e18-4904-a3dd-cc047445c8fb": {"__data__": {"id_": "4f9f2bb3-0e18-4904-a3dd-cc047445c8fb", "embedding": null, "metadata": {"Header_1": " Build a little web service", "filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "725ac22e-09f4-4365-8e8b-9c2c01cf0359", "node_type": "4", "metadata": {"filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "hash": "68f12f61c892078b4cd4328018a58573934c863f42aba1d1d4bada744c788a9a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7da7c67-b1e1-494a-914b-1a31f4f65fec", "node_type": "1", "metadata": {"Header_1": " Verify our index", "filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "hash": "65e2546b29bd8305356abbad2b9ac7c984cbb167a4f2ac12bb2479e103b1afb7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "86c66c86-7b19-40c2-a23d-a9de0544111d", "node_type": "1", "metadata": {"Header_1": " You\u2019re done!"}, "hash": "a3a75a24c9928870ff3b1fe57e989c68223f40f26d80f95a4e25382d021ec393", "class_name": "RelatedNodeInfo"}}, "text": "Build a little web service\n\nIt\u2019s no good having an index that just runs as a script! Let\u2019s make an API out\nof this thing. We\u2019ll need two new dependencies:\n\n    \n    \n    pip install flask flask-cors\n\nLoad in our dependencies as before into a new file:\n\n    \n    \n    from flask import Flask, request, jsonify\n    from flask_cors import CORS, cross_origin\n    import qdrant_client\n    from llama_index.llms import Ollama\n    from llama_index import (\n        VectorStoreIndex,\n        ServiceContext,\n    )\n    from llama_index.vector_stores.qdrant import QdrantVectorStore\n\nGet the vector store, the LLM and the index loaded:\n\n    \n    \n    # re-initialize the vector store\n    client = qdrant_client.QdrantClient(\n        path=\"./qdrant_data\"\n    )\n    vector_store = QdrantVectorStore(client=client, collection_name=\"tweets\")\n    \n    # get the LLM again\n    llm = Ollama(model=\"mixtral\")\n    service_context = ServiceContext.from_defaults(llm=llm,embed_model=\"local\")\n    # load the index from the vector store\n    index = VectorStoreIndex.from_vector_store(vector_store=vector_store,service_context=service_context)\n\nSet up a really basic Flask server:\n\n    \n    \n    app = Flask(__name__)\n    cors = CORS(app)\n    app.config['CORS_HEADERS'] = 'Content-Type'\n    \n    # This is just so you can easily tell the app is running\n    @app.route('/')\n    def hello_world():\n        return 'Hello, World!'\n\nAnd add a route that accepts a query (as form data), queries the LLM and\nreturns the response:\n\n    \n    \n    @app.route('/process_form', methods=['POST'])\n    @cross_origin()\n    def process_form():\n        query = request.form.get('query')\n        if query is not None:\n            query_engine = index.as_query_engine(similarity_top_k=20)\n            response = query_engine.query(query)\n            return jsonify({\"response\": str(response)})\n        else:\n            return jsonify({\"error\": \"query field is missing\"}), 400\n    \n    if __name__ == '__main__':\n        app.run()\n\nNote those last two lines, they\u2019re important! ` flask run ` is incompatible\nwith the way LlamaIndex loads dependencies, so you will need to run this API\ndirectly like so (assuming your file is called ` app.py ` )\n\n    \n    \n    python app.py\n\nWith your API up and running, you can use cURL to send a request and verify\nit:\n\n    \n    \n    curl --location '&lt;http://127.0.0.1:5000/process_form&gt;' \\\\\n    --form 'query=\"What does the author think about Star Trek?\"'", "mimetype": "text/plain", "start_char_idx": 7252, "end_char_idx": 9707, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "86c66c86-7b19-40c2-a23d-a9de0544111d": {"__data__": {"id_": "86c66c86-7b19-40c2-a23d-a9de0544111d", "embedding": null, "metadata": {"Header_1": " You\u2019re done!", "filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "725ac22e-09f4-4365-8e8b-9c2c01cf0359", "node_type": "4", "metadata": {"filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "hash": "68f12f61c892078b4cd4328018a58573934c863f42aba1d1d4bada744c788a9a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f9f2bb3-0e18-4904-a3dd-cc047445c8fb", "node_type": "1", "metadata": {"Header_1": " Build a little web service", "filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "hash": "b478251376bae759c5ee3d382a572d705a0cff395cb7652d8c832a11b8dfadb3", "class_name": "RelatedNodeInfo"}}, "text": "You\u2019re done!\n\nWe covered a few things here:\n\n  * Getting Ollama to run Mixtral locally \n  * Using LlamaIndex to query Mixtral 8x7b \n  * Building and querying an index over your data using Qdrant vector store \n  * Wrapping your index into a very simple web API \n  * All open-source, free, and running locally! \n\nI hope this was a fun, quick introduction to running local models with\nLlamaIndex!", "mimetype": "text/plain", "start_char_idx": 9712, "end_char_idx": 10105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"6a487131-7719-4252-b25b-4246e23dabf3": {"doc_hash": "d142e540fba79bf287407abb962a8138540306d9564164c63c391c267e334c43", "ref_doc_id": "725ac22e-09f4-4365-8e8b-9c2c01cf0359"}, "526299ed-8a9a-45aa-9a2d-8ed1984d1818": {"doc_hash": "ce600b6a46f02b2d32cc6cff87d1c1dc0ce62a565f5c388993fb7b8357b1695a", "ref_doc_id": "725ac22e-09f4-4365-8e8b-9c2c01cf0359"}, "66030d39-d4a1-4db4-85bc-e2644d808a6e": {"doc_hash": "1cc7a10ce8faac034568efcc1deb94b0cea09cdc046e573ba63d2430fcc95e8e", "ref_doc_id": "725ac22e-09f4-4365-8e8b-9c2c01cf0359"}, "f91a59cd-fcce-45a7-8a44-2ade5a072f00": {"doc_hash": "8e989518b89149fc5b9da468232f33e1e23ff73c47eaf72973b6537adc2f96b1", "ref_doc_id": "725ac22e-09f4-4365-8e8b-9c2c01cf0359"}, "45e6871d-552e-46cc-919f-fabfc96eae20": {"doc_hash": "70be1e9e3a3279f1415d7a10a0a82392b573d41c929fb5d2361ab39643d57c42", "ref_doc_id": "725ac22e-09f4-4365-8e8b-9c2c01cf0359"}, "e7da7c67-b1e1-494a-914b-1a31f4f65fec": {"doc_hash": "65e2546b29bd8305356abbad2b9ac7c984cbb167a4f2ac12bb2479e103b1afb7", "ref_doc_id": "725ac22e-09f4-4365-8e8b-9c2c01cf0359"}, "4f9f2bb3-0e18-4904-a3dd-cc047445c8fb": {"doc_hash": "b478251376bae759c5ee3d382a572d705a0cff395cb7652d8c832a11b8dfadb3", "ref_doc_id": "725ac22e-09f4-4365-8e8b-9c2c01cf0359"}, "86c66c86-7b19-40c2-a23d-a9de0544111d": {"doc_hash": "4aaa3ccd2198b50f62835eb5839be976b4747e7ca64cf5c2953b0ba1de961b66", "ref_doc_id": "725ac22e-09f4-4365-8e8b-9c2c01cf0359"}}, "docstore/ref_doc_info": {"725ac22e-09f4-4365-8e8b-9c2c01cf0359": {"node_ids": ["6a487131-7719-4252-b25b-4246e23dabf3", "526299ed-8a9a-45aa-9a2d-8ed1984d1818", "66030d39-d4a1-4db4-85bc-e2644d808a6e", "f91a59cd-fcce-45a7-8a44-2ade5a072f00", "45e6871d-552e-46cc-919f-fabfc96eae20", "e7da7c67-b1e1-494a-914b-1a31f4f65fec", "4f9f2bb3-0e18-4904-a3dd-cc047445c8fb", "86c66c86-7b19-40c2-a23d-a9de0544111d"], "metadata": {"filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}}}}