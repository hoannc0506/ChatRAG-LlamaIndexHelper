{"docstore/data": {"2cbe996b-c9fe-449b-85cd-b74ffc184c0d": {"__data__": {"id_": "2cbe996b-c9fe-449b-85cd-b74ffc184c0d", "embedding": null, "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "240202a6-e005-4547-9754-00921aac012c", "node_type": "4", "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "c15c0cc7c3c802851f12ee4a799c82e531e6e57691aead1dd58c81e29dfa8a12", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74f5304e-b062-4c0e-b4ba-1228d49e46c9", "node_type": "1", "metadata": {"Header_1": " Step 1: Figuring out how AWS works"}, "hash": "ab632c3b3486532f5137e88cf9ae6b202b5c05b61646bf0f11f00ae0f7372832", "class_name": "RelatedNodeInfo"}}, "text": "Over the holidays, I was running some retrieval benchmarks with LlamaIndex. I\nfound myself rebuilding an index repeatedly with 30K documents, and finding\nwaiting 10\u201320 minutes each time was too grating.\n\nSo to solve this, issue, I decided to bite the bullet and figure out how to\ndeploy LlamaIndex to AWS, and create a scalable ETL pipeline for indexing my\ndata. This brought the processing time down to around 5 minutes!\n\nProposed system architecture\n\nIf you want to skip the detailed steps, you can jump to the code at the\nfollowing repository:\n\n[ https://github.com/run-llama/llamaindex_aws_ingestion\n](https://github.com/run-llama/llamaindex_aws_ingestion)\n\n**NOTE:** I am not an AWS expert, and had zero experience with it before this\nproject. There are likely ways to improve upon the design I came up with. This\nblog merely documents my first foray into getting a system working on AWS. My\nhope is that this helps other people get started, and opens the door for other\nengineers deploying more scale-able systems.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1020, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "74f5304e-b062-4c0e-b4ba-1228d49e46c9": {"__data__": {"id_": "74f5304e-b062-4c0e-b4ba-1228d49e46c9", "embedding": null, "metadata": {"Header_1": " Step 1: Figuring out how AWS works", "filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "240202a6-e005-4547-9754-00921aac012c", "node_type": "4", "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "c15c0cc7c3c802851f12ee4a799c82e531e6e57691aead1dd58c81e29dfa8a12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2cbe996b-c9fe-449b-85cd-b74ffc184c0d", "node_type": "1", "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "25db95e0c14f34b7088ee1a9338c28f227c10e04811343b736c18ea91b9161da", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71127e13-2b33-41dc-8aa7-611553d692ac", "node_type": "1", "metadata": {"Header_1": " Step 1: Figuring out how AWS works", "Header_2": " Note on how deployments work"}, "hash": "125244a879f763e23cfa672aba324eb6198b716ddd4b2d51d23c48ef45c494da", "class_name": "RelatedNodeInfo"}}, "text": "Step 1: Figuring out how AWS works\n\nTo use AWS effectively, there are several packages and tools that you will\nneed:\n\n  1. [ AWS account signup ](https://portal.aws.amazon.com/billing/signup#/start/email)\n  2. [ Install AWS CLI ](https://docs.aws.amazon.com/eks/latest/userguide/setting-up.html)\n  3. Used to authenticate your AWS account for CLI tools \n  4. [ Install eksctl ](https://eksctl.io/installation/)\n  5. Used to create ` EKS ` clusters easily \n  6. [ Install kubectl ](https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html)\n  7. Used to configure and debug deployments, pods, services, etc. \n  8. [ Install Docker ](https://www.docker.com/products/docker-desktop/)\n\nAs you will see, nearly all AWS deployments revolve around ` yaml ` files that\ndescribe what you are deploying and how they connect together, as well as some\nCLI commands to actually run the deployment.\n\nIf at any time you aren\u2019t sure what\u2019s going on, I found it helpful to visit\nthe AWS dashboard and explore the resources I had actually deployed. Usually,\nyou will want to visit. I had the pages below favourited in AWS. Also,\nremember to set your region properly in the top right!\n\nMy AWS console favourites", "mimetype": "text/plain", "start_char_idx": 1025, "end_char_idx": 2229, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71127e13-2b33-41dc-8aa7-611553d692ac": {"__data__": {"id_": "71127e13-2b33-41dc-8aa7-611553d692ac", "embedding": null, "metadata": {"Header_1": " Step 1: Figuring out how AWS works", "Header_2": " Note on how deployments work", "filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "240202a6-e005-4547-9754-00921aac012c", "node_type": "4", "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "c15c0cc7c3c802851f12ee4a799c82e531e6e57691aead1dd58c81e29dfa8a12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "74f5304e-b062-4c0e-b4ba-1228d49e46c9", "node_type": "1", "metadata": {"Header_1": " Step 1: Figuring out how AWS works", "filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "2030f588c182cfa364e5c8502f278702410041760e84e36cf5b5f0dee228b83e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9de1f0b7-370d-47cc-9fd4-2fc08e96a0b0", "node_type": "1", "metadata": {"Header_1": " Step 1: Figuring out how AWS works", "Header_2": " Helpful CLI Commands"}, "hash": "d365f463502834e888c743e817e26eb8a196a8aed0ec0dda3f57a8a85cfad3aa", "class_name": "RelatedNodeInfo"}}, "text": "Note on how deployments work\n\nFor a majority of deployments, you will typically have\n\n  1. The cluster \n  2. The deployed app, scaled to X replicas \n  3. A load balancer, to balance the incoming requests between X replicas \n\nIn the examples below, most will have a ` yaml ` for the deployed app, a `\nyaml ` for the load balancer, and a command to create the cluster you want to\nrun on.", "mimetype": "text/plain", "start_char_idx": 2235, "end_char_idx": 2620, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9de1f0b7-370d-47cc-9fd4-2fc08e96a0b0": {"__data__": {"id_": "9de1f0b7-370d-47cc-9fd4-2fc08e96a0b0", "embedding": null, "metadata": {"Header_1": " Step 1: Figuring out how AWS works", "Header_2": " Helpful CLI Commands", "filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "240202a6-e005-4547-9754-00921aac012c", "node_type": "4", "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "c15c0cc7c3c802851f12ee4a799c82e531e6e57691aead1dd58c81e29dfa8a12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71127e13-2b33-41dc-8aa7-611553d692ac", "node_type": "1", "metadata": {"Header_1": " Step 1: Figuring out how AWS works", "Header_2": " Note on how deployments work", "filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "e032db32b0916b266ee59bfccbce61ac4984012ad3e78c6caee089bb87bf48d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d31f719-65f4-4e79-bfa3-1de6d985e993", "node_type": "1", "metadata": {"Header_1": " Step 2: Deploying Text Embeddings Interface"}, "hash": "5e029e57fc0e301566a237e6414e35e1c118d70a79bedfdcec0455b98def1444", "class_name": "RelatedNodeInfo"}}, "text": "Helpful CLI Commands\n\nA few CLI commands proved to be extremely helpful for debugging and monitoring\ndeployments.\n\n    \n    \n    # get the state of pods/deployments\n    kubectl get pods\n    kubectl get deployments\n    \n    # useful for seeing logs/events of pods + full yaml config\n    kubectl describe pod <pod name>\n    kubectl logs <pod name>\n    \n    # list clusters kubectl knows about\n    kubectl config get-contexts\n    \n    # switch kubectl to another cluster\n    kubectl config use-context <context name>\n    \n    # delete things\n    kubectl delete <pod/deployment/service> <name>", "mimetype": "text/plain", "start_char_idx": 2626, "end_char_idx": 3215, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d31f719-65f4-4e79-bfa3-1de6d985e993": {"__data__": {"id_": "5d31f719-65f4-4e79-bfa3-1de6d985e993", "embedding": null, "metadata": {"Header_1": " Step 2: Deploying Text Embeddings Interface", "filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "240202a6-e005-4547-9754-00921aac012c", "node_type": "4", "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "c15c0cc7c3c802851f12ee4a799c82e531e6e57691aead1dd58c81e29dfa8a12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9de1f0b7-370d-47cc-9fd4-2fc08e96a0b0", "node_type": "1", "metadata": {"Header_1": " Step 1: Figuring out how AWS works", "Header_2": " Helpful CLI Commands", "filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "caa138837b7a3379b235d06c2a6045c098de71bbe5ec1b5b728b27f5ff1b2064", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8bf36691-b3a4-4ab6-8107-bd5f4307842c", "node_type": "1", "metadata": {"Header_1": " Step 3: Deploying RabbitMQ"}, "hash": "13726fb400a679e7e3a1c54135d5583537d17f98b9bbecbd75115bf9c3375211", "class_name": "RelatedNodeInfo"}}, "text": "Step 2: Deploying Text Embeddings Interface\n\nIn order to run embeddings fast, we will deploy an embeddings server using\nHuggingFace\u2019s [ Text Embedding Interface\n](https://github.com/huggingface/text-embeddings-inference) (TEI). This server\nhas production-level features and optimizations out-of-the-box, including\ncontinuous batching, flash-attention, rust implementation, and more.\nHuggingFace provides prebuilt docker images to simplify deployment.\n\nHowever, the first step to running embeddings fast is to have a GPU. If you\njust signed up for AWS, you will have to request a quota increase. For me, I\nrequested a few times for G5 instances (which run an Nvidia A10G GPU), and\nafter a few days of testing on CPU, AWS gave me access to use up to 4 G5\ninstances.\n\nOnce you have a quota for GPU instances (like G5 nodes), you can create your\ncluster and deploy\n\n    \n    \n    eksctl create cluster --name embeddings --node-type=g5.xlarge --nodes 1\n    sleep 5\n    kubectl create -f ./tei-deployment.yaml\n    sleep 5\n    kubectl create -f ./tei-service.yaml\n    sleep 5\n    echo \"Embeddings URL is: &lt;http://$&gt;(kubectl get svc tei-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\"\n\nThe code above will create a cluster, a deployment (i.e. our TEI server) and a\nload balancer server.\n\nYou can see the yaml configs in [ the repo ](https://github.com/run-\nllama/llamaindex_aws_ingestion/tree/main/tei) , and you can edit them as\nneeded.\n\n**NOTE:** Make sure to write down the URL printed at the end! If you forget,\nyou can get the URL in the ` EKS ` page on AWS. You\u2019ll want the external IP\nfor the load balancer.", "mimetype": "text/plain", "start_char_idx": 3220, "end_char_idx": 4851, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8bf36691-b3a4-4ab6-8107-bd5f4307842c": {"__data__": {"id_": "8bf36691-b3a4-4ab6-8107-bd5f4307842c", "embedding": null, "metadata": {"Header_1": " Step 3: Deploying RabbitMQ", "filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "240202a6-e005-4547-9754-00921aac012c", "node_type": "4", "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "c15c0cc7c3c802851f12ee4a799c82e531e6e57691aead1dd58c81e29dfa8a12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d31f719-65f4-4e79-bfa3-1de6d985e993", "node_type": "1", "metadata": {"Header_1": " Step 2: Deploying Text Embeddings Interface", "filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "fbf923a3b7b4f5958812fc13490f43aac64b6e91cdb5ef4ccfe293b628987b3f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1da429b8-969c-4d63-9362-5a113fbad8ef", "node_type": "1", "metadata": {"Header_1": " Step 4: Deploying IngestionPipeline Workers"}, "hash": "6fc66851d4f5bceadbd6fa99753e862d44426dad74f17d3ee6d4b6478bf1b6be", "class_name": "RelatedNodeInfo"}}, "text": "Step 3: Deploying RabbitMQ\n\nRabbitMQ is where we will queue documents to be ingested. RabbitMQ is a\nmessage broker system that allows for powerful yet simple queuing of tasks.\nSince some ingestion tasks (like metadata extraction, embeddings) can be slow,\nthe more naive approach of a REST API would leave connections open while data\nis processed. Instead, using a queue allows us to quickly upload data and\noffload processing to scalable message consumer(s). It also allows us to add\nparallelism with ease, where in our system, each ` Document ` object is\nprocessed independently by a consumer.\n\nDeploying RabbitMQ on ` EKS ` was a little tricky, but using the RabbitMQ\noperator installed with ` krew ` , many things are abstracted away.\n\nFirst, you need to create your cluster. For whatever reason, this didn\u2019t work\nunless I also specified the zones\n\n    \n    \n    eksctl create cluster \\\n      --name mqCluster \\\n      --zones us-east-1a,us-east-1b,us-east-1c,us-east-1d,us-east-1f\n\nSince RabbitMQ needs storage, and each replica needs to share the same\nstorage, we should give our cluster permission to provision and use ` EBS `\nfor storage. This was a frustrating step to figure out since most existing\nguides skip this detail!\n\n    \n    \n    eksctl utils associate-iam-oidc-provider \\\n      --cluster=mqCluster \\\n      --region us-east-1 \\\n      --approve\n    sleep 5\n    eksctl create iamserviceaccount \\\n        --name ebs-csi-controller-sa \\\n        --namespace kube-system \\\n        --cluster mqCluster \\\n        --role-name AmazonEKS_EBS_CSI_DriverRole \\\n        --role-only \\\n        --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \\\n        --approve\n    sleep 5\n    eksctl create addon \\\n      --name aws-ebs-csi-driver \\\n      --cluster mqCluster \\\n      --service-account-role-arn arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):role/AmazonEKS_EBS_CSI_DriverRole \\\n      --force\n\nFrom there, we can install the RabbitMQ operator and create our deployment\n\n    \n    \n    kubectl apply -f &lt;https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml&gt;\n    sleep 5\n    kubectl apply -f rabbitmqcluster.yaml\n    sleep 5\n    echo \"RabbitMQ URL is: $(kubectl get svc production-rabbitmqcluster -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\"\n\nAs usual, the code for all this can be found in the [ git repo\n](https://github.com/run-llama/llamaindex_aws_ingestion/tree/main/rabbitmq) .\n\n**NOTE:** Make sure to write down the URL printed at the end! If you forget,\nyou can get the URL in the ` EKS ` page on AWS. You\u2019ll want the external IP\nfor the load balancer.\n\nYou can monitor your RabbitMQ queues by visiting \u201c<rabbitmq_url>:15672\u201d and\nsigning in with \u201cguest\u201d/\u201dguest\u201d.", "mimetype": "text/plain", "start_char_idx": 4856, "end_char_idx": 7642, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1da429b8-969c-4d63-9362-5a113fbad8ef": {"__data__": {"id_": "1da429b8-969c-4d63-9362-5a113fbad8ef", "embedding": null, "metadata": {"Header_1": " Step 4: Deploying IngestionPipeline Workers", "filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "240202a6-e005-4547-9754-00921aac012c", "node_type": "4", "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "c15c0cc7c3c802851f12ee4a799c82e531e6e57691aead1dd58c81e29dfa8a12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8bf36691-b3a4-4ab6-8107-bd5f4307842c", "node_type": "1", "metadata": {"Header_1": " Step 3: Deploying RabbitMQ", "filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "a731c5030439ed8ea483756797ef60dc8e0e1ff3419436658eb7bb1644358aeb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da3cba00-95da-44a1-ad56-dafa87002648", "node_type": "1", "metadata": {"Header_1": " Step 5: Making a User-Facing Lambda Function"}, "hash": "c5513afc49391031b20ceb90442a13fefe4b41ec3530a485033006ae85cf9105", "class_name": "RelatedNodeInfo"}}, "text": "Step 4: Deploying IngestionPipeline Workers\n\nThis is where the real meat of work comes in. We need to create a ` consumer `\nthat will endlessly pull from our ` RabbitMQ ` queue, ingest data with the\nhelp of TEI, and then put that data into our vector db.\n\nTo do this, we can make a FastAPI server that does two things\n\n  1. Starts a thread to consume from our queue \n  2. Starts a webserver, to enable us to specify a readiness check, and gives us room to add more features in the future (i.e. probing queue status, logs, etc.) \n\nFirst, we write our code, as you can see in [ worker.py\n](https://github.com/run-\nllama/llamaindex_aws_ingestion/blob/main/worker/worker.py)\n\nThen, we dockerize our app by creating a simple [ Dockerfile\n](https://github.com/run-\nllama/llamaindex_aws_ingestion/blob/main/worker/Dockerfile) and running:\n\n    \n    \n    docker build -t <image_name> .\n    docker tag <image_name>:latest <image_name>:<version>\n    docker push <image_name>:<version>\n\nWith our app dockerized, we can complete the ` worker-deployment.yaml ` file\nby filling in\n\n  * Our embeddings URL under ` TEI_URL `\n  * Our rabbit-mq URL under ` RABBITMQ_URL `\n  * Our image name under container image \n  * Our cluster details (in this case, a weaviate URL and API key) \n\nWith the ` yaml ` file complete, now we can properly deploy the worker\n\n    \n    \n    eksctl create cluster --name mq-workers --zones us-east-1a,us-east-1b,us-east-1c,us-east-1d,us-east-1f\n    sleep 5\n    kubectl create -f ./worker-deployment.yaml\n    sleep 5\n    kubectl create -f ./worker-service.yaml", "mimetype": "text/plain", "start_char_idx": 7647, "end_char_idx": 9215, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da3cba00-95da-44a1-ad56-dafa87002648": {"__data__": {"id_": "da3cba00-95da-44a1-ad56-dafa87002648", "embedding": null, "metadata": {"Header_1": " Step 5: Making a User-Facing Lambda Function", "filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "240202a6-e005-4547-9754-00921aac012c", "node_type": "4", "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "c15c0cc7c3c802851f12ee4a799c82e531e6e57691aead1dd58c81e29dfa8a12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1da429b8-969c-4d63-9362-5a113fbad8ef", "node_type": "1", "metadata": {"Header_1": " Step 4: Deploying IngestionPipeline Workers", "filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "4e9abba8454ff815a50ef642a68e603c303e9633186bc7a5f863462af27d0021", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d30140f-81ab-4cdf-8123-e0567cdca1a4", "node_type": "1", "metadata": {"Header_1": " Step 6: Reap the Scaling Benefits"}, "hash": "8963cb244c29ab4242c16f49c34e807d4e7b1537d82649962563d147907c91bf", "class_name": "RelatedNodeInfo"}}, "text": "Step 5: Making a User-Facing Lambda Function\n\nOur lambda function will rely on a single external dependency \u2014 ` pika ` \u2014\nwhich is used to communicate with RabbitMQ.\n\nCreate a python file called ` lambda_function.py ` with the following code:\n\n    \n    \n    import pika\n    import json\n    \n    def lambda_handler(event, context):\n        try:\n            body = json.loads(event.get('body', '{}'))\n        except:\n            body = event.get('body', {})\n            \n        user = body.get('user', '')\n        documents = body.get('documents', [])\n        if not user or not documents:\n            return {\n                'statusCode': 400,\n                'body': json.dumps('Missing user or documents')\n            }\n        \n        credentials = pika.PlainCredentials(\"guest\", \"guest\")\n        parameters = pika.ConnectionParameters(\n            host=\"hostname.amazonaws.com\", \n            port=5672, \n            credentials=credentials\n        )\n        \n        connection = pika.BlockingConnection(parameters=parameters)\n        channel = connection.channel()\n        channel.queue_declare(queue='etl')\n    \n        for document in documents:\n            data = {\n                'user': user,\n                'documents': [document]\n            }\n            channel.basic_publish(\n                exchange=\"\", \n                routing_key='etl', \n                body=json.dumps(data)\n            )\n    \n        return {\n            'statusCode': 200,\n            'body': json.dumps('Documents queued for ingestion')\n        }\n\nThe function above processes incoming requests, and publishes each document as\na single message in our rabbitmq cluster.\n\nTo deploy a lambda file with dependencies, we need to create a zip of our\nlambda function + all dependencies. To do this, we can create a `\nrequirements.txt ` file with our dependencies and run:\n\n    \n    \n    pip install -r requirements.txt -t .\n    zip -r9 ../ingestion_lambda.zip . -x \"*.git*\" \"*setup.sh*\" \"*requirements.txt*\" \"*.zip*\"\n\nWith our code and zip file in hand, head over to the Lambda AWS page in your\nbrowser.\n\n  1. Select ` Create function `\n  2. Give it a name, select a python runtime (I used Python 3.11) \n  3. Click ` Create function ` at the bottom \n  4. In the code editor, you\u2019ll see an ` Upload from ` button \u2014 click that, and upload your zip file \n  5. Click test, give the test a name, and paste the following JSON \n\n    \n    \n    {\n        \"body\": {\"user\": \"Test\", \"documents\": [{\"text\": \"test\"}]}\n    }\n\nOnce the test works, the ` Deploy ` button will not be grayed out, and you can\nclick it.\n\nYour public URL will be listed in the upper right pane under ` Function URL `\n\u2014 this is the URL you can use to call your lambda function from anywhere!", "mimetype": "text/plain", "start_char_idx": 9220, "end_char_idx": 11958, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d30140f-81ab-4cdf-8123-e0567cdca1a4": {"__data__": {"id_": "2d30140f-81ab-4cdf-8123-e0567cdca1a4", "embedding": null, "metadata": {"Header_1": " Step 6: Reap the Scaling Benefits", "filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "240202a6-e005-4547-9754-00921aac012c", "node_type": "4", "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "c15c0cc7c3c802851f12ee4a799c82e531e6e57691aead1dd58c81e29dfa8a12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da3cba00-95da-44a1-ad56-dafa87002648", "node_type": "1", "metadata": {"Header_1": " Step 5: Making a User-Facing Lambda Function", "filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "f46b1e360dc499f6c5d53db477fcfda27cabd48e07ed104b0fbf5df3860882b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95782508-a676-4937-adbc-b0cc02e1f0b1", "node_type": "1", "metadata": {"Header_1": " Step 7: Clean-up"}, "hash": "2da20741320c3b156dda56b881fd4214f88807c3f912f8634bbeba2fefa84895", "class_name": "RelatedNodeInfo"}}, "text": "Step 6: Reap the Scaling Benefits\n\nNow, we can run our system end-to-end!\n\nTo ingest data, you can run:\n\n    \n    \n    import requests\n    from llama_index import Document, SimpleDirectoryReader\n    \n    documents = SimpleDirectoryReader(\"./data\").load_data()\n    \n    # this will also be the namespace for the vector store \n    # -- for weaviate, it needs to start with a captial and only alpha-numeric\n    user = \"Loganm\" \n    \n    # upload in batches\n    for batch_idx in range(0, len(documents), 30):\n      documents_batch = documents[batch_idx:batch_idx+30]\n      body = {\n        'user': user,\n        'documents': [doc.json() for doc in documents_batch]\n      }\n    \n     # use the URL of our lambda function here\n     response = requests.post(\"&lt;lambda_url&gt;\", json=body)\n     print(response.text)\n\nThen, to use our data:\n\n    \n    \n    from llama_index import VectorStoreIndex\n    from llama_index.vector_stores import WeaviateVectorStore\n    import weaviate\n    \n    auth_config = weaviate.AuthApiKey(api_key=\"...\")\n    client = weaviate.Client(url=\"...\", auth_client_secret=auth_config)\n    vector_store = WeaviateVectorStore(weaviate_client=client, class_prefix=\"&lt;user&gt;\")\n    index = VectorStoreIndex.from_vector_store(vector_store)", "mimetype": "text/plain", "start_char_idx": 11963, "end_char_idx": 13217, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95782508-a676-4937-adbc-b0cc02e1f0b1": {"__data__": {"id_": "95782508-a676-4937-adbc-b0cc02e1f0b1", "embedding": null, "metadata": {"Header_1": " Step 7: Clean-up", "filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "240202a6-e005-4547-9754-00921aac012c", "node_type": "4", "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "c15c0cc7c3c802851f12ee4a799c82e531e6e57691aead1dd58c81e29dfa8a12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2d30140f-81ab-4cdf-8123-e0567cdca1a4", "node_type": "1", "metadata": {"Header_1": " Step 6: Reap the Scaling Benefits", "filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "ffeea667e096720003740ee2ebbd86d5531ed4b91e2557eb07e2a722eb4e7244", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8cf6dc25-15b4-4c0b-a64a-cd89e52642d7", "node_type": "1", "metadata": {"Header_1": " Conclusion"}, "hash": "1a577c6eada9eeb2c24cdc95379318ad5e01ac7577d1d255f4b69088863a0e60", "class_name": "RelatedNodeInfo"}}, "text": "Step 7: Clean-up\n\nAWS doesn\u2019t make it easy to estimate costs of all this. But after running and\ntesting things for a few days, I had only spent ~$40CAD. This included leaving\nsome services running overnight (whoops!).\n\nWhen you are done with your deployment, you\u2019ll want to delete the resources so\nthat you aren\u2019t charged for things you aren\u2019t using. To delete my clusters, I\nran the following:\n\n    \n    \n    eksctl delete cluster embeddings\n    eksctl delete cluster mq-worker\n    kubectl rabbitmq delete production-rabbitmqcluster\n\nThen, in the AWS UI console, I deleted any remaining resources on the ` EC2 `\nand ` CloudFormation ` pages, as well as double-checking that everything was\ndeleted on the ` EKS ` page.", "mimetype": "text/plain", "start_char_idx": 13222, "end_char_idx": 13940, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8cf6dc25-15b4-4c0b-a64a-cd89e52642d7": {"__data__": {"id_": "8cf6dc25-15b4-4c0b-a64a-cd89e52642d7", "embedding": null, "metadata": {"Header_1": " Conclusion", "filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "240202a6-e005-4547-9754-00921aac012c", "node_type": "4", "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "c15c0cc7c3c802851f12ee4a799c82e531e6e57691aead1dd58c81e29dfa8a12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95782508-a676-4937-adbc-b0cc02e1f0b1", "node_type": "1", "metadata": {"Header_1": " Step 7: Clean-up", "filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "f1fe5835af00b2b578537d31e33c47f4bc37a4589663214d8125b0f673ce3a97", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b7ae76b-a9d4-4768-b9f7-c732358552e4", "node_type": "1", "metadata": {"Header_1": " Next Steps"}, "hash": "9a40f34440197dcddc401cde0083c11f6588e246a12a038a42aed3e724fd06b5", "class_name": "RelatedNodeInfo"}}, "text": "Conclusion\n\nUsing this setup, I was able to reduce index-construction times for creating\nlarge indexes dramatically. Before, it would take about 10\u201320 minutes to\ncreate the index for 25K documents, and with this setup (2 rabbitmq nodes, 2\nworkers, 2 embeddings), it was down to 5 minutes! And with more scaling, it\ncould be even faster.", "mimetype": "text/plain", "start_char_idx": 13945, "end_char_idx": 14281, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b7ae76b-a9d4-4768-b9f7-c732358552e4": {"__data__": {"id_": "0b7ae76b-a9d4-4768-b9f7-c732358552e4", "embedding": null, "metadata": {"Header_1": " Next Steps", "filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "240202a6-e005-4547-9754-00921aac012c", "node_type": "4", "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "c15c0cc7c3c802851f12ee4a799c82e531e6e57691aead1dd58c81e29dfa8a12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8cf6dc25-15b4-4c0b-a64a-cd89e52642d7", "node_type": "1", "metadata": {"Header_1": " Conclusion", "filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "98493380306306592e0dd4607b78e6b87ba36eb0ebe5a7ebe2ffb5f8125ec05f", "class_name": "RelatedNodeInfo"}}, "text": "Next Steps\n\nFrom here, there are several improvements that I can think of\n\n  * better secrets management \n  * adding auto-scaling \n  * adding a retrieval lambda function (would require making a docker image for lambda + llama-index) \n  * adding queue stats to the fastapi server \n  * deploying redis for document management on the IngestionPipeline \n\nI encourage anyone to take this work and build off it. Be sure share any\nimprovement on the github repository as well!", "mimetype": "text/plain", "start_char_idx": 14286, "end_char_idx": 14755, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"2cbe996b-c9fe-449b-85cd-b74ffc184c0d": {"doc_hash": "25db95e0c14f34b7088ee1a9338c28f227c10e04811343b736c18ea91b9161da", "ref_doc_id": "240202a6-e005-4547-9754-00921aac012c"}, "74f5304e-b062-4c0e-b4ba-1228d49e46c9": {"doc_hash": "2030f588c182cfa364e5c8502f278702410041760e84e36cf5b5f0dee228b83e", "ref_doc_id": "240202a6-e005-4547-9754-00921aac012c"}, "71127e13-2b33-41dc-8aa7-611553d692ac": {"doc_hash": "e032db32b0916b266ee59bfccbce61ac4984012ad3e78c6caee089bb87bf48d5", "ref_doc_id": "240202a6-e005-4547-9754-00921aac012c"}, "9de1f0b7-370d-47cc-9fd4-2fc08e96a0b0": {"doc_hash": "caa138837b7a3379b235d06c2a6045c098de71bbe5ec1b5b728b27f5ff1b2064", "ref_doc_id": "240202a6-e005-4547-9754-00921aac012c"}, "5d31f719-65f4-4e79-bfa3-1de6d985e993": {"doc_hash": "fbf923a3b7b4f5958812fc13490f43aac64b6e91cdb5ef4ccfe293b628987b3f", "ref_doc_id": "240202a6-e005-4547-9754-00921aac012c"}, "8bf36691-b3a4-4ab6-8107-bd5f4307842c": {"doc_hash": "a731c5030439ed8ea483756797ef60dc8e0e1ff3419436658eb7bb1644358aeb", "ref_doc_id": "240202a6-e005-4547-9754-00921aac012c"}, "1da429b8-969c-4d63-9362-5a113fbad8ef": {"doc_hash": "4e9abba8454ff815a50ef642a68e603c303e9633186bc7a5f863462af27d0021", "ref_doc_id": "240202a6-e005-4547-9754-00921aac012c"}, "da3cba00-95da-44a1-ad56-dafa87002648": {"doc_hash": "f46b1e360dc499f6c5d53db477fcfda27cabd48e07ed104b0fbf5df3860882b5", "ref_doc_id": "240202a6-e005-4547-9754-00921aac012c"}, "2d30140f-81ab-4cdf-8123-e0567cdca1a4": {"doc_hash": "ffeea667e096720003740ee2ebbd86d5531ed4b91e2557eb07e2a722eb4e7244", "ref_doc_id": "240202a6-e005-4547-9754-00921aac012c"}, "95782508-a676-4937-adbc-b0cc02e1f0b1": {"doc_hash": "f1fe5835af00b2b578537d31e33c47f4bc37a4589663214d8125b0f673ce3a97", "ref_doc_id": "240202a6-e005-4547-9754-00921aac012c"}, "8cf6dc25-15b4-4c0b-a64a-cd89e52642d7": {"doc_hash": "98493380306306592e0dd4607b78e6b87ba36eb0ebe5a7ebe2ffb5f8125ec05f", "ref_doc_id": "240202a6-e005-4547-9754-00921aac012c"}, "0b7ae76b-a9d4-4768-b9f7-c732358552e4": {"doc_hash": "3d8b0df7e7f1ee5d8c56cfa9eadeca4733a4deb40626a2f0caee7a65f29e8044", "ref_doc_id": "240202a6-e005-4547-9754-00921aac012c"}}, "docstore/ref_doc_info": {"240202a6-e005-4547-9754-00921aac012c": {"node_ids": ["2cbe996b-c9fe-449b-85cd-b74ffc184c0d", "74f5304e-b062-4c0e-b4ba-1228d49e46c9", "71127e13-2b33-41dc-8aa7-611553d692ac", "9de1f0b7-370d-47cc-9fd4-2fc08e96a0b0", "5d31f719-65f4-4e79-bfa3-1de6d985e993", "8bf36691-b3a4-4ab6-8107-bd5f4307842c", "1da429b8-969c-4d63-9362-5a113fbad8ef", "da3cba00-95da-44a1-ad56-dafa87002648", "2d30140f-81ab-4cdf-8123-e0567cdca1a4", "95782508-a676-4937-adbc-b0cc02e1f0b1", "8cf6dc25-15b4-4c0b-a64a-cd89e52642d7", "0b7ae76b-a9d4-4768-b9f7-c732358552e4"], "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}}}}