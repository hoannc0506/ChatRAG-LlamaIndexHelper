{"docstore/data": {"e8e0d000-d4ed-4d37-a81b-a0bc1ac66536": {"__data__": {"id_": "e8e0d000-d4ed-4d37-a81b-a0bc1ac66536", "embedding": null, "metadata": {"filename": "secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.md", "extension": ".md", "title": "Secure RAG with LlamaIndex and LLM Guard by Protect AI", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eb8803c1-4ea5-4d69-93df-65f93bb170a4", "node_type": "4", "metadata": {"filename": "secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.md", "extension": ".md", "title": "Secure RAG with LlamaIndex and LLM Guard by Protect AI", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai"}, "hash": "2f45e06923ed901a30c0fa57d3ce6ee85fc35e12f61ece228b8945d8cdb75a7b", "class_name": "RelatedNodeInfo"}}, "text": "_This is a guest post by Protect AI._\n\nWe believe that RAG will be one of the preferred approaches for enterprises\nwhen developing LLM applications to generate prompt responses that are more\nrelevant, and accurate, tailored to and based on company-specific content.\nHowever, while analyzing web pages with ChatGPT may leave the LLM vulnerable\nto injections embedded within the webpage, it is crucial to recognize that\ninjections may also be concealed within the vector database or knowledge graph\nwhere data is retrieved and injected into the LLM.\n\nThat is why we\u2019re thrilled to describe how LLM Guard by Protect AI can secure\nyour data sources accessed for context in your LLM application, built with\nLlamaIndex.\n\n[ LLM Guard ](https://llm-guard.com/) is an open source solution by [ Protect\nAI ](https://protectai.com/llm-guard) designed to fortify the security of\nLarge Language Models ( [ LLMs\n](https://www.helpnetsecurity.com/2023/05/10/security-privacy-risks-large-\nlanguage-models-video/) ). It is designed for easy integration and deployment\nin production environments. It provides extensive security scanners for both\nprompts and responses of LLMs to detect, redact, and sanitize against\nadversarial prompt attacks, data leakage, and integrity breaches (e.g.\noffensive content, hallucination).\n\nLLM Guard was built for a straightforward purpose: despite the potential of\nLLMs, corporate adoption has been hesitant. This reluctance stems from the\nsignificant security risks and a lack of control and observability of\nimplementing these technologies. With over 2.5M downloads of its models, and a\nGoogle Patch Reward, LLM Guard is the open source standard and market leader\nin LLM security at inference.\n\n**Secure RAG with LlamaIndex**\n\nIn the following [ example ](https://llm-\nguard.com/usage/notebooks/llama_index_rag/) , we showcase a practical approach\nto improve the security of your RAG application. Specifically, we will explore\na RAG application designed to facilitate the automated screening of candidate\nCVs by HR teams. Within the batch of CVs, there exists a diverse pool of\ncandidates, including one who lacks experience and consequently is not the\nmost suitable candidate. The nature of the attack manifests as an embedded\nprompt injection within the CV of this particular candidate, concealed in\nwhite text, rendering it challenging to detect with the naked eye.\n\n[ In the notebook example ](https://llm-\nguard.com/usage/notebooks/llama_index_rag/) , we conducted the attack\ninitially and then repeated the process subsequent to fortifying the\napplication with LLM Guard. With this example, we show how you can use LLM\nGuard and LlamaIndex for both input and output scanning of documents to detect\nany malicious content. Although ideally, we should scan documents before the [\ningestion\n](https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/transformations.html)\n, for simplicity in the example, we chose to do scanning during [ retrieval\n](https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/root.html)\n. In real-use cases, it's critical to do the scanning both during retrieval of\nreal-time data from APIs (not vector stores) which we still need to verify as\nit can contain poisoned sources of information. For output scanning, it can\nsimply be done by taking the results generated by LlamaIndex and running them\nthrough LLM Guard.\n\n    \n    \n    llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1, output_parser=output_parser)\n    \n    service_context = ServiceContext.from_defaults(\n        llm=llm, \n        transformations=transformations,\n        callback_manager=callback_manager,\n    )\n    index = VectorStoreIndex.from_documents(\n        documents, service_context=service_context\n    )\n    \n    input_scanners = [\n        Anonymize(vault, entity_types=[\"PERSON\", \"EMAIL_ADDRESS\", \"EMAIL_ADDRESS_RE\", \"PHONE_NUMBER\"]), \n        Toxicity(), \n        PromptInjection(),\n        Secrets()\n    ]\n    \n    llm_guard_postprocessor = LLMGuardNodePostProcessor(\n        scanners=input_scanners,\n        fail_fast=False,\n        skip_scanners=[\"Anonymize\"],\n    )\n    \n    query_engine = index.as_query_engine(\n        similarity_top_k=3,\n        node_postprocessors=[llm_guard_postprocessor]\n    )\n    response = query_engine.query(\"I am screening candidates for adult caregiving opportunity. Please recommend me an experienced person. Return just a name\")\n    print(str(response))\n    \n\n**LLM Guard protects your LLM applications**\n\nAs demonstrated in the practical example of securing an HR screening\napplication with [ LLM Guard ](https://llm-guard.com/) , the significance of\nmitigating potential attacks, cannot be overstated. Besides that, as LLMs\nevolve rapidly and embed advanced capabilities like agency and multi-modality,\nthe complexity and impact of potential breaches escalate significantly. Thus,\nprioritizing RAG security becomes not just a necessity but rather fundamental\nin safeguarding against increasingly sophisticated threats and ensuring the\nintegrity of critical enterprise LLM applications.\n\nTry out LLM Guard by going to our [ library\n](https://github.com/protectai/llm-guard) or [ documentation ](https://llm-\nguard.com/) . Also, [ join our Slack\n](https://join.slack.com/t/laiyerai/shared_invite/zt-28jv3ci39-sVxXrLs3rQdaN3mIl9IT~w)\nchannel for any questions!", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5368, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"e8e0d000-d4ed-4d37-a81b-a0bc1ac66536": {"doc_hash": "834a11ee0ad0937b4c2c484d4df03bebf08a1771634774107306b376ed458467", "ref_doc_id": "eb8803c1-4ea5-4d69-93df-65f93bb170a4"}}, "docstore/ref_doc_info": {"eb8803c1-4ea5-4d69-93df-65f93bb170a4": {"node_ids": ["e8e0d000-d4ed-4d37-a81b-a0bc1ac66536"], "metadata": {"filename": "secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.md", "extension": ".md", "title": "Secure RAG with LlamaIndex and LLM Guard by Protect AI", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai"}}}}