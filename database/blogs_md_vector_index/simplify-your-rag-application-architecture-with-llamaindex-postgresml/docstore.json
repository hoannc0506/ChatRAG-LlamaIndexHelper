{"docstore/data": {"0ffa65b6-0d9b-419d-8b74-cac0079ec9a1": {"__data__": {"id_": "0ffa65b6-0d9b-419d-8b74-cac0079ec9a1", "embedding": null, "metadata": {"filename": "simplify-your-rag-application-architecture-with-llamaindex-postgresml.md", "extension": ".md", "title": "Simplify your RAG application architecture with LlamaIndex + PostgresML", "date": "May 28, 2024", "url": "https://www.llamaindex.ai/blog/simplify-your-rag-application-architecture-with-llamaindex-postgresml"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c38b3110-e1cd-45f7-b1be-005106e7a0f2", "node_type": "4", "metadata": {"filename": "simplify-your-rag-application-architecture-with-llamaindex-postgresml.md", "extension": ".md", "title": "Simplify your RAG application architecture with LlamaIndex + PostgresML", "date": "May 28, 2024", "url": "https://www.llamaindex.ai/blog/simplify-your-rag-application-architecture-with-llamaindex-postgresml"}, "hash": "2224e0455fac6f885d92b9c08aecec8ceadac8a390451d094c71be1bddf71f20", "class_name": "RelatedNodeInfo"}}, "text": "We\u2019re happy to announce the recent integration of LlamaIndex with PostgresML \u2014\na comprehensive machine learning platform built on PostgreSQL. The PostgresML\nManaged Index allows LlamaIndex users to seamlessly manage document storage,\nsplitting, embedding, and retrieval. By using PostgresML as the backend, users\nbenefit from a streamlined and optimized process for Retrieval-Augmented\nGeneration (RAG). This integration unifies embedding, vector search, and text\ngeneration into a single network call, resulting in faster, more reliable, and\neasier-to-manage RAG workflows.\n\n**The problem with typical RAG workflows**\n\nTypical Retrieval-Augmented Generation (RAG) workflows come with significant\ndrawbacks, particularly for users.\n\nPoor performance is a major issue, as these workflows involve multiple network\ncalls to different services for embedding, vector storage, and text\ngeneration, leading to increased latency. Additionally, there are privacy\nconcerns when sensitive data is sent to various LLM providers. These user-\ncentric issues are compounded by other challenges:\n\n  * Increased dev time to master new technologies \n  * Complicated maintenance and scalability issues due to multiple points of failure \n  * Costly vendors required for multiple services \n\nThe diagram above illustrates the complexity, showing how each component\ninteracts across different services \u2014 exacerbating these problems.\n\n**Solution**\n\nThe PostgresML Managed Index offers a comprehensive solution to the challenges\nof typical RAG workflows.\n\nBy managing document storage, splitting, embedding generation, and retrieval\nall within a single system, PostgresML significantly reduces dev time, scaling\ncosts, and overall spend when you eliminate the need for multiple point\nsolutions. Most importantly, it enhances the user experience by consolidating\nembedding, vector search, and text generation into a single network call \u2014\nresulting in improved performance and reduced latency. Additionally, the use\nof open-source models ensures transparency and flexibility, while operating\nwithin the database addresses privacy concerns and provides users with a\nsecure and efficient RAG workflow.\n\n**About PostgresML**\n\nPostgresML [ [ github ](https://github.com/postgresml/postgresml) || [ website\n](https://postgresml.org/) || [ docs ](https://postgresml.org/docs) ] allows\nusers to take advantage of the fundamental relationship between data and\nmodels, by moving the models to your database rather than constantly moving\ndata to the models. This in-database approach to AI architecture results in\nmore scalable, reliable and efficient applications. On the PostgresML cloud,\nyou can perform vector operations, create embeddings, and generate real-time\noutputs in one process, directly where your data resides.\n\nKey highlights:\n\n  * Model Serving - GPU accelerated inference engine for interactive applications, with no additional networking latency or reliability costs \n  * Model Store - Access to open-source models including state of the art LLMs from Hugging Face, and track changes in performance between versions \n  * Model Training - Train models with your application data using more than 50 algorithms for regression, classification or clustering tasks; fine tune pre-trained models like Llama and BERT to improve performance \n  * Feature Store - Scalable access to model inputs, including vector, text, categorical, and numeric data: vector database, text search, knowledge graph and application data all in one low-latency system \n  * Python and JavaScript SDKs - SDK clients can perform advanced ML/AI tasks in a single SQL request without having to transfer additional data, models, hardware or dependencies to your application \n  * Serverless deployments - Enjoy instant autoscaling, so your applications can handle peak loads without overprovisioning \n\nPostgresML has a range of capabilities. In the following sections, we\u2019ll guide\nyou through just one use case \u2013 RAG \u2013 and how to use the PostgresML Managed\nIndex on LlamaIndex to build a better RAG app.\n\n**How it works in LlamaIndex**\n\nLet\u2019s look at a simple question-answering example using the PostgresML Managed\nIndex. For this example, we will be using Paul Graham\u2019s essays.\n\n**Step 1: Get Your Database Connection String**\n\nIf you haven\u2019t already, [ create your PostgresML account\n](https://postgresml.org/signup) . You\u2019ll get $100 in free credits when you\ncomplete your profile.\n\nSet the PGML_DATABASE_URL environment variable:\n\n    \n    \n    export PGML_DATABASE_URL=\"{YOUR_CONNCECTION_STRING}\"\n\nAlternatively, you can pass the pgml_database_url argument when creating the\nindex.\n\n**Step 2: Create the PostgresML Managed Index**\n\nFirst install Llama_index and the PostgresML Managed Index component:\n\n    \n    \n    pip install llama_index llama-index-indices-managed-postgresml\n\nThen load in the data:\n\n    \n    \n    mkdir data\n    curl -o data/paul_graham_essay.txt https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n\nFinally create the PostgresML Managed Index:\n\n    \n    \n    from llama_index.core.readers import SimpleDirectoryReader\n    from llama_index.indices.managed.postgresml import PostgresMLIndex\n    \n    \n    documents = SimpleDirectoryReader(\"data\").load_data()\n    index = PostgresMLIndex.from_documents(\n        documents, collection_name=\"llama-index-example\"\n    )\n\nNote the collection_name is used to uniquely identify the index you are\nworking with.\n\nHere we are using the SimpleDirectoryReader to load in the documents and then\nwe construct the PostgresMLIndex from those documents.\n\nThis workflow does not require document preprocessing. Instead, the documents\nare sent directly to PostgresML where they are stored, split, and embedded per\nthe pipeline specification. For more information on pipelines see: [\nhttps://postgresml.org/docs/api/client-sdk/pipelines\n](https://postgresml.org/docs/api/client-sdk/pipelines) Custom Pipelines can\nbe passed into the PostgresML index at creation, but by default documents are\nsplit using the recursive_character splitter and embedded with\nintfloat/e5-small-v2 .\n\n**Step 3: Querying**\n\nNow that we have created our index we can use it for retrieval and querying:\n\n    \n    \n    retriever = index.as_retriever()\n    docs = retriever.retrieve(\"Was the author puzzled by the IBM 1401?\")\n    for doc in docs:\n        print(doc)\n\nPostgreML does embedding and retrieval in a single network call. Compare this\nquery against other common LlamaIndex embedding and vector storage\nconfigurations and you will notice a significant speed up.\n\nUsing the PostgresML Index as a query_engine is just as easy:\n\n    \n    \n    response = index.as_query_engine().query(\"Was the author puzzled by the IBM 1401?\")\n    print(response)\n\nOnce again, notice how fast the response was! The PostgresML Managed Index is\ndoing embedding, retrieval, and augmented generation in one network call. The\nspeed up becomes even more apparent when streaming:\n\n    \n    \n    query_engine = index.as_query_engine(streaming=True)\n    results = query_engine.query(\"Was the author puzzled by the IBM 1401?\")\n    for text in results.response_gen:\n        print(text, end=\"\", flush=True)\n\nNote that by default the query_engine uses meta-llama/Meta-Llama-3-8B-Instruct\nbut this is completely configurable.\n\n**Key takeaways**\n\nThe PostgresML Managed Index uniquely unifies embedding, vector search, and\ntext generation into a single network call. LlamaIndex users can expect\nfaster, more reliable, and easier-to-manage RAG workflows by using PostgresML\nas the backend.\n\nTo get started with PostgresML and LlamaIndex, you can follow the PostgresML\nintro [ guide ](https://postgresml.org/docs/introduction/getting-started/) to\nsetup your account, and the examples above with your own data.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 7823, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"0ffa65b6-0d9b-419d-8b74-cac0079ec9a1": {"doc_hash": "ed30f7e82e753e75e06b5e522598131e366ff2394c3a5d5401b2d1818ea2e956", "ref_doc_id": "c38b3110-e1cd-45f7-b1be-005106e7a0f2"}}, "docstore/ref_doc_info": {"c38b3110-e1cd-45f7-b1be-005106e7a0f2": {"node_ids": ["0ffa65b6-0d9b-419d-8b74-cac0079ec9a1"], "metadata": {"filename": "simplify-your-rag-application-architecture-with-llamaindex-postgresml.md", "extension": ".md", "title": "Simplify your RAG application architecture with LlamaIndex + PostgresML", "date": "May 28, 2024", "url": "https://www.llamaindex.ai/blog/simplify-your-rag-application-architecture-with-llamaindex-postgresml"}}}}