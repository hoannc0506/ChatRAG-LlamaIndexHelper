{"docstore/data": {"1061aa1e-cf53-4444-881a-8d892543a68f": {"__data__": {"id_": "1061aa1e-cf53-4444-881a-8d892543a68f", "embedding": null, "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8107e05-ec19-4777-904a-4f2881a6f88d", "node_type": "4", "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "8dbc3145c7b909a6f9a4bd4ea8ea7b7ac8679e6d4b729522cd4258a0aa5323e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72a0a548-3796-484d-894f-f63a57fe34d2", "node_type": "1", "metadata": {"Header_2": " Our Mission Goes Beyond RAG"}, "hash": "ce26c470646269ede871e3111047390725ec9351dbbe9ea2ece0192d8a28aa4c", "class_name": "RelatedNodeInfo"}}, "text": "Google recently released [ Gemini 1.5 Pro with a 1M context window\n](https://blog.google/technology/ai/google-gemini-next-generation-model-\nfebruary-2024/#gemini-15) , available to a limited set of developers and\nenterprise customers. Its performance has caught the imagination of [ AI\nTwitter ](https://x.com/alliekmiller/status/1760522046251962459?s=20) . It [\nachieves 99.7% recall in the \u201cNeedle in a Haystack\u201d\n](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf)\nexperiment popularized by [ Greg Kamradt\n](https://x.com/GregKamradt/status/1722386725635580292?s=20) . Early users\nhave shared results feeding dozens of research papers, financial reports at\nonce and report impressive results in terms of its ability to synthesize\nacross vast troves of information.\n\nNaturally, this begs the question - is RAG dead? Some [ folks think so\n](https://twitter.com/francis_yao_/status/1759962812229800012?s=46&t=pfae6EnnrBq2o8ok0KpVqw)\n, while others [ disagree\n](https://x.com/ptsi/status/1758511307433947625?s=20) . Those in the first\ncamp make valid points. Most small data use cases can fit within a 1-10M\ncontext window. Tokens will get cheaper and faster to process over time.\nHaving an LLM natively interleave retrieval/generation via attention layers\nleads to a higher response quality compared to the one-shot retrieval present\nin naive RAG.\n\nWe were fortunate to have a preview of Gemini 1.5 Pro\u2019s capabilities, and\nthrough playing around with it developed a thesis for how context-augmented\nLLM applications will evolve. This blog post clarifies **our mission as a data\nframework** along with **our view of what long-context LLM architectures will\nlook like.** Our view is that while long-context LLMs will simplify certain\nparts of the RAG pipeline (e.g. chunking), there will need to be evolved RAG\narchitectures to handle the new use cases that long-context LLMs bring along.\nNo matter what new paradigms emerge, our mission at LlamaIndex is to build\ntooling towards that future.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2016, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72a0a548-3796-484d-894f-f63a57fe34d2": {"__data__": {"id_": "72a0a548-3796-484d-894f-f63a57fe34d2", "embedding": null, "metadata": {"Header_2": " Our Mission Goes Beyond RAG", "filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8107e05-ec19-4777-904a-4f2881a6f88d", "node_type": "4", "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "8dbc3145c7b909a6f9a4bd4ea8ea7b7ac8679e6d4b729522cd4258a0aa5323e0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1061aa1e-cf53-4444-881a-8d892543a68f", "node_type": "1", "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "6fb18ed5d848c92e812c5e3c571e85e8f3574408b9ceb97f8ce1f6e239625934", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98b8db05-fced-4915-a7e4-50a29799f286", "node_type": "1", "metadata": {"Header_2": " Initial Gemini 1.5 Pro Observations"}, "hash": "aa9501051861f43b98d550164a6037c27c831af79c039fefceed1f90c14fc8d8", "class_name": "RelatedNodeInfo"}}, "text": "Our Mission Goes Beyond RAG\n\nThe goal of LlamaIndex is very simple: **enable developers to build LLM\napplications over their data.** This mission goes beyond just RAG. To date we\nhave invested a considerable amount of effort in advancing RAG techniques for\nexisting LLMs, and we\u2019ve done so because it\u2019s enabled developers to unlock\ndozens of new use cases such as QA over semi-structured data, over complex\ndocuments, and agentic reasoning in a multi-doc setting.\n\nBut we\u2019re also excited about Gemini Pro, and we will continue to advance\nLlamaIndex as a production data framework in a long-context LLM future.\n\n**An LLM framework is intrinsically valuable.** As an open-source data\nframework, LlamaIndex paves the cowpaths towards building any LLM use case\nfrom prototype to production. A framework makes it easier to build these use\ncases versus building from scratch. We enable _all_ developers to build for\nthese use cases, whether it\u2019s setting up the proper architecture using our\ncore abstractions or leveraging the hundreds of integrations in our ecosystem.\nNo matter what the underlying LLM advancements are and whether RAG continues\nto exist in its current form, we continue to make the framework production-\nready, including watertight abstractions, first-class documentation, and\nconsistency.\n\n[ We also launched LlamaCloud last week\n](https://blog.llamaindex.ai/introducing-llamacloud-and-llamaparse-\naf8cedf9006b) . Our mission for LlamaCloud remains building the data infra\nenabling any enterprise to make their vast unstructured, semi-structured, and\nstructured data sources production-ready for use with LLMs.", "mimetype": "text/plain", "start_char_idx": 2022, "end_char_idx": 3646, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "98b8db05-fced-4915-a7e4-50a29799f286": {"__data__": {"id_": "98b8db05-fced-4915-a7e4-50a29799f286", "embedding": null, "metadata": {"Header_2": " Initial Gemini 1.5 Pro Observations", "filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8107e05-ec19-4777-904a-4f2881a6f88d", "node_type": "4", "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "8dbc3145c7b909a6f9a4bd4ea8ea7b7ac8679e6d4b729522cd4258a0aa5323e0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72a0a548-3796-484d-894f-f63a57fe34d2", "node_type": "1", "metadata": {"Header_2": " Our Mission Goes Beyond RAG", "filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "6435e620425b9874238f9e0581990800911fd98ba73a890b1b92a86098e47f1c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0b0c3b2-2079-4dca-b7d4-0236d6324ffe", "node_type": "1", "metadata": {"Header_2": " Long Contexts Resolve Some Pain Points, but some Challenges Remain"}, "hash": "b8cf30b62712113d6b30c88a02db0b221b823b88198d44330a5dc98b9dbbcec4", "class_name": "RelatedNodeInfo"}}, "text": "Initial Gemini 1.5 Pro Observations\n\nDuring our initial testing we played around with some PDFs: SEC 10K Filings,\nArXiv papers, this monster [ Schematic Design Binder\n](https://www.lowellhsproject.com/DocumentCenter/View/236/LHS-Schematic-\nDesign-Binder) , and more. We will do a lot more deeper analyses once the APIs\nare available, but in the meantime we share observations below.\n\nGemini results are impressive and consistent with what we\u2019ve seen in the\ntechnical report and on socials:\n\n  * **Gemini has impressive recall of specific details:** We threw in 100k-1M tokens of context, and asked questions over very specific details in these documents (unstructured text and tabular data), and in all cases Gemini was able to recall the details. See above for Gemini comparing table results in the 2019 Uber 10K Filing. \n  * **Gemini has impressive summarization capabilities.** The model can analyze large swaths of information across multiple documents and synthesize answers. \n\nThis figure shows a question-response pair from Gemini over the 2019 Uber 10K\nfiling. The question and answer is shown at the top and the source table is\nshown at the bottom. Gemini is able to return the correct answer.\n\nThere are some parts where we noticed Gemini struggles a bit.\n\n  * **Gemini doesn\u2019t read all tables and charts correctly.** Gemini Pro still has a hard time being able to read figures and complex tables. \n  * **Gemini can take a long time.** Returning an answer over the Uber 10K Filing (~160k) takes ~20 seconds. Returning an answer over the LHS Schematic Design Binder (~890k) takes ~60+ seconds. \n  * **Gemini can hallucinate page numbers.** When asked to give a summary but also with page number citations, Gemini hallucinated the sources. \n\nAn example where Gemini 1.5 Pro still hallucinates. The model hallucinates a\nnumber when asked about the total number of gross bookings across all segments\n- the number is visible in the chart and can also be pieced together from the\ntable.\n\nDirectionally though it\u2019s an exciting glimpse of the future and warrants a\nbigger discussion on which RAG paradigms will fade and new architectures that\nwill emerge. See below!", "mimetype": "text/plain", "start_char_idx": 3652, "end_char_idx": 5820, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0b0c3b2-2079-4dca-b7d4-0236d6324ffe": {"__data__": {"id_": "b0b0c3b2-2079-4dca-b7d4-0236d6324ffe", "embedding": null, "metadata": {"Header_2": " Long Contexts Resolve Some Pain Points, but some Challenges Remain", "filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8107e05-ec19-4777-904a-4f2881a6f88d", "node_type": "4", "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "8dbc3145c7b909a6f9a4bd4ea8ea7b7ac8679e6d4b729522cd4258a0aa5323e0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "98b8db05-fced-4915-a7e4-50a29799f286", "node_type": "1", "metadata": {"Header_2": " Initial Gemini 1.5 Pro Observations", "filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "09ea496d4471e6f29ece2493acf6c66fb726da5b92a756fe8d75bb4c8c728221", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1999ddae-4bea-4c7d-8ea5-5438bd9fb7f9", "node_type": "1", "metadata": {"Header_2": " Towards New RAG Architectures"}, "hash": "a567b619c31d2a4b5384d75fd4b6741fb2da4d6803f48e7ebede02e529c8a8fb", "class_name": "RelatedNodeInfo"}}, "text": "Long Contexts Resolve Some Pain Points, but some Challenges Remain\n\nGemini 1.5 Pro is just the first of many long-context LLMs to emerge, which\nwill inevitably change how users are building RAG.\n\nHere are some existing RAG pain points that we believe long-context LLMs will\nsolve:\n\n  1. **Developers will worry less about how to precisely tune chunking algorithms.** We honestly think this will be a huge blessing to LLM developers. Long-context LLMs enable native chunk sizes to be bigger. Assuming per-token cost and latency also go down, developers will no longer have to split hairs deciding how to split their chunks into granular strips through tuning chunking separators, chunk sizes, and careful metadata injection. Long-context LLMs enable chunks to be at the level of entire documents, or at the very least groups of pages. \n  2. **Developers will need to spend less time tuning retrieval and chain-of-thought over single documents** . An issue with small-chunk top-k RAG is that while certain questions may be answered over a specific snippet of the document, other questions require deep analysis between sections or between two documents (for instance comparison queries). For these use cases, developers will no longer have to rely on a chain-of-thought agent to do two retrievals against a weak retriever; instead, they can just one-shot prompt the LLM to obtain the answer. \n  3. **Summarization will be easier.** This is related to the above statement. A lot of summarization strategies over big documents involve \u201chacks\u201d such as sequential refinement or hierarchical summarization (see our [ response synthesis modules ](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html) as a reference guide). This can now be alleviated with a single LLM call. \n  4. **Personalized memory will be better and easier to build:** A key issue for building conversational assistants is figuring out how to load sufficient conversational context into the prompt window. 4k tokens easily overflows this window for very basic web search agents - if it decides to load in a Wikipedia page for instance, that text will easily overflow the context. 1M-10M context windows will let developers more easily implement conversational memory with fewer compression hacks (e.g. vector search or automatic KG construction). \n\nThere are, however, some lingering challenges:\n\n  1. **10M tokens is not enough for large document corpuses - kilodoc retrieval is still a challenge.** 1M tokens is around ~7 Uber SEC 10K filings. 10M tokens would be around ~70 filings. 10M tokens is roughly bounded by 40MB of data. While this is enough for many \u201csmall\u201d document corpuses, many knowledge corpuses in the enterprise are in the gigabytes or terabytes. To build LLM-powered systems over these knowledge corpuses, developers will still need to build in some way of retrieving this data to augment language models with context. \n  2. **Embedding models are lagging behind in context length.** So far the largest context window we\u2019ve seen for embeddings are [ 32k from together.ai ](https://hazyresearch.stanford.edu/blog/2024-01-11-m2-bert-retrieval) . This means that even if the chunks used for synthesis with long-context LLMs can be big, any text chunks used for retrieval still need to be a lot smaller. \n  3. **Cost and Latency.** Yes, all cost and latency concerns are alleviated with time. Nevertheless, stuffing a 1M context window takes ~60 seconds and can cost anywhere from $0.50 to $20 with current pricing. An solution to this that [ Yao Fu ](https://twitter.com/Francis_YAO_) brought up is that a [ KV Cache ](https://x.com/Francis_YAO_/status/1759962812229800012?s=20) can cache the document activations, so that any subsequent generations can reuse the same cache. Which leads to our next point below. \n  4. **A KV Cache takes up a significant amount of GPU memory, and has sequential dependencies** . We chatted with Yao and he mentioned that at the moment, caching 1M tokens worth of activations would use up approximately 100GB of GPU memory, or 2 H100s. There are also interesting challenges on how to best manage the cache especially when the underlying corpus is big - since each activation is a function of all tokens leading up to it, replacing any document in the KV cache would affect all activations following the document.", "mimetype": "text/plain", "start_char_idx": 5826, "end_char_idx": 10192, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1999ddae-4bea-4c7d-8ea5-5438bd9fb7f9": {"__data__": {"id_": "1999ddae-4bea-4c7d-8ea5-5438bd9fb7f9", "embedding": null, "metadata": {"Header_2": " Towards New RAG Architectures", "filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8107e05-ec19-4777-904a-4f2881a6f88d", "node_type": "4", "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "8dbc3145c7b909a6f9a4bd4ea8ea7b7ac8679e6d4b729522cd4258a0aa5323e0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0b0c3b2-2079-4dca-b7d4-0236d6324ffe", "node_type": "1", "metadata": {"Header_2": " Long Contexts Resolve Some Pain Points, but some Challenges Remain", "filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "55af3c0ebd9f02cbaf5ef8a4f129fd45859700c7927f7db8c531cdbe2bdbfd21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db7eb40d-ca50-4dae-ae76-a5b3099047fc", "node_type": "1", "metadata": {"Header_2": " Towards New RAG Architectures", "Header_3": " 1\\. Small to Big Retrieval over Documents"}, "hash": "7e7a13a7740a558276b494d9adcd76d099ff513c7760967984a5cbdf647b3640", "class_name": "RelatedNodeInfo"}}, "text": "Towards New RAG Architectures\n\nProper usage of long-context LLMs will necessitate new architectures to best\ntake advantage of their capabilities, while working around their remaining\nconstraints. We outline some proposals below.", "mimetype": "text/plain", "start_char_idx": 10199, "end_char_idx": 10427, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db7eb40d-ca50-4dae-ae76-a5b3099047fc": {"__data__": {"id_": "db7eb40d-ca50-4dae-ae76-a5b3099047fc", "embedding": null, "metadata": {"Header_2": " Towards New RAG Architectures", "Header_3": " 1\\. Small to Big Retrieval over Documents", "filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8107e05-ec19-4777-904a-4f2881a6f88d", "node_type": "4", "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "8dbc3145c7b909a6f9a4bd4ea8ea7b7ac8679e6d4b729522cd4258a0aa5323e0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1999ddae-4bea-4c7d-8ea5-5438bd9fb7f9", "node_type": "1", "metadata": {"Header_2": " Towards New RAG Architectures", "filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "cf80027e8199158d9c51c31056760ce03623bcb04ab24dae783f8fb1103d701f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff28c350-e12f-40d3-a2bc-b9267ff6c337", "node_type": "1", "metadata": {"Header_2": " Towards New RAG Architectures", "Header_3": " 2\\. Intelligent Routing for Latency/Cost Tradeoffs"}, "hash": "3650da71f4e537650d44a4f2f35ee9956b83b8f9450a95106fb46bf6977db67f", "class_name": "RelatedNodeInfo"}}, "text": "1\\. Small to Big Retrieval over Documents\n\nTo the extent that long-context LLMs need retrieval augmentation over big\nknowledge bases (e.g. in the gigabytes), we will need **small-to-big\nretrieval:** index and retrieve small chunks, but have each chunk link to big\nchunks that will ultimately be fed to LLMs during synthesis.\n\nThis architecture already exists in LlamaIndex in different forms ( [ sentence\nwindow retriever\n](https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/MetadataReplacementDemo.html)\nand [ recursive retrieval over chunk sizes\n](https://docs.llamaindex.ai/en/stable/examples/retrievers/recursive_retriever_nodes.html)\n), but can be scaled up even more for long-context LLMs - embed document\nsummaries, but link to entire documents.\n\nOne reason we want to embed and index smaller chunks is due to the fact that\ncurrent embedding models are not keeping up with LLMs in terms of context\nlength. Another reason is that there can actually be retrieval benefits in\nhaving multiple granular embedding representations compared to a single\ndocument-level embedding for a document. If there is a single embedding for a\ndocument, then that embedding has the burden of encoding all information\nthroughout the entire document. On the other hand, we\u2019ve found that embedding\nmany smaller chunks and having each small chunk link to a bigger chunk, will\nlead to better retrieval of the relevant information.\n\nCheck out the diagram above for an illustration of two flavors of small-to-big\nretrieval. One is indexing document summaries and linking them to documents,\nand the other is indexing smaller chunks within a document and linking them to\nthe document. Of course, you could also do both - a general best practice for\nimproving retrieval is to just try out multiple techniques at once and fuse\nthe results later.", "mimetype": "text/plain", "start_char_idx": 10434, "end_char_idx": 12268, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff28c350-e12f-40d3-a2bc-b9267ff6c337": {"__data__": {"id_": "ff28c350-e12f-40d3-a2bc-b9267ff6c337", "embedding": null, "metadata": {"Header_2": " Towards New RAG Architectures", "Header_3": " 2\\. Intelligent Routing for Latency/Cost Tradeoffs", "filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8107e05-ec19-4777-904a-4f2881a6f88d", "node_type": "4", "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "8dbc3145c7b909a6f9a4bd4ea8ea7b7ac8679e6d4b729522cd4258a0aa5323e0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db7eb40d-ca50-4dae-ae76-a5b3099047fc", "node_type": "1", "metadata": {"Header_2": " Towards New RAG Architectures", "Header_3": " 1\\. Small to Big Retrieval over Documents", "filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "e84e2ee221b9bde926f404d582c0961c15bf84576888d824167085415810e52e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "babf01fa-7ca1-44af-b5ba-44343db97305", "node_type": "1", "metadata": {"Header_2": " Towards New RAG Architectures", "Header_3": " 3\\. Retrieval Augmented KV Caching"}, "hash": "db89ae11a6602815f53fee4d7f79bdf4207977b7ee8b5bd124563cbc73edc5c4", "class_name": "RelatedNodeInfo"}}, "text": "2\\. Intelligent Routing for Latency/Cost Tradeoffs\n\nThe arrival of long-context LLMs will inevitably raise questions on the amount\nof context that is suitable for each use case. Injecting LLMs with long\ncontext comes with real cost and latency tradeoffs and isn\u2019t suitable for\nevery use case or even every question. Although cost and latency will decrease\nin the future, we anticipate users will need to think carefully about this\ntradeoff for the next year or two.\n\nCertain questions that are asking about specific details are well suited for\nexisting RAG techniques of top-k retrieval and synthesis.\n\nMore complex questions require more context from disparate pieces of different\ndocuments, and in those settings it is less clear how to correctly answer\nthese questions while optimizing for latency and cost:\n\n  * Summarization questions require going over entire documents. \n  * Multi-part questions can be solved by doing chain-of-thought and interleaving retrieval and reasoning; they can also be solved by shoving all context into the prompt. \n\nWe imagine an intelligent routing layer that operates on top of multiple RAG\nand LLM synthesis pipelines over a knowledge base. Given a question, the\nrouter can ideally choose an optimal strategy in terms of cost and latency in\nterms of retrieving context to answer the question. This ensures that a single\ninterface can handle different types of questions while not becoming\nprohibitively expensive.", "mimetype": "text/plain", "start_char_idx": 12275, "end_char_idx": 13726, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "babf01fa-7ca1-44af-b5ba-44343db97305": {"__data__": {"id_": "babf01fa-7ca1-44af-b5ba-44343db97305", "embedding": null, "metadata": {"Header_2": " Towards New RAG Architectures", "Header_3": " 3\\. Retrieval Augmented KV Caching", "filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8107e05-ec19-4777-904a-4f2881a6f88d", "node_type": "4", "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "8dbc3145c7b909a6f9a4bd4ea8ea7b7ac8679e6d4b729522cd4258a0aa5323e0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff28c350-e12f-40d3-a2bc-b9267ff6c337", "node_type": "1", "metadata": {"Header_2": " Towards New RAG Architectures", "Header_3": " 2\\. Intelligent Routing for Latency/Cost Tradeoffs", "filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "2b3c32cc82c3b9b3db86ff2ae4253bfeb09fdc044598b3c9715f6f0c24ac7371", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bfd56f4b-f448-4b45-b999-a86c6ea74763", "node_type": "1", "metadata": {"Header_2": " Towards New RAG Architectures", "Header_3": " What\u2019s Next"}, "hash": "f660def70191420e5350af1c2d68f77032ffe573e9210337ba8578c050803635", "class_name": "RelatedNodeInfo"}}, "text": "3\\. Retrieval Augmented KV Caching\n\nAn optimization that Google and other companies are certainly working on is\nresolving latency and cost concerns through a **KV Cache.** At a high-level, a\nKV cache stores activations from pre-existing key and query vectors in an\nattention layer, preventing the need to recompute activations across the\nentire text sequence during LLM generation (we found [ this\n](https://medium.com/@plienhar/llm-inference-series-3-kv-caching-\nunveiled-048152e461c8) to be a nice intro reference to how a KV Cache works).\n\nUsing a KV Cache to cache all document tokens within the context window\nprevents the need to recompute activations for these tokens on subsequent\nconversations, bringing down latency and cost significantly.\n\nBut this leads to interesting retrieval strategies on how to best use the\ncache, particularly for knowledge corpuses that exceed the context length. We\nimagine a \u201c **retrieval augmented caching** \u201d paradigm emerging, where we want\nto retrieve the most relevant documents that the user would want to answer,\nwith the expectation that they will continue to use the documents that are in\nthe cache.\n\nThis could involve interleaving retrieval strategies with [ traditional\ncaching algorithms ](https://www.notion.so/Long-Context-Window-Blog-\nPost-e2e3faaac23140eabc0e066ce2783890?pvs=21) such as LRU caching. But a\ndifference with existing KV cache architectures is that the position matters,\nsince the cached vector is a function of all tokens leading up to that\nposition, not just the tokens in the document itself. This means that you\ncan\u2019t just swap out a chunk from the KV cache without affecting all cached\ntokens that occur after it positionally.\n\nIn general the API interface for using a KV Cache is up in the air. It\u2019s also\nup in the air as to whether the nature of the cache itself will evolve or\nalgorithms will evolve to best leverage the cache.", "mimetype": "text/plain", "start_char_idx": 13733, "end_char_idx": 15637, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bfd56f4b-f448-4b45-b999-a86c6ea74763": {"__data__": {"id_": "bfd56f4b-f448-4b45-b999-a86c6ea74763", "embedding": null, "metadata": {"Header_2": " Towards New RAG Architectures", "Header_3": " What\u2019s Next", "filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8107e05-ec19-4777-904a-4f2881a6f88d", "node_type": "4", "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "8dbc3145c7b909a6f9a4bd4ea8ea7b7ac8679e6d4b729522cd4258a0aa5323e0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "babf01fa-7ca1-44af-b5ba-44343db97305", "node_type": "1", "metadata": {"Header_2": " Towards New RAG Architectures", "Header_3": " 3\\. Retrieval Augmented KV Caching", "filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "9fe13a4b7df2dca62e68440b609530b7beaf6c14791c33dbacf818764e854427", "class_name": "RelatedNodeInfo"}}, "text": "What\u2019s Next\n\nWe believe the future of LLM applications is bright, and we are excited to be\nat the forefront of this rapidly evolving field. We invite developers and\nresearchers to join us in exploring the possibilities of long-context LLMs and\nbuilding the next generation of intelligent applications.", "mimetype": "text/plain", "start_char_idx": 15644, "end_char_idx": 15945, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"1061aa1e-cf53-4444-881a-8d892543a68f": {"doc_hash": "6fb18ed5d848c92e812c5e3c571e85e8f3574408b9ceb97f8ce1f6e239625934", "ref_doc_id": "e8107e05-ec19-4777-904a-4f2881a6f88d"}, "72a0a548-3796-484d-894f-f63a57fe34d2": {"doc_hash": "6435e620425b9874238f9e0581990800911fd98ba73a890b1b92a86098e47f1c", "ref_doc_id": "e8107e05-ec19-4777-904a-4f2881a6f88d"}, "98b8db05-fced-4915-a7e4-50a29799f286": {"doc_hash": "09ea496d4471e6f29ece2493acf6c66fb726da5b92a756fe8d75bb4c8c728221", "ref_doc_id": "e8107e05-ec19-4777-904a-4f2881a6f88d"}, "b0b0c3b2-2079-4dca-b7d4-0236d6324ffe": {"doc_hash": "55af3c0ebd9f02cbaf5ef8a4f129fd45859700c7927f7db8c531cdbe2bdbfd21", "ref_doc_id": "e8107e05-ec19-4777-904a-4f2881a6f88d"}, "1999ddae-4bea-4c7d-8ea5-5438bd9fb7f9": {"doc_hash": "cf80027e8199158d9c51c31056760ce03623bcb04ab24dae783f8fb1103d701f", "ref_doc_id": "e8107e05-ec19-4777-904a-4f2881a6f88d"}, "db7eb40d-ca50-4dae-ae76-a5b3099047fc": {"doc_hash": "e84e2ee221b9bde926f404d582c0961c15bf84576888d824167085415810e52e", "ref_doc_id": "e8107e05-ec19-4777-904a-4f2881a6f88d"}, "ff28c350-e12f-40d3-a2bc-b9267ff6c337": {"doc_hash": "2b3c32cc82c3b9b3db86ff2ae4253bfeb09fdc044598b3c9715f6f0c24ac7371", "ref_doc_id": "e8107e05-ec19-4777-904a-4f2881a6f88d"}, "babf01fa-7ca1-44af-b5ba-44343db97305": {"doc_hash": "9fe13a4b7df2dca62e68440b609530b7beaf6c14791c33dbacf818764e854427", "ref_doc_id": "e8107e05-ec19-4777-904a-4f2881a6f88d"}, "bfd56f4b-f448-4b45-b999-a86c6ea74763": {"doc_hash": "1ab40f8cbbbdef2fde58038ceb74d150111e4255a566bd0b6e0f9ea1ab38ed8c", "ref_doc_id": "e8107e05-ec19-4777-904a-4f2881a6f88d"}}, "docstore/ref_doc_info": {"e8107e05-ec19-4777-904a-4f2881a6f88d": {"node_ids": ["1061aa1e-cf53-4444-881a-8d892543a68f", "72a0a548-3796-484d-894f-f63a57fe34d2", "98b8db05-fced-4915-a7e4-50a29799f286", "b0b0c3b2-2079-4dca-b7d4-0236d6324ffe", "1999ddae-4bea-4c7d-8ea5-5438bd9fb7f9", "db7eb40d-ca50-4dae-ae76-a5b3099047fc", "ff28c350-e12f-40d3-a2bc-b9267ff6c337", "babf01fa-7ca1-44af-b5ba-44343db97305", "bfd56f4b-f448-4b45-b999-a86c6ea74763"], "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}}}}