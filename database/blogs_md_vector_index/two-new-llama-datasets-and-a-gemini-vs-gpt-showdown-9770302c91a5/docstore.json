{"docstore/data": {"3d9327e5-cadd-4009-8c8f-013157d1a779": {"__data__": {"id_": "3d9327e5-cadd-4009-8c8f-013157d1a779", "embedding": null, "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36e74daf-749b-4e62-b3a4-95002d7fcda1", "node_type": "4", "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "9fb2edde61de382b2df9e3f6054378477ffe6b3d62bce33dc4213cc5993ef185", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c44061f-c6b4-4463-848b-e9da70b5f4e6", "node_type": "1", "metadata": {"Header_1": " Intro"}, "hash": "ebd1240a2669a4de172bbd379c7dc9a7661b3a593c61976325f18c3f2f9666a4", "class_name": "RelatedNodeInfo"}}, "text": "(Authored by Andrei Fajardo at LlamaIndex)\n\nThe llama-dataset collection. Each labelled llama-dataset is comprised of its\nassociated labelled examples. With examples, we make predictions with the\nappropriate object depending on the task. After making predictions, we can\nevaluate the performance of the object by measuring some distance between\npredictions and the corresponding references.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 390, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c44061f-c6b4-4463-848b-e9da70b5f4e6": {"__data__": {"id_": "9c44061f-c6b4-4463-848b-e9da70b5f4e6", "embedding": null, "metadata": {"Header_1": " Intro", "filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36e74daf-749b-4e62-b3a4-95002d7fcda1", "node_type": "4", "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "9fb2edde61de382b2df9e3f6054378477ffe6b3d62bce33dc4213cc5993ef185", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d9327e5-cadd-4009-8c8f-013157d1a779", "node_type": "1", "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "7205c01d33db125bcd8b08ca32eba0196cf1288b65cb5f8dc8d8309495f6888c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e442ae71-bb0b-4a98-82b2-3763846c8359", "node_type": "1", "metadata": {"Header_1": " A primer on the new llama-datasets"}, "hash": "273c69c606e1c4971f80f0276648d5e99c89ba8a6169875d5c3c5ccca58181ef", "class_name": "RelatedNodeInfo"}}, "text": "Intro\n\nA few weeks back, we [ launched ](/introducing-llama-datasets-aadb9994ad9e)\nour very first set of llama-datasets, namely the ` LabelledRagDataset ` . The\nmain purpose of these llama-datasets is to provide builders with the means to\nbenchmark their LLM systems in an effective and efficient manner. In the\ncouple of weeks since that launch date, we\u2019ve amassed over a dozen `\nLabelledRagDataset ` s via both staff and community contributions (all of\nwhich are available for download through [ LlamaHub ](https://llamahub.ai) )!\n\nThe fun doesn\u2019t stop there though: today we\u2019re introducing two new llama-\ndataset types: ` LabelledEvaluatorDataset ` and the `\nLabelledPairwiseEvaluatorDataset ` . These new llama-dataset types are meant\nfor evaluating or benchmarking an LLM evaluator. Indeed, the adopted standard\nfor evaluating LLM responses is to use a strong LLM as an evaluator. This\napproach is certainly more scalable, faster, and cheaper than using human\nevaluators via crowdsourcing. However, these LLM evaluators themselves must\nalso be continuously evaluated rather than blindly trusted.\n\nIn this post, we provide a brief overview of the new llama-datasets as well as\nprovide some very interesting results from benchmarking Google\u2019s Gemini and\nOpenAI\u2019s GPT models as LLM evaluators on the MT-Bench datasets which we\u2019ve\nconverted into the new llama-dataset types.", "mimetype": "text/plain", "start_char_idx": 395, "end_char_idx": 1770, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e442ae71-bb0b-4a98-82b2-3763846c8359": {"__data__": {"id_": "e442ae71-bb0b-4a98-82b2-3763846c8359", "embedding": null, "metadata": {"Header_1": " A primer on the new llama-datasets", "filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36e74daf-749b-4e62-b3a4-95002d7fcda1", "node_type": "4", "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "9fb2edde61de382b2df9e3f6054378477ffe6b3d62bce33dc4213cc5993ef185", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c44061f-c6b4-4463-848b-e9da70b5f4e6", "node_type": "1", "metadata": {"Header_1": " Intro", "filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "201433a58ffb3bafac1afd036102e68850756f47f3dfd6693823ac2aaaa3d727", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ba2e8dc-a4f9-4cad-aed9-f27adddd7fdc", "node_type": "1", "metadata": {"Header_1": " Benchmarking Gemini and GPT models as LLM evaluators: Gemini achieves"}, "hash": "cbaaf88c188d012b421d98bd86bf0e1429e491587bdf690eda7f257e5d0adf06", "class_name": "RelatedNodeInfo"}}, "text": "A primer on the new llama-datasets\n\nBefore getting into the new llama-datasets, recall that with `\nLabelledRagDataset ` our end goal was to use it to evaluate or benchmark a\nRetrieval-Augmented Generation (RAG) system. The way to do that with our\nllama-dataset abstractions is to build a ` QueryEngine ` (i.e., a RAG system)\nand then use it to make \u201cpredictions\u201d over the ` LabelledRagDataset ` . With\nthe predictions in hand, we can evaluate the quality of these predictions by\ncomparing it to the corresponding reference attributes of the `\nLabelledRagDataset ` .\n\nBenchmarking flow with LabelledRagDataset. With a query engine, predictions\nare made over every labelled example. We can then compare predicted responses\nand contexts with the reference versions (i.e., labels). This flow is\nconveniently handled via the RagEvaluatorPack.\n\nIn a similar vein, the new llama-datasets are meant to benchmark an LLM\nevaluator. Let\u2019s go through the first kind, the ` LabelledEvaluatorDataset ` .\nHere, instead of the RAG system making predictions on a ` LabelledRagDataset `\nwe have an LLM evaluator making \u201cpredictions\u201d over a `\nLabelledEvaluatorDataset ` \u2014 predictions in this context means that the LLM\nevaluator is evaluating the response produced by another LLM model to a given\nquery. As before, with the predictions in hand, we can measure the goodness of\nthe LLM evaluator\u2019s evaluations by comparing it to the corresponding reference\nattributes of the ` LabelledEvaluatorDataset ` .\n\nBenchmarking flow with LabelledEvaluatorDataset. With a supplied evaluator,\npredictions are made over every example. In this context, a prediction is an\nevaluation of the answer to the query and optional contexts and ground truth\nanswer. With these predictions in hand, we can evaluate how good the\nevaluations are by comparing them to the reference feedbacks and scores. A\nllama-pack called EvaluatorBenchmarkerPack makes benchmarking a one-liner.\n\nThe second llama-dataset we\u2019re introducing today can be considered an\nextension of the first one. The ` LabelledPairwiseEvaluatorDataset ` is\nsimilarly used for benchmarking an LLM evaluator. However, there is a subtle\ndifference in the evaluation task as here the LLM evaluator compares two\ngenerated answers from two separate LLMs. Outside of this difference, the flow\nfor using this llama-dataset to benchmark an evaluator remains the same.\n\nBenchmarking flow with LabelledPairwiseEvaluatorDataset. With a supplied\nevaluator, predictions are made over every example. In this context, a\nprediction is an evaluation of two answers to the query and optional contexts\nand ground truth answer. That is, the LLM evaluator ranks the two answers to\ndetermine the superior one. With these predictions in hand, we can evaluate\nhow good the evaluations are by comparing them to the reference feedbacks and\nscores. A llama-pack called EvaluatorBenchmarkerPack makes benchmarking a one-\nliner.", "mimetype": "text/plain", "start_char_idx": 1775, "end_char_idx": 4694, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ba2e8dc-a4f9-4cad-aed9-f27adddd7fdc": {"__data__": {"id_": "5ba2e8dc-a4f9-4cad-aed9-f27adddd7fdc", "embedding": null, "metadata": {"Header_1": " Benchmarking Gemini and GPT models as LLM evaluators: Gemini achieves", "filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36e74daf-749b-4e62-b3a4-95002d7fcda1", "node_type": "4", "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "9fb2edde61de382b2df9e3f6054378477ffe6b3d62bce33dc4213cc5993ef185", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e442ae71-bb0b-4a98-82b2-3763846c8359", "node_type": "1", "metadata": {"Header_1": " A primer on the new llama-datasets", "filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "e5d2fe60e329f10eb013d0068bbd0f0a6e47d009b7ae366e95007d69afc46af5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d546ef2-07f6-4368-b257-48a7feeb0f31", "node_type": "1", "metadata": {"Header_1": " Mini MT-Bench Single Grading Dataset"}, "hash": "9518bbe54001e1ac8bd7301af23d7bfb66aebe634f857e0b4539d7d7ee09c607", "class_name": "RelatedNodeInfo"}}, "text": "Benchmarking Gemini and GPT models as LLM evaluators: Gemini achieves\nGPT-3.5 performance!\n\nIn this section, we will put our new llama-dataset types to use in order to\npit Gemini Pro against GPT models. For this, we\u2019re going to use slightly\nadapted versions of the MT-Bench dataset. These adapted versions have been\nmade available for download and use through [ LlamaHub ](https://llamahub.ai)\nalong with today\u2019s release!", "mimetype": "text/plain", "start_char_idx": 4699, "end_char_idx": 5120, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d546ef2-07f6-4368-b257-48a7feeb0f31": {"__data__": {"id_": "8d546ef2-07f6-4368-b257-48a7feeb0f31", "embedding": null, "metadata": {"Header_1": " Mini MT-Bench Single Grading Dataset", "filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36e74daf-749b-4e62-b3a4-95002d7fcda1", "node_type": "4", "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "9fb2edde61de382b2df9e3f6054378477ffe6b3d62bce33dc4213cc5993ef185", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ba2e8dc-a4f9-4cad-aed9-f27adddd7fdc", "node_type": "1", "metadata": {"Header_1": " Benchmarking Gemini and GPT models as LLM evaluators: Gemini achieves", "filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "3346bb624d20e2953372fd9ac0d0d79a9d44a7a52f8a87bf2b450e39cbc55301", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e97e7bd0-8c39-407c-bd3c-6e151dad63ae", "node_type": "1", "metadata": {"Header_1": " Benchmark Results"}, "hash": "66b850ce839ad187012b9c6cc190f11888703122a75fc53f337324d5ec31b93b", "class_name": "RelatedNodeInfo"}}, "text": "Mini MT-Bench Single Grading Dataset\n\nThis llama-dataset is a ` LabelledEvaluatorDataset ` and is a miniature\nversion of the MT-Bench single-grading dataset. In particular, we consider all\nof the 160 original questions (i.e., 80 x 2, since MT Bench is a two-turn\nquestion dataset), but only the responses produced by Llama2-70b. For the\nreference evaluations, we use GPT-4. As with the original ` LabelledRagDataset\n` , we\u2019ve produced a new llama-pack ` EvaluatorBenchmarkerPack ` (of course,\nalso made available in today\u2019s release!) to make benchmarking an LLM evaluator\non the new llama-datasets relatively effortless. The below snippet of code is\nhow you can replicate the results of this benchmark\n\n    \n    \n    from llama_index.llama_dataset import download_llama_dataset\n    from llama_index.llama_pack import download_llama_pack\n    from llama_index.evaluation import CorrectnessEvaluator\n    from llama_index.llms import Gemini\n    from llama_index import ServiceContext\n    \n    \n    # download dataset\n    evaluator_dataset, _ = download_llama_dataset(\n        \"MiniMtBenchSingleGradingDataset\", \"./mini_mt_bench_data\"\n    )# define evaluator\n    gemini_pro_context = ServiceContext.from_defaults(\n        llm = Gemini(model=\"models/gemini-pro\", temperature=0)\n    )\n    evaluator = CorrectnessEvaluator(service_context=gemini_pro_context)# download EvaluatorBenchmarkerPack and define the benchmarker\n    EvaluatorBenchmarkerPack = download_llama_pack(\"EvaluatorBenchmarkerPack\", \"./pack\")\n    evaluator_benchmarker = EvaluatorBenchmarkerPack(\n        evaluator=evaluators[\"gpt-3.5\"],\n        eval_dataset=evaluator_dataset,\n        show_progress=True,\n    )# produce the benchmark result\n    benchmark_df = await evaluator_benchmarker.arun(\n    \t\tbatch_size=5,\n    \t\tsleep_time_in_seconds=0.5\n    )", "mimetype": "text/plain", "start_char_idx": 5125, "end_char_idx": 6936, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e97e7bd0-8c39-407c-bd3c-6e151dad63ae": {"__data__": {"id_": "e97e7bd0-8c39-407c-bd3c-6e151dad63ae", "embedding": null, "metadata": {"Header_1": " Benchmark Results", "filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36e74daf-749b-4e62-b3a4-95002d7fcda1", "node_type": "4", "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "9fb2edde61de382b2df9e3f6054378477ffe6b3d62bce33dc4213cc5993ef185", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d546ef2-07f6-4368-b257-48a7feeb0f31", "node_type": "1", "metadata": {"Header_1": " Mini MT-Bench Single Grading Dataset", "filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "a204141653712ff2055bc80b7c9b127b6ec79c5080f63a2ab8bc07ab7194328a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "73674001-612d-4f8d-9dab-af48f1e9809d", "node_type": "1", "metadata": {"Header_1": " MT-Bench Human Judgement Dataset"}, "hash": "a0edbdcfdcdd34dddaefae038d100790a321642afb5bb59afaf16363fb095cf9", "class_name": "RelatedNodeInfo"}}, "text": "Benchmark Results\n\nInvalid_predictions occurs whenever the LLM evaluator fails to produce the\ndesired output structure and as well as other exceptions. Correlations\nrepresent the correlations with the scores produced by each of the evaluators\nwith the reference scores produced by the reference evaluator GPT-4.\nSimilarly, the remaining two metrics, MAE (i.e., mean absolute error, which is\na sum of the absolute differences between each pair of evaluator and reference\nscores) and Hamming (i.e., which counts how many times evaluator and reference\nscores are equivalent), are computed with the scores produced by the\nevaluators and those from the reference evaluator.\n\n**Observations**\n\n  * It seems that Gemini-Pro and GPT-3.5 are quite close in terms of their closeness to the reference evaluator GPT-4! \n  * As for GPT-4 versus the reference GPT-4, this is mostly used for assessing self-consistency of the LLM, for which we see it does a fairly good job at that.", "mimetype": "text/plain", "start_char_idx": 6941, "end_char_idx": 7908, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "73674001-612d-4f8d-9dab-af48f1e9809d": {"__data__": {"id_": "73674001-612d-4f8d-9dab-af48f1e9809d", "embedding": null, "metadata": {"Header_1": " MT-Bench Human Judgement Dataset", "filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36e74daf-749b-4e62-b3a4-95002d7fcda1", "node_type": "4", "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "9fb2edde61de382b2df9e3f6054378477ffe6b3d62bce33dc4213cc5993ef185", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e97e7bd0-8c39-407c-bd3c-6e151dad63ae", "node_type": "1", "metadata": {"Header_1": " Benchmark Results", "filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "ac75846fa3203392dd9aefeb439d4bc3cb3092ea559d9fa543680c335404a156", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f05a7a9-1e15-4b6e-8188-b08cc548555b", "node_type": "1", "metadata": {"Header_1": " Benchmark Results"}, "hash": "7b43eba8242a8d7d5aca2c5d2987af946b46bb6c6576f4c025c3402e59b616c3", "class_name": "RelatedNodeInfo"}}, "text": "MT-Bench Human Judgement Dataset\n\nFor this benchmark, we\u2019ll evaluate the LLM evaluators on the task of ranking\ntwo LLM responses, to determine which of the two is the better one. And it is\nfor this such task that ` LabelledPairwiseEvaluatorDataset ` exists. The\nllama-dataset that we\u2019ve curated here is a slightly adapted version of the\noriginal MT-Bench Human Judgement dataset. Specifically, in the original\ndataset, there are some replications with respect to the triple (query, model\nA, model B) examples since for some of these more than one human evaluation\nwas provided. Since our prompt allows the LLM evaluator to deem a tie, and to\nour knowledge, this wasn\u2019t made an option for the human evaluators, we have\naggregated the results across the different human evaluations to get the\nproportion of times model A wins versus model B for each triple (query, model\nA, model B). We then say that human evaluators deem a tie if the proportion\nlies between 0.4 and 0.6. It should be emphasized here that the reference\nevaluations are provided by humans, and so the benchmark metrics that we\nproduce and share here represent the LLM agreement with humans.\n\n(We skip showing the code snipped to produce the results here, because they\u2019re\nessentially the same as the previously shared code snipper with the exception\nof requiring a ` PairwiseComparisonEvaluator ` instead of a `\nCorrectnessEvaluator ` .)", "mimetype": "text/plain", "start_char_idx": 7914, "end_char_idx": 9315, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f05a7a9-1e15-4b6e-8188-b08cc548555b": {"__data__": {"id_": "4f05a7a9-1e15-4b6e-8188-b08cc548555b", "embedding": null, "metadata": {"Header_1": " Benchmark Results", "filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36e74daf-749b-4e62-b3a4-95002d7fcda1", "node_type": "4", "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "9fb2edde61de382b2df9e3f6054378477ffe6b3d62bce33dc4213cc5993ef185", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "73674001-612d-4f8d-9dab-af48f1e9809d", "node_type": "1", "metadata": {"Header_1": " MT-Bench Human Judgement Dataset", "filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "a8ccf01340ed4313142bce04e46e0340fe77ba279a993d8a9e482e51966d8bf2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4c95036-5523-4df0-8102-590438921cd1", "node_type": "1", "metadata": {"Header_1": " Go now and evaluate your evaluators (and eat your veggies)!"}, "hash": "a8e7bbf6f47badfb3380a522d6ffa69971c938b36ad64366da14f3f012118c1d", "class_name": "RelatedNodeInfo"}}, "text": "Benchmark Results\n\nInvalid_predictions are as defined in the previous benchmark. Inconclusive\u2019s\nrepresent the case when an LLM evaluator flips its winner after prompting it\nwith the same evaluation task but instead flipping the order of presentation\nof the two answers (i.e. to mitigate against position bias). Two agreement\nrates, with the inclusion and exclusion of ties, are also produced \u2014 note that\nthese are both conditional in the event that the prediction (or evaluation) is\nvalid.\n\n**Observations**\n\n  * In terms of agreement rates, all three models seem quite close. Note again that these are conditional on the prediction/evaluation being valid. And so, one should \u201cdiscount\u201d these with the invalid and inconclusive counts. \n  * Gemini Pro and GPT-3.5 seem to be a bit more assertive than GPT-4 resulting in only 50\u201360 ties to GPT-4\u2019s 100 ties. \n  * Overall, it seems that Gemini Pro is up to snuff with GPT models, and would say that it outperforms GPT-3.5!", "mimetype": "text/plain", "start_char_idx": 9320, "end_char_idx": 10289, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d4c95036-5523-4df0-8102-590438921cd1": {"__data__": {"id_": "d4c95036-5523-4df0-8102-590438921cd1", "embedding": null, "metadata": {"Header_1": " Go now and evaluate your evaluators (and eat your veggies)!", "filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36e74daf-749b-4e62-b3a4-95002d7fcda1", "node_type": "4", "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "9fb2edde61de382b2df9e3f6054378477ffe6b3d62bce33dc4213cc5993ef185", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f05a7a9-1e15-4b6e-8188-b08cc548555b", "node_type": "1", "metadata": {"Header_1": " Benchmark Results", "filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "9468b677d52f5711e8014ea6142b4a5eb69bf2abd2c9863777167883ebc22df7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "090be34f-b3e0-4d87-8836-0500364a3dd5", "node_type": "1", "metadata": {"Header_1": " Related Links"}, "hash": "2d677f7229874a8cd0ac3faa09bf36f5a16fae481887fe4ccdd91c7e8e315159", "class_name": "RelatedNodeInfo"}}, "text": "Go now and evaluate your evaluators (and eat your veggies)!\n\nIt is, for obvious reasons, important to evaluate your LLM evaluators, as\nthese are now being relied upon to evaluate the performance of our LLM systems\n\u2014 a broken compass is not really helpful! With these newly introduced llama-\ndatasets, we hope that it is easy for you to compile your own benchmark\ndatasets on your own data, and then even easier to produce your benchmark\nmetrics. As mentioned before, the two llama-datasets discussed in this blog\nare available for download and use through [ LlamaHub ](https://llamahub.ai) .\nBe sure to visit and make use of the datasets there to build an exhaustive\nbenchmark suite! (We welcome contributed llama-datasets as well!)", "mimetype": "text/plain", "start_char_idx": 10295, "end_char_idx": 11027, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "090be34f-b3e0-4d87-8836-0500364a3dd5": {"__data__": {"id_": "090be34f-b3e0-4d87-8836-0500364a3dd5", "embedding": null, "metadata": {"Header_1": " Related Links", "filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36e74daf-749b-4e62-b3a4-95002d7fcda1", "node_type": "4", "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "9fb2edde61de382b2df9e3f6054378477ffe6b3d62bce33dc4213cc5993ef185", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4c95036-5523-4df0-8102-590438921cd1", "node_type": "1", "metadata": {"Header_1": " Go now and evaluate your evaluators (and eat your veggies)!", "filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "ac571393090ef9160de064871e269e55a59e508d76f2b95c91caf2daa28512fd", "class_name": "RelatedNodeInfo"}}, "text": "Related Links\n\n  * [ MT-Bench Human Judgement Benchmarking Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/evaluation/mt_bench_human_judgement.ipynb)\n  * [ MT-Bench Single Grading Benchmarking Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/evaluation/mt_bench_single_grading.ipynb)", "mimetype": "text/plain", "start_char_idx": 11032, "end_char_idx": 11370, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"3d9327e5-cadd-4009-8c8f-013157d1a779": {"doc_hash": "7205c01d33db125bcd8b08ca32eba0196cf1288b65cb5f8dc8d8309495f6888c", "ref_doc_id": "36e74daf-749b-4e62-b3a4-95002d7fcda1"}, "9c44061f-c6b4-4463-848b-e9da70b5f4e6": {"doc_hash": "201433a58ffb3bafac1afd036102e68850756f47f3dfd6693823ac2aaaa3d727", "ref_doc_id": "36e74daf-749b-4e62-b3a4-95002d7fcda1"}, "e442ae71-bb0b-4a98-82b2-3763846c8359": {"doc_hash": "e5d2fe60e329f10eb013d0068bbd0f0a6e47d009b7ae366e95007d69afc46af5", "ref_doc_id": "36e74daf-749b-4e62-b3a4-95002d7fcda1"}, "5ba2e8dc-a4f9-4cad-aed9-f27adddd7fdc": {"doc_hash": "3346bb624d20e2953372fd9ac0d0d79a9d44a7a52f8a87bf2b450e39cbc55301", "ref_doc_id": "36e74daf-749b-4e62-b3a4-95002d7fcda1"}, "8d546ef2-07f6-4368-b257-48a7feeb0f31": {"doc_hash": "a204141653712ff2055bc80b7c9b127b6ec79c5080f63a2ab8bc07ab7194328a", "ref_doc_id": "36e74daf-749b-4e62-b3a4-95002d7fcda1"}, "e97e7bd0-8c39-407c-bd3c-6e151dad63ae": {"doc_hash": "ac75846fa3203392dd9aefeb439d4bc3cb3092ea559d9fa543680c335404a156", "ref_doc_id": "36e74daf-749b-4e62-b3a4-95002d7fcda1"}, "73674001-612d-4f8d-9dab-af48f1e9809d": {"doc_hash": "a8ccf01340ed4313142bce04e46e0340fe77ba279a993d8a9e482e51966d8bf2", "ref_doc_id": "36e74daf-749b-4e62-b3a4-95002d7fcda1"}, "4f05a7a9-1e15-4b6e-8188-b08cc548555b": {"doc_hash": "9468b677d52f5711e8014ea6142b4a5eb69bf2abd2c9863777167883ebc22df7", "ref_doc_id": "36e74daf-749b-4e62-b3a4-95002d7fcda1"}, "d4c95036-5523-4df0-8102-590438921cd1": {"doc_hash": "ac571393090ef9160de064871e269e55a59e508d76f2b95c91caf2daa28512fd", "ref_doc_id": "36e74daf-749b-4e62-b3a4-95002d7fcda1"}, "090be34f-b3e0-4d87-8836-0500364a3dd5": {"doc_hash": "d6b83b2652e80a0a1b2dbab02b233ae875856b129a67309d364c27295d1caf2f", "ref_doc_id": "36e74daf-749b-4e62-b3a4-95002d7fcda1"}}, "docstore/ref_doc_info": {"36e74daf-749b-4e62-b3a4-95002d7fcda1": {"node_ids": ["3d9327e5-cadd-4009-8c8f-013157d1a779", "9c44061f-c6b4-4463-848b-e9da70b5f4e6", "e442ae71-bb0b-4a98-82b2-3763846c8359", "5ba2e8dc-a4f9-4cad-aed9-f27adddd7fdc", "8d546ef2-07f6-4368-b257-48a7feeb0f31", "e97e7bd0-8c39-407c-bd3c-6e151dad63ae", "73674001-612d-4f8d-9dab-af48f1e9809d", "4f05a7a9-1e15-4b6e-8188-b08cc548555b", "d4c95036-5523-4df0-8102-590438921cd1", "090be34f-b3e0-4d87-8836-0500364a3dd5"], "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}}}}