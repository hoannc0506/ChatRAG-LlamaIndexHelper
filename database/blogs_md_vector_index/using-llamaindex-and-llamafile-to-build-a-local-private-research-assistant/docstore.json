{"docstore/data": {"44ee249f-7977-4a22-a309-4e4bf8608717": {"__data__": {"id_": "44ee249f-7977-4a22-a309-4e4bf8608717", "embedding": null, "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c3c97dc-7de4-40cb-972c-08a109346c7b", "node_type": "4", "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "b9758283ffec56a30a8f3ccef0d62673749b98c47cbbfb461b14b6b706bf8c4e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5bbf595-c800-4c51-a061-b6587ae5f7a6", "node_type": "1", "metadata": {"Header_2": " Download and run a llamafile"}, "hash": "fc259ba8f54dd69a59d676d99adbd040c5fb59689447e9560578f9003a70c604", "class_name": "RelatedNodeInfo"}}, "text": "_This is a guest post from our friends at Mozilla about[ Llamafile\n](https://future.mozilla.org/news/llamafile-four-months-of-progress-towards-\ndemocratizing-ai/) _\n\n[ llamafile ](https://github.com/Mozilla-Ocho/llamafile) , an open source\nproject from Mozilla, is one of the simplest ways to run a large language\nmodel (LLM) on your laptop. All you have to do is download a llamafile from [\nHuggingFace ](https://huggingface.co/models?sort=trending&search=llamafile)\nthen run the file. That's it. **On most computers, you won't need to install\nanything.**\n\nThere are a few reasons why you might want to run an LLM on your laptop,\nincluding:\n\n1\\. Privacy: Running locally means you won't have to share your data with\nthird parties.\n\n2\\. High availability: Run your LLM-based app without an internet connection.\n\n3\\. Bring your own model: You can easily test many different open-source LLMs\n(anything available on HuggingFace) and see which one works best for your\ntask.\n\n4\\. Free debugging/testing: Local LLMs allow you to test many parts of an LLM-\nbased system without paying for API calls.\n\nIn this blog post, we'll show how to set up a llamafile and use it to run a\nlocal LLM on your computer. Then, we'll show how to use LlamaIndex with your\nllamafile as the LLM & embedding backend for a local RAG-based research\nassistant. You won't have to sign up for any cloud service or send your data\nto any third party--everything will just run on your laptop.\n\nNote: You can also get all of the example code below as a Jupyter notebook\nfrom our [ GitHub repo ](https://github.com/Mozilla-Ocho/llamafile-llamaindex-\nexamples) .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1623, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5bbf595-c800-4c51-a061-b6587ae5f7a6": {"__data__": {"id_": "c5bbf595-c800-4c51-a061-b6587ae5f7a6", "embedding": null, "metadata": {"Header_2": " Download and run a llamafile", "filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c3c97dc-7de4-40cb-972c-08a109346c7b", "node_type": "4", "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "b9758283ffec56a30a8f3ccef0d62673749b98c47cbbfb461b14b6b706bf8c4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44ee249f-7977-4a22-a309-4e4bf8608717", "node_type": "1", "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "dedade5328517ec32cd89f75319374f52c671e9d9f7968c6f2a8888d93a28c6a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72921740-3af6-46eb-8732-1ae543498ca8", "node_type": "1", "metadata": {"Header_2": " Download and run a llamafile", "Header_3": " Step 1: Download a llamafile"}, "hash": "8c8ac472ac0d83aaabcaff9a5ca0fe8cb2eb22e4634e55b7b239a6f68361c0d6", "class_name": "RelatedNodeInfo"}}, "text": "Download and run a llamafile\n\nFirst, what is a llamafile? A llamafile is an executable LLM that you can run\non your own computer. It contains the weights for a given open source LLM, as\nwell as everything needed to actually run that model on your computer. There's\nnothing to install or configure (with a few caveats, discussed [ here\n](https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file#gotchas) ).\n\nEach llamafile bundles 1) model weights & metadata in gguf format + 2) a copy\nof [ `llama.cpp` ](https://github.com/ggerganov/llama.cpp) specially compiled\nusing [Cosmopolitan Libc](https://github.com/jart/cosmopolitan). This allows\nthe models to run on most computers without additional installation.\nllamafiles also come with a ChatGPT-like browser interface, a CLI, and an\nOpenAI-compatible REST API for chat models.\n\nThere are only 2 steps to setting up a llamafile:\n\n1\\. Download a llamafile\n\n2\\. Make the llamafile executable\n\nWe'll go through each step in detail below.", "mimetype": "text/plain", "start_char_idx": 1629, "end_char_idx": 2616, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72921740-3af6-46eb-8732-1ae543498ca8": {"__data__": {"id_": "72921740-3af6-46eb-8732-1ae543498ca8", "embedding": null, "metadata": {"Header_2": " Download and run a llamafile", "Header_3": " Step 1: Download a llamafile", "filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c3c97dc-7de4-40cb-972c-08a109346c7b", "node_type": "4", "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "b9758283ffec56a30a8f3ccef0d62673749b98c47cbbfb461b14b6b706bf8c4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5bbf595-c800-4c51-a061-b6587ae5f7a6", "node_type": "1", "metadata": {"Header_2": " Download and run a llamafile", "filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "521b9de8ee579ce101ce43e6756beeca4f5710d59fd065a1d80b9df143453160", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10b1a54c-ff72-4111-8909-171c6d16ae9c", "node_type": "1", "metadata": {"Header_2": " Download and run a llamafile", "Header_3": " Step 2: Make the llamafile executable"}, "hash": "27d1f81d72747f86754bd308188280a2d298d77824cd6a70bc5fdac8c910c01f", "class_name": "RelatedNodeInfo"}}, "text": "Step 1: Download a llamafile\n\nThere are many llamafiles available on the [ HuggingFace model hub\n](https://huggingface.co/models?sort=trending&search=llamafile) (just search\nfor 'llamafile') but for the purpose of this walkthrough, we'll use [\nTinyLlama-1.1B\n](https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile)\n(0.67 GB, [ model info\n](https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile) ). To\ndownload the model, you can either click this download link: [ TinyLlama-1.1B\n](https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile?download=true)\nor open a terminal and use something like `wget`. The download should take\n5-10 minutes depending on the quality of your internet connection.\n\n    \n    \n    wget https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile \n\nThis model is small and won't be very good at actually answering questions\nbut, since it's a relatively quick download and its inference speed will allow\nyou to index your vector store in just a few minutes, it's good enough for the\nexamples below. For a higher-quality LLM, you may want to use a larger model\nlike [ Mistral-7B-Instruct\n](https://huggingface.co/Mozilla/Mistral-7B-Instruct-v0.2-llamafile/resolve/main/mistral-7b-instruct-v0.2.Q4_0.llamafile?download=true)\n(5.15 GB, [ model info\n](https://huggingface.co/Mozilla/Mistral-7B-Instruct-v0.2-llamafile) ).", "mimetype": "text/plain", "start_char_idx": 2623, "end_char_idx": 4163, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10b1a54c-ff72-4111-8909-171c6d16ae9c": {"__data__": {"id_": "10b1a54c-ff72-4111-8909-171c6d16ae9c", "embedding": null, "metadata": {"Header_2": " Download and run a llamafile", "Header_3": " Step 2: Make the llamafile executable", "filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c3c97dc-7de4-40cb-972c-08a109346c7b", "node_type": "4", "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "b9758283ffec56a30a8f3ccef0d62673749b98c47cbbfb461b14b6b706bf8c4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72921740-3af6-46eb-8732-1ae543498ca8", "node_type": "1", "metadata": {"Header_2": " Download and run a llamafile", "Header_3": " Step 1: Download a llamafile", "filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "774b186a70c28cef2465558b91c9b157bd797e3d20fdb3f7ea1bd1d853d6e601", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0414ed7d-18b9-49d8-9caa-172696c2324d", "node_type": "1", "metadata": {"Header_2": " Download and run a llamafile", "Header_3": " Kick the tires"}, "hash": "05f827ec3f7616b0f7bc042855432f019c14fcccc0708a1e8c9c0fba2cdc21f1", "class_name": "RelatedNodeInfo"}}, "text": "Step 2: Make the llamafile executable\n\nIf you didn't download the llamafile from the command line, figure out where\nyour browser stored your downloaded llamafile.\n\nNow, open your computer's terminal and, if necessary, go to the directory\nwhere your llamafile is stored: `cd path/to/downloaded/llamafile`\n\n**If you're using macOS, Linux, or BSD** , you'll need to grant permission for\nyour computer to execute this new file. (You only need to do this once.):\n\n**If you're on Windows, instead just rename the file by adding \".exe\" on the\nend** e.g. rename `TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile` to\n`TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile.exe`\n\n    \n    \n    chmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile", "mimetype": "text/plain", "start_char_idx": 4170, "end_char_idx": 4884, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0414ed7d-18b9-49d8-9caa-172696c2324d": {"__data__": {"id_": "0414ed7d-18b9-49d8-9caa-172696c2324d", "embedding": null, "metadata": {"Header_2": " Download and run a llamafile", "Header_3": " Kick the tires", "filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c3c97dc-7de4-40cb-972c-08a109346c7b", "node_type": "4", "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "b9758283ffec56a30a8f3ccef0d62673749b98c47cbbfb461b14b6b706bf8c4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10b1a54c-ff72-4111-8909-171c6d16ae9c", "node_type": "1", "metadata": {"Header_2": " Download and run a llamafile", "Header_3": " Step 2: Make the llamafile executable", "filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "78d2f10fbb59efd8d3f2928090c6b55baa92c0d4de1bb1137a134d1f83bc25be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14cf6910-1ca4-414b-a2bc-b7ffc18a5a76", "node_type": "1", "metadata": {"Header_2": " Download and run a llamafile", "Header_3": " Summary: Download and run a llamafile"}, "hash": "f2004224f0219b8c4135b6cb19644f54a558ea53b5dab1df73cb13366b94a41d", "class_name": "RelatedNodeInfo"}}, "text": "Kick the tires\n\nNow, your llamafile should be ready to go. First, you can check which version\nof the llamafile library was used to build the llamafile binary you should\ndownloaded:\n\n    \n    \n    ./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --version\n    \n    llamafile v0.7.0\n\nThis post was written using a model built with `llamafile v0.7.0`. If your\nllamafile displays a different version and some of the steps below don't work\nas expected, please [ post an issue on the llamafile issue tracker\n](https://github.com/Mozilla-Ocho/llamafile/issues) .\n\nThe easiest way to use your llamafile is via its built-in chat interface. In a\nterminal, run\n\n    \n    \n    ./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile\n\nYour browser should open automatically and display a chat interface. (If it\ndoesn't, just open your browser and point it at http://localhost:8080). When\nyou're done chatting, return to your terminal and hit `Control-C` to shut down\nllamafile. If you're running these commands inside a notebook, just interrupt\nthe notebook kernel to stop the llamafile.\n\nIn the rest of this walkthrough, we'll be using the llamafile's built-in\ninference server instead of the browser interface. The llamafile's server\nprovides a REST API for interacting with the TinyLlama LLM via HTTP. Full\nserver API documentation is available [ here ](https://github.com/Mozilla-\nOcho/llamafile/blob/main/llama.cpp/server/README.md#api-endpoints) . To start\nthe llamafile in server mode, run:\n\n    \n    \n    ./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding", "mimetype": "text/plain", "start_char_idx": 4891, "end_char_idx": 6450, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14cf6910-1ca4-414b-a2bc-b7ffc18a5a76": {"__data__": {"id_": "14cf6910-1ca4-414b-a2bc-b7ffc18a5a76", "embedding": null, "metadata": {"Header_2": " Download and run a llamafile", "Header_3": " Summary: Download and run a llamafile", "filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c3c97dc-7de4-40cb-972c-08a109346c7b", "node_type": "4", "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "b9758283ffec56a30a8f3ccef0d62673749b98c47cbbfb461b14b6b706bf8c4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0414ed7d-18b9-49d8-9caa-172696c2324d", "node_type": "1", "metadata": {"Header_2": " Download and run a llamafile", "Header_3": " Kick the tires", "filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "b4270f973e32087ed4aac8cc83ac324987b81fa97d3b0685a2b7e870bf9b2302", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9742ffe2-23fa-43ca-9ea8-fbc0ed288d03", "node_type": "1", "metadata": {"Header_2": " Build a research assistant using LlamaIndex and llamafile"}, "hash": "52c5042e8d007d7cadba84233525e52a28fb59369bf3c9b90b57320a8335ae3b", "class_name": "RelatedNodeInfo"}}, "text": "Summary: Download and run a llamafile\n\n    \n    \n    # 1. Download the llamafile-ized model\n    wget https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile\n    \n    # 2. Make it executable (you only need to do this once)\n    chmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile\n    \n    # 3. Run in server mode\n    ./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding", "mimetype": "text/plain", "start_char_idx": 6457, "end_char_idx": 6909, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9742ffe2-23fa-43ca-9ea8-fbc0ed288d03": {"__data__": {"id_": "9742ffe2-23fa-43ca-9ea8-fbc0ed288d03", "embedding": null, "metadata": {"Header_2": " Build a research assistant using LlamaIndex and llamafile", "filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c3c97dc-7de4-40cb-972c-08a109346c7b", "node_type": "4", "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "b9758283ffec56a30a8f3ccef0d62673749b98c47cbbfb461b14b6b706bf8c4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14cf6910-1ca4-414b-a2bc-b7ffc18a5a76", "node_type": "1", "metadata": {"Header_2": " Download and run a llamafile", "Header_3": " Summary: Download and run a llamafile", "filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "8e6eb22cf8756cc928d96ad371072f6874157dc87b150717b106c70f048839a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0775a7e2-77b2-43d8-b1c1-5e130d7c5678", "node_type": "1", "metadata": {"Header_2": " Build a research assistant using LlamaIndex and llamafile", "Header_3": " Start your llamafile server and configure LlamaIndex"}, "hash": "790ccadaaaee4be7d02b96c2df535c9bf66fdf18ba6ef73128853340ad9bee2d", "class_name": "RelatedNodeInfo"}}, "text": "Build a research assistant using LlamaIndex and llamafile\n\nNow, we'll show how to use LlamaIndex with your llamafile to build a research\nassistant to help you learn about some topic of interest--for this post, we\nchose [ homing pigeons ](https://en.wikipedia.org/wiki/Homing_pigeon) . We'll\nshow how to prepare your data, index into a vector store, then query it.\n\nOne of the nice things about running an LLM locally is privacy. You can mix\nboth \"public data\" like Wikipedia pages and \"private data\" without worrying\nabout sharing your data with a third party. Private data could include e.g.\nyour private notes on a topic or PDFs of classified content. As long as you\nuse a local LLM (and a local vector store), you won't have to worry about\nleaking data. Below, we'll show how to combine both types of data. Our vector\nstore will include Wikipedia pages, an Army manual on caring for homing\npigeons, and some brief notes we took while we were reading about this topic.\n\nTo get started, download our example data:\n\n    \n    \n    mkdir data\n    \n    # Download 'The Homing Pigeon' manual from Project Gutenberg\n    wget https://www.gutenberg.org/cache/epub/55084/pg55084.txt -O data/The_Homing_Pigeon.txt\n    \n    # Download some notes on homing pigeons\n    wget https://gist.githubusercontent.com/k8si/edf5a7ca2cc3bef7dd3d3e2ca42812de/raw/24955ee9df819e21975b1dd817938c1bfe955634/homing_pigeon_notes.md -O data/homing_pigeon_notes.md\n\nNext, we'll need to install LlamaIndex and a few of its integrations:\n\n    \n    \n    # Install llama-index\n    pip install llama-index-core\n    # Install llamafile integrations and SimpleWebPageReader\n    pip install llama-index-embeddings-llamafile llama-index-llms-llamafile llama-index-readers-web", "mimetype": "text/plain", "start_char_idx": 6915, "end_char_idx": 8651, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0775a7e2-77b2-43d8-b1c1-5e130d7c5678": {"__data__": {"id_": "0775a7e2-77b2-43d8-b1c1-5e130d7c5678", "embedding": null, "metadata": {"Header_2": " Build a research assistant using LlamaIndex and llamafile", "Header_3": " Start your llamafile server and configure LlamaIndex", "filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c3c97dc-7de4-40cb-972c-08a109346c7b", "node_type": "4", "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "b9758283ffec56a30a8f3ccef0d62673749b98c47cbbfb461b14b6b706bf8c4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9742ffe2-23fa-43ca-9ea8-fbc0ed288d03", "node_type": "1", "metadata": {"Header_2": " Build a research assistant using LlamaIndex and llamafile", "filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "f37c4f34891eb42598c3642a30c0dc5e1755c64f33578267e76b561cdfbf509e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85225d9f-cc74-49a9-85e1-19a600acc871", "node_type": "1", "metadata": {"Header_2": " Build a research assistant using LlamaIndex and llamafile", "Header_3": " Prepare your data and build a vector store"}, "hash": "ecedefbd33802a3db94ed4aaf9711b1fb657603c43849113eb8a6a2e872c90a7", "class_name": "RelatedNodeInfo"}}, "text": "Start your llamafile server and configure LlamaIndex\n\nIn this example, we'll use the same llamafile to both produce the embeddings\nthat will get indexed in our vector store and as the LLM that will answer\nqueries later on. (However, there is no reason you can't use one llamafile for\nthe embeddings and separate llamafile for the LLM functionality--you would\njust need to start the llamafile servers on different ports.)\n\nTo start the llamafile server, open a terminal and run:\n\n    \n    \n    ./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding --port 8080\n\nNow, we'll configure LlamaIndex to use this llamafile:\n\n    \n    \n    # Configure LlamaIndex\n    from llama_index.core import Settings\n    from llama_index.embeddings.llamafile import LlamafileEmbedding\n    from llama_index.llms.llamafile import Llamafile\n    from llama_index.core.node_parser import SentenceSplitter\n    \n    Settings.embed_model = LlamafileEmbedding(base_url=\"http://localhost:8080\")\n    \n    Settings.llm = Llamafile(\n    \tbase_url=\"http://localhost:8080\",\n    \ttemperature=0,\n    \tseed=0\n    )\n    \n    # Also set up a sentence splitter to ensure texts are broken into semantically-meaningful chunks (sentences) that don't take up the model's entire\n    # context window (2048 tokens). Since these chunks will be added to LLM prompts as part of the RAG process, we want to leave plenty of space for both\n    # the system prompt and the user's actual question.\n    Settings.transformations = [\n    \tSentenceSplitter(\n        \tchunk_size=256,\n        \tchunk_overlap=5\n    \t)\n    ]", "mimetype": "text/plain", "start_char_idx": 8658, "end_char_idx": 10240, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85225d9f-cc74-49a9-85e1-19a600acc871": {"__data__": {"id_": "85225d9f-cc74-49a9-85e1-19a600acc871", "embedding": null, "metadata": {"Header_2": " Build a research assistant using LlamaIndex and llamafile", "Header_3": " Prepare your data and build a vector store", "filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c3c97dc-7de4-40cb-972c-08a109346c7b", "node_type": "4", "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "b9758283ffec56a30a8f3ccef0d62673749b98c47cbbfb461b14b6b706bf8c4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0775a7e2-77b2-43d8-b1c1-5e130d7c5678", "node_type": "1", "metadata": {"Header_2": " Build a research assistant using LlamaIndex and llamafile", "Header_3": " Start your llamafile server and configure LlamaIndex", "filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "3a7e11ea2748f8a8479f5ebc8e5cd5cc757c2bea7bf9583d1fd4d62399bc01f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "775e84d4-dc68-480e-9fec-b0f1f44ca9ea", "node_type": "1", "metadata": {"Header_2": " Build a research assistant using LlamaIndex and llamafile", "Header_3": " Query your research assistant"}, "hash": "d0495321245ef3563ad704d56b484c0721bdb30852b67465c97b3f0fcccc5e92", "class_name": "RelatedNodeInfo"}}, "text": "Prepare your data and build a vector store\n\nNow, we'll load our data and index it.\n\n    \n    \n    # Load local data\n    from llama_index.core import SimpleDirectoryReader\n    local_doc_reader = SimpleDirectoryReader(input_dir='./data')\n    docs = local_doc_reader.load_data(show_progress=True)\n    \n    # We'll load some Wikipedia pages as well\n    from llama_index.readers.web import SimpleWebPageReader\n    urls = [\n    \t'https://en.wikipedia.org/wiki/Homing_pigeon',\n    \t'https://en.wikipedia.org/wiki/Magnetoreception',\n    ]\n    web_reader = SimpleWebPageReader(html_to_text=True)\n    docs.extend(web_reader.load_data(urls))\n    \n    # Build the index\n    from llama_index.core import VectorStoreIndex\n    \n    index = VectorStoreIndex.from_documents(\n    \tdocs,\n    \tshow_progress=True,\n    )\n    \n    # Save the index\n    index.storage_context.persist(persist_dir=\"./storage\")", "mimetype": "text/plain", "start_char_idx": 10247, "end_char_idx": 11131, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "775e84d4-dc68-480e-9fec-b0f1f44ca9ea": {"__data__": {"id_": "775e84d4-dc68-480e-9fec-b0f1f44ca9ea", "embedding": null, "metadata": {"Header_2": " Build a research assistant using LlamaIndex and llamafile", "Header_3": " Query your research assistant", "filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c3c97dc-7de4-40cb-972c-08a109346c7b", "node_type": "4", "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "b9758283ffec56a30a8f3ccef0d62673749b98c47cbbfb461b14b6b706bf8c4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85225d9f-cc74-49a9-85e1-19a600acc871", "node_type": "1", "metadata": {"Header_2": " Build a research assistant using LlamaIndex and llamafile", "Header_3": " Prepare your data and build a vector store", "filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "e37ae7d587107bae33ddaa8da13cd1307c72dff95a9a3b60d5edbd3a3a8a2f67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea847198-7e90-493d-8f8c-3e4a6458de6a", "node_type": "1", "metadata": {"Header_2": " Conclusion"}, "hash": "d6309b0666b63c19eef10f9d53307287e1affb05949d79111ca64ec77eb580b7", "class_name": "RelatedNodeInfo"}}, "text": "Query your research assistant\n\nFinally, we're ready to ask some questions about homing pigeons.\n\n    \n    \n    query_engine = index.as_query_engine()\n    print(query_engine.query(\"What were homing pigeons used for?\"))\n    \n    \n    \tHoming pigeons were used for a variety of purposes, including military reconnaissance, communication, and transportation. They were also used for scientific research, such as studying the behavior of birds in flight and their migration patterns. In addition, they were used for religious ceremonies and as a symbol of devotion and loyalty. Overall, homing pigeons played an important role in the history of aviation and were a symbol of the human desire for communication and connection.\n    \n    \n    print(query_engine.query(\"When were homing pigeons first used?\"))\n    \n    \n    The context information provided in the given context is that homing pigeons were first used in the 19th century. However, prior knowledge would suggest that homing pigeons have been used for navigation and communication for centuries.", "mimetype": "text/plain", "start_char_idx": 11138, "end_char_idx": 12188, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea847198-7e90-493d-8f8c-3e4a6458de6a": {"__data__": {"id_": "ea847198-7e90-493d-8f8c-3e4a6458de6a", "embedding": null, "metadata": {"Header_2": " Conclusion", "filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c3c97dc-7de4-40cb-972c-08a109346c7b", "node_type": "4", "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "b9758283ffec56a30a8f3ccef0d62673749b98c47cbbfb461b14b6b706bf8c4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "775e84d4-dc68-480e-9fec-b0f1f44ca9ea", "node_type": "1", "metadata": {"Header_2": " Build a research assistant using LlamaIndex and llamafile", "Header_3": " Query your research assistant", "filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "6115717bc794a7320dd5a2d2ccb206b72e754bdf3a20299def59cc82d3d16344", "class_name": "RelatedNodeInfo"}}, "text": "Conclusion\n\nIn this post, we've shown how to download and set up an LLM running locally\nvia llamafile. Then, we showed how to use this LLM with LlamaIndex to build a\nsimple RAG-based research assistant for learning about homing pigeons. Your\nassistant ran 100% locally: you didn't have to pay for API calls or send data\nto a third party.\n\nAs a next step, you could try running the examples above with a better model\nlike [ Mistral-7B-Instruct\n](https://huggingface.co/Mozilla/Mistral-7B-Instruct-v0.2-llamafile/resolve/main/mistral-7b-instruct-v0.2.Q4_0.llamafile?download=true)\n. You could also try building a research assistant for different topic like\n\"semiconductors\" or \"how to bake bread\".\n\nTo find out more about llamafile, check out the project on [ GitHub\n](https://github.com/Mozilla-Ocho/llamafile) , read this [ blog post\n](https://justine.lol/oneliners/) on bash one-liners using LLMs, or say hi to\nthe community on [ Discord ](https://discord.com/invite/teDuGYVTB2) .", "mimetype": "text/plain", "start_char_idx": 12194, "end_char_idx": 13175, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"44ee249f-7977-4a22-a309-4e4bf8608717": {"doc_hash": "dedade5328517ec32cd89f75319374f52c671e9d9f7968c6f2a8888d93a28c6a", "ref_doc_id": "9c3c97dc-7de4-40cb-972c-08a109346c7b"}, "c5bbf595-c800-4c51-a061-b6587ae5f7a6": {"doc_hash": "521b9de8ee579ce101ce43e6756beeca4f5710d59fd065a1d80b9df143453160", "ref_doc_id": "9c3c97dc-7de4-40cb-972c-08a109346c7b"}, "72921740-3af6-46eb-8732-1ae543498ca8": {"doc_hash": "774b186a70c28cef2465558b91c9b157bd797e3d20fdb3f7ea1bd1d853d6e601", "ref_doc_id": "9c3c97dc-7de4-40cb-972c-08a109346c7b"}, "10b1a54c-ff72-4111-8909-171c6d16ae9c": {"doc_hash": "78d2f10fbb59efd8d3f2928090c6b55baa92c0d4de1bb1137a134d1f83bc25be", "ref_doc_id": "9c3c97dc-7de4-40cb-972c-08a109346c7b"}, "0414ed7d-18b9-49d8-9caa-172696c2324d": {"doc_hash": "b4270f973e32087ed4aac8cc83ac324987b81fa97d3b0685a2b7e870bf9b2302", "ref_doc_id": "9c3c97dc-7de4-40cb-972c-08a109346c7b"}, "14cf6910-1ca4-414b-a2bc-b7ffc18a5a76": {"doc_hash": "8e6eb22cf8756cc928d96ad371072f6874157dc87b150717b106c70f048839a4", "ref_doc_id": "9c3c97dc-7de4-40cb-972c-08a109346c7b"}, "9742ffe2-23fa-43ca-9ea8-fbc0ed288d03": {"doc_hash": "f37c4f34891eb42598c3642a30c0dc5e1755c64f33578267e76b561cdfbf509e", "ref_doc_id": "9c3c97dc-7de4-40cb-972c-08a109346c7b"}, "0775a7e2-77b2-43d8-b1c1-5e130d7c5678": {"doc_hash": "3a7e11ea2748f8a8479f5ebc8e5cd5cc757c2bea7bf9583d1fd4d62399bc01f3", "ref_doc_id": "9c3c97dc-7de4-40cb-972c-08a109346c7b"}, "85225d9f-cc74-49a9-85e1-19a600acc871": {"doc_hash": "e37ae7d587107bae33ddaa8da13cd1307c72dff95a9a3b60d5edbd3a3a8a2f67", "ref_doc_id": "9c3c97dc-7de4-40cb-972c-08a109346c7b"}, "775e84d4-dc68-480e-9fec-b0f1f44ca9ea": {"doc_hash": "6115717bc794a7320dd5a2d2ccb206b72e754bdf3a20299def59cc82d3d16344", "ref_doc_id": "9c3c97dc-7de4-40cb-972c-08a109346c7b"}, "ea847198-7e90-493d-8f8c-3e4a6458de6a": {"doc_hash": "1dddd8c3f3ca44e27a6f448811cc6997e1f6476793e8199cc4913c4e23ec832c", "ref_doc_id": "9c3c97dc-7de4-40cb-972c-08a109346c7b"}}, "docstore/ref_doc_info": {"9c3c97dc-7de4-40cb-972c-08a109346c7b": {"node_ids": ["44ee249f-7977-4a22-a309-4e4bf8608717", "c5bbf595-c800-4c51-a061-b6587ae5f7a6", "72921740-3af6-46eb-8732-1ae543498ca8", "10b1a54c-ff72-4111-8909-171c6d16ae9c", "0414ed7d-18b9-49d8-9caa-172696c2324d", "14cf6910-1ca4-414b-a2bc-b7ffc18a5a76", "9742ffe2-23fa-43ca-9ea8-fbc0ed288d03", "0775a7e2-77b2-43d8-b1c1-5e130d7c5678", "85225d9f-cc74-49a9-85e1-19a600acc871", "775e84d4-dc68-480e-9fec-b0f1f44ca9ea", "ea847198-7e90-493d-8f8c-3e4a6458de6a"], "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}}}}