{"docstore/data": {"0e6108aa-5c46-4704-96f0-d211af93c3d3": {"__data__": {"id_": "0e6108aa-5c46-4704-96f0-d211af93c3d3", "embedding": null, "metadata": {"Header_1": " **Summary**", "filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0dec2af8-569d-4446-ad81-67bf451cc3b2", "node_type": "4", "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "af8d98ece7e1a1cac7bf823a56c9995022f6eaac0ea91d442c4c37000c95b26a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "425a1d42-4fc4-47d5-aa9a-e5fb842dafa0", "node_type": "1", "metadata": {"Header_1": " **Introduction and Background**"}, "hash": "aae962d91b63ddf4ec97bd466d92a5bc8dc93ba0ebd063db20beccf3441d0472", "class_name": "RelatedNodeInfo"}}, "text": "**Summary**\n\nThis blog post outlines some of the core abstractions we have created in [\nLlamaIndex ](https://github.com/jerryjliu/llama_index) around LLM-powered\nretrieval and reranking, which helps to create enhancements to document\nretrieval beyond naive top-k embedding-based lookup.\n\nLLM-powered retrieval can return more relevant documents than embedding-based\nretrieval, with the tradeoff being much higher latency and cost. We show how\nusing embedding-based retrieval as a first-stage pass, and second-stage\nretrieval as a reranking step can help provide a happy medium. We provide\nresults over the Great Gatsby and the Lyft SEC 10-k.\n\nTwo-stage retrieval pipeline: 1) Top-k embedding retrieval, then 2) LLM-based\nreranking", "mimetype": "text/plain", "start_char_idx": 3, "end_char_idx": 733, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "425a1d42-4fc4-47d5-aa9a-e5fb842dafa0": {"__data__": {"id_": "425a1d42-4fc4-47d5-aa9a-e5fb842dafa0", "embedding": null, "metadata": {"Header_1": " **Introduction and Background**", "filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0dec2af8-569d-4446-ad81-67bf451cc3b2", "node_type": "4", "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "af8d98ece7e1a1cac7bf823a56c9995022f6eaac0ea91d442c4c37000c95b26a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e6108aa-5c46-4704-96f0-d211af93c3d3", "node_type": "1", "metadata": {"Header_1": " **Summary**", "filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "a38148ff8c3f65688251ea64862b6a8721b17cd960bc8138ee3e3fb1b448a85b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1834449a-67db-4fb9-8446-c322e366cd3c", "node_type": "1", "metadata": {"Header_1": " **LLM Retrieval and Reranking**"}, "hash": "6f1095cbc3eacbc20d8b9357ebac3db1b416b5d2fad005bfd9ac75846695eaa0", "class_name": "RelatedNodeInfo"}}, "text": "**Introduction and Background**\n\nThere has been a wave of \u201cBuild a chatbot over your data\u201d applications in the\npast few months, made possible with frameworks like [ LlamaIndex\n](https://github.com/jerryjliu/llama_index) and [ LangChain\n](https://github.com/hwchase17/langchain) . A lot of these applications use a\nstandard stack for retrieval augmented generation (RAG):\n\n  * Use a vector store to store unstructured documents (knowledge corpus) \n  * Given a query, use a **retrieval model** to retrieve relevant documents from the corpus, and a **synthesis model** to generate a response. \n  * The **retrieval model** fetches the top-k documents by embedding similarity to the query. \n\nIn this stack, the retrieval model is not a novel idea; the concept of top-k\nembedding-based semantic search has been around for at least a decade, and\ndoesn\u2019t involve the LLM at all.\n\nThere are a lot of benefits to embedding-based retrieval:\n\n  * It\u2019s very fast to compute dot products. Doesn\u2019t require any model calls during query-time. \n  * Even if not perfect, embeddings can encode the semantics of the document and query reasonably well. There\u2019s a class of queries where embedding-based retrieval returns very relevant results. \n\nYet for a variety of reasons, embedding-based retrieval can be imprecise and\nreturn irrelevant context to the query, which in turn degrades the quality of\nthe overall RAG system, regardless of the quality of the LLM.\n\nThis is also not a new problem: one approach to resolve this in existing IR\nand recommendation systems is to create a **two stage process** . The first\nstage uses embedding-based retrieval with a high top-k value to maximize\nrecall while accepting a lower precision. Then the second stage uses a\nslightly more computationally expensive process that is higher precision and\nlower recall (for instance with BM25) to \u201crerank\u201d the existing retrieved\ncandidates.\n\nCovering the downsides of embedding-based retrieval is worth an entire series\nof blog posts. This blog post is an initial exploration of an alternative\nretrieval method and how it can (potentially) augment embedding-based\nretrieval methods.", "mimetype": "text/plain", "start_char_idx": 738, "end_char_idx": 2878, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1834449a-67db-4fb9-8446-c322e366cd3c": {"__data__": {"id_": "1834449a-67db-4fb9-8446-c322e366cd3c", "embedding": null, "metadata": {"Header_1": " **LLM Retrieval and Reranking**", "filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0dec2af8-569d-4446-ad81-67bf451cc3b2", "node_type": "4", "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "af8d98ece7e1a1cac7bf823a56c9995022f6eaac0ea91d442c4c37000c95b26a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "425a1d42-4fc4-47d5-aa9a-e5fb842dafa0", "node_type": "1", "metadata": {"Header_1": " **Introduction and Background**", "filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "ff7803421e11e2353e85cfecfda0296c05a937fe31c70fe9d53787c5cb778e3e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aed54e74-2373-4cc7-ba47-406fabc2109f", "node_type": "1", "metadata": {"Header_1": " **LLM Retrieval and Reranking**", "Header_2": " **LLM Retriever** ` (ListIndexLLMRetriever) `"}, "hash": "bc77a516eba87745c0dca50a18dafafb5f51b5dddc85c082672dae89132a9666", "class_name": "RelatedNodeInfo"}}, "text": "**LLM Retrieval and Reranking**\n\nOver the past week, we\u2019ve developed a variety of initial abstractions around\nthe concept of \u201cLLM-based\u201d retrieval and reranking. At a high-level, this\napproach uses the LLM to decide which document(s) / text chunk(s) are relevant\nto the given query. The input prompt would consist of a set of candidate\ndocuments, and the LLM is tasked with selecting the relevant set of documents\nas well as scoring their relevance with an internal metric.\n\nSimple diagram of how LLM-based retrieval works\n\nAn example prompt would look like the following:\n\n    \n    \n    A list of documents is shown below. Each document has a number next to it along with a summary of the document. A question is also provided.\n      Respond with the numbers of the documents you should consult to answer the question, in order of relevance, as well\n      as the relevance score. The relevance score is a number from 1\u201310 based on how relevant you think the document is to the question.\n      Do not include any documents that are not relevant to the question.\n      Example format:\n      Document 1:\n      <summary of document 1>\n      Document 2:\n      <summary of document 2>\n      \u2026\n      Document 10:\n      <summary of document 10>\n      Question: <question>\n      Answer:\n      Doc: 9, Relevance: 7\n      Doc: 3, Relevance: 4\n      Doc: 7, Relevance: 3\n      Let's try this now:\n      {context_str}\n      Question: {query_str}\n      Answer:\n\nThe prompt format implies that the text for each document should be relatively\nconcise. There are two ways of feeding in the text to the prompt corresponding\nto each document:\n\n  * You can directly feed in the raw text corresponding to the document. This works well if the document corresponds to a bite-sized text chunk. \n  * You can feed in a condensed summary for each document. This would be preferred if the document itself corresponds to a long-piece of text. We do this under the hood with our new [ document summary index ](https://medium.com/llamaindex-blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec) , but you can also choose to do it yourself. \n\nGiven a collection of documents, we can then create document \u201cbatches\u201d and\nsend each batch into the LLM input prompt. The output of each batch would be\nthe set of relevant documents + relevance scores within that batch. The final\nretrieval response would aggregate relevant documents from all batches.\n\nYou can use our abstractions in two forms: as a standalone retriever module (\n` ListIndexLLMRetriever ` ) or a reranker module ( ` LLMRerank ` ). The\nremainder of this blog primarily focuses on the reranker module given the\nspeed/cost.", "mimetype": "text/plain", "start_char_idx": 2883, "end_char_idx": 5554, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aed54e74-2373-4cc7-ba47-406fabc2109f": {"__data__": {"id_": "aed54e74-2373-4cc7-ba47-406fabc2109f", "embedding": null, "metadata": {"Header_1": " **LLM Retrieval and Reranking**", "Header_2": " **LLM Retriever** ` (ListIndexLLMRetriever) `", "filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0dec2af8-569d-4446-ad81-67bf451cc3b2", "node_type": "4", "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "af8d98ece7e1a1cac7bf823a56c9995022f6eaac0ea91d442c4c37000c95b26a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1834449a-67db-4fb9-8446-c322e366cd3c", "node_type": "1", "metadata": {"Header_1": " **LLM Retrieval and Reranking**", "filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "feb6a4ef33727d6b26262dc41ba49bea88b7cef36e11979a33cd8865f32fdbae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0381717d-9e19-4665-b80f-90282a91b6e6", "node_type": "1", "metadata": {"Header_1": " **LLM Retrieval and Reranking**", "Header_2": " **LLM Reranker (LLMRerank)**"}, "hash": "3c24dafbc8f7690e85c6e28e6c46cf262b8b9c1a24742bc02b5430c1c3309ec6", "class_name": "RelatedNodeInfo"}}, "text": "**LLM Retriever** ` (ListIndexLLMRetriever) `\n\nThis module is defined over a list index, which simply stores a set of nodes\nas a flat list. You can build the list index over a set of documents and then\nuse the LLM retriever to retrieve the relevant documents from the index.\n\n    \n    \n    from llama_index import GPTListIndex\n    from llama_index.indices.list.retrievers import ListIndexLLMRetriever\n    index = GPTListIndex.from_documents(documents, service_context=service_context)\n    # high - level API\n    query_str = \"What did the author do during his time in college?\"\n    retriever = index.as_retriever(retriever_mode=\"llm\")\n    nodes = retriever.retrieve(query_str)\n    # lower-level API\n    retriever = ListIndexLLMRetriever()\n    response_synthesizer = ResponseSynthesizer.from_args()\n    query_engine = RetrieverQueryEngine(retriever=retriever, response_synthesizer=response_synthesizer)\n    response = query_engine.query(query_str)\n\n**Use Case:** This could potentially be used in place of our vector store\nindex. You use the LLM instead of embedding-based lookup to select the nodes.", "mimetype": "text/plain", "start_char_idx": 5560, "end_char_idx": 6658, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0381717d-9e19-4665-b80f-90282a91b6e6": {"__data__": {"id_": "0381717d-9e19-4665-b80f-90282a91b6e6", "embedding": null, "metadata": {"Header_1": " **LLM Retrieval and Reranking**", "Header_2": " **LLM Reranker (LLMRerank)**", "filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0dec2af8-569d-4446-ad81-67bf451cc3b2", "node_type": "4", "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "af8d98ece7e1a1cac7bf823a56c9995022f6eaac0ea91d442c4c37000c95b26a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aed54e74-2373-4cc7-ba47-406fabc2109f", "node_type": "1", "metadata": {"Header_1": " **LLM Retrieval and Reranking**", "Header_2": " **LLM Retriever** ` (ListIndexLLMRetriever) `", "filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "aebcb15589e0cdbd9d74dfefedc3190bb46106fe4c6c7a2eb0865c7c7710f01a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4be06149-1b5f-4fd9-8a30-c75df61d003b", "node_type": "1", "metadata": {"Header_1": " **LLM Retrieval and Reranking**", "Header_2": " **Limitations/Caveats**"}, "hash": "29d551a5d182c744989b53e91dd3acfb7835393841ce17e4f9cbc0ea202e0425", "class_name": "RelatedNodeInfo"}}, "text": "**LLM Reranker (LLMRerank)**\n\nThis module is defined as part of our ` NodePostprocessor ` abstraction, which\nis defined for second-stage processing after an initial retrieval pass.\n\nThe postprocessor can be used on its own or as part of a `\nRetrieverQueryEngine ` call. In the below example we show how to use the\npostprocessor as an independent module after an initial retriever call from a\nvector index.\n\n    \n    \n    from llama_index.indices.query.schema import QueryBundle\n    query_bundle = QueryBundle(query_str)\n    # configure retriever\n    retriever = VectorIndexRetriever(\n    index=index,\n    similarity_top_k=vector_top_k,\n    )\n    retrieved_nodes = retriever.retrieve(query_bundle)\n    # configure reranker\n    reranker = LLMRerank(choice_batch_size=5, top_n=reranker_top_n, service_context=service_context)\n    retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)", "mimetype": "text/plain", "start_char_idx": 6664, "end_char_idx": 7566, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4be06149-1b5f-4fd9-8a30-c75df61d003b": {"__data__": {"id_": "4be06149-1b5f-4fd9-8a30-c75df61d003b", "embedding": null, "metadata": {"Header_1": " **LLM Retrieval and Reranking**", "Header_2": " **Limitations/Caveats**", "filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0dec2af8-569d-4446-ad81-67bf451cc3b2", "node_type": "4", "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "af8d98ece7e1a1cac7bf823a56c9995022f6eaac0ea91d442c4c37000c95b26a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0381717d-9e19-4665-b80f-90282a91b6e6", "node_type": "1", "metadata": {"Header_1": " **LLM Retrieval and Reranking**", "Header_2": " **LLM Reranker (LLMRerank)**", "filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "10e2265ef8e720d3a38a8d1e1980bc53c5b72ef99d58b208db17d85cc4cc7620", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1ebf776-a0e2-4079-862e-7ec8a0c54bcc", "node_type": "1", "metadata": {"Header_1": " **Initial Experimental Results**"}, "hash": "2969935691829af0444b8d88dcf61b3664bf28a92f77d7fb5e30a32514ed3166", "class_name": "RelatedNodeInfo"}}, "text": "**Limitations/Caveats**\n\nThere are certain limitations and caveats to LLM-based retrieval, especially\nwith this initial version.\n\n  * LLM-based retrieval is orders of magnitude slower than embedding-based retrieval. Embedding search over thousands or even millions of embeddings can take less than a second. Each LLM prompt of 4000 tokens to OpenAI can take minutes to complete. \n  * Using third-party LLM API\u2019s costs money. \n  * The current method of batching documents may not be optimal, because it relies on an assumption that document batches can be scored independently of each other. This lacks a global view of the ranking for all documents. \n\nUsing the LLM to retrieve and rank every node in the document corpus can be\nprohibitively expensive. This is why using the LLM as a second-stage reranking\nstep, after a first-stage embedding pass, can be helpful.", "mimetype": "text/plain", "start_char_idx": 7572, "end_char_idx": 8436, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1ebf776-a0e2-4079-862e-7ec8a0c54bcc": {"__data__": {"id_": "c1ebf776-a0e2-4079-862e-7ec8a0c54bcc", "embedding": null, "metadata": {"Header_1": " **Initial Experimental Results**", "filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0dec2af8-569d-4446-ad81-67bf451cc3b2", "node_type": "4", "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "af8d98ece7e1a1cac7bf823a56c9995022f6eaac0ea91d442c4c37000c95b26a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4be06149-1b5f-4fd9-8a30-c75df61d003b", "node_type": "1", "metadata": {"Header_1": " **LLM Retrieval and Reranking**", "Header_2": " **Limitations/Caveats**", "filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "3e501a78bfa410fae11ef93f46c119f4adb4a8acdde797aeeaec23c7a2aefd2f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0827a410-7acf-4116-9f14-85b1266805c7", "node_type": "1", "metadata": {"Header_1": " **Initial Experimental Results**", "Header_2": " **The Great Gatsby**"}, "hash": "9e9fde6f8de3b6f5bde81d5ec0c21ced6f48961740cae2053a7f3318d4663495", "class_name": "RelatedNodeInfo"}}, "text": "**Initial Experimental Results**\n\nLet\u2019s take a look at how well LLM reranking works!\n\nWe show some comparisons between naive top-k embedding-based retrieval as well\nas the two-stage retrieval pipeline with a first-stage embedding-retrieval\nfilter and second-stage LLM reranking. We also showcase some results of pure\nLLM-based retrieval (though we don\u2019t showcase as many results given that it\ntends to run a lot slower than either of the first two approaches).\n\nWe analyze results over two very different sources of data: the Great Gatsby\nand the 2021 Lyft SEC 10-k. We only analyze results over the \u201cretrieval\u201d\nportion and not synthesis to better isolate the performance of different\nretrieval methods.\n\nThe results are presented in a qualitative fashion. A next step would\ndefinitely be more comprehensive evaluation over an entire dataset!", "mimetype": "text/plain", "start_char_idx": 8441, "end_char_idx": 9283, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0827a410-7acf-4116-9f14-85b1266805c7": {"__data__": {"id_": "0827a410-7acf-4116-9f14-85b1266805c7", "embedding": null, "metadata": {"Header_1": " **Initial Experimental Results**", "Header_2": " **The Great Gatsby**", "filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0dec2af8-569d-4446-ad81-67bf451cc3b2", "node_type": "4", "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "af8d98ece7e1a1cac7bf823a56c9995022f6eaac0ea91d442c4c37000c95b26a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1ebf776-a0e2-4079-862e-7ec8a0c54bcc", "node_type": "1", "metadata": {"Header_1": " **Initial Experimental Results**", "filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "646bbaaecb4fee215a581699ad33c7ce24bd8824da4cdd3f270f01fe61da5c58", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d6a60e55-6917-4d87-a237-965f2904d66c", "node_type": "1", "metadata": {"Header_1": " **Initial Experimental Results**", "Header_2": " **2021 Lyft SEC 10-K**"}, "hash": "9d6079a5a3048c4e4d9311e3827a48b43ce914ad488da298eea2dbd1bd4463af", "class_name": "RelatedNodeInfo"}}, "text": "**The Great Gatsby**\n\nIn our first example, we load in the Great Gatsby as a ` Document ` object,\nand build a vector index over it (with chunk size set to 512).\n\n    \n    \n    # LLM Predictor (gpt-3.5-turbo) + service context\n    llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size_limit=512)\n    # load documents\n    documents = SimpleDirectoryReader('../../../examples/gatsby/data').load_data()\n    index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n\nWe then define a ` get_retrieved_nodes ` function \u2014 this function can either\ndo just embedding-based retrieval over the index, or embedding-based retrieval\n+ reranking.\n\n    \n    \n    def get_retrieved_nodes(\n        query_str, vector_top_k=10, reranker_top_n=3, with_reranker=False\n    ):\n      query_bundle = QueryBundle(query_str)\n      # configure retriever\n      retriever = VectorIndexRetriever(\n        index=index,\n        similarity_top_k=vector_top_k,\n      )\n      retrieved_nodes = retriever.retrieve(query_bundle)\n      if with_reranker:\n        # configure reranker\n        reranker = LLMRerank(choice_batch_size=5, top_n=reranker_top_n, service_context=service_context)\n        retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n      return retrieved_nodes\n\nWe then ask some questions. With embedding-based retrieval we set k=3. With\ntwo-stage retrieval we set k=10 for embedding retrieval and n=3 for LLM-based\nreranking.\n\n**Question: \u201dWho was driving the car that hit Myrtle?\u201d**\n\n(For those of you who are not familiar with the Great Gatsby, the narrator\nfinds out later on from Gatsby that Daisy was actually the one driving the\ncar, but Gatsby takes the blame for her).\n\nThe top retrieved contexts are shown in the images below. We see that in\nembedding-based retrieval, the top two texts contain semantics of the car\ncrash but give no details as to who was actually responsible. Only the third\ntext contains the proper answer.\n\nRetrieved context using top-k embedding lookup (baseline)\n\nIn contrast, the two-stage approach returns just one relevant context, and it\ncontains the correct answer.\n\nRetrieved context using two-stage pipeline (embedding lookup then rerank)", "mimetype": "text/plain", "start_char_idx": 9289, "end_char_idx": 11619, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6a60e55-6917-4d87-a237-965f2904d66c": {"__data__": {"id_": "d6a60e55-6917-4d87-a237-965f2904d66c", "embedding": null, "metadata": {"Header_1": " **Initial Experimental Results**", "Header_2": " **2021 Lyft SEC 10-K**", "filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0dec2af8-569d-4446-ad81-67bf451cc3b2", "node_type": "4", "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "af8d98ece7e1a1cac7bf823a56c9995022f6eaac0ea91d442c4c37000c95b26a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0827a410-7acf-4116-9f14-85b1266805c7", "node_type": "1", "metadata": {"Header_1": " **Initial Experimental Results**", "Header_2": " **The Great Gatsby**", "filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "4072b2fe207d9c30b3b2795001e4565645f1949b03b17110ba30fa8dfe8bc52a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "965a76dd-8569-47e8-bab8-faa9f7250b1a", "node_type": "1", "metadata": {"Header_1": " **Conclusion**"}, "hash": "698b6290b5ee2cce4c6a54a4e5a0194121266ca582d5c7e8a513030868b279f0", "class_name": "RelatedNodeInfo"}}, "text": "**2021 Lyft SEC 10-K**\n\nWe want to ask some questions over the 2021 Lyft SEC 10-K, specifically about\nthe COVID-19 impacts and responses. The Lyft SEC 10-K is 238 pages long, and a\nctrl-f for \u201cCOVID-19\u201d returns 127 matches.\n\nWe use a similar setup as the Gatsby example above. The main differences are\nthat we set the chunk size to 128 instead of 512, we set k=5 for the embedding\nretrieval baseline, and an embedding k=40 and reranker n=5 for the two-stage\napproach.\n\nWe then ask the following questions and analyze the results.\n\n**Question: \u201cWhat initiatives are the company focusing on independently of\nCOVID-19?\u201d**\n\nResults for the baseline are shown in the image above. We see that results\ncorresponding to indices 0, 1, 3, 4, are about measures directly in response\nto Covid-19, even though the question was specifically about company\ninitiatives that were independent of the COVID-19 pandemic.\n\nRetrieved context using top-k embedding lookup (baseline)\n\nWe get more relevant results in approach 2, by widening the top-k to 40 and\nthen using an LLM to filter for the top-5 contexts. The independent company\ninitiatives include \u201cexpansion of Light Vehicles\u201d (1), \u201cincremental\ninvestments in brand/marketing\u201d (2), international expansion (3), and\naccounting for misc. risks such as natural disasters and operational risks in\nterms of financial performance (4).\n\nRetrieved context using two-stage pipeline (embedding lookup then rerank)", "mimetype": "text/plain", "start_char_idx": 11625, "end_char_idx": 13064, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "965a76dd-8569-47e8-bab8-faa9f7250b1a": {"__data__": {"id_": "965a76dd-8569-47e8-bab8-faa9f7250b1a", "embedding": null, "metadata": {"Header_1": " **Conclusion**", "filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0dec2af8-569d-4446-ad81-67bf451cc3b2", "node_type": "4", "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "af8d98ece7e1a1cac7bf823a56c9995022f6eaac0ea91d442c4c37000c95b26a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6a60e55-6917-4d87-a237-965f2904d66c", "node_type": "1", "metadata": {"Header_1": " **Initial Experimental Results**", "Header_2": " **2021 Lyft SEC 10-K**", "filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "7cceea3c13770269213d4df1b783567d4549e5cb61c4aa3729348ec4d96a2293", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11de1c4e-a66a-4f60-94b7-33c97dbaf08a", "node_type": "1", "metadata": {"Header_1": " **Conclusion**", "Header_2": " Resources"}, "hash": "240d8a5d4c4f898ebf37744903047fb17dc6db7b34b72fe9a8b42dba768e39e7", "class_name": "RelatedNodeInfo"}}, "text": "**Conclusion**\n\nThat\u2019s it for now! We\u2019ve added some initial functionality to help support LLM-\naugmented retrieval pipelines, but of course there\u2019s a ton of future steps\nthat we couldn\u2019t quite get to. Some questions we\u2019d love to explore:\n\n  * How our LLM reranking implementation compares to other reranking methods (e.g. BM25, Cohere Rerank, etc.) \n  * What the optimal values of embedding top-k and reranking top-n are for the two stage pipeline, accounting for latency, cost, and performance. \n  * Exploring different prompts and text summarization methods to help determine document relevance \n  * Exploring if there\u2019s a class of applications where LLM-based retrieval on its own would suffice, without embedding-based filtering (maybe over smaller document collections?)", "mimetype": "text/plain", "start_char_idx": 13069, "end_char_idx": 13844, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11de1c4e-a66a-4f60-94b7-33c97dbaf08a": {"__data__": {"id_": "11de1c4e-a66a-4f60-94b7-33c97dbaf08a", "embedding": null, "metadata": {"Header_1": " **Conclusion**", "Header_2": " Resources", "filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0dec2af8-569d-4446-ad81-67bf451cc3b2", "node_type": "4", "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "af8d98ece7e1a1cac7bf823a56c9995022f6eaac0ea91d442c4c37000c95b26a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "965a76dd-8569-47e8-bab8-faa9f7250b1a", "node_type": "1", "metadata": {"Header_1": " **Conclusion**", "filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "de852e22435208ea6d7c3361fda19d2e50ba42811c970c7755412d0f345a744c", "class_name": "RelatedNodeInfo"}}, "text": "Resources\n\nYou can play around with the notebooks yourself!\n\n[ Great Gatsby Notebook\n](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/node_postprocessor/LLMReranker-\nGatsby.ipynb)\n\n[ 2021 Lyft 10-K Notebook\n](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/node_postprocessor/LLMReranker-\nLyft-10k.ipynb)", "mimetype": "text/plain", "start_char_idx": 13851, "end_char_idx": 14190, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"0e6108aa-5c46-4704-96f0-d211af93c3d3": {"doc_hash": "a38148ff8c3f65688251ea64862b6a8721b17cd960bc8138ee3e3fb1b448a85b", "ref_doc_id": "0dec2af8-569d-4446-ad81-67bf451cc3b2"}, "425a1d42-4fc4-47d5-aa9a-e5fb842dafa0": {"doc_hash": "ff7803421e11e2353e85cfecfda0296c05a937fe31c70fe9d53787c5cb778e3e", "ref_doc_id": "0dec2af8-569d-4446-ad81-67bf451cc3b2"}, "1834449a-67db-4fb9-8446-c322e366cd3c": {"doc_hash": "feb6a4ef33727d6b26262dc41ba49bea88b7cef36e11979a33cd8865f32fdbae", "ref_doc_id": "0dec2af8-569d-4446-ad81-67bf451cc3b2"}, "aed54e74-2373-4cc7-ba47-406fabc2109f": {"doc_hash": "aebcb15589e0cdbd9d74dfefedc3190bb46106fe4c6c7a2eb0865c7c7710f01a", "ref_doc_id": "0dec2af8-569d-4446-ad81-67bf451cc3b2"}, "0381717d-9e19-4665-b80f-90282a91b6e6": {"doc_hash": "10e2265ef8e720d3a38a8d1e1980bc53c5b72ef99d58b208db17d85cc4cc7620", "ref_doc_id": "0dec2af8-569d-4446-ad81-67bf451cc3b2"}, "4be06149-1b5f-4fd9-8a30-c75df61d003b": {"doc_hash": "3e501a78bfa410fae11ef93f46c119f4adb4a8acdde797aeeaec23c7a2aefd2f", "ref_doc_id": "0dec2af8-569d-4446-ad81-67bf451cc3b2"}, "c1ebf776-a0e2-4079-862e-7ec8a0c54bcc": {"doc_hash": "646bbaaecb4fee215a581699ad33c7ce24bd8824da4cdd3f270f01fe61da5c58", "ref_doc_id": "0dec2af8-569d-4446-ad81-67bf451cc3b2"}, "0827a410-7acf-4116-9f14-85b1266805c7": {"doc_hash": "4072b2fe207d9c30b3b2795001e4565645f1949b03b17110ba30fa8dfe8bc52a", "ref_doc_id": "0dec2af8-569d-4446-ad81-67bf451cc3b2"}, "d6a60e55-6917-4d87-a237-965f2904d66c": {"doc_hash": "7cceea3c13770269213d4df1b783567d4549e5cb61c4aa3729348ec4d96a2293", "ref_doc_id": "0dec2af8-569d-4446-ad81-67bf451cc3b2"}, "965a76dd-8569-47e8-bab8-faa9f7250b1a": {"doc_hash": "de852e22435208ea6d7c3361fda19d2e50ba42811c970c7755412d0f345a744c", "ref_doc_id": "0dec2af8-569d-4446-ad81-67bf451cc3b2"}, "11de1c4e-a66a-4f60-94b7-33c97dbaf08a": {"doc_hash": "5b490090b052b1abe2bace1ca1c7fd6da214cdf20f6b052d890e800353d45d21", "ref_doc_id": "0dec2af8-569d-4446-ad81-67bf451cc3b2"}}, "docstore/ref_doc_info": {"0dec2af8-569d-4446-ad81-67bf451cc3b2": {"node_ids": ["0e6108aa-5c46-4704-96f0-d211af93c3d3", "425a1d42-4fc4-47d5-aa9a-e5fb842dafa0", "1834449a-67db-4fb9-8446-c322e366cd3c", "aed54e74-2373-4cc7-ba47-406fabc2109f", "0381717d-9e19-4665-b80f-90282a91b6e6", "4be06149-1b5f-4fd9-8a30-c75df61d003b", "c1ebf776-a0e2-4079-862e-7ec8a0c54bcc", "0827a410-7acf-4116-9f14-85b1266805c7", "d6a60e55-6917-4d87-a237-965f2904d66c", "965a76dd-8569-47e8-bab8-faa9f7250b1a", "11de1c4e-a66a-4f60-94b7-33c97dbaf08a"], "metadata": {"Header_1": " **Summary**", "filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}}}}