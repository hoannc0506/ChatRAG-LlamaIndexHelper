{"docstore/data": {"e9a86f1d-59f0-4564-b74c-c31a1becc9a2": {"__data__": {"id_": "e9a86f1d-59f0-4564-b74c-c31a1becc9a2", "embedding": null, "metadata": {"filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f949af62-9f8c-47d6-8528-d98ace1cf2bf", "node_type": "4", "metadata": {"filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "hash": "5d82c2b900fbabdf56c600b6fabe576eca7b25c810b27484985a87d5442478d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6ad0005-1bbe-4379-8474-e40db3b9c67d", "node_type": "1", "metadata": {"Header_1": " About Us"}, "hash": "b9ff75b810bed0b2728ebd4acc705407344b7f14a58930341f1601ce5402b673", "class_name": "RelatedNodeInfo"}}, "text": "**Co-Authors:**\n\n  * Akash Sharma, founder and CEO, Vellum \n  * Jerry Liu, co-founder and CEO, LlamaIndex", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6ad0005-1bbe-4379-8474-e40db3b9c67d": {"__data__": {"id_": "b6ad0005-1bbe-4379-8474-e40db3b9c67d", "embedding": null, "metadata": {"Header_1": " About Us", "filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f949af62-9f8c-47d6-8528-d98ace1cf2bf", "node_type": "4", "metadata": {"filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "hash": "5d82c2b900fbabdf56c600b6fabe576eca7b25c810b27484985a87d5442478d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9a86f1d-59f0-4564-b74c-c31a1becc9a2", "node_type": "1", "metadata": {"filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "hash": "c08e0a9ac4a3d82d9f2058fdbaaf9d37baaebafd3ba729803d3b883dadc51efe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdfe6c16-fdd9-4cef-a6ca-3ba4fc0f6019", "node_type": "1", "metadata": {"Header_1": " Why we partnered on this integration"}, "hash": "1de649d2db75742c90b6db78da85c3a51d76ef44152c5e71a3c64f5384f99d8b", "class_name": "RelatedNodeInfo"}}, "text": "About Us\n\nThe central mission of [ LlamaIndex\n](https://github.com/jerryjliu/llama_index) is to provide an interface between\nLarge Language Models (LLM\u2019s), and your private, external data. Over the past\nfew months, it has become one of the most popular open-source frameworks for\nLLM data augmentation (context-augmented generation), for a variety of use\ncases: question-answering, summarization, structured queries, and more.\n\n[ Vellum ](http://vellum.ai/) is a developer platform to build high quality\nLLM applications. The platform provides best-in-class tooling for prompt\nengineering, unit testing, regression testing, monitoring & versioning of in-\nproduction traffic and model fine tuning. Vellum\u2019s platform helps companies\nsave countless engineering hours to build internal tooling and instead use\nthat time to build end user facing applications.", "mimetype": "text/plain", "start_char_idx": 111, "end_char_idx": 965, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bdfe6c16-fdd9-4cef-a6ca-3ba4fc0f6019": {"__data__": {"id_": "bdfe6c16-fdd9-4cef-a6ca-3ba4fc0f6019", "embedding": null, "metadata": {"Header_1": " Why we partnered on this integration", "filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f949af62-9f8c-47d6-8528-d98ace1cf2bf", "node_type": "4", "metadata": {"filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "hash": "5d82c2b900fbabdf56c600b6fabe576eca7b25c810b27484985a87d5442478d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6ad0005-1bbe-4379-8474-e40db3b9c67d", "node_type": "1", "metadata": {"Header_1": " About Us", "filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "hash": "4414bc2507e1eeaf86f88f2025dcdd3efb4511f8d520df7fb43f4c7acb414934", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e076eb1-11e0-4d1b-a319-c260dc391fcd", "node_type": "1", "metadata": {"Header_1": " Why we partnered on this integration", "Header_2": " Unit testing your prompts"}, "hash": "a959df4747fd3f173c8e3958eb102bfdc5b4ebb7348701dd7d5ff28cfd240984", "class_name": "RelatedNodeInfo"}}, "text": "Why we partnered on this integration\n\nUntil recently, LlamaIndex users did not have a way to do prompt engineering\nand unit testing pre-production and versioning/monitoring the prompts post\nproduction. Prompt engineering and unit testing is key to ensure that your LLM\nfeature is producing reliable results in production. Here\u2019s an example of\nsimple prompt that produces vastly different results between GPT-3, GPT-3.5\nand GPT-4:", "mimetype": "text/plain", "start_char_idx": 970, "end_char_idx": 1399, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e076eb1-11e0-4d1b-a319-c260dc391fcd": {"__data__": {"id_": "4e076eb1-11e0-4d1b-a319-c260dc391fcd", "embedding": null, "metadata": {"Header_1": " Why we partnered on this integration", "Header_2": " Unit testing your prompts", "filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f949af62-9f8c-47d6-8528-d98ace1cf2bf", "node_type": "4", "metadata": {"filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "hash": "5d82c2b900fbabdf56c600b6fabe576eca7b25c810b27484985a87d5442478d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdfe6c16-fdd9-4cef-a6ca-3ba4fc0f6019", "node_type": "1", "metadata": {"Header_1": " Why we partnered on this integration", "filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "hash": "9689a52ef2309446fbbf70ec542389b94c1635a240ebecd9413ad1a8f3a5bd74", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b81a672-0781-46a5-9f5d-f9c496a58578", "node_type": "1", "metadata": {"Header_1": " Why we partnered on this integration", "Header_2": " Regression testing in production"}, "hash": "c550325682ec9019ccf22b3f49f317a6cdfd36ea2706d470d31122a6d09596ed", "class_name": "RelatedNodeInfo"}}, "text": "Unit testing your prompts\n\nCreating a unit test bank is a proactive approach to ensure prompt reliability\n\u2014 it\u2019s best practice to run 50\u2013100 test cases before putting prompts in\nproduction. The test bank should comprise scenarios & edge cases anticipated\nin production, think of this as QAing your feature before it goes to\nproduction. The prompts should \u201cpass\u201d these test cases based on your\nevaluation criteria. Use Vellum Test Suites to upload test cases in bulk via\nCSV upload.", "mimetype": "text/plain", "start_char_idx": 1405, "end_char_idx": 1886, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b81a672-0781-46a5-9f5d-f9c496a58578": {"__data__": {"id_": "2b81a672-0781-46a5-9f5d-f9c496a58578", "embedding": null, "metadata": {"Header_1": " Why we partnered on this integration", "Header_2": " Regression testing in production", "filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f949af62-9f8c-47d6-8528-d98ace1cf2bf", "node_type": "4", "metadata": {"filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "hash": "5d82c2b900fbabdf56c600b6fabe576eca7b25c810b27484985a87d5442478d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e076eb1-11e0-4d1b-a319-c260dc391fcd", "node_type": "1", "metadata": {"Header_1": " Why we partnered on this integration", "Header_2": " Unit testing your prompts", "filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "hash": "4b61de088de0cc329479d217dbcf75adf0b0598d706405a542839ef45f013d22", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "852c4d5f-7cd0-43de-865e-2766b83a27ef", "node_type": "1", "metadata": {"Header_1": " Best practices to leverage the integration"}, "hash": "75636d4e39ae190d584f4b53f051d9a75e6f9be08e7ded42d78702e89f8406d0", "class_name": "RelatedNodeInfo"}}, "text": "Regression testing in production\n\nDespite how well you test before sending a prompt in production, edge cases\ncan appear when in production. This is expected, so no stress! Through the\nVellum integration, LlamaIndex users can change prompts and get prompt\nversioning without making any code changes. While doing that, however, it\u2019s\nbest practice to run historical inputs that were sent to the prompt in\nproduction to the new prompt and confirm it doesn\u2019t break any existing\nbehavior. LLMs are sometimes unpredictable, even changing the word \u201cgood\u201d to\n\u201cgreat\u201d in a prompt can result in differing outputs!", "mimetype": "text/plain", "start_char_idx": 1892, "end_char_idx": 2495, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "852c4d5f-7cd0-43de-865e-2766b83a27ef": {"__data__": {"id_": "852c4d5f-7cd0-43de-865e-2766b83a27ef", "embedding": null, "metadata": {"Header_1": " Best practices to leverage the integration", "filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f949af62-9f8c-47d6-8528-d98ace1cf2bf", "node_type": "4", "metadata": {"filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "hash": "5d82c2b900fbabdf56c600b6fabe576eca7b25c810b27484985a87d5442478d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b81a672-0781-46a5-9f5d-f9c496a58578", "node_type": "1", "metadata": {"Header_1": " Why we partnered on this integration", "Header_2": " Regression testing in production", "filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "hash": "cbf5a0d983ac9d17b50bb878deb46cc196adf213016dfd29f6be0ce902ef00ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02ae5948-1d4b-4494-ab82-71dd9c51c6e4", "node_type": "1", "metadata": {"Header_1": " Best practices to leverage the integration", "Header_2": " How to access the integration"}, "hash": "2860dd8c49a881188a3d39dece5e17dabb8e66a49c6e4d442f39c439a4dd5f76", "class_name": "RelatedNodeInfo"}}, "text": "Best practices to leverage the integration", "mimetype": "text/plain", "start_char_idx": 2500, "end_char_idx": 2542, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02ae5948-1d4b-4494-ab82-71dd9c51c6e4": {"__data__": {"id_": "02ae5948-1d4b-4494-ab82-71dd9c51c6e4", "embedding": null, "metadata": {"Header_1": " Best practices to leverage the integration", "Header_2": " How to access the integration", "filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f949af62-9f8c-47d6-8528-d98ace1cf2bf", "node_type": "4", "metadata": {"filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "hash": "5d82c2b900fbabdf56c600b6fabe576eca7b25c810b27484985a87d5442478d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "852c4d5f-7cd0-43de-865e-2766b83a27ef", "node_type": "1", "metadata": {"Header_1": " Best practices to leverage the integration", "filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "hash": "cc86bf20636a0e2d986cd56571530d3d022929237a82610ead55100ce5426757", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c193e40-fbd9-4d7b-aa92-373af62f2036", "node_type": "1", "metadata": {"Header_1": " Best practices to leverage the integration", "Header_2": " Prompt engineering tips in context augmented use cases"}, "hash": "f7de0593347d5a29c4626b2cf7d711e63cd733e4783f784556d733d43493b1ec", "class_name": "RelatedNodeInfo"}}, "text": "How to access the integration\n\n[ This\n](https://github.com/jerryjliu/llama_index/blob/main/examples/vellum/Vellum%20Integration%20Demo.ipynb)\ndemo notebook goes into detail on how you can use Vellum to manage prompts\nwithin LlamaIndex.\n\n**Prerequisites**\n\n  1. Sign up for a free Vellum account at [ app.vellum.ai/signup ](https://app.vellum.ai/signup)\n  2. Go to [ app.vellum.ai/api-keys ](https://app.vellum.ai/api-keys) and generate a Vellum API key. Note it down somewhere. \n\n**Auto-Register Prompts & Make Predictions Through Vellum **\n\nIf you import a prompt in LlamaIndex, the VellumPredictor class will used to\nauto-register a prompt with Vellum to make predictions.\n\nBy registering a prompt with Vellum, Vellum will create:\n\n  1. A \u201cSandbox\u201d \u2014 an environment where you can iterate on the prompt, it\u2019s model, provider, params, etc.; and \n  2. A \u201cDeployment\u201d \u2014 a thin API proxy between you and LLM providers and offering prompt versioning, request monitoring, and more \n\nYou can use VellumPromptRegistry to retrieve information about the registered\nprompt and get links to open its corresponding Sandbox and Deployment in\nVellum\u2019s UI. More details about Vellum\u2019s Sandbox and Deployment features can\nbe found [ here ](https://www.notion.so/Vellum-LlamaIndex-\nintegration-096fb3f141ac49c695b7fcb6c70c0519?pvs=21)", "mimetype": "text/plain", "start_char_idx": 2548, "end_char_idx": 3865, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c193e40-fbd9-4d7b-aa92-373af62f2036": {"__data__": {"id_": "0c193e40-fbd9-4d7b-aa92-373af62f2036", "embedding": null, "metadata": {"Header_1": " Best practices to leverage the integration", "Header_2": " Prompt engineering tips in context augmented use cases", "filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f949af62-9f8c-47d6-8528-d98ace1cf2bf", "node_type": "4", "metadata": {"filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "hash": "5d82c2b900fbabdf56c600b6fabe576eca7b25c810b27484985a87d5442478d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02ae5948-1d4b-4494-ab82-71dd9c51c6e4", "node_type": "1", "metadata": {"Header_1": " Best practices to leverage the integration", "Header_2": " How to access the integration", "filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "hash": "39acc26de8182faaa9fe60a9341e32a4941ef3535ff7c9f30d39f147bcd9b111", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4add875b-db7d-4d37-b45e-37f86adc8e95", "node_type": "1", "metadata": {"Header_1": " Best practices to leverage the integration", "Header_2": " Measuring prompt quality, before production"}, "hash": "ae4fa4dfc0682b50d379b3e25d301d768fdd7b8edbbaf09aae3452eb6e16b44e", "class_name": "RelatedNodeInfo"}}, "text": "Prompt engineering tips in context augmented use cases\n\nThink of the Large Language Model as a smart college graduate that needs\ninstructions if the task at hand is not clear. If you\u2019re not getting good\nresults with the default prompt templates, follow these instructions:\n\n  1. Add use case specific details to the prompt to guide what the model focuses on. \n  2. Create 5\u201310 input scenarios to test performance \n  3. Iterate a few times: (i) Tweak the prompt by adding more specific instructions or examples for the scenarios with bad results, (ii) Evaluate against the target response for each scenario \n  4. In parallel, test out different foundation models and model providers using Vellum\u2019s Sandbox. Maybe Claude or PaLM does better than GPT-4 for your use case. \n  5. If you would like additional reasoning or explanation, use a more prescriptive approach: \n\n  * Add detailed step by step instructions to the end of the prompt and ask the LLM to walk though those steps when creating it\u2019s answer: \n  * e.g. (1) \u2026 (2) \u2026 (3) \u2026 \u2026 (6) Output a JSON with the following typescript schema \n  * This is convenient because it\u2019s simple to parse out the JSON blob from the LLM output \n  * However this causes more tokens to be generated so is slower and costs more, but it\u2019s not nearly as expensive and slow as chaining multiple calls", "mimetype": "text/plain", "start_char_idx": 3871, "end_char_idx": 5201, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4add875b-db7d-4d37-b45e-37f86adc8e95": {"__data__": {"id_": "4add875b-db7d-4d37-b45e-37f86adc8e95", "embedding": null, "metadata": {"Header_1": " Best practices to leverage the integration", "Header_2": " Measuring prompt quality, before production", "filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f949af62-9f8c-47d6-8528-d98ace1cf2bf", "node_type": "4", "metadata": {"filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "hash": "5d82c2b900fbabdf56c600b6fabe576eca7b25c810b27484985a87d5442478d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c193e40-fbd9-4d7b-aa92-373af62f2036", "node_type": "1", "metadata": {"Header_1": " Best practices to leverage the integration", "Header_2": " Prompt engineering tips in context augmented use cases", "filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "hash": "2bcdb26c83def9d42a8b5534b769228d4da41c7dbe54f99213ae3438f3212bca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44b21908-aec1-4be4-aeb7-4ba83782a02d", "node_type": "1", "metadata": {"Header_1": " Best practices to leverage the integration", "Header_2": " Measuring prompt quality, once in production"}, "hash": "b8d046215ca4958486a2e6a44deaf00cf6c291fe367fccd3b925c05be9545acf", "class_name": "RelatedNodeInfo"}}, "text": "Measuring prompt quality, before production\n\nOne of the common reasons why evaluating LLM model quality is hard is that\nthere\u2019s no defined framework. The evaluation metric depends on your use case.\nThis [ blog ](https://www.vellum.ai/blog/how-to-evaluate-the-quality-of-large-\nlanguage-models-for-production-use-cases) goes in more detail, but in summary,\nthe evaluation approach depends on type of use case:\n\n  * **Classification:** accuracy, recall, precision, F score and confusion matrices for a deeper evaluation \n  * **Data extraction:** Validate that the output is syntactically valid and the expected keys are present in the generated response \n  * **SQL/Code generation:** Validate that the output is syntactically valid and running it will return the expected values \n  * **Creative output:** Semantic similarity between model generated response and target response using cross-encoders \n\nVellum\u2019s Sandbox and Test Suites offer **Exact Match, Regex Match, Semantic\nSimilarity & Webhook ** as evaluation criteria. You get a clear indication of\nwhich test cases \u201cpass\u201d, given your evaluation criteria\n\n**Testing in Vellum Sandbox**\n\n**Testing in Vellum Test Suites**", "mimetype": "text/plain", "start_char_idx": 5208, "end_char_idx": 6382, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44b21908-aec1-4be4-aeb7-4ba83782a02d": {"__data__": {"id_": "44b21908-aec1-4be4-aeb7-4ba83782a02d", "embedding": null, "metadata": {"Header_1": " Best practices to leverage the integration", "Header_2": " Measuring prompt quality, once in production", "filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f949af62-9f8c-47d6-8528-d98ace1cf2bf", "node_type": "4", "metadata": {"filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "hash": "5d82c2b900fbabdf56c600b6fabe576eca7b25c810b27484985a87d5442478d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4add875b-db7d-4d37-b45e-37f86adc8e95", "node_type": "1", "metadata": {"Header_1": " Best practices to leverage the integration", "Header_2": " Measuring prompt quality, before production", "filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "hash": "65f78a8224db3cc55f6cfb794ccbcc7a23129d6e84101d684472ad98ec150c98", "class_name": "RelatedNodeInfo"}}, "text": "Measuring prompt quality, once in production\n\nUser feedback is the ultimate source of truth for model quality \u2014 if there\u2019s a\nway for your users to either implicitly or explicitly tell you whether they\nthe response is \u201cgood\u201d or \u201cbad,\u201d that\u2019s what you should track and improve!\n\nExplicit user feedback is collected when your users respond with something\nlike a or in your UI when interacting with the LLM output. Asking explicitly\nmay not result in enough volume of feedback to measure overall quality. If\nyour feedback collection rates are low, we suggest using implicit feedback if\npossible.\n\nImplicit feedback is based on how users react to the output generated by the\nLLM. For example, if you generate a first draft of en email for a user and\nthey send it without making edits, that\u2019s likely a good response! If they hit\nregenerate, or re-write the whole thing, that\u2019s probably not a good response.\nImplicit feedback collection may not be possible for all use cases, but it can\nbe a powerful gauge of quality.\n\nUse Vellum\u2019s Actuals endpoint to track the quality of each completion and\ntrack results in the Completions and Monitoring tabs of your Deployment.", "mimetype": "text/plain", "start_char_idx": 6388, "end_char_idx": 7547, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"e9a86f1d-59f0-4564-b74c-c31a1becc9a2": {"doc_hash": "c08e0a9ac4a3d82d9f2058fdbaaf9d37baaebafd3ba729803d3b883dadc51efe", "ref_doc_id": "f949af62-9f8c-47d6-8528-d98ace1cf2bf"}, "b6ad0005-1bbe-4379-8474-e40db3b9c67d": {"doc_hash": "4414bc2507e1eeaf86f88f2025dcdd3efb4511f8d520df7fb43f4c7acb414934", "ref_doc_id": "f949af62-9f8c-47d6-8528-d98ace1cf2bf"}, "bdfe6c16-fdd9-4cef-a6ca-3ba4fc0f6019": {"doc_hash": "9689a52ef2309446fbbf70ec542389b94c1635a240ebecd9413ad1a8f3a5bd74", "ref_doc_id": "f949af62-9f8c-47d6-8528-d98ace1cf2bf"}, "4e076eb1-11e0-4d1b-a319-c260dc391fcd": {"doc_hash": "4b61de088de0cc329479d217dbcf75adf0b0598d706405a542839ef45f013d22", "ref_doc_id": "f949af62-9f8c-47d6-8528-d98ace1cf2bf"}, "2b81a672-0781-46a5-9f5d-f9c496a58578": {"doc_hash": "cbf5a0d983ac9d17b50bb878deb46cc196adf213016dfd29f6be0ce902ef00ea", "ref_doc_id": "f949af62-9f8c-47d6-8528-d98ace1cf2bf"}, "852c4d5f-7cd0-43de-865e-2766b83a27ef": {"doc_hash": "cc86bf20636a0e2d986cd56571530d3d022929237a82610ead55100ce5426757", "ref_doc_id": "f949af62-9f8c-47d6-8528-d98ace1cf2bf"}, "02ae5948-1d4b-4494-ab82-71dd9c51c6e4": {"doc_hash": "39acc26de8182faaa9fe60a9341e32a4941ef3535ff7c9f30d39f147bcd9b111", "ref_doc_id": "f949af62-9f8c-47d6-8528-d98ace1cf2bf"}, "0c193e40-fbd9-4d7b-aa92-373af62f2036": {"doc_hash": "2bcdb26c83def9d42a8b5534b769228d4da41c7dbe54f99213ae3438f3212bca", "ref_doc_id": "f949af62-9f8c-47d6-8528-d98ace1cf2bf"}, "4add875b-db7d-4d37-b45e-37f86adc8e95": {"doc_hash": "65f78a8224db3cc55f6cfb794ccbcc7a23129d6e84101d684472ad98ec150c98", "ref_doc_id": "f949af62-9f8c-47d6-8528-d98ace1cf2bf"}, "44b21908-aec1-4be4-aeb7-4ba83782a02d": {"doc_hash": "57cc05634029d321d5c408e48639914e64147ec01fd2c0d6b2e8f63fd83f1fa1", "ref_doc_id": "f949af62-9f8c-47d6-8528-d98ace1cf2bf"}}, "docstore/ref_doc_info": {"f949af62-9f8c-47d6-8528-d98ace1cf2bf": {"node_ids": ["e9a86f1d-59f0-4564-b74c-c31a1becc9a2", "b6ad0005-1bbe-4379-8474-e40db3b9c67d", "bdfe6c16-fdd9-4cef-a6ca-3ba4fc0f6019", "4e076eb1-11e0-4d1b-a319-c260dc391fcd", "2b81a672-0781-46a5-9f5d-f9c496a58578", "852c4d5f-7cd0-43de-865e-2766b83a27ef", "02ae5948-1d4b-4494-ab82-71dd9c51c6e4", "0c193e40-fbd9-4d7b-aa92-373af62f2036", "4add875b-db7d-4d37-b45e-37f86adc8e95", "44b21908-aec1-4be4-aeb7-4ba83782a02d"], "metadata": {"filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}}}}