{"docstore/metadata": {"/workspace/projects/LlamindexHelper/data/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.html": {"doc_hash": "e622a0a8d1532d5ca3d35838e53ef4c193b6d4170eba7de5c72e033171779acc"}, "/workspace/projects/LlamindexHelper/data/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.html": {"doc_hash": "10894293b5a982499949a7a0e6d8050e1fa0075765f31179217da413a4ba6825"}, "/workspace/projects/LlamindexHelper/data/agentic-rag-with-llamaindex-2721b8a49ff6.html": {"doc_hash": "8fb9802b11fdf1a7cc9b88f317602b2d90a2f243e9ddf1f50ef979e235feede9"}, "/workspace/projects/LlamindexHelper/data/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.html": {"doc_hash": "6117b1853de7eeb213f072f64310f9bba29c1a0aae4b95f4c59ccd1e327cbd0d"}, "/workspace/projects/LlamindexHelper/data/announcing-llamaindex-0-9-719f03282945.html": {"doc_hash": "aca8508a39c87881acdb169298f52331e0b3eedfd67890790ebe9784f483caa4"}, "/workspace/projects/LlamindexHelper/data/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications.html": {"doc_hash": "1d53368128eb93b54f8c8b110bcca0df2fb08851faf458adef76e22718bb3012"}, "/workspace/projects/LlamindexHelper/data/automate-online-tasks-with-multion-and-llamaindex.html": {"doc_hash": "894afdbb92b37830e2ac1215218827f6edaea406b370ad1f3b9d036dabbad82f"}, "/workspace/projects/LlamindexHelper/data/batch-inference-with-mymagic-ai-and-llamaindex.html": {"doc_hash": "578651e8f54de7132014b1c90fc522a3201e4e626534ed4cbf1b34732f19d514"}, "/workspace/projects/LlamindexHelper/data/becoming-proficient-in-document-extraction-32aa13046ed5.html": {"doc_hash": "40ea2774b8d6c54eb4ba9c0aa74af21bea7224c49ba04cfeebcd4d009a85fc77"}, "/workspace/projects/LlamindexHelper/data/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.html": {"doc_hash": "bb868d306d901b7bc178f0192989d9c2ecb13c290871d38a7a72e6199f141d31"}, "/workspace/projects/LlamindexHelper/data/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.html": {"doc_hash": "dd44295c7f9b8a3020236cf3a814b563f42b840759287cbaaad66d1f95c1da01"}, "/workspace/projects/LlamindexHelper/data/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.html": {"doc_hash": "a7f6558fefefdf18ec77d448af4d10c08435b31fa9c85df192a22bf636b7fb76"}, "/workspace/projects/LlamindexHelper/data/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.html": {"doc_hash": "10fe3cf898890c5d60d26e6c7b0660c2117223b48477c6b97dfc27d3f2bb4bee"}, "/workspace/projects/LlamindexHelper/data/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.html": {"doc_hash": "2cb63adcbf829a2e0e1248a1980a9cd453f999f473487c73da7bc863b964b261"}, "/workspace/projects/LlamindexHelper/data/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.html": {"doc_hash": "c09c8a73cb9cf8549563f71f6069ea8468573b66b05b90ee565f46b2c35b69d3"}, "/workspace/projects/LlamindexHelper/data/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.html": {"doc_hash": "97251c2225b2a5a3c110438fb6617c6d335393bcfe50502a3e62cfc776d0972f"}, "/workspace/projects/LlamindexHelper/data/building-a-multi-agent-concierge-system.html": {"doc_hash": "b7b67465b0b17a3070e7cdf6896b119d219e43be8c5c931de0c30065385d2a7e"}, "/workspace/projects/LlamindexHelper/data/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.html": {"doc_hash": "2fab045a04f59d835e1e71bd23d0acf61e5ee685d58b66e44154be60b1b84adb"}, "/workspace/projects/LlamindexHelper/data/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.html": {"doc_hash": "c329a00f6c3a8e16d49a10c44b5fb00b7a63138eef832fc126ac172f5055e8a1"}, "/workspace/projects/LlamindexHelper/data/building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.html": {"doc_hash": "6ff7436b1ebeb96d532ff78bdc64f6f25ddccb1890d080ea44f2b9cecaaa6521"}, "/workspace/projects/LlamindexHelper/data/building-better-tools-for-llm-agents-f8c5a6714f11.html": {"doc_hash": "2ef0e32d85e1ae85a2a5dceb743e6dca36b962cc3275eeb7fa271cc1deb90d64"}, "/workspace/projects/LlamindexHelper/data/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.html": {"doc_hash": "c25fd948b07e935b2db7f409242879ac31c614f7a759b1b3d60ff907e18475ba"}, "/workspace/projects/LlamindexHelper/data/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.html": {"doc_hash": "c681188e63de5e65eca9df669f45e84e0730d03c9c2d3c15a4e1dd6378d19066"}, "/workspace/projects/LlamindexHelper/data/building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf.html": {"doc_hash": "17e175992d30d72337fa7cc9b2c6db479714d442597cc0899a1a1786630204e2"}, "/workspace/projects/LlamindexHelper/data/building-the-data-framework-for-llms-bca068e89e0e.html": {"doc_hash": "5d32cc09c3e0a26afcd544bf5583ec13d28a1580ba066e3c2453fe222e81fc09"}, "/workspace/projects/LlamindexHelper/data/case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud.html": {"doc_hash": "db9176a943fc7ba0eba98c6db6369371f7cefea227456d55cebceb4ecb8918e3"}, "/workspace/projects/LlamindexHelper/data/case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex.html": {"doc_hash": "b4e685fd963f76e37461e933b0e9d47b2b3baad56c815a8ce8010dc08f7f4ffc"}, "/workspace/projects/LlamindexHelper/data/chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c.html": {"doc_hash": "b2274e0eca102b0efeac06d7883733f851ac5ca1ec6ea2778b012e15b5f3dbf7"}, "/workspace/projects/LlamindexHelper/data/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.html": {"doc_hash": "ec3d95e1a3543b655893c0c726a060c2093d3c18cb7806149125297dbede1367"}, "/workspace/projects/LlamindexHelper/data/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191.html": {"doc_hash": "ec76452fef823a094db37eba231c494f0701dc89847801fce1bfae82930a6be7"}, "/workspace/projects/LlamindexHelper/data/customizing-property-graph-index-in-llamaindex.html": {"doc_hash": "0390773b6bbfa862d9a52bfccbfe7aa9048d900a79b53dd73cd7a60e346e007c"}, "/workspace/projects/LlamindexHelper/data/data-agents-eed797d7972f.html": {"doc_hash": "14256eb33cb86681debec0901a21e0e36a03e740b9a7fe069b7ba0bd62d75ff1"}, "/workspace/projects/LlamindexHelper/data/data-agents-zapier-nla-67146395ce1.html": {"doc_hash": "3bf712da96abb1ce5edbb763f1dca86488386fcc893fd55f0494604d82d38ae5"}, "/workspace/projects/LlamindexHelper/data/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.html": {"doc_hash": "3b294b4e2110534ce96a43eea55308c7fa35298aa5f277c62e13e979349dd3b6"}, "/workspace/projects/LlamindexHelper/data/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.html": {"doc_hash": "5b20e24d12c344d9469d3e66539504ad9924c8cf02c63061f64aece86c17c79a"}, "/workspace/projects/LlamindexHelper/data/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.html": {"doc_hash": "10fcc17d09c9359d561b7fda943a6bc421cdb1374a1f2f27b5fbd1cdae26b2d8"}, "/workspace/projects/LlamindexHelper/data/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.html": {"doc_hash": "8db6207f4ac01f95c1a6219b51d1fab3e23377112f2ed36da62a0133fd28725f"}, "/workspace/projects/LlamindexHelper/data/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.html": {"doc_hash": "af11b5eda5d181a6a54b7e41dd8190f7aa7626c3a45a98e1ef42af317064cd87"}, "/workspace/projects/LlamindexHelper/data/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.html": {"doc_hash": "fc5a50fcd9707c0146040b8916947dc2bd0a20f73c7f38a45011f2db127e71ed"}, "/workspace/projects/LlamindexHelper/data/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.html": {"doc_hash": "c50a7cfb5e9442b43c1e44ef6e08a6943c70d7b43f3bf78e4c1eccad6d2579eb"}, "/workspace/projects/LlamindexHelper/data/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.html": {"doc_hash": "b55b0b4c394a6d30a901e020006797f9f5a3f9e41d9690ae0352f5c4ff66f831"}, "/workspace/projects/LlamindexHelper/data/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.html": {"doc_hash": "a0e6c7ee213a82414547672ab6b78d80f5f7d8e5a0bac4c7b1fbfdf5aef7cdc6"}, "/workspace/projects/LlamindexHelper/data/how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa.html": {"doc_hash": "0a290a27ee0674ef21d53a1d5a6f0b8cd5cc951da2ca74d53f928f661eca3157"}, "/workspace/projects/LlamindexHelper/data/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.html": {"doc_hash": "5fdf499ea48ad0ac25e4001ecca52ac0562dd180b016d02bac81a9aac364188d"}, "/workspace/projects/LlamindexHelper/data/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.html": {"doc_hash": "a29cd2c197f62871cfb1d980cd3811497b1c31c2f5e697faf0efe539b69c09ba"}, "/workspace/projects/LlamindexHelper/data/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.html": {"doc_hash": "e592b0e0b7fba3a86eec671b54f27b2b66c6b0c4cd2736277aaf27316a739ad4"}, "/workspace/projects/LlamindexHelper/data/improving-vector-search-reranking-with-postgresml-and-llamaindex.html": {"doc_hash": "1cd1f21cc33eab2280192a688cc2cd399139d33908422525fdf1298cf97b15b1"}, "/workspace/projects/LlamindexHelper/data/introducing-airbyte-sources-within-llamaindex-42209071722f.html": {"doc_hash": "405e51e240f1ff561fc6608637d4da7032589bc4bfecb72e112ddf5ca6093ee9"}, "/workspace/projects/LlamindexHelper/data/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html": {"doc_hash": "2ac1c499694039b98eb7b69f4e24372dfa4329f1807d344883100680d94f3c79"}, "/workspace/projects/LlamindexHelper/data/introducing-llama-datasets-aadb9994ad9e.html": {"doc_hash": "8341f3c53cf894870147babb6bc96d317464fc0d157a24c9ca6bbb85a32df1d6"}, "/workspace/projects/LlamindexHelper/data/introducing-llama-packs-e14f453b913a.html": {"doc_hash": "3ef4965e06665d616975934417f3167e789b892622f481dc9de7c0131a857cef"}, "/workspace/projects/LlamindexHelper/data/introducing-llamacloud-and-llamaparse-af8cedf9006b.html": {"doc_hash": "2e6958ae806cb8ccb26c790df3ae5bc47c2fe92ff888c7df79e084fb61f8548e"}, "/workspace/projects/LlamindexHelper/data/introducing-llamaindex-ts-89f41a1f24ab.html": {"doc_hash": "8e6e6f018089f80304e8ae15ea24a1c2abd30563685886f7eac90ad05f879c38"}, "/workspace/projects/LlamindexHelper/data/introducing-query-pipelines-025dc2bb0537.html": {"doc_hash": "5fcd57187a04008345dcaa755500264e38b61a80673c3379ff5968e4d34f44e8"}, "/workspace/projects/LlamindexHelper/data/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.html": {"doc_hash": "7bfb460380386e106768e6fc9536cd7ca75aaccf2944efe4baa1e83744ee9b02"}, "/workspace/projects/LlamindexHelper/data/introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41.html": {"doc_hash": "78a77bf175c9d1930752916347e0af150fdd1b50994cba1d8481c5bb3de884a3"}, "/workspace/projects/LlamindexHelper/data/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms.html": {"doc_hash": "4c4cd4fd3ac4dfcbf7eceea7d576b9feab428c4cfbe2a438d332f83fed644868"}, "/workspace/projects/LlamindexHelper/data/join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb.html": {"doc_hash": "1f8c57cca64b2a5ee151ce335521d84e4f5a1d0877ebb8f9c48fc2890ba752e5"}, "/workspace/projects/LlamindexHelper/data/launching-the-first-genai-native-document-parsing-platform.html": {"doc_hash": "d70b6f53f93ec60d63eb4b5302605fb4b2499d2f7931ff94438b92f042d35db7"}, "/workspace/projects/LlamindexHelper/data/llama-index-prem-ai-join-forces-51702fecedec.html": {"doc_hash": "bee41dba5bb7b39b81185c11de7381ffafc26e06e6cc9e79e844e3160590b657"}, "/workspace/projects/LlamindexHelper/data/llamacloud-built-for-enterprise-llm-app-builders.html": {"doc_hash": "633606a74dfe5853c6ee3c7f6dd0762b8c8c583e10495c9d42c739968cc5e76f"}, "/workspace/projects/LlamindexHelper/data/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.html": {"doc_hash": "f84474a265815f58e348abdd29799b7d7b57ac059fa306dfe6f44f5980ae0cfc"}, "/workspace/projects/LlamindexHelper/data/llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim.html": {"doc_hash": "e4ea5b1da0e94a0545a315b7fdfb296c598604d9b70d36357da9e7efd6825fa8"}, "/workspace/projects/LlamindexHelper/data/llamaindex-and-transformers-agents-67042ee1d8d6.html": {"doc_hash": "fe74273127808f5e41607dcaaf0be0a7dcb49e31b7e7ab731970686a27af9f43"}, "/workspace/projects/LlamindexHelper/data/llamaindex-and-weaviate-ba3ff1cbf5f4.html": {"doc_hash": "49955bb0c3c828891d4bda70049037fbc761c15e630cf249857bb0bcc3cf1d1a"}, "/workspace/projects/LlamindexHelper/data/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.html": {"doc_hash": "125159dff9a47594af61732245e01ed66002bd82a90131e08baeb0292958780a"}, "/workspace/projects/LlamindexHelper/data/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.html": {"doc_hash": "b0968316e709257488eed7145e24313f73266550ff18cd724cec0c137e6fbe44"}, "/workspace/projects/LlamindexHelper/data/llamaindex-gemini-8d7c3b9ea97e.html": {"doc_hash": "328a86f2cc33c0c18358fd8ee5f34d574da128d495616d7bf1c8c45b3ce3f8a2"}, "/workspace/projects/LlamindexHelper/data/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.html": {"doc_hash": "d01e8afcf24c399e302e87848d1108ce590055f4a70e238e8dca74e80a03af8a"}, "/workspace/projects/LlamindexHelper/data/llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff.html": {"doc_hash": "a18719eb34c25302fb18b8f61b9b0530f090c75cd4348b8e0e7766eee55bd4e0"}, "/workspace/projects/LlamindexHelper/data/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.html": {"doc_hash": "a3eab900960a3de1f2b0c04711592f525cf82796484bca03547eb624bc226c97"}, "/workspace/projects/LlamindexHelper/data/llamaindex-news-special-edition-openai-developer-day-e955f16db4e2.html": {"doc_hash": "21f445f1a6402547177b62aef9eb99ade9be75ff9ccda88e9a4a7bc098a5fafe"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-02-13-26fa79601ba5.html": {"doc_hash": "147971629299913862b72199d4e22ad685074865e166b980be6aaaf861aa683e"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-17-33514cbc04a2.html": {"doc_hash": "d06853ebd28c7cb07a0bb6b2891dc38e3256d66a84e11231a14e56fee3b7ee98"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-24-4a76204eeaa3.html": {"doc_hash": "e69baaa3690bb762986f0c7c0513e39c95f482a14e63ef008e7e6fc4c6c3ad0d"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-31-36244e2b3f0c.html": {"doc_hash": "d2c7e891f6146ee71575a768a943e3656e52520ad2a0fc174b5e2c0338fff544"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-07-cf20b9a833aa.html": {"doc_hash": "b0343638464cad3d64462cdd2898dc10dafda7e796b30334467ddc590e3d1123"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-14-dad06ae4284a.html": {"doc_hash": "7f0f6b000bd1591ef2cadf0a8fb34a2cdfc70ee55e5c2e7e1515377e35f370df"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-21-aa3a71e339f8.html": {"doc_hash": "12ca24bd50039036505baad0e0de41060f428952e7a37197f8567db5ec923765"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-28-a31be430a786.html": {"doc_hash": "211a0f961eb7e859d723b12ec6ca8e6df30e11240bc897db03532223ad79104f"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-05-faf5ab930264.html": {"doc_hash": "d45508549dfde710ff822c44460bf4116422a5a294eb858d8b18ecef352bb73b"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-12-4a5d542fbb1e.html": {"doc_hash": "36618229597305a14af3fee930f32d9a2c1a65316c1e39d8887ea3f8318f36c6"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-19-2965a2d03726.html": {"doc_hash": "58ea14915c47a8d01b4e84fb387c28379f4b91dd8b4bdadfe14fb9d0ba7a256f"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-02-f349db8c1842.html": {"doc_hash": "780758123860007109dc0e52b7a8cf11848bfa8b872dddbad7156e0253dd1ea0"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-09-6209000da2e6.html": {"doc_hash": "8d9907fb6454b0f40a571c2a77724db199c31eb5bca264477501d8b4516866e1"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-16-752195bed96d.html": {"doc_hash": "23bcd4934b03bd750402e272e142b76dc01dd7fed20edd795547a6e8f4ef35fc"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-23-11ee2c211bab.html": {"doc_hash": "0d2190aebff2fb35c9ede6e93ef7f06a74535500b90b05b6db3080a353f92336"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-30-0d01eb0d8cef.html": {"doc_hash": "d0767ad763deec7cd75f5c1bda9022e9146e4b4f8137c84723d8dd01dc9766a6"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-06-9a303130ad9f.html": {"doc_hash": "03742682a85e544071a8ff4e273cf3fe6d3123df0a7509388f58d5bc830ad21d"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4.html": {"doc_hash": "4daa1a51a94f739d9c8945017236f7481f76688a3ba3a0af21e2ed4bea15a2bd"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-27-4b9102a0f824.html": {"doc_hash": "cbc12f05386cd46d1f8e7ecf98d9db9e8ec13b010a289e68700d4abcca085a1e"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-05.html": {"doc_hash": "951430a5bc04cad70d11849f0441123fc4662962810e3819e701961c3e51108a"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-12.html": {"doc_hash": "1ce98b90b3592745ee4965c6e724f3dfbfd14237bc1337cfa5f9fbb60e04e0c9"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-19.html": {"doc_hash": "ef372aa291ffc53119df859ee021c76213dbfd049bf725e5168222987f46702f"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-26.html": {"doc_hash": "aeefd8a7ef5da38141f211b9bfd907b90bf156ad9b03b229bd3e502103d1aff0"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-02.html": {"doc_hash": "b6038056331642b5861c05255a44dfc639fc8441719af85b4f4aa5103aeaee12"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-09.html": {"doc_hash": "32f2f3de01380a514a19752b6a218d4f1c6ebabfef861f965c21c910b2fdc952"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-16.html": {"doc_hash": "7ada839f5b9520d58fea4f72422a6ec7c5a3b03068958a3df0b689b7ffe508e3"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-23.html": {"doc_hash": "154bb75d7342a83305bed8d463206a8a7b6a8b3b86a7a928f150566d312ff86e"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-30.html": {"doc_hash": "90facaf75d350d9eace318d69e723aaf16df303a67660133a0d4f99af73cac65"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-07.html": {"doc_hash": "820354584311ce412620c5c072434ea5648c378e36584e5f5a18242a10eb02ff"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-14.html": {"doc_hash": "7da1d0a5d4a1f3317ea48d089cc1443328368e2b84dd3bb68383fd988d09e936"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-21.html": {"doc_hash": "d348b4b726a8cac38d5ecd5960ed8973c14f2815718aee48538914f3e6f68130"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-28.html": {"doc_hash": "a756c3a770c3f2cbde542e907b61ff2e90d824b614db8c3991f16ac5c9f694b8"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-04.html": {"doc_hash": "30537a0f6aa9ae3aa698c1deb53e00fc1420fde3ad53c6d378e3920169020a03"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-11.html": {"doc_hash": "2f4e44edbe7f18082cd706c2c761f2cefac5accb0ff099c25284c44892729d14"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-18.html": {"doc_hash": "9b856e0b34ab165ab5598430dab3f4b71b6f02b3f030facbf4a15e1e305a8e09"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-25.html": {"doc_hash": "b9b47fb1033a08711c788e6cbfa967ae6d1a41f573ddfbd8cdee80c0e09fe973"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-02.html": {"doc_hash": "78a345e94e14bd99668a0ba4ff5df1c398159e73ebc650d3180d4b0b0315cb01"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-09.html": {"doc_hash": "cc14c0258d97286e97e15eb8beabddeb1e692e7015b52bf02d990446259ac09b"}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-16.html": {"doc_hash": "b3de3b7dce6e073b8a813e74f026e7cee04e9e0929f2c8b42822633199dd5ded"}, "/workspace/projects/LlamindexHelper/data/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.html": {"doc_hash": "a8084dae7a6f4719d715cd009c849eb7265088843291132dd7250e626ef0c32e"}, "/workspace/projects/LlamindexHelper/data/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.html": {"doc_hash": "1f9435be8813e6872f5ce015845019db9403e41a25099d32a266ea14216943ef"}, "/workspace/projects/LlamindexHelper/data/llamaindex-turns-1-f69dcdd45fe3.html": {"doc_hash": "93d65351a4dfdefd75edf5b136bd525b13d8860e94efaf8fb7aad55763325d18"}, "/workspace/projects/LlamindexHelper/data/llamaindex-update-07-10-2023-4ceebdab96cb.html": {"doc_hash": "57c674bc45f424b1e1e456fff5231cea2751ded9c3a0c65bc9af9d2bbfceaca2"}, "/workspace/projects/LlamindexHelper/data/llamaindex-update-08-01-2023-185514d9b897.html": {"doc_hash": "4dc5247533a0b51d5487e5c7f22616c2354310813e11b533ac72c4fed895a1b3"}, "/workspace/projects/LlamindexHelper/data/llamaindex-update-09-03-2023-4a7c21c0f60b.html": {"doc_hash": "5f71ae857503defb0e4732e4b1ce50744230cad636affee822d1f58b0874afb2"}, "/workspace/projects/LlamindexHelper/data/llamaindex-update-20-09-2023-86ed66f78bac.html": {"doc_hash": "ed5e7a8b928efd35be6c36fa5949eb2af59bebf2e5adecedd9566bca320323a1"}, "/workspace/projects/LlamindexHelper/data/llamaindex-update-2023-10-10-3718a3d19fb9.html": {"doc_hash": "fbe8f20275396f075954f359d5ede9ba4a1af3995a814fad8baf4818b5b0507b"}, "/workspace/projects/LlamindexHelper/data/llamaindex-update-6-26-2023-ed30a9d45f84.html": {"doc_hash": "50db8707eda6a0307e43672195cf77d151278becee8376eefd89448badaccd8d"}, "/workspace/projects/LlamindexHelper/data/llamaindex-v0-10-838e735948f8.html": {"doc_hash": "455a94bcb19f046324e6c2572d19354bb586fc9bb80911ad745fb92017b1fe0b"}, "/workspace/projects/LlamindexHelper/data/llamaindex-vectara-7a3889cd34cb.html": {"doc_hash": "b5103e80f3ccfadd408ad7f989535272e9fb24f71b26d9fb2cf3fd3b7bc00b48"}, "/workspace/projects/LlamindexHelper/data/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.html": {"doc_hash": "126ef641399332f32bdc250e3d248cf25e186b6f4373ce1c42bc0ef42acbc5f2"}, "/workspace/projects/LlamindexHelper/data/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.html": {"doc_hash": "e0676ba93dd5e5fabef3b60e1100ee5778499da1eadba4bc6a08b3646ff38e86"}, "/workspace/projects/LlamindexHelper/data/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.html": {"doc_hash": "3f0b841da5d461b2881fdca3c40804a9c1ab31e8f06c12f5e73e4edc40c7b12a"}, "/workspace/projects/LlamindexHelper/data/multi-modal-rag-621de7525fea.html": {"doc_hash": "849b79b8e790c8c6c12214528629ca0335c51156fe39b87fcc5a15bf89ad2511"}, "/workspace/projects/LlamindexHelper/data/multimodal-rag-building-ainimal-go-fecf8404ed97.html": {"doc_hash": "45412031e5764515857c28ecd043f1b418f38f7542b71985f82e392e58450e82"}, "/workspace/projects/LlamindexHelper/data/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.html": {"doc_hash": "4fda312600e23e933925fe752fc9fbf7104ea94155e8f013bdf3eaaa2d332b10"}, "/workspace/projects/LlamindexHelper/data/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.html": {"doc_hash": "72aa6c7dacc12c624a8a1ac91894ba875892c43a00c4531bc0d9d455abbcba0b"}, "/workspace/projects/LlamindexHelper/data/newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.html": {"doc_hash": "1d4d8a989442ebc975661ef276378a52d372f7e7115a51fbc52c276aac65e81a"}, "/workspace/projects/LlamindexHelper/data/nvidia-research-rag-with-long-context-llms-7d94d40090c4.html": {"doc_hash": "1a641d109358ca55607a3d66646e46605318903454398a5988dc60b8d69656d5"}, "/workspace/projects/LlamindexHelper/data/one-click-open-source-rag-observability-with-langfuse.html": {"doc_hash": "7d7815f4836c827eda7d9ef2644a819c11e89fbb25dfb1540b4ce7ae68c5e10d"}, "/workspace/projects/LlamindexHelper/data/openai-cookbook-evaluating-rag-systems-fe393c61fb93.html": {"doc_hash": "bd5e8fba007541791b2e48f4056f678b1c6f4321c0dcfdab4c6547fb0e76b075"}, "/workspace/projects/LlamindexHelper/data/pii-detector-hacking-privacy-in-rag.html": {"doc_hash": "efbcdef3054ff2c567677396669244bd813ff98227476146555f15f465637c49"}, "/workspace/projects/LlamindexHelper/data/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.html": {"doc_hash": "9fe860045c98b7ea94c8ca8172790b3b6dada29515a8c66e90fb0447cb25adc3"}, "/workspace/projects/LlamindexHelper/data/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.html": {"doc_hash": "0cb5d6286e5bb51298885ca672ae003cb5705bba007b9da4948419d7de62badb"}, "/workspace/projects/LlamindexHelper/data/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.html": {"doc_hash": "0e7a691e8160b7e8388ad76f8306d998104a86f4410a92e65222a786f2bc6c8b"}, "/workspace/projects/LlamindexHelper/data/retrieving-privacy-safe-documents-over-a-network.html": {"doc_hash": "d601f5f6a3f967fa14019796cf5e93383088c229753d73f56f6f3db56d7b4467"}, "/workspace/projects/LlamindexHelper/data/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.html": {"doc_hash": "e42498454830e4e968baac3fab3c02800b2accac3cd51897409080666e421f79"}, "/workspace/projects/LlamindexHelper/data/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.html": {"doc_hash": "3f32afd136785eb4b02d83ac5c183812322acdad9141dd558d06283fe986f7b0"}, "/workspace/projects/LlamindexHelper/data/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.html": {"doc_hash": "209cc9fd6966436241836dcca4c9c02439fc09503b384bd4b9bbc351274b2593"}, "/workspace/projects/LlamindexHelper/data/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.html": {"doc_hash": "6af6fd50a8567d6491d7a9f5f2398eaf8e00a9009055770d72adda46cab2835b"}, "/workspace/projects/LlamindexHelper/data/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d.html": {"doc_hash": "b214253c5be161d00cfe5b50646c37966cabb3fd832cb15000900d42c7a1153e"}, "/workspace/projects/LlamindexHelper/data/simplify-your-rag-application-architecture-with-llamaindex-postgresml.html": {"doc_hash": "1de78c55852abf8b2153ecf54590a46c70224e35c4599bc73e1d24b4b7ebf835"}, "/workspace/projects/LlamindexHelper/data/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.html": {"doc_hash": "dda22db7a59e83713cf318592ff12150d63fe022f082a82064a4a064cf84e7b4"}, "/workspace/projects/LlamindexHelper/data/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.html": {"doc_hash": "7d5d24270f903f1af56392ded4464e9bf761653864cd8fe747d8cad1386c9638"}, "/workspace/projects/LlamindexHelper/data/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.html": {"doc_hash": "beb66d0ebec5818e2eaafd47edcd70963f00c48795962a04da2126e0feba1bbf"}, "/workspace/projects/LlamindexHelper/data/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.html": {"doc_hash": "d311a4f51b97b2034e31ce7fba884a7bdaf10612a2db32813f2883ffd0efdaa1"}, "/workspace/projects/LlamindexHelper/data/the-latest-updates-to-llamacloud.html": {"doc_hash": "d63a623374096b01b56660202c55c793a48d3993b62eaaaf3e5fb15806b9c83a"}, "/workspace/projects/LlamindexHelper/data/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.html": {"doc_hash": "fc40ff6dc2c470bccf3c37c98d71f267750a5535e6f219689f1218af738b0e77"}, "/workspace/projects/LlamindexHelper/data/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.html": {"doc_hash": "e75e88705fa4b67503227f352849fa71c5269748facc18fe9c9ef825c1aa23d4"}, "/workspace/projects/LlamindexHelper/data/towards-long-context-rag.html": {"doc_hash": "af28f6d3ccf60dd832d4bdba18b08798ea5f9ccf70260a1933e9a12c285b681a"}, "/workspace/projects/LlamindexHelper/data/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.html": {"doc_hash": "1b3cbb1db33a74502176df46f79a2bad28549e2a0365ddcc09859d6b6ff2c07a"}, "/workspace/projects/LlamindexHelper/data/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.html": {"doc_hash": "5438c45a743a5eaaa81bcabc6d5fc7f280e932f14ebfe982835f2b3c20db0604"}, "/workspace/projects/LlamindexHelper/data/unlocking-the-3rd-dimension-for-generative-ai-part-1.html": {"doc_hash": "1737d321ca023ec6ac6d78108c2965b651747fff0014368694c4d60c6810c528"}, "/workspace/projects/LlamindexHelper/data/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.html": {"doc_hash": "62cde2b61ee8fa5b8732f7e9769965c12a284d359ef363f153012a749440c889"}, "/workspace/projects/LlamindexHelper/data/using-llms-for-retrieval-and-reranking-23cf2d3a14b6.html": {"doc_hash": "1b88e4f8f641450fb09a9fad4d906ee35f926083d5bd2fa7ed1c5ea32ba20e9e"}, "/workspace/projects/LlamindexHelper/data/vellum-llamaindex-integration-58b476a1e33f.html": {"doc_hash": "5fafb46bc77becfed63072af8c423cf799d5d2142ca41be59e18b800dd4e1d46"}, "/workspace/projects/LlamindexHelper/data/zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc.html": {"doc_hash": "15e093f14b9f38934cc59942b4153a40a80aa2eb227f7efa3cd010aeec3c849f"}, "d7be752a-0e06-424d-8aee-3a98e5e85a5c": {"doc_hash": "7d0c9d2a06901f729d08fa37ab254fd825b78ffd58342553c1b89ffa19bb6d68", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.html"}, "fd4001fc-71fa-4139-a074-30a1e7623240": {"doc_hash": "ab59fa92a29ca81d8915022af0811bb3598dceba3b347367cc78b448c5ad7b82", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.html"}, "c8449782-e685-40ae-b9e3-7444396543c4": {"doc_hash": "24ec9d686be0fdd436d2e125de268d36e345e213b30dfa090d56b30f05522f4e", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/agentic-rag-with-llamaindex-2721b8a49ff6.html"}, "e0af1e9c-54d5-4fc6-b29c-200c841ef247": {"doc_hash": "e012c083c67486b3a75ed759518acb446dac1be08cc0290706535f6045c52baf", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.html"}, "2d74700e-cd3e-4388-a062-02ab00aef449": {"doc_hash": "28f45a9d51fb1d08693b1cf5e1487bdcdab8a36fec872fb1347b1049d4001f97", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/announcing-llamaindex-0-9-719f03282945.html"}, "b21d0b46-68a2-4661-a1d0-00ed2b425b11": {"doc_hash": "6c460571c1fb275640cb5cbfde042cb974553c79df032bd275ebb105bdc9c392", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications.html"}, "7dafcb22-17ae-4dd3-8487-0b01dcb3f669": {"doc_hash": "8709bf8437d831f7d3479e37429be7fd3e807259e39bd245ac3642befb935a46", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/automate-online-tasks-with-multion-and-llamaindex.html"}, "542e4fa9-6577-4c98-a8c7-72ca69e69993": {"doc_hash": "f0a49d466d9d893de9f2013718fe35be972c7b3e52de5b42c95007368c11c306", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/batch-inference-with-mymagic-ai-and-llamaindex.html"}, "4f6ca503-a707-49a5-8bc7-a5e0d6a923be": {"doc_hash": "af434a72d5655b83d8a366ce0eaa15967b62b8e19b71cf7d30b939709c7d551b", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/becoming-proficient-in-document-extraction-32aa13046ed5.html"}, "721d9c47-002c-446a-b063-8586ab40dfe4": {"doc_hash": "458df2b59bce66a86c63a8efe6fcc514a4a7a684327e1eb85c57374c2c1b4e77", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.html"}, "ab682f7b-a39a-4347-a397-ea949f7645c2": {"doc_hash": "360cacd5d08bc61728d3763f5c4d2cc53d2a579e1fe3672ba5a5a947eea73545", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.html"}, "677eaae0-49f8-42b6-aeef-77f6c9a29fe7": {"doc_hash": "f99f0271382645e1e4983f43eb986982c406e48a806b6b56740ae571b6d85328", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.html"}, "91bbd0bf-7804-48e4-b8f1-c45ebe769478": {"doc_hash": "b13aaacdfd93a275f28c1bc099ec2f5415e91f9f6d500bf8326770053ab37bec", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.html"}, "eb53e9aa-77c0-4337-b93c-8d845533424c": {"doc_hash": "048a7a746eb4f4925c2fcf73164080ef98a545486c506af0ca7f4de03305bdd8", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.html"}, "aa501d43-7ee2-4310-a6bb-bac4f0169314": {"doc_hash": "09e212aec5efdd71712b3d398974e8ecb3fc28166584d1379c0509d5113e5d3b", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.html"}, "bcdb8818-7166-47e9-adb9-9b7fcacdd629": {"doc_hash": "1ce992024db133e8d88f3fc9f6531c20001a52d92baf88c9a4dea1fa691f0ce2", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.html"}, "9e41746c-026f-4163-9c90-e76d8317896c": {"doc_hash": "0bb2c0df667eff27771c53fb0e315e7ec1015ae6133b7c84bf44801903acc661", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/building-a-multi-agent-concierge-system.html"}, "1c6e6f5a-fd22-479c-aa6f-883a608eff98": {"doc_hash": "f831f0078c5ebe870e8039a712bf55cd4e5f2db2c482735348c3e52ba092e1cd", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.html"}, "53114f4b-875f-4c80-a26b-dade1c3fb97b": {"doc_hash": "66ef35f8a6b54521ddc131b7af4df76c17d5ecf0468d3969cac7a26cc2e01a9e", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.html"}, "cb3fad68-987a-4b78-8d97-36528341f73f": {"doc_hash": "a73c6ba4c3180f78a356cc610ad90266e6e0df750e4e9571a421afd98e338e10", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.html"}, "72caf566-b9af-486b-aa5a-5519e5509e53": {"doc_hash": "bc1dd2807abb01cd343d571bcf7473b217fa38f5649de79bdfdd309e17e0b2fd", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/building-better-tools-for-llm-agents-f8c5a6714f11.html"}, "a75e40b3-ef51-4482-a252-f4cb730e78c6": {"doc_hash": "54bf4acc60ee5905c6c79ddfc471a8ab55ba257cf9ddca2efa3de6d3ba38ea3e", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.html"}, "3c2a141f-c9f0-4abd-b500-8cb1106bc427": {"doc_hash": "5b03621f93e551e7ff99d1b70b47085cc0f15fed75960df80c7c138355dfc896", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.html"}, "4bf5fad7-81d8-42a0-8759-88e33e229195": {"doc_hash": "74d63085bea719d1d8b5fc1a7f4c724b8a5956db535c265a9527401ba1d83e18", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf.html"}, "7603167c-80bd-4ae6-ba83-be688bcc6ff2": {"doc_hash": "cb5e3a377942d388b54080266b4e4de4efa5115b852f2e48dcab76bb772e17be", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/building-the-data-framework-for-llms-bca068e89e0e.html"}, "b27e9bc5-89de-4fee-aaaf-8bd4a02091b6": {"doc_hash": "a321026a629403d73a9798a238cc418905d5ad58f0bb9e5b639088c64642261c", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud.html"}, "ef7ab3d9-ad12-44f8-81e5-a733fe37ed70": {"doc_hash": "b4210b020afa8043e997321c80da549b3e6a8cdb64b5578ff128673ee6a695f3", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex.html"}, "eccad25c-8678-4c75-be21-1b172414e304": {"doc_hash": "f14445b9bcacc4e0c718607fe908b702753b337a23dc5962fc3738b914c66d8b", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c.html"}, "43c11e43-a9e1-4e20-a578-35a234992fc0": {"doc_hash": "3a03762ee3cd47e991a4e4ba34010427e5a32a5b03796b9e9adf6e77df92bdeb", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.html"}, "093dd541-ece9-485b-b41d-4ae80b733e86": {"doc_hash": "c85ffc6283275e9b5f78de50f19490ca6fcc809fcb78b832435efe6fa6f107ac", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191.html"}, "1ebeacf6-ead5-42b9-8080-9a1316efd4d6": {"doc_hash": "8e40542f801e8fcf885f6b121ed6137dd464bd449c0c89424ed960d4f0354f7d", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/customizing-property-graph-index-in-llamaindex.html"}, "a56f2b2d-eb47-487d-b108-690cae3ec9f6": {"doc_hash": "4691366e62824910d847076a20d77e55db06cfc12247d3c6bab0f6f420cbf86a", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/data-agents-eed797d7972f.html"}, "45d275d9-debd-445a-aae0-6fc18f97f963": {"doc_hash": "f4324ada455e8c5a1f56e554c73465e9ac6613e7375d32a37e84db64e8a77f74", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/data-agents-zapier-nla-67146395ce1.html"}, "1298488c-751b-4e4b-8609-0fc822bbeda4": {"doc_hash": "6a0d38012fa94d323d5e53531ecbe7623eeccf3edc7cdb2a400c2abef0f1c075", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.html"}, "58797dbc-24cc-4e0c-911a-c75a82476b66": {"doc_hash": "9a3f96d3b80120c9642ef7c2b919ed2d7ee7227c17003dd82046a698fca271f1", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.html"}, "10abc301-d5d8-4f47-98f0-448a326fae47": {"doc_hash": "3d2cb61cf7763e1cfdeacf525ca9a61328b4abbdc6e6ab8cbfd245a9b3196a7f", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.html"}, "cf9e1e55-3c01-4ad7-9212-ab5716ef2216": {"doc_hash": "6bdc4ba310097eb049e924bb1a8612a177177ada331bb7a818af15ce246acc9d", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.html"}, "2ab33830-349c-4cfd-a9df-27023e133c9c": {"doc_hash": "68f021ba88e163ad7840826e8f38c5a5e201616c863615ababb08faa73e3b927", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.html"}, "1fb1c0dd-614c-4cce-a613-fa39c6634069": {"doc_hash": "43ff19bc97153acca06bd711bb1507cc719932f36cc8056cc29c7a4c249b4141", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.html"}, "e5133220-b12b-46a4-bbea-dcdc289b1280": {"doc_hash": "5f571acb57d2d691b7242d9f3c5a78fffd27c9cf73b1c9b50b7cd1beb744b408", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.html"}, "cb09bc93-1d03-429b-bf6b-157bf6d0e135": {"doc_hash": "0ca5d48ed9e30a053c1e1b090600db5d31697444f899cd4420621081098d13bb", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.html"}, "fcd4f869-7e6b-4140-8092-7f0fded29c81": {"doc_hash": "ce90213f9edd36f1cb4a95261136d2da96404f8187cb59cb4091e8aac9b89383", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.html"}, "7e016859-6da8-439e-859d-93470687a784": {"doc_hash": "e2bb28f014bd90b2c21cff449443d7f851ac3eddc27d87cddc1cf12a979042f4", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa.html"}, "015332b0-8eea-4d0a-8af8-d3ec7fdc0849": {"doc_hash": "917720515c333d13bf3f4adbeec2fb65ed1780e5615368269ae6de6eb499d824", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.html"}, "19be6fb5-5454-421a-a25c-65cd25babeb4": {"doc_hash": "008b0d9d286b627d73fe38ac3a646961faf38150079b9185b55de95e314c0ecb", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.html"}, "b0411a47-97c5-43ae-98c4-efb40e22c9bf": {"doc_hash": "bf572feffe2a6eb912c54a67bc8f5e2389063a0c99b41284f28f20efef8831c0", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.html"}, "d4a76c0d-8011-4cfa-ba89-4bd18f779538": {"doc_hash": "b229511fdd422a02481ceb395f78caa511b8cd9ad463534a7e7c305db141d0f9", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/improving-vector-search-reranking-with-postgresml-and-llamaindex.html"}, "ad700478-309e-467d-bfcc-15ed542fa03e": {"doc_hash": "c40e64bc3dd4b2feb4bb85231474438f56c54d903d9acf25d5ff2dc2b8cc7305", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/introducing-airbyte-sources-within-llamaindex-42209071722f.html"}, "c18d9bd8-763e-4a8c-b32f-2e8c87b5c3be": {"doc_hash": "c12447427e1ff7540931cf13ce8925be3c67cf899018a1c5b2e96419c44ded44", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html"}, "0f337657-1fcf-4602-80ec-e2765f1ca312": {"doc_hash": "943ad76d038fb44c331f3baccc412c93d23020b23522c54a3e0d0777cc2bbe8e", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/introducing-llama-datasets-aadb9994ad9e.html"}, "0d551e45-4afb-475f-84cf-6e916d1bb625": {"doc_hash": "54f5afe24c2eb2e9bcf2949da42a5bef77628a590ce780d71d19dad7941de21f", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/introducing-llama-packs-e14f453b913a.html"}, "75b64a9d-5435-4697-a5a1-e6f68fd72ee7": {"doc_hash": "d8f2c5cf0b829471a6e4c728e57cd55ef14f1fe5b4a35cade25ddb618a8562d6", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/introducing-llamacloud-and-llamaparse-af8cedf9006b.html"}, "67a39a17-1d98-4e44-b55e-0c34fbe4931a": {"doc_hash": "30aefd9e519465110c840df9fdbb380d28eddef03b792ef73440afe5721c7683", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/introducing-llamaindex-ts-89f41a1f24ab.html"}, "29b4215a-bbc5-47b4-8d27-5747dc7f4dcd": {"doc_hash": "a4800fcfc96f1cc9ef241c4013f814f6888cdf80252c351303ca7ec03ec76ee8", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/introducing-query-pipelines-025dc2bb0537.html"}, "0d8709fd-deca-456e-8637-22e1d560e2d5": {"doc_hash": "0f3c9b8cfb298f6d6360f026cb959e8be201719bc83749c8d382b3c0a75a8e7f", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.html"}, "45b64eb6-f8fd-4097-9e2e-1cf2b25b01d7": {"doc_hash": "3cced5285777a4881f9bb39322c6d6fb5cfd222743a12be2ff2db6b461d5e6ab", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41.html"}, "5da260ff-81af-437d-bbc7-e67593307b87": {"doc_hash": "afcdf2458ba1329fa5ce79b551d4d6e72ec4b17efe35d3c72a8a89ede3d52e70", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms.html"}, "6f6bbd07-b237-4787-a3ce-fab279884a70": {"doc_hash": "d0891245c72971301aa4d2b8779c40abe3afd09e47e01e27cff38bdcb4febf46", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb.html"}, "9252e368-47eb-4114-922a-b9098ed2a1e4": {"doc_hash": "2ee5f9034775c8436ac16b55a368540d9f8578a3e0479c3747675c2cf02cc21f", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/launching-the-first-genai-native-document-parsing-platform.html"}, "b4d5e3eb-45fb-4b5e-8bfb-ccecefd2ab7b": {"doc_hash": "b07538b6f2a7a647f24fe263503fbcfd2b4941aee1d6f6c0fe101c5f8df9388f", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llama-index-prem-ai-join-forces-51702fecedec.html"}, "7ba8e8f5-55df-4d69-9171-68459d3b96eb": {"doc_hash": "f54807229f96ba38793a3533a1e11541a48cc11686747199ef7d58d6f2c8ccbc", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamacloud-built-for-enterprise-llm-app-builders.html"}, "eaf39a01-bafe-4c52-85af-37d412ae586f": {"doc_hash": "ff4046007eb385fba844d1632cc75420e6f41f7081e0601890b4797b5b771499", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.html"}, "e84ce598-1e29-4f9b-b8b8-d1a63678ac26": {"doc_hash": "f69087e109c267ea155b693c10db34533c8c42ed78b6fde13c053e05b51cd1a9", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim.html"}, "c71a83e3-f32a-4445-9ee3-cad21e9a0778": {"doc_hash": "c4bf33e3a7e50087efca3beed293ac56b0b7cd48289e5fba56398ca82a08fc13", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-and-transformers-agents-67042ee1d8d6.html"}, "3e5d98e6-d512-4b30-b531-d13042572fd3": {"doc_hash": "15198cb47871e35cd7a7202a988ae8a3c68eb2b35feddd94e4c008bd354a4f16", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-and-weaviate-ba3ff1cbf5f4.html"}, "83e4306e-ad51-472d-8de8-5db4fe1e12e9": {"doc_hash": "d8b40809839023f2b6ea0c5aa5a78ebcaf519933ba7c0898ccf59f0a545ffe3d", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.html"}, "a368acbf-5e8a-4f67-90fb-681d10d26379": {"doc_hash": "0a41c1918b04be741f881045c20850576fdc2bf131b75ecc91ab23630d4e27c7", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.html"}, "d4312cad-4695-4e8f-84d4-793758a8c966": {"doc_hash": "aaa753ee9ce76bafed33aec13c6266098142c192583bbe00493d319b296e1353", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-gemini-8d7c3b9ea97e.html"}, "75c83f20-cfc9-4004-a2f1-3b5844923fdc": {"doc_hash": "9bc1fb003229c1e72f556024923100950b8b73f2a086f9765e172d406f5953ff", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.html"}, "b2394993-0632-41dc-9b85-fb6398ac127d": {"doc_hash": "4ee4efaa9e852705714d8cb1c92cb478f56710f9184a42e927e1fe81da57e3dc", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff.html"}, "ec34043d-40a0-4bd1-9d3f-3c1d34b74d6f": {"doc_hash": "42362189d8baeba3e08aecacc9abbe3d674b0a6071772f62a9565f8818d0dc45", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.html"}, "fd9bd377-d076-4e2f-ad94-4c70fb203431": {"doc_hash": "b10e5cb2e7e60079bef9cde3e6d970a60ceea6eeb407f571c9e4596961561090", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-news-special-edition-openai-developer-day-e955f16db4e2.html"}, "286dc06c-44d3-4ddb-81fe-c5e32f246a8e": {"doc_hash": "8241d8250a3cb5393d6d06ed0fb7e2f0ef60fb3bf50cf245753c7906a1528243", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-02-13-26fa79601ba5.html"}, "1b13ab53-9b38-4ac2-b634-9361051cd562": {"doc_hash": "a5b0e3ce620774a194de479298991ab164d14e3d3a3e048156c2f4b131fed779", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-17-33514cbc04a2.html"}, "de3e81b8-63b3-4ea1-8dbd-0e9608cd55c7": {"doc_hash": "12172af3c0357d375db959480268173cc4a7bf3f015e7d26f3f789e6e835cc14", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-24-4a76204eeaa3.html"}, "ffec9c1b-f29d-4596-89ee-323ae052143e": {"doc_hash": "529d16a1ba81ddf6c8db45c91170501b49976fdea099cd94c4c1f735faaaf33b", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-31-36244e2b3f0c.html"}, "bb45192e-23b8-4f4a-adb4-ebc82f1f3440": {"doc_hash": "bf0d782edd1179ade8497ba980670c9b28ea8b6eb28b3f9ea9f1392c0001ef03", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-07-cf20b9a833aa.html"}, "dae6e131-5d00-4a87-8cee-c2e54de14693": {"doc_hash": "9feb8f94a497b6be1150d26ead792e27eba49bd9ab0bc607c8d24a2739a7292a", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-14-dad06ae4284a.html"}, "1c6b3573-c71f-4c67-a579-7be2e27c9846": {"doc_hash": "f1d7f7b7b5a0b9a38893d30641312298d4e57b2494a0c5826e5bc27a1ff29bde", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-21-aa3a71e339f8.html"}, "8b11322e-cfc5-490a-9095-8a596889d6dc": {"doc_hash": "68094a79abf06a217f6a7d910e1decaca01198cac82fe31b07c1924ad1af8469", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-28-a31be430a786.html"}, "b917ee22-46b2-4ba6-b9f4-499ee3e46976": {"doc_hash": "12cc5c2735fdaa98d2a26e1ef13af2b8af77ccacb0f2938616be13945f995759", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-05-faf5ab930264.html"}, "2896da82-3211-4bde-8115-d8d376dfeeb2": {"doc_hash": "263be9bd34a585e89e02f07e803671b852bdf4b759ffd14483cac8224059182a", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-12-4a5d542fbb1e.html"}, "5fcab154-b832-4485-b490-a1fc906a44ce": {"doc_hash": "892d4d3cdb24fd0a09f8136fe84d130a5b3dbba7a6ecac98c3ad7505d09d18f2", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-19-2965a2d03726.html"}, "982f24c2-20a2-49d4-9f3a-cb216cb40c75": {"doc_hash": "b4d00e31937e103ca83d0f7961e2f8e0159d3f83a343d0708aefc55a73255a53", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-02-f349db8c1842.html"}, "547215de-f1fe-402d-a7cb-bce82b5fb107": {"doc_hash": "79068a618dd25259c3ce8161c75e541990c6035d8f9291088f1f9d315efb7ebe", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-09-6209000da2e6.html"}, "b293f6a2-825e-4e15-9fa8-bacd8a1a6255": {"doc_hash": "b85bbeffb00d6ac94e0ca31306d4c0691922ba9c976bb23f311601e1271d4235", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-16-752195bed96d.html"}, "d0215726-4154-4f22-abca-96c226bdf066": {"doc_hash": "4accbbc1e57bc84bf489064582cd42e018e7376435a292f4be432cafaf15c233", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-23-11ee2c211bab.html"}, "befbcdfe-273d-447f-a8a8-443643f2518e": {"doc_hash": "5a97a5005bfe26250f7a72a8e3567e42adcd88dd5d667b79b1e5bcffb74fe1c6", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-30-0d01eb0d8cef.html"}, "651f6397-c19d-4a2f-9fdb-2a657699aa69": {"doc_hash": "2897907b63571e5232e003db8b14ad74f191702b1615147e1f2f8380bf2d9e42", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-06-9a303130ad9f.html"}, "c292cf04-4202-46ef-9615-ebd8b4f691e1": {"doc_hash": "805e18a0697d30de7dcfb4a508700451e16f05f4dbe0bf853a69cd36307ff1e7", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4.html"}, "8b59e0f9-8a67-446d-a31a-47f64aabb86c": {"doc_hash": "d0e835745cac1f776486e582e9980e372466d06a9c0d9ec16c58c3ce3cc1d5ff", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-27-4b9102a0f824.html"}, "309f00b5-e781-4d35-998a-88bcadea4915": {"doc_hash": "8cb92adb6c6d894b49dc428b89cbfa21cd1a640200d5c0bf72937a8fa34d9513", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-05.html"}, "5507c3b5-7743-46ce-85e1-540285feec65": {"doc_hash": "776d3def0f00e750aabd32a7bcea7fda99869fd5ca7f8444546d4c531e454992", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-12.html"}, "fae755c6-64eb-4e29-b14e-ec1f5fe151b0": {"doc_hash": "e164886cc468b5db3561dd95c93530084b9dc0639957d1eafef09fa9d803baac", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-19.html"}, "ee8f4315-d714-4901-be24-598390cde4ae": {"doc_hash": "9f15fab2019360364167ae9957c7ef5ac72e3b6480c6ca76a2aa8d7a64ae1cdc", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-26.html"}, "2645d982-aadc-4e88-8d16-a0cda3215a7c": {"doc_hash": "380c5616994483eb06d4c2b5db82d4abfe208c1c537ed359ab43a7dc1bddb833", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-02.html"}, "309648a9-822c-44ba-aa22-09311c2de138": {"doc_hash": "799f740d65f4929893f95c9f607c9e1369efcba4b1270bfea589a11dcc8e6c74", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-09.html"}, "231ef033-a676-4456-9ec2-d998a21fddba": {"doc_hash": "8fe9dd5e26895517874f7bdcf87655ec118323803657cbcd0fd13a4c655a53c0", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-16.html"}, "fa6018c4-71e9-4a2b-a80d-629135ec97a9": {"doc_hash": "5f1f436a72d918aaf8acfedf2a756bd78132640185db4df6f23868848cde347f", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-23.html"}, "017f7ad5-1379-4285-8155-0a6ccf3c4800": {"doc_hash": "9d1880fe36066bd9a3b4ea65199437b1c8a301fd468f6305e8ad85400bcff8e2", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-30.html"}, "60ecb273-470b-431a-a3ec-512ae5c68015": {"doc_hash": "03c2c820c04099ca966cb7c438560181da94c2e7dbbcf3d98477f853ac6b5b0b", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-07.html"}, "64c022dc-191e-4e0c-b783-6676845d219e": {"doc_hash": "74dc9d0a5ccc028f143803d543995b5659af1537a26f87fa2f1617fcd20b6d18", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-14.html"}, "e40d8b2d-e098-40e0-88d3-a53b9ae8308b": {"doc_hash": "14dea1551f46f847f614b3cf92abdb5b1065655d03a7e1e550bfd32e0b213f2b", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-21.html"}, "4d064c70-90b1-4465-97e2-2eabafc0feed": {"doc_hash": "9953af3ba18cbe33a96b762dc5729847f1a55f160743dd800ef226300a26e7a1", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-28.html"}, "8180bf62-60e1-450e-9589-4b0667a566bf": {"doc_hash": "19856402720e7e12e4efe3deeac598d6cbd4b222cdda60da77377552e00db7b4", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-04.html"}, "ed9e395e-b9b9-4821-a4f9-06cc2b7e5329": {"doc_hash": "2b2b4ab8b5f593438a81d6867f0d1628d94e105eea6601682ca84cd59733a44d", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-11.html"}, "3d42b824-bce9-4a63-9053-cb9e91c8801f": {"doc_hash": "c8cd095bc6aa9623620c6fc1898929f6021b1300ac9105bf2e2ec800b6b56409", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-18.html"}, "51420421-c965-4215-85a4-5d3e334431ab": {"doc_hash": "33c9945d9098d64f0cdc5b6dfe78ce6306b96e8cf763479aaec58a1d29dec0cc", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-25.html"}, "9ab4246f-4d29-4686-abe8-2f0c710de25e": {"doc_hash": "4b18a89c6a6f3798cd58c03e866bed4bda0a09860d47a982bb1eb9f004390ff0", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-02.html"}, "141f71f0-efff-4a47-8861-6b1a809a0a15": {"doc_hash": "8eb9b47909a794d12d8529920c189f07c28458b0c09071b0baa75ece475e43c7", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-09.html"}, "257dc101-bd52-471d-9104-3ab57355740d": {"doc_hash": "7ff587b808ca04db315ab75c1784dcf4db5e6b9359c56905e37e9b9713b4cf40", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-16.html"}, "94092881-2ff9-4817-a884-f349c4240baf": {"doc_hash": "28946699ab66e252fce2d90201e8f0d0411b514b7dc4ebf259ec3f0c523eaef3", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.html"}, "4ddf56f3-b705-465d-984a-043fc1b59144": {"doc_hash": "4465e95f0e16e535e19bb8961d01186baba2c357a9db31b576dd7c7cb9816916", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.html"}, "525538fd-c063-4fa1-bf29-c8f33d33b1fc": {"doc_hash": "4104df3e6312f23a0fc0c51b91bcd28c0a4a7db84eab9a4c92a51bff366a74c0", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-turns-1-f69dcdd45fe3.html"}, "a900c947-cee8-4e8f-9915-97ae111d8b8a": {"doc_hash": "e61468a293579a4328b3837894d8c1129dd556933c826b001b6b21f8463714df", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-update-07-10-2023-4ceebdab96cb.html"}, "c9abdf40-a41f-4099-8d3f-e24b5f91a5c3": {"doc_hash": "0ad8d19210a14613bc281dc6de53f28c4374b1bab5f5928c51f4800c3db40265", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-update-08-01-2023-185514d9b897.html"}, "1b725d51-5e7f-41e5-9688-ac6ad095544b": {"doc_hash": "d621fd9f832e2aa95be26d13513838896406e9c32843f5e00a0f4708a36ff2a8", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-update-09-03-2023-4a7c21c0f60b.html"}, "3cae4701-e505-4058-9d4c-7ecef3082ea5": {"doc_hash": "a0772f359f46ba6045203d48db50ae28b3d78ed83d6420d04967b190fb0ff584", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-update-20-09-2023-86ed66f78bac.html"}, "b857c6d8-1c28-4c5e-ad15-aeb0017a4539": {"doc_hash": "bb5f53d4efc81b94860f50766605e4fdd9d8e8451091cb4aa2529b9f566a873a", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-update-2023-10-10-3718a3d19fb9.html"}, "6c5f7ca6-b5b6-4fbe-8e7e-19da0ce2f118": {"doc_hash": "c69675cdae84f3212e70e4c18020c6aa7c66ecc0eff8034b49316357c59018ba", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-update-6-26-2023-ed30a9d45f84.html"}, "6fb7feeb-82a2-4dab-bf53-1e4295f8cd5e": {"doc_hash": "8d64499320c7e5807963ce1c9b1dfb3c0872334f5ee8f2f8d1d56901d90554e0", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-v0-10-838e735948f8.html"}, "ca5cd227-31a4-4889-a697-58fde080ff56": {"doc_hash": "eea07219ff1d4ebeb09ba42fcd9d1572345b0815eb21c977ab27c000e9973db0", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-vectara-7a3889cd34cb.html"}, "16f2c1f8-4c52-4e46-b267-afb162402d47": {"doc_hash": "064bb89bfba557012f4833110b1f3d75af983663ad0ba7c168e021a6afbe170a", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.html"}, "53468c8b-eb55-4386-8249-8e2dc11ff5a0": {"doc_hash": "5ca5a66a9d3a18e04b6f02e4f1fe4952891b1b825f6f78469a67e4bfb2c5888c", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.html"}, "46618cc7-3c24-4ae8-a484-5105fe1c8c62": {"doc_hash": "8b7a328327f821a664c3b896d2e6584af68a9b3cd33d14948f8b90f2f0f7ffea", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.html"}, "2934dde5-632a-4630-bd67-e76f5036cedd": {"doc_hash": "72f98dc2aa6905db29ef51f42d7c30e1247e1ad9aa655ffadabd1e5e93271980", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/multi-modal-rag-621de7525fea.html"}, "5b6ae502-fa08-4814-a455-5bf4588d5787": {"doc_hash": "382b37b9ee722d850e6af56db595f05ae51121005d09ffdf4b76beae1849ced8", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/multimodal-rag-building-ainimal-go-fecf8404ed97.html"}, "4802a12a-b25c-4d24-8cdf-6a91a0f50268": {"doc_hash": "5c5a2be705340a53fb61b4376e0e41bfe779543aea2696f86192b61e63344aa6", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.html"}, "96a79d46-9c47-43d1-a537-51395ef4596b": {"doc_hash": "4e92155e36415c75537b634a83a2873884a6c527ad6e3a696d50277200db29b0", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.html"}, "14357e15-3916-4aa0-aaa6-b3214b2a64d2": {"doc_hash": "ba754840fc1bbecc1342cb0653d21ce6b4378a44eafc0bc9def09a6584f9ced2", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.html"}, "0212bfaa-89d6-456c-a78f-aa8ce03094df": {"doc_hash": "d1f4ee6b8e1394fa401e31749d6df96febe3d5ceb20c39011317efdee103bd19", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/nvidia-research-rag-with-long-context-llms-7d94d40090c4.html"}, "3939f831-8ef2-4f7a-b240-ed12c62e4306": {"doc_hash": "8b53175576760a860e4557cf10f78a31bb7ca898cb8449265306ed3bac86a2b5", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/one-click-open-source-rag-observability-with-langfuse.html"}, "80cb2875-d534-42ff-88b0-2b247c4a4365": {"doc_hash": "b682d3a0ffa616605462aa8dcf5005ad75a55254e9741546abdcac309fb4a86a", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/openai-cookbook-evaluating-rag-systems-fe393c61fb93.html"}, "d177694f-0d97-438b-a3ce-7696eaf89084": {"doc_hash": "84eebb10bad8bb31fafe88c55c9af9a30b8d09385975ccc1de68384157241aa6", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/pii-detector-hacking-privacy-in-rag.html"}, "c7886207-4b88-4a1a-8e30-820391e9deaf": {"doc_hash": "0f8a231013f37fb31c4b78ab42964f90f7901f019cb1708d93e6024718b6eae1", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.html"}, "b2862724-1e15-4d32-bd5b-11992d50b4f7": {"doc_hash": "df1631143b0650b9b90e328f663751a08751ca4a80e47ed8eab7e135288e3481", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.html"}, "2d336599-7330-4600-95eb-3b84a5276fe7": {"doc_hash": "40fb933bf005ae1d1eaa6c4546a7734a430e22a9395bc3e45c8601d4c822ead8", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.html"}, "9383589f-e840-4b67-b2a0-56c8ddc6bdbc": {"doc_hash": "48cd68f1d22df21ae681415b012dd53a4cb84249aa139aafe1f26a55861798a4", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/retrieving-privacy-safe-documents-over-a-network.html"}, "b9858923-6720-4415-a97f-908b5f503af9": {"doc_hash": "ace862b38cbfa4169b7490fb6044fd4167363d0c53301faa02eb32bee209512b", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.html"}, "8a037895-5207-4bfc-a218-4686c6572581": {"doc_hash": "aef13396b3de96965c48c06682358fed7cecc9d267c7ec884316678678b2ac4d", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.html"}, "8d0b0051-1b7f-4ed8-951c-f4fcd00755af": {"doc_hash": "3c11780d9824826be96b1498eec78357136a24b2ac8bbcdc191524eb804b6d29", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.html"}, "a85b9a85-1bb3-4a38-bcfd-07da5bc554f0": {"doc_hash": "85197d3d6ad6a8deed683b60eda191552c9462ddfcdc6d9e00770b3aa6a717e4", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.html"}, "8e89f7d7-9a58-4ca0-b468-42d0d14c0287": {"doc_hash": "2ccc37a2b95b0cf843f726652508100ca2f16ea9af559c19a92e8247e5af1c5e", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d.html"}, "d44d1b66-a6d3-4de4-97e6-449624b3fce1": {"doc_hash": "429cf01ec2c66968695ecc907b630b8faac9677fffe45cdf7b688e5b84e43b67", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/simplify-your-rag-application-architecture-with-llamaindex-postgresml.html"}, "8ba07468-3aef-4f59-b2e4-a4fc9643f4d0": {"doc_hash": "e578ac8e4770a8069b2aa05c4abf31cf63c9a4b8009c31517d12969084280b88", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.html"}, "07e7f65a-092e-4a2f-8f4a-db3039b3e8de": {"doc_hash": "50ee91481983d35c86a7f4b149ec9c88ae7121ecb9d13b8eed54f3e766c309da", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.html"}, "79bd7357-de68-4075-b944-67130bc2e1c3": {"doc_hash": "cf1336292a0e4742ba0678741357e2e0c6feb7f54b5be4ad0db0fcfc154f5e02", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.html"}, "512f2072-c2a8-4b01-844c-5a4905be82a4": {"doc_hash": "c21ce49360a637d5a26053dac21f3af3d9ffde66e162ee72caceca8825677060", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.html"}, "56225273-6ced-4d10-a708-beb42b52bc19": {"doc_hash": "b361805db8a4fab8aec6ba3ebbc791c98372820047adbaa3923cbcff81b68e74", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/the-latest-updates-to-llamacloud.html"}, "8ab1d3a1-cacc-48e3-afd0-efeed2a51579": {"doc_hash": "a3531a8139a20f4494db2392da2732b0edd83fa667dba6d08d506785a5110ab6", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.html"}, "886b75ed-9414-4389-9e46-fed3f2f5e0ac": {"doc_hash": "e41a3bd7a1eec00c7996f7b69cdc6e5a2e6fff0c8b5ed4c0b2c751b7519287de", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.html"}, "89666c95-e8ad-4519-81cd-3bac4c737a3b": {"doc_hash": "01d6a9908b04c4b4840431e19c3b4f86e1784b65021d58caf60ed627ddc6b92d", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/towards-long-context-rag.html"}, "9b3d12de-a059-49a6-875c-0b6d7afdd4c6": {"doc_hash": "f734b7b1c0ae9b5e6ff19b41947730bffb5b5616214158ed93fd65b9c9eead2d", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.html"}, "44d7b787-127b-4136-8076-08c9bc871bd5": {"doc_hash": "32de611f54f4b959268537c78350a5e6f16157034628d1bbfd32631ba0995d15", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.html"}, "781d1d55-0b3b-450b-a735-1f91508a309d": {"doc_hash": "deee3c51338d5fb011b700b0454c51e32161ecb7ae2f08b10ed0131b5d27f69a", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/unlocking-the-3rd-dimension-for-generative-ai-part-1.html"}, "29e5f7c9-ebbf-4ae8-a2e8-3e2dd022921c": {"doc_hash": "e3d80bce1b235ea1afb3c0a9f2ef6d79603260aada0e0571af953f8234873d0b", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.html"}, "a7eb7f5e-5364-48ce-aa1f-77e9aa159c2c": {"doc_hash": "289b91862f504ba277112b81079c952aa6f8a6e1e8e4d7a83efd048967254ce3", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/using-llms-for-retrieval-and-reranking-23cf2d3a14b6.html"}, "5612d787-8f42-4b63-90d7-ab5d6e37df5c": {"doc_hash": "98c4fcb632297fa09b144a07c349d14109911728dd1609ef79cf5671dafa13bc", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/vellum-llamaindex-integration-58b476a1e33f.html"}, "fbb41713-78d4-4c48-87be-05927fc3dba6": {"doc_hash": "599a1ed5dd0f4147d45bfcf6cd9b6e182a2dcb6e1adf715622ae2976bc18941e", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc.html"}, "83001b14-fb2d-400e-b31b-3a48501408ce": {"doc_hash": "0f6528e78d94a558b82e03c15ac06aa70e3bfff248015cb525353b2c5e9286d3", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.html"}, "90d0dad3-83e4-48ff-969a-41a450639f14": {"doc_hash": "59033e022ca9593b7096ad75fb49278c5eacc019cad2ec7d732d5f15f1fdb1ad", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.html"}, "8459bd37-a59b-4eb1-b346-49c451ffd64b": {"doc_hash": "20f0cd2a44795de1d58b7d8f36b8725254a4250d676d3f704a92bcd0ba3ea63e", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/agentic-rag-with-llamaindex-2721b8a49ff6.html"}, "78e2927e-d1d5-46d0-9f7d-a83732e7369b": {"doc_hash": "c3308febee8e5e944cd44d5313a78fecaf0d9a27667ddd0445514da319a5f7f1", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.html"}, "77ba3a80-549e-47e9-bbf3-f70fbaa39686": {"doc_hash": "3649bbf315a36cea13fac0f0366a2a4f0a8ae2ad6c5e8aa9be92194a3a36f01d", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/announcing-llamaindex-0-9-719f03282945.html"}, "c07eb866-c51d-4e7a-b9af-1df89655b2d6": {"doc_hash": "2a2a639925a33d8fb95c5f36d4ec665d196c1532cc1cb742ade7117c5d982ca0", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications.html"}, "6c94c9c1-8eaa-45d4-befa-54e84f2ecebe": {"doc_hash": "0b137e326d7c196580ae990af84a0bab08ad963495bdd61ef52d8c752959f84f", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/automate-online-tasks-with-multion-and-llamaindex.html"}, "f32cd777-600a-4a8e-91f1-c5340cfd85ea": {"doc_hash": "f4c833ec4984a7fa3209286694a2cb58a6c40a5028646385fc8da96b6e87001f", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/batch-inference-with-mymagic-ai-and-llamaindex.html"}, "51cf26f6-53c6-47e1-8ff7-3b845c7744cc": {"doc_hash": "57bdf7f79c29685fef12562dc8ed9c4d642e44363301b06e721cbd4363c26e27", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/becoming-proficient-in-document-extraction-32aa13046ed5.html"}, "2e1d8446-49c6-4183-ad1a-6f5dfc6bc6da": {"doc_hash": "f454d16e2d2e92a19a5eb35078350cfa48d5cbdcbde0bc03da9f2b6d6ef8cc3c", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.html"}, "71360fb6-350f-4905-aef8-19f79ee01ff5": {"doc_hash": "8873d69151943b2122c454cc588a10d967f0f478d3f7abc869202d2322d6899d", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.html"}, "d1003737-974b-4885-96e1-a46fd31a3e22": {"doc_hash": "c92f1271fc2b6bc96ddcc9c9620a7b7ca1f8ce40d228fb9fc068a7a632254dee", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.html"}, "4d6dc69f-9dae-4907-a8c0-14331d223732": {"doc_hash": "33f2487cd06c4e92f1bc56a209c8430a8f5619559e5a8bfd828e59eeb2239de5", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.html"}, "e5fb4319-54c6-4233-91cc-2367005a7202": {"doc_hash": "d7281ac359e52f17f9e64fede1a483c5abb28c4ab6a1ca04b8fa60f66959106e", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.html"}, "cf82d9b0-25ef-4a65-959c-bc0f5589ced8": {"doc_hash": "927a62a664be2c24ff9cfed8fed854251b6bab77407b5de12617703db30d8a15", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.html"}, "288844f4-181a-4fb5-813b-81b6f7fe1b2f": {"doc_hash": "edf8b5f68a200e0c4e3f162ad3edb9d61fc83c22e7320a0603eaa530f8eadf46", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.html"}, "7c22a9a7-21f8-4e51-9b36-542b2a213711": {"doc_hash": "f78f49a84df868fd858cc5346364b63d6edb40abe1a7ef06f8c64ba33d68408a", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/building-a-multi-agent-concierge-system.html"}, "650a5a60-f82f-432e-9069-94ced09b1840": {"doc_hash": "fa35a1d3ea89ed380a149eea673c6478d63f86f11836769e6f5673df8520fbb3", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.html"}, "5761f8d9-2484-4561-80f6-fb3eb3b80d7c": {"doc_hash": "0327b2d5c2e25aa85a5afeacbbded9129afcd49fb220602971661099dd8bf62b", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.html"}, "1f12b488-aa4a-44fe-8351-c959e234c53e": {"doc_hash": "d6909bb9d27869d09106a981d910684054d71f109fa39346a4486b201e67fec2", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.html"}, "7f9f3383-657e-4718-868e-2beea6e45e25": {"doc_hash": "11e1c910290c50b5275e942b42f7e3507e3b737f27a7e37ffddc26b147b4c56a", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/building-better-tools-for-llm-agents-f8c5a6714f11.html"}, "46aef392-a42f-4159-846f-713d933c31d9": {"doc_hash": "92a8ba2e5602885cd49a8adb2a6e201a0a942c0fc4287b2ab1af0b4108e2a127", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.html"}, "efdb4bc2-a61c-4fa3-9b1f-3ee7a9fd6f02": {"doc_hash": "0fcace800661a4c22f07b1387fdae71e81f9586725bfae8b2491beda3d10b3e3", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.html"}, "29dde6e0-b00a-4ec8-8fc9-91c1a11cced8": {"doc_hash": "bd177f1c92862865df6ca686f3088ecda01ff8017faf5685c5b7c40ff47974dd", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf.html"}, "fc5bc645-280c-44b1-8f55-a923ad5a53b4": {"doc_hash": "c336359d8ff78e9ec345d3252a2b16c710ec068952a1af7938ba621de16ecb89", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/building-the-data-framework-for-llms-bca068e89e0e.html"}, "bc255221-4c45-42cd-9448-37ce4bf85975": {"doc_hash": "95334b8251b6adbeca67ab4ca9cca75c295052ef710d2c69c912f1d2e58bc8c7", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud.html"}, "ec2945bc-f663-4011-96cb-9f322642710a": {"doc_hash": "8883325a48ab0cf3f6a7994b9c00d7698955528a7f0a512b8a6fa068fd845858", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex.html"}, "ad5e5614-fd2d-40b8-b5fd-77e2351f5c1c": {"doc_hash": "f5fc4133cc15747ad4538d625db62ad4a32c6005e7fcd64b7c437ecc173e71aa", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c.html"}, "13840a23-c7d3-466a-a5ef-3ca13ad3c289": {"doc_hash": "0be31b9e7ebadab90f762e4badc121ed65e3648a18a8df90a6c7febaeb0b15bc", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.html"}, "4ac9b548-a6ce-44f0-a298-8b1bc6870b58": {"doc_hash": "1a1e33510364714b16729bce5d93c0d0c1979933d94d8070c55965ad07e4aa35", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191.html"}, "6f9d733b-3654-42e7-8055-4a5b1cc7929b": {"doc_hash": "fce1b92a4d43631861655fc9b8548733eaa06f4ead2a8adc8e39cdcbd30f9e30", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/customizing-property-graph-index-in-llamaindex.html"}, "c0c3b479-4e27-488f-96bb-2f7958531f88": {"doc_hash": "53fa9a1fc20098efa60abbac070b84ca27c2a2dd6286fda501a02ad9e5fb7661", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/data-agents-eed797d7972f.html"}, "b89fc93f-8fe1-455b-b2e2-3f039fc4c821": {"doc_hash": "0ee96ca0ed5bab0cb5f795d069a086b5271dcb605b5a0fbe0ba7b98f6d8b8582", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/data-agents-zapier-nla-67146395ce1.html"}, "dc3aae11-61f2-45bc-97a1-6485c18dcfbe": {"doc_hash": "10c45fbcbd5f6733aac33ecb53f1dcbf6307b2fc2c144794de4f2a7885c120f1", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.html"}, "8d53a5a5-a61b-48d1-9726-c37659c594b0": {"doc_hash": "d1a1387b705cd720be6ca2656c419e0451a5f4084744909062eef752aa3584c9", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.html"}, "3a4b10b9-3792-4e0b-9b47-a409b6735b7f": {"doc_hash": "713ee3ecade468575a853007b399e906f63bab0ec4a33abe72a7278d1e2f212a", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.html"}, "c239bd6c-3955-4c0a-a528-3ec9f4e865bf": {"doc_hash": "f8498966422e84d64c89c50dd1229d1063985cbd4ad08e069ad1f47b812d44ab", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.html"}, "5c6a1117-be96-4783-aa30-8d8d0f1f0fc7": {"doc_hash": "199babffea20cf46e57eb74fb225312bf807978d53d65fe1ebbe3d9f134f43bd", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.html"}, "492d2827-5f2e-4694-b90e-02beba3aab66": {"doc_hash": "c97b74f3e1cafa4c6ddc40fcf252d97cc6906153975c7f4ad862f365b08ca8a1", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.html"}, "5c818674-12f6-4454-98fd-f5ca4e575a4a": {"doc_hash": "88a4e1a8a8ebfe9a259dcdcd5935223f560cdb3543b3433091f1f5fa984c59a3", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.html"}, "e24625a9-19a9-4658-aead-0413b1fa8d86": {"doc_hash": "4b2c7fd53ce1bc7fefc5b97720545c7b62c7b7f686b8e0ea6bc40b59a15630f4", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.html"}, "b12f7641-553c-46e2-b38a-96edbfdf383e": {"doc_hash": "6f6c6cd85406da2d3ed261ff2b01aa27cf6fbb97a0b3c16a7bd86ea931a6e186", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.html"}, "4d400bfc-be64-4be7-91c4-cee2a8590dfc": {"doc_hash": "c518cb14a0cf56eeabfba9fb768fd1fd28bf2781ed2cba880e6d099c2a64487d", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa.html"}, "a0e32d09-7d5e-4924-a122-f574b006ce43": {"doc_hash": "633c17e1ce0738448630588c7637b98a34fbee92046ce42331ae3c4512587276", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.html"}, "3f3c8fa1-e2ab-4f59-84db-9579abf12516": {"doc_hash": "b8759b078d1b20b77a12242a22bef9c44f017b99bc02c70d39bbac34e9cf1fd4", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.html"}, "076ff5b2-eeac-4b00-a2d3-a69736af563f": {"doc_hash": "f0a1a42013d81442cbd93440ab7df39ba5f06832c3ddebd35ab4f80230e6e749", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.html"}, "7182dfc4-59d1-40bb-a441-0eaae03678b7": {"doc_hash": "6a66eb2230d7d2765095efa221a3e9667a3b16aa0e4728ff1be3b1e0bcc51898", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/improving-vector-search-reranking-with-postgresml-and-llamaindex.html"}, "e6c2f380-dae5-46d1-b859-51ecf810ab63": {"doc_hash": "5d26b454b895db6658b96c124b97b589fdcb962ece7a3cf9195b0d06ba93d235", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/introducing-airbyte-sources-within-llamaindex-42209071722f.html"}, "60ff6c04-bd0a-4e31-926c-93f341377cab": {"doc_hash": "547931d272a624000f73fd0607bf384bd66adfb353003c1ef991776a7ec0c28d", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html"}, "abfec326-12d7-4050-8535-fb19b5e8d1ec": {"doc_hash": "7a81696a632d7110958689abd6d62e87f22595b720f3cf3f90789d45f9790c5a", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/introducing-llama-datasets-aadb9994ad9e.html"}, "123f9c77-f764-495f-b50a-463c23d9ef6d": {"doc_hash": "15989e8175a11fe96b5d9333fe9ea5d3aea2737040e9594779ccbbdc1f21d3d1", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/introducing-llama-packs-e14f453b913a.html"}, "1d700344-f592-4c7e-ad65-58c9a2b23dae": {"doc_hash": "889ef537a9de825376a9f44d2de1c65b8c06c7c4314fb12df1a2d5520bdaad98", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/introducing-llamacloud-and-llamaparse-af8cedf9006b.html"}, "93df4a84-3e21-488e-be5c-574732ab057f": {"doc_hash": "4681edc545c4dd5fbf9def592f79a020315b5d6448fd62128794c244def19489", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/introducing-llamaindex-ts-89f41a1f24ab.html"}, "23c0c342-b5ff-4097-9372-42a689349c3d": {"doc_hash": "da51698f0f44cfc29f4d2f4b4dae89edfe8db0928180578452c47867cd7d213b", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/introducing-query-pipelines-025dc2bb0537.html"}, "f6dc427b-b915-4647-a282-6d68bc6fc466": {"doc_hash": "a6401666cbf7903d369d88d3827d55798dd497af3137fdc9c8fa55b55b288559", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.html"}, "3f8bbe65-f50d-4736-91b8-f2c2d95f664c": {"doc_hash": "4ddafeef36b60bb1811eb6d8e4991e0b618743ba4c8e11ccadce0d0a8e9963d6", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41.html"}, "b34bddd8-397b-49b8-aee0-35fa2312c3c7": {"doc_hash": "8cbb69602c0f35376bce40922d414ad995824a2a01d9ffda749474e7f3a707e3", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms.html"}, "88fb8b71-9417-463e-9594-a800a6bd7002": {"doc_hash": "36e9f5b7a396e3f211948fb3dcbae179ec78ef2f26b81f6f38923fd1b083e456", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb.html"}, "edde25d6-2068-483d-9f6c-d1efc27c2289": {"doc_hash": "60f774ce44caa0fce16d2789f1529b987f5bfc6a7591531c7ba6fb489ee642e6", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/launching-the-first-genai-native-document-parsing-platform.html"}, "0a331bec-d6d4-44e9-b437-bc33c4cbcdf0": {"doc_hash": "480f65c7d950a097e038aab1730ace72079ded931a4f3bb27a3f744e896330c9", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llama-index-prem-ai-join-forces-51702fecedec.html"}, "01310167-4545-4da6-832e-2377043f67a9": {"doc_hash": "c3b3edf671e2a7bf3f4bf0e8b9bacdd15ab558dd040ef7f1a9a4593f15a222d7", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamacloud-built-for-enterprise-llm-app-builders.html"}, "871bde9f-8afd-44ee-9e8a-778424484ad6": {"doc_hash": "2f14a8d344f34b3b29d475adfa25493e5acb12d87a665358047808813689b202", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.html"}, "0413acd1-7b0e-4b3d-bd40-f5b40cc74b56": {"doc_hash": "3e32d0bbcc4425d821e475d57ed4d4600274e44ccd0fecbe6806b5cf9731af84", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim.html"}, "1fc794d0-dd4b-4232-ac2d-bcfdd2f737c9": {"doc_hash": "34f71e0507344a448a7cae149839da8b56753cfff1f2abf0831daedb07235d8c", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-and-transformers-agents-67042ee1d8d6.html"}, "1e80d500-411f-412e-8c7d-0ba3aabb3930": {"doc_hash": "87721585ffaebc7ebbebdb5efa271552435c9f2fc5ed66f2c8a7df49435c3fa1", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-and-weaviate-ba3ff1cbf5f4.html"}, "90128250-545b-4fcd-92b9-504e7d59be68": {"doc_hash": "90807cfda31abcdaf81883d6a695d5760285bb5246854449aed583e7d62671b7", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.html"}, "e2a7e039-d3fb-49dc-a063-62b9bcf32e48": {"doc_hash": "f47478a29940d8c4d6bf2c9e7d06e17790cee848d213a004a5a1935776088c4f", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.html"}, "bd573d2c-d04b-4397-9d42-579528d43f12": {"doc_hash": "80b92e2913fdc6d37c9b1c5aaf0f69ee1de9c63890093ff3eccdd49c4318861f", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-gemini-8d7c3b9ea97e.html"}, "f2a9fcbf-4e45-4a09-9280-2ab03dbd2b91": {"doc_hash": "8951db8fde7c99d5857a4e5e887f97bf1bd7abacdf567cd49c3e9b7b39eb39e5", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.html"}, "d0855239-4e1a-41e7-8feb-593b16c4512d": {"doc_hash": "b7e6f1489cd8b9505d7c9480113e008ed801904abc092c00ac5a9db9f3c150aa", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff.html"}, "155d642c-6e3b-4c3b-9178-080677b69062": {"doc_hash": "0df61137e51cf64993b57ac576c65bdabaada2b571a923f10171d3e2deedd876", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.html"}, "b94ada67-94c1-4eb7-a4dd-6f203bb98d8c": {"doc_hash": "5d6c82e7db9079361cc822d2985824977d998729df9ab95cd89823d237c45e08", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-news-special-edition-openai-developer-day-e955f16db4e2.html"}, "49f1def6-1d5e-41b6-9a7a-8bcb23d9a250": {"doc_hash": "2773df3d340681f622ff82e6054ce71f888d60827705fba3ac83709daa7001a0", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-02-13-26fa79601ba5.html"}, "db379d9e-d881-4f08-bb6a-bf7ea6c03076": {"doc_hash": "240bfad5eb9486ba1417831ff70c84892dea74c638ef0773e617357243caa541", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-17-33514cbc04a2.html"}, "ae3fa659-adca-476a-a932-11719039fdc4": {"doc_hash": "548c219df234942e24c6150feacd8fe79c71c7d9757a2bbe2e0f5d27c20bc742", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-24-4a76204eeaa3.html"}, "8eb79a05-ff5d-4057-86bf-9f1bbd6bc8b3": {"doc_hash": "7c0438fbaa5ecae750d956083e3e910c55500649ca965775fa2a74dd26464270", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-31-36244e2b3f0c.html"}, "369ac48f-7909-45e3-b24d-7c27e92212db": {"doc_hash": "2b1b05c1138985a1befc8a5f5863156a92b313c8ce481bf49490ca581f8d33ea", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-07-cf20b9a833aa.html"}, "91d0debf-63e5-4b72-b5de-658f6d9bbcdf": {"doc_hash": "45cdd38f48a9148787fe4f00a8c82fa3bec919b1d685494d51254db52d88abaa", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-14-dad06ae4284a.html"}, "e56ce492-07a5-4612-a6fe-5677c2b2ecf0": {"doc_hash": "7179784d21fa705645715ca55f99d4dba1455a8446af316ee89c19bb668aa16b", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-21-aa3a71e339f8.html"}, "0f199c3b-e433-46ed-b4ec-67d697c45bb8": {"doc_hash": "d3d0664ca4491282a75f21a543deb9688191b778920c16f284578fe3a00d1df2", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-28-a31be430a786.html"}, "ace7cd41-9c8e-45ae-a906-cbd7bdea2de3": {"doc_hash": "d4529e7989b76f77354d5adb6c51bc8b2ff856c8c04fd4b064df7dc140d8f792", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-05-faf5ab930264.html"}, "393e9985-d181-4b5c-b17a-e2641a6d865d": {"doc_hash": "2e5888bce42940586ad572d621921c146b80e7b2954e8119c6fbe91cd7e761ae", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-12-4a5d542fbb1e.html"}, "eec58ba9-7ab0-455a-9b62-b68a9faf700d": {"doc_hash": "3097e6ad96dbcec9be5542335735e593ab00738d3edcf6116c85a4b3d8040f3d", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-19-2965a2d03726.html"}, "f2b6eec6-c895-492e-8b70-45b0c98b4436": {"doc_hash": "bb322d61541c95c2f37c8c6158b2cabdc63bb1935303835a0f432b1440131dd6", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-02-f349db8c1842.html"}, "ab81fa3d-da8f-42cf-8b30-fa58fab75ffb": {"doc_hash": "fc8243811ee95706c1ff0ad0746ef3b36f15e6b20ef447d6531240c6ba7e3d94", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-09-6209000da2e6.html"}, "b4f4106f-d3e0-42c8-97a3-3014f377c37e": {"doc_hash": "d3bff0fb11382689c61590343f1d0eda8dcd2563ecdf240d394e97c586b2947a", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-16-752195bed96d.html"}, "5eabadc2-9f73-4400-81df-e9167e1c1595": {"doc_hash": "552ecf3ba450aeaf2e3cce273bf0c95cb84e388f8e2a8cc13c5592b56090d782", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-23-11ee2c211bab.html"}, "6916b918-0b45-4117-bab9-1c322353fb32": {"doc_hash": "a84f0de039ddf09c289515ae8e99af1f334707942ca995ac703f1767e42c7e1f", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-30-0d01eb0d8cef.html"}, "df738da3-01fa-4c8f-accf-3057e7d28913": {"doc_hash": "9fb553dda632171fdeb8400b155f95e93f86982ebfa66464312934c3265b8889", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-06-9a303130ad9f.html"}, "3bdedc09-2d20-4d3c-b0e7-5048dc2d3a23": {"doc_hash": "a059ed655b73c43f0bc0a781ecb89dc791ea4a3239fbd392ac567d245b9b646a", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4.html"}, "a921b43b-bc2d-4860-9b27-e223b02236fe": {"doc_hash": "812ee4428db8418e1d074572447cd08630cc0886de862c885edd80c9502702d8", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-27-4b9102a0f824.html"}, "7a547101-75f8-4471-92bf-c1b454ce94fb": {"doc_hash": "aa98a9692b019a934e5a5e132f4097ae208139e039480b6d02ac437d7caa9641", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-05.html"}, "57234cb7-d09a-4463-bbcc-fa30974cfa17": {"doc_hash": "1027a9d218227f7e98536e73d1ccedfc99fd624b06f7f2053f6a241afb454219", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-12.html"}, "6678a088-8c15-4de7-bc3a-1644f9402804": {"doc_hash": "76efa4c1a15838fc3f2230aee7e8d1a7f87897f3e61eef02c9cbc92332a14679", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-19.html"}, "8d67b4d2-3a1f-44eb-8833-57deaecf65db": {"doc_hash": "5a467ea2e73bf678c6e6fe896286fae87bcc284d781bef7821df9048d8b5e27a", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-26.html"}, "75952d41-8f05-41a6-9a17-8a13e46d162e": {"doc_hash": "d4a2af03a95da57528710dcd2afefa1b557fba9671ed48cf2098e986ffc365fd", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-02.html"}, "3d1d146f-234d-4722-9a44-7cae1d22ebeb": {"doc_hash": "118af4c2be01bc473991a015420ced0d497be733974540bd765ad5f1bb0c6e8e", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-09.html"}, "da7a008c-194b-4127-8013-c0b28d2ac838": {"doc_hash": "074500b76f83120ae9b38232e30b7417dc88ce44cd7afc837950fa0a287aab9d", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-16.html"}, "969e0723-8b7a-400b-85b0-14e539747a3f": {"doc_hash": "ff0d877742a36443b6f0e618bfd5dbb70e188ff30626c0f7e7ccc35319e3d506", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-23.html"}, "f10e29e1-4b5a-4907-b71e-704f75f98031": {"doc_hash": "93d804947c40f2833b3cccd9ccd7f11918e69a72213ff0570af02b1a310fe9b9", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-30.html"}, "c0a1109d-4e46-4423-b755-80a3bb4af515": {"doc_hash": "18011188f01cebec68d29f551129923528ebb910cf0b91c69f6d97c633753630", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-07.html"}, "3f1ff09e-313f-4b07-9807-32dc5a2b9fc2": {"doc_hash": "a0dd1618b4384151d0f7984fffb6244ec6b778368326d25d395a519d69a768cf", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-14.html"}, "b086fcf1-f08b-4afc-8097-9471a965c1ce": {"doc_hash": "7bb42e500409c7887f956f05d97d6191e815f0183331392aeb691eb8d6e45556", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-21.html"}, "097d486c-14e7-46cd-819f-029c8fdedf6f": {"doc_hash": "6525f2d38702962e876a5a29a93feea7735760f653d0445895d4f0f16a90fab3", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-28.html"}, "124f07cf-be0c-49f5-bd03-4d3dc22ff9eb": {"doc_hash": "d93f58170c7aa12eed8eae20eb7a9e0004cd8ef96b9b1a36a71c4c5b62ad285b", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-04.html"}, "afd66fa0-1373-44e1-af17-88f0aeb7d1c6": {"doc_hash": "4ceb57863d8adf9e7d3fdfef5f43f2e991671ded3ede4fafcdf334808d8ab59f", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-11.html"}, "aad60b80-c0fb-4533-a3d9-887157de0f0a": {"doc_hash": "aac442f8454b512b0965407a77090b51949c4b539cee6eafd44bc7365be52322", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-18.html"}, "c70e20ef-07ef-40b0-b508-debf9e34f01c": {"doc_hash": "e5a67f2c1c93938bf05a8353f3ce668a06b9131a2126ce15323f6d361aa692a6", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-25.html"}, "07f91239-eab0-4b77-9d44-83bbfed20201": {"doc_hash": "3c9828f921ffc7956ebbfa63ed51597f6e8d0ac70337307514241cf298cf8103", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-02.html"}, "87813008-0ec9-4f71-b345-d58f4909ecd6": {"doc_hash": "e18bc4607460bdcb38bd3d9039ffa18d05b272392e8c5e49e7a7385d07598b91", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-09.html"}, "442318f5-bd6e-4b42-8c8d-905dde904aca": {"doc_hash": "b21e17687508649130224fc72dc668600d3deaa85a3b538408e4168298d1744d", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-16.html"}, "54c4d011-d894-42de-8b06-3f68bfdfae0a": {"doc_hash": "5d9ddf5ca369dd6e5421a67eca4157250598d724445fff0c637512a779d31954", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.html"}, "e550df24-500a-47da-871e-45c9c237c6c5": {"doc_hash": "d5e2bcc24c62eca3d654ff1ef70865903fc66ef07294cbfa59a58a77b896c1df", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.html"}, "7122d6bc-8686-4963-8965-c45b7dcc93da": {"doc_hash": "de1542e64ac1399a5cac2dc0bb7140de627b0bb6533cf96ca5b6aa7b37812c9d", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-turns-1-f69dcdd45fe3.html"}, "4f20eb78-358b-4525-950a-d014c9b5a4e4": {"doc_hash": "238760b151328e20bd98fffe6975a48efa562ee292f1c44da5cf5006c3424fee", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-update-07-10-2023-4ceebdab96cb.html"}, "7a7ff547-14ef-496b-a8e6-4ed239047a07": {"doc_hash": "d0e000e29297bca5ec657fbf31a84e140a158441201edddbe85fc0f83db7e85f", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-update-08-01-2023-185514d9b897.html"}, "5e35d539-6390-48bb-9205-e3b17e1cd9e0": {"doc_hash": "d7639cd360a2a0d2b03142c8a1b2941bdecc73f0d06e4fb5ceac2adc3149ceef", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-update-09-03-2023-4a7c21c0f60b.html"}, "3285e0fd-d0b5-48f4-b79e-57eb2c3e4b26": {"doc_hash": "88ee26fb46534d0a05e63c75e5fb126d80e47a54d9f9b518b4a4b4e1f048b171", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-update-20-09-2023-86ed66f78bac.html"}, "9bb11259-1760-43ad-b6f5-00a7d237f2fb": {"doc_hash": "e46357889da460215203207d2189bbe166cc9c4fde8def7b78bebcefdfe11d4d", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-update-2023-10-10-3718a3d19fb9.html"}, "e10a12c9-04f9-4687-8409-d2b28ecf0a17": {"doc_hash": "fd7c5d40578a762ba73b8d71b83b8a98e7f8d775b6728826e7fb9f760603bc6e", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-update-6-26-2023-ed30a9d45f84.html"}, "081baa8f-e7a1-4804-b75e-e6c7702709b3": {"doc_hash": "cd7717ae3895039415cf4b4638766a877c0cad968690fc0dae71c73dbf4a7f83", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-v0-10-838e735948f8.html"}, "a382cabf-7150-4c27-8e29-9158df4d8cd8": {"doc_hash": "d77fbddf7da6e309b9ed444454ec0b701230eacd5061076791934d7246257ac6", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-vectara-7a3889cd34cb.html"}, "dd19d281-9b4e-442f-8f4f-c9a6007f3ff6": {"doc_hash": "8d7894f64d58a16702e7e002bf7fd1d78c613beeda0852a1a4c9736fd102860d", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.html"}, "6530c364-f6dc-4b04-9631-9c18b4241caf": {"doc_hash": "0c54893a267f99f5a83509f18f5989d21de44d90854e081845c8f70e5dc73c7e", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.html"}, "9260d03a-a64d-44d8-837a-371c97930488": {"doc_hash": "2896609dc4f25aa89738c5a13c2370fcb736ff9f9b14b2640dc8ae7fe0ed3005", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.html"}, "ec91c3cb-204a-4184-b27c-60145bb5ea4c": {"doc_hash": "ce3f82545c8c1c03b31ebc7b5ad8cf41a8a51dcc031c9e9083e96d4c83ae5734", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/multi-modal-rag-621de7525fea.html"}, "713febfb-2255-496c-9648-087c01d69fcf": {"doc_hash": "3cfd2719559343ceb8d8d5cdec5d93a6b0d72886cc71cc07a422d0bfea624bd7", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/multimodal-rag-building-ainimal-go-fecf8404ed97.html"}, "1b398c01-24b6-4aa4-afda-d1b0fab6dd83": {"doc_hash": "37fac0b6236dba640de65d234f1f33bfef25d0088fb896f51fac0c496c708c1f", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.html"}, "7516225c-ac9f-4846-b9df-7b9fd66a796e": {"doc_hash": "18bd1bad1d900245f9db7b8f7f3ffc62181f1364ae9b19994628bf91d6969c98", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.html"}, "bdb955cb-3791-4a33-bb8b-9b49ae4dc880": {"doc_hash": "7d4837fe6ad8940ac6d488eec0c0cea6f0d240841ec4a18ff922cbeecd1f798c", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.html"}, "003fa2ae-c366-4f7e-b6d1-2b03b19a8c23": {"doc_hash": "767b0a75ebafb012727f9a9cecd8b7d1d50495a12c3bd42a8ab09ea8e31144c8", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/nvidia-research-rag-with-long-context-llms-7d94d40090c4.html"}, "097b6b16-d85c-42fc-863c-cf2d6a082ee2": {"doc_hash": "306141735c6ab2b8b22460bcfa0b1479545a0d834c2e81a28e04cbe55493bc3d", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/one-click-open-source-rag-observability-with-langfuse.html"}, "79033d51-6e72-4407-bcd3-8a1c1ea243f3": {"doc_hash": "ad922fcff86a51427df804c6f04bc95656ac8a1ce91246db2d7b73006ca83eb0", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/openai-cookbook-evaluating-rag-systems-fe393c61fb93.html"}, "4c78b9be-10aa-4203-b3a5-95c6452201a0": {"doc_hash": "ada1ea3823e8b1eea171bdbe41571eef9329e6d5162bc17a0a4e646fa6898e1f", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/pii-detector-hacking-privacy-in-rag.html"}, "c990777c-dce7-4e5b-bb02-54cdd9180314": {"doc_hash": "ba7b29d91fdfbca05167ffd9946cbf53ccd7de642e27e69f742f288543360877", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.html"}, "2716650b-a624-4ddc-8de6-9664b0a43286": {"doc_hash": "d1a9234ec7778da8040ec6bb6384e61440d6cba8e2115a0843c084b990ac0e36", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.html"}, "3a8628c5-14f9-493f-83fc-5ec0da7827f1": {"doc_hash": "3810b4651701bccb8a56535c0f5f1c7109b2d77e0f4cec4410edd2400df0c4cc", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.html"}, "4e63a095-cf8f-4bfc-922d-5ae7cee288a2": {"doc_hash": "74b7e648e3fbebf1f18c5a4b1ddc8697cbb68c367a18cfb28e15372522bf3e8e", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/retrieving-privacy-safe-documents-over-a-network.html"}, "c8cee370-e834-4710-af6e-1c4d640d1d22": {"doc_hash": "01a491fa5fa73c9003a56148eccad74c405764a25edcf66581029fda2f1c4657", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.html"}, "28df59df-30a7-4aee-90b7-cb990c758c92": {"doc_hash": "513e00802578648c81670278c5d7de37e3b1989516bf3463e056f00096f3a0bb", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.html"}, "7e6bcfe0-e8f2-47bc-975f-c4e0a1ec6170": {"doc_hash": "6304b8685a38bf7dc2445dabc9722cadca5e8949068df28470a6fd423133c7ba", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.html"}, "c7312d12-c832-4281-a316-83f147e4a5b0": {"doc_hash": "6bbda7b2deba764d9b73decbfd297d674af75212d94acecda02e65b8d8505956", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.html"}, "8e3da68c-661c-4d88-80dd-4a6b639985f2": {"doc_hash": "45fcddf303d0b04a2ce3a776b146b87e283dc9f9d21d861460a1c1289987a170", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d.html"}, "d713bde2-c216-45ca-a50c-4d82447ddcc9": {"doc_hash": "1e5cb27caab5e4a5d715397bbdac47b8633bab0624ba9206a7c61989b5a5d37e", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/simplify-your-rag-application-architecture-with-llamaindex-postgresml.html"}, "ed5e1673-d674-4279-8727-32d03c128bf7": {"doc_hash": "4f8c980a7a01c13d9563b691cfada93234132749b7e6f9e9574985eb840650db", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.html"}, "d49bc2cf-8eb4-4a55-8753-f055713a20c0": {"doc_hash": "9bf91c9fe908c45ea00c400ed261a1624b1990191f6ed22d49321647b6be000b", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.html"}, "d3bea69b-d7cf-4192-b5f1-bda08ed5f075": {"doc_hash": "2ddaab7c98e0f421dcd20e4c2a940325fe7417ef2b03887ab2113e760a8cc183", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.html"}, "dca7137f-1b4b-49cf-b8e1-365b6ee4f349": {"doc_hash": "9fc5017d93e7bd4d815c022457a7b3706916536e84dfe7fed84d83836c686b0d", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.html"}, "8bf758c5-7993-4480-86e4-31efc1b5b2e1": {"doc_hash": "f7d3d6553c9fbb6cd1e5057b5b424b5b56db348f4c62f5a12f4c8aaa03bd6ad2", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/the-latest-updates-to-llamacloud.html"}, "7aeff5aa-425a-4a5d-b684-c669ed5b53ad": {"doc_hash": "895e60d09a3d0e334dba9764fd5e3dc350e68da7cb1c32aeb232d4ac87c1edbc", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.html"}, "68cacd2a-53b3-4748-87b4-3daca98a318b": {"doc_hash": "656a25c314864235907370f41618f06cc8027dedaa12d258b961c9391db875af", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.html"}, "907c913e-6863-4e67-ba21-99654c410ecd": {"doc_hash": "611cebc28b1fb0e26f20ffc655432cb6b639f13680f402218241e1161dcc30a2", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/towards-long-context-rag.html"}, "2e713d08-09cb-465a-84ad-f47c210ab63d": {"doc_hash": "34357c849f720799cff402c96d1a9043879f703442d7c932c8094a7566dbb482", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.html"}, "9a277129-75eb-4cf1-8762-14f409a7daba": {"doc_hash": "eb931385f230e8e502538d909a3b73980f9721d2e8a95832831d6a04ffa36784", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.html"}, "bb7b2a61-3cdd-4d17-adcf-7f7dee63829f": {"doc_hash": "ea022236498166584eaf0dc8e2b3b3e3bc37ed3b58843f86e44674e84671dd24", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/unlocking-the-3rd-dimension-for-generative-ai-part-1.html"}, "0a2f8bfb-56cc-4800-b16e-bddb54595e21": {"doc_hash": "80144e3ec84a3767c14983d4b3e4e0ae7efe6486e903dab51a9e79f65bfc2550", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.html"}, "b6de8c2f-dc98-46a4-be20-8f3960df6968": {"doc_hash": "4ae7259c720d577b298b0200f5415a25a8c612288c7f65f318728d2406777454", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/using-llms-for-retrieval-and-reranking-23cf2d3a14b6.html"}, "9c957df3-76e6-4ecd-9cb6-7c86f1819cdc": {"doc_hash": "fc75ede2efd1d48db55b34402a5c401ecad8a0d75c4ac16154498ecfab4caecb", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/vellum-llamaindex-integration-58b476a1e33f.html"}, "5d4d5ede-c2ae-41d9-b9a9-8b0394f363af": {"doc_hash": "7b98bea72270f2380f0e5f476f77d97313ec8bdaf19e63fb73e12c1d32770bcf", "ref_doc_id": "/workspace/projects/LlamindexHelper/data/zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc.html"}}, "docstore/data": {"d7be752a-0e06-424d-8aee-3a98e5e85a5c": {"__data__": {"id_": "d7be752a-0e06-424d-8aee-3a98e5e85a5c", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.html", "file_name": "a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.html", "file_type": "text/html", "file_size": 24708, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.html", "file_name": "a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.html", "file_type": "text/html", "file_size": 24708, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "e622a0a8d1532d5ca3d35838e53ef4c193b6d4170eba7de5c72e033171779acc", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  It\u2019s the start of a new year and perhaps you\u2019re looking to break into the RAG scene by building your very first RAG system. Or, maybe you\u2019ve built Basic RAG systems and are now looking to enhance them to something more advanced in order to better handle your user\u2019s queries and data structures.\n </p>\n <p>\n  In either case, knowing where or how to begin may be a challenge in and of itself! If that\u2019s true, then hopefully this blog post points you in the right direction for your next steps, and moreover, provides for you a mental model for you to anchor your decisions when building advanced RAG systems.\n </p>\n <p>\n  The RAG cheat sheet shared above was greatly inspired by a recent RAG survey paper (\n  <a href=\"https://arxiv.org/pdf/2312.10997.pdf\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   \u201cRetrieval-Augmented Generation for Large Language Models: A Survey\u201d Gao, Yunfan, et al. 2023\n  </a>\n  ).\n </p>\n <h1>\n  Basic RAG\n </h1>\n <p>\n  Mainstream RAG as defined today involves retrieving documents from an external knowledge database and passing these along with the user\u2019s query to an LLM for response generation. In other words, RAG involves a Retrieval component, an External Knowledge database and a Generation component.\n </p>\n <p>\n  <strong>\n   LlamaIndex Basic RAG Recipe:\n  </strong>\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"503a\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> SimpleDirectoryReader, VectorStoreIndex\n\n<span class=\"hljs-comment\"># load data</span>\ndocuments = SimpleDirectoryReader(input_dir=<span class=\"hljs-string\">\"...\"</span>).load_data()\n\n<span class=\"hljs-comment\"># build VectorStoreIndex that takes care of chunking documents</span>\n<span class=\"hljs-comment\"># and encoding chunks to embeddings for future retrieval</span>\nindex = VectorStoreIndex.from_documents(documents=documents)\n\n<span class=\"hljs-comment\"># The QueryEngine class is equipped with the generator</span>\n<span class=\"hljs-comment\"># and facilitates the retrieval and generation steps</span>\nquery_engine = index.as_query_engine()\n\n<span class=\"hljs-comment\"># Use your Default RAG</span>\nresponse = query_engine.query(<span class=\"hljs-string\">\"A user's query\"</span>)</span></pre>\n <h1>\n  Success Requirements for RAG\n </h1>\n <p>\n  In order for a RAG system to be deemed as a success (in the sense of providing useful and relevant answers to user questions), there are really only two high level requirements:\n </p>\n <ol>\n  <li>\n   Retrieval must be able to find the most relevant documents to a user query.\n  </li>\n  <li>\n   Generation must be able to make good use of the retrieved documents to sufficiently answer the user query.\n  </li>\n </ol>\n <h1>\n  Advanced RAG\n </h1>\n <p>\n  With the success requirements defined, we can then say that building advanced RAG is really about the application of more sophisticated techniques and strategies (to the Retrieval or Generation components) to ensure that they are ultimately met. Furthermore, we can categorize a sophisticated technique as either one that addresses one of the two high-level success requirements independent (more or less) of the other, or one that addresses both of these requirements simultaneously.\n </p>\n <h1>\n  Advanced techniques for Retrieval must be able to find the most relevant documents to a user query\n </h1>\n <p>\n  Below we briefly describe a couple of the more sophisticated techniques to help achieve the first success requirement.\n </p>\n <ol>\n  <li>\n   <strong>\n    Chunk-Size Optimization:\n   </strong>\n   Since LLMs are restricted by context length, it is necessary to chunk documents when building the External Knowledge database. Chunks that are too big or too small can pose problems for the Generation component leading to in accurate responses.\n  </li>\n </ol>\n <p>\n  <strong>\n   LlamaIndex Chunk Size Optimization Recipe\n  </strong>\n  (\n  <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/param_optimizer/param_optimizer.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   notebook guide\n  </a>\n  )\n  <strong>\n   :\n  </strong>\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"0631\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> ServiceContext\n<span class=\"hljs-keyword\">from</span> llama_index.param_tuner.base <span class=\"hljs-keyword\">import</span> ParamTuner, RunResult\n<span class=\"hljs-keyword\">from</span> llama_index.evaluation <span class=\"hljs-keyword\">import</span> SemanticSimilarityEvaluator, BatchEvalRunner\n\n<span class=\"hljs-comment\">### Recipe</span>\n<span class=\"hljs-comment\">### Perform hyperparameter tuning as in traditional ML via grid-search</span>\n<span class=\"hljs-comment\">### 1. Define an objective function that ranks different parameter combos</span>\n<span class=\"hljs-comment\">### 2. Build ParamTuner object</span>\n<span class=\"hljs-comment\">### 3. Execute hyperparameter tuning with ParamTuner.tune()</span>\n\n<span class=\"hljs-comment\"># 1. Define objective function</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">objective_function</span>(<span class=\"hljs-params\">params_dict</span>):\n    chunk_size = params_dict[<span class=\"hljs-string\">\"chunk_size\"</span>]\n    docs = params_dict[<span class=\"hljs-string\">\"docs\"</span>]\n    top_k = params_dict[<span class=\"hljs-string\">\"top_k\"</span>]\n    eval_qs = params_dict[<span class=\"hljs-string\">\"eval_qs\"</span>]\n    ref_response_strs = params_dict[<span class=\"hljs-string\">\"ref_response_strs\"</span>]\n\n    <span class=\"hljs-comment\"># build RAG pipeline</span>\n    index = _build_index(chunk_size, docs)  <span class=\"hljs-comment\"># helper function not shown here</span>\n    query_engine = index.as_query_engine(similarity_top_k=top_k)\n  \n    <span class=\"hljs-comment\"># perform inference with RAG pipeline on a provided questions `eval_qs`</span>\n    pred_response_objs = get_responses(\n        eval_qs, query_engine, show_progress=<span class=\"hljs-literal\">True</span>\n    )\n\n    <span class=\"hljs-comment\"># perform evaluations of predictions by comparing them to reference</span>\n    <span class=\"hljs-comment\"># responses `ref_response_strs`</span>\n    evaluator = SemanticSimilarityEvaluator(...)\n    eval_batch_runner = BatchEvalRunner(\n        {<span class=\"hljs-string\">\"semantic_similarity\"</span>: evaluator}, workers=<span class=\"hljs-number\">2</span>, show_progress=<span class=\"hljs-literal\">True</span>\n    )\n    eval_results = eval_batch_runner.evaluate_responses(\n        eval_qs, responses=pred_response_objs, reference=ref_response_strs\n    )\n\n    <span class=\"hljs-comment\"># get semantic similarity metric</span>\n    mean_score = np.array(\n        [r.score <span class=\"hljs-keyword\">for</span> r <span class=\"hljs-keyword\">in</span> eval_results[<span class=\"hljs-string\">\"semantic_similarity\"</span>]]\n    ).mean()\n\n    <span class=\"hljs-keyword\">return</span> RunResult(score=mean_score, params=params_dict)\n\n<span class=\"hljs-comment\"># 2. Build ParamTuner object</span>\nparam_dict = {<span class=\"hljs-string\">\"chunk_size\"</span>: [<span class=\"hljs-number\">256</span>, <span class=\"hljs-number\">512</span>, <span class=\"hljs-number\">1024</span>]} <span class=\"hljs-comment\"># params/values to search over</span>\nfixed_param_dict = { <span class=\"hljs-comment\"># fixed hyperparams</span>\n  <span class=\"hljs-string\">\"top_k\"</span>: <span class=\"hljs-number\">2</span>,\n    <span class=\"hljs-string\">\"docs\"</span>: docs,\n    <span class=\"hljs-string\">\"eval_qs\"</span>: eval_qs[:<span class=\"hljs-number\">10</span>],\n    <span class=\"hljs-string\">\"ref_response_strs\"</span>: ref_response_strs[:<span class=\"hljs-number\">10</span>],\n}\nparam_tuner = ParamTuner(\n    param_fn=objective_function,\n    param_dict=param_dict,\n    fixed_param_dict=fixed_param_dict,\n    show_progress=<span class=\"hljs-literal\">True</span>,\n)\n\n<span class=\"hljs-comment\"># 3. Execute hyperparameter search</span>\nresults = param_tuner.tune()\nbest_result = results.best_run_result\nbest_chunk_size = results.best_run_result.params[<span class=\"hljs-string\">\"chunk_size\"</span>]</span></pre>\n <p>\n  <strong>\n   2. Structured External Knowledge:\n  </strong>\n  In complex scenarios, it may be necessary to build your external knowledge with a bit more structure than the basic vector index so as to permit recursive retrievals or routed retrieval when dealing with sensibly separated external knowledge sources.\n </p>\n <p>\n  <strong>\n   LlamaIndex Recursive Retrieval Recipe\n  </strong>\n  (\n  <a href=\"https://docs.llamaindex.ai/en/stable/examples/retrievers/recursive_retriever_nodes.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   notebook guide\n  </a>\n  )\n  <strong>\n   :\n  </strong>\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"4a9d\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> SimpleDirectoryReader, VectorStoreIndex\n<span class=\"hljs-keyword\">from</span> llama_index.node_parser <span class=\"hljs-keyword\">import</span> SentenceSplitter\n<span class=\"hljs-keyword\">from</span> llama_index.schema <span class=\"hljs-keyword\">import</span> IndexNode\n\n<span class=\"hljs-comment\">### Recipe</span>\n<span class=\"hljs-comment\">### Build a recursive retriever that retrieves using small chunks</span>\n<span class=\"hljs-comment\">### but passes associated larger chunks to the generation stage</span>\n\n<span class=\"hljs-comment\"># load data</span>\ndocuments = SimpleDirectoryReader(\n  input_file=<span class=\"hljs-string\">\"some_data_path/llama2.pdf\"</span>\n).load_data()\n\n<span class=\"hljs-comment\"># build parent chunks via NodeParser</span>\nnode_parser = SentenceSplitter(chunk_size=<span class=\"hljs-number\">1024</span>)\nbase_nodes = node_parser.get_nodes_from_documents(documents)\n\n<span class=\"hljs-comment\"># define smaller child chunks</span>\nsub_chunk_sizes = [<span class=\"hljs-number\">256</span>, <span class=\"hljs-number\">512</span>]\nsub_node_parsers = [\n    SentenceSplitter(chunk_size=c, chunk_overlap=<span class=\"hljs-number\">20</span>) <span class=\"hljs-keyword\">for</span> c <span class=\"hljs-keyword\">in</span> sub_chunk_sizes\n]\nall_nodes = []\n<span class=\"hljs-keyword\">for</span> base_node <span class=\"hljs-keyword\">in</span> base_nodes:\n    <span class=\"hljs-keyword\">for</span> n <span class=\"hljs-keyword\">in</span> sub_node_parsers:\n        sub_nodes = n.get_nodes_from_documents([base_node])\n        sub_inodes = [\n            IndexNode.from_text_node(sn, base_node.node_id) <span class=\"hljs-keyword\">for</span> sn <span class=\"hljs-keyword\">in</span> sub_nodes\n        ]\n        all_nodes.extend(sub_inodes)\n    <span class=\"hljs-comment\"># also add original node to node</span>\n    original_node = IndexNode.from_text_node(base_node, base_node.node_id)\n    all_nodes.append(original_node)\n\n<span class=\"hljs-comment\"># define a VectorStoreIndex with all of the nodes</span>\nvector_index_chunk = VectorStoreIndex(\n    all_nodes, service_context=service_context\n)\nvector_retriever_chunk = vector_index_chunk.as_retriever(similarity_top_k=<span class=\"hljs-number\">2</span>)\n\n<span class=\"hljs-comment\"># build RecursiveRetriever</span>\nall_nodes_dict = {n.node_id: n <span class=\"hljs-keyword\">for</span> n <span class=\"hljs-keyword\">in</span> all_nodes}\nretriever_chunk = RecursiveRetriever(\n    <span class=\"hljs-string\">\"vector\"</span>,\n    retriever_dict={<span class=\"hljs-string\">\"vector\"</span>: vector_retriever_chunk},\n    node_dict=all_nodes_dict,\n    verbose=<span class=\"hljs-literal\">True</span>,\n)\n\n<span class=\"hljs-comment\"># build RetrieverQueryEngine using recursive_retriever</span>\nquery_engine_chunk = RetrieverQueryEngine.from_args(\n    retriever_chunk, service_context=service_context\n)\n\n<span class=\"hljs-comment\"># perform inference with advanced RAG (i.e. query engine)</span>\nresponse = query_engine_chunk.query(\n    <span class=\"hljs-string\">\"Can you tell me about the key concepts for safety finetuning\"</span>\n)</span></pre>\n <p>\n  <strong>\n   Other useful links\n  </strong>\n </p>\n <p>\n  We have several of guides demonstrating the application of other advanced techniques to help ensure accurate retrieval in complex cases. Here are links to a select few of them:\n </p>\n <ol>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/query_engine/knowledge_graph_rag_query_engine.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Building External Knowledge using Knowledge Graphs\n   </a>\n  </li>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/vector_stores/elasticsearch_auto_retriever.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Performing Mixed Retrieval with Auto Retrievers\n   </a>\n  </li>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/retrievers/simple_fusion.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Building Fusion Retrievers\n   </a>\n  </li>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/finetuning/embeddings/finetune_embedding.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Fine-tuning Embedding Models used in Retrieval\n   </a>\n  </li>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/query_transformations/HyDEQueryTransformDemo.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Transforming Query Embeddings (HyDE)\n   </a>\n  </li>\n </ol>\n <h1>\n  Advanced techniques for Generation must be able to make good use of the retrieved documents\n </h1>\n <p>\n  Similar to previous section, we provide a couple of examples of the sophisticated techniques under this category, which can be described as ensuring that the retrieved documents are well aligned to the LLM of the Generator.\n </p>\n <ol>\n  <li>\n   <strong>\n    Information Compression:\n   </strong>\n   Not only are LLMs are restricted by context length, but there can be response degradation if the retrieved documents carry too much noise (i.e. irrelevant information).\n  </li>\n </ol>\n <p>\n  <strong>\n   LlamaIndex Information Compression Recipe\n  </strong>\n  (\n  <a href=\"https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/LongLLMLingua.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   notebook guide\n  </a>\n  )\n  <strong>\n   :\n  </strong>\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"f078\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> SimpleDirectoryReader, VectorStoreIndex\n<span class=\"hljs-keyword\">from</span> llama_index.query_engine <span class=\"hljs-keyword\">import</span> RetrieverQueryEngine\n<span class=\"hljs-keyword\">from</span> llama_index.postprocessor <span class=\"hljs-keyword\">import</span> LongLLMLinguaPostprocessor\n\n<span class=\"hljs-comment\">### Recipe</span>\n<span class=\"hljs-comment\">### Define a Postprocessor object, here LongLLMLinguaPostprocessor</span>\n<span class=\"hljs-comment\">### Build QueryEngine that uses this Postprocessor on retrieved docs</span>\n\n<span class=\"hljs-comment\"># Define Postprocessor</span>\nnode_postprocessor = LongLLMLinguaPostprocessor(\n    instruction_str=<span class=\"hljs-string\">\"Given the context, please answer the final question\"</span>,\n    target_token=<span class=\"hljs-number\">300</span>,\n    rank_method=<span class=\"hljs-string\">\"longllmlingua\"</span>,\n    additional_compress_kwargs={\n        <span class=\"hljs-string\">\"condition_compare\"</span>: <span class=\"hljs-literal\">True</span>,\n        <span class=\"hljs-string\">\"condition_in_question\"</span>: <span class=\"hljs-string\">\"after\"</span>,\n        <span class=\"hljs-string\">\"context_budget\"</span>: <span class=\"hljs-string\">\"+100\"</span>,\n        <span class=\"hljs-string\">\"reorder_context\"</span>: <span class=\"hljs-string\">\"sort\"</span>,  <span class=\"hljs-comment\"># enable document reorder</span>\n    },\n)\n\n<span class=\"hljs-comment\"># Define VectorStoreIndex</span>\ndocuments = SimpleDirectoryReader(input_dir=<span class=\"hljs-string\">\"...\"</span>).load_data()\nindex = VectorStoreIndex.from_documents(documents)\n\n<span class=\"hljs-comment\"># Define QueryEngine</span>\nretriever = index.as_retriever(similarity_top_k=<span class=\"hljs-number\">2</span>)\nretriever_query_engine = RetrieverQueryEngine.from_args(\n    retriever, node_postprocessors=[node_postprocessor]\n)\n\n<span class=\"hljs-comment\"># Used your advanced RAG</span>\nresponse = retriever_query_engine.query(<span class=\"hljs-string\">\"A user query\"</span>)</span></pre>\n <p>\n  <strong>\n   2. Result Re-Rank:\n  </strong>\n  LLMs suffer from the so-called \u201cLost in the Middle\u201d phenomena which stipulates that LLMs focus on the extreme ends of the prompts. In light of this, it is beneficial to re-rank retrieved documents before passing them off to the Generation component.\n </p>\n <p>\n  <strong>\n   LlamaIndex Re-Ranking For Better Generation Recipe\n  </strong>\n  (\n  <a href=\"https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/CohereRerank.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   notebook guide\n  </a>\n  )\n  <strong>\n   :\n  </strong>\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"28c3\"><span class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> SimpleDirectoryReader, VectorStoreIndex\n<span class=\"hljs-keyword\">from</span> llama_index.postprocessor.cohere_rerank <span class=\"hljs-keyword\">import</span> CohereRerank\n<span class=\"hljs-keyword\">from</span> llama_index.postprocessor <span class=\"hljs-keyword\">import</span> LongLLMLinguaPostprocessor\n\n<span class=\"hljs-comment\">### Recipe</span>\n<span class=\"hljs-comment\">### Define a Postprocessor object, here CohereRerank</span>\n<span class=\"hljs-comment\">### Build QueryEngine that uses this Postprocessor on retrieved docs</span>\n\n<span class=\"hljs-comment\"># Build CohereRerank post retrieval processor</span>\napi_key = os.environ[<span class=\"hljs-string\">\"COHERE_API_KEY\"</span>]\ncohere_rerank = CohereRerank(api_key=api_key, top_n=<span class=\"hljs-number\">2</span>)\n\n<span class=\"hljs-comment\"># Build QueryEngine (RAG) using the post processor</span>\ndocuments = SimpleDirectoryReader(<span class=\"hljs-string\">\"./data/paul_graham/\"</span>).load_data()\nindex = VectorStoreIndex.from_documents(documents=documents)\nquery_engine = index.as_query_engine(\n    similarity_top_k=<span class=\"hljs-number\">10</span>,\n    node_postprocessors=[cohere_rerank],\n)\n\n<span class=\"hljs-comment\"># Use your advanced RAG</span>\nresponse = query_engine.query(\n    <span class=\"hljs-string\">\"What did Sam Altman do in this essay?\"</span>\n)</span></pre>\n <h1>\n  Advanced techniques for simultaneously addressing Retrieval and Generation success requirements\n </h1>\n <p>\n  In this sub section, we consider sophisticated methods that use the synergy of retrieval and generation in order to achieve both better retrieval as well as more accurate generated responses to user queries).\n </p>\n <ol>\n  <li>\n   <strong>\n    Generator-Enhanced Retrieval:\n   </strong>\n   These techniques make use of the LLM\u2019s inherent reasoning abilities to refine the user query before retrieval is performed so as to better indicate what exactly it requires to provide a useful response.\n  </li>\n </ol>\n <p>\n  <strong>\n   LlamaIndex Generator-Enhanced Retrieval Recipe\n  </strong>\n  (\n  <a href=\"https://docs.llamaindex.ai/en/stable/examples/query_engine/flare_query_engine.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   notebook guide\n  </a>\n  )\n  <strong>\n   :\n  </strong>\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"ac1d\"><span class=\"hljs-keyword\">from</span> llama_index.llms <span class=\"hljs-keyword\">import</span> OpenAI\n<span class=\"hljs-keyword\">from</span> llama_index.query_engine <span class=\"hljs-keyword\">import</span> FLAREInstructQueryEngine\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> (\n    VectorStoreIndex,\n    SimpleDirectoryReader,\n    ServiceContext,\n)\n<span class=\"hljs-comment\">### Recipe</span>\n<span class=\"hljs-comment\">### Build a FLAREInstructQueryEngine which has the generator LLM play</span>\n<span class=\"hljs-comment\">### a more active role in retrieval by prompting it to elicit retrieval</span>\n<span class=\"hljs-comment\">### instructions on what it needs to answer the user query.</span>\n\n<span class=\"hljs-comment\"># Build FLAREInstructQueryEngine</span>\ndocuments = SimpleDirectoryReader(<span class=\"hljs-string\">\"./data/paul_graham\"</span>).load_data()\nindex = VectorStoreIndex.from_documents(documents)\nindex_query_engine = index.as_query_engine(similarity_top_k=<span class=\"hljs-number\">2</span>)\nservice_context = ServiceContext.from_defaults(llm=OpenAI(model=<span class=\"hljs-string\">\"gpt-4\"</span>))\nflare_query_engine = FLAREInstructQueryEngine(\n    query_engine=index_query_engine,\n    service_context=service_context,\n    max_iterations=<span class=\"hljs-number\">7</span>,\n    verbose=<span class=\"hljs-literal\">True</span>,\n)\n\n<span class=\"hljs-comment\"># Use your advanced RAG</span>\nresponse = flare_query_engine.query(\n    <span class=\"hljs-string\">\"Can you tell me about the author's trajectory in the startup world?\"</span>\n)</span></pre>\n <p>\n  <strong>\n   2. Iterative Retrieval-Generator RAG:\n  </strong>\n  For some complex cases, multi-step reasoning may be required to provide a useful and relevant answer to the user query.\n </p>\n <p>\n  <strong>\n   LlamaIndex Iterative Retrieval-Generator Recipe (\n  </strong>\n  <a href=\"https://docs.llamaindex.ai/en/stable/examples/evaluation/RetryQuery.html#retry-query-engine\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   notebook guide\n  </a>\n  <strong>\n   ):\n  </strong>\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"cd1c\"><span class=\"hljs-keyword\">from</span> llama_index.query_engine <span class=\"hljs-keyword\">import</span> RetryQueryEngine\n<span class=\"hljs-keyword\">from</span> llama_index.evaluation <span class=\"hljs-keyword\">import</span> RelevancyEvaluator\n\n<span class=\"hljs-comment\">### Recipe</span>\n<span class=\"hljs-comment\">### Build a RetryQueryEngine which performs retrieval-generation cycles</span>\n<span class=\"hljs-comment\">### until it either achieves a passing evaluation or a max number of</span>\n<span class=\"hljs-comment\">### cycles has been reached</span>\n\n<span class=\"hljs-comment\"># Build RetryQueryEngine</span>\ndocuments = SimpleDirectoryReader(<span class=\"hljs-string\">\"./data/paul_graham\"</span>).load_data()\nindex = VectorStoreIndex.from_documents(documents)\nbase_query_engine = index.as_query_engine()\nquery_response_evaluator = RelevancyEvaluator() <span class=\"hljs-comment\"># evaluator to critique </span>\n                                                <span class=\"hljs-comment\"># retrieval-generation cycles</span>\nretry_query_engine = RetryQueryEngine(\n    base_query_engine, query_response_evaluator\n)\n\n<span class=\"hljs-comment\"># Use your advanced rag</span>\nretry_response = retry_query_engine.query(<span class=\"hljs-string\">\"A user query\"</span>)</span></pre>\n <h1>\n  Measurement Aspects of RAG\n </h1>\n <p>\n  Evaluating RAG systems are, of course, of utmost importance. In their survey paper, Gao, Yunfan et al. indicate 7 measurement aspects as seen in the top-right portion of the attached RAG cheat sheet. The llama-index library consists of several evaluation abstractions as well as integrations to RAGAs in order to help builders gain an understanding of the level to which their RAG system achieves the success requirements through the lens of these measurement aspects. Below, we list a select few of the evaluation notebook guides.\n </p>\n <ol>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/evaluation/answer_and_context_relevancy.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Answer Relevancy and Context Relevancy\n   </a>\n  </li>\n  <li>\n   <a href=\"https://www.notion.so/LlamaIndex-Platform-0754edd9af1c4159bde12649c184c8ef?pvs=21\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Faithfulness\n   </a>\n  </li>\n  <li>\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/evaluation/retrieval/retriever_eval.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Retrieval Evaluation\n   </a>\n  </li>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/evaluation/batch_eval.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Batch Evaluations with BatchEvalRunner\n   </a>\n  </li>\n </ol>\n <h1>\n  You\u2019re Now Equipped To Do Advanced RAG\n </h1>\n <p>\n  After reading this blog post, we hope that you feel more equipped and confident to apply some of these sophisticated techniques for building Advanced RAG systems!\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 24683, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd4001fc-71fa-4139-a074-30a1e7623240": {"__data__": {"id_": "fd4001fc-71fa-4139-a074-30a1e7623240", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.html", "file_name": "a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.html", "file_type": "text/html", "file_size": 11715, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.html", "file_name": "a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.html", "file_type": "text/html", "file_size": 11715, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "10894293b5a982499949a7a0e6d8050e1fa0075765f31179217da413a4ba6825", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  In this blog post, we introduce a brand new LlamaIndex data structure: a Document Summary Index. We describe how it can help offer better retrieval performance compared to traditional semantic search, and also walk through an example.\n </p>\n <h1>\n  Background\n </h1>\n <p>\n  One of the core use cases of Large Language Models (LLMs) is question-answering over your own data. To do this, we pair the LLM with a \u201cretrieval\u201d model that can perform information retrieval over a knowledge corpus, and perform response synthesis over the retrieved texts using the LLM. This overall framework is called Retrieval-Augmented Generation.\n </p>\n <p>\n  Most users building LLM-powered QA systems today tend to do some form of the following:\n </p>\n <ol>\n  <li>\n   Take source documents, split each one into text chunks\n  </li>\n  <li>\n   Store text chunks in a vector db\n  </li>\n  <li>\n   During query-time, retrieve text chunks by embedding similarity and/or keyword filters.\n  </li>\n  <li>\n   Perform response synthesis\n  </li>\n </ol>\n <p>\n  For a variety of reasons, this approach provides limited retrieval performance.\n </p>\n <h2>\n  Limitations of Existing Approaches\n </h2>\n <p>\n  There are a few limitations of embedding retrieval using text chunks.\n </p>\n <ul>\n  <li>\n   <strong>\n    Text chunks lack global context.\n   </strong>\n   Oftentimes the question requires context beyond what is indexed in a specific chunk.\n  </li>\n  <li>\n   <strong>\n    Careful tuning of top-k / similarity score thresholds.\n   </strong>\n   Make the value too small and you\u2019ll miss context. Make the value too big and cost/latency might increase with more irrelevant context.\n  </li>\n  <li>\n   <strong>\n    Embeddings don\u2019t always select the most relevant context for a question.\n   </strong>\n   Embeddings are inherently determined separately between text and the context.\n  </li>\n </ul>\n <p>\n  Adding keyword filters are one way to enhance the retrieval results. But that comes with its own set of challenges. We would need to adequately determine the proper keywords for each document, either manually or through an NLP keyword extraction/topic tagging model. Also we would need to adequately infer the proper keywords from the query.\n </p>\n <h1>\n  Document Summary Index\n </h1>\n <figure>\n  <figcaption class=\"qn fe qo pz qa qp qq be b bf z dt\">\n   A diagram for the Document Summary Index\n  </figcaption>\n </figure>\n <p>\n  We propose a new index in\n  <a href=\"https://github.com/jerryjliu/llama_index\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaIndex\n  </a>\n  that will extract/index an\n  <strong>\n   unstructured text summary for each document\n  </strong>\n  . This index can help enhance retrieval performance beyond existing retrieval approaches. It helps to index more information than a single text chunk, and carries more semantic meaning than keyword tags. It also allows for a more flexible form of retrieval: we can do both LLM retrieval and embedding-based retrieval.\n </p>\n <h2>\n  How It Works\n </h2>\n <p>\n  During build-time, we ingest each document, and use a LLM to extract a summary from each document. We also split the document up into text chunks (nodes). Both the summary and the nodes are stored within our\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/how_to/storage.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Document Store\n  </a>\n  abstraction. We maintain a mapping from the summary to the source document/nodes.\n </p>\n <p>\n  During query-time, we retrieve relevant documents to the query based on their summaries, using the following approaches:\n </p>\n <ul>\n  <li>\n   <strong>\n    LLM-based Retrieval:\n   </strong>\n   We present sets of document summaries to the LLM, and ask the LLM to determine which documents are relevant + their relevance score.\n  </li>\n  <li>\n   <strong>\n    Embedding-based Retrieval:\n   </strong>\n   We retrieve relevant documents based on summary embedding similarity (with a top-k cutoff).\n  </li>\n </ul>\n <p>\n  Note that this approach of retrieval for document summaries (even with the embedding-based approach) is different than embedding-based retrieval over text chunks. The retrieval classes for the document summary index retrieve\n  <strong>\n   all nodes\n  </strong>\n  for any selected document, instead of returning relevant chunks at the node-level.\n </p>\n <p>\n  Storing summaries for a document also enables\n  <strong>\n   LLM-based retrieval\n  </strong>\n  . Instead of feeding the entire document to the LLM in the beginning, we can first have the LLM inspect the concise document summary to see if it\u2019s relevant to the query at all. This leverages the reasoning capabilities of LLM\u2019s which are more advanced than embedding-based lookup, but avoids the cost/latency of feeding the entire document to the LLM\n </p>\n <h2>\n  Additional Insights\n </h2>\n <p>\n  Document retrieval with summaries can be thought of as a \u201cmiddle ground\u201d between semantic search and brute-force summarization across all docs. We look up documents based on summary relevance with the given query, and then return all *nodes* corresponding to the retrieved docs.\n </p>\n <p>\n  Why should we do this? This retrieval method gives user more context than top-k over a text-chunk, by retrieving context at a document-level. But, it\u2019s also a more flexible/automatic approach than topic modeling; no more worrying about whether your text has the right keyword tags!\n </p>\n <h1>\n  Example\n </h1>\n <p>\n  Let\u2019s walk through an example that showcases the document summary index, over Wikipedia articles about different cities.\n </p>\n <p>\n  The rest of this guide showcases the relevant code snippets. You can find the\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/index_structs/doc_summary/DocSummary.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   full walkthrough here\n  </a>\n  (and here\u2019s the\n  <a href=\"https://github.com/jerryjliu/llama_index/blob/main/docs/examples/index_structs/doc_summary/DocSummary.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   notebook link\n  </a>\n  ).\n </p>\n <p>\n  We can build the\n  <code class=\"cw qs qt qu qv b\">\n   GPTDocumentSummaryIndex\n  </code>\n  over a set of documents, and pass in a\n  <code class=\"cw qs qt qu qv b\">\n   ResponseSynthesizer\n  </code>\n  object to synthesize summaries for the documents.\n </p>\n <pre><span class=\"qz nz gt qv b bf ra rb l rc rd\" id=\"1784\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> (\n    SimpleDirectoryReader,\n    LLMPredictor,\n    ServiceContext,\n    ResponseSynthesizer\n)\n<span class=\"hljs-keyword\">from</span> llama_index.indices.document_summary <span class=\"hljs-keyword\">import</span> GPTDocumentSummaryIndex\n<span class=\"hljs-keyword\">from</span> langchain.chat_models <span class=\"hljs-keyword\">import</span> ChatOpenAI\n\n<span class=\"hljs-comment\"># load docs, define service context</span>\n...\n\n<span class=\"hljs-comment\"># build the index</span>\nresponse_synthesizer = ResponseSynthesizer.from_args(response_mode=<span class=\"hljs-string\">\"tree_summarize\"</span>, use_async=<span class=\"hljs-literal\">True</span>)\ndoc_summary_index = GPTDocumentSummaryIndex.from_documents(\n    city_docs, \n    service_context=service_context,\n    response_synthesizer=response_synthesizer\n)</span></pre>\n <p>\n  Once the index is built, we can get the summary for any given document:\n </p>\n <pre><span class=\"qz nz gt qv b bf ra rb l rc rd\" id=\"3180\">summary = doc_summary_index.get_document_summary(\"Boston\")</span></pre>\n <p>\n  Next, let\u2019s walk through an example LLM-based retrieval over the index.\n </p>\n <pre><span class=\"qz nz gt qv b bf ra rb l rc rd\" id=\"b47a\"><span class=\"hljs-keyword\">from</span> llama_index.indices.document_summary <span class=\"hljs-keyword\">import</span> DocumentSummaryIndexRetriever\n\nretriever = DocumentSummaryIndexRetriever(\n    doc_summary_index,\n    <span class=\"hljs-comment\"># choice_select_prompt=choice_select_prompt,</span>\n    <span class=\"hljs-comment\"># choice_batch_size=choice_batch_size,</span>\n    <span class=\"hljs-comment\"># format_node_batch_fn=format_node_batch_fn,</span>\n    <span class=\"hljs-comment\"># parse_choice_select_answer_fn=parse_choice_select_answer_fn,</span>\n    <span class=\"hljs-comment\"># service_context=service_context</span>\n)\nretrieved_nodes = retriever.retrieve(<span class=\"hljs-string\">\"What are the sports teams in Toronto?\"</span>)\n<span class=\"hljs-built_in\">print</span>(retrieved_nodes[<span class=\"hljs-number\">0</span>].score)\n<span class=\"hljs-built_in\">print</span>(retrieved_nodes[<span class=\"hljs-number\">0</span>].node.get_text())The retriever will retrieve a <span class=\"hljs-built_in\">set</span> of relevant nodes <span class=\"hljs-keyword\">for</span> a given index.</span></pre>\n <p>\n  Note that the LLM returns relevance scores in addition to the document text:\n </p>\n <pre><span class=\"qz nz gt qv b bf ra rb l rc rd\" id=\"fde9\">8.0\nToronto ( (listen) t\u0259-RON-toh; locally [t\u0259\u02c8\u0279\u0252\u027e\u0303\u0259] or [\u02c8t\u0279\u0252\u027e\u0303\u0259]) is the capital city of the Canadian province of Ontario. With a recorded population of 2,794,356 in 2021, it is the most populous city in Canada...</span></pre>\n <p>\n  We can also use the index as part of an overall query engine, to not only retrieve the relevant context, but also synthesize a response to a given question. We can do this through both the high-level API as well as lower-level API.\n </p>\n <p>\n  <strong>\n   High-level API\n  </strong>\n </p>\n <pre><span class=\"qz nz gt qv b bf ra rb l rc rd\" id=\"c21b\">query_engine = doc_summary_index.as_query_engine(\n  response_mode=<span class=\"hljs-string\">\"tree_summarize\"</span>, use_async=<span class=\"hljs-literal\">True</span>\n)\nresponse = query_engine.query(<span class=\"hljs-string\">\"What are the sports teams in Toronto?\"</span>)\n<span class=\"hljs-built_in\">print</span>(response)</span></pre>\n <p>\n  <strong>\n   Lower-level API\n  </strong>\n </p>\n <pre><span class=\"qz nz gt qv b bf ra rb l rc rd\" id=\"d22f\"><span class=\"hljs-comment\"># use retriever as part of a query engine</span>\n<span class=\"hljs-keyword\">from</span> llama_index.query_engine <span class=\"hljs-keyword\">import</span> RetrieverQueryEngine\n\n<span class=\"hljs-comment\"># configure response synthesizer</span>\nresponse_synthesizer = ResponseSynthesizer.from_args()\n\n<span class=\"hljs-comment\"># assemble query engine</span>\nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    response_synthesizer=response_synthesizer,\n)\n\n<span class=\"hljs-comment\"># query</span>\nresponse = query_engine.query(<span class=\"hljs-string\">\"What are the sports teams in Toronto?\"</span>)\n<span class=\"hljs-built_in\">print</span>(response)</span></pre>\n <h1>\n  <strong>\n   Next Steps\n  </strong>\n </h1>\n <p>\n  The approach of autosummarization over any piece of text is really exciting. We\u2019re excited to develop extensions in two areas:\n </p>\n <ul>\n  <li>\n   Continue exploring autosummarization in different layers. Currently it\u2019s at the doc-level, but what about summarizing a big text chunk into a smaller one? (e.g. a one-liner).\n  </li>\n  <li>\n   Continue exploring LLM-based retrieval, which summarization helps to unlock.\n  </li>\n </ul>\n <p>\n  Also we\u2019re sharing the example guide/notebook below in case you missed it above:\n </p>\n <p>\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/index_structs/doc_summary/DocSummary.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Document Summary Guide\n  </a>\n </p>\n <p>\n  <a href=\"https://github.com/jerryjliu/llama_index/blob/main/docs/examples/index_structs/doc_summary/DocSummary.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Notebook Link\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 11670, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8449782-e685-40ae-b9e3-7444396543c4": {"__data__": {"id_": "c8449782-e685-40ae-b9e3-7444396543c4", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/agentic-rag-with-llamaindex-2721b8a49ff6.html", "file_name": "agentic-rag-with-llamaindex-2721b8a49ff6.html", "file_type": "text/html", "file_size": 7713, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/agentic-rag-with-llamaindex-2721b8a49ff6.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/agentic-rag-with-llamaindex-2721b8a49ff6.html", "file_name": "agentic-rag-with-llamaindex-2721b8a49ff6.html", "file_type": "text/html", "file_size": 7713, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "8fb9802b11fdf1a7cc9b88f317602b2d90a2f243e9ddf1f50ef979e235feede9", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  The topic of Agentic RAG explores how agents can be incorporated into existing\n  RAG pipelines for enhanced, conversational search and retrieval.\n </p>\n <h1>\n  Introduction\n </h1>\n <p>\n  <strong>\n   Considering the architecture below, it is evident how Agentic RAG creates\n      an implementation which easily scales. New documents can be added with\n      each new set being managed by a sub-agent.\n  </strong>\n </p>\n <p>\n  The basic structure of LlamaIndex\u2019s approach called Agentic RAG is shown in\n  the diagram below where a large set of documents are ingested, in this case it\n  was limited to 100.\n </p>\n <p>\n  The large corpus of data is broken up into smaller documents. An agent is\n  created for each document, and each of the numerous document agents have the\n  power of search via embeddings and to summarise the response.\n </p>\n <p>\n  A top-level agent is created over the set of document agents. The meta-agent /\n  top-level agent performs tool retrieval and then uses Chain-of-Thought to\n  answer the user\u2019s question.\n </p>\n <p>\n  The Rerank endpoint computes a relevance score for the query and each\n  document, and returns a sorted list from the most to the least relevant\n  document.\n </p>\n <h1>\n  Notebook Example\n </h1>\n <p>\n  Here you will find a Colab\n  <a href=\"https://github.com/cobusgreyling/LlamaIndex/blob/d8902482a247c76c7902ded143a875d5580f072a/Agentic_RAG_Multi_Document_Agents-v1.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   notebook\n  </a>\n  with a fully working and executed example of this\n  <a href=\"https://github.com/cobusgreyling/LlamaIndex/blob/d8902482a247c76c7902ded143a875d5580f072a/Agentic_RAG_Multi_Document_Agents-v1.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   implementation\n  </a>\n  .\n </p>\n <p>\n  To run the notebook, you will need an OpenAI and Cohere API key\u2026\n </p>\n <pre><span>import osimport openaiimport cohereos.environ[\"OPENAI_API_KEY\"] = \"Your API Key goes here\"os.environ[\"COHERE_API_KEY\"] = \"Your API Key goes here\"</span></pre>\n <p>\n  For each document agent, the system prompt is:\n </p>\n <pre><span>You are a specialized agent designed to answer queries about the `{file_base}.html` part of the LlamaIndex docs.You must ALWAYS use at least one of the tools provided when answering a question; do NOT rely on prior knowledge.</span></pre>\n <p>\n  The system prompt for the meta-agent /top-agent:\n </p>\n <pre><span> You are an agent designed to answer queries about the documentation. Please always use the tools provided to answer a question. Do not rely on prior knowledge</span></pre>\n <h2>\n  Working Example\n </h2>\n <p>\n  I asked the agent the following question:\n </p>\n <p>\n  <code>\n   Tell me about LlamaIndex connectors\n  </code>\n </p>\n <p>\n  Below is the detailed answer generated by the agent:\n </p>\n <p>\n  <strong>\n   <em>\n    Thought: I need to use a tool to help me answer the question.\n   </em>\n  </strong>\n </p>\n <p>\n  <strong>\n   <em>\n    Action: tool_latest_search Action Input: {\u2018input\u2019: \u2018LlamaIndex\n      connectors\u2019}\n   </em>\n  </strong>\n </p>\n <p>\n  Added user message to memory: LlamaIndex connectors\n </p>\n <p>\n  === Calling Function ===\n  <br>\n   Calling function: vector_tool_latest_search\n  with args: {\n   <br>\n    \u201cinput\u201d: \u201cLlamaIndex connectors\u201d\n    <br>\n     }\n     <br>\n      Got output:\n  The LlamaIndex connectors can be found on the\n  /content/docs.llamaindex.ai/en/latest/search.html page.\n  ========================\n     </br>\n    </br>\n   </br>\n  </br>\n </p>\n <p>\n  <strong>\n   <em>\n    Observation: The LlamaIndex connectors are not specifically mentioned in\n      the `latest_search.html` part of the LlamaIndex docs. You may need to\n      check other sections of the documentation for detailed information about\n      LlamaIndex connectors.\n   </em>\n  </strong>\n </p>\n <p>\n  <strong>\n   <em>\n    Thought: I need to search for information about LlamaIndex connectors in\n      a different tool.\n   </em>\n  </strong>\n </p>\n <p>\n  <strong>\n   <em>\n    Action: tool_latest_index Action Input: {\u2018input\u2019: \u2018LlamaIndex\n      connectors\u2019}\n   </em>\n  </strong>\n </p>\n <p>\n  Added user message to memory: LlamaIndex connectors\n </p>\n <p>\n  === Calling Function ===\n  <br>\n   Calling function: vector_tool_latest_index with\n  args: {\n   <br>\n    \u201cinput\u201d: \u201cLlamaIndex connectors\u201d\n    <br>\n     }\n    </br>\n   </br>\n  </br>\n </p>\n <p>\n  Got output: LlamaIndex provides data connectors that allow you to ingest your\n  existing data from various sources and formats. These connectors can work with\n  APIs, PDFs, SQL, and more. By using LlamaIndex connectors, you can easily\n  bring your data into the LlamaIndex ecosystem and make it accessible for\n  natural language access and retrieval. ========================\n </p>\n <p>\n  <strong>\n   <em>\n    Observation: LlamaIndex offers data connectors that enable you to import\n      your existing data from a variety of sources and formats. These connectors\n      are compatible with APIs, PDFs, SQL, and more. Utilizing LlamaIndex\n      connectors allows you to seamlessly integrate your data into the\n      LlamaIndex ecosystem, making it readily available for natural language\n      access and retrieval.\n   </em>\n  </strong>\n </p>\n <p>\n  <strong>\n   <em>\n    Thought: I can answer without using any more tools.\n   </em>\n  </strong>\n </p>\n <p>\n  <strong>\n   <em>\n    Answer: LlamaIndex connectors are used to import existing data from\n      various sources and formats into the LlamaIndex ecosystem. These\n      connectors are compatible with APIs, PDFs, SQL, and more, allowing\n      seamless integration of data for natural language access and\n      retrieval.\n   </em>\n  </strong>\n </p>\n <p>\n  Below is a snipped from the Colab notebook:\n </p>\n <figure>\n  <figcaption>\n   <a href=\"https://github.com/cobusgreyling/LlamaIndex/blob/d8902482a247c76c7902ded143a875d5580f072a/Agentic_RAG_Multi_Document_Agents-v1.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Source\n   </a>\n  </figcaption>\n </figure>\n <p>\n  This complex implementation from LlamaIndex is an example of multi-document\n  agents which can:\n </p>\n <ol>\n  <li>\n   Select documents relevant to a user query\n  </li>\n  <li>\n   Execute an agentic loop over the documents relevant to the query; including\n    chain-of-thought, summarisation and reranking.\n  </li>\n </ol>\n <h1>\n  In Conclusion\n </h1>\n <p>\n  This implementation by LlamaIndex illustrates a few key principles\u2026\n </p>\n <ol>\n  <li>\n   Agentic RAG, where an agent approach is followed for a RAG implementation\n    adds resilience and intelligence to the RAG implementation.\n  </li>\n  <li>\n   It is a good illustration of multi-agent orchestration.\n  </li>\n  <li>\n   This architecture serves as a good reference framework of how scaling an\n    agent can be optimised with a second tier of smaller worker-agents.\n  </li>\n  <li>\n   Agentic RAG is an example of a controlled and well defined\n   <strong>\n    <em>\n     autonomous\n    </em>\n   </strong>\n   <strong>\n    <em>\n     agent\n    </em>\n   </strong>\n   implementation.\n  </li>\n  <li>\n   One of the most sought-after enterprise LLM implementation types are RAG,\n    Agentic RAG is a natural progression of this.\n  </li>\n  <li>\n   It is easy to envisage how this architecture can grow and expand over an\n    organisation with more sub bots being added.\n  </li>\n </ol>\n <p>\n  <strong>\n   <em>\n    \u2b50\ufe0f Follow me on\n   </em>\n  </strong>\n  <a href=\"https://www.linkedin.com/in/cobusgreyling/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    <em>\n     LinkedIn\n    </em>\n   </strong>\n  </a>\n  <strong>\n   <em>\n    for updates on Large Language Models \u2b50\ufe0f\n   </em>\n  </strong>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 7664, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e0af1e9c-54d5-4fc6-b29c-200c841ef247": {"__data__": {"id_": "e0af1e9c-54d5-4fc6-b29c-200c841ef247", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.html", "file_name": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.html", "file_type": "text/html", "file_size": 18536, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.html", "file_name": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.html", "file_type": "text/html", "file_size": 18536, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "6117b1853de7eeb213f072f64310f9bba29c1a0aae4b95f4c59ccd1e327cbd0d", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h1>\n  Introduction\n </h1>\n <p>\n  The C3 Voice Assistant is my latest project aimed at making Large Language Model (LLM) and Retrieval-Augmented Generation (RAG) applications more accessible. This voice-activated assistant caters to a broad audience, including those facing typing challenges or accessibility issues.\n </p>\n <h1>\n  Features\n </h1>\n <ul>\n  <li>\n   <strong>\n    Voice Activation:\n   </strong>\n   Initiated by saying \u201cC3.\u201d Alternatively, users can click the blue ring to activate the listening mode of the app. The wake word \u201cC3\u201d is configurable and you can choose any other word.\n  </li>\n  <li>\n   <strong>\n    Universal Accessibility:\n   </strong>\n   Ideal for users preferring voice commands or facing typing challenges.\n  </li>\n  <li>\n   <strong>\n    LLM Integration:\n   </strong>\n   Capable of general queries and document-specific inquiries (e.g., Nvidia\u2019s FY 2023 10K report).\n  </li>\n  <li>\n   <strong>\n    User-Friendly Interface:\n   </strong>\n   The interface of the AI voice assistant is designed for simplicity and ease of use, focusing on voice chat interactions. It features a minimalistic and user-friendly React.js layout. Additionally, there is a convenient sidebar that displays the entire chat history in text format, allowing users to review and reflect on their interactions with the AI.\n  </li>\n </ul>\n <h1>\n  The Tech Stack\n </h1>\n <p>\n  The app is built on a robust and flexible tech stack that ensures a smooth, reliable, and efficient user experience. Here\u2019s an overview:\n </p>\n <ul>\n  <li>\n   <strong>\n    Frontend:\n   </strong>\n   The user interface is a custom application developed using React.js. It\u2019s designed to be minimalistic yet highly functional, prioritizing ease of use and accessibility.\n  </li>\n  <li>\n   <strong>\n    Backend:\n   </strong>\n   The server-side operations are powered by Python Flask. I\u2019ve utilized the innovative \u2018create-llama\u2019 feature from LlamaIndex, which significantly streamlines the development process.\n  </li>\n  <li>\n   <strong>\n    Hosting:\n   </strong>\n   For a seamless performance, the frontend of the C3 Voice Assistant is hosted on Vercel. The backend, on the other hand, is deployed on Render, ensuring efficient management and operation of server-side tasks.\n  </li>\n </ul>\n <h1>\n  Building the Frontend\n </h1>\n <p>\n  The frontend, built with React.js, focuses on user interaction and accessibility. The\n  <code class=\"cw qb qc qd qe b\">\n   App.js\n  </code>\n  script incorporates features like wake word recognition, speech-to-text conversion, state management, and dynamic UI elements like speech bubbles and spinners.\n </p>\n <h2>\n  1. Component and State Initialization\n </h2>\n <p>\n  This section sets up the React component and initializes various states, such as\n  <code class=\"cw qb qc qd qe b\">\n   appState\n  </code>\n  to track the current state of the app (idle, listening, speaking), and\n  <code class=\"cw qb qc qd qe b\">\n   transcript\n  </code>\n  to store the text transcribed from user speech.\n </p>\n <pre><span class=\"qx nb gt qe b bf qy qz l ra rb\" id=\"0e15\"><span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">React</span>, { useState, useRef, useEffect } <span class=\"hljs-keyword\">from</span> <span class=\"hljs-string\">\"react\"</span>;\n<span class=\"hljs-keyword\">import</span> <span class=\"hljs-string\">\"./App.css\"</span>;\n\n<span class=\"hljs-keyword\">const</span> <span class=\"hljs-title class_\">App</span> = () =&amp;gt; {\n  <span class=\"hljs-keyword\">const</span> [appState, setAppState] = <span class=\"hljs-title function_\">useState</span>(<span class=\"hljs-string\">\"idle\"</span>);\n  <span class=\"hljs-keyword\">const</span> [transcript, setTranscript] = <span class=\"hljs-title function_\">useState</span>(<span class=\"hljs-string\">\"\"</span>);\n  <span class=\"hljs-comment\">// Additional state and ref declarations...</span>\n};</span></pre>\n <h2>\n  2. Speech Recognition Setup\n </h2>\n <p>\n  In this useEffect hook, two speech recognition instances are initialized: one for detecting the wake word \u201cC3\u201d and another for the main speech recognition. This setup ensures that the app starts listening for commands when \u201cC3\u201d is mentioned.\n </p>\n <p>\n  You can easily swap \u201cC3\u201d with any other wake word of your choice.\n </p>\n <pre><span class=\"qx nb gt qe b bf qy qz l ra rb\" id=\"d657\">  <span class=\"hljs-title function_\">useEffect</span>(() =&amp;gt; {\n    <span class=\"hljs-comment\">// Wake word listener setup</span>\n    <span class=\"hljs-keyword\">const</span> <span class=\"hljs-title class_\">WakeWordSpeechRecognition</span> =\n      <span class=\"hljs-variable language_\">window</span>.<span class=\"hljs-property\">SpeechRecognition</span> || <span class=\"hljs-variable language_\">window</span>.<span class=\"hljs-property\">webkitSpeechRecognition</span>;\n    <span class=\"hljs-keyword\">if</span> (<span class=\"hljs-title class_\">WakeWordSpeechRecognition</span> &amp;amp;&amp;amp; !wakeWordRecognitionRef.<span class=\"hljs-property\">current</span>) {\n      wakeWordRecognitionRef.<span class=\"hljs-property\">current</span> = <span class=\"hljs-keyword\">new</span> <span class=\"hljs-title class_\">WakeWordSpeechRecognition</span>();\n      wakeWordRecognitionRef.<span class=\"hljs-property\">current</span>.<span class=\"hljs-property\">continuous</span> = <span class=\"hljs-literal\">true</span>;\n      wakeWordRecognitionRef.<span class=\"hljs-property\">current</span>.<span class=\"hljs-property\">interimResults</span> = <span class=\"hljs-literal\">false</span>;\n\n      wakeWordRecognitionRef.<span class=\"hljs-property\">current</span>.<span class=\"hljs-property\">onresult</span> = (event) =&amp;gt; {\n        <span class=\"hljs-keyword\">const</span> transcript = event.<span class=\"hljs-property\">results</span>[event.<span class=\"hljs-property\">results</span>.<span class=\"hljs-property\">length</span> - <span class=\"hljs-number\">1</span>][<span class=\"hljs-number\">0</span>].<span class=\"hljs-property\">transcript</span>\n          .<span class=\"hljs-title function_\">trim</span>()\n          .<span class=\"hljs-title function_\">toLowerCase</span>();\n        <span class=\"hljs-keyword\">if</span> (transcript.<span class=\"hljs-title function_\">includes</span>(<span class=\"hljs-string\">\"c3\"</span>)) {\n          <span class=\"hljs-title function_\">toggleRecording</span>(); <span class=\"hljs-comment\">// Start the main speech recognition process</span>\n        }\n      };\n\n      wakeWordRecognitionRef.<span class=\"hljs-property\">current</span>.<span class=\"hljs-title function_\">start</span>();\n    }\n\n    <span class=\"hljs-comment\">// Main speech recognition setup</span>\n    <span class=\"hljs-keyword\">const</span> <span class=\"hljs-title class_\">SpeechRecognition</span> =\n      <span class=\"hljs-variable language_\">window</span>.<span class=\"hljs-property\">SpeechRecognition</span> || <span class=\"hljs-variable language_\">window</span>.<span class=\"hljs-property\">webkitSpeechRecognition</span>;\n    <span class=\"hljs-keyword\">if</span> (<span class=\"hljs-title class_\">SpeechRecognition</span> &amp;amp;&amp;amp; !recognitionRef.<span class=\"hljs-property\">current</span>) {\n      recognitionRef.<span class=\"hljs-property\">current</span> = <span class=\"hljs-keyword\">new</span> <span class=\"hljs-title class_\">SpeechRecognition</span>();\n      recognitionRef.<span class=\"hljs-property\">current</span>.<span class=\"hljs-property\">continuous</span> = <span class=\"hljs-literal\">false</span>;\n      recognitionRef.<span class=\"hljs-property\">current</span>.<span class=\"hljs-property\">interimResults</span> = <span class=\"hljs-literal\">false</span>;\n\n      recognitionRef.<span class=\"hljs-property\">current</span>.<span class=\"hljs-property\">onresult</span> = (event) =&amp;gt; {\n        <span class=\"hljs-keyword\">const</span> lastResultIndex = event.<span class=\"hljs-property\">results</span>.<span class=\"hljs-property\">length</span> - <span class=\"hljs-number\">1</span>;\n        <span class=\"hljs-keyword\">const</span> transcriptResult = event.<span class=\"hljs-property\">results</span>[lastResultIndex][<span class=\"hljs-number\">0</span>].<span class=\"hljs-property\">transcript</span>;\n        <span class=\"hljs-title function_\">setTranscript</span>(transcriptResult);\n        <span class=\"hljs-title function_\">setAppState</span>(<span class=\"hljs-string\">\"playing\"</span>);\n        <span class=\"hljs-title function_\">setShowSpeechBubble</span>(<span class=\"hljs-literal\">true</span>);\n        <span class=\"hljs-built_in\">setTimeout</span>(() =&amp;gt; <span class=\"hljs-title function_\">setShowSpeechBubble</span>(<span class=\"hljs-literal\">false</span>), speechBubbleTimeout);\n        <span class=\"hljs-title function_\">fetchResponseFromLLM</span>(transcriptResult);\n      };\n\n      recognitionRef.<span class=\"hljs-property\">current</span>.<span class=\"hljs-property\">onend</span> = () =&amp;gt; {\n        <span class=\"hljs-title function_\">setShowSpinner</span>(<span class=\"hljs-literal\">true</span>);\n      };\n    }\n  }, []);</span></pre>\n <h2>\n  3. Handling User Speech and Response\n </h2>\n <p>\n  <code class=\"cw qb qc qd qe b\">\n   toggleRecording\n  </code>\n  controls the speech recognition process, while\n  <code class=\"cw qb qc qd qe b\">\n   fetchResponseFromLLM\n  </code>\n  sends the user's speech to the LLM backend and handles the response. This response is then spoken out via speech synthesis and also used to update the chat history displayed on the UI.\n </p>\n <pre><span class=\"qx nb gt qe b bf qy qz l ra rb\" id=\"79b9\"> <span class=\"hljs-keyword\">const</span> toggleRecording = () =&amp;gt; {\n    <span class=\"hljs-keyword\">try</span> {\n      <span class=\"hljs-keyword\">if</span> (appState === <span class=\"hljs-string\">\"idle\"</span>) {\n        recognitionRef.<span class=\"hljs-property\">current</span>.<span class=\"hljs-title function_\">start</span>();\n        <span class=\"hljs-title function_\">setAppState</span>(<span class=\"hljs-string\">\"listening\"</span>);\n      } <span class=\"hljs-keyword\">else</span> <span class=\"hljs-keyword\">if</span> (appState === <span class=\"hljs-string\">\"listening\"</span>) {\n        recognitionRef.<span class=\"hljs-property\">current</span>.<span class=\"hljs-title function_\">stop</span>();\n      }\n    } <span class=\"hljs-keyword\">catch</span> (error) {\n    }\n  };</span></pre>\n <pre><span class=\"qx nb gt qe b bf qy qz l ra rb\" id=\"05ef\">  <span class=\"hljs-keyword\">const</span> fetchResponseFromLLM = <span class=\"hljs-keyword\">async</span> (text) =&amp;gt; {\n    <span class=\"hljs-keyword\">try</span> {\n      <span class=\"hljs-keyword\">const</span> response = <span class=\"hljs-keyword\">await</span> <span class=\"hljs-title function_\">fetch</span>(\n        <span class=\"hljs-string\">`https://c3-python-nostream.onrender.com/api/chat`</span>,\n        {\n          <span class=\"hljs-attr\">method</span>: <span class=\"hljs-string\">\"POST\"</span>,\n          <span class=\"hljs-attr\">headers</span>: { <span class=\"hljs-string\">\"Content-Type\"</span>: <span class=\"hljs-string\">\"application/json\"</span> },\n          <span class=\"hljs-attr\">body</span>: <span class=\"hljs-title class_\">JSON</span>.<span class=\"hljs-title function_\">stringify</span>({\n            <span class=\"hljs-attr\">messages</span>: [\n              {\n                <span class=\"hljs-attr\">role</span>: <span class=\"hljs-string\">\"user\"</span>,\n                <span class=\"hljs-attr\">content</span>:\n                  <span class=\"hljs-string\">\"You are an AI voice assistant called C3. You can provide any general information as well as answer basic questions about the Nvidia 10k report for year ended Jan 2023\"</span> +\n                  text,\n              },\n            ],\n          }),\n        }\n      );\n      <span class=\"hljs-keyword\">const</span> data = <span class=\"hljs-keyword\">await</span> response.<span class=\"hljs-title function_\">json</span>();\n\n      <span class=\"hljs-title function_\">setChatHistory</span>((prevHistory) =&amp;gt; [\n        ...prevHistory,\n        { <span class=\"hljs-attr\">query</span>: text, <span class=\"hljs-attr\">response</span>: data.<span class=\"hljs-property\">result</span>.<span class=\"hljs-property\">content</span> },\n      ]);\n      <span class=\"hljs-title function_\">speak</span>(data.<span class=\"hljs-property\">result</span>.<span class=\"hljs-property\">content</span>);\n    } <span class=\"hljs-keyword\">catch</span> (error) {\n      <span class=\"hljs-variable language_\">console</span>.<span class=\"hljs-title function_\">error</span>(<span class=\"hljs-string\">\"Error communicating with LLM:\"</span>, error);\n    }\n  };</span></pre>\n <h2>\n  4. Speech Synthesis\n </h2>\n <p>\n  The\n  <code class=\"cw qb qc qd qe b\">\n   speak\n  </code>\n  function takes the text response from the LLM and uses the SpeechSynthesis API to read it aloud, providing an interactive experience for the user.\n </p>\n <pre><span class=\"qx nb gt qe b bf qy qz l ra rb\" id=\"e336\">  <span class=\"hljs-keyword\">const</span> speak = (text) =&amp;gt; {\n    <span class=\"hljs-keyword\">if</span> (synthRef.<span class=\"hljs-property\">current</span> &amp;amp;&amp;amp; text) {\n      <span class=\"hljs-keyword\">const</span> utterance = <span class=\"hljs-keyword\">new</span> <span class=\"hljs-title class_\">SpeechSynthesisUtterance</span>(text);\n\n      <span class=\"hljs-keyword\">const</span> voices = <span class=\"hljs-variable language_\">window</span>.<span class=\"hljs-property\">speechSynthesis</span>.<span class=\"hljs-title function_\">getVoices</span>();\n      <span class=\"hljs-keyword\">if</span> (voices.<span class=\"hljs-property\">length</span> &amp;gt; <span class=\"hljs-number\">0</span>) {\n        utterance.<span class=\"hljs-property\">voice</span> = voices[<span class=\"hljs-number\">3</span>]; <span class=\"hljs-comment\">// You can change this to select different voices</span>\n      }\n\n      utterance.<span class=\"hljs-property\">onstart</span> = () =&amp;gt; {\n        <span class=\"hljs-variable language_\">console</span>.<span class=\"hljs-title function_\">log</span>(<span class=\"hljs-string\">\"TTS starts speaking\"</span>);\n        <span class=\"hljs-title function_\">setShowSpinner</span>(<span class=\"hljs-literal\">false</span>);\n      };\n\n      utterance.<span class=\"hljs-property\">onend</span> = () =&amp;gt; {\n        <span class=\"hljs-title function_\">setAppState</span>(<span class=\"hljs-string\">\"idle\"</span>);\n        <span class=\"hljs-keyword\">if</span> (wakeWordRecognitionRef.<span class=\"hljs-property\">current</span>) {\n          wakeWordRecognitionRef.<span class=\"hljs-property\">current</span>.<span class=\"hljs-title function_\">start</span>(); <span class=\"hljs-comment\">// Restart wake word listener after speaking</span>\n        }\n      };\n      synthRef.<span class=\"hljs-property\">current</span>.<span class=\"hljs-title function_\">speak</span>(utterance);\n    }</span></pre>\n <h2>\n  5. UI Rendering\n </h2>\n <p>\n  The return statement of the\n  <code class=\"cw qb qc qd qe b\">\n   App\n  </code>\n  function contains the JSX code for rendering the app's UI. This includes buttons for starting/stopping the voice interaction, a display area for the transcript, and a chat sidebar showing the history of interactions.\n </p>\n <p>\n  By combining voice recognition, LLM integration, and speech synthesis, this frontend component provides a comprehensive and accessible interface for interacting with the C3 Voice Assistant.\n </p>\n <h1>\n  Backend Server Setup\n </h1>\n <ol>\n  <li>\n   Initialize Create-Llama: Run\n   <code class=\"cw qb qc qd qe b\">\n    npx create-llama@latest\n   </code>\n   in your terminal.\n  </li>\n  <li>\n   Follow the prompts to set up a Python FastAPI backend, which we can be integrated with our frontend.\n  </li>\n  <li>\n   Use\n   <code class=\"cw qb qc qd qe b\">\n    poetry install\n   </code>\n   and\n   <code class=\"cw qb qc qd qe b\">\n    poetry shell\n   </code>\n   to prepare the environment.\n  </li>\n  <li>\n   Create a\n   <code class=\"cw qb qc qd qe b\">\n    .env\n   </code>\n   file with\n   <code class=\"cw qb qc qd qe b\">\n    OPENAI_API_KEY=&lt;openai_api_key&gt;\n   </code>\n   .\n  </li>\n  <li>\n   Generate Embeddings (optional): If a\n   <code class=\"cw qb qc qd qe b\">\n    ./data\n   </code>\n   directory exists, run\n   <code class=\"cw qb qc qd qe b\">\n    python app/engine/generate.py\n   </code>\n   .\n  </li>\n  <li>\n   Execute\n   <code class=\"cw qb qc qd qe b\">\n    python main.py\n   </code>\n   to start the server.\n  </li>\n  <li>\n   Test the API: Use\n   <code class=\"cw qb qc qd qe b\">\n    curl --location 'localhost:8000/api/chat' --header 'Content-Type: application/json' --data '{ \"messages\": [{ \"role\": \"user\", \"content\": \"Hello\" }] }'\n   </code>\n   to test.\n  </li>\n  <li>\n   Modify API behavior in\n   <code class=\"cw qb qc qd qe b\">\n    app/api/routers/chat.py\n   </code>\n   . The server supports CORS for all origins, alterable with the\n   <code class=\"cw qb qc qd qe b\">\n    ENVIRONMENT=prod\n   </code>\n   setting.\n  </li>\n </ol>\n <h1>\n  Integration\n </h1>\n <p>\n  Once the backend server is set up, integrating it with the frontend is straightforward. Simply update the\n  <code class=\"cw qb qc qd qe b\">\n   fetchResponseFromLLM\n  </code>\n  function in your frontend's\n  <code class=\"cw qb qc qd qe b\">\n   App.js\n  </code>\n  to call the backend server URL. This change ensures that when the frontend makes a request, it communicates with your newly configured backend, thus effectively integrating the two components.\n </p>\n <h1>\n  Final Thoughts\n </h1>\n <p>\n  Wrapping up, the C3 Voice Assistant isn\u2019t just a tech showcase; it\u2019s a stride towards democratizing AI. It\u2019s about making powerful AI tools, like LLMs and RAG, accessible and user-friendly. This project is more than lines of code \u2014 it\u2019s a push to break down tech barriers and empower everyone.\n </p>\n <p>\n  Your thoughts and feedback are invaluable \u2014 let\u2019s make AI more accessible together!\n </p>\n <p>\n  Link to Github Repo:\n  <a href=\"https://github.com/AI-ANK/C3-Voice-Assistant-UI\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Frontend\n  </a>\n  and\n  <a href=\"https://github.com/AI-ANK/c3-python-nostream\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Backend\n  </a>\n </p>\n <p>\n  <a href=\"https://www.linkedin.com/in/harshadsuryawanshi/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Connect with Me on LinkedIn\n  </a>\n </p>\n <p>\n  <a href=\"https://www.linkedin.com/posts/harshadsuryawanshi_ai-llamaindex-gpt3-activity-7149796976442740736-1lXj?utm_source=share&amp;utm_medium=member_desktop\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Linkedin Post\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 18489, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d74700e-cd3e-4388-a062-02ab00aef449": {"__data__": {"id_": "2d74700e-cd3e-4388-a062-02ab00aef449", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/announcing-llamaindex-0-9-719f03282945.html", "file_name": "announcing-llamaindex-0-9-719f03282945.html", "file_type": "text/html", "file_size": 18985, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/announcing-llamaindex-0-9-719f03282945.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/announcing-llamaindex-0-9-719f03282945.html", "file_name": "announcing-llamaindex-0-9-719f03282945.html", "file_type": "text/html", "file_size": 18985, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "aca8508a39c87881acdb169298f52331e0b3eedfd67890790ebe9784f483caa4", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Our hard-working team is delighted to announce our latest major release, LlamaIndex 0.9! You can get it right now:\n </p>\n <p>\n  <code class=\"cw om on oo op b\">\n   pip install --upgrade llama_index\n  </code>\n </p>\n <p>\n  In LlamaIndex v0.9, we are taking the time to refine several key aspects of the user experience, including token counting, text splitting, and more!\n </p>\n <p>\n  As part of this, there are some new features and minor changes to current usage that developers should be aware of:\n </p>\n <ul>\n  <li>\n   New\n   <code class=\"cw om on oo op b\">\n    IngestionPipline\n   </code>\n   concept for ingesting and transforming data\n  </li>\n  <li>\n   Data ingestion and transforms are now automatically cached\n  </li>\n  <li>\n   Updated interface for node parsing/text splitting/metadata extraction modules\n  </li>\n  <li>\n   Changes to the default tokenizer, as well as customizing the tokenizer\n  </li>\n  <li>\n   Packaging/Installation changes with PyPi (reduced bloat, new install options)\n  </li>\n  <li>\n   More predictable and consistent import paths\n  </li>\n  <li>\n   Plus, in beta: MultiModal RAG Modules for handling text and images!\n  </li>\n </ul>\n <p>\n  Have questions or concerns? You can\n  <a href=\"https://github.com/run-llama/llama_index/issues\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   report an issue\n  </a>\n  on GitHub or\n  <a href=\"https://discord.com/invite/eN6D2HQ4aX\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   ask a question on our Discord\n  </a>\n  !\n </p>\n <p>\n  Read on for more details on our new features and changes.\n </p>\n <h1>\n  IngestionPipeline \u2014 New abstraction for purely ingesting data\n </h1>\n <p>\n  Sometimes, all you want is to ingest and embed nodes from data sources, for instance if your application allows users to upload new data. New in LlamaIndex V0.9 is the concept of an\n  <code class=\"cw om on oo op b\">\n   IngestionPipepline\n  </code>\n  .\n </p>\n <p>\n  An\n  <code class=\"cw om on oo op b\">\n   IngestionPipeline\n  </code>\n  uses a new concept of\n  <code class=\"cw om on oo op b\">\n   Transformations\n  </code>\n  that are applied to input data.\n </p>\n <p>\n  What is a\n  <code class=\"cw om on oo op b\">\n   Transformation\n  </code>\n  though? It could be a:\n </p>\n <ul>\n  <li>\n   text splitter\n  </li>\n  <li>\n   node parser\n  </li>\n  <li>\n   metadata extractor\n  </li>\n  <li>\n   embeddings model\n  </li>\n </ul>\n <p>\n  Here\u2019s a quick example of the basic usage pattern:\n </p>\n <pre><span class=\"qk pa gt op b bf ql qm l qn qo\" id=\"d8ed\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> Document\n<span class=\"hljs-keyword\">from</span> llama_index.embeddings <span class=\"hljs-keyword\">import</span> OpenAIEmbedding\n<span class=\"hljs-keyword\">from</span> llama_index.text_splitter <span class=\"hljs-keyword\">import</span> SentenceSplitter\n<span class=\"hljs-keyword\">from</span> llama_index.extractors <span class=\"hljs-keyword\">import</span> TitleExtractor\n<span class=\"hljs-keyword\">from</span> llama_index.ingestion <span class=\"hljs-keyword\">import</span> IngestionPipeline, IngestionCache\n\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=<span class=\"hljs-number\">25</span>, chunk_overlap=<span class=\"hljs-number\">0</span>),\n        TitleExtractor(),\n        OpenAIEmbedding(),\n    ]\n)\nnodes = pipeline.run(documents=[Document.example()])</span></pre>\n <h1>\n  Transformation Caching\n </h1>\n <p>\n  Each time you run the same\n  <code class=\"cw om on oo op b\">\n   IngestionPipeline\n  </code>\n  object, it caches a hash of the input nodes + transformations and the output of that transformation for each transformation in the pipeline.\n </p>\n <p>\n  In subsequent runs, if there is a cache hit, that transformation will be skipped and the cached result will be used instead. The greatly speeds up duplicate runs, and can help improve iteration times when deciding which transformations to use.\n </p>\n <p>\n  Here\u2019s an example with a saving and loading a local cache:\n </p>\n <pre><span class=\"qk pa gt op b bf ql qm l qn qo\" id=\"1882\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> Document\n<span class=\"hljs-keyword\">from</span> llama_index.embeddings <span class=\"hljs-keyword\">import</span> OpenAIEmbedding\n<span class=\"hljs-keyword\">from</span> llama_index.text_splitter <span class=\"hljs-keyword\">import</span> SentenceSplitter\n<span class=\"hljs-keyword\">from</span> llama_index.extractors <span class=\"hljs-keyword\">import</span> TitleExtractor\n<span class=\"hljs-keyword\">from</span> llama_index.ingestion <span class=\"hljs-keyword\">import</span> IngestionPipeline, IngestionCache\n\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=<span class=\"hljs-number\">25</span>, chunk_overlap=<span class=\"hljs-number\">0</span>),\n        TitleExtractor(),\n        OpenAIEmbedding(),\n    ]\n)\n<span class=\"hljs-comment\"># will only execute full pipeline once</span>\nnodes = pipeline.run(documents=[Document.example()])\nnodes = pipeline.run(documents=[Document.example()])\n<span class=\"hljs-comment\"># save and load</span>\npipeline.cache.persist(<span class=\"hljs-string\">\"./test_cache.json\"</span>)\nnew_cache = IngestionCache.from_persist_path(<span class=\"hljs-string\">\"./test_cache.json\"</span>)\nnew_pipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=<span class=\"hljs-number\">25</span>, chunk_overlap=<span class=\"hljs-number\">0</span>),\n        TitleExtractor(),\n    ],\n    cache=new_cache,\n)\n<span class=\"hljs-comment\"># will run instantly due to the cache</span>\nnodes = pipeline.run(documents=[Document.example()])</span></pre>\n <p>\n  And here\u2019s another example using Redis as a cache and Qdrant as a vector store. Running this will directly insert the nodes into your vector store and cache each transformation step in Redis.\n </p>\n <pre><span class=\"qk pa gt op b bf ql qm l qn qo\" id=\"14d4\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> Document\n<span class=\"hljs-keyword\">from</span> llama_index.embeddings <span class=\"hljs-keyword\">import</span> OpenAIEmbedding\n<span class=\"hljs-keyword\">from</span> llama_index.text_splitter <span class=\"hljs-keyword\">import</span> SentenceSplitter\n<span class=\"hljs-keyword\">from</span> llama_index.extractors <span class=\"hljs-keyword\">import</span> TitleExtractor\n<span class=\"hljs-keyword\">from</span> llama_index.ingestion <span class=\"hljs-keyword\">import</span> IngestionPipeline, IngestionCache\n<span class=\"hljs-keyword\">from</span> llama_index.ingestion.cache <span class=\"hljs-keyword\">import</span> RedisCache\n<span class=\"hljs-keyword\">from</span> llama_index.vector_stores.qdrant <span class=\"hljs-keyword\">import</span> QdrantVectorStore\n\n<span class=\"hljs-keyword\">import</span> qdrant_client\nclient = qdrant_client.QdrantClient(location=<span class=\"hljs-string\">\":memory:\"</span>)\nvector_store = QdrantVectorStore(client=client, collection_name=<span class=\"hljs-string\">\"test_store\"</span>)\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=<span class=\"hljs-number\">25</span>, chunk_overlap=<span class=\"hljs-number\">0</span>),\n        TitleExtractor(),\n        OpenAIEmbedding(),\n    ],\n    cache=IngestionCache(cache=RedisCache(), collection=<span class=\"hljs-string\">\"test_cache\"</span>),\n    vector_store=vector_store,\n)\n<span class=\"hljs-comment\"># Ingest directly into a vector db</span>\npipeline.run(documents=[Document.example()])\n<span class=\"hljs-comment\"># Create your index</span>\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> VectorStoreIndex\nindex = VectorStoreIndex.from_vector_store(vector_store)</span></pre>\n <h1>\n  Custom Transformations\n </h1>\n <p>\n  Implementing custom transformations is easy! Let\u2019s add a transform to remove special characters from the text before calling embeddings.\n </p>\n <p>\n  The only real requirement for transformations is that they must accept a list of nodes and return a list of nodes.\n </p>\n <pre><span class=\"qk pa gt op b bf ql qm l qn qo\" id=\"b3ca\"><span class=\"hljs-keyword\">import</span> re\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> Document\n<span class=\"hljs-keyword\">from</span> llama_index.embeddings <span class=\"hljs-keyword\">import</span> OpenAIEmbedding\n<span class=\"hljs-keyword\">from</span> llama_index.text_splitter <span class=\"hljs-keyword\">import</span> SentenceSplitter\n<span class=\"hljs-keyword\">from</span> llama_index.ingestion <span class=\"hljs-keyword\">import</span> IngestionPipeline\n<span class=\"hljs-keyword\">from</span> llama_index.schema <span class=\"hljs-keyword\">import</span> TransformComponent\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">TextCleaner</span>(<span class=\"hljs-title class_ inherited__\">TransformComponent</span>):\n  <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__call__</span>(<span class=\"hljs-params\">self, nodes, **kwargs</span>):\n    <span class=\"hljs-keyword\">for</span> node <span class=\"hljs-keyword\">in</span> nodes:\n      node.text = re.sub(<span class=\"hljs-string\">r'[^0-9A-Za-z ]'</span>, <span class=\"hljs-string\">\"\"</span>, node.text)\n    <span class=\"hljs-keyword\">return</span> nodes\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=<span class=\"hljs-number\">25</span>, chunk_overlap=<span class=\"hljs-number\">0</span>),\n        TextCleaner(),\n        OpenAIEmbedding(),\n    ],\n)\nnodes = pipeline.run(documents=[Document.example()])</span></pre>\n <h1>\n  Node Parsing/Text Splitting \u2014 Flattened and Simplified Interface\n </h1>\n <p>\n  We\u2019ve made our interface for parsing and splitting text a lot cleaner.\n </p>\n <h1>\n  Before:\n </h1>\n <pre><span class=\"qk pa gt op b bf ql qm l qn qo\" id=\"c330\"><span class=\"hljs-keyword\">from</span> llama_index.node_parser <span class=\"hljs-keyword\">import</span> SimpleNodeParser\n<span class=\"hljs-keyword\">from</span> llama_index.node_parser.extractors <span class=\"hljs-keyword\">import</span> (\n\tMetadataExtractor, TitleExtractor\n) \n<span class=\"hljs-keyword\">from</span> llama_index.text_splitter <span class=\"hljs-keyword\">import</span> SentenceSplitter\n\nnode_parser = SimpleNodeParser(\n  text_splitter=SentenceSplitter(chunk_size=<span class=\"hljs-number\">512</span>),\n  metadata_extractor=MetadataExtractor(\n  extractors=[TitleExtractor()]\n ),\n)\nnodes = node_parser.get_nodes_from_documents(documents)</span></pre>\n <h1>\n  After:\n </h1>\n <pre><span class=\"qk pa gt op b bf ql qm l qn qo\" id=\"5692\"><span class=\"hljs-keyword\">from</span> llama_index.text_splitter <span class=\"hljs-keyword\">import</span> SentenceSplitter\n<span class=\"hljs-keyword\">from</span> llama_index.extractors <span class=\"hljs-keyword\">import</span> TitleExtractor \n\nnode_parser = SentenceSplitter(chunk_size=<span class=\"hljs-number\">512</span>)\nextractor = TitleExtractor()\n\n<span class=\"hljs-comment\"># use transforms directly</span>\nnodes = node_parser(documents)\nnodes = extractor(nodes)</span></pre>\n <p>\n  Previously, the\n  <code class=\"cw om on oo op b\">\n   NodeParser\n  </code>\n  object in LlamaIndex had become extremely bloated, holding both text splitters and metadata extractors, which caused both pains for users when changing these components, and pains for us trying to maintain and develop them.\n </p>\n <p>\n  In V0.9, we have\n  <strong>\n   flattened\n  </strong>\n  the entire interface into a single\n  <code class=\"cw om on oo op b\">\n   TransformComponent\n  </code>\n  abstraction, so that these transformations are easier to setup, use, and customize.\n </p>\n <p>\n  We\u2019ve done our best to minimize the impacts on users, but the main thing to note is that\n  <code class=\"cw om on oo op b\">\n   <strong>\n    SimpleNodeParser\n   </strong>\n  </code>\n  <strong>\n   has been removed\n  </strong>\n  , and other node parsers and text splitters have been elevated to have the same features, just with different parsing and splitting techniques.\n </p>\n <p>\n  Any old imports of\n  <code class=\"cw om on oo op b\">\n   SimpleNodeParser\n  </code>\n  will redirect to the most equivalent module,\n  <code class=\"cw om on oo op b\">\n   SentenceSplitter\n  </code>\n  .\n </p>\n <p>\n  Furthermore, the wrapper object\n  <code class=\"cw om on oo op b\">\n   <strong>\n    MetadataExtractor\n   </strong>\n  </code>\n  <strong>\n   has been removed\n  </strong>\n  , in favour of using extractors directly.\n </p>\n <p>\n  Full documentation for all this can be found below:\n </p>\n <ul>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Node Parsers and Text Splitters\n   </a>\n  </li>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/indexing/metadata_extraction.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Metadata Extractors\n   </a>\n  </li>\n </ul>\n <h1>\n  Tokenization and Token Counting \u2014 Improved defaults and Customization\n </h1>\n <p>\n  A big pain point in LlamaIndex previously was tokenization. Many components used a non-configurable\n  <code class=\"cw om on oo op b\">\n   gpt2\n  </code>\n  tokenizer for token counting, causing headaches for users using non-OpenAI models, or even some hacky fixes\n  <a href=\"https://github.com/run-llama/llama_index/blob/336a88db4f13cfc598c473f9b5a3bc073b5d7ef4/llama_index/indices/prompt_helper.py#L119\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   like this\n  </a>\n  for OpenAI models too!\n </p>\n <p>\n  In LlamaIndex V0.9, this\n  <strong>\n   global tokenizer is now configurable and defaults to the CL100K tokenizer\n  </strong>\n  to match our default GPT-3.5 LLM.\n </p>\n <p>\n  The single requirement for a tokenizer is that it is a callable function, that takes a string, and returns a list.\n </p>\n <p>\n  Some examples of configuring this are below:\n </p>\n <pre><span class=\"qk pa gt op b bf ql qm l qn qo\" id=\"5d70\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> set_global_tokenizer\n\n<span class=\"hljs-comment\"># tiktoken</span>\n<span class=\"hljs-keyword\">import</span> tiktoken\nset_global_tokenizer(\n  tiktoken.encoding_for_model(<span class=\"hljs-string\">\"gpt-3.5-turbo\"</span>).encode\n)\n<span class=\"hljs-comment\"># huggingface</span>\n<span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer\nset_global_tokenizer(\n  AutoTokenizer.from_pretrained(<span class=\"hljs-string\">\"HuggingFaceH4/zephyr-7b-beta\"</span>).encode\n)</span></pre>\n <p>\n  Furthermore, the\n  <code class=\"cw om on oo op b\">\n   TokenCountingHandler\n  </code>\n  has gotten an upgrade with better token counting, as well as using token counts from API responses directly when available.\n </p>\n <h1>\n  Packaging \u2014 Reduced Bloat\n </h1>\n <p>\n  In an effort to modernize the packaging of LlamaIndex, V0.9 also comes with changes to installation.\n </p>\n <p>\n  The biggest change here is that\n  <code class=\"cw om on oo op b\">\n   LangChain\n  </code>\n  is now an optional package, and will not be installed by default.\n </p>\n <p>\n  To install\n  <code class=\"cw om on oo op b\">\n   LangChain\n  </code>\n  as part of your llama-index installation you can follow the example below. There are also other installation options depending on your needs, and we are welcoming further contributions to the extras in the future.\n </p>\n <pre><span class=\"qk pa gt op b bf ql qm l qn qo\" id=\"09ee\"># installs langchain\npip install llama-index[langchain]\n \n# installs tools needed for running local models\npip install llama-index[local_models]\n\n# installs tools needed for postgres\npip install llama-index[postgres]\n\n# combinations!\npip isntall llama-index[local_models,postgres]</span></pre>\n <p>\n  <strong>\n   If you were previously importing\n  </strong>\n  <code class=\"cw om on oo op b\">\n   <strong>\n    langchain\n   </strong>\n  </code>\n  <strong>\n   modules\n  </strong>\n  in your code, please update your project packaging requirements appropriately.\n </p>\n <h1>\n  Import Paths \u2014 More Consistent and Predictable\n </h1>\n <p>\n  We are making two changes to our import paths:\n </p>\n <ol>\n  <li>\n   We\u2019ve removed uncommonly used imports from the root level to make importing\n   <code class=\"cw om on oo op b\">\n    llama_index\n   </code>\n   faster\n  </li>\n  <li>\n   We now have a consistent policy for making \u201cuser-facing\u201d concepts import-able at level-1 modules.\n  </li>\n </ol>\n <pre><span class=\"qk pa gt op b bf ql qm l qn qo\" id=\"13d2\"><span class=\"hljs-keyword\">from</span> llama_index.llms <span class=\"hljs-keyword\">import</span> OpenAI, ...\n<span class=\"hljs-keyword\">from</span> llama_index.embeddings <span class=\"hljs-keyword\">import</span> OpenAIEmbedding, ...\n<span class=\"hljs-keyword\">from</span> llama_index.prompts <span class=\"hljs-keyword\">import</span> PromptTemplate, ...\n<span class=\"hljs-keyword\">from</span> llama_index.readers <span class=\"hljs-keyword\">import</span> SimpleDirectoryReader, ...\n<span class=\"hljs-keyword\">from</span> llama_index.text_splitter <span class=\"hljs-keyword\">import</span> SentenceSplitter, ...\n<span class=\"hljs-keyword\">from</span> llama_index.extractors <span class=\"hljs-keyword\">import</span> TitleExtractor, ...\n<span class=\"hljs-keyword\">from</span> llama_index.vector_stores <span class=\"hljs-keyword\">import</span> SimpleVectorStore, ...</span></pre>\n <p>\n  We still expose some of the most commonly used modules at the root level.\n </p>\n <pre><span class=\"qk pa gt op b bf ql qm l qn qo\" id=\"299d\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> SimpleDirectoryReader, VectorStoreIndex, ...</span></pre>\n <h1>\n  MultiModal RAG\n </h1>\n <p>\n  Given the recent announcements of the GPT-4V API, multi-modal use cases are more accessible than ever before.\n </p>\n <p>\n  To help users use these features, we\u2019ve started to introduce a number of new modules to help support use-cases for MultiModal RAG:\n </p>\n <ul>\n  <li>\n   MultiModal LLMs (GPT-4V, Llava, Fuyu, etc.)\n  </li>\n  <li>\n   MultiModal Embeddings (i.e clip) for join image-text embedding/retrieval\n  </li>\n  <li>\n   MultiModal RAG, combining indexes and query engines\n  </li>\n </ul>\n <p>\n  Our documentation has a\n  <a href=\"https://docs.llamaindex.ai/en/latest/examples/multi_modal/gpt4v_multi_modal_retrieval.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   full guide to multi-modal retrieval\n  </a>\n  .\n </p>\n <h1>\n  Thanks for all your support!\n </h1>\n <p>\n  As an open-source project we couldn\u2019t exist without our\n  <a href=\"https://github.com/run-llama/llama_index/graphs/contributors\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   hundreds of contributors\n  </a>\n  . We are so grateful for them and the support of the hundreds of thousands of LlamaIndex users around the world. See you on the Discord!\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 18952, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b21d0b46-68a2-4661-a1d0-00ed2b425b11": {"__data__": {"id_": "b21d0b46-68a2-4661-a1d0-00ed2b425b11", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications.html", "file_name": "arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications.html", "file_type": "text/html", "file_size": 3887, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications.html", "file_name": "arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications.html", "file_type": "text/html", "file_size": 3887, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "1d53368128eb93b54f8c8b110bcca0df2fb08851faf458adef76e22718bb3012", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  Strategic alliance and joint product promises to broaden the adoption of generative AI across industries\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Arize AI, a pioneer and leader in AI observability and LLM evaluation, and LlamaIndex, a leading data framework for LLM applications, debuted a new joint offering today called LlamaTrace, a hosted version of Arize OSS Phoenix.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  According to a soon-to-release survey, 47.7% of AI engineers and developers building generative AI applications are leveraging retrieval today in their LLM Applications. By connecting data to generative AI, orchestration frameworks like LlamaIndex can be game-changers in accelerating generative AI development. However, for many teams and enterprises technical challenges remain in getting modern LLM systems \u2013 with layers of abstraction \u2013 ready for the real world.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  To help, Arize and LlamaIndex are debuting an LLM tracing and observability platform that works natively with the LlamaIndex and Arize ecosystem. With a foundation based on\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://phoenix.arize.com/\" rel=\"noreferrer noopener\">\n   Arize Phoenix OSS\n  </a>\n  , the hosted version of Phoenix offers the ability to persist application telemetry data generated during AI development in order to better experiment, iterate, and collaborate in development or production.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The solution has a foundation in open source and features a fully hosted, online, persistent deployment option for teams that do not want to self host. AI engineers can instantly log traces, persist datasets, run experiments, run evaluations \u2013 and share those insights with colleagues.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The new offering is available today, and can be accessed through either a LlamaIndex or Arize account.\n </p>\n <blockquote class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  \u201cWe share a vision with LlamaIndex in enabling builders to reduce the time it takes to deploy generative AI into production but in a way that is super battle hardened for business-critical use cases,\u201d said Jason Lopatecki, CEO and Co-Founder of Arize. \u201cAs leaders in our respective spaces with a common philosophy in empowering AI engineers and developers, we\u2019re uniquely positioned here to do something that can move modern LLMOps forward and broaden adoption.\u201d\n </blockquote>\n <blockquote class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  \u201cPrototyping a RAG pipeline or agent is easy, but every AI engineer needs the right data processing layer, orchestration framework, and experimentation/monitoring tool in order to take these applications to production. LlamaTrace by Arize offers the richest toolkit we\u2019ve seen in enabling developers to observe, debug, and evaluate every granular step of a very complex LLM workflow, and it nicely complements the production-ready data platform and orchestration framework that LlamaCloud and LlamaIndex offer.\u201d - Jerry Liu, CEO of LlamaIndex\n </blockquote>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   About Arize AI\n  </strong>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Arize AI is an AI observability and LLM evaluation platform that helps teams deliver and maintain more successful AI in production. Arize\u2019s automated monitoring and observability platform allows teams to quickly detect issues when they emerge, troubleshoot why they happened, and improve overall performance across both traditional ML and generative use cases. Arize is headquartered in Berkeley, CA.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3862, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7dafcb22-17ae-4dd3-8487-0b01dcb3f669": {"__data__": {"id_": "7dafcb22-17ae-4dd3-8487-0b01dcb3f669", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/automate-online-tasks-with-multion-and-llamaindex.html", "file_name": "automate-online-tasks-with-multion-and-llamaindex.html", "file_type": "text/html", "file_size": 10641, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/automate-online-tasks-with-multion-and-llamaindex.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/automate-online-tasks-with-multion-and-llamaindex.html", "file_name": "automate-online-tasks-with-multion-and-llamaindex.html", "file_type": "text/html", "file_size": 10641, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "894afdbb92b37830e2ac1215218827f6edaea406b370ad1f3b9d036dabbad82f", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   Introduction\n  </strong>\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  MultiOn is an AI agents platform designed to facilitate the autonomous completion of tasks in any web environment. It empowers developers to build AI agents that can manage online activities from start to finish, handling everything from simple data retrieval to complex interactions.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  LlamaIndex complements this by providing an orchestration framework that bridges the gap between private and public data essential for building applications with Large Language Models. It facilitates data ingestion, indexing, and querying, making it indispensable for developers looking to leverage generative AI.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In this article, we'll demonstrate how MultiOn's capabilities can be seamlessly integrated within the LlamaIndex framework, showcasing a practical application that leverages both technologies to automate and streamline web interactions.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   Technical walkthrough: Integrating MultiOn with LlamaIndex\n  </strong>\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Let\u2019s explore a practical example where MultiOn and LlamaIndex work in tandem to manage email interactions and web browsing.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   Step 1: Setting Up the Environment\n  </strong>\n  We begin by setting up our AI agent with the necessary configurations and API keys:\n </p>\n <pre><code><span class=\"hljs-keyword\">import</span> openai\n<span class=\"hljs-keyword\">from</span> llama_index.agent.openai <span class=\"hljs-keyword\">import</span> OpenAIAgent\nopenai.api_key = <span class=\"hljs-string\">\"sk-your-key\"</span>\n\n<span class=\"hljs-keyword\">from</span> llama_index.tools.multion <span class=\"hljs-keyword\">import</span> MultionToolSpec\nmultion_tool = MultionToolSpec(api_key=<span class=\"hljs-string\">\"your-multion-key\"</span>)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   Step 2: Integrating Gmail Search Tool\n  </strong>\n  Next, we integrate a Gmail search tool to help our agent fetch and analyze emails, providing the necessary context for further actions:\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index.tools.google <span class=\"hljs-keyword\">import</span> GmailToolSpec\n<span class=\"hljs-keyword\">from</span> llama_index.core.tools.ondemand_loader_tool <span class=\"hljs-keyword\">import</span> OnDemandLoaderTool\n\ngmail_tool = GmailToolSpec()\ngmail_loader_tool = OnDemandLoaderTool.from_tool(\n    gmail_tool.to_tool_list()[<span class=\"hljs-number\">1</span>],\n    name=<span class=\"hljs-string\">\"gmail_search\"</span>,\n    description=<span class=\"hljs-string\">\"\"\"\n         This tool allows you to search the users gmail inbox and give directions for how to summarize or process the emails\n\n        You must always provide a query to filter the emails, as well as a query_str to process the retrieved emails.\n        All parameters are required\n        \n        If you need to reply to an email, ask this tool to build the reply directly\n        Examples:\n            query='from:adam subject:dinner', max_results=5, query_str='Where are adams favourite places to eat'\n            query='dentist appointment', max_results=1, query_str='When is the next dentist appointment'\n            query='to:jerry', max_results=1, query_str='summarize and then create a response email to jerrys latest email'\n            query='is:inbox', max_results=5, query_str='Summarize these emails'\n    \"\"\"</span>\n)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   Step 3: Initialize agent\n  </strong>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Initialise the agent with tools and a system prompt\n </p>\n <pre><code>agent = OpenAIAgent.from_tools(\n    [*multion_tool.to_tool_list(), gmail_loader_tool],\n    system_prompt=<span class=\"hljs-string\">\"\"\"\n\t    You are an AI agent that assists the user in crafting email responses based on previous conversations.\n\t    \n\t    The gmail_search tool connects directly to an API to search and retrieve emails, and answer questions based on the content.\n\t    The browse tool allows you to control a web browser with natural language to complete arbitrary actions on the web.\n\t    \n\t    Use these two tools together to gain context on past emails and respond to conversations for the user.\n    \"\"\"</span>\n)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   Step 4: Agent Execution Flow\n  </strong>\n  With our tools integrated, the agent is now equipped to perform a series of tasks:\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   1. Search and Summarize Emails\n  </strong>\n  : The agent uses LlamaIndex's Gmail tool to fetch relevant emails and summarize the content, providing a basis for drafting a response.\n </p>\n <pre><code><span class=\"hljs-built_in\">print</span>(agent.chat(<span class=\"hljs-string\">\"browse to the latest email from Julian and open the email\"</span>))</code></pre>\n <pre><code>Added user message to memory: browse to the latest email from Julian and open the email\n=== Calling Function ===\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Browse to the latest email from Julian and open the email\"}\nPlease visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&amp;client_id=1054044249014.apps.googleusercontent.com&amp;redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&amp;scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.compose+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.readonly&amp;state=JSdsfdsi990sddsd&amp;access_type=offline\nGot output: Open the email from Julian to view the latest communication.\n========================\n \nI have opened the latest email from Julian for you to view. If you need any specific information or action to be taken, please let me know.</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   2. Generate Response\n  </strong>\n  : Based on the summarized information, the agent crafts an appropriate response to the email chain.\n </p>\n <pre><code><span class=\"hljs-built_in\">print</span>(agent.chat(\n\t<span class=\"hljs-string\">\"Summarize the email chain with julian and create a response to the last email that confirms all the details\"</span>\n))</code></pre>\n <pre><code>Added user message to memory: Summarize the email chain with julian and create a response to the last email that confirms all the details\n=== Calling Function ===\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Summarize the email chain with Julian and create a response to the last email confirming all the details\"}\nGot output: The email chain with Julian involved a change in an event scheduled for Friday, August 6, 2021, from 15:30 to 16:00 United Kingdom Time on Google Meet. The instructions for joining were provided in the description. The email also included contact information for joining the meeting. Julian and Nassar were listed as attendees, with Julian being the organizer. The email was authenticated and passed SPF and DKIM checks.\n\nIn response to the last email, I would confirm all the details of the event change, reiterating the date, time, platform (Google Meet), and any specific instructions provided. I would express gratitude for the update and confirm attendance at the revised event timing.\n========================\n\nBased on the email chain with Julian, here is a summary:\n- The event scheduled for Friday, August 6, 2021, has been changed from 15:30 to 16:00 United Kingdom Time on Google Meet.\n- Instructions for joining the meeting were provided in the email.\n- Attendees included Julian and Nassar, with Julian as the organizer.\n- The email passed SPF and DKIM checks.\n\nTo respond and confirm all the details, you can mention the revised event date and time, the platform (Google Meet), and express gratitude for the update. Confirm your attendance at the new timing. Let me know if you would like me to draft the response email for you.</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   3. Send Email through MultiOn\n  </strong>\n  : Finally, the generated response is passed to the MultiOn agent, which manages the action of sending the email through the web browser.\n </p>\n <pre><code><span class=\"hljs-built_in\">print</span>(agent.chat(\n\t<span class=\"hljs-string\">\"pass the entire generated email to the browser and have it send the email as a reply to the chain\"</span>\n))</code></pre>\n <pre><code>Added user message to memory: pass the entire generated email to the browser and have it send the email as a reply to the chain\n=== Calling Function ===\nCalling function: browse with args: {\"cmd\": \"Compose a reply email to Julian confirming the event change to Fri 6 Aug 2021 from 15:30 to 16:00 UK Time on Google Meet. Express readiness to attend and thank Julian for the details.\"}\nGot output: Email response sent to Julian\n========================</code></pre>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Next Steps\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  MultiOn is an officially supported tool on LlamaHub, the central page for all LlamaIndex integrations (from tools to LLMs to vector stores). Check out the\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://llamahub.ai/l/tools/llama-index-tools-multion?from=\" rel=\"noreferrer noopener\">\n   LlamaHub page\n  </a>\n  here.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  If you\u2019re interested in running through this tutorial on building a browser + Gmail-powered agent yourself, check out our\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/llama-index-integrations/tools/llama-index-tools-multion/examples/multion.ipynb\" rel=\"noreferrer noopener\">\n   notebook\n  </a>\n  .\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The integration of MultiOn and LlamaIndex offers a powerful toolkit for developers aiming to automate and streamline online tasks. As these technologies evolve, they will continue to unlock new potentials in AI application, significantly impacting how developers interact with digital environments and manage data.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 10636, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "542e4fa9-6577-4c98-a8c7-72ca69e69993": {"__data__": {"id_": "542e4fa9-6577-4c98-a8c7-72ca69e69993", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/batch-inference-with-mymagic-ai-and-llamaindex.html", "file_name": "batch-inference-with-mymagic-ai-and-llamaindex.html", "file_type": "text/html", "file_size": 15092, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/batch-inference-with-mymagic-ai-and-llamaindex.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/batch-inference-with-mymagic-ai-and-llamaindex.html", "file_name": "batch-inference-with-mymagic-ai-and-llamaindex.html", "file_type": "text/html", "file_size": 15092, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "578651e8f54de7132014b1c90fc522a3201e4e626534ed4cbf1b34732f19d514", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <em>\n   This is a guest post from MyMagic AI.\n  </em>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://mymagic.ai/\" rel=\"noreferrer noopener\">\n   MyMagic AI\n  </a>\n  allows processing and analyzing large datasets with AI. MyMagic AI offers a powerful API for\n  <em>\n   batch\n  </em>\n  inference (also known as\n  <em>\n   offline\n  </em>\n  or\n  <em>\n   delayed\n  </em>\n  inference) that brings various open-source Large Language Models (LLMs) such as Llama 70B, Mistral 7B, Mixtral 8x7B, CodeLlama70b, and advanced Embedding models to its users. Our framework is designed to perform data extraction, summarization, categorization, sentiment analysis, training data generation, and embedding, to name a few. And now it's integrated directly into LlamaIndex!\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Part 1: batch inference\n </h2>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  How It Works:\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   1. Setup\n  </strong>\n  :\n </p>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Organize Your Data in an AWS S3 or GCS Bucket:\n   <ol>\n    <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n     Create a folder using your user ID assigned to you upon registration.\n    </li>\n    <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n     Inside that folder, create another folder (called a \"session\") to store all the files you need for your tasks.\n    </li>\n   </ol>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Purpose of the 'Session' Folder:\n   <ol>\n    <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n     This \"Session\" folder keeps your files separate from others, making sure that your tasks run on the right set of files. You can name your session subfolder anything you like.\n    </li>\n   </ol>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Granting Access to MyMagic AI:\n   <ol>\n    <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n     To allow MyMagic AI to securely access your files in the cloud, follow the setup instructions provided in the\n     <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.mymagic.ai/\" rel=\"noreferrer noopener\">\n      MyMagic AI documentation\n     </a>\n     .\n    </li>\n   </ol>\n  </li>\n </ol>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   2. Install\n  </strong>\n  : Install both MyMagic AI\u2019s API integration and LlamaIndex library:\n </p>\n <pre><code>pip install llama-index\npip install llama-index-llms-mymagic</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   3. API Request:\n  </strong>\n  The llamaIndex library is a wrapper around MyMagic AI\u2019s API. What it does under the hood is simple: it sends a POST request to the MyMagic AI API while specifying the model, storage provider, bucket name, session name, and other necessary details.\n </p>\n <pre><code><span class=\"hljs-keyword\">import</span> asyncio\n<span class=\"hljs-keyword\">from</span> llama_index.llms.mymagic <span class=\"hljs-keyword\">import</span> MyMagicAI\n\nllm = MyMagicAI(\n    api_key=<span class=\"hljs-string\">\"user_...\"</span>, <span class=\"hljs-comment\"># provided by MyMagic AI upon sign-up</span>\n    storage_provider=<span class=\"hljs-string\">\"s3\"</span>,\n    bucket_name=<span class=\"hljs-string\">\"batch-bucket\"</span>, <span class=\"hljs-comment\"># you may name anything</span>\n    session=<span class=\"hljs-string\">\"my-session\"</span>,\n    role_arn=<span class=\"hljs-string\">\"arn:aws:iam::&lt;your account id&gt;:role/mymagic-role\"</span>,\n    system_prompt=<span class=\"hljs-string\">\"You are an AI assistant that helps to summarize the documents without essential loss of information\"</span>, <span class=\"hljs-comment\"># default prompt at https://docs.mymagic.ai/api-reference/endpoint/create</span>\n    region=<span class=\"hljs-string\">\"eu-west-2\"</span>,\n)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We have designed the integration to allow the user to set up the bucket and data together with the system prompt when instantiating the llm object. Other inputs, e.g. question (i.e. your prompt), model and max_tokens are dynamic requirements when submitting complete and acomplete requests.\n </p>\n <pre><code>resp = llm.complete(\n    question=<span class=\"hljs-string\">\"Summarise this in one sentence.\"</span>,\n    model=<span class=\"hljs-string\">\"mixtral8x7\"</span>, \n    max_tokens=<span class=\"hljs-number\">20</span>,  <span class=\"hljs-comment\"># default is 10</span>\n)\n<span class=\"hljs-built_in\">print</span>(resp)\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">main</span>():\n    aresp = <span class=\"hljs-keyword\">await</span> llm.acomplete(\n        question=<span class=\"hljs-string\">\"Summarize this in one sentence.\"</span>,\n        model=<span class=\"hljs-string\">\"llama7b\"</span>,\n        max_tokens=<span class=\"hljs-number\">20</span>,\n    )\n    <span class=\"hljs-built_in\">print</span>(aresp)\n\nasyncio.run(main())</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  This dynamic entry allows developers to experiment with different prompts and models in their workflow while also controlling for model output to cap their spending limit. MyMagic AI\u2019s backend supports both synchronous requests (complete) and asynchronous requests (acomplete). It is advisable, however, to use our async endpoints as much as possible as batch jobs are inherently asynchronous with potentially long processing times (depending on the size of your data).\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Currently, we do not support chat or achat methods as our API is not designed for real-time interactive experience. However, we are planning to add those methods in the future that will function in a \u201cbatch way\u201d. The user queries will be aggregated and appended as one prompt (to give the chat context) and sent to all files at once.\n </p>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  Use Cases\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  While there are myriads of use cases, here we provide a few to help motivate our users. Feel free to embed our API in your workflows that are good fit for batch processing.\n </p>\n <h4 class=\"Text_text__zPO0D Text_text-size-32__koGps\">\n  1. Extraction\n </h4>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Imagine needing to extract specific information from millions of files stored in a bucket. Information from all files will be extracted with one API call instead of a million sequential ones.\n </p>\n <h4 class=\"Text_text__zPO0D Text_text-size-32__koGps\">\n  2. Classification\n </h4>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  For businesses looking to classify customer reviews such as positive, neutral, and negative. With one request you can start processing the requests over the weekend and get them ready by Monday morning.\n </p>\n <h4 class=\"Text_text__zPO0D Text_text-size-32__koGps\">\n  3. Embedding\n </h4>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Embedding text files for further machine learning applications is another powerful use case of MyMagic AI's API. You will be ready for your vector db in a matter of days not weeks.\n </p>\n <h4 class=\"Text_text__zPO0D Text_text-size-32__koGps\">\n  4. Training (Fine-tuning) Data Generation\n </h4>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Imagine generating thousands of synthetic data for your fine-tuning tasks. With MyMagic AI\u2019s API, you can reduce the generation time by a factor of 5-10x compared to GPT-3.5.\n </p>\n <h4 class=\"Text_text__zPO0D Text_text-size-32__koGps\">\n  5. Transcription\n </h4>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  MyMagic AI\u2019s API supports different types of files, so it is also easy to batch transcribe many mp3 or mp4 files in your bucket.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Part 2: Integration with LlamaIndex\u2019s RAG Pipeline\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The output from batch inference processes, often voluminous, can seamlessly integrate into LlamaIndex's RAG pipeline for effective data storage and retrieval.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  This section demonstrates how to use the Llama3 model from the Ollama library coupled with BGE embedding to manage information storage and execute queries. Please ensure the following prerequisites are installed and Llama3 model is pulled:\n </p>\n <pre><code>pip install llama-index-embeddings-huggingface\ncurl -fsSL https://ollama.com/install.sh | sh\nollama pull llama3</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  For this demo, we have run a batch summarization job on 5 Amazon reviews (but this might be millions in some real scenarios) and saved the results as reviews_1_5.json:\n </p>\n <pre><code>{\n  \"id_review1\": {\n    \"query\": \"Summarize the document!\",\n    \"output\": \"The document describes a family with a young boy who believes there is a zombie in his closet, while his parents are constantly fighting. The movie is criticized for its inconsistent genre, described as a slow-paced drama with occasional thriller elements. The review praises the well-playing parents and the decent dialogs but criticizes the lack of a boogeyman-like horror element. The overall rating is 3 out of 10.\"\n  },\n  \"id_review2\": {\n    \"query\": \"Summarize the document!\",\n    \"output\": \"The document is a positive review of a light-hearted Woody Allen comedy. The reviewer praises the witty dialogue, likable characters, and Woody Allen's control over his signature style. The film is noted for making the reviewer laugh more than any recent Woody Allen comedy and praises Scarlett Johansson's performance. It concludes by calling the film a great comedy to watch with friends.\"\n  },\n  \"id_review3\": {\n    \"query\": \"Summarize the document!\",\n    \"output\": \"The document describes a well-made film about one of the great masters of comedy, filmed in an old-time BBC fashion that adds realism. The actors, including Michael Sheen, are well-chosen and convincing. The production is masterful, showcasing realistic details like the fantasy of the guard and the meticulously crafted sets of Orton and Halliwell's flat. Overall, it is a terrific and well-written piece.\"\n  },\n  \"id_review4\": {\n    \"query\": \"Summarize the document!\",\n    \"output\": \"Petter Mattei's 'Love in the Time of Money' is a visually appealing film set in New York, exploring human relations in the context of money, power, and success. The characters, played by a talented cast including Steve Buscemi and Rosario Dawson, are connected in various ways but often unaware of their shared links. The film showcases the different stages of loneliness experienced by individuals in a big city. Mattei successfully portrays the world of these characters, creating a luxurious and sophisticated look. The film is a modern adaptation of Arthur Schnitzler's play on the same theme. Mattei's work is appreciated, and viewers look forward to his future projects.\"\n  },\n  \"id_review5\": {\n    \"query\": \"Summarize the document!\",\n    \"output\": \"The document describes the TV show 'Oz', set in the Oswald Maximum Security State Penitentiary. Known for its brutality, violence, and lack of privacy, it features an experimental section of the prison called Em City, where all the cells have glass fronts and face inwards. The show goes where others wouldn't dare, featuring graphic violence, injustice, and the harsh realities of prison life. The viewer may become comfortable with uncomfortable viewing if they can embrace their darker side.\"\n  },\n  \"token_count\": 3391\n}\n</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Now let\u2019s embed and store this document and ask questions using LlamaIndex\u2019s query engine. Bring in our dependencies:\n </p>\n <pre><code><span class=\"hljs-keyword\">import</span> os\n\n<span class=\"hljs-keyword\">from</span> llama_index.embeddings.huggingface <span class=\"hljs-keyword\">import</span> HuggingFaceEmbedding\n<span class=\"hljs-keyword\">from</span> llama_index.core.indices.vector_store <span class=\"hljs-keyword\">import</span> VectorStoreIndex\n<span class=\"hljs-keyword\">from</span> llama_index.core.settings <span class=\"hljs-keyword\">import</span> Settings\n<span class=\"hljs-keyword\">from</span> llama_index.core.readers <span class=\"hljs-keyword\">import</span> SimpleDirectoryReader\n<span class=\"hljs-keyword\">from</span> llama_index.llms.ollama <span class=\"hljs-keyword\">import</span> Ollama</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Configure the embedding model and Llama3 model\n </p>\n <pre><code>embed_model = HuggingFaceEmbedding(model_name=<span class=\"hljs-string\">\"BAAI/bge-base-en-v1.5\"</span>)\nllm = Ollama(model=<span class=\"hljs-string\">\"llama3\"</span>, request_timeout=<span class=\"hljs-number\">300.0</span>)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Update settings for the indexing pipeline:\n </p>\n <pre><code>Settings.llm = llm\nSettings.embed_model = embed_model\nSettings.chunk_size = <span class=\"hljs-number\">512</span> <span class=\"hljs-comment\"># This parameter defines the size of text chunks for embedding</span>\n\ndocuments = SimpleDirectoryReader(<span class=\"hljs-string\">\"reviews_1_5.json\"</span>).load_data() <span class=\"hljs-comment\">#Modify path for your case</span></code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Now create our index, our query engine and run a query:\n </p>\n <pre><code>index = VectorStoreIndex.from_documents(documents, show_progress=<span class=\"hljs-literal\">True</span>)\n\nquery_engine = index.as_query_engine(similarity_top_k=<span class=\"hljs-number\">3</span>)\n\nresponse = query_engine.query(<span class=\"hljs-string\">\"What is the least favourite movie?\"</span>)\n<span class=\"hljs-built_in\">print</span>(response)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Output:\n </p>\n <pre><code>Based on query results, the least favourite movie is: review 1 with a rating of 3 out of 10.</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Now we know that the review 1 is the least favorite movie among these reviews.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Next Steps\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  This shows how batch inference combined with real-time inference can be a powerful tool for analyzing, storing and retrieving information from massive amounts of data.\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://vivacious-river-18.authkit.app/sign-up?redirect_uri=https://api.mymagic.ai/workspace\" rel=\"noreferrer noopener\">\n   Get started with MyMagic AI\u2019s API\n  </a>\n  today!\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 15069, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f6ca503-a707-49a5-8bc7-a5e0d6a923be": {"__data__": {"id_": "4f6ca503-a707-49a5-8bc7-a5e0d6a923be", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/becoming-proficient-in-document-extraction-32aa13046ed5.html", "file_name": "becoming-proficient-in-document-extraction-32aa13046ed5.html", "file_type": "text/html", "file_size": 14667, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/becoming-proficient-in-document-extraction-32aa13046ed5.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/becoming-proficient-in-document-extraction-32aa13046ed5.html", "file_name": "becoming-proficient-in-document-extraction-32aa13046ed5.html", "file_type": "text/html", "file_size": 14667, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "40ea2774b8d6c54eb4ba9c0aa74af21bea7224c49ba04cfeebcd4d009a85fc77", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h2>\n  <strong>\n   Introduction\n  </strong>\n </h2>\n <p>\n  In the domain of document handling, accurately extracting crucial information\n  from images has posed an enduring obstacle. Despite Optical Character\n  Recognition (OCR) advancements in converting images to editable text, it faces\n  numerous intricacies with diverse document formats and quality. Here enters\n  Zephyr 7b LLM, a pioneering remedy that, coupled with LlamaIndex, directly\n  addresses these hurdles, heralding a transformative era in image-based\n  document extraction.\n </p>\n <figure>\n  <figcaption>\n   Source:\n   <a href=\"https://corca.substack.com/p/top-llm-papers-of-the-week-95e?utm_source=profile&amp;utm_medium=reader2\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Zephyr-llama-index\n   </a>\n  </figcaption>\n </figure>\n <h2>\n  <strong>\n   The OCR Dilemma: Obstacles and Constraints Optical Character\n  </strong>\n </h2>\n <p>\n  Recognition (OCR), though potent, faces impediments such as:\n </p>\n <ol>\n  <li>\n   <strong>\n    Diverse Document Formats\n   </strong>\n   : Documents exhibit intricate\n    layouts, fonts, and structures, posing challenges for traditional OCR to\n    precisely interpret and extract information.\n  </li>\n  <li>\n   <strong>\n    Quality and Clarity\n   </strong>\n   : Images with low resolution,\n    blurriness, or skewed angles hinder OCR\u2019s accuracy in deciphering text.\n  </li>\n  <li>\n   <strong>\n    Handwritten and Cursive Content\n   </strong>\n   : OCR often struggles with\n    handwritten text or cursive fonts, resulting in errors or incomplete\n    extraction.\n  </li>\n  <li>\n   <strong>\n    Multilingual Complexity\n   </strong>\n   : Processing documents in multiple\n    languages poses a challenge for OCR systems lacking proficiency in\n    recognizing and extracting varied linguistic content.\n  </li>\n </ol>\n <figure>\n  <figcaption>\n   Source: Created by Author using MidJourney\n  </figcaption>\n </figure>\n <h2>\n  <strong>\n   Zephyr 7b LLM: Narrowing the Divide\n  </strong>\n </h2>\n <p>\n  Zephyr 7b LLM revolutionizes the landscape by tackling these inherent\n  constraints of OCR technology:\n </p>\n <ol>\n  <li>\n   <strong>\n    Advanced Machine Learning Algorithms:\n   </strong>\n  </li>\n </ol>\n <p>\n  Employing state-of-the-art machine learning algorithms, Zephyr 7b LLM\n  undergoes extensive training with diverse document formats and languages. This\n  equips it to adapt and learn from various document structures, resulting in\n  heightened accuracy and robust extraction capabilities.\n </p>\n <p>\n  <strong>\n   2. Contextual Comprehension:\n  </strong>\n </p>\n <p>\n  Diverging from conventional OCR, Zephyr 7b LLM doesn\u2019t merely identify\n  individual characters; it comprehends the context in which these characters\n  exist. This contextual understanding significantly reduces errors, ensuring\n  precise extraction even from intricate document layouts.\n </p>\n <p>\n  <strong>\n   3. Adaptive Image Processing:\n  </strong>\n </p>\n <p>\n  The fusion with LlamaIndex amplifies Zephyr 7b LLM\u2019s ability to handle images\n  of varying resolutions or qualities. Leveraging adaptive image processing\n  techniques, it rectifies distortions, enhances clarity, and optimizes images\n  for meticulous OCR analysis.\n </p>\n <p>\n  <strong>\n   4. Multilingual Proficiency:\n  </strong>\n </p>\n <p>\n  Zephyr 7b LLM surpasses language barriers. Its multilingual proficiency\n  facilitates seamless content extraction from documents in various languages,\n  extending global accessibility for businesses dealing with multilingual\n  documentation.\n </p>\n <figure>\n  <figcaption>\n   Source: Created by Author using MidJourney\n  </figcaption>\n </figure>\n <h2>\n  <strong>\n   Implementation of Code\n  </strong>\n </h2>\n <p>\n  The collaboration between Zephyr 7b LLM and LlamaIndex signifies a pivotal\n  transformation in document extraction. By merging Zephyr\u2019s advanced OCR\n  capabilities with LlamaIndex\u2019s image enhancement and data organization\n  features, this integration presents a comprehensive solution:\n </p>\n <ol>\n  <li>\n   <strong>\n    Augmented Precision\n   </strong>\n   : The fusion of Zephyr\u2019s machine\n    learning expertise and LlamaIndex\u2019s image enhancement markedly heightens the\n    accuracy of extracted data, diminishing errors and enhancing overall\n    efficiency.\n  </li>\n  <li>\n   <strong>\n    Efficient Workflow\n   </strong>\n   : Users experience an optimized workflow,\n    enabling swift extraction and conversion of image-based documents into\n    structured, actionable data, facilitating expedited decision-making\n    processes.\n  </li>\n  <li>\n   <strong>\n    Adaptability Across Document Varieties\n   </strong>\n   : This integration\n    empowers users to handle diverse document formats and languages\n    effortlessly, granting access to previously challenging document types for\n    extraction and analysis.\n  </li>\n </ol>\n <figure>\n  <figcaption>\n   Source: Image created by Author using MidJourney\n  </figcaption>\n </figure>\n <p>\n  <strong>\n   Step 1: Install and Import Libraries\n  </strong>\n </p>\n <pre><span>!pip install llama-index transformers accelerate sentencepiece bitsandbytes -q</span></pre>\n <p>\n  <strong>\n   Step 2: Load the Model\n  </strong>\n </p>\n <pre><span>import torchfrom transformers import BitsAndBytesConfigfrom llama_index.prompts import PromptTemplatefrom llama_index.llms import HuggingFaceLLMquantization_config = BitsAndBytesConfig(    load_in_4bit=True,    bnb_4bit_compute_dtype=torch.float16,    bnb_4bit_quant_type=\"nf4\",    bnb_4bit_use_double_quant=True,)def messages_to_prompt(messages):  prompt = \"\"  for message in messages:    if message.role == 'system':      prompt += f\"&lt;|system|&gt;\\n{message.content}&lt;/s&gt;\\n\"    elif message.role == 'user':      prompt += f\"&lt;|user|&gt;\\n{message.content}&lt;/s&gt;\\n\"    elif message.role == 'assistant':      prompt += f\"&lt;|assistant|&gt;\\n{message.content}&lt;/s&gt;\\n\"  # ensure we start with a system prompt, insert blank if needed  if not prompt.startswith(\"&lt;|system|&gt;\\n\"):    prompt = \"&lt;|system|&gt;\\n&lt;/s&gt;\\n\" + prompt  # add final assistant prompt  prompt = prompt + \"&lt;|assistant|&gt;\\n\"  return promptllm = HuggingFaceLLM(    model_name=\"HuggingFaceH4/zephyr-7b-alpha\",    tokenizer_name=\"HuggingFaceH4/zephyr-7b-alpha\",    query_wrapper_prompt=PromptTemplate(\"&lt;|system|&gt;\\n&lt;/s&gt;\\n&lt;|user|&gt;\\n{query_str}&lt;/s&gt;\\n&lt;|assistant|&gt;\\n\"),    context_window=3900,    max_new_tokens=2000,    model_kwargs={\"quantization_config\": quantization_config},    # tokenizer_kwargs={},    generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},    messages_to_prompt=messages_to_prompt,    device_map=\"auto\",)</span></pre>\n <pre><span>from llama_index import ServiceContext, set_global_service_contextservice_context = ServiceContext.from_defaults(llm=llm, embed_model=\"local:BAAI/bge-small-en-v1.5\")</span></pre>\n <pre><span>set_global_service_context(service_context)</span></pre>\n <p>\n  <strong>\n   Step 3: Storing your index\n  </strong>\n </p>\n <pre><span>from llama_index import SimpleDirectoryReader, VectorStoreIndexfrom llama_index.readers.file.base import (    DEFAULT_FILE_READER_CLS,    ImageReader,)from llama_index.response.notebook_utils import (    display_response,    display_image,)from llama_index.indices.query.query_transform.base import (    ImageOutputQueryTransform,)filename_fn = lambda filename: {\"file_name\": filename}llama_reader = SimpleDirectoryReader(    input_dir=\"/content/llama\",    file_metadata=filename_fn,)llama_documents = llama_reader.load_data()llama_index = VectorStoreIndex.from_documents(llama_documents)</span></pre>\n <p>\n  <strong>\n   Step 4: Query\n  </strong>\n  <a href=\"https://github.com/andysingal/CV_public/tree/main/zephyr-7b-alpha\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    Transformations\n   </strong>\n  </a>\n </p>\n <pre><span>from llama_index.query_engine import TransformQueryEnginequery_engine = llama_index.as_query_engine(similarity_top_k=2)query_engine = TransformQueryEngine(    query_engine, query_transform=ImageOutputQueryTransform(width=400))llama_response = query_engine.query(    \"Show an image to illustrate how tree index works and explain briefly\",)display_response(llama_response)#OutputFinal Response: I am not capable of displaying images. however, i can provide you with an explanation of how tree index works.tree index is a data structure that organizes data in a hierarchical manner, similar to a tree. it is commonly used in databases to improve query performance.when querying a tree index, the process involves traversing from the root node down to the leaf nodes. the number of child nodes chosen per parent node is determined by the child_branch_factor parameter.for example, if child_branch_factor=1, a query will choose one child node given a parent node. if child_branch_factor=2, a query will choose two child nodes per parent.the following image illustrates how a tree index works:! Tree Index Examplein this example, the tree index is built from a set of nodes (which become leaf nodes in this tree). when querying this index, the process involves traversing from the root node down to the leaf nodes. for instance, if we want to find a specific node with the value \"x\", we would start at the root node and follow the left branch (since \"x\" is less than \"a\") to the next level. we would then follow the left branch again to reach the leaf node with the value \"x\".i hope this helps clarify how tree index works!</span></pre>\n <p>\n  <strong>\n   Step 5: Lets read the\n  </strong>\n  <a href=\"https://github.com/andysingal/CV_public/tree/main/zephyr-7b-alpha\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    receipts\n   </strong>\n  </a>\n </p>\n <pre><span>from llama_index.readers.file.base import DEFAULT_FILE_READER_CLSfrom llama_index.readers.file.image_reader import ImageReaderimage_parser =ImageReader(    keep_image=True,    parse_text=True    )file_extractor = DEFAULT_FILE_READER_CLSfile_extractor.update({    \".jpg\": image_parser,    \".png\": image_parser,    \".jpeg\": image_parser,    })receipt_reader = SimpleDirectoryReader(    input_dir=\"/content/data\",    file_metadata=filename_fn,    file_extractor=file_extractor,)receipt_documents = receipt_reader.load_data()print(len(receipt_documents))#Output3</span></pre>\n <pre><span>receipts_index = VectorStoreIndex.from_documents(receipt_documents)from llama_index.query_engine import TransformQueryEnginequery_engine = receipts_index.as_query_engine()receipts_response = query_engine.query(    \"When was the last time I went to RESTAURANT and how much did I spend? this data is in your latest vector index.\",)display_response(receipts_response)# Output Final Response: Based on the given context information, the last time the querying individual went to RESTAURANT was on July 5, 2019, and they spent $164.00.</span></pre>\n <h2>\n  Conclusion\n </h2>\n <p>\n  In summary, the fusion of Zephyr 7b LLM and LlamaIndex initiates a new chapter\n  in image-based document extraction. Beyond addressing OCR\u2019s inherent\n  challenges, it enhances the precision and efficiency of data extraction from\n  images, fostering improved productivity and decision-making in\n  document-focused workflows.\n </p>\n <p>\n  \u201cStay connected and support my work through various platforms:\n </p>\n <ul>\n  <li>\n   <a href=\"https://medium.com/u/8df3bf3c40ae\" rel=\"noopener\">\n    GitHub\n   </a>\n   : For\n    all my open-source projects and Notebooks, you can visit my GitHub profile\n    at\n   <a href=\"https://github.com/andysingal\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    https://github.com/andysingal\n   </a>\n   . If you find my content valuable, don\u2019t hesitate to leave a star.\n  </li>\n  <li>\n   Patreon: If you\u2019d like to provide additional support, you can consider\n    becoming a patron on my Patreon page at\n   <a href=\"https://www.patreon.com/AndyShanu\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    https://www.patreon.com/AndyShanu\n   </a>\n   .\n  </li>\n  <li>\n   <a href=\"https://medium.com/u/504c7870fdb6\" rel=\"noopener\">\n    Medium\n   </a>\n   : You\n    can read my latest articles and insights on Medium at\n   <a href=\"https://medium.com/@andysingal\" rel=\"noopener\">\n    https://medium.com/@andysingal\n   </a>\n   .\n  </li>\n  <li>\n   <a href=\"https://medium.com/u/29b47aa8cce3\" rel=\"noopener\">\n    The Kaggle\n   </a>\n   :\n    Check out my Kaggle profile for data science and machine learning projects\n    at\n   <a href=\"https://www.kaggle.com/alphasingal\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    https://www.kaggle.com/alphasingal\n   </a>\n   .\n  </li>\n  <li>\n   <a href=\"https://medium.com/u/b1574f0c6c5e\" rel=\"noopener\">\n    Hugging Face\n   </a>\n   :\n    For natural language processing and AI-related projects, you can explore my\n    Huggingface profile at\n   <a href=\"https://huggingface.co/Andyrasika\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    https://huggingface.co/Andyrasika\n   </a>\n   .\n  </li>\n  <li>\n   YouTube: To watch my video content, visit my YouTube channel at\n   <a href=\"https://www.youtube.com/@andy111007\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    https://www.youtube.com/@andy111007\n   </a>\n   .\n  </li>\n  <li>\n   LinkedIn: To stay updated on my latest projects and posts, you can follow me\n    on LinkedIn. Here is the link to my profile:\n   <a href=\"https://www.linkedin.com/in/ankushsingal/.%22\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    https://www.linkedin.com/in/ankushsingal/.\"\n   </a>\n  </li>\n </ul>\n <p>\n  Requests and questions: If you have a project in mind that you\u2019d like me to\n  work on or if you have any questions about the concepts I\u2019ve explained, don\u2019t\n  hesitate to let me know. I\u2019m always looking for new ideas for future Notebooks\n  and I love helping to resolve any doubts you might have.\n </p>\n <p>\n  Remember, each \u201cLike\u201d, \u201cShare\u201d, and \u201cStar\u201d greatly contributes to my work and\n  motivates me to continue producing more quality content. Thank you for your\n  support!\n </p>\n <p>\n  If you enjoyed this story, feel free\n  <a href=\"https://medium.com/@andysingal\" rel=\"noopener\">\n   to subscribe\n  </a>\n  to Medium, and you will get notifications when my new articles will be\n  published, as well as full access to thousands of stories from other authors.\n </p>\n <p>\n  Resource:\n </p>\n <ul>\n  <li>\n   <a href=\"https://github.com/andysingal/CV_public/tree/main/zephyr-7b-alpha\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Data used for above code\n   </a>\n  </li>\n  <li>\n   <a href=\"https://gpt-index.readthedocs.io/en/stable/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    llama-index\n   </a>\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 14624, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "721d9c47-002c-446a-b063-8586ab40dfe4": {"__data__": {"id_": "721d9c47-002c-446a-b063-8586ab40dfe4", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.html", "file_name": "boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.html", "file_type": "text/html", "file_size": 21486, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.html", "file_name": "boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.html", "file_type": "text/html", "file_size": 21486, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "bb868d306d901b7bc178f0192989d9c2ecb13c290871d38a7a72e6199f141d31", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  <strong>\n   UPDATE\n  </strong>\n  : The pooling method for the Jina AI embeddings has been adjusted to use mean pooling, and the results have been updated accordingly. Notably, the\n  <code class=\"cw oi oj ok ol b\">\n   JinaAI-v2-base-en\n  </code>\n  with\n  <code class=\"cw oi oj ok ol b\">\n   bge-reranker-large\n  </code>\n  now exhibits a Hit Rate of 0.938202 and an MRR (Mean Reciprocal Rank) of 0.868539 and with\n  <code class=\"cw oi oj ok ol b\">\n   CohereRerank\n  </code>\n  exhibits a Hit Rate of 0.932584, and an MRR of 0.873689.\n </p>\n <p>\n  When building a Retrieval Augmented Generation (RAG) pipeline, one key component is the Retriever. We have a variety of embedding models to choose from, including OpenAI, CohereAI, and open-source sentence transformers. Additionally, there are several rerankers available from CohereAI and sentence transformers.\n </p>\n <p>\n  But with all these options, how do we determine the best mix for top-notch retrieval performance? How do we know which embedding model fits our data best? Or which reranker boosts our results the most?\n </p>\n <p>\n  In this blog post, we\u2019ll use the\n  <code class=\"cw oi oj ok ol b\">\n   Retrieval Evaluation\n  </code>\n  module from LlamaIndex to swiftly determine the best combination of embedding and reranker models. Let's dive in!\n </p>\n <p>\n  Let\u2019s first start with understanding the metrics available in\n  <code class=\"cw oi oj ok ol b\">\n   Retrieval Evaluation\n  </code>\n </p>\n <h1>\n  Understanding Metrics in Retrieval Evaluation:\n </h1>\n <p>\n  To gauge the efficacy of our retrieval system, we primarily relied on two widely accepted metrics:\n  <strong>\n   Hit Rate\n  </strong>\n  and\n  <strong>\n   Mean Reciprocal Rank (MRR)\n  </strong>\n  . Let\u2019s delve into these metrics to understand their significance and how they operate.\n </p>\n <p>\n  <strong>\n   Hit Rate:\n  </strong>\n </p>\n <p>\n  Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it\u2019s about how often our system gets it right within the top few guesses.\n </p>\n <p>\n  <strong>\n   Mean Reciprocal Rank (MRR):\n  </strong>\n </p>\n <p>\n  For each query, MRR evaluates the system\u2019s accuracy by looking at the rank of the highest-placed relevant document. Specifically, it\u2019s the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it\u2019s second, the reciprocal rank is 1/2, and so on.\n </p>\n <p>\n  Now that we\u2019ve established the scope and familiarized ourselves with the metrics, it\u2019s time to dive into the experiment. For a hands-on experience, you can also follow along using our\n  <a href=\"https://colab.research.google.com/drive/1TxDVA__uimVPOJiMEQgP5fwHiqgKqm4-?usp=sharing\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Google Colab Notebook\n  </a>\n </p>\n <h1>\n  Setting Up the Environment\n </h1>\n <pre><span class=\"py on gt ol b bf pz qa l qb qc\" id=\"df6f\">!pip install llama-index sentence-transformers cohere anthropic voyageai protobuf pypdf</span></pre>\n <h1>\n  Setting Up the Keys\n </h1>\n <pre><span class=\"py on gt ol b bf pz qa l qb qc\" id=\"6659\">openai_api_key = <span class=\"hljs-string\">'YOUR OPENAI API KEY'</span>\ncohere_api_key = <span class=\"hljs-string\">'YOUR COHEREAI API KEY'</span>\nanthropic_api_key = <span class=\"hljs-string\">'YOUR ANTHROPIC API KEY'</span>\nopenai.api_key = openai_api_key</span></pre>\n <h1>\n  Download the Data\n </h1>\n <p>\n  We will use Llama2 paper for this experiment. Let\u2019s download the paper.\n </p>\n <pre><span class=\"py on gt ol b bf pz qa l qb qc\" id=\"525a\">!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"llama2.pdf\"</span></pre>\n <h1>\n  Load the Data\n </h1>\n <p>\n  Let\u2019s load the data. We will use Pages from start to 36 for the experiment which excludes table of contents, references, and appendix.\n </p>\n <p>\n  This data was then parsed by converted to nodes, which represent chunks of data we\u2019d like to retrieve. We did use chunk_size as 512.\n </p>\n <pre><span class=\"py on gt ol b bf pz qa l qb qc\" id=\"dfd8\">documents = SimpleDirectoryReader(input_files=[\"llama2.pdf\"]).load_data()\n\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=512)\nnodes = node_parser.get_nodes_from_documents(documents)</span></pre>\n <h1>\n  Generating Question-Context Pairs:\n </h1>\n <p>\n  For evaluation purposes, we created a dataset of question-context pairs. This dataset can be seen as a set of questions and their corresponding context from our data. To remove bias for the evaluation of embedding(OpenAI/ CohereAI) and Reranker (CohereAI), we use Anthropic LLM to generate Question-Context Pairs.\n </p>\n <p>\n  Let\u2019s initialize a prompt template to generate question-context pairs.\n </p>\n <pre><span class=\"py on gt ol b bf pz qa l qb qc\" id=\"6e11\"># Prompt to generate questions\nqa_generate_prompt_tmpl = \"\"\"\\\nContext information is below.\n\n---------------------\n{context_str}\n---------------------\n\nGiven the context information and not prior knowledge.\ngenerate only questions based on the below query.\n\nYou are a Professor. Your task is to setup \\\n{num_questions_per_chunk} questions for an upcoming \\\nquiz/examination. The questions should be diverse in nature \\\nacross the document. The questions should not contain options, not start with Q1/ Q2. \\\nRestrict the questions to the context information provided.\\\n\"\"\"</span></pre>\n <pre><span class=\"py on gt ol b bf pz qa l qb qc\" id=\"13b2\">llm = Anthropic(api_key=anthropic_api_key)\nqa_dataset = generate_question_context_pairs(\n    nodes, llm=llm, num_questions_per_chunk=<span class=\"hljs-number\">2</span>\n)</span></pre>\n <p>\n  Function to filter out sentences such as \u2014\n  <code class=\"cw oi oj ok ol b\">\n   Here are 2 questions based on provided context\n  </code>\n </p>\n <pre><span class=\"py on gt ol b bf pz qa l qb qc\" id=\"b03a\"><span class=\"hljs-comment\"># function to clean the dataset</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">filter_qa_dataset</span>(<span class=\"hljs-params\">qa_dataset</span>):\n    <span class=\"hljs-string\">\"\"\"\n    Filters out queries from the qa_dataset that contain certain phrases and the corresponding\n    entries in the relevant_docs, and creates a new EmbeddingQAFinetuneDataset object with\n    the filtered data.\n\n    :param qa_dataset: An object that has 'queries', 'corpus', and 'relevant_docs' attributes.\n    :return: An EmbeddingQAFinetuneDataset object with the filtered queries, corpus and relevant_docs.\n    \"\"\"</span>\n\n    <span class=\"hljs-comment\"># Extract keys from queries and relevant_docs that need to be removed</span>\n    queries_relevant_docs_keys_to_remove = {\n        k <span class=\"hljs-keyword\">for</span> k, v <span class=\"hljs-keyword\">in</span> qa_dataset.queries.items()\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-string\">'Here are 2'</span> <span class=\"hljs-keyword\">in</span> v <span class=\"hljs-keyword\">or</span> <span class=\"hljs-string\">'Here are two'</span> <span class=\"hljs-keyword\">in</span> v\n    }\n\n    <span class=\"hljs-comment\"># Filter queries and relevant_docs using dictionary comprehensions</span>\n    filtered_queries = {\n        k: v <span class=\"hljs-keyword\">for</span> k, v <span class=\"hljs-keyword\">in</span> qa_dataset.queries.items()\n        <span class=\"hljs-keyword\">if</span> k <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span> queries_relevant_docs_keys_to_remove\n    }\n    filtered_relevant_docs = {\n        k: v <span class=\"hljs-keyword\">for</span> k, v <span class=\"hljs-keyword\">in</span> qa_dataset.relevant_docs.items()\n        <span class=\"hljs-keyword\">if</span> k <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span> queries_relevant_docs_keys_to_remove\n    }\n\n    <span class=\"hljs-comment\"># Create a new instance of EmbeddingQAFinetuneDataset with the filtered data</span>\n    <span class=\"hljs-keyword\">return</span> EmbeddingQAFinetuneDataset(\n        queries=filtered_queries,\n        corpus=qa_dataset.corpus,\n        relevant_docs=filtered_relevant_docs\n    )\n\n<span class=\"hljs-comment\"># filter out pairs with phrases `Here are 2 questions based on provided context`</span>\nqa_dataset = filter_qa_dataset(qa_dataset)</span></pre>\n <h1>\n  Custom Retriever:\n </h1>\n <p>\n  To identify the optimal retriever, we employ a combination of an embedding model and a reranker. Initially, we establish a base\n  <code class=\"cw oi oj ok ol b\">\n   VectorIndexRetriever\n  </code>\n  . Upon retrieving the nodes, we then introduce a reranker to further refine the results. It\u2019s worth noting that for this particular experiment, we\u2019ve set similarity_top_k to 10 and picked top-5 with reranker. However, feel free to adjust this parameter based on the needs of your specific experiment. We are showing the code here with\n  <code class=\"cw oi oj ok ol b\">\n   OpenAIEmbedding\n  </code>\n  , please refer to the\n  <a href=\"https://colab.research.google.com/drive/1TxDVA__uimVPOJiMEQgP5fwHiqgKqm4-?usp=sharing\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   notebook\n  </a>\n  for code with other embeddings.\n </p>\n <pre><span class=\"py on gt ol b bf pz qa l qb qc\" id=\"d790\">embed_model = OpenAIEmbedding()\nservice_context = ServiceContext.from_defaults(llm=None, embed_model = embed_model)\nvector_index = VectorStoreIndex(nodes, service_context=service_context)\nvector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k = 10)</span></pre>\n <pre><span class=\"py on gt ol b bf pz qa l qb qc\" id=\"89b3\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">CustomRetriever</span>(<span class=\"hljs-title class_ inherited__\">BaseRetriever</span>):\n    <span class=\"hljs-string\">\"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"</span>\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">\n        self,\n        vector_retriever: VectorIndexRetriever,\n    </span>) -&amp;gt; <span class=\"hljs-literal\">None</span>:\n        <span class=\"hljs-string\">\"\"\"Init params.\"\"\"</span>\n\n        self._vector_retriever = vector_retriever\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">_retrieve</span>(<span class=\"hljs-params\">self, query_bundle: QueryBundle</span>) -&amp;gt; <span class=\"hljs-type\">List</span>[NodeWithScore]:\n        <span class=\"hljs-string\">\"\"\"Retrieve nodes given query.\"\"\"</span>\n\n    retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n\n    <span class=\"hljs-keyword\">if</span> reranker != <span class=\"hljs-string\">'None'</span>:\n      retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n       <span class=\"hljs-keyword\">else</span>:\n          retrieved_nodes = retrieved_nodes[:<span class=\"hljs-number\">5</span>]\n         \n       <span class=\"hljs-keyword\">return</span> retrieved_nodes\n\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">_aretrieve</span>(<span class=\"hljs-params\">self, query_bundle: QueryBundle</span>) -&amp;gt; <span class=\"hljs-type\">List</span>[NodeWithScore]:\n        <span class=\"hljs-string\">\"\"\"Asynchronously retrieve nodes given query.\n\n        Implemented by the user.\n\n        \"\"\"</span>\n        <span class=\"hljs-keyword\">return</span> self._retrieve(query_bundle)\n\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">aretrieve</span>(<span class=\"hljs-params\">self, str_or_query_bundle: QueryType</span>) -&amp;gt; <span class=\"hljs-type\">List</span>[NodeWithScore]:\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">isinstance</span>(str_or_query_bundle, <span class=\"hljs-built_in\">str</span>):\n            str_or_query_bundle = QueryBundle(str_or_query_bundle)\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-keyword\">await</span> self._aretrieve(str_or_query_bundle)\n\ncustom_retriever = CustomRetriever(vector_retriever)</span></pre>\n <h1>\n  Evaluation:\n </h1>\n <p>\n  To evaluate our retriever, we computed the Mean Reciprocal Rank (MRR) and Hit Rate metrics:\n </p>\n <pre><span class=\"py on gt ol b bf pz qa l qb qc\" id=\"cfb0\">retriever_evaluator = RetrieverEvaluator.from_metric_names(\n    [<span class=\"hljs-string\">\"mrr\"</span>, <span class=\"hljs-string\">\"hit_rate\"</span>], retriever=custom_retriever\n)\neval_results = <span class=\"hljs-keyword\">await</span> retriever_evaluator.aevaluate_dataset(qa_dataset)</span></pre>\n <h1>\n  Results:\n </h1>\n <p>\n  We put various embedding models and rerankers to the test. Here are the models we considered:\n </p>\n <p>\n  <strong>\n   Embedding Models\n  </strong>\n  :\n </p>\n <ul>\n  <li>\n   <a href=\"https://platform.openai.com/docs/guides/embeddings\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    OpenAI Embedding\n   </a>\n  </li>\n  <li>\n   <a href=\"https://www.voyageai.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Voyage Embedding\n   </a>\n  </li>\n  <li>\n   <a href=\"https://txt.cohere.com/introducing-embed-v3/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    CohereAI Embedding\n   </a>\n   (v2.0/ v3.0)\n  </li>\n  <li>\n   <a href=\"https://huggingface.co/jinaai/jina-embeddings-v2-small-en\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Jina Embeddings\n   </a>\n   (small/ base)\n  </li>\n  <li>\n   <a href=\"https://huggingface.co/BAAI/bge-large-en\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    BAAI/bge-large-en\n   </a>\n  </li>\n  <li>\n   <a href=\"https://developers.generativeai.google/tutorials/embeddings_quickstart\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Google PaLM Embedding\n   </a>\n  </li>\n </ul>\n <p>\n  <strong>\n   Rerankers\n  </strong>\n  :\n </p>\n <ul>\n  <li>\n   <a href=\"https://txt.cohere.com/rerank/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    CohereAI\n   </a>\n  </li>\n  <li>\n   <a href=\"https://huggingface.co/BAAI/bge-reranker-base\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    bge-reranker-base\n   </a>\n  </li>\n  <li>\n   <a href=\"https://huggingface.co/BAAI/bge-reranker-large\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    bge-reranker-large\n   </a>\n  </li>\n </ul>\n <blockquote>\n  <p class=\"nk nl qp nm b nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh gm bj\" id=\"015c\">\n   It\u2019s worth mentioning that these results provide a solid insight into performance for this particular dataset and task. However, actual outcomes may differ based on data characteristics, dataset size, and other variables like chunk_size, similarity_top_k, and so on.\n  </p>\n </blockquote>\n <p>\n  The table below showcases the evaluation results based on the metrics of Hit Rate and Mean Reciprocal Rank (MRR):\n </p>\n <h1>\n  Analysis:\n </h1>\n <h2>\n  <strong>\n   Performance by Embedding:\n  </strong>\n </h2>\n <ul>\n  <li>\n   <strong>\n    OpenAI\n   </strong>\n   : Showcases top-tier performance, especially with the\n   <code class=\"cw oi oj ok ol b\">\n    <strong>\n     CohereRerank\n    </strong>\n   </code>\n   (0.926966 hit rate, 0.86573 MRR) and\n   <code class=\"cw oi oj ok ol b\">\n    <strong>\n     bge-reranker-large\n    </strong>\n   </code>\n   (0.910112 hit rate, 0.855805 MRR), indicating strong compatibility with reranking tools.\n  </li>\n  <li>\n   <strong>\n    bge-large\n   </strong>\n   : Experiences significant improvement with rerankers, with the best results from\n   <code class=\"cw oi oj ok ol b\">\n    <strong>\n     CohereRerank\n    </strong>\n   </code>\n   (0.876404 hit rate, 0.822753 MRR).\n  </li>\n  <li>\n   <strong>\n    llm-embedder\n   </strong>\n   : Benefits greatly from reranking, particularly with\n   <code class=\"cw oi oj ok ol b\">\n    <strong>\n     CohereRerank\n    </strong>\n   </code>\n   (0.882022 hit rate, 0.830243 MRR), which offers a substantial performance boost.\n  </li>\n  <li>\n   <strong>\n    Cohere\n   </strong>\n   : Cohere\u2019s latest v3.0 embeddings outperform v2.0 and, with the integration of native CohereRerank, significantly improve its metrics, boasting a 0.88764 hit rate and a 0.836049 MRR.\n  </li>\n  <li>\n   <strong>\n    Voyage\n   </strong>\n   : Has strong initial performance that is further amplified by\n   <code class=\"cw oi oj ok ol b\">\n    <strong>\n     CohereRerank\n    </strong>\n   </code>\n   (0.91573 hit rate, 0.851217 MRR), suggesting high responsiveness to reranking.\n  </li>\n  <li>\n   <strong>\n    JinaAI\n   </strong>\n   : Very strong performance, sees notable gains with\n   <code class=\"cw oi oj ok ol b\">\n    <strong>\n     bge-reranker-large\n    </strong>\n   </code>\n   (0.938202 hit rate, 0.868539 MRR) and\n   <code class=\"cw oi oj ok ol b\">\n    <strong>\n     CohereRerank\n    </strong>\n   </code>\n   (0.932584 hit rate, 0.873689), indicating that reranking significantly boosts its performance.\n  </li>\n  <li>\n   <strong>\n    Google-PaLM\n   </strong>\n   : The model demonstrates strong performance, with measurable gains when using the\n   <code class=\"cw oi oj ok ol b\">\n    <strong>\n     CohereRerank\n    </strong>\n   </code>\n   (0.910112 hit rate, 0.855712 MRR). This indicates that reranking provides a clear boost to its overall results.\n  </li>\n </ul>\n <h2>\n  <strong>\n   Impact of Rerankers\n  </strong>\n  :\n </h2>\n <ul>\n  <li>\n   <strong>\n    WithoutReranker\n   </strong>\n   : This provides the baseline performance for each embedding.\n  </li>\n  <li>\n   <strong>\n    bge-reranker-base\n   </strong>\n   : Generally improves both hit rate and MRR across embeddings.\n  </li>\n  <li>\n   <strong>\n    bge-reranker-large\n   </strong>\n   : This reranker frequently offers the highest or near-highest MRR for embeddings. For several embeddings, its performance rivals or surpasses that of the\n   <code class=\"cw oi oj ok ol b\">\n    <strong>\n     CohereRerank\n    </strong>\n   </code>\n   .\n  </li>\n  <li>\n   <strong>\n    CohereRerank\n   </strong>\n   : Consistently enhances performance across all embeddings, often providing the best or near-best results.\n  </li>\n </ul>\n <h2>\n  <strong>\n   Necessity of Rerankers\n  </strong>\n  :\n </h2>\n <ul>\n  <li>\n   The data clearly indicates the significance of rerankers in refining search results. Nearly all embeddings benefit from reranking, showing improved hit rates and MRRs.\n  </li>\n  <li>\n   Rerankers, especially\n   <code class=\"cw oi oj ok ol b\">\n    <strong>\n     CohereRerank\n    </strong>\n   </code>\n   , have demonstrated their capability to transform any embedding into a competitive one.\n  </li>\n </ul>\n <h2>\n  <strong>\n   Overall Superiority\n  </strong>\n  :\n </h2>\n <ul>\n  <li>\n   When considering both hit rate and MRR, the combinations of\n   <code class=\"cw oi oj ok ol b\">\n    <strong>\n     OpenAI + CohereRerank\n    </strong>\n   </code>\n   and\n   <code class=\"cw oi oj ok ol b\">\n    <strong>\n     JinaAI-Base + bge-reranker-large/ CohereRerank\n    </strong>\n   </code>\n   emerge as top contenders.\n  </li>\n  <li>\n   However, the consistent improvement brought by the\n   <code class=\"cw oi oj ok ol b\">\n    <strong>\n     CohereRerank/ bge-reranker-large\n    </strong>\n   </code>\n   rerankers across various embeddings make them the standout choice for enhancing search quality, regardless of the embedding in use.\n  </li>\n </ul>\n <p>\n  In summary, to achieve the peak performance in both hit rate and MRR, the combination of\n  <code class=\"cw oi oj ok ol b\">\n   <strong>\n    OpenAI\n   </strong>\n  </code>\n  or\n  <code class=\"cw oi oj ok ol b\">\n   <strong>\n    JinaAI-Base\n   </strong>\n  </code>\n  embeddings with the\n  <code class=\"cw oi oj ok ol b\">\n   <strong>\n    CohereRerank/bge-reranker-large\n   </strong>\n  </code>\n  reranker stands out.\n </p>\n <blockquote>\n  <p class=\"rl rm gt be rn ro rp rq rr rs rt oh dt\" id=\"4791\">\n   Please be aware that our benchmarks are intended to offer a reproducible script for your own data. Nevertheless, treat these figures as estimates and proceed with caution when interpreting them.\n  </p>\n </blockquote>\n <h1>\n  Conclusions:\n </h1>\n <p>\n  In this blog post, we have demonstrated how to evaluate and enhance retriever performance using various embeddings and rerankers. Below are our final conclusions.\n </p>\n <ul>\n  <li>\n   <strong>\n    Embeddings\n   </strong>\n   : The\n   <code class=\"cw oi oj ok ol b\">\n    <strong>\n     OpenAI\n    </strong>\n   </code>\n   and\n   <code class=\"cw oi oj ok ol b\">\n    <strong>\n     JinaAI-Base\n    </strong>\n   </code>\n   embeddings, especially when paired with the\n   <code class=\"cw oi oj ok ol b\">\n    <strong>\n     CohereRerank/bge-reranker-large\n    </strong>\n   </code>\n   reranker, set the gold standard for both hit rate and MRR.\n  </li>\n  <li>\n   <strong>\n    Rerankers\n   </strong>\n   : The influence of rerankers, particularly\n   <code class=\"cw oi oj ok ol b\">\n    <strong>\n     CohereRerank/bge-reranker-large\n    </strong>\n   </code>\n   , cannot be overstated. They play a key role in improving the MRR for many embeddings, showing their importance in making search results better.\n  </li>\n  <li>\n   <strong>\n    Foundation is Key\n   </strong>\n   : Choosing the right embedding for the initial search is essential; even the best reranker can\u2019t help much if the basic search results aren\u2019t good.\n  </li>\n  <li>\n   <strong>\n    Working Together:\n   </strong>\n   To get the best out of retrievers, it\u2019s important to find the right mix of embeddings and rerankers. This study shows how important it is to carefully test and find the best pairing.\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 21443, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab682f7b-a39a-4347-a397-ea949f7645c2": {"__data__": {"id_": "ab682f7b-a39a-4347-a397-ea949f7645c2", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.html", "file_name": "bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.html", "file_type": "text/html", "file_size": 11333, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.html", "file_name": "bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.html", "file_type": "text/html", "file_size": 11333, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "dd44295c7f9b8a3020236cf3a814b563f42b840759287cbaaad66d1f95c1da01", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <em>\n   Co-authored by:\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/u/e84f937083e3?source=post_page-----db42e26ab4f3--------------------------------\" rel=\"noreferrer noopener\">\n    Riya Jagetia\n   </a>\n   ,\n  </em>\n  <em>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/u/7d279643046e?source=post_page-----db42e26ab4f3--------------------------------\" rel=\"noreferrer noopener\">\n    Tarun Malik\n   </a>\n   ,\n  </em>\n  <em>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/u/6fe4060f2a53?source=post_page-----db42e26ab4f3--------------------------------\" rel=\"noreferrer noopener\">\n    Divija N\n   </a>\n   ,\n  </em>\n  <em>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/u/361137e928c3?source=post_page-----db42e26ab4f3--------------------------------\" rel=\"noreferrer noopener\">\n    Sharon Tan\n   </a>\n   ,\n  </em>\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/u/7bdf7b817eec?source=post_page-----db42e26ab4f3--------------------------------\" rel=\"noreferrer noopener\">\n   <em>\n    Zehra Rizvi\n   </em>\n  </a>\n  ,\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/u/b75312a598d6?source=post_page-----db42e26ab4f3--------------------------------\" rel=\"noreferrer noopener\">\n   <em>\n    Amanda Piyapanee\n   </em>\n  </a>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  At the recent LlamaIndex RAG-a-thon [1], our team\u2019s\n  <strong>\n   \u201cCounselor Copilot\u201d\n  </strong>\n  won 2nd prize in the Traditional track and 1st prize in the Datastax/AstraDB category. More details can be found on our\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://devpost.com/software/counselor-copilot\" rel=\"noreferrer noopener\">\n   DevPost\n  </a>\n  [2] writeup.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Introduction\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Against the backdrop of growing strain on mental health services [3, 4], non-profit organizations like The Trevor Project [5] are a critical part of the care ecosystem. Focusing on helping LGBTQ+ youth who are contemplating suicide, The Trevor Project provides accessible crisis services including via TrevorText, an online chat service with trained volunteer counselors.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Problem: The Dual Challenge Faced by Crisis Counselors\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  However, TrevorText counselors face significant challenges. Not only is there high demand for counselors during busy times like holidays and night shifts, but also, counselors have to juggle a number of administrative tasks such as sifting through forms, responding to messages across multiple chats, and locating relevant local resources. This not only increases the risk of counselors burning out but also hampers their ability to provide timely and effective care.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In light of these challenges, there\u2019s a pressing need for innovative solutions to bridge the gap between the demand and supply of crisis services.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  While our hackathon project focused on augmenting TrevorText, our product can be easily extended to general crisis chat alternatives as well.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  The Winning Solution: An AI Copilot for Crisis Counselors\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Counselor Copilot is a real-time assistant for crisis counselors that takes into account contact context and chat history to suggest replies so that counselors can focus on what they do best: providing care.\n  <strong>\n   There is no prompting that is needed from counselors; the copilot works seamlessly in the background.\n  </strong>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Further, the copilot never directly replies to contacts; instead, replies are suggested and can be edited.\n </p>\n <figure>\n  <figcaption>\n   Counselor copilot takes into account contact context and chat history to provide real-time reply suggestions to the counselors\n  </figcaption>\n </figure>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Specifically, the copilot automates counselor tasks that include but are not limited to:\n </p>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Retrieving and synthesizing contact data from complex PDFs in real-time. This also provides counselors context on their contacts when conversations are initiated.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Assessing from the chat context if emergency intervention is required. If so, suggesting escalation to a supervisor.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Using existing resources and guidelines from the organization to suggest appropriate replies.\n  </li>\n </ol>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  4. Searching for location-specific resources for contacts, and quickly sharing those resources via email.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  5. Completing case forms in a CRM for contacts, including summarizing the interaction.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  While these tasks are important and necessary, they pull attention away from conversations with youth in crisis and take up precious time.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  With Counselor copilot, these tasks are completed when they are required and without any prompting from counselors, providing more bandwidth for counselors and ultimately leading to higher-quality conversations with patients.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Below is a demo of our solution:\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  How we built it\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  When the chat is initiated, the Counselor Copilot gets the contact\u2019s data from the CRM, which is stored in complex PDFs. We used LlamaParse to extract relevant contact data in real-time and then provide a summary of that data to counselors as context at the beginning of each conversation.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Further, we used a LlamaIndex ReAct Agent to monitor the conversation and \u2014 based on the chat history and contact context \u2014 deploy the right tool. Tools at the ReAct Agent\u2019s disposal include:\n </p>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Escalating the conversation to a supervisor\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Suggesting a response and related resources based on The Trevor Project\u2019s guidelines\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Searching the web for location-specific resources and sending the resources to the contact\n  </li>\n </ol>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  For tool #2, we created a vector database that contains The Trevor Project\u2019s documents, which highlight key guidelines for counselors based on different scenarios and situations that they may face. We used RAG to retrieve resources relevant to the conversation, and GPT4 to draft a response for the counselor based on those resources, both of which are essential due to the sensitive nature of the conversation.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Lastly, we used the conversation content to fill out a form with key Salesforce fields (e.g. name, age, city, state), as well as to summarize the conversation.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Possible Extensions\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We\u2019re excited by the potential for others to build on our work [6] and extend Counselor Copilot further. Some ideas include:\n </p>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Reduce costs and improve quality of suggested responses: Fine-tune a state-of-the-art open-source LLM on extracts of chat conversations conducted by counselors\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   More targeted conversation management: Add a tool for the agent to assess the stage of the conversation, given that there are recommended styles and questions for each stage (e.g. establishing rapport, risk assessment)\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Closed-loop feedback cycle: Allow counselors to thumbs-up or thumbs-down selected responses, as a natural way to collect human feedback which can be used for further model or agent training\n  </li>\n </ol>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Conclusion: A Step Toward Efficient and Effective Crisis Care\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Our AI copilot for crisis counselors represents a significant step toward more efficient and effective crisis care. By automating administrative tasks, it frees up counselors to focus on their core mission of providing youth in crisis a safe place to talk. This not only enhances the quality of care provided but also addresses the pressing issue of counselor shortage by maximizing the impact of existing resources. As we continue to refine and expand this technology, we envision a future where crisis counseling is more accessible, responsive, and impactful for all those in need\u200b\u200b.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  References\n </h2>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://rag-a-thon.devpost.com/\" rel=\"noreferrer noopener\">\n    https://rag-a-thon.devpost.com/\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://devpost.com/software/counselor-copilot\" rel=\"noreferrer noopener\">\n    https://devpost.com/software/counselor-copilot\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.mhanational.org/issues/state-mental-health-america\" rel=\"noreferrer noopener\">\n    https://www.mhanational.org/issues/state-mental-health-america\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.aamc.org/news/growing-psychiatrist-shortage-enormous-demand-mental-health-services\" rel=\"noreferrer noopener\">\n    https://www.aamc.org/news/growing-psychiatrist-shortage-enormous-demand-mental-health-services\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.thetrevorproject.org/\" rel=\"noreferrer noopener\">\n    https://www.thetrevorproject.org/\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/zrizvi93/trevorhack\" rel=\"noreferrer noopener\">\n    https://github.com/zrizvi93/trevorhack\n   </a>\n  </li>\n </ol>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 11306, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "677eaae0-49f8-42b6-aeef-77f6c9a29fe7": {"__data__": {"id_": "677eaae0-49f8-42b6-aeef-77f6c9a29fe7", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.html", "file_name": "bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.html", "file_type": "text/html", "file_size": 8278, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.html", "file_name": "bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.html", "file_type": "text/html", "file_size": 8278, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "a7f6558fefefdf18ec77d448af4d10c08435b31fa9c85df192a22bf636b7fb76", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Author:\n  <a href=\"https://twitter.com/hexapode\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Pierre-Loic Doulcet\n  </a>\n </p>\n <p>\n  As programmers, we often find ourselves limited by language barriers. Documentation for various programming frameworks and tools is predominantly available in English, and increasingly in languages like Chinese, creating challenges for non-native speakers. I faced similar obstacles in my early programming days, and it was only through community efforts like\n  <a href=\"https://traduc.org/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   traduc.org\n  </a>\n  \u2019s translation of man pages that I could surmount them.\n </p>\n <p>\n  Today, we are excited to unveil a solution to this pervasive issue:\n  <a href=\"https://www.npmjs.com/package/autotranslatedoc\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   AutoTranslateDoc\n  </a>\n  , a command-line tool designed to democratize access to technical documentation by breaking down language barriers.\n </p>\n <p>\n  <strong>\n   How AutoTranslateDoc Works\n  </strong>\n </p>\n <ol>\n  <li>\n   Collect the Documentation: The tool connects to GitHub, identifying and downloading .md and .mdx files from any repository.\n  </li>\n  <li>\n   Chunk and Prepare: The documentation is then chunked or split for translation.\n  </li>\n  <li>\n   Translate Efficiently: Utilizing the power of LLMs like GPT-3.5 and GPT-4, each chunk of documentation is translated accurately.\n  </li>\n  <li>\n   Verify and Enhance: The translation is automatically verified, with retranslation if needed, ensuring the highest quality.\n  </li>\n  <li>\n   Consolidate: Finally, the chunks are amalgamated back into a cohesive document.\n  </li>\n </ol>\n <p>\n  Our initial tests on translating the llamaIndexTS documentation have been highly promising. You can now read our docs in over a dozen languages including\n  <a href=\"https://ts.llamaindex.ai/zh-Hans/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Chinese\n  </a>\n  ,\n  <a href=\"https://ts.llamaindex.ai/fr/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   French\n  </a>\n  , and\n  <a href=\"https://ts.llamaindex.ai/es/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Spanish\n  </a>\n  !\n </p>\n <p>\n  <strong>\n   Getting Started\n  </strong>\n </p>\n <p>\n  Install AutoTranslateDoc easily via npm, or clone the repo (\n  <a href=\"https://github.com/run-llama/automatic-doc-translate\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://github.com/run-llama/automatic-doc-translate\n  </a>\n  ) :\n </p>\n <pre><span class=\"oz pa gt ow b bf pb pc l pd pe\" id=\"9c2c\">npm install -g autotranslatedoc</span></pre>\n <p>\n  Try it out with run-lama/LlamaIndexTS or your favorite repo! You will need a\n  <a href=\"https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   GitHub Personal Access Token\n  </a>\n  and an\n  <a href=\"https://platform.openai.com/api-keys\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   OpenAI API Key\n  </a>\n  (the tool will prompt you to set these):\n </p>\n <pre><span class=\"oz pa gt ow b bf pb pc l pd pe\" id=\"0a66\"># Translate\nautotranslatedoc translate run-llama LlamaIndexTS -d apps/docs -l fr</span></pre>\n <pre><span class=\"pj pa gt ow b is pk pl l jh pe\" id=\"fb93\">#build\nautotranslatedoc build run-llama LlamaIndexTS -d apps/docs -l fr</span></pre>\n <p>\n  This translates the directory\n  <code class=\"cw pm pn po ow b\">\n   apps/docs\n  </code>\n  in the GitHub repo\n  <code class=\"cw pm pn po ow b\">\n   run-llama/LlamaIndexTS\n  </code>\n  .\n </p>\n <p>\n  <strong>\n   Improving Accuracy and Consistency\n  </strong>\n </p>\n <p>\n  Our commitment to improving translation accuracy led us to innovate in both the translation process and verification methods.\n </p>\n <p>\n  <strong>\n   Strategic Document Splitting:\n  </strong>\n </p>\n <p>\n  We approach translation by dividing each page of the documentation into sections. To provide enhanced context and coherence, each section\u2019s title hierarchy is appended to its respective chunk during translation. This technique ensures that the translated content maintains the original structure and thematic relevance.\n </p>\n <p>\n  <strong>\n   Rigorous Translation Verification:\n  </strong>\n </p>\n <p>\n  Our verification process is designed to rigorously assess the accuracy of translations. We employ several checks on the translated documentation:\n </p>\n <ul>\n  <li>\n   Translation Length Check: We compare the length of the translated text with the original to ensure consistency.\n  </li>\n  <li>\n   Title Hierarchy Analysis: We verify that no new sections are inadvertently added in the translation.\n  </li>\n  <li>\n   Link Count Validation: The number of hyperlinks is matched against the original to ensure none are missed or added unnecessarily.\n  </li>\n  <li>\n   Code Block Accuracy: The presence and correctness of code blocks in the translation are checked against the original document.\n  </li>\n </ul>\n <p>\n  These checks address common issues with LLMs, such as hallucination or omission, and prompt retranslation when necessary. This rigorous process significantly enhances the accuracy of our translations. Moreover, we incorporate a unique self-critique feature, where the LLM evaluates its own translation output, further refining the quality.\n </p>\n <p>\n  This dual approach of meticulous chunking and thorough verification ensures that our translations are not only accurate but also contextually relevant, maintaining the integrity and utility of the original documentation.\n </p>\n <p>\n  <strong>\n   Managing Documentation Updates: Keeping Translations Current\n  </strong>\n </p>\n <p>\n  Documentation, by its nature, is a dynamic entity that evolves over time. Recognizing this, we\u2019ve integrated a robust system into AutoDocTranslate to manage documentation updates efficiently.\n </p>\n <p>\n  <strong>\n   Historical Tracking through JSON:\n  </strong>\n </p>\n <p>\n  When translating a repository using our tool, a .json file is generated, chronicling the history of translations. This file is crucial for tracking changes and versions in the documentation. It serves as a foundation for differential translation, a process that identifies and translates only the newly added or modified content. This feature can be accessed through the\n  <code class=\"cw pm pn po ow b\">\n   autotranslatedoc update\n  </code>\n  command, streamlining the maintenance of up-to-date translations.\n </p>\n <p>\n  <strong>\n   Future Enhancements:\n  </strong>\n </p>\n <p>\n  We are actively working on enhancing this system with the following features:\n </p>\n <p>\n  <strong>\n   Manual Change Integration:\n  </strong>\n  Recognizing that translations might undergo manual edits post-generation, we are developing functionality to account for these manual changes during updates. This will ensure that any human revisions are retained and only new or altered sections from the source documentation are translated in subsequent updates.\n </p>\n <p>\n  <strong>\n   GUI for Translation Management:\n  </strong>\n  To further simplify the process of translation editing, tracking, and verification, we\u2019re in the early stages of developing a graphical user interface (GUI). This interface will allow users to interact more intuitively with the translations. An experimental version of this feature can be accessed through the\n  <code class=\"cw pm pn po ow b\">\n   autotranslatedoc serve\n  </code>\n  command. This GUI will enable users to visually navigate through the translations, make edits, and verify the accuracy of the content more efficiently.\n </p>\n <p>\n  By continually updating and refining these features, AutoDocTranslate aims to stay at the forefront of making technical documentation universally accessible and easy to maintain in multiple languages.\n </p>\n <p>\n  <strong>\n   The Future of Technical Documentation\n  </strong>\n </p>\n <p>\n  AutoDocTranslate is more than a tool; it\u2019s a step towards an inclusive, barrier-free tech world where language is no longer an impediment to learning and growth. We\u2019re excited to see how it empowers programmers across the globe.\n </p>\n <p>\n  Join us in this journey and contribute to a more accessible programming community!\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 8265, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "91bbd0bf-7804-48e4-b8f1-c45ebe769478": {"__data__": {"id_": "91bbd0bf-7804-48e4-b8f1-c45ebe769478", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.html", "file_name": "build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.html", "file_type": "text/html", "file_size": 13116, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.html", "file_name": "build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.html", "file_type": "text/html", "file_size": 13116, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "10fe3cf898890c5d60d26e6c7b0660c2117223b48477c6b97dfc27d3f2bb4bee", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  <strong>\n   Co-authors:\n  </strong>\n </p>\n <ul>\n  <li>\n   Prakul Agarwal \u2014 Senior Product Manager, Machine Learning at MongoDB\n  </li>\n  <li>\n   Jerry Liu \u2014 co-founder at LlamaIndex\n  </li>\n </ul>\n <p>\n  <strong>\n   Update (6/22/2023):\n  </strong>\n  The preferred way to use LlamaIndex + MongoDB is now with our MongoDBAtlasVectorSearch class. Take a look at our guide here:\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/MongoDBAtlasVectorSearch.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/MongoDBAtlasVectorSearch.html\n  </a>\n </p>\n <h1>\n  <strong>\n   Summary\n  </strong>\n </h1>\n <p>\n  Large Language Models (LLMs) like ChatGPT have revolutionized the way users can get answers to their questions. However, the \u201cknowledge\u201d of LLMs is restricted by what they were trained on, which for ChatGPT means publicly available information on the internet till September 2021. How can LLMs answer questions using private knowledge sources like your company\u2019s data and unlock its true transformative power?\n </p>\n <p>\n  This blog will discuss how LlamaIndex and MongoDB can enable you to achieve this outcome quickly. The\n  <a href=\"https://colab.research.google.com/drive/1SNIeLW38Nvx6MtL3-_LPS2XTIzqD4gS6?usp=sharing\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   attached notebook\n  </a>\n  provides a code walkthrough on how to query any PDF document using English queries.\n </p>\n <h1>\n  <strong>\n   Background\n  </strong>\n </h1>\n <p>\n  Traditionally, AI has been used to analyze data, identify patterns and make predictions based on existing data. The recent advancements have led to AI becoming better at generating new things (rather than just analyzing existing things). This is referred to as Generative AI. Generative AI is powered mainly by machine learning models called Large Language Models (LLM). LLMs are pre-trained on large quantities of publicly available text. There are various proprietary LLMs from companies like OpenAI, Cohere, AI21, as well as a lot of emerging open-source LLMs like Llama, Dolly, etc.\n </p>\n <p>\n  There are 2 main scenarios where the knowledge of LLMs falls short:\n </p>\n <ul>\n  <li>\n   Private data such as your company\u2019s internal knowledge base spread across PDFs, Google Docs, Wiki pages, and applications like Salesforce and Slack\n  </li>\n  <li>\n   Newer data than when the LLMs were last trained. Example question: Who is the most recent UK prime minister?\n  </li>\n </ul>\n <p>\n  There are 2 main paradigms currently for extending the amazing reasoning and knowledge generation capabilities of LLMs: Model finetuning and in-context learning.\n </p>\n <p>\n  Model Finetuning can be more complex and expensive to operationalize. There are also some open questions like how to delete information from a fine-tuned model to ensure you comply with local laws (ex. GDPR in Europe), and for changing data you need to fine-tune again constantly.\n </p>\n <p>\n  In-context learning requires inserting the new data as part of the input prompts to the LLM. To perform this data augmentation in a secure, high performance and cost-effective manner is where tools like LlamaIndex and MongoDB Developer Data Platform can help.\n </p>\n <h1>\n  <strong>\n   Introduction to LlamaIndex\n  </strong>\n </h1>\n <p>\n  LlamaIndex provides a simple, flexible interface to connect LLMs with external data.\n </p>\n <ul>\n  <li>\n   Offers data connectors to various data sources and data formats (APIs, PDFs, docs, etc).\n  </li>\n  <li>\n   Provides indices over the unstructured and structured data for use with LLMs.\n  </li>\n  <li>\n   Structures external information so that it can be used with the prompt window limitations of any LLM.\n  </li>\n  <li>\n   Exposes a query interface which takes in an input prompt and returns a knowledge-augmented output.\n  </li>\n </ul>\n <h1>\n  <strong>\n   MongoDB as the Datastore\n  </strong>\n </h1>\n <p>\n  It is effortless to store the ingested documents (i.e. Node objects), index metadata, etc to MongoDB using the inbuilt abstractions in LlamaIndex. There is an option to store the \u201cdocuments\u201d as an actual collection in MongoDB using\n  <code class=\"cw pk pl pm pn b\">\n   MongoDocumentStore\n  </code>\n  . There is an option to persist the \u201cIndexes\u201d using the\n  <code class=\"cw pk pl pm pn b\">\n   MongoIndexStore\n  </code>\n  .\n </p>\n <p>\n  Storing LlamaIndex\u2019s documents and indexes in a database becomes necessary in a couple of scenarios:\n </p>\n <ol>\n  <li>\n   Use cases with large datasets may require more than in-memory storage.\n  </li>\n  <li>\n   Ingesting and processing data from various sources (for example, PDFs, Google Docs, Slack).\n  </li>\n  <li>\n   The requirement to continuously maintain updates from the underlying data sources.\n  </li>\n </ol>\n <p>\n  Being able to persist this data enables processing the data once and then being able to query it for various downstream applications.\n </p>\n <h1>\n  <strong>\n   MongoDB Atlas\n  </strong>\n </h1>\n <p>\n  MongoDB offers a free forever Atlas cluster in the public cloud service of your choice. This can be accomplished very quickly by following this\n  <a href=\"https://www.mongodb.com/developer/products/atlas/free-atlas-cluster/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   tutorial\n  </a>\n  . Or you can get started directly\n  <a href=\"https://www.mongodb.com/cloud/atlas/register\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   here\n  </a>\n  .\n </p>\n <h1>\n  <strong>\n   Use of LLMs\n  </strong>\n </h1>\n <p>\n  LlamaIndex uses LangChain\u2019s (another popular framework for building Generative AI applications) LLM modules and allows for customizing the underlying LLM to be used (default being OpenAI\u2019s text-davinci-003 model). The chosen LLM is always used by LlamaIndex to construct the final answer and is sometimes used during index creation as well.\n </p>\n <h1>\n  <strong>\n   The workflow\n  </strong>\n </h1>\n <ol>\n  <li>\n   Connect private knowledge sources using LlamaIndex connectors (offered through\n   <a href=\"https://llamahub.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaHub\n   </a>\n   ).\n  </li>\n  <li>\n   Load in the Documents. A Document represents a lightweight container around the data source.\n  </li>\n  <li>\n   Parse the Documents objects into Node objects. Nodes represent \u201cchunks\u201d of source Documents (ex. a text chunk). These node objects can be persisted to a MongoDB collection or kept in memory.\n  </li>\n  <li>\n   Construct Index from Nodes. There are various kinds of indexes in LlamaIndex like \u201cList Index\u201d (this stores nodes as Sequential chain), \u201cVector Store Index\u201d (this stores each node and a corresponding embedding in a vector store). Depending on the type of Index, these indexes can be persisted into a MongoDB collection or a Vector Database.\n  </li>\n  <li>\n   Finally query the index. This is where the the query is parsed, relevant Nodes retrieved through the use of indexes, and provided as an input to a \u201cLarge Language Model\u201d (LLM). Different types of queries can use different indexes.\n  </li>\n </ol>\n <figure>\n  <figcaption class=\"qe fe qf pp pq qg qh be b bf z dt\">\n   LlamaIndex + MongoDB Workflow Diagram\n  </figcaption>\n </figure>\n <h1>\n  <strong>\n   Getting questions answered over your private data\n  </strong>\n </h1>\n <p>\n  We want to query the \u201cGPT-4 Technical Report\u201d published by OpenAI in March 2023. This was a\n  <a href=\"https://arxiv.org/pdf/2303.08774.pdf\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   PDF document\n  </a>\n  with 100 pages. This is a recent publication, so was not included as part of the original ChatGPT training data.\n </p>\n <p>\n  Here\u2019s the summary of the various queries we can ask the PDF.\n </p>\n <blockquote>\n  <p class=\"na nb ql nc b nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx gm bj\" id=\"ee1a\">\n   <strong>\n    <em class=\"gt\">\n     Query\n    </em>\n   </strong>\n  </p>\n  <p class=\"na nb ql nc b nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx gm bj\" id=\"6dfd\">\n   <em class=\"gt\">\n    \u201cHow does GPT4 do on the bar exam?\u201d\n   </em>\n  </p>\n  <p class=\"na nb ql nc b nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx gm bj\" id=\"dc53\">\n   <strong>\n    <em class=\"gt\">\n     Response\n    </em>\n   </strong>\n  </p>\n  <p class=\"na nb ql nc b nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx gm bj\" id=\"227c\">\n   <em class=\"gt\">\n    \u201cGPT-4 performs well on the Uniform Bar Exam, with a score in the top 10% of test takers (Table 1, Figure 4).\u201d\n   </em>\n  </p>\n  <p class=\"na nb ql nc b nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx gm bj\" id=\"a4a5\">\n   <strong>\n    <em class=\"gt\">\n     LLM token usage\n    </em>\n   </strong>\n  </p>\n  <p class=\"na nb ql nc b nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx gm bj\" id=\"5da7\">\n   <em class=\"gt\">\n    Total embedding token usage: 18 tokens\nTotal LLM token usage: 1889 tokens\n   </em>\n  </p>\n  <p class=\"na nb ql nc b nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx gm bj\" id=\"e56d\">\n   <strong>\n    <em class=\"gt\">\n     Query\n    </em>\n   </strong>\n  </p>\n  <p class=\"na nb ql nc b nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx gm bj\" id=\"193b\">\n   <em class=\"gt\">\n    \u201cHow much better is GPT-4 in reducing hallucinations over GPT-3.5?\u201d\n   </em>\n  </p>\n  <p class=\"na nb ql nc b nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx gm bj\" id=\"7763\">\n   <strong>\n    <em class=\"gt\">\n     Response\n    </em>\n   </strong>\n  </p>\n  <p class=\"na nb ql nc b nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx gm bj\" id=\"b7f0\">\n   <em class=\"gt\">\n    \u201cGPT-4 improves on the latest GPT-3.5 model by 19 percentage points, with significant gains across all topics.\u201d\n   </em>\n  </p>\n  <p class=\"na nb ql nc b nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx gm bj\" id=\"df71\">\n   <strong>\n    <em class=\"gt\">\n     Query\n    </em>\n   </strong>\n  </p>\n  <p class=\"na nb ql nc b nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx gm bj\" id=\"5ba0\">\n   <em class=\"gt\">\n    \u201cWhat issues were observed after fine-tuning GPT-4 with RHLF??\u201d\n   </em>\n  </p>\n  <p class=\"na nb ql nc b nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx gm bj\" id=\"cede\">\n   <strong>\n    <em class=\"gt\">\n     Response\n    </em>\n   </strong>\n  </p>\n  <p class=\"na nb ql nc b nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx gm bj\" id=\"e124\">\n   <em class=\"gt\">\n    \u201cAfter fine-tuning GPT-4 with RHLF, issues observed included the model becoming overly cautious in certain ways, such as refusing innocuous requests and excessively hedging or \u201coverrefusing\u201d. Additionally, the model was still quite brittle and sometimes exhibited undesired behaviors based on prompts where instructions to labelers were underspecified.\u201d\n   </em>\n  </p>\n  <p class=\"na nb ql nc b nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx gm bj\" id=\"79de\">\n   <strong>\n    <em class=\"gt\">\n     Query\n    </em>\n   </strong>\n  </p>\n  <p class=\"na nb ql nc b nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx gm bj\" id=\"58c4\">\n   <em class=\"gt\">\n    \u201cWhat is RBRM?\u201d\n   </em>\n  </p>\n  <p class=\"na nb ql nc b nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx gm bj\" id=\"354c\">\n   <strong>\n    <em class=\"gt\">\n     Response\n    </em>\n   </strong>\n  </p>\n  <p class=\"na nb ql nc b nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx gm bj\" id=\"11af\">\n   <em class=\"gt\">\n    \u201cRBRM stands for Rule-Based Reward Model. It is a technique used to provide an additional reward signal to the GPT-4 policy model during PPO fine-tuning on a subset of training prompts. The RBRM takes three things as input: the prompt (optional), the output from the policy model, and a human-written rubric (e.g., a set of rules in multiple-choice style) for how this output should be evaluated. The RBRM then classifies the output based on the rubric.\u201d\n   </em>\n  </p>\n </blockquote>\n <p>\n  The screenshots below show how the PDF document is converted into \u201cLlamaIndex nodes\u201d and \u201cLlamaIndex indices\u201d and persisted into MongoDB.\n </p>\n <h1>\n  <strong>\n   Relevant Resources\n  </strong>\n </h1>\n <p>\n  Further details can be found here. Also check out the reference notebook below!\n </p>\n <p>\n  Reading data from MongoDB:\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/data_connectors/MongoDemo.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   link\n  </a>\n </p>\n <p>\n  Various Indexes in LlamaIndex:\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/guides/primer/index_guide.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   link\n  </a>\n </p>\n <h2>\n  <strong>\n   Reference Notebook\n  </strong>\n </h2>\n <p>\n  <a href=\"https://colab.research.google.com/drive/1SNIeLW38Nvx6MtL3-_LPS2XTIzqD4gS6?usp=sharing\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://colab.research.google.com/drive/1SNIeLW38Nvx6MtL3-_LPS2XTIzqD4gS6?usp=sharing\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 13023, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb53e9aa-77c0-4337-b93c-8d845533424c": {"__data__": {"id_": "eb53e9aa-77c0-4337-b93c-8d845533424c", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.html", "file_name": "build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.html", "file_type": "text/html", "file_size": 8801, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.html", "file_name": "build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.html", "file_type": "text/html", "file_size": 8801, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "2cb63adcbf829a2e0e1248a1980a9cd453f999f473487c73da7bc863b964b261", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  <strong>\n   Authors:\n  </strong>\n  Anupam Datta, Shayak Sen, Jerry Liu, Simon Suo\n </p>\n <p>\n  <strong>\n   Source Link:\n  </strong>\n  <a href=\"https://truera.com/build-and-evaluate-llm-apps-with-llamaindex-and-trulens/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://truera.com/build-and-evaluate-llm-apps-with-llamaindex-and-trulens/\n  </a>\n </p>\n <p>\n  LlamaIndex is a popular open source framework for building LLM apps. TruLens is an open source library for evaluating, tracking, and iterating on LLM apps to improve their quality. The LlamaIndex and TruLens teams are actively collaborating to enable LLM app developers to rapidly build, evaluate, and iterate on their apps.\n </p>\n <p>\n  In the latest release of TruLens, we introduce tracing for LlamaIndex based LLM applications that allow you to evaluate and track your experiments with just a few lines of code. This lets you automatically evaluate a number of different components of the application stack including:\n </p>\n <ul>\n  <li>\n   App inputs and outputs\n  </li>\n  <li>\n   LLM calls\n  </li>\n  <li>\n   Retrieved context chunks from an index\n  </li>\n  <li>\n   Latency\n  </li>\n  <li>\n   Cost and Token Counts (coming soon!)\n  </li>\n </ul>\n <p>\n  Check out this\n  <a href=\"https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.3.0/trulens_eval/examples/vector-dbs/llama_index/quickstart.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   notebook\n  </a>\n  to get started and read along to get a step by step view.\n </p>\n <h1>\n  How do I actually use this?\n </h1>\n <h2>\n  Build A LlamaIndex App\n </h2>\n <p>\n  LlamaIndex lets you connect your data to LLMs and rapidly build applications for a number of different use cases.\n </p>\n <pre><span class=\"qv ov gt qs b bf qw qx l qy qz\" id=\"8655\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(<span class=\"hljs-string\">'llama_index/data'</span>).load_data()\nindex = VectorStoreIndex.from_documents(documents)\n\nquery_engine = index.as_query_engine()</span></pre>\n <p>\n  Once you build your app, you can easily query your data:\n </p>\n <pre><span class=\"qv ov gt qs b bf qw qx l qy qz\" id=\"5ac3\">response = query_engine.query(<span class=\"hljs-string\">\"What did the author do growing up?\"</span>)\n<span class=\"hljs-built_in\">print</span>(response)</span></pre>\n <p>\n  And you get an appropriate response.\n </p>\n <pre><span class=\"qv ov gt qs b bf qw qx l qy qz\" id=\"8f84\">Growing up, the author wrote <span class=\"hljs-type\">short</span> stories, programmed on an IBM <span class=\"hljs-number\">1401</span>, and nagged his father to buy him a TRS-<span class=\"hljs-number\">80</span> microcomputer. He wrote simple games, a program to predict how high his model rockets would fly, and a word processor. He also studied philosophy in college, but switched to AI after becoming bored with it. He then took art classes at Harvard and applied to art schools, eventually attending RISD.</span></pre>\n <h2>\n  Wrap A LlamaIndex App with TruLens\n </h2>\n <p>\n  With TruLens, you can wrap LlamaIndex query engines with a TruLlama wrapper. This wrapper preserves all LlamaIndex behavior, but traces all of the intermediate steps so that they can be individually evaluated.\n </p>\n <pre><span class=\"qv ov gt qs b bf qw qx l qy qz\" id=\"a23f\"><span class=\"hljs-keyword\">from</span> trulens_eval <span class=\"hljs-keyword\">import</span> TruLlama\nl = TruLlama(query_engine)</span></pre>\n <p>\n  The wrapped app can now be queried in the exact same way:\n </p>\n <pre><span class=\"qv ov gt qs b bf qw qx l qy qz\" id=\"672d\">response = l.query(<span class=\"hljs-string\">\"What did the author do growing up?\"</span>)\n<span class=\"hljs-built_in\">print</span>(response)</span></pre>\n <p>\n  Except, now the details of the query are logged by TruLens.\n </p>\n <h2>\n  Add Feedback Functions\n </h2>\n <p>\n  Now to evaluate the behavior of your models, we can add feedback functions to your wrapped application. Note that as a developer you only need to\n  <strong>\n   add a few lines of code\n  </strong>\n  to start using feedback functions in your apps. You can also easily add functions tailored to the needs of your application.\n </p>\n <p>\n  Our goal with feedback functions is to programmatically check the app for quality metrics.\n </p>\n <ul>\n  <li>\n   The first feedback function checks for language match between the prompt and the response. It\u2019s a useful check since a natural user expectation is that the response is in the same language as the prompt. It is implemented with a call to a HuggingFace API that programmatically checks for language match.\n  </li>\n  <li>\n   The next feedback function checks how relevant the answer is to the question by using an Open AI LLM that is prompted to produce a relevance score.\n  </li>\n  <li>\n   Finally, the third feedback function checks how relevant individual chunks retrieved from the vector database are to the question, again using an OpenAI LLM in a similar manner. This is useful because the retrieval step from a vector database may produce chunks that are not relevant to the question and the quality of the final response would be better if these chunks are filtered out before producing the final response.\n  </li>\n </ul>\n <pre><span class=\"qv ov gt qs b bf qw qx l qy qz\" id=\"ccad\"><span class=\"hljs-keyword\">from</span> trulens_eval <span class=\"hljs-keyword\">import</span> TruLlama, Tru, Query, Feedback, feedback\n\n<span class=\"hljs-comment\"># Initialize Huggingface-based feedback function collection class:</span>\nhugs = feedback.Huggingface()\nopenai = feedback.OpenAI()\n<span class=\"hljs-comment\"># Define a language match feedback function using HuggingFace.</span>\nf_lang_match = Feedback(hugs.language_match).on_input_output()\n<span class=\"hljs-comment\"># By default this will check language match on the main app input and main app</span>\n<span class=\"hljs-comment\"># output.</span>\n\n<span class=\"hljs-comment\"># Question/answer relevance between overall question and answer.</span>\nf_qa_relevance = Feedback(openai.relevance).on_input_output()\n\n<span class=\"hljs-comment\"># Question/statement relevance between question and each context chunk.</span>\nf_qs_relevance = Feedback(openai.qs_relevance).on_input().on(\n    TruLlama.select_source_nodes().node.text\n).aggregate(np.<span class=\"hljs-built_in\">min</span>)\n\n\nfeedbacks = [f_lang_match, f_qa_relevance, f_qs_relevance]\n\nl = TruLlama(app=query_engine, feedbacks=feedbacks)</span></pre>\n <h2>\n  Explore In Dashboard\n </h2>\n <p>\n  Every query that is tracked can now be viewed in the TruLens dashboard. After running the feedback functions on a set of records (interactions), you can see the aggregate results of the evaluation on a leaderboard; then drill down into an app version and examine how it is performing on individual records. These steps can help you understand the quality of an app version and its failure modes.\n </p>\n <p>\n  In this example, the model is doing fairly well on the relevance and language match feedback evaluations, but seems to be doing poorly on qs_relevance. This can be an indicator that the retrieved chunks are often irrelevant. This can be a significant source of \u201challucinations\u201d in retrieval-augmented generative AI apps.\n </p>\n <p>\n  We can now drill down and identify specific instances where this may be an issue:\n </p>\n <p>\n  Let\u2019s look at a good example first. \u201cWhat did the author do growing up?\u201d\n </p>\n <p>\n  In this example, we retrieved two chunks from the index both of which were fairly relevant to the the question and as a result the LLM summarizes it into a relevant and factually correct answer.\n </p>\n <p>\n  On the other hand, let\u2019s look at an example where this didn\u2019t go so well: \u201cWhere was the author born?\u201d. In this example, the app confidently provides an incorrect answer.\n </p>\n <p>\n  In this example, the two pieces of context retrieved had moderate relevance to the question. Further, neither context contained the answer. Even though our relevance feedback function (which doesn\u2019t check for factual correctness) didn\u2019t detect an issue, because the underlying chunks were not very relevant, this was a strong indicator that something was off. Indeed, this is an example of the model hallucinating on a question that is fairly easy to fact check.\n </p>\n <h2>\n  Iterate on your App\n </h2>\n <p>\n  Once you find issues like this with your app, it can be helpful to iterate on your prompts, models and chunking approaches to optimize your app. As you do this, you can track the performance of each version of your model with TruLens. Here is an example of a dashboard with multiple iterations testing against each other.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 8776, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa501d43-7ee2-4310-a6bb-bac4f0169314": {"__data__": {"id_": "aa501d43-7ee2-4310-a6bb-bac4f0169314", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.html", "file_name": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.html", "file_type": "text/html", "file_size": 27200, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.html", "file_name": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.html", "file_type": "text/html", "file_size": 27200, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "c09c8a73cb9cf8549563f71f6069ea8468573b66b05b90ee565f46b2c35b69d3", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Co-authors: Jerry Liu (CEO at LlamaIndex), Amog Kamsetty (Software Engineer at Anyscale)\n </p>\n <p>\n  (\n  <strong>\n   note:\n  </strong>\n  this is cross-posted from the original blog post on Anyscale\u2019s website.\n  <a href=\"https://www.anyscale.com/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-ray\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Check it out here\n  </a>\n  !)\n </p>\n <p>\n  In this blog, we showcase how you can use LlamaIndex and Ray to build a query engine to answer questions and generate insights about Ray itself, given its documentation and blog posts.\n </p>\n <p>\n  We\u2019ll give a quick introduction of LlamaIndex + Ray, and then walk through a step-by-step tutorial on building and deploying this query engine. We make use of both Ray Datasets to parallelize building indices as well as Ray Serve to build deployments.\n </p>\n <h1>\n  Introduction\n </h1>\n <p>\n  Large Language Models (LLMs) offer the promise of allowing users to extract complex insights from their unstructured text data. Retrieval-augmented generation pipelines have emerged as a common pattern for developing LLM applications allowing users to effectively perform semantic search over a collection of documents.\n </p>\n <figure>\n  <figcaption class=\"pu fe pv nz oa pw px be b bf z dt\">\n   <em class=\"py\">\n    Example of retrieval augmented generation. Relevant context is pulled from a set of documents and included in the LLM input prompt.\n   </em>\n  </figcaption>\n </figure>\n <p>\n  However, when productionizing these applications over many different data sources, there are a few challenges:\n </p>\n <ol>\n  <li>\n   Tooling for indexing data from many different data sources\n  </li>\n  <li>\n   Handling complex queries over different data sources\n  </li>\n  <li>\n   Scaling indexing to thousands or millions of documents\n  </li>\n  <li>\n   Deploying a scalable LLM application into production\n  </li>\n </ol>\n <p>\n  Here, we showcase how\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaIndex\n  </a>\n  and\n  <a href=\"https://docs.ray.io/en/latest/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Ray\n  </a>\n  are the perfect setup for this task.\n </p>\n <p>\n  LlamaIndex is a data framework for building LLM applications, and solves Challenges #1 and #2. It also provides a comprehensive toolkit allowing users to connect their private data with a language model. It offers a variety of tools to help users first ingest and index their data \u2014 convert different formats of unstructured and structured data into a format that the language model can use, and query their private data.\n </p>\n <p>\n  Ray is a powerful framework for scalable AI that solves Challenges #3 and #4. We can use it to dramatically accelerate ingest, inference, pretraining, and also effortlessly deploy and scale the query capabilities of LlamaIndex into the cloud.\n </p>\n <p>\n  More specifically, we showcase a very relevant use case \u2014 highlighting Ray features that are present in both the documentation as well as the Ray blog posts!\n </p>\n <h1>\n  Data Ingestion and Embedding Pipeline\n </h1>\n <p>\n  We use LlamaIndex + Ray to ingest, parse, embed and store Ray docs and blog posts in a parallel fashion. For the most part, these steps are duplicated across the two data sources, so we show the steps for just the documentation below.\n </p>\n <p>\n  Code for this part of the blog is\n  <a href=\"https://github.com/amogkam/llama_index_ray/blob/main/create_vector_index.py\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   available here\n  </a>\n  .\n </p>\n <figure>\n  <figcaption class=\"pu fe pv nz oa pw px be b bf z dt\">\n   <em class=\"py\">\n    Sequential pipeline with \u201cingest\u201d, \u201cparse\u201d and \u201cembed\u201d stages. Files are processed sequentially resulting in poor hardware utilization and long computation time.\n   </em>\n  </figcaption>\n </figure>\n <figure>\n  <figcaption class=\"pu fe pv nz oa pw px be b bf z dt\">\n   <em class=\"py\">\n    Parallel pipeline. Thanks to Ray we can process multiple input files simultaneously. Parallel processing has much better performance, because hardware is better utilized.\n   </em>\n  </figcaption>\n </figure>\n <h1>\n  Load Data\n </h1>\n <p>\n  We start by ingesting these two sources of data. We first fetch both data sources and download the HTML files.\n </p>\n <p>\n  We then need to load and parse these files. We can do this with the help of LlamaHub, our community-driven repository of 100+ data loaders from various API\u2019s, file formats (.pdf, .html, .docx), and databases. We use an HTML data loader offered by\n  <a href=\"https://github.com/Unstructured-IO/unstructured\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Unstructured\n  </a>\n  .\n </p>\n <pre><span class=\"ql or gt qi b bf qm qn l qo qp\" id=\"ff6e\"><span class=\"hljs-keyword\">from</span> typing <span class=\"hljs-keyword\">import</span> <span class=\"hljs-type\">Dict</span>, <span class=\"hljs-type\">List</span>\n<span class=\"hljs-keyword\">from</span> pathlib <span class=\"hljs-keyword\">import</span> Path\n\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> download_loader\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> Document\n\n<span class=\"hljs-comment\"># Step 1: Logic for loading and parsing the files into llama_index documents.</span>\nUnstructuredReader = download_loader(<span class=\"hljs-string\">\"UnstructuredReader\"</span>)\nloader = UnstructuredReader()\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">load_and_parse_files</span>(<span class=\"hljs-params\">file_row: <span class=\"hljs-type\">Dict</span>[<span class=\"hljs-built_in\">str</span>, Path]</span>) -&amp;gt; <span class=\"hljs-type\">Dict</span>[<span class=\"hljs-built_in\">str</span>, Document]:\n    documents = []\n    file = file_row[<span class=\"hljs-string\">\"path\"</span>]\n    <span class=\"hljs-keyword\">if</span> file.is_dir():\n        <span class=\"hljs-keyword\">return</span> []\n    <span class=\"hljs-comment\"># Skip all non-html files like png, jpg, etc.</span>\n    <span class=\"hljs-keyword\">if</span> file.suffix.lower() == <span class=\"hljs-string\">\".html\"</span>:\n        loaded_doc = loader.load_data(file=file, split_documents=<span class=\"hljs-literal\">False</span>)\n        loaded_doc[<span class=\"hljs-number\">0</span>].extra_info = {<span class=\"hljs-string\">\"path\"</span>: <span class=\"hljs-built_in\">str</span>(file)}\n        documents.extend(loaded_doc)\n    <span class=\"hljs-keyword\">return</span> [{<span class=\"hljs-string\">\"doc\"</span>: doc} <span class=\"hljs-keyword\">for</span> doc <span class=\"hljs-keyword\">in</span> documents]</span></pre>\n <p>\n  Unstructured offers a robust suite of parsing tools on top of various files. It is able to help sanitize HTML documents by stripping out information like tags and formatting the text accordingly.\n </p>\n <h2>\n  Scaling Data Ingest\n </h2>\n <p>\n  Since we have many HTML documents to process, loading/processing each one serially is inefficient and slow. This is an opportunity to use Ray and distribute execution of the `load_and_parse_files` method across multiple CPUs or GPUs.\n </p>\n <pre><span class=\"ql or gt qi b bf qm qn l qo qp\" id=\"7248\">import ray\n\n<span class=\"hljs-comment\"># Get the paths for the locally downloaded documentation.</span>\nall_docs_gen = <span class=\"hljs-title class_\">Path</span>(<span class=\"hljs-string\">\"./docs.ray.io/\"</span>).rglob(<span class=\"hljs-string\">\"*\"</span>)\nall_docs = [{<span class=\"hljs-string\">\"path\"</span>: doc.resolve()} <span class=\"hljs-keyword\">for</span> doc <span class=\"hljs-keyword\">in</span> all_docs_gen]\n\n<span class=\"hljs-comment\"># Create the Ray Dataset pipeline</span>\nds = ray.data.from_items(all_docs)\n\n<span class=\"hljs-comment\"># Use `flat_map` since there is a 1:N relationship.</span>\n<span class=\"hljs-comment\"># Each filepath returns multiple documents.</span>\nloaded_docs = ds.flat_map(load_and_parse_files)</span></pre>\n <h1>\n  Parse Files\n </h1>\n <p>\n  Now that we\u2019ve loaded the documents, the next step is to parse them into Node objects \u2014 a \u201cNode\u201d object represents a more granular chunk of text, derived from the source documents. Node objects can be used in the input prompt as context; by setting a small enough chunk size, we can make sure that inserting Node objects do not overflow the context limits.\n </p>\n <p>\n  We define a function called `convert_documents_into_nodes` which converts documents into nodes using a simple text splitting strategy.\n </p>\n <pre><span class=\"ql or gt qi b bf qm qn l qo qp\" id=\"4064\"><span class=\"hljs-comment\"># Step 2: Convert the loaded documents into llama_index Nodes. This will split the documents into chunks.</span>\n<span class=\"hljs-keyword\">from</span> llama_index.node_parser <span class=\"hljs-keyword\">import</span> SimpleNodeParser\n<span class=\"hljs-keyword\">from</span> llama_index.data_structs <span class=\"hljs-keyword\">import</span> Node\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">convert_documents_into_nodes</span>(<span class=\"hljs-params\">documents: <span class=\"hljs-type\">Dict</span>[<span class=\"hljs-built_in\">str</span>, Document]</span>) -&amp;gt; <span class=\"hljs-type\">Dict</span>[<span class=\"hljs-built_in\">str</span>, Node]:\n    parser = SimpleNodeParser()\n    document = documents[<span class=\"hljs-string\">\"doc\"</span>]\n    nodes = parser.get_nodes_from_documents([document]) \n    <span class=\"hljs-keyword\">return</span> [{<span class=\"hljs-string\">\"node\"</span>: node} <span class=\"hljs-keyword\">for</span> node <span class=\"hljs-keyword\">in</span> nodes]</span></pre>\n <h2>\n  Run Parsing in Parallel\n </h2>\n <p>\n  Since we have many documents, processing each document into nodes serially is inefficient and slow. We use Ray `flat_map` method to process documents into nodes in parallel:\n </p>\n <pre><span class=\"ql or gt qi b bf qm qn l qo qp\" id=\"b29b\">\n# Use `flat_map` since there is a 1:N relationship. Each document returns multiple nodes.\nnodes = loaded_docs.flat_map(convert_documents_into_nodes)</span></pre>\n <h1>\n  Generate Embeddings\n </h1>\n <p>\n  We then generate embeddings for each Node using a Hugging Face Sentence Transformers model. We can do this with the help of LangChain\u2019s embedding abstraction.\n </p>\n <p>\n  Similar to document loading/parsing, embedding generation can similarly be parallelized with Ray. We wrap these embedding operations into a helper class, called `EmbedNodes`, to take advantage of Ray abstractions.\n </p>\n <pre><span class=\"ql or gt qi b bf qm qn l qo qp\" id=\"32a9\"><span class=\"hljs-comment\"># Step 3: Embed each node using a local embedding model.</span>\n<span class=\"hljs-keyword\">from</span> langchain.embeddings.huggingface <span class=\"hljs-keyword\">import</span> HuggingFaceEmbeddings\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">EmbedNodes</span>:\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self</span>):\n        self.embedding_model = HuggingFaceEmbeddings(\n            <span class=\"hljs-comment\"># Use all-mpnet-base-v2 Sentence_transformer.</span>\n            <span class=\"hljs-comment\"># This is the default embedding model for LlamaIndex/Langchain.</span>\n            model_name=<span class=\"hljs-string\">\"sentence-transformers/all-mpnet-base-v2\"</span>, \n            model_kwargs={<span class=\"hljs-string\">\"device\"</span>: <span class=\"hljs-string\">\"cuda\"</span>},\n            <span class=\"hljs-comment\"># Use GPU for embedding and specify a large enough batch size to maximize GPU utilization.</span>\n            <span class=\"hljs-comment\"># Remove the \"device\": \"cuda\" to use CPU instead.</span>\n            encode_kwargs={<span class=\"hljs-string\">\"device\"</span>: <span class=\"hljs-string\">\"cuda\"</span>, <span class=\"hljs-string\">\"batch_size\"</span>: <span class=\"hljs-number\">100</span>}\n            )\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__call__</span>(<span class=\"hljs-params\">self, node_batch: <span class=\"hljs-type\">Dict</span>[<span class=\"hljs-built_in\">str</span>, <span class=\"hljs-type\">List</span>[Node]]</span>) -&amp;gt; <span class=\"hljs-type\">Dict</span>[<span class=\"hljs-built_in\">str</span>, <span class=\"hljs-type\">List</span>[Node]]:\n        nodes = node_batch[<span class=\"hljs-string\">\"node\"</span>]\n        text = [node.text <span class=\"hljs-keyword\">for</span> node <span class=\"hljs-keyword\">in</span> nodes]\n        embeddings = self.embedding_model.embed_documents(text)\n        <span class=\"hljs-keyword\">assert</span> <span class=\"hljs-built_in\">len</span>(nodes) == <span class=\"hljs-built_in\">len</span>(embeddings)\n\n        <span class=\"hljs-keyword\">for</span> node, embedding <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">zip</span>(nodes, embeddings):\n            node.embedding = embedding\n        <span class=\"hljs-keyword\">return</span> {<span class=\"hljs-string\">\"embedded_nodes\"</span>: nodes}</span></pre>\n <p>\n  Afterwards, generating an embedding for each node is as simple as calling the following operation in Ray:\n </p>\n <pre><span class=\"ql or gt qi b bf qm qn l qo qp\" id=\"b2a1\"><span class=\"hljs-comment\"># Use `map_batches` to specify a batch size to maximize GPU utilization.</span>\n<span class=\"hljs-comment\"># We define `EmbedNodes` as a class instead of a function so we only initialize the embedding model once. </span>\n\n<span class=\"hljs-comment\"># This state can be reused for multiple batches.</span>\nembedded_nodes = nodes.map_batches(\n    EmbedNodes, \n    batch_size=<span class=\"hljs-number\">100</span>, \n    <span class=\"hljs-comment\"># Use 1 GPU per actor.</span>\n    num_gpus=<span class=\"hljs-number\">1</span>,\n    <span class=\"hljs-comment\"># There are 4 GPUs in the cluster. Each actor uses 1 GPU. So we want 4 total actors.</span>\n    compute=ActorPoolStrategy(size=<span class=\"hljs-number\">4</span>))\n\n<span class=\"hljs-comment\"># Step 5: Trigger execution and collect all the embedded nodes.</span>\nray_docs_nodes = []\n<span class=\"hljs-keyword\">for</span> row <span class=\"hljs-keyword\">in</span> embedded_nodes.iter_rows():\n    node = row[<span class=\"hljs-string\">\"embedded_nodes\"</span>]\n    <span class=\"hljs-keyword\">assert</span> node.embedding <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>\n    ray_docs_nodes.append(node)</span></pre>\n <h1>\n  Data Indexing\n </h1>\n <p>\n  The next step is to store these nodes within an \u201cindex\u201d in LlamaIndex. An index is a core abstraction in LlamaIndex to \u201cstructure\u201d your data in a certain way \u2014 this structure can then be used for downstream LLM retrieval + querying. An index can interface with a storage or vector store abstraction.\n </p>\n <p>\n  The most commonly used index abstraction within LlamaIndex is our vector index, where each node is stored along with an embedding. In this example, we use a simple in-memory vector store, but you can also choose to specify any one of LlamaIndex\u2019s 10+ vector store integrations as the storage provider (e.g. Pinecone, Weaviate, Chroma).\n </p>\n <p>\n  We build two vector indices: one over the documentation nodes, and another over the blog post nodes and persist them to disk. Code is\n  <a href=\"https://github.com/amogkam/llama_index_ray/blob/main/create_vector_index.py#L102:L131\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   available here\n  </a>\n  .\n </p>\n <pre><span class=\"ql or gt qi b bf qm qn l qo qp\" id=\"569b\">from llama_index import GPTVectorStoreIndex\n\n# Store Ray Documentation embeddings\nray_docs_index = GPTVectorStoreIndex(nodes=ray_docs_nodes)\nray_docs_index.storage_context.persist(persist_dir=\"/tmp/ray_docs_index\")\n\n# Store Anyscale blog post embeddings\nray_blogs_index = GPTVectorStoreIndex(nodes=ray_blogs_nodes)\nray_blogs_index.storage_context.persist(persist_dir=\"/tmp/ray_blogs_index\")</span></pre>\n <p>\n  <strong>\n   That\u2019s it in terms of building a data pipeline using LlamaIndex + Ray Data\n  </strong>\n  !\n </p>\n <p>\n  Your data is now ready to be used within your LLM application. Check out our next section for how to use advanced LlamaIndex query capabilities on top of your data.\n </p>\n <h1>\n  Data Querying\n </h1>\n <p>\n  LlamaIndex provides both simple and advanced query capabilities on top of your data + indices. The central abstraction within LlamaIndex is called a \u201cquery engine.\u201d A query engine takes in a natural language query input and returns a natural language \u201coutput\u201d. Each index has a \u201cdefault\u201d corresponding query engine. For instance, the default query engine for a vector index first performs top-k retrieval over the vector store to fetch the most relevant documents.\n </p>\n <p>\n  These query engines can be easily derived from each index:\n </p>\n <pre><span class=\"ql or gt qi b bf qm qn l qo qp\" id=\"0f91\">ray_docs_engine = ray_docs_index.as_query_engine(similarity_top_k=5, service_context=service_context)\n\nray_blogs_engine = ray_blogs_index.as_query_engine(similarity_top_k=5, service_context=service_context)</span></pre>\n <p>\n  LlamaIndex also provides more advanced query engines for multi-document use cases \u2014 for instance, we may want to ask how a given feature in Ray is highlighted in both the documentation and blog. `SubQuestionQueryEngine` can take in other query engines as input. Given an existing question, it can decide to break down the question into simpler questions over any subset of query engines; it will execute the simpler questions and combine results at the top-level.\n </p>\n <p>\n  This abstraction is quite powerful; it can perform semantic search over one document, or combine results across multiple documents.\n </p>\n <p>\n  For instance, given the following question \u201cWhat is Ray?\u201d, we can break this into sub-questions \u201cWhat is Ray according to the documentation\u201d, and \u201cWhat is Ray according to the blog posts\u201d over the document query engine and blog query engine respectively.\n </p>\n <pre><span class=\"ql or gt qi b bf qm qn l qo qp\" id=\"86bd\"># Define a sub-question query engine, that can use the individual query engines as tools.\n        query_engine_tools = [\n            QueryEngineTool(\n                query_engine=self.ray_docs_engine,\n                metadata=ToolMetadata(name=\"ray_docs_engine\", description=\"Provides information about the Ray documentation\")\n            ),\n            QueryEngineTool(\n                query_engine=self.ray_blogs_engine, \n                metadata=ToolMetadata(name=\"ray_blogs_engine\", description=\"Provides information about Ray blog posts\")\n            ),\n        ]\n\nsub_query_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools, service_context=service_context, use_async=False)</span></pre>\n <p>\n  Have a look at\n  <a href=\"https://github.com/amogkam/llama_index_ray/blob/main/deploy_app.py#L22:L56\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   deploy_app.py\n  </a>\n  to review the full implementation.\n </p>\n <h1>\n  Deploying with Ray Serve\n </h1>\n <p>\n  We\u2019ve now created an incredibly powerful query module over your data. As a next step, what if we could seamlessly deploy this function to production and serve users? Ray Serve makes this incredibly easy to do. Ray Serve is a scalable compute layer for serving ML models and LLMs that enables serving individual models or creating composite model pipelines where you can independently deploy, update, and scale individual components.\n </p>\n <p>\n  To do this, you just need to do the following steps:\n </p>\n <ol>\n  <li>\n   Define an outer class that can \u201cwrap\u201d a query engine, and expose a \u201cquery\u201d endpoint\n  </li>\n  <li>\n   Add a `@ray.serve.deployment` decorator on this class\n  </li>\n  <li>\n   Deploy the Ray Serve application\n  </li>\n </ol>\n <p>\n  It will look something like the following:\n </p>\n <pre><span class=\"ql or gt qi b bf qm qn l qo qp\" id=\"1d2e\"><span class=\"hljs-keyword\">from</span> ray <span class=\"hljs-keyword\">import</span> serve\n\n<span class=\"hljs-meta\">@serve.deployment</span>\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">QADeployment</span>:\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self</span>):\n self.query_engine = ...\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">query</span>(<span class=\"hljs-params\">self, query: <span class=\"hljs-built_in\">str</span></span>):\n            response =  self.query_engine.query(query)\n            source_nodes = response.source_nodes\n            source_str = <span class=\"hljs-string\">\"\"</span>\n            <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-built_in\">len</span>(source_nodes)):\n                node = source_nodes[i]\n                source_str += <span class=\"hljs-string\">f\"Sub-question <span class=\"hljs-subst\">{i+<span class=\"hljs-number\">1</span>}</span>:\\n\"</span>\n                source_str += node.node.text\n                source_str += <span class=\"hljs-string\">\"\\n\\n\"</span>\n            <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">f\"Response: <span class=\"hljs-subst\">{<span class=\"hljs-built_in\">str</span>(response)}</span> \\n\\n\\n <span class=\"hljs-subst\">{source_str}</span>\\n\"</span>\n\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__call__</span>(<span class=\"hljs-params\">self, request: Request</span>):\n        query = request.query_params[<span class=\"hljs-string\">\"query\"</span>]\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">str</span>(self.query(query))\n\n<span class=\"hljs-comment\"># Deploy the Ray Serve application.</span>\ndeployment = QADeployment.bind()</span></pre>\n <p>\n  Have a look at the\n  <a href=\"https://github.com/amogkam/llama_index_ray/blob/main/deploy_app.py\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   deploy_app.py\n  </a>\n  for full implementation.\n </p>\n <h1>\n  Example Queries\n </h1>\n <p>\n  Once we\u2019ve deployed the application, we can query it with questions about Ray.\n </p>\n <p>\n  We can query just one of the data sources:\n </p>\n <pre><span class=\"ql or gt qi b bf qm qn l qo qp\" id=\"3c55\">Q: \"What is Ray Serve?\"\n\nRay Serve is a system for deploying and managing applications on a Ray\ncluster. It provides APIs for deploying applications, managing replicas, and\nmaking requests to applications. It also provides a command line interface\n(CLI) for managing applications and a dashboard for monitoring applications.</span></pre>\n <p>\n  But, we can also provide complex queries that require synthesis across both the documentation and the blog posts. These complex queries are easily handled by the subquestion-query engine that we defined.\n </p>\n <pre><span class=\"ql or gt qi b bf qm qn l qo qp\" id=\"5b3a\">Q: \"Compare and contrast how the Ray docs and the Ray blogs present Ray Serve\"\n\nResponse: \nThe Ray docs and the Ray blogs both present Ray Serve as a web interface\nthat provides metrics, charts, and other features to help Ray users\nunderstand and debug Ray applications. However, the Ray docs provide more\ndetailed information, such as a Quick Start guide, user guide, production\nguide, performance tuning guide, development workflow guide, API reference,\nexperimental Java API, and experimental gRPC support. Additionally, the Ray\ndocs provide a guide for migrating from 1.x to 2.x. On the other hand, the\nRay blogs provide a Quick Start guide, a User Guide, and Advanced Guides to\nhelp users get started and understand the features of Ray Serve.\nAdditionally, the Ray blogs provide examples and use cases to help users\nunderstand how to use Ray Serve in their own projects.\n\n---\n\nSub-question 1\n\nSub question: How does the Ray docs present Ray Serve\n\nResponse: \nThe Ray docs present Ray Serve as a web interface that provides metrics,\ncharts, and other features to help Ray users understand and debug Ray\napplications. It provides a Quick Start guide, user guide, production guide,\nperformance tuning guide, and development workflow guide. It also provides\nan API reference, experimental Java API, and experimental gRPC support.\nFinally, it provides a guide for migrating from 1.x to 2.x.\n\n---\n\nSub-question 2\n\nSub question: How does the Ray blogs present Ray Serve\n\nResponse: \nThe Ray blog presents Ray Serve as a framework for distributed applications\nthat enables users to handle HTTP requests, scale and allocate resources,\ncompose models, and more. It provides a Quick Start guide, a User Guide, and\nAdvanced Guides to help users get started and understand the features of Ray\nServe. Additionally, it provides examples and use cases to help users\nunderstand how to use Ray Serve in their own projects.</span></pre>\n <h1>\n  Conclusion\n </h1>\n <p>\n  In this example, we showed how you can build a scalable data pipeline and a powerful query engine using LlamaIndex + Ray. We also demonstrated how to deploy LlamaIndex applications using Ray Serve. This allows you to effortlessly ask questions and synthesize insights about Ray across disparate data sources!\n </p>\n <p>\n  We used LlamaIndex \u2014 a data framework for building LLM applications \u2014 to load, parse, embed and index the data. We ensured efficient and fast parallel execution by using Ray. Then, we used LlamaIndex querying capabilities to perform semantic search over a single document, or combine results across multiple documents. Finally, we used Ray Serve to package the application for production use.\n </p>\n <p>\n  Implementation in open source, code is available on GitHub:\n  <a href=\"https://github.com/amogkam/llama_index_ray\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaIndex-Ray-app\n  </a>\n </p>\n <h1>\n  What\u2019s next?\n </h1>\n <p>\n  Visit LlamaIndex\n  <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   site\n  </a>\n  and\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   docs\n  </a>\n  to learn more about this data framework for building LLM applications.\n </p>\n <p>\n  Visit\n  <a href=\"https://docs.ray.io/en/latest/ray-overview/use-cases.html#llms-and-gen-ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Ray docs\n  </a>\n  to learn more about how to build and deploy scalable LLM apps.\n </p>\n <p>\n  Join our communities!\n </p>\n <ul>\n  <li>\n   <a href=\"https://forms.gle/9TSdDYUgxYs8SA9e8\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Join Ray community\n   </a>\n   on Slack and Ray #LLM channel.\n  </li>\n  <li>\n   You can also join the LlamaIndex\n   <a href=\"https://discord.gg/UB58qbeq\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    community on discord\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  We have our\n  <a href=\"https://raysummit.anyscale.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Ray Summit 2023\n  </a>\n  early-bird registration open until 6/30. Secure your spot, save some money, savor the community camaraderie at the summit.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 27109, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bcdb8818-7166-47e9-adb9-9b7fcacdd629": {"__data__": {"id_": "bcdb8818-7166-47e9-adb9-9b7fcacdd629", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.html", "file_name": "building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.html", "file_type": "text/html", "file_size": 8818, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.html", "file_name": "building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.html", "file_type": "text/html", "file_size": 8818, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "97251c2225b2a5a3c110438fb6617c6d335393bcfe50502a3e62cfc776d0972f", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h1>\n  What is a Retriever?\n </h1>\n <p>\n  Recently, retrieval augmented generation (RAG) has enabled language models to reduce hallucinations, improve response quality, and maintain up-to-date knowledge of the world\n  <em class=\"pk\">\n   without\n  </em>\n  requiring retraining of the model itself. This is done by equipping a language model with a\n  <em class=\"pk\">\n   retriever\n  </em>\n  and a\n  <em class=\"pk\">\n   database.\n  </em>\n  At inference time, a RAG system uses the retriever to select relevant documents from the database, and passes them to the language model context window.\n </p>\n <p>\n  Today, the most popular type of retriever is based on an embedding model. This embedding model converts all of the documents in the database to a vector representation. Then, at inference time, it converts the query to a vector representation, and retrieves the most similar documents to the query vector from the database.\n </p>\n <p>\n  In this post, we are going to show you how to build a fully open source retriever using LlamaIndex and Nomic Embed, the first fully open source embedding model to exceed OpenAI Ada performance on both short and long context benchmarks.\n </p>\n <h1>\n  Why Open Source?\n </h1>\n <p>\n  As AI becomes deployed in increasingly high impact domains, such as defense, medicine, and finance, end-to-end auditability of the entire system becomes a key component of safe AI deployment. Unfortunately, the closed source embedding models used in most RAG systems today have deliberately obfuscated training protocols and cannot be audited.\n </p>\n <p>\n  Further, as organizations adopting AI begin to mature, reliance on closed source embedding models will result in vendor lock-in and a limited ability to modify the embedding model to suit the needs of the business.\n </p>\n <p>\n  Luckily, fully open source embedding models like Nomic Embed offer end-to-end auditability of the training process as well as a strong basis for further improvements and modifications of the model.\n </p>\n <h1>\n  How To\n </h1>\n <p>\n  To build an open source retriever with LlamaIndex and Nomic Embed, we will start by importing the relevant libraries\n </p>\n <pre><span class=\"pz np gt pw b bf qa qb l qc qd\" id=\"b238\"><span class=\"hljs-keyword\">from</span> llama_index.embeddings <span class=\"hljs-keyword\">import</span> NomicEmbedding\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> (\n    VectorStoreIndex,\n    SimpleDirectoryReader,\n    ServiceContext,\n)</span></pre>\n <p>\n  Next, we need to download some data for our database. For this example, we are going to use an essay by Paul Graham, which we download from\n  <a href=\"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   here\n  </a>\n  and place into a directory named ./data/paul_graham.\n </p>\n <p>\n  Now, it\u2019s time to get the vectors for the documents in our database. To do this, we are going to use the LlamaIndex SimpleDirectoryReader and Nomic\u2019s hosted inference service. You\u2019ll have to replace &lt;NOMIC_API_KEY&gt; with your Nomic API key, which you can get after signing up for Nomic Atlas\n  <a href=\"https://atlas.nomic.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   here\n  </a>\n  .\n </p>\n <pre><span class=\"pz np gt pw b bf qa qb l qc qd\" id=\"d514\">documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\nnomic_api_key = \"<span class=\"hljs-symbol\">&amp;lt;</span>NOMIC_API_KEY<span class=\"hljs-symbol\">&amp;gt;</span>\"\nembed_model = NomicEmbedding(\n    api_key=nomic_api_key,\n    model_name=\"nomic-embed-text-v1\",\n    task_type=\"search_document\"\n)\nservice_context = ServiceContext.from_defaults(\n    embed_model=embed_model, chunk_size=1024,\n)\nindex = VectorStoreIndex.from_documents(\n    documents=documents, service_context=service_context, show_progress=True\n)</span></pre>\n <p>\n  Notice that we set task_type to search_document in NomicEmbedding. Nomic Embed supports many different types of tasks, and search_document is optimized for building representations of documents for RAG databases.\n </p>\n <p>\n  Once our database is set up, we are ready to build our retriever. Using LlamaIndex, this is as simple as a few lines of python:\n </p>\n <pre><span class=\"pz np gt pw b bf qa qb l qc qd\" id=\"8cdb\">embed_model = NomicEmbedding(\n    api_key=nomic_api_key,\n    model_name=\"nomic-embed-text-v1\",\n    task_type=\"search_query\"\n)\n\nservice_context = ServiceContext.from_defaults(\n    embed_model=embed_model\n)\n\nsearch_query_retriever = index.as_retriever(service_context=service_context, similarity_top_k=1)</span></pre>\n <p>\n  Again, notice that we are using a new NomicEmbedding model with task_type set to search_query. This task type is optimized for embedding queries for search over a retrieval database.\n </p>\n <p>\n  Finally, we can use our retriever to surface relevant documents given user queries! As an example:\n </p>\n <pre><span class=\"pz np gt pw b bf qa qb l qc qd\" id=\"2400\">retrieved_nodes_nomic = retriever_nomic.retrieve(\n    \"What software did Paul write?\"\n)</span></pre>\n <p>\n  returns a document that describes Paul\u2019s first programs:\n </p>\n <pre><span class=\"pz np gt pw b bf qa qb l qc qd\" id=\"43d3\">Node ID: 380fbb0e-6fc1-41de-a4f6-3f22cd508df3\nSimilarity: <span class=\"hljs-number\">0.6087318771843091</span>\nText: What I Worked On\n\nFebruary <span class=\"hljs-number\">2021</span>\n\nBefore college the two main things I worked on, outside of school, were writing <span class=\"hljs-keyword\">and</span> programming. I didn<span class=\"hljs-string\">'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district'</span>s <span class=\"hljs-number\">1401</span> happened to be <span class=\"hljs-keyword\">in</span> the basement of our junior high school, <span class=\"hljs-keyword\">and</span> my friend Rich Draves <span class=\"hljs-keyword\">and</span> I got permission to use it. It was like a mini Bond villain<span class=\"hljs-string\">'s lair down there, with all these alien-looking machines \u2014 CPU, disk drives, printer, card reader \u2014 sitting up on a raised floor under bright fluorescent lights.\n\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\n\nI was puzzled by the 1401. I couldn'</span>t figure out what to do <span class=\"hljs-keyword\">with</span> it. And <span class=\"hljs-keyword\">in</span> retrospect there<span class=\"hljs-string\">'s not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn'</span>t have <span class=\"hljs-built_in\">any</span> data stored on punched cards. The only other option was to do things that didn<span class=\"hljs-string\">'t rely on any input, like calculate approximations of pi, but I didn'</span>t know enough math to do anything interesting of that <span class=\"hljs-built_in\">type</span>. So I<span class=\"hljs-string\">'m not surprised I can'</span>t remember <span class=\"hljs-built_in\">any</span> programs I wrote, because they can<span class=\"hljs-string\">'t have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn'</span>t. On a machine without time-sharing, this was a social <span class=\"hljs-keyword\">as</span> well <span class=\"hljs-keyword\">as</span> a technical error, <span class=\"hljs-keyword\">as</span> the data center manage<span class=\"hljs-string\">r's expression made clear.\n\nWith microcomputers, everything changed. </span></span></pre>\n <h1>\n  Conclusion &amp; Next Steps\n </h1>\n <p>\n  In this post, we showed you how to build a fully open source retriever using Nomic Embed and LlamaIndex. If you want to dive deeper, you can find the source code for Nomic Embed\n  <a href=\"https://github.com/nomic-ai/contrastors\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   here\n  </a>\n  . You can also use Nomic\n  <a href=\"https://atlas.nomic.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Atlas\n  </a>\n  to visualize your retrieval database, and\n  <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaIndex\n  </a>\n  to connect it to a generative model for full RAG.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 8805, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e41746c-026f-4163-9c90-e76d8317896c": {"__data__": {"id_": "9e41746c-026f-4163-9c90-e76d8317896c", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-a-multi-agent-concierge-system.html", "file_name": "building-a-multi-agent-concierge-system.html", "file_type": "text/html", "file_size": 22858, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/building-a-multi-agent-concierge-system.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-a-multi-agent-concierge-system.html", "file_name": "building-a-multi-agent-concierge-system.html", "file_type": "text/html", "file_size": 22858, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "b7b67465b0b17a3070e7cdf6896b119d219e43be8c5c931de0c30065385d2a7e", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Why build this?\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Interactive chat bots are by this point a familiar solution to customer service, and agents are a frequent component of chat bot implementations. They provide memory, introspection, tool use and other features necessary for a competent bot.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We have become interested in larger-scale chatbots: ones that can complete dozens of tasks, some of which have dependencies on each other, using hundreds of tools. What would that agent look like? It would have an enormous system prompt and a huge number of tools to choose from, which can be confusing for an agent.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Imagine a bank implementing a system that can:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Look up the price of a specific stock\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Authenticate a user\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Check your account balance\n   <ul>\n    <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n     Which requires the user be authenticated\n    </li>\n   </ul>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Transfer money between accounts\n   <ul>\n    <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n     Which requires the user be authenticated\n    </li>\n    <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n     And also that the user checks their account balance first\n    </li>\n   </ul>\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Each of these top-level tasks has sub-tasks, for instance:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   The stock price lookup might need to look up the stock symbol first\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   The user authentication would need to gather a username and a password\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   The account balance would need to know which of the user's accounts to check\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Coming up with a single primary prompt for all of these tasks and sub-tasks would be very complex. So instead, we designed a multi-agent system with agents responsible for each top-level task, plus a \"concierge\" agent that can direct the user to the correct agent.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  What we built\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We built a system of agents to complete the above tasks.\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/multi-agent-concierge/\" rel=\"noreferrer noopener\">\n   It's open-source\n  </a>\n  ! There are four basic \"task\" agents:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   A stock lookup agent (which takes care of sub-tasks like looking up symbols)\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   An authentication agent (which asks for username and password)\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   An account balance agent (which takes care of sub-tasks like selecting an account)\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   A money transfer agent (which takes care of tasks like asking what account to transfer to, and how much)\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  There are also three \"meta\" agents:\n </p>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   A\n   <strong>\n    concierge agent\n   </strong>\n   : this agent is responsible for interacting with the user when they first arrive, letting them know what sort of tasks are available, and providing feedback when tasks are complete.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   An\n   <strong>\n    orchestration agent\n   </strong>\n   : this agent never provides output directly to the user. Instead, it looks at what the user is currently trying to accomplish, and responds with the plain-text name of the agent that should handle the task. The code then routes to that agent.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   A\n   <strong>\n    continuation agent\n   </strong>\n   : it's sometimes necessary to chain agents together to complete a task. For instance, to check your account balance, you need to be authenticated first. The authentication agent doesn't know if you were simply trying to authenticate yourself or if it's part of a chain, and it doesn't need to. When the authentication agent completes, the continuation agent checks chat history to see what the original task was, and if there's more to do, it formulates a new request to the orchestration agent to get you there without further user input.\n  </li>\n </ol>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  A\n  <strong>\n   global state\n  </strong>\n  keeps track of the user and their current state, shared between all the agents.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The flow of the the system looks something like this:\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  The system in action\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  To get a sense of how this works in practice, here's sample output including helpful debug statements. Output that would be ordinarily shown to the user has two\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   &gt;&gt;\n  </code>\n  , while user input has one\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   &gt;\n  </code>\n  .\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  At the beginning of the conversation nothing's happened yet, so you get routed to the concierge:\n </p>\n <pre><code>No current speaker, asking orchestration agent to decide\n\nConcierge agent selected\n\n&gt;&gt; Hi there! How can I assist you today? Here are some things I can help you with:\n\n&gt;&gt; Looking up a stock price\n\n&gt;&gt; Authenticating you\n\n&gt;&gt; Checking an account balance (requires authentication first)\n\n&gt;&gt; Transferring money between accounts (requires authentication and checking an account balance first)\n\n&gt;&gt; What would you like to do?\n\n&gt; Transfer money</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The \"transfer money\" task requires authentication. The orchestration agent checks if you're authenticated while deciding how to route you (it does this twice for some reason, it's a demo!):\n </p>\n <pre><code>No current speaker, asking orchestration agent to decide\n\nOrchestrator is checking if authenticated\n\nOrchestrator is checking if authenticated\n\nAuth agent selected</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  It correctly determines you're not authenticated, so it routes you to the authentication agent:\n </p>\n <pre><code>&gt;&gt; To transfer money, I need to authenticate you first. Could you please provide your username and password?\n\n&gt; seldo</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  This is a fun part: you've provided input, but it's not sufficient to complete the task (you didn't give a password). So when the flow goes back to the orchestration agent, the global state indicates that the \"authenticate\" agent is already running and hasn't completed yet, so it routes back to the authentication agent, and does that again for the password:\n </p>\n <pre><code>There's already a speaker: authenticate\n\nAuth agent selected\n\nRecording username\n\n&gt;&gt; Thank you! Now, could you please provide your password?\n\n&gt; monkey\n\nThere's already a speaker: authenticate\n\nAuth agent selected\n\nLogging in seldo\n\nChecking if authenticated\n\nAuthentication is complete</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Now the auth agent has called a done() function that indicates to the global state that it has completed its task. So the flow now goes to the continuation agent, which looks at the chat history and sees that the user was trying to transfer money. So it generates a prompt, as if spoken by the user, and sends that to the orchestration agent:\n </p>\n <pre><code>&gt;&gt; You have been successfully authenticated. Another agent will assist you with transferring money.\n\nAsking the continuation agent to decide what to do next\n\nContinuation agent said \"I would like to transfer money.\"\n\nNo current speaker, asking orchestration agent to decide\n\nOrchestrator checking if account has a balance\n\nOrchestrator checking if account has a balance\n\nAccount balance agent selected</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Now you're authenticated, but you haven't checked your balance yet, which the orchestration agent knows is necessary for transferring money. So it routes you to the account balance agent (after checking twice for some reason):\n </p>\n <pre><code>&gt;&gt; Before you can transfer money, you need to check your account balance. Let's start by looking up your account balance. Could you please provide the name of the account you're interested in?\n\n&gt; Checking\n\nThere's already a speaker: account_balance\n\nAccount balance agent selected\n\nLooking up account ID for Checking\n\nLooking up account balance for 1234567890\n\nAccount balance lookup is complete\n\n&gt;&gt; Your Checking account has a balance of $1000. Another agent will assist you with transferring money.\n\nAsking the continuation agent to decide what to do next\n\nContinuation agent said \"I would like to transfer money.\"\n\nNo current speaker, asking orchestration agent to decide\n\nTransfer money agent selected</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The account balance agent asks you which account, uses a tool to get the ID for that account, and then marks itself as done. The continuation agent kicks in again and sees that you still haven't completed your original task of transferring money, so it prompts the orchestrator agent again. Unfortunately the orchestrator gets a little confused, and loops twice before finally routing you to the transfer money agent:\n </p>\n <pre><code>Money transfer is complete\n\n&gt;&gt; Another agent will assist you with transferring money.\n\nAsking the continuation agent to decide what to do next\n\nContinuation agent said \"I would like to transfer money.\"\n\nNo current speaker, asking orchestration agent to decide\n\nTransfer money agent selected\n\nMoney transfer is complete\n\n&gt;&gt; Another agent will assist you with transferring money.\n\nAsking the continuation agent to decide what to do next\n\nContinuation agent said \"I would like to transfer money.\"\n\nNo current speaker, asking orchestration agent to decide\n\nOrchestrator checking if account has a balance\n\nTransfer money agent selected\n\n&gt;&gt; You have already checked your account balance. Please provide the following details to proceed with the money transfer:\n\n&gt;&gt; 1. The account ID to which you want to transfer the money.\n\n&gt;&gt; 2. The amount you want to transfer.\n\n&gt; To account ID 1234324\n\nThere's already a speaker: transfer_money\n\nTransfer money agent selected\n\n&gt;&gt; How much would you like to transfer to account ID 1234324?\n\n&gt; 500\n\nThere's already a speaker: transfer_money\n\nTransfer money agent selected\n\nChecking if balance is sufficient\n\nTransferring 500 from 1234567890 account 1234324\n\nMoney transfer is complete\n\n&gt;&gt; The transfer of $500 to account ID 1234324 has been successfully completed. If you need any further assistance, feel free to ask!\n\nAsking the continuation agent to decide what to do next\n\nContinuation agent said no_further_tasks</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We've reached the end of the task! The continuation agent sees that there are no further tasks, and routes you back to the concierge.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  The code\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Now let's look at some highlights of the code that gets all of this done. The core of the system is a central loop that runs forever. At the core of that is a very simple block that simply asks the orchestration agent who should speak next, and sets the\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   next_speaker\n  </code>\n  value which is contained in the state object that is passed between all the agents. Note that if there's already a sub-agent speaking, that agent gets to keep speaking.\n </p>\n <pre><code>current_history = root_memory.get()\n\n<span class=\"hljs-comment\"># who should speak next?</span>\n<span class=\"hljs-keyword\">if</span> (state[<span class=\"hljs-string\">\"current_speaker\"</span>]):\n  <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"There's already a speaker: <span class=\"hljs-subst\">{state[<span class=\"hljs-string\">'current_speaker'</span>]}</span>\"</span>)\n  next_speaker = state[<span class=\"hljs-string\">\"current_speaker\"</span>]\n<span class=\"hljs-keyword\">else</span>:\n  <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"No current speaker, asking orchestration agent to decide\"</span>)\n  orchestration_response = orchestration_agent_factory(state).chat(\n    user_msg_str, \n    chat_history=current_history\n  )\n  next_speaker = <span class=\"hljs-built_in\">str</span>(orchestration_response).strip()</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The orchestration agent has a very strict prompt; its output only goes to other machines. It includes a natural-language summary of the dependencies between agents:\n </p>\n <pre><code>    system_prompt = (<span class=\"hljs-string\">f\"\"\"\n        You are on orchestration agent.\n        Your job is to decide which agent to run based on the current state of the user and what they've asked to do. Agents are identified by short strings.\n        What you do is return the name of the agent to run next. You do not do anything else.\n        \n        The current state of the user is:\n        <span class=\"hljs-subst\">{pprint.pformat(state, indent=<span class=\"hljs-number\">4</span>)}</span>\n\n        If a current_speaker is already selected in the state, simply output that value.\n\n        If there is no current_speaker value, look at the chat history and the current state and you MUST return one of these strings identifying an agent to run:\n        * \"<span class=\"hljs-subst\">{Speaker.STOCK_LOOKUP.value}</span>\" - if they user wants to look up a stock price (does not require authentication)\n        * \"<span class=\"hljs-subst\">{Speaker.AUTHENTICATE.value}</span>\" - if the user needs to authenticate\n        * \"<span class=\"hljs-subst\">{Speaker.ACCOUNT_BALANCE.value}</span>\" - if the user wants to look up an account balance\n            * If they want to look up an account balance, but they haven't authenticated yet, return \"<span class=\"hljs-subst\">{Speaker.AUTHENTICATE.value}</span>\" instead\n        * \"<span class=\"hljs-subst\">{Speaker.TRANSFER_MONEY.value}</span>\" - if the user wants to transfer money between accounts (requires authentication and checking an account balance first)\n            * If they want to transfer money, but is_authenticated returns false, return \"<span class=\"hljs-subst\">{Speaker.AUTHENTICATE.value}</span>\" instead\n            * If they want to transfer money, but has_balance returns false, return \"<span class=\"hljs-subst\">{Speaker.ACCOUNT_BALANCE.value}</span>\" instead\n        * \"<span class=\"hljs-subst\">{Speaker.CONCIERGE.value}</span>\" - if the user wants to do something else, or hasn't said what they want to do, or you can't figure out what they want to do. Choose this by default.\n\n        Output one of these strings and ONLY these strings, without quotes.\n        NEVER respond with anything other than one of the above five strings. DO NOT be helpful or conversational.\n    \"\"\"</span>)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  A simple if-else block takes the output of the orchestration agent and uses it to instantiate the sub-agent to run next. This is when the state object gets passed to each sub-agent:\n </p>\n <pre><code>        <span class=\"hljs-keyword\">if</span> next_speaker == Speaker.STOCK_LOOKUP:\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Stock lookup agent selected\"</span>)\n            current_speaker = stock_lookup_agent_factory(state)\n            state[<span class=\"hljs-string\">\"current_speaker\"</span>] = next_speaker\n        <span class=\"hljs-keyword\">elif</span> next_speaker == Speaker.AUTHENTICATE:\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Auth agent selected\"</span>)\n            current_speaker = auth_agent_factory(state)\n            state[<span class=\"hljs-string\">\"current_speaker\"</span>] = next_speaker\n        <span class=\"hljs-keyword\">elif</span> next_speaker == Speaker.ACCOUNT_BALANCE:\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Account balance agent selected\"</span>)\n            current_speaker = account_balance_agent_factory(state)\n            state[<span class=\"hljs-string\">\"current_speaker\"</span>] = next_speaker\n        <span class=\"hljs-keyword\">elif</span> next_speaker == Speaker.TRANSFER_MONEY:\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Transfer money agent selected\"</span>)\n            current_speaker = transfer_money_agent_factory(state)\n            state[<span class=\"hljs-string\">\"current_speaker\"</span>] = next_speaker\n        <span class=\"hljs-keyword\">elif</span> next_speaker == Speaker.CONCIERGE:\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Concierge agent selected\"</span>)\n            current_speaker = concierge_agent_factory(state)\n        <span class=\"hljs-keyword\">else</span>:\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Orchestration agent failed to return a valid speaker; ask it to try again\"</span>)\n            is_retry = <span class=\"hljs-literal\">True</span>\n            <span class=\"hljs-keyword\">continue</span></code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  And then the full chat history is passed as part of a regular chat message to the newly-instantiated agent:\n </p>\n <pre><code>response = current_speaker.chat(user_msg_str, chat_history=current_history)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The agent reads its prompt and the user input and decides what to say. As we saw in our very first block of code, if the speaker is already selected, then the loop will keep talking to the current sub-agent. This continues until the sub-agent has completed its task, at which point its prompt instructs it to call the\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   done()\n  </code>\n  function:\n </p>\n <pre><code>    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">done</span>() -&gt; <span class=\"hljs-literal\">None</span>:\n        <span class=\"hljs-string\">\"\"\"When you complete your task, call this tool.\"\"\"</span>\n        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Money transfer is complete\"</span>)\n        state[<span class=\"hljs-string\">\"current_speaker\"</span>] = <span class=\"hljs-literal\">None</span>\n        state[<span class=\"hljs-string\">\"just_finished\"</span>] = <span class=\"hljs-literal\">True</span></code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  This modifies the state, setting the current speaker to none. This triggers the outer loop to run the continuation agent, to see if there's anything else to do:\n </p>\n <pre><code>        <span class=\"hljs-keyword\">elif</span> state[<span class=\"hljs-string\">\"just_finished\"</span>] == <span class=\"hljs-literal\">True</span>:\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Asking the continuation agent to decide what to do next\"</span>)\n            user_msg_str = <span class=\"hljs-built_in\">str</span>(continuation_agent_factory(state).chat(<span class=\"hljs-string\">\"\"\"\n                Look at the chat history to date and figure out what the user was originally trying to do.\n                They might have had to do some sub-tasks to complete that task, but what we want is the original thing they started out trying to do.                                                                      \n                Formulate a sentence as if written by the user that asks to continue that task.\n                If it seems like the user really completed their task, output \"no_further_task\" only.\n            \"\"\"</span>, chat_history=current_history))\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Continuation agent said <span class=\"hljs-subst\">{user_msg_str}</span>\"</span>)\n            <span class=\"hljs-keyword\">if</span> user_msg_str == <span class=\"hljs-string\">\"no_further_task\"</span>:\n                user_msg_str = <span class=\"hljs-built_in\">input</span>(<span class=\"hljs-string\">\"&gt;&gt; \"</span>).strip()\n            state[<span class=\"hljs-string\">\"just_finished\"</span>] = <span class=\"hljs-literal\">False</span></code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The continuation agent's prompt instructs it to reply as if it were the user asking to perform a task, or to output\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   no_further_task\n  </code>\n  if there's no more to do. If there's a new task, the output of the continuation agent becomes the input to the orchestrator, which selects a new speaker. If there's no further task, the loop pauses for more user input.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  And that's the full system! The sub-agents can be arbitrarily complicated, multi-turn systems in themselves, and the outer loop doesn't need to know how they work, just how they depend on each other.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  What's next\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We think there's some novel stuff in here: coordinating multiple agents \"speaking\" simultaneously, creating implicit \"chains\" of agents through natural language instructions, using a \"continuation\" agent to manage those chains, and using a global state this way. We're excited to see what you do with the patterns we've laid out here. Don't forget to\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/multi-agent-concierge/\" rel=\"noreferrer noopener\">\n   check out the open-source repo\n  </a>\n  !\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 22857, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c6e6f5a-fd22-479c-aa6f-883a608eff98": {"__data__": {"id_": "1c6e6f5a-fd22-479c-aa6f-883a608eff98", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.html", "file_name": "building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.html", "file_type": "text/html", "file_size": 30765, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.html", "file_name": "building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.html", "file_type": "text/html", "file_size": 30765, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "2fab045a04f59d835e1e71bd23d0acf61e5ee685d58b66e44154be60b1b84adb", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  In this post we\u2019re going to walk you through the process of building and deploying a Slackbot that listens to your conversations, learns from them, and uses that knowledge to answer questions about what\u2019s going on in your Slack workspace. We\u2019ll also deploy it to production on Render!\n </p>\n <h1>\n  Things you\u2019ll need to start\n </h1>\n <ul>\n  <li>\n   Rudimentary understanding of LlamaIndex. If you haven\u2019t got that, the\n   <a href=\"https://docs.llamaindex.ai/en/stable/getting_started/starter_example.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    starter tutorial\n   </a>\n   in our documentation will give you as much as you need to understand this tutorial and takes only a few minutes.\n  </li>\n  <li>\n   A working knowledge of Python, and Python 3.11 or higher installed\n  </li>\n  <li>\n   A Slack workspace you can install apps to (so you\u2019ll need to be an admin)\n  </li>\n  <li>\n   A clone of\n   <a href=\"https://github.com/run-llama/llamabot\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    our Slackbot repo\n   </a>\n   on your local machine. We\u2019ll be referring to files in this repo throughout the post.\n  </li>\n </ul>\n <h1>\n  Step 1: Create a Slack app, and install it to your workspace\n </h1>\n <p>\n  This is the most complicated step, because Slack is very picky about permissions.\n </p>\n <p>\n  The very first version of your Slackbot is going to be only about 20 lines of code. All it does is provide a \u201cchallenge\u201d endpoint that Slack needs to verify your app is available. You can see this code as the file\n  <code class=\"cw qc qd qe qf b\">\n   1_flask.py\n  </code>\n  in the repo. Let's walk through it.\n </p>\n <p>\n  First we bring in your dependencies. You\u2019ll need to install these with pip or poetry if you don\u2019t have them already.\n </p>\n <pre><span class=\"qo or gt qf b bf qp qq l qr qs\" id=\"62df\"><span class=\"hljs-keyword\">from</span> flask <span class=\"hljs-keyword\">import</span> Flask, request, jsonify</span></pre>\n <p>\n  Now we\u2019ll create your flask app and set it up so it can run in development.\n </p>\n <pre><span class=\"qo or gt qf b bf qp qq l qr qs\" id=\"691b\">flask_app = Flask(__name__)\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n    flask_app.run(port=<span class=\"hljs-number\">3000</span>)</span></pre>\n <p>\n  Between those lines we\u2019ll add our basic route: if a POST request is received that contains a JSON object with a\n  <code class=\"cw qc qd qe qf b\">\n   challenge\n  </code>\n  key, we'll return the value of that key. Otherwise we'll do nothing.\n </p>\n <pre><span class=\"qo or gt qf b bf qp qq l qr qs\" id=\"c25c\"><span class=\"hljs-meta\">@flask_app.route(<span class=\"hljs-params\"><span class=\"hljs-string\">\"/\"</span>, methods=[<span class=\"hljs-string\">\"POST\"</span>]</span>)</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">slack_challenge</span>():\n    <span class=\"hljs-keyword\">if</span> request.json <span class=\"hljs-keyword\">and</span> <span class=\"hljs-string\">\"challenge\"</span> <span class=\"hljs-keyword\">in</span> request.json:\n        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Received challenge\"</span>)\n        <span class=\"hljs-keyword\">return</span> jsonify({<span class=\"hljs-string\">\"challenge\"</span>: request.json[<span class=\"hljs-string\">\"challenge\"</span>]})\n    <span class=\"hljs-keyword\">else</span>:\n        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Got unknown request incoming\"</span>)\n        <span class=\"hljs-built_in\">print</span>(request.json)\n    <span class=\"hljs-keyword\">return</span></span></pre>\n <h1>\n  Make your app available to Slack\n </h1>\n <p>\n  To configure a Slack app, it needs to be running somewhere Slack can see it. So let\u2019s run our Slack app:\n </p>\n <pre><span class=\"qo or gt qf b bf qp qq l qr qs\" id=\"9edb\">python 1_flask.py</span></pre>\n <p>\n  And we\u2019ll set it up so the world can see it using\n  <a href=\"https://ngrok.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   ngrok\n  </a>\n  . You\u2019ll need to download and install ngrok for this step. Once you have it installed, run the following command so it can find our app running on port 3000:\n </p>\n <pre><span class=\"qo or gt qf b bf qp qq l qr qs\" id=\"45a9\">ngrok http 3000</span></pre>\n <p>\n  ngrok will give you an HTTPS url like\n  <code class=\"cw qc qd qe qf b\">\n   https://1bf6-64-38-189-168.ngrok-free.app\n  </code>\n  . Make a note of it, because we need to give that to Slack. Also keep in mind that if you stop ngrok and start it again, this URL will change and you'll need to tell Slack about that. You'll only need this during development.\n </p>\n <h1>\n  Register your app with Slack\n </h1>\n <p>\n  Go to the\n  <a href=\"https://api.slack.com/apps\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Slack API site\n  </a>\n  and click \u201cCreate New App\u201d. You\u2019ll see a screen like this, you\u2019ll want to pick \u201cfrom scratch\u201d:\n </p>\n <p>\n  Pick a nice friendly name and the workspace you want to install it to. You\u2019ll see a screen like this:\n </p>\n <p>\n  Next you\u2019ll want to set up what permissions your app needs. Click the \u201cPermissions\u201d link in the bottom right:\n </p>\n <p>\n  This will bring you to the \u201cscopes\u201d screen where you\u2019ll need to add all the scopes you see in this picture, namely:\n </p>\n <ul>\n  <li>\n   channels:read \u2014 the lets your app see what channels are avaialble\n  </li>\n  <li>\n   channels:join \u2014 this lets your app join channels\n  </li>\n  <li>\n   channels:history \u2014 this lets your app see previous messages in channels\n  </li>\n  <li>\n   chat:write \u2014 this lets your app send messages\n  </li>\n  <li>\n   users:read \u2014 this lets your app see people\u2019s names\n  </li>\n </ul>\n <p>\n  Once you\u2019ve saved those scopes, scroll up to \u201cInstall to workspace\u201d to install your app.\n </p>\n <p>\n  You now need to tell Slack where your app is so you can receive messages from it. Click the \u201cEvent Subscriptions\u201d link in the left nav and fill it out so it looks something like this, specifically:\n </p>\n <ul>\n  <li>\n   Set your Request URL to that URL that ngrok gave you earlier\n  </li>\n  <li>\n   Subscribe to the\n   <code class=\"cw qc qd qe qf b\">\n    message.channels\n   </code>\n   event\n  </li>\n </ul>\n <p>\n  If your app is running and ngrok is correctly tunneling, your Request URL should be Verified.\n </p>\n <p>\n  Phew! That was a lot. Your Slack app is now registered and Slack will send it messages. But to get those messages, you have to tell it to join a channel.\n </p>\n <h1>\n  Step 2: Join a channel, and reply to messages\n </h1>\n <p>\n  To do this we\u2019ll need to extend our app. You can see the final result of this step in\n  <code class=\"cw qc qd qe qf b\">\n   2_join_and_reply.py\n  </code>\n  . Let's walk through what we've added:\n </p>\n <pre><span class=\"qo or gt qf b bf qp qq l qr qs\" id=\"5982\">import dotenv, os\ndotenv.load_dotenv()</span></pre>\n <p>\n  We need some environment variables, so you\u2019ll need to add these lines and install\n  <code class=\"cw qc qd qe qf b\">\n   python-dotenv\n  </code>\n  . You'll also need to create a\n  <code class=\"cw qc qd qe qf b\">\n   .env\n  </code>\n  file in the root of your project with three values:\n </p>\n <ul>\n  <li>\n   <code class=\"cw qc qd qe qf b\">\n    OPENAI_API_KEY\n   </code>\n   : your OpenAI API key. You don't need this quite yet but you may as well\n   <a href=\"https://platform.openai.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    get it now\n   </a>\n   .\n  </li>\n  <li>\n   <code class=\"cw qc qd qe qf b\">\n    SLACK_BOT_TOKEN\n   </code>\n   : you can find this in the \"OAuth and Permissions\" section of your Slack app.\n  </li>\n  <li>\n   <code class=\"cw qc qd qe qf b\">\n    SLACK_SIGNING_SECRET\n   </code>\n   : you can find this in the \"Basic Information\" section of your Slack app.\n  </li>\n </ul>\n <p>\n  We\u2019re going to use Slack\u2019s handy Python SDK to build our app, so pip install\n  <code class=\"cw qc qd qe qf b\">\n   slack-bolt\n  </code>\n  and then update all our imports:\n </p>\n <pre><span class=\"qo or gt qf b bf qp qq l qr qs\" id=\"8a8e\"><span class=\"hljs-keyword\">from</span> slack_bolt <span class=\"hljs-keyword\">import</span> App\n<span class=\"hljs-keyword\">from</span> flask <span class=\"hljs-keyword\">import</span> Flask, request, jsonify\n<span class=\"hljs-keyword\">from</span> slack_bolt.adapter.flask <span class=\"hljs-keyword\">import</span> SlackRequestHandler</span></pre>\n <p>\n  Now initialize a Slack Bolt app using those secrets we set just now:\n </p>\n <pre><span class=\"qo or gt qf b bf qp qq l qr qs\" id=\"27ee\">app = App(\n    token=os.environ.get(<span class=\"hljs-string\">\"SLACK_BOT_TOKEN\"</span>),\n    signing_secret=os.environ.get(<span class=\"hljs-string\">\"SLACK_SIGNING_SECRET\"</span>)\n)\nhandler = SlackRequestHandler(app)</span></pre>\n <p>\n  To listen to messages, the bot has to be in a channel. You can get it to join any and all public channels, but for the purposes of testing I\u2019ve created a channel called\n  <code class=\"cw qc qd qe qf b\">\n   #bot-testing\n  </code>\n  and that's the one it's joining here:\n </p>\n <pre><span class=\"qo or gt qf b bf qp qq l qr qs\" id=\"be3e\">channel_list = app.client.conversations_list().<span class=\"hljs-type\">data</span>\n<span class=\"hljs-variable\">channel</span> <span class=\"hljs-operator\">=</span> next((channel <span class=\"hljs-keyword\">for</span> channel in channel_list.get(<span class=\"hljs-string\">'channels'</span>) <span class=\"hljs-keyword\">if</span> channel.get(<span class=\"hljs-string\">\"name\"</span>) == <span class=\"hljs-string\">\"bot-testing\"</span>), None)\nchannel_id = channel.get(<span class=\"hljs-string\">'id'</span>)\napp.client.conversations_join(channel=channel_id)</span></pre>\n <p>\n  <code class=\"cw qc qd qe qf b\">\n   app.client\n  </code>\n  is the Bolt framework's Slack WebClient, so you can do anything a WebClient can do directly from within the framework. The final addition here is a very simple message listener:\n </p>\n <pre><span class=\"qo or gt qf b bf qp qq l qr qs\" id=\"c21c\"><span class=\"hljs-meta\">@app.message()</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">reply</span>(<span class=\"hljs-params\">message, say</span>):\n    <span class=\"hljs-built_in\">print</span>(message)\n    say(<span class=\"hljs-string\">\"Yes?\"</span>)</span></pre>\n <p>\n  In the Bolt framework, the\n  <code class=\"cw qc qd qe qf b\">\n   @app.message\n  </code>\n  decorator tells the framework to trigger this method when it receives a message event. The\n  <code class=\"cw qc qd qe qf b\">\n   say\n  </code>\n  parameter is a function that will send a message back to the channel the message came from. So this code will send a message back to the channel saying \"Yes?\" every time it receives a message.\n </p>\n <p>\n  Let\u2019s try it out! Stop running\n  <code class=\"cw qc qd qe qf b\">\n   1_flask.py\n  </code>\n  and run\n  <code class=\"cw qc qd qe qf b\">\n   python 2_join_and_reply.py\n  </code>\n  instead. You don't need to restart\n  <code class=\"cw qc qd qe qf b\">\n   ngrok\n  </code>\n  , it will continue to send messages to port 3000 as before. Here's me trying it out:\n </p>\n <p>\n  Success! We have a very annoying bot that replies to every single thing anybody says. We can do better!\n </p>\n <h1>\n  Step 3: reply only to messages that mention the bot\n </h1>\n <p>\n  This is a pretty simple change on the surface, but Slack\u2019s incoming message format is a little complicated so we have to add a fair bit of code. You can see the final results in\n  <code class=\"cw qc qd qe qf b\">\n   3_reply_to_mentions.py\n  </code>\n  .\n </p>\n <p>\n  First, to tell when our bot is being mentioned, we need our bot\u2019s User ID. Under the hood, Slack doesn\u2019t use user names or even @-handles, but a globally unique ID across all Slack installations. We have to get that:\n </p>\n <pre><span class=\"qo or gt qf b bf qp qq l qr qs\" id=\"337f\">auth_response = app.client.auth_test()\nbot_user_id = auth_response[\"user_id\"]</span></pre>\n <p>\n  Now we add an annoyingly complicated chunk of code that parses through Slack\u2019s message object to see what user is mentioned in an incoming message. If it\u2019s the bot, the bot replies, otherwise it just ignores the message. As we go further, we\u2019ll treat messages to the bot as \u201cqueries\u201d and any other message as a \u201cfact\u201d for it to store, but we won\u2019t be storing it just yet.\n </p>\n <pre><span class=\"qo or gt qf b bf qp qq l qr qs\" id=\"18cf\"><span class=\"hljs-meta\">@app.message()</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">reply</span>(<span class=\"hljs-params\">message, say</span>):\n    <span class=\"hljs-keyword\">if</span> message.get(<span class=\"hljs-string\">'blocks'</span>):\n        <span class=\"hljs-keyword\">for</span> block <span class=\"hljs-keyword\">in</span> message.get(<span class=\"hljs-string\">'blocks'</span>):\n            <span class=\"hljs-keyword\">if</span> block.get(<span class=\"hljs-string\">'type'</span>) == <span class=\"hljs-string\">'rich_text'</span>:\n                <span class=\"hljs-keyword\">for</span> rich_text_section <span class=\"hljs-keyword\">in</span> block.get(<span class=\"hljs-string\">'elements'</span>):\n                    <span class=\"hljs-keyword\">for</span> element <span class=\"hljs-keyword\">in</span> rich_text_section.get(<span class=\"hljs-string\">'elements'</span>):\n                        <span class=\"hljs-keyword\">if</span> element.get(<span class=\"hljs-string\">'type'</span>) == <span class=\"hljs-string\">'user'</span> <span class=\"hljs-keyword\">and</span> element.get(<span class=\"hljs-string\">'user_id'</span>) == bot_user_id:\n                            <span class=\"hljs-keyword\">for</span> element <span class=\"hljs-keyword\">in</span> rich_text_section.get(<span class=\"hljs-string\">'elements'</span>):\n                                <span class=\"hljs-keyword\">if</span> element.get(<span class=\"hljs-string\">'type'</span>) == <span class=\"hljs-string\">'text'</span>:\n                                    query = element.get(<span class=\"hljs-string\">'text'</span>)\n                                    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Somebody asked the bot: <span class=\"hljs-subst\">{query}</span>\"</span>)\n                                    say(<span class=\"hljs-string\">\"Yes?\"</span>)\n                                    <span class=\"hljs-keyword\">return</span>\n    <span class=\"hljs-comment\"># otherwise do something else with it</span>\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Saw a fact: \"</span>, message.get(<span class=\"hljs-string\">'text'</span>))</span></pre>\n <p>\n  Oof. That took a while to get right! But now our bot only replies when it\u2019s mentioned:\n </p>\n <h1>\n  Step 4: use LlamaIndex to store facts and answer questions\n </h1>\n <p>\n  We\u2019re all the way at step 4 and we still haven\u2019t done anything with LlamaIndex! But now\u2019s the time. In\n  <code class=\"cw qc qd qe qf b\">\n   4_incremental_rag.py\n  </code>\n  you'll see a demonstration of a simple command-line Python script that uses LlamaIndex to store facts and answer questions. I won't walk you through every line (the script has helpful comments for that), but let's look at the important ones. Remember to\n  <code class=\"cw qc qd qe qf b\">\n   pip install llama-index\n  </code>\n  !\n </p>\n <p>\n  First we create a new\n  <code class=\"cw qc qd qe qf b\">\n   VectorStoreIndex\n  </code>\n  , an in-memory\n  <a href=\"https://docs.llamaindex.ai/en/stable/understanding/indexing/indexing.html#vector-store-index\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   vector store\n  </a>\n  where we'll be storing our facts. It's empty to start with.\n </p>\n <pre><span class=\"qo or gt qf b bf qp qq l qr qs\" id=\"1358\">index = VectorStoreIndex([])</span></pre>\n <p>\n  Next we create 3\n  <code class=\"cw qc qd qe qf b\">\n   Document\n  </code>\n  objects and insert them each into our index. Real documents can be huge blocks of text, whole PDFs, even images, but these are just some simple, Slack-message-sized facts.\n </p>\n <pre><span class=\"qo or gt qf b bf qp qq l qr qs\" id=\"4923\">doc1 = Document(text=\"Molly is a cat\")\ndoc2 = Document(text=\"Doug is a dog\")\ndoc3 = Document(text=\"Carl is a rat\")\n\nindex.insert(doc1)\nindex.insert(doc2)\nindex.insert(doc3)</span></pre>\n <p>\n  And finally we create a\n  <a href=\"https://docs.llamaindex.ai/en/stable/understanding/querying/querying.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   query engine\n  </a>\n  from our index and ask it a question:\n </p>\n <pre><span class=\"qo or gt qf b bf qp qq l qr qs\" id=\"c918\"><span class=\"hljs-comment\"># run a query</span>\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(<span class=\"hljs-string\">\"Who is Molly?\"</span>)\n<span class=\"hljs-built_in\">print</span>(response)</span></pre>\n <p>\n  The result is \u201cMolly is a cat\u201d plus a whole lot of debugging info because we turned on noisy debugging in\n  <code class=\"cw qc qd qe qf b\">\n   4_incremental_rag.py\n  </code>\n  . You can see the prompt we sent to the LLM, the context it retrieved from the index, and the response it generated and sent back to us.\n </p>\n <h1>\n  Step 5: use LlamaIndex to store facts and answer questions in Slack\n </h1>\n <p>\n  In\n  <code class=\"cw qc qd qe qf b\">\n   5_rag_in_slack.py\n  </code>\n  we are combining the two things we had before: script 3, where we reply to queries, and script 4, where we store facts and answer questions. Once again we won't walk through every line, but here are the important changes:\n </p>\n <p>\n  First\n  <code class=\"cw qc qd qe qf b\">\n   pip install llama-index\n  </code>\n  if you didn't already, and bring in your deps. Initialize your index while you're at it:\n </p>\n <pre><span class=\"qo or gt qf b bf qp qq l qr qs\" id=\"05ed\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> VectorStoreIndex, Document\n\nindex = VectorStoreIndex([])</span></pre>\n <p>\n  Where previously we were just replying with \u201cYes?\u201d (line 73) let\u2019s instead send a query to the query engine and reply with the response:\n </p>\n <pre><span class=\"qo or gt qf b bf qp qq l qr qs\" id=\"d686\">query = element.get(<span class=\"hljs-string\">'text'</span>)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(query)\nsay(str(response))</span></pre>\n <p>\n  And where previously we were just noting that we\u2019d seen a fact (line 82), let\u2019s store it in the index:\n </p>\n <pre><span class=\"qo or gt qf b bf qp qq l qr qs\" id=\"fe76\">index.insert(Document(text=message.get(<span class=\"hljs-string\">'text'</span>)))</span></pre>\n <p>\n  The result is a Slackbot that can answer questions about what it\u2019s been told:\n </p>\n <p>\n  Amazing! You can easily imagine a bot that listens to everybody\u2019s conversations and is able to answer questions about things people said weeks or months ago, saving everybody time and effort searching through old messages.\n </p>\n <h1>\n  Step 6: persist our memory\n </h1>\n <p>\n  Our bot has a critical flaw though: the index is stored only in memory. If we restart the bot, it forgets everything:\n </p>\n <p>\n  In\n  <code class=\"cw qc qd qe qf b\">\n   6_qdrant.py\n  </code>\n  we bring in\n  <a href=\"https://qdrant.tech/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Qdrant\n  </a>\n  , an open-source, local vector database that stores these facts on disk instead. That way if we restart our bot it remembers what was said before.\n  <code class=\"cw qc qd qe qf b\">\n   pip install qdrant-client\n  </code>\n  and bring in some new deps:\n </p>\n <pre><span class=\"qo or gt qf b bf qp qq l qr qs\" id=\"1f81\"><span class=\"hljs-keyword\">import</span> qdrant_client\n<span class=\"hljs-keyword\">from</span> llama_index.vector_stores.qdrant <span class=\"hljs-keyword\">import</span> QdrantVectorStore</span></pre>\n <p>\n  Now we\u2019ll initialize the Qdrant client, attach it to a storage context, and give that storage context to our index when we initialize it:\n </p>\n <pre><span class=\"qo or gt qf b bf qp qq l qr qs\" id=\"b019\">client = qdrant_client.QdrantClient(\n    path=\"./qdrant_data\"\n)\nvector_store = QdrantVectorStore(client=client, collection_name=\"slack_messages\")\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\nindex = VectorStoreIndex([],storage_context=storage_context)</span></pre>\n <p>\n  That\u2019s it for this step! Your bot now survives reboots, and remembers that I typoed \u201cDoug\u201d as \u201cDough\u201d and was too lazy to fix it for the screenshot:\n </p>\n <h1>\n  Step 7: make recent messages more important\n </h1>\n <p>\n  We now have a pretty capable bot! But it has a subtle problem: people can say conflicting things, and it doesn\u2019t have a way to decide who was \u201cright\u201d, such as when I change my mind about what the dog\u2019s name should be:\n </p>\n <p>\n  In real Slack conversations, as a situation evolves people might move from saying a project is \u201cin planning\u201d to \u201cunderway\u201d to \u201claunched\u201d. So we need a way to tell the bot that more recent messages are more important than older ones.\n </p>\n <p>\n  To make this happen we have to do quite a bit of refactoring, the final results of which you can see in\n  <code class=\"cw qc qd qe qf b\">\n   7_recency.py\n  </code>\n  . First we need a bunch of new deps:\n </p>\n <pre><span class=\"qo or gt qf b bf qp qq l qr qs\" id=\"f751\"><span class=\"hljs-keyword\">import</span> datetime, uuid\n<span class=\"hljs-keyword\">from</span> llama_index.schema <span class=\"hljs-keyword\">import</span> TextNode\n<span class=\"hljs-keyword\">from</span> llama_index.prompts <span class=\"hljs-keyword\">import</span> PromptTemplate\n<span class=\"hljs-keyword\">from</span> llama_index.postprocessor <span class=\"hljs-keyword\">import</span> FixedRecencyPostprocessor\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> set_global_handler</span></pre>\n <p>\n  To make recent messages more important, we have to know when a message was sent. To do that we are going to stop inserting\n  <code class=\"cw qc qd qe qf b\">\n   Documents\n  </code>\n  into the index and instead insert\n  <code class=\"cw qc qd qe qf b\">\n   Nodes\n  </code>\n  , to which we're going to attach the timestamp as metadata (under the hood, our Documents were always being converted into Nodes anyway so this doesn't change much):\n </p>\n <pre><span class=\"qo or gt qf b bf qp qq l qr qs\" id=\"856c\">dt_object = datetime.datetime.fromtimestamp(<span class=\"hljs-type\">float</span>(message.get(<span class=\"hljs-string\">'ts'</span>)))\nformatted_time = dt_object.strftime(<span class=\"hljs-string\">'%Y-%m-%d %H:%M:%S'</span>)\n\n# get the message <span class=\"hljs-type\">text</span>\n<span class=\"hljs-variable\">text</span> <span class=\"hljs-operator\">=</span> message.get(<span class=\"hljs-string\">'text'</span>)\n# create a node with <span class=\"hljs-type\">metadata</span>\n<span class=\"hljs-variable\">node</span> <span class=\"hljs-operator\">=</span> TextNode(\n    text=text,\n    id_=str(uuid.uuid4()),\n    metadata={\n        <span class=\"hljs-string\">\"when\"</span>: formatted_time\n    }\n)\nindex.insert_nodes([node])</span></pre>\n <p>\n  I\u2019ve also factored out the reply logic from message handling into its own function,\n  <code class=\"cw qc qd qe qf b\">\n   answer_question\n  </code>\n  , just to make things a little easier to read. The first thing we're going to change is the prompt that we give to our LLM: we have to tell it that more recent messages are important. To do this we create a prompt template:\n </p>\n <pre><span class=\"qo or gt qf b bf qp qq l qr qs\" id=\"28ed\">template = (\n    <span class=\"hljs-string\">\"Your context is a series of chat messages. Each one is tagged with 'who:' \\n\"</span>\n    <span class=\"hljs-string\">\"indicating who was speaking and 'when:' indicating when they said it, \\n\"</span>\n    <span class=\"hljs-string\">\"followed by a line break and then what they said. There can be up to 20 chat messages.\\n\"</span>\n    <span class=\"hljs-string\">\"The messages are sorted by recency, so the most recent one is first in the list.\\n\"</span>\n    <span class=\"hljs-string\">\"The most recent messages should take precedence over older ones.\\n\"</span>\n    <span class=\"hljs-string\">\"---------------------\\n\"</span>\n    <span class=\"hljs-string\">\"{context_str}\"</span>\n    <span class=\"hljs-string\">\"\\n---------------------\\n\"</span>\n    <span class=\"hljs-string\">\"You are a helpful AI assistant who has been listening to everything everyone has been saying. \\n\"</span>\n    <span class=\"hljs-string\">\"Given the most relevant chat messages above, please answer this question: {query_str}\\n\"</span>\n)\nqa_template = <span class=\"hljs-title class_\">PromptTemplate</span>(template)</span></pre>\n <p>\n  The fun thing about working with LLMs is how often you end up just describing what you\u2019re doing in English and that being what you send to the LLM. A prompt template will automatically get the\n  <code class=\"cw qc qd qe qf b\">\n   context_str\n  </code>\n  and\n  <code class=\"cw qc qd qe qf b\">\n   query_str\n  </code>\n  from the query engine. But we have to set this template on our query engine, like so:\n </p>\n <pre><span class=\"qo or gt qf b bf qp qq l qr qs\" id=\"81ea\">query_engine.update_prompts(\n    {\"response_synthesizer:text_qa_template\": qa_template}\n)</span></pre>\n <p>\n  Now there\u2019s two more things we\u2019re going to change. We\u2019re going to take the results we get from the vector store and sort them by recency, something LlamaIndex has a built-in class for. It\u2019s called the\n  <code class=\"cw qc qd qe qf b\">\n   FixedRecencyPostprocessor\n  </code>\n  . We tell it the key that holds the timestamp (which we defined earlier on the nodes, above) and how many results it should return:\n </p>\n <pre><span class=\"qo or gt qf b bf qp qq l qr qs\" id=\"4492\">postprocessor = FixedRecencyPostprocessor(\n    top_k=20, \n    date_key=\"when\", # the key in the metadata to find the date\n    service_context=ServiceContext.from_defaults()\n)</span></pre>\n <p>\n  Then we need to create our query engine with the postprocessor attached:\n </p>\n <pre><span class=\"qo or gt qf b bf qp qq l qr qs\" id=\"6e1f\">query_engine = index.as_query_engine(similarity_top_k=20, node_postprocessors=[postprocessor])</span></pre>\n <p>\n  While we were at it we did our final thing, which was pass\n  <code class=\"cw qc qd qe qf b\">\n   similarity_top_k=20\n  </code>\n  , which means the vector store will give us 20 Slack messages as context (the default is just 2, because usually the chunks of text in a Node are a lot bigger).\n </p>\n <p>\n  Tada! Now the bot knows to take more recent statements as the truth.\n </p>\n <h1>\n  Step 8: draw the rest of the owl\n </h1>\n <p>\n  This bot is working pretty well now, but I was having such fun when building it I got carried away and added two more features:\n </p>\n <ul>\n  <li>\n   I attached metadata about\n   <em class=\"re\">\n    who\n   </em>\n   was speaking, not just when, so the bot can answer questions like \u201cWhat did Logan say about the project?\u201d\n  </li>\n  <li>\n   My colleagues interacting with the bot tried to ask follow-up questions in a thread, like we do with each other. So I added a way for the bot to understand that it\u2019s in a thread, and treat replies in a thread as follow-up questions, even if the user doesn\u2019t mention the bot directly:\n  </li>\n </ul>\n <p>\n  The code to make both of those happen is in\n  <code class=\"cw qc qd qe qf b\">\n   8_rest_of_the_owl.py\n  </code>\n  but I'm not going to be stepping through it line by line. We have to deploy this thing!\n </p>\n <h1>\n  Step 9: deploy to Render\n </h1>\n <p>\n  Until now we\u2019ve been working with local scripts running through the ngrok tunnel, but even the most dedicated coder turns their laptop off sometimes. Let\u2019s put this thing on a real server.\n </p>\n <h1>\n  Login to Render\n </h1>\n <p>\n  We\u2019ll be deploying to\n  <a href=\"https://render.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Render\n  </a>\n  , a Python-friendly hosting service that\u2019s free for small projects. Sign up for an account (I recommend logging in with GitHub).\n </p>\n <h1>\n  Create a new GitHub repository\n </h1>\n <p>\n  Render deploys things from GitHub repositories, so you\u2019ll need to create a new one and copy 2 files from our existing repo into it:\n </p>\n <ul>\n  <li>\n   <code class=\"cw qc qd qe qf b\">\n    pyproject.toml\n   </code>\n  </li>\n  <li>\n   <code class=\"cw qc qd qe qf b\">\n    8_rest_of_the_owl.py\n   </code>\n   which we're going to rename to \"app.py\" for simplicity.\n  </li>\n </ul>\n <p>\n  Commit those and push them up to GitHub.\n </p>\n <h1>\n  Create a new Render web service\n </h1>\n <p>\n  In Render, create a new web service. Connect it to the repo on GitHub you just created:\n </p>\n <p>\n  Render will probably automatically detect that this is a Python app but you should make sure the following settings are correct:\n </p>\n <ul>\n  <li>\n   Name: any name you choose\n  </li>\n  <li>\n   Region: any region is fine\n  </li>\n  <li>\n   Branch: main\n  </li>\n  <li>\n   Root directory: (blank, meaning root)\n  </li>\n  <li>\n   Runtime: Python 3\n  </li>\n  <li>\n   Build command:\n   <code class=\"cw qc qd qe qf b\">\n    poetry install\n   </code>\n  </li>\n  <li>\n   Start command:\n   <code class=\"cw qc qd qe qf b\">\n    gunicorn app:flask_app\n   </code>\n   (this will definitely need to be set)\n  </li>\n </ul>\n <p>\n  You\u2019ll also need to scroll down and set some environment variables:\n </p>\n <ul>\n  <li>\n   PYTHON_VERSION: 3.11.6 (or whatever version you\u2019re using)\n  </li>\n  <li>\n   OPENAI_API_KEY: your OpenAI API key\n  </li>\n  <li>\n   SLACK_BOT_TOKEN: your Slack bot token\n  </li>\n  <li>\n   SLACK_SIGNING_SECRET: your Slack signing secret from before\n  </li>\n </ul>\n <p>\n  Then click deploy and away you go!\n </p>\n <p>\n  You now have a production Slack bot listening to messages, remembering, learning, and replying. Congratulations!\n </p>\n <h1>\n  What next?\n </h1>\n <p>\n  There\u2019s a whole bunch of features you could add to this bot, roughly in increasing order of difficulty:\n </p>\n <ul>\n  <li>\n   Join every channel instead of just one, clearly!\n  </li>\n  <li>\n   Add a way to tell the bot to forget things (delete nodes)\n  </li>\n  <li>\n   Give the bot the ability to use more than one index, such as an index of your documentation, or connected to your email, or your calendar\n  </li>\n  <li>\n   Give the bot \u201ctags\u201d so it can attach metadata to nodes and answer questions only with (or ignore) things that have been tagged a certain way\n  </li>\n  <li>\n   Add multi-modal abilities, so the bot can read images and even reply with generated images\n  </li>\n  <li>\n   And tons more!\n  </li>\n </ul>\n <p>\n  This bot is a lot of fun to play with and was a lot of fun to build, I hope you enjoyed learning about Slackbots and LlamaIndex as much as I enjoyed writing this tutorial!\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 30552, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "53114f4b-875f-4c80-a26b-dade1c3fb97b": {"__data__": {"id_": "53114f4b-875f-4c80-a26b-dade1c3fb97b", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.html", "file_name": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.html", "file_type": "text/html", "file_size": 14878, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.html", "file_name": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.html", "file_type": "text/html", "file_size": 14878, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "c329a00f6c3a8e16d49a10c44b5fb00b7a63138eef832fc126ac172f5055e8a1", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Over the past year, Large Language Models (LLMs) like GPT-4 have not only transformed how we interact with machines but also have redefined the possibilities within the realm of natural language processing (NLP). A notable trend in this evolution is the increasing popularity of open-source LLMs like Llama 2, Falcon, OPT and Yi. Some may prefer them over their commercial counterparts in terms of accessibility, data security and privacy, customization potential, cost, and vendor dependency. Among the tools gaining increasing traction in the LLM space are OpenLLM and LlamaIndex \u2014 two powerful platforms that, when combined, unlock new use cases for building AI-driven applications.\n </p>\n <p>\n  <a href=\"https://github.com/bentoml/OpenLLM\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   OpenLLM\n  </a>\n  is an open-source platform for deploying and operating any open-source LLMs in production. Its flexibility and ease of use make it an ideal choice for AI application developers seeking to harness the power of LLMs. You can easily fine-tune, serve, deploy, and monitor LLMs in a wide range of creative and practical applications.\n </p>\n <p>\n  <a href=\"https://github.com/run-llama/llama_index\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaIndex\n  </a>\n  provides a comprehensive framework for managing and retrieving private and domain-specific data. It acts as a bridge between the extensive knowledge of LLMs and the unique, contextual data needs of specific applications.\n </p>\n <p>\n  OpenLLM\u2019s support for a diverse range of open-source LLMs and LlamaIndex\u2019s ability to seamlessly integrate custom data sources provide great customization for developers in both communities. This combination allows them to create AI solutions that are both highly intelligent and properly tailored to specific data contexts, which is very important for query-response systems.\n </p>\n <p>\n  In this blog post, I will explain how you can leverage the combined strengths of OpenLLM and LlamaIndex to build an intelligent query-response system. This system can understand, process, and respond to queries by tapping into a custom corpus.\n </p>\n <h1>\n  Setting up the environment\n </h1>\n <p>\n  The first step is to create a virtual environment in your machine, which helps prevent conflicts with other Python projects you might be working on. Let\u2019s just call it\n  <code class=\"cw pp pq pr ps b\">\n   llamaindex-openllm\n  </code>\n  and activate it.\n </p>\n <pre><span class=\"qb on gt ps b bf qc qd l qe qf\" id=\"4dca\">python -m venv llamaindex-openllm\nsource llamaindex-openllm/bin/activate</span></pre>\n <p>\n  Install the required packages. This command installs OpenLLM with the optional\n  <code class=\"cw pp pq pr ps b\">\n   vllm\n  </code>\n  component (I will explain it later).\n </p>\n <pre><span class=\"qb on gt ps b bf qc qd l qe qf\" id=\"f553\">pip install \"openllm[vllm]\" llama-index llama-index-llms-openllm llama-index-embeddings-huggingface</span></pre>\n <p>\n  For handling requests, you need to have an LLM server. Here, I use the following command to start a Llama 2 7B local server at\n  <a href=\"http://localhost:3000\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   http://localhost:3000\n  </a>\n  . Feel free to choose any model that fits your needs. If you already have a remote LLM server, you can skip this step.\n </p>\n <pre><span class=\"qb on gt ps b bf qc qd l qe qf\" id=\"1add\">openllm start meta-llama/Llama-2-7b-chat-hf --backend vllm</span></pre>\n <p>\n  OpenLLM automatically selects the most suitable runtime implementation for the model. For models with vLLM support, OpenLLM uses vLLM by default. Otherwise, it falls back to PyTorch. vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. According to\n  <a href=\"https://www.anyscale.com/blog/continuous-batching-llm-inference\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   this report\n  </a>\n  , you can achieve 23x LLM inference throughput while reducing P50 latency using vLLM.\n </p>\n <blockquote>\n  <p class=\"nn no qj np b nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok gm bj\" id=\"b16c\">\n   <strong>\n    <em class=\"gt\">\n     Note\n    </em>\n   </strong>\n   <em class=\"gt\">\n    : To use the vLLM backend, you need a GPU with at least the Ampere architecture (or newer) and CUDA version 11.8. This demo uses a machine with an Ampere A100\u201380G GPU. If your machine has a compatible GPU, you can also choose vLLM. Otherwise, simply install the standard OpenLLM package (\n   </em>\n   <code class=\"cw pp pq pr ps b\">\n    <em class=\"gt\">\n     pip install openllm\n    </em>\n   </code>\n   <em class=\"gt\">\n    ) in the previous command.\n   </em>\n  </p>\n </blockquote>\n <h1>\n  v1: Creating a simple completion service\n </h1>\n <p>\n  Before building a query-response system, let\u2019s get familiar with the integration of OpenLLM and LlamaIndex and use it to create a simple completion service.\n </p>\n <p>\n  The integration offers two APIs for interactions with LLMs:\n </p>\n <p>\n  1.\n  <code class=\"cw pp pq pr ps b\">\n   OpenLLM\n  </code>\n  : This can be used to initiate a local LLM server directly without the need for starting a separate one using commands like\n  <code class=\"cw pp pq pr ps b\">\n   openllm start\n  </code>\n  . Here\u2019s how you can use it:\n </p>\n <pre><span class=\"qb on gt ps b bf qc qd l qe qf\" id=\"2629\"><span class=\"hljs-keyword\">from</span> llama_index.llms.openllm <span class=\"hljs-keyword\">import</span> OpenLLM\nllm = OpenLLM(<span class=\"hljs-string\">'meta-llama/Llama-2-7b-chat-hf'</span>)</span></pre>\n <p>\n  2.\n  <code class=\"cw pp pq pr ps b\">\n   OpenLLMAPI\n  </code>\n  : This can be used to interact with a server hosted elsewhere, like the Llama 2 7B model I started previously.\n </p>\n <p>\n  Let\u2019s try the\n  <code class=\"cw pp pq pr ps b\">\n   complete\n  </code>\n  endpoint and see if the Llama 2 7B model is able to tell what OpenLLM is by completing the sentence \u201cOpenLLM is an open source tool for\u201d.\n </p>\n <pre><span class=\"qb on gt ps b bf qc qd l qe qf\" id=\"e018\"><span class=\"hljs-keyword\">from</span> llama_index.llms.openllm <span class=\"hljs-keyword\">import</span> OpenLLMAPI\n\nremote_llm = OpenLLMAPI(address=<span class=\"hljs-string\">\"http://localhost:3000\"</span>)\n\ncompletion_response = remote_llm.complete(<span class=\"hljs-string\">\"OpenLLM is an open source tool for\"</span>, max_new_tokens=<span class=\"hljs-number\">1024</span>)\n<span class=\"hljs-built_in\">print</span>(completion_response)</span></pre>\n <p>\n  Run this script and here is the output:\n </p>\n <pre><span class=\"qb on gt ps b bf qc qd l qe qf\" id=\"5000\">learning lifelong learning models. It is designed to be easy to use, even <span class=\"hljs-keyword\">for</span> those without extensive knowledge of machine learning. OpenLLM allows users to train, evaluate, and deploy lifelong learning models using a variety of datasets and algorithms.\n\nOpenLLM provides a number of features that make it useful <span class=\"hljs-keyword\">for</span> learning lifelong learning models. Some of these features include:\n\n<span class=\"hljs-number\">1.</span> Easy-to-use interface: OpenLLM provides an easy-to-use interface that makes it simple to train, evaluate, and deploy lifelong learning models.\n<span class=\"hljs-number\">2.</span> Support <span class=\"hljs-keyword\">for</span> a variety of datasets: OpenLLM supports a variety of datasets, including images, text, and time-series data.\n<span class=\"hljs-number\">3.</span> Support <span class=\"hljs-keyword\">for</span> a variety of algorithms: OpenLLM supports a variety of algorithms <span class=\"hljs-keyword\">for</span> lifelong learning, including neural networks, decision trees, and support <span class=\"hljs-built_in\">vector</span> machines.\n<span class=\"hljs-number\">4.</span> Evaluation tools: OpenLLM provides a number of evaluation tools that allow users to assess the performance of their lifelong learning models.\n<span class=\"hljs-number\">5.</span> Deployment tools: OpenLLM provides a number of deployment tools that allow users to deploy their lifelong learning models in a variety of environments.\n\nOpenLLM is written in Python and is available under an open source license. It is designed to be used in a variety of settings, including research, education, and industry.\n\nSome potential use cases <span class=\"hljs-keyword\">for</span> OpenLLM include:\n\n<span class=\"hljs-number\">1.</span> Training lifelong learning models <span class=\"hljs-keyword\">for</span> image classification: OpenLLM could be used to train a lifelong learning model to classify images based on their content.\n<span class=\"hljs-number\">2.</span> Training lifelong learning models <span class=\"hljs-keyword\">for</span> natural language processing: OpenLLM could be used to train a lifelong learning model to process and analyze natural language text.\n<span class=\"hljs-number\">3.</span> Training lifelong learning models <span class=\"hljs-keyword\">for</span> time-series data: OpenLLM could be used to train a lifelong learning model to predict <span class=\"hljs-built_in\">future</span> values in a time-series dataset.\n<span class=\"hljs-number\">4.</span> Deploying lifelong learning models in a production environment: OpenLLM could be used to deploy a lifelong learning model in a production environment, such as a recommendation system or a fraud detection system.\n\nOverall, OpenLLM is a powerful tool <span class=\"hljs-keyword\">for</span> learning lifelong learning models. Its ease of use, flexibility, and support <span class=\"hljs-keyword\">for</span> a variety of datasets and algorithms make it a valuable resource <span class=\"hljs-keyword\">for</span> researchers and practitioners in a variety of fields.</span></pre>\n <p>\n  Obviously, the model couldn\u2019t correctly explain OpenLLM with some hallucinations \ud83e\udd23. Nevertheless, the code works well as the server outputs a response for the request. This is a good start as we proceed with building our system.\n </p>\n <h1>\n  v2: Enhancing with a query-response system\n </h1>\n <p>\n  The initial version revealed a key limitation: the model\u2019s lack of specific knowledge about OpenLLM. One solution is to feed the model with domain-specific information, allowing it to learn and respond according to topic-specific queries. This is where LlamaIndex comes into play, enabling you to build a local knowledge base with pertinent information. Specifically, you create a directory (for example,\n  <code class=\"cw pp pq pr ps b\">\n   data\n  </code>\n  ) and build an index for all the documents in the folder.\n </p>\n <p>\n  Create a folder and let\u2019s import the GitHub README file of OpenLLM into the folder:\n </p>\n <pre><span class=\"qb on gt ps b bf qc qd l qe qf\" id=\"530d\">mkdir data\ncd data\nwget https://github.com/bentoml/OpenLLM/blob/main/README.md</span></pre>\n <p>\n  Go back to the previous directory and create a script called\n  <code class=\"cw pp pq pr ps b\">\n   starter.py\n  </code>\n  like the following:\n </p>\n <pre><span class=\"qb on gt ps b bf qc qd l qe qf\" id=\"f7eb\"><span class=\"hljs-keyword\">from</span> llama_index.core <span class=\"hljs-keyword\">import</span> VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n<span class=\"hljs-keyword\">from</span> llama_index.llms.openllm <span class=\"hljs-keyword\">import</span> OpenLLMAPI\n<span class=\"hljs-keyword\">from</span> llama_index.core.node_parser <span class=\"hljs-keyword\">import</span> SentenceSplitter\n\n<span class=\"hljs-comment\"># Change the address to your OpenLLM server</span>\nllm = OpenLLMAPI(address=<span class=\"hljs-string\">\"http://localhost:3000\"</span>)\n\n<span class=\"hljs-comment\"># Break down the document into manageable chunks (each of size 1024 characters, with a 20-character overlap)</span>\ntext_splitter = SentenceSplitter(chunk_size=<span class=\"hljs-number\">1024</span>, chunk_overlap=<span class=\"hljs-number\">20</span>)\n\n<span class=\"hljs-comment\"># Create a ServiceContext with the custom model and all the configurations</span>\nservice_context = ServiceContext.from_defaults(\n    llm=llm,\n    embed_model=<span class=\"hljs-string\">\"local\"</span>,\n    text_splitter=text_splitter,\n    context_window=<span class=\"hljs-number\">8192</span>,\n    num_output=<span class=\"hljs-number\">4096</span>,\n)\n\n<span class=\"hljs-comment\"># Load documents from the data directory</span>\ndocuments = SimpleDirectoryReader(<span class=\"hljs-string\">\"data\"</span>).load_data()\n\n<span class=\"hljs-comment\"># Build an index over the documents using the customized LLM in the ServiceContext</span>\nindex = VectorStoreIndex.from_documents(documents, service_context=service_context)\n\n<span class=\"hljs-comment\"># Query your data using the built index</span>\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(<span class=\"hljs-string\">\"What is OpenLLM?\"</span>)\n<span class=\"hljs-built_in\">print</span>(response)</span></pre>\n <p>\n  To improve the quality of your response, I recommend you define a\n  <code class=\"cw pp pq pr ps b\">\n   SentenceSplitter\n  </code>\n  to provide finer control over the input processing, leading to better output quality.\n </p>\n <p>\n  In addition, you can set\n  <code class=\"cw pp pq pr ps b\">\n   streaming=True\n  </code>\n  to stream your response:\n </p>\n <pre><span class=\"qb on gt ps b bf qc qd l qe qf\" id=\"2534\">query_engine = index.as_query_engine(streaming=True)\nresponse = query_engine.query(\"What is OpenLLM?\")\nresponse.print_response_stream()</span></pre>\n <p>\n  Your directory structure should look like this now:\n </p>\n <pre><span class=\"qb on gt ps b bf qc qd l qe qf\" id=\"3ec6\">\u251c\u2500\u2500 starter.py\n\u2514\u2500\u2500 data\n    \u2514\u2500\u2500 README.md</span></pre>\n <p>\n  Run\n  <code class=\"cw pp pq pr ps b\">\n   starter.py\n  </code>\n  to test the query-response system. The output should be consistent with the content of the OpenLLM README. Here is the response I received:\n </p>\n <blockquote>\n  <p class=\"nn no qj np b nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok gm bj\" id=\"2074\">\n   OpenLLM is an open-source platform for deploying and managing large language models (LLMs) in a variety of environments, including on-premises, cloud, and edge devices. It provides a comprehensive suite of tools and features for fine-tuning, serving, deploying, and monitoring LLMs, simplifying the end-to-end deployment workflow for LLMs.\n  </p>\n </blockquote>\n <h1>\n  Conclusion\n </h1>\n <p>\n  The exploration in this article underscores the importance of customizing AI tools to fit specific needs. By using OpenLLM for flexible deployment of LLMs and LlamaIndex for data management, I have demonstrated how to create an AI-powered system. It not only understands and processes queries but also delivers responses based on a unique knowledge base. I hope this blog post has inspired you to explore more capabilities and use cases of OpenLLM and LlamaIndex. Happy coding! \u2328\ufe0f\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 14826, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cb3fad68-987a-4b78-8d97-36528341f73f": {"__data__": {"id_": "cb3fad68-987a-4b78-8d97-36528341f73f", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.html", "file_name": "building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.html", "file_type": "text/html", "file_size": 10568, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.html", "file_name": "building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.html", "file_type": "text/html", "file_size": 10568, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "6ff7436b1ebeb96d532ff78bdc64f6f25ddccb1890d080ea44f2b9cecaaa6521", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h1>\n  <strong>\n   Introduction\n  </strong>\n </h1>\n <p>\n  <a href=\"https://github.com/jerryjliu/llama_index\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaIndex (GPT Index)\n  </a>\n  offers an interface to connect your Large Language Models (LLMs) with external data. LlamaIndex provides various data structures to index your data, such as the list index, vector index, keyword index, and tree index. It offers both a high-level API and low-level API \u2014 the high-level API allows you to build a Question-Answering (QA) system in just five lines of code, whereas the lower-level API allows you to customize various aspects of retrieval and synthesis.\n </p>\n <p>\n  However, taking these systems into production requires careful evaluation of the performance of the overall system \u2014 the quality of the outputs given the inputs. Evaluation of retrieval-augmented generation can be challenging because the user would need to come up with a dataset of relevant questions for a given context. To overcome these obstacles, LlamaIndex provides Question Generation and label-free Evaluation modules.\n </p>\n <p>\n  In this blog, we will discuss the three-step evaluation process using Question Generation and Evaluation modules:\n </p>\n <ol>\n  <li>\n   Question Generation from the document\n  </li>\n  <li>\n   Generate answers/source nodes for questions using LlamaIndex QueryEngine abstractions, which manage the interaction between the LLM and data indices.\n  </li>\n  <li>\n   Evaluate if the question (query), answer, and source nodes are matching/inline\n  </li>\n </ol>\n <h1>\n  <strong>\n   1. Question Generation\n  </strong>\n </h1>\n <p>\n  It should be noted that this approach does not require ground-truth labels. The purpose of question generation is to generate an initial dataset of inputs over context that can be used to evaluate the question-answering system.\n </p>\n <p>\n  LlamaIndex offers the DataGenerator class, which generates questions from a given document using ListIndex. By default, it uses OpenAI ChatGPT (get-3.5-turbo) for question generation.\n </p>\n <pre><span class=\"qc nb gt pz b bf qd qe l qf qg\" id=\"3ace\"><span class=\"hljs-keyword\">from</span> llama_index.evaluation <span class=\"hljs-keyword\">import</span> DatasetGenerator\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> SimpleDirectoryReader\n\n<span class=\"hljs-comment\"># Load documents</span>\nreader = SimpleDirectoryReader(<span class=\"hljs-string\">\"./data\"</span>)\ndocuments = reader.load_data()\n\n<span class=\"hljs-comment\"># Generate Question</span>\ndata_generator = DatasetGenerator.from_documents(documents)\nquestion = data_generator.generate_questions_from_nodes()</span></pre>\n <h1>\n  <strong>\n   2. Generate Answers/Source Nodes (Context)\n  </strong>\n </h1>\n <p>\n  Using List Index, we generate answers and source nodes for the generated questions in the response object.\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> GPTVectorStoreIndex, SimpleDirectoryReader, load_index_from_storage, StorageContext\n\n<span class=\"hljs-comment\"># load documents</span>\ndocuments = SimpleDirectoryReader(<span class=\"hljs-string\">'./data'</span>).load_data()\n\n<span class=\"hljs-comment\"># Create Index</span>\nindex = GPTVectorStoreIndex.from_documents(documents)\n\n<span class=\"hljs-comment\"># save index to disk</span>\nindex.set_index_id(<span class=\"hljs-string\">\"vector_index\"</span>)\nindex.storage_context.persist(<span class=\"hljs-string\">'storage'</span>)\n\n<span class=\"hljs-comment\"># rebuild storage context</span>\nstorage_context = StorageContext.from_defaults(persist_dir=<span class=\"hljs-string\">'storage'</span>)\n<span class=\"hljs-comment\"># load index</span>\nindex = load_index_from_storage(storage_context, index_id=<span class=\"hljs-string\">\"vector_index\"</span>)\n\n<span class=\"hljs-comment\"># Query the index</span>\nquery_engine = index.as_query_engine(similarity_top_k=<span class=\"hljs-number\">3</span>)\nresponse = query_engine.query(&amp;lt;Query&amp;gt;)\n\n<span class=\"hljs-comment\"># Response object has both response and source nodes.</span></code></pre>\n <h1>\n  <strong>\n   3. Evaluation\n  </strong>\n </h1>\n <p>\n  The evaluation module can be used to answer the following three questions:\n </p>\n <ol>\n  <li>\n   Are the response generated and source nodes (context) matching? \u2014 Response + Source Nodes (Context)\n  </li>\n  <li>\n   Are response generated, source nodes (context), and query matching? \u2014 Query + Response + Source Nodes (Context)\n  </li>\n  <li>\n   Which source nodes of the retrieved source nodes are used to generate a response? \u2014 Query + Response + Individual Source Nodes (Context)\n  </li>\n </ol>\n <p>\n  Evaluation can be done with some combination of the query, context, and response, combining these with LLM calls.\n </p>\n <h2>\n  <strong>\n   Response + Source Nodes (Context)\n  </strong>\n </h2>\n <p>\n  This function answers the question: Are the response generated and source nodes (context) matching?\n </p>\n <p>\n  The response object for a given query returns both the response and source nodes (context) with which it generated the response. We can now evaluate the response against the retrieved sources \u2014 without taking into account the query! This allows you to measure hallucination \u2014 if the response does not match the retrieved sources, this means that the model may be \u201challucinating\u201d an answer since it is not rooting the answer in the context provided to it in the prompt.\n </p>\n <p>\n  The result is a binary response \u2014 either \u201cYES/NO\u201d.\n </p>\n <ul>\n  <li>\n   YES \u2014 Response and Source Nodes (Context) are matching.\n  </li>\n  <li>\n   NO \u2014 Response and Source Nodes (Context) are not matching.\n  </li>\n </ul>\n <pre><span class=\"qc nb gt pz b bf qd qe l qf qg\" id=\"563d\"><span class=\"hljs-keyword\">from</span> llama_index.evaluation <span class=\"hljs-keyword\">import</span> ResponseEvaluator\n\n<span class=\"hljs-comment\"># build service context</span>\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=<span class=\"hljs-number\">0</span>, model_name=<span class=\"hljs-string\">\"gpt-4\"</span>))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\n<span class=\"hljs-comment\"># Build index and get response object</span>\n...\n\n<span class=\"hljs-comment\"># define evaluator</span>\nevaluator = ResponseEvaluator(service_context=service_context)\n\n<span class=\"hljs-comment\"># evaluate using the response object</span>\neval_result = evaluator.evaluate(response)</span></pre>\n <h2>\n  <strong>\n   Query + Response + Source Nodes (Context)\n  </strong>\n </h2>\n <p>\n  This function answers the question: Are response generated, source nodes (context), and query matching?\n </p>\n <p>\n  Often with the \u201cResponse + Source Nodes (Context)\u201d approach, the response generated is in line with the source nodes but may not be the answer to the query. Therefore, considering the query along with the response and source nodes is a good approach for a more accurate analysis.\n </p>\n <p>\n  The goal is to determine if the response + source context answers the query. The result is a binary response \u2014 either \u201cYES/NO\u201d.\n </p>\n <ul>\n  <li>\n   YES \u2014 Query, Response, and Source Nodes (Context) are matching.\n  </li>\n  <li>\n   NO \u2014 Query, Response, and Source Nodes (Context) are not matching.\n  </li>\n </ul>\n <pre><span class=\"qc nb gt pz b bf qd qe l qf qg\" id=\"b15b\"><span class=\"hljs-keyword\">from</span> llama_index.evaluation <span class=\"hljs-keyword\">import</span> QueryResponseEvaluator\n\n<span class=\"hljs-comment\"># build service context</span>\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=<span class=\"hljs-number\">0</span>, model_name=<span class=\"hljs-string\">\"gpt-4\"</span>))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\n<span class=\"hljs-comment\"># Build index and get response object</span>\n...\n\n<span class=\"hljs-comment\"># define evaluator</span>\nevaluator = QueryResponseEvaluator(service_context=service_context)\n\n<span class=\"hljs-comment\"># evaluate using the response object</span>\neval_result = evaluator.evaluate(query, response)</span></pre>\n <h2>\n  <strong>\n   Query + Response + Individual Source Nodes (Context)\n  </strong>\n </h2>\n <p>\n  This function answers the question: Which source nodes of the retrieved source nodes are used to generate a response?\n </p>\n <p>\n  Often in the real world, the source nodes can be nodes from different documents. In these cases, it\u2019s important to understand which source nodes are relevant and show those documents to the users. This mode of evaluation will look at each source node and see if each source node contains an answer to the query.\n </p>\n <pre><span class=\"qc nb gt pz b bf qd qe l qf qg\" id=\"6fe3\"><span class=\"hljs-keyword\">from</span> llama_index.evaluation <span class=\"hljs-keyword\">import</span> QueryResponseEvaluator\n\n<span class=\"hljs-comment\"># build service context</span>\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=<span class=\"hljs-number\">0</span>, model_name=<span class=\"hljs-string\">\"gpt-4\"</span>))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\n<span class=\"hljs-comment\"># build index and get response object </span>\n...\n\n<span class=\"hljs-comment\"># define evaluator</span>\nevaluator = QueryResponseEvaluator(service_context=service_context)\n\n<span class=\"hljs-comment\"># evaluate using the response object</span>\neval_result = evaluator.evaluate_source_nodes(response)</span></pre>\n <p>\n  Google Colab notebook for Evaluating QA systems using LlamaIndex \u2014\n </p>\n <div>\n  <a href=\"https://colab.research.google.com/drive/1J7ZaTx746T9Xaglr-9PhdB5knnHs25ws?usp=sharing&amp;source=post_page-----3f02e9d87ce1--------------------------------\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <div class=\"rg ab jb\">\n    <div class=\"rh ab cn ca ri rj\">\n     <h2 class=\"be gu is z jj rk jl jm rl jo jq gs bj\">\n      Google Colaboratory\n     </h2>\n     <div class=\"rm l\">\n      <h3 class=\"be b is z jj rk jl jm rl jo jq dt\">\n       Evaluating QA systems using LlamaIndex\n      </h3>\n     </div>\n    </div>\n   </div>\n  </a>\n </div>\n <h1>\n  <strong>\n   Conclusion\n  </strong>\n </h1>\n <p>\n  LlamaIndex provides a comprehensive solution for building and evaluating QA systems without the need for ground-truth labels. By using the Question Generation and Evaluation modules, you can ensure that your system is accurate and reliable, making it suitable for production environments.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 10521, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72caf566-b9af-486b-aa5a-5519e5509e53": {"__data__": {"id_": "72caf566-b9af-486b-aa5a-5519e5509e53", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-better-tools-for-llm-agents-f8c5a6714f11.html", "file_name": "building-better-tools-for-llm-agents-f8c5a6714f11.html", "file_type": "text/html", "file_size": 26798, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/building-better-tools-for-llm-agents-f8c5a6714f11.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-better-tools-for-llm-agents-f8c5a6714f11.html", "file_name": "building-better-tools-for-llm-agents-f8c5a6714f11.html", "file_type": "text/html", "file_size": 26798, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "2ef0e32d85e1ae85a2a5dceb743e6dca36b962cc3275eeb7fa271cc1deb90d64", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Over the past month I\u2019ve been diving into the world of Large Language Model (LLM) Agents and building out LlamaIndex\u2019s library of tools for use with agents. I helped to lead the LlamaHub Tools effort as part of broader\n  <a href=\"https://medium.com/llamaindex-blog/data-agents-eed797d7972f\" rel=\"noopener\">\n   Data Agents launch\n  </a>\n  last week.\n </p>\n <p>\n  In the process of building out LlamaHub Tools I\u2019ve collected some techniques for creating effective and easy to use tools, and want to share some of my thoughts.\n </p>\n <h1>\n  Context on LlamaHub Tools\n </h1>\n <p>\n  <a href=\"https://llamahub.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaHub Tools\n  </a>\n  allow LLMs like ChatGPT to connect to APIs and act on a user\u2019s behalf to create, read, update and delete data. Examples of tools that we\u2019ve put together include\n  <a href=\"https://llamahub.ai/l/tools-gmail\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   drafting and sending emails\n  </a>\n  ,\n  <a href=\"https://llamahub.ai/l/tools-google_calendar\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   reading and creating Google Calendar invite\n  </a>\n  s,\n  <a href=\"https://llamahub.ai/l/tools-wikipedia\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   searching Wikipedia\n  </a>\n  , and that\u2019s just a few of the 15 tools we are releasing on launch.\n </p>\n <h2>\n  Overview of tool abstractions\n </h2>\n <p>\n  So how exactly do LlamaHub Tools work? The LlamaHub tool abstractions allow you to easily write Python functions that can be understood and called by Agents. Instead of trying to make an Agent do complicated mathematics for example, we can provide the Agent with a Tool that calls Wolfram Alpha and provides the result to the Agent:\n </p>\n <pre><span class=\"qn on gt qk b bf qo qp l qq qr\" id=\"14b4\"><span class=\"hljs-keyword\">from</span> llama_index.tools.base <span class=\"hljs-keyword\">import</span> BaseToolSpec\n\nQUERY_URL_TMPL = <span class=\"hljs-string\">\"http://api.wolframalpha.com/v1/result?appid={app_id}&amp;amp;i={query}\"</span>\n\n<span class=\"hljs-comment\"># Inherit from the LlamaIndex BaseToolSpec abstraction</span>\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">WolframAlphaToolSpec</span>(<span class=\"hljs-title class_ inherited__\">BaseToolSpec</span>):\n\n  <span class=\"hljs-comment\"># Define the functions that we export to the LLM</span>\n    spec_functions = [<span class=\"hljs-string\">\"wolfram_alpha_query\"</span>]\n\n  <span class=\"hljs-comment\"># Initialize with our wolfram alpha API key</span>\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, app_id: <span class=\"hljs-type\">Optional</span>[<span class=\"hljs-built_in\">str</span>] = <span class=\"hljs-literal\">None</span></span>) -&amp;gt; <span class=\"hljs-literal\">None</span>:\n        <span class=\"hljs-string\">\"\"\"Initialize with parameters.\"\"\"</span>\n        self.token = app_id\n  \n  <span class=\"hljs-comment\"># Our function to be called by the Agent</span>\n  <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">wolfram_alpha_query</span>(<span class=\"hljs-params\">self, query: <span class=\"hljs-built_in\">str</span></span>):\n          <span class=\"hljs-string\">\"\"\"\n          Make a query to wolfram alpha about a mathematical or scientific problem.\n  \n          Example inputs:\n              \"(7 * 12 ^ 10) / 321\"\n              \"How many calories are there in a pound of strawberries\"\n  \n          Args:\n              query (str): The query to be passed to wolfram alpha.\n  \n          \"\"\"</span>\n          response = requests.get(QUERY_URL_TMPL.<span class=\"hljs-built_in\">format</span>(app_id=self.token, query=urllib.parse.quote_plus(query)))\n          <span class=\"hljs-keyword\">return</span> response.text</span></pre>\n <p>\n  The above code is enough to define a LlamaIndex Tool that allows the Agent to query to Wolfram Alpha. No more incorrect guesses at math problems! We can initialize an instance of the Tool Spec like this:\n </p>\n <pre><span class=\"qn on gt qk b bf qo qp l qq qr\" id=\"c47c\"># Initialize an instance of the Tool\nwolfram_spec = WolframAlphaToolSpec(app_id=\"your-key\")\n# Convert the Tool Spec to a list of tools. In this case we just have one tool.\ntools = wolfram_spec.to_tool_list()\n# Convert the tool to an OpenAI function and inspect\nprint(tools[0].metadata.to_openai_function())</span></pre>\n <p>\n  Here\u2019s the cleaned up output of the print statement:\n </p>\n <pre><span class=\"qn on gt qk b bf qo qp l qq qr\" id=\"5f4f\">{\n  <span class=\"hljs-string\">'description'</span>: <span class=\"hljs-string\">'\n    Make a query to wolfram alpha about a mathematical or scientific problem.\n  \n          Example inputs:\n              \"(7 * 12 ^ 10) / 321\"\n              \"How many calories are there in a pound of strawberries\"\n  \n          Args:\n              query (str): The query to be passed to wolfram alpha.'</span>,\n  <span class=\"hljs-string\">'name'</span>: <span class=\"hljs-string\">'wolfram_alpha_query'</span>,\n  <span class=\"hljs-string\">'parameters'</span>: {\n    <span class=\"hljs-string\">'properties'</span>: {<span class=\"hljs-string\">'query'</span>: {<span class=\"hljs-string\">'title'</span>: <span class=\"hljs-string\">'Query'</span>, <span class=\"hljs-string\">'type'</span>: <span class=\"hljs-string\">'string'</span>}},\n    <span class=\"hljs-string\">'title'</span>: <span class=\"hljs-string\">'wolfram_alpha_query'</span>,\n    <span class=\"hljs-string\">'type'</span>: <span class=\"hljs-string\">'object'</span>\n  }\n}</span></pre>\n <p>\n  We can see that the\n  <a href=\"https://en.wikipedia.org/wiki/Docstring\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   docstring\n  </a>\n  describing how to use the Tool get passed to the Agent. Additionally, the parameters, type info and function name are passed along to give the Agent a strong idea on how it can use this function. All of this information is essentially acting as the prompt for how the agent understands the tool.\n </p>\n <p>\n  Inheriting from the BaseToolSpec class means it\u2019s very simple to write Tools for Agents to use. In fact, the above tool definition is only 9 lines of code, ignoring white space, imports and comments. We can easily get the function ready for Agents to use without any heavy boilerplate or modifications. Let\u2019s look at loading the Tool into an OpenAI Agent:\n </p>\n <pre><span class=\"qn on gt qk b bf qo qp l qq qr\" id=\"bbe7\">agent = OpenAIAgent.from_tools(tools, verbose=True)\nagent.chat(<span class=\"hljs-string\">'What is (7 * 12 ^ 10) / 321'</span>)\n<span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\" OUTPUT:\n=== Calling Function ===\nCalling function: wolfram_alpha_query with args: {\n  \"</span>query<span class=\"hljs-string\">\": \"</span>(<span class=\"hljs-number\">7</span> * <span class=\"hljs-number\">12</span> ^ <span class=\"hljs-number\">10</span>) / <span class=\"hljs-number\">14</span><span class=\"hljs-string\">\"\n}\nGot output: 30958682112\n========================\nResponse(response='The result of the expression (7 * 12 ^ 10) / 14 is 30,958,682,112.', source_nodes=[], metadata=None)\n\"</span><span class=\"hljs-string\">\"\"</span></span></pre>\n <p>\n  And we can test out passing this query to ChatGPT without the tools:\n </p>\n <pre><span class=\"qn on gt qk b bf qo qp l qq qr\" id=\"46fd\">&amp;gt; <span class=\"hljs-string\">'What is (7 * 12 ^ 10) / 321'</span>\n<span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\nTo calculate the expression (7 * 12^10) / 14, you need to follow the order of operations, which is parentheses, exponents, multiplication, and division (from left to right).\n\nStep 1: Calculate the exponent 12^10.\n12^10 = 619,173,642,24.\n\nStep 2: Multiply 7 by the result from Step 1.\n7 * 619,173,642,24 = 4,333,215,496,68.\n\nStep 3: Divide the result from Step 2 by 14.\n4,333,215,496,68 / 14 = 309,515,392,62.\n\nTherefore, the result of the expression (7 * 12^10) / 14 is 309,515,392,62.\n\"</span><span class=\"hljs-string\">\"\"</span></span></pre>\n <p>\n  This example should show how easily you can write new Tools for use with Agents. For the rest of the blog post I\u2019ll be talking about tips and tricks I\u2019ve found to write more functional and effective tools. Hopefully by the end of the blog post you are excited to write and contribute some Tools of your own!\n </p>\n <h1>\n  Techniques for building better tools\n </h1>\n <p>\n  Below are a variety of tactics for writing more usable and functional tools to minimize friction when interfacing with the Agent. Not all of the tactics apply to every tool, but usually at least a few of the techniques below will prove valuable.\n </p>\n <h2>\n  Writing useful tool prompts\n </h2>\n <p>\n  Here\u2019s an example of the function signature and docstring for a tool that an Agent can call to create a draft email.\n </p>\n <pre><span class=\"qn on gt qk b bf qo qp l qq qr\" id=\"bde1\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">create_draft</span>(<span class=\"hljs-params\">\n        <span class=\"hljs-variable language_\">self</span>,\n        <span class=\"hljs-symbol\">to:</span> <span class=\"hljs-title class_\">List</span>[str],\n        <span class=\"hljs-symbol\">subject:</span> str,\n        <span class=\"hljs-symbol\">message:</span> str\n    </span>) -&amp;gt; <span class=\"hljs-symbol\">str:</span>\n        <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"Create and insert a draft email.\n           Print the returned draft's message and id.\n           Returns: Draft object, including draft id and message meta data.\n\n        Args:\n            to (List[str]): The email addresses to send the message to, eg ['adam@example.com']\n            subject (str): The subject for the event\n            message (str): The message for the event\n        \"</span><span class=\"hljs-string\">\"\"</span></span></pre>\n <p>\n  This prompt takes advantage of a few different patterns to ensure that the agent can use the tool effectively:\n </p>\n <ul>\n  <li>\n   Give a concise description of the function and its purpose\n  </li>\n  <li>\n   Inform the Agent on what data will be returned from this function\n  </li>\n  <li>\n   List the arguments that the function accepts, with descriptions and type information\n  </li>\n  <li>\n   Give example values for arguments with a specific format, eg adam@example.com\n  </li>\n </ul>\n <p>\n  Tool prompts should be concise as to not take up too much length in context, but also informative enough that the agent can use the tool without making mistakes.\n </p>\n <h2>\n  Making tools tolerant of partial inputs\n </h2>\n <p>\n  One way to help Agents make fewer mistakes is to write tools that are more\n  <em class=\"ra\">\n   tolerant\n  </em>\n  of their inputs, for example by making inputs optional when the value can be inferred from somewhere else. Take the example of drafting an email, but this time let\u2019s consider a tool that updates a draft email:\n </p>\n <pre><span class=\"qn on gt qk b bf qo qp l qq qr\" id=\"77ab\">def <span class=\"hljs-title function_\">update_draft</span><span class=\"hljs-params\">(\n        self,\n        draft_id: str,\n        to: Optional[List[str]] = None,\n        subject: Optional[str] = None,\n        message: Optional[str] = None,\n    )</span> -&amp;gt; str:\n        <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"Update a draft email.\n           Print the returned draft's message and id.\n           This function is required to be passed a draft_id that is obtained when creating messages\n           Returns: Draft object, including draft id and message meta data.\n\n        Args:\n            draft_id (str): the id of the draft to be updated\n            to (Optional[str]): The email addresses to send the message to\n            subject (Optional[str]): The subject for the event\n            message (Optional[str]): The message for the event\n        \"</span><span class=\"hljs-string\">\"\"</span></span></pre>\n <p>\n  The Gmail API\n  <strong>\n   requires\n  </strong>\n  all of the above values when updating a draft, however using just the\n  <code class=\"cw rb rc rd qk b\">\n   draft_id\n  </code>\n  we can fetch the current content of the draft and use the existing values as defaults if the Agent did not provide the values when updating the draft:\n </p>\n <pre><span class=\"qn on gt qk b bf qo qp l qq qr\" id=\"4342\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">update_draft</span>(<span class=\"hljs-params\">...</span>):\n  ...\n  draft = self.get_draft(draft_id)\n  headers = draft[<span class=\"hljs-string\">'message'</span>][<span class=\"hljs-string\">'payload'</span>][<span class=\"hljs-string\">'headers'</span>]\n  <span class=\"hljs-keyword\">for</span> header <span class=\"hljs-keyword\">in</span> headers:\n      <span class=\"hljs-keyword\">if</span> header[<span class=\"hljs-string\">'name'</span>] == <span class=\"hljs-string\">'To'</span> <span class=\"hljs-keyword\">and</span> <span class=\"hljs-keyword\">not</span> to:\n          to = header[<span class=\"hljs-string\">'value'</span>]\n      <span class=\"hljs-keyword\">elif</span> header[<span class=\"hljs-string\">'name'</span>] == <span class=\"hljs-string\">'Subject'</span> <span class=\"hljs-keyword\">and</span> <span class=\"hljs-keyword\">not</span> subject:\n          subject = header[<span class=\"hljs-string\">'value'</span>]\n    <span class=\"hljs-keyword\">elif</span> header[<span class=\"hljs-string\">'name'</span>] == <span class=\"hljs-string\">'Message'</span> <span class=\"hljs-keyword\">and</span> <span class=\"hljs-keyword\">not</span> message:\n      message = header[<span class=\"hljs-string\">'values'</span>]\n  ...</span></pre>\n <p>\n  By providing the above logic in the\n  <code class=\"cw rb rc rd qk b\">\n   update_draft\n  </code>\n  function, the Agent can invoke\n  <code class=\"cw rb rc rd qk b\">\n   update_draft\n  </code>\n  with only one of the fields (and the\n  <code class=\"cw rb rc rd qk b\">\n   draft_id\n  </code>\n  ), and we can update the draft as the user expects. This means that in more circumstances the Agent can complete the task successfully, instead of returning an error or needing to ask for more information.\n </p>\n <h2>\n  Validating input and Agent error handling\n </h2>\n <p>\n  Despite best efforts at prompting and tolerance, we can end up in circumstances where the Agent invokes a tool in a way that it can\u2019t complete the task at hand. However, we can detect this and prompt the Agent to recover the error on its own.\n </p>\n <p>\n  For example, in the\n  <code class=\"cw rb rc rd qk b\">\n   update_draft\n  </code>\n  example above, what do we do if the agent calls the function without a\n  <code class=\"cw rb rc rd qk b\">\n   draft_id\n  </code>\n  ? We could simply pass along the null value and return an error from the Gmail API library, but we could also detect that a null\n  <code class=\"cw rb rc rd qk b\">\n   draft_id\n  </code>\n  will invariably cause an error, and return a prompt for the agent instead:\n </p>\n <pre><span class=\"qn on gt qk b bf qo qp l qq qr\" id=\"42b1\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">update_draft</span>(<span class=\"hljs-params\">...</span>):\n  <span class=\"hljs-keyword\">if</span> draft_id == <span class=\"hljs-literal\">None</span>:\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"You did not provide a draft id when calling this function. If you previously created or retrieved the draft, the id is available in context\"</span></span></pre>\n <p>\n  Now, if the Agent invokes\n  <code class=\"cw rb rc rd qk b\">\n   update_draft\n  </code>\n  without a\n  <code class=\"cw rb rc rd qk b\">\n   draft_id\n  </code>\n  , it is made aware of the exact mistake it made and given instructions on how it can correct the issue.\n </p>\n <p>\n  In my experience working with this tool, the Agent will often immediately call the\n  <code class=\"cw rb rc rd qk b\">\n   update_draft\n  </code>\n  function in the correct way when receiving this prompt, or if there is no\n  <code class=\"cw rb rc rd qk b\">\n   draft_id\n  </code>\n  available, it will inform the user of the issue and ask the user for a\n  <code class=\"cw rb rc rd qk b\">\n   draft_id\n  </code>\n  . Either scenario is much better than crashing or returning an opaque error from a library to the user.\n </p>\n <h2>\n  Providing simple functions related to the tool\n </h2>\n <p>\n  Agents can struggle at what would otherwise be simple functions for a computer to calculate. For example, when building a tool for creating events in Google Calendar, a user may prompt the Agent with something like this:\n </p>\n <blockquote>\n  <p class=\"nn no ra np b nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok gm bj\" id=\"fca1\">\n   <em class=\"gt\">\n    Create an event on my Calendar to discuss the Tools PR with\n   </em>\n   <a href=\"mailto:adam@example.com\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <em class=\"gt\">\n     adam@example.com\n    </em>\n   </a>\n   <em class=\"gt\">\n    tomorrow at 4pm\n   </em>\n  </p>\n </blockquote>\n <p>\n  Can you see the problem? If we try asking ChatGPT what day it is:\n </p>\n <pre><span class=\"qn on gt qk b bf qo qp l qq qr\" id=\"b470\">agent.chat('what day is it?')\n# &gt; I apologize for the confusion. As an AI language model, I don't have real-time data or access to the current date. My responses are based on the information I was last trained on, which is up until September 2021. To find out the current day, I recommend checking your device's clock, referring to a calendar, or checking an online source for the current date.</span></pre>\n <p>\n  Agents won\u2019t know what the current date is, and so the Agent would either call the function incorrectly, providing a string like\n  <code class=\"cw rb rc rd qk b\">\n   tomorrow\n  </code>\n  for the date, hallucinate a date sometime in the past based on when it was trained, or put the burden on the user to tell it the date. All of the above actions cause friction and frustration for the user.\n </p>\n <p>\n  Instead, in the Google Calendar Tool Spec we provide a simple deterministic function for the agent to call if it needs to fetch the date:\n </p>\n <pre><span class=\"qn on gt qk b bf qo qp l qq qr\" id=\"3a97\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">get_date</span>(<span class=\"hljs-params\">self</span>):\n        <span class=\"hljs-string\">\"\"\"\n        A function to return todays date.\n        Call this before any other functions if you are unaware of the current date\n        \"\"\"</span>\n        <span class=\"hljs-keyword\">return</span> datetime.date.today()</span></pre>\n <p>\n  Now, when the Agent tries to handle the prompt above, it can first call the function to get the date and then create the event as the user requested, inferring the date for \u201ctomorrow\u201d or \u201ca week from now\u201d. No errors, no guesses and no need for further user interaction!\n </p>\n <h2>\n  Returning prompts from functions that perform mutations\n </h2>\n <p>\n  Some functions perform mutations to data in a way that it isn\u2019t clear what useful data can be returned from the function, back to the agent. For example, in the Google Calendar tool if an event is successfully created it doesn\u2019t make sense to return the content of the event back to the Agent, as the agent just passed in all of the information and thus has it in context.\n </p>\n <p>\n  Generally with functions that are focused on mutations (create, update, delete) we can help the Agent understand its actions better by using the return value of these functions to further prompt the agent. For example, from the Google Calendar\n  <code class=\"cw rb rc rd qk b\">\n   create_event\n  </code>\n  tool we could do the following:\n </p>\n <pre><span class=\"qn on gt qk b bf qo qp l qq qr\" id=\"ae80\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">create_event</span>(<span class=\"hljs-params\">...</span>):\n  ...\n  <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">'Event created succesfully! You can move onto the next step.'</span>  </span></pre>\n <p>\n  This helps the agent register that the action succeeded and encourages it to complete the action it was prompted for, especially if creating the google calendar event is only a single step in a multiple step instruction. We can still return ids as part of these prompts as well:\n </p>\n <pre><span class=\"qn on gt qk b bf qo qp l qq qr\" id=\"5af9\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">create_event</span>(<span class=\"hljs-params\">...</span>):\n  ...\n  event = service.events().insert(...).execute()\n  <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">'Event created with id {event.id}! You can move onto the next step.'</span></span></pre>\n <h2>\n  Storing large responses in indices for the Agent to read\n </h2>\n <p>\n  One consideration when building tools that has been mentioned already is the size of the context window the Agent has. Currently, LLMs tend to have context windows from 4k-16k tokens, however it can certainly be larger or smaller. If the size of the data that a tool would return is larger than the context window, the Agent will be unable to process the data and error out.\n </p>\n <p>\n  One consideration when building tools that has been mentioned already is the size of the context window the Agent has. Currently, LLMs tend to have context windows from 4k-16k tokens, however it can certainly be larger or smaller. If the size of the data that a tool would return is larger than the context window, the Agent will be unable to process the data and error out.\n </p>\n <p>\n  The only consideration that needs to be made when creating tools that might need to be wrapped by the LoadAndSearchTool, is they need to return a list of LlamaIndex documents. For a tool that returns a string, the only modification you need to make to have it be compatible with the LoadAndSearchTool is wrapping it in a document and an array:\n </p>\n <pre><span class=\"qn on gt qk b bf qo qp l qq qr\" id=\"b54f\"><span class=\"hljs-keyword\">from</span> llama_index.readers.schema.base <span class=\"hljs-keyword\">import</span> Document\n\n<span class=\"hljs-comment\"># Not compatible</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">large_text_response_function</span>():\n  ...\n  <span class=\"hljs-keyword\">return</span> result\n\n<span class=\"hljs-comment\"># LoadAndSearch compatible</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">large_text_response_function</span>():\n  ...\n  <span class=\"hljs-keyword\">return</span> [Document(text=result)]</span></pre>\n <h2>\n  Verify how the Agent understands the tool\n </h2>\n <p>\n  A useful technique for debugging tools in development is to\n  <strong>\n   ask the Agent about its own tools\n  </strong>\n  : the tools it has available, what arguments the tools accept, what those arguments represent, and what the tool is used for. The responses of the Agent are useful in determining where your prompts might be lacking or helping pinpoint why an Agent is failing to successfully use a tool that you are developing.\n </p>\n <p>\n  An example conversation debugging the Google Calendar Tool Spec, assuming you have setup an Agent similar to the example notebook:\n </p>\n <pre><span class=\"qn on gt qk b bf qo qp l qq qr\" id=\"fad1\">agent.<span class=\"hljs-title function_\">chat</span>(<span class=\"hljs-string\">'what tools do you have available'</span>)\n# I have the following tools available\n#   <span class=\"hljs-number\">1.</span> <span class=\"hljs-string\">`load_data`</span>: <span class=\"hljs-title class_\">This</span> tool allows me to load data <span class=\"hljs-keyword\">from</span> your calendar. <span class=\"hljs-title class_\">It</span> can retrieve a specified number <span class=\"hljs-keyword\">of</span> events starting <span class=\"hljs-keyword\">from</span> a given date.\n#   <span class=\"hljs-number\">2.</span> <span class=\"hljs-string\">`create_event`</span>: <span class=\"hljs-title class_\">This</span> tool allows me to create an event on your calendar. <span class=\"hljs-title class_\">You</span> can provide details such <span class=\"hljs-keyword\">as</span> the title, description, location, start datetime, end datetime, and attendees.\n#   <span class=\"hljs-number\">3.</span> <span class=\"hljs-string\">`get_date`</span>: <span class=\"hljs-title class_\">This</span> tool returns today<span class=\"hljs-string\">'s date. It can be used to ensure that the correct date is used when working with other tools.\n# Please let me know if you would like more information about any of these tools or if you have any specific requests.\nagent.chat('</span>what <span class=\"hljs-variable language_\">arguments</span> does the load_data tool take<span class=\"hljs-string\">')\n# '</span><span class=\"hljs-title class_\">The</span> <span class=\"hljs-string\">`load_data`</span> tool takes the following <span class=\"hljs-attr\">arguments</span>:\n#   <span class=\"hljs-number\">1.</span> <span class=\"hljs-string\">`number_of_results`</span> (optional, <span class=\"hljs-attr\">default</span>: <span class=\"hljs-number\">100</span>): <span class=\"hljs-title class_\">Specifies</span> the number <span class=\"hljs-keyword\">of</span> events to <span class=\"hljs-keyword\">return</span> <span class=\"hljs-keyword\">from</span> your calendar.\n#   <span class=\"hljs-number\">2.</span> <span class=\"hljs-string\">`start_date`</span> (optional): <span class=\"hljs-title class_\">Specifies</span> the start date <span class=\"hljs-keyword\">from</span> which to retrieve events. <span class=\"hljs-title class_\">It</span> can be provided <span class=\"hljs-keyword\">as</span> a string <span class=\"hljs-keyword\">in</span> <span class=\"hljs-variable constant_\">ISO</span> format (e.<span class=\"hljs-property\">g</span>., <span class=\"hljs-string\">\"2023-07-05\"</span>) or <span class=\"hljs-keyword\">as</span> a <span class=\"hljs-string\">`datetime.date`</span> object.\n# <span class=\"hljs-title class_\">You</span> can use these <span class=\"hljs-variable language_\">arguments</span> to customize the number <span class=\"hljs-keyword\">of</span> events and the start date <span class=\"hljs-keyword\">for</span> retrieving data <span class=\"hljs-keyword\">from</span> your calendar.</span></pre>\n <h1>\n  Concluding thoughts\n </h1>\n <p>\n  Building tools for Agents requires you to think critically about how users will interact with the Agent, and to try and anticipate any possible usage patterns. Building well thought out tools that can be resilient to errors and issues requires careful consideration and testing of possible use cases. I hope that these reflections and techniques assist you in creating new tools for Agents, and don\u2019t forget to share your tools on\n  <a href=\"https://llama-hub-ui.vercel.app/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaHub\n  </a>\n  .\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 26753, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a75e40b3-ef51-4482-a252-f4cb730e78c6": {"__data__": {"id_": "a75e40b3-ef51-4482-a252-f4cb730e78c6", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.html", "file_name": "building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.html", "file_type": "text/html", "file_size": 9815, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.html", "file_name": "building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.html", "file_type": "text/html", "file_size": 9815, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "c25fd948b07e935b2db7f409242879ac31c614f7a759b1b3d60ff907e18475ba", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h1>\n  Introduction:\n </h1>\n <p>\n  The concept of Multi-Tenancy in RAG (Retriever-Augmented Generation) systems has become increasingly vital, especially when it comes to data security and privacy. Multi-Tenancy, in simple terms, refers to a system\u2019s ability to serve multiple users (\u2018tenants\u2019) independently and securely.\n </p>\n <p>\n  Consider this scenario: In a RAG system, there are two users, User-1 and User-2. Both have their own set of documents which they have indexed into the system. The critical aspect of Multi-Tenancy here is that when User-1 queries the system, they should only receive answers from the documents they have indexed, and not from the documents indexed by User-2, and vice versa. This separation is crucial for maintaining data confidentiality and security, as it prevents the accidental or unauthorized cross-referencing of private information between different users.\n </p>\n <p>\n  In the context of Multi-Tenancy in RAG systems, this means designing a system that not only understands and retrieves information effectively but also strictly adheres to user-specific data boundaries. Each user\u2019s interaction with the system is isolated, ensuring that the retriever component of the RAG pipeline accesses only the information relevant and permitted for that particular user. This approach is important in scenarios where sensitive or proprietary data is involved, as it safeguards against data leaks and privacy breaches.\n </p>\n <p>\n  In this blog post, we will look into Building a Multi-Tenancy RAG System with LlamaIndex.\n </p>\n <h2>\n  Solving Multi-Tenancy Challenges\n </h2>\n <p>\n  The key to managing Multi-Tenancy lies within the metadata. When indexing documents, we incorporate user-specific information into the metadata before adding it to the index. This ensures that each document is uniquely tied to an individual user.\n </p>\n <p>\n  During the query phase, the retriever uses this metadata to filter and only access documents associated with the querying user. Subsequently, it performs a semantic search to retrieve the most relevant information segments, or \u2018top_k chunks\u2019, for that user. By implementing this approach, we effectively prevent the unauthorized cross-referencing of private information between different users, upholding the integrity and confidentiality of each user\u2019s data.\n </p>\n <p>\n  Now that we\u2019ve discussed the concept, let\u2019s dive into the construction of a Multi-Tenancy RAG system. For an in-depth step-by-step guide, feel free to follow along with the subsequent instructions in our\n  <a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/multi_tenancy/multi_tenancy_rag.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Google Colab Notebook\n  </a>\n  .\n </p>\n <h2>\n  Download Data:\n </h2>\n <p>\n  We will use\n  <code class=\"cw qf qg qh qi b\">\n   An LLM Compiler for Parallel Function Calling\n  </code>\n  and\n  <code class=\"cw qf qg qh qi b\">\n   Dense X Retrieval: What Retrieval Granularity Should We Use?\n  </code>\n  papers for the demonstrations.\n </p>\n <pre><span class=\"qr np gt qi b bf qs qt l qu qv\" id=\"7f1c\">!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2312.04511.pdf\" -O \"llm_compiler.pdf\"\n!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2312.06648.pdf\" -O \"dense_x_retrieval.pdf\"</span></pre>\n <h2>\n  Load Data:\n </h2>\n <p>\n  We will load the data of\n  <code class=\"cw qf qg qh qi b\">\n   LLMCompiler\n  </code>\n  paper for user\n  <code class=\"cw qf qg qh qi b\">\n   Jerry\n  </code>\n  and\n  <code class=\"cw qf qg qh qi b\">\n   Dense X Retrieval\n  </code>\n  paper for user\n  <code class=\"cw qf qg qh qi b\">\n   Ravi\n  </code>\n </p>\n <pre><span class=\"qr np gt qi b bf qs qt l qu qv\" id=\"31a0\">reader = SimpleDirectoryReader(input_files=['dense_x_retrieval.pdf'])\ndocuments_jerry = reader.load_data()\n\nreader = SimpleDirectoryReader(input_files=['llm_compiler.pdf'])\ndocuments_ravi = reader.load_data()</span></pre>\n <h2>\n  Create An Empty Index:\n </h2>\n <p>\n  We will initially create an empty index to which we can insert documents, each tagged with metadata containing the user information.\n </p>\n <pre><span class=\"qr np gt qi b bf qs qt l qu qv\" id=\"8cca\">index = VectorStoreIndex.from_documents(documents=[])</span></pre>\n <h2>\n  Ingestion Pipeline:\n </h2>\n <p>\n  The IngestionPipeline is useful for data ingestion and performing transformations, including chunking, metadata extraction, and more. Here we utilize it to create nodes, which are then inserted into the index.\n </p>\n <pre><span class=\"qr np gt qi b bf qs qt l qu qv\" id=\"7502\">pipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=512, chunk_overlap=20),\n    ]\n)</span></pre>\n <h2>\n  Update Metadata and Insert Documents:\n </h2>\n <p>\n  We will update the metadata of the documents with each user and insert the documents into the index.\n </p>\n <pre><span class=\"qr np gt qi b bf qs qt l qu qv\" id=\"6822\"><span class=\"hljs-comment\"># For user Jerry</span>\n<span class=\"hljs-keyword\">for</span> document <span class=\"hljs-keyword\">in</span> documents_jerry:\n    document.metadata[<span class=\"hljs-string\">'user'</span>] = <span class=\"hljs-string\">'Jerry'</span>\n\nnodes = pipeline.run(documents=documents_jerry)\n<span class=\"hljs-comment\"># Insert nodes into the index</span>\nindex.insert_nodes(nodes)\n\n<span class=\"hljs-comment\"># For user Ravi</span>\n<span class=\"hljs-keyword\">for</span> document <span class=\"hljs-keyword\">in</span> documents_ravi:\n    document.metadata[<span class=\"hljs-string\">'user'</span>] = <span class=\"hljs-string\">'Ravi'</span>\n\nnodes = pipeline.run(documents=documents_ravi)\n<span class=\"hljs-comment\"># Insert nodes into the index</span>\nindex.insert_nodes(nodes)</span></pre>\n <h2>\n  Define Query Engines:\n </h2>\n <p>\n  We will define query engines for both the users with necessary filters.\n </p>\n <pre><span class=\"qr np gt qi b bf qs qt l qu qv\" id=\"1dc2\"># For Jerry\njerry_query_engine = index.as_query_engine(\n    filters=MetadataFilters(\n        filters=[\n            ExactMatchFilter(\n                key=\"user\",\n                value=\"Jerry\",\n            )\n        ]\n    ),\n    similarity_top_k=3\n)\n\n# For Ravi\nravi_query_engine = index.as_query_engine(\n    filters=MetadataFilters(\n        filters=[\n            ExactMatchFilter(\n                key=\"user\",\n                value=\"Ravi\",\n            )\n        ]\n    ),\n    similarity_top_k=3\n)</span></pre>\n <h2>\n  Querying:\n </h2>\n <pre><span class=\"qr np gt qi b bf qs qt l qu qv\" id=\"650a\"># Jerry has Dense X Rerieval paper and should be able to answer following question.\nresponse = jerry_query_engine.query(\n    \"what are propositions mentioned in the paper?\"\n)</span></pre>\n <blockquote>\n  <p class=\"om on qz oo b op pk or os ot pl ov ow ox pm oz pa pb pn pd pe pf po ph pi pj gm bj\" id=\"1345\">\n   The paper mentions propositions as an alternative retrieval unit choice. Propositions are defined as atomic expressions of meanings in text that correspond to distinct pieces of meaning in the text. They are minimal and cannot be further split into separate propositions. Each proposition is contextualized and self-contained, including all the necessary context from the text to interpret its meaning. The paper demonstrates the concept of propositions using an example about the Leaning Tower of Pisa, where the passage is split into three propositions, each corresponding to a distinct factoid about the tower.\n  </p>\n </blockquote>\n <pre><span class=\"qr np gt qi b bf qs qt l qu qv\" id=\"a693\"># Ravi has LLMCompiler paper\nresponse = ravi_query_engine.query(\"what are steps involved in LLMCompiler?\")</span></pre>\n <blockquote>\n  <p class=\"om on qz oo b op pk or os ot pl ov ow ox pm oz pa pb pn pd pe pf po ph pi pj gm bj\" id=\"8279\">\n   LLMCompiler consists of three key components: an LLM Planner, a Task Fetching Unit, and an Executor. The LLM Planner identifies the execution flow by defining different function calls and their dependencies based on user inputs. The Task Fetching Unit dispatches the function calls that can be executed in parallel after substituting variables with the actual outputs of preceding tasks. Finally, the Executor executes the dispatched function calling tasks using the associated tools. These components work together to optimize the parallel function calling performance of LLMs.\n  </p>\n </blockquote>\n <pre><span class=\"qr np gt qi b bf qs qt l qu qv\" id=\"451a\"># This should not be answered as Jerry does not have information about LLMCompiler\nresponse = jerry_query_engine.query(\"what are steps involved in LLMCompiler?\")</span></pre>\n <blockquote>\n  <p class=\"om on qz oo b op pk or os ot pl ov ow ox pm oz pa pb pn pd pe pf po ph pi pj gm bj\" id=\"e477\">\n   I\u2019m sorry, but I couldn\u2019t find any information about the steps involved in LLMCompiler in the given context.\n  </p>\n </blockquote>\n <p>\n  As demonstrated, if Jerry queries the system regarding a document indexed by Ravi, the system does not retrieve any answers from that document.\n </p>\n <h1>\n  What\u2019s Next?\n </h1>\n <p>\n  We have included a\n  <a href=\"https://github.com/run-llama/llama-hub/tree/main/llama_hub/llama_packs/multi_tenancy_rag\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   MultiTenancyRAGPack\n  </a>\n  within the LlamaPacks and\n  <a href=\"https://replit.com/@LlamaIndex/LlamaIndex-Multi-Tenancy-RAG#README.md\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Replit template\n  </a>\n  which offers a Streamlit interface for hands-on experience. Be sure to explore it.\n </p>\n <h2>\n  References:\n </h2>\n <p>\n  <a href=\"https://qdrant.tech/documentation/tutorials/llama-index-multitenancy/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Multi-Tenancy RAG with Qdrant and LlamaIndex.\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 9790, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c2a141f-c9f0-4abd-b500-8cb1106bc427": {"__data__": {"id_": "3c2a141f-c9f0-4abd-b500-8cb1106bc427", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.html", "file_name": "building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.html", "file_type": "text/html", "file_size": 12899, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.html", "file_name": "building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.html", "file_type": "text/html", "file_size": 12899, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "c681188e63de5e65eca9df669f45e84e0730d03c9c2d3c15a4e1dd6378d19066", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  In the ever-evolving landscape of AI, OpenAI\u2019s ChatGPT with vision capabilities has opened a new chapter. It\u2019s an exciting time for developers and creators as we explore the fusion of visual understanding with conversational AI. Inspired by this innovation, I set out to build my own multi-modal prototype, not just as a replica but as a launchpad for more advanced and tailored visual-language applications.\n </p>\n <p>\n  The tools at our disposal are nothing short of extraordinary.\n  <strong>\n   KOSMOS-2\n  </strong>\n  a true powerhouse in painting vivid narratives from mere pixels, making image captioning seem almost magical. Then there\u2019s the Google\n  <strong>\n   PaLM API\n  </strong>\n  , bringing a level of conversational depth that truly understands and responds with relevance. And of course, there\u2019s\n  <strong>\n   LlamaIndex\n  </strong>\n  - the brains of the operation, orchestrating these elements with such finesse that the interaction flows as naturally as a conversation between old friends.\n </p>\n <h1>\n  Features Overview\n </h1>\n <p>\n  The outcome of my curiosity and coding is a Streamlit app \u2014 a prototype that stands as an homage and alternative to ChatGPT\u2019s vision capabilities. Here\u2019s what it brings to the table:\n </p>\n <ul>\n  <li>\n   <strong>\n    Real-Time Image Interaction:\n   </strong>\n   Upload your images and instantly dive into a dialogue about them.\n  </li>\n  <li>\n   <strong>\n    Automatic Captioning with KOSMOS-2:\n   </strong>\n   Microsoft\u2019s AI model offers a descriptive base for the conversation.\n  </li>\n  <li>\n   <strong>\n    Conversational Depth with PaLM:\n   </strong>\n   Google\u2019s language model ensures the chat is as rich and nuanced as the images themselves.\n  </li>\n  <li>\n   <strong>\n    User-Friendly Interface:\n   </strong>\n   Streamlit powers an intuitive and clean UI, making it easy for anyone to navigate and interact.\n  </li>\n </ul>\n <h1>\n  Deep Dive into the Tech Stack\n </h1>\n <p>\n  The project is a symphony of technologies, each playing a crucial role:\n </p>\n <ul>\n  <li>\n   Microsoft AI\n   <strong>\n    KOSMOS-2\n   </strong>\n   via Replicate breathes life into images by providing them a narrative.\n  </li>\n  <li>\n   Google\n   <strong>\n    PaLM API\n   </strong>\n   adds the layer of linguistic intelligence, making the conversation about the images insightful and engaging.\n  </li>\n  <li>\n   <strong>\n    LlamaIndex\n   </strong>\n   acts as the maestro, coordinating the models to work in harmony.\n  </li>\n </ul>\n <h1>\n  Unveiling\n  <code class=\"cw qe qf qg qh b\">\n   app.py\n  </code>\n  : The Core of the Application\n </h1>\n <p>\n  The\n  <code class=\"cw qe qf qg qh b\">\n   app.py\n  </code>\n  script is the heart of the app, where we bring together KOSMOS-2 and PaLM with Llamaindex to create a seamless multimodal experience. Let\u2019s walk through it, from start to finish.\n </p>\n <p>\n  <strong>\n   1. Initial Setup\n  </strong>\n </p>\n <p>\n  We start by importing the necessary libraries and setting up our Streamlit page. Here, we lay the groundwork for image processing and conversation management.\n </p>\n <pre><span class=\"ql om gt qh b bf qm qn l qo qp\" id=\"035c\"><span class=\"hljs-keyword\">import</span> streamlit <span class=\"hljs-keyword\">as</span> st\n<span class=\"hljs-keyword\">import</span> extra_streamlit_components <span class=\"hljs-keyword\">as</span> stx\n<span class=\"hljs-keyword\">import</span> requests\n<span class=\"hljs-keyword\">from</span> PIL <span class=\"hljs-keyword\">import</span> Image\n<span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span> AutoProcessor, AutoModelForVision2Seq\n<span class=\"hljs-keyword\">from</span> io <span class=\"hljs-keyword\">import</span> BytesIO\n<span class=\"hljs-keyword\">import</span> replicate\n<span class=\"hljs-keyword\">from</span> llama_index.llms.palm <span class=\"hljs-keyword\">import</span> PaLM\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> ServiceContext, VectorStoreIndex, Document\n<span class=\"hljs-keyword\">from</span> llama_index.memory <span class=\"hljs-keyword\">import</span> ChatMemoryBuffer\n<span class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\">import</span> datetime\n\nst.set_page_config(layout=<span class=\"hljs-string\">\"wide\"</span>)\nst.write(<span class=\"hljs-string\">\"My version of ChatGPT vision. You can upload an image and start chatting with the LLM about the image\"</span>)</span></pre>\n <p>\n  <strong>\n   2. User Interface\n  </strong>\n </p>\n <p>\n  Next, we craft the sidebar and the main area, ensuring that the user knows who created the app and has access to other projects, enhancing credibility and engagement.\n </p>\n <pre><span class=\"ql om gt qh b bf qm qn l qo qp\" id=\"aec3\"># Sidebar\nst.sidebar.markdown('## Created By')\nst.sidebar.markdown(\"[Harshad Suryawanshi](https://www.linkedin.com/in/harshadsuryawanshi/)\")\nst.sidebar.markdown('## Other Projects')\n# ...sidebar content continues</span></pre>\n <p>\n  <strong>\n   3. Image Upload and Processing\n  </strong>\n </p>\n <p>\n  Upon uploading an image, the app not only displays it but also invokes the\n  <code class=\"cw qe qf qg qh b\">\n   get_image_caption\n  </code>\n  function to generate a relevant caption. This function, decorated with\n  <code class=\"cw qe qf qg qh b\">\n   @st.cache\n  </code>\n  for caching, uses the KOSMOS-2 model through Replicate to provide a brief description of the uploaded image. The description is then used as the basis for the initial conversation with the user.\n </p>\n <pre><span class=\"ql om gt qh b bf qm qn l qo qp\" id=\"699f\"><span class=\"hljs-meta\">@st.cache</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">get_image_caption</span>(<span class=\"hljs-params\">image_data</span>):\n    input_data = {\n        <span class=\"hljs-string\">\"image\"</span>: image_data,\n        <span class=\"hljs-string\">\"description_type\"</span>: <span class=\"hljs-string\">\"Brief\"</span>\n    }\n    output = replicate.run(\n        <span class=\"hljs-string\">\"lucataco/kosmos-2:3e7b211c29c092f4bcc8853922cc986baa52efe255876b80cac2c2fbb4aff805\"</span>,\n        <span class=\"hljs-built_in\">input</span>=input_data\n    )\n    <span class=\"hljs-comment\"># Split the output string on the newline character and take the first item</span>\n    text_description = output.split(<span class=\"hljs-string\">'\\n\\n'</span>)[<span class=\"hljs-number\">0</span>]\n    <span class=\"hljs-keyword\">return</span> text_description</span></pre>\n <p>\n  <strong>\n   4. Conversational Flow with PaLM and Llamaindex\n  </strong>\n </p>\n <p>\n  With the image caption in hand, the\n  <code class=\"cw qe qf qg qh b\">\n   create_chat_engine\n  </code>\n  function is called to set up the chat engine. This function is crucial as it establishes the context for the conversation and initializes the PaLM API for interaction.\n </p>\n <pre><span class=\"ql om gt qh b bf qm qn l qo qp\" id=\"acc0\"><span class=\"hljs-meta\">@st.cache_resource</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">create_chat_engine</span>(<span class=\"hljs-params\">img_desc, api_key</span>):\n    llm = PaLM(api_key=api_key)\n    service_context = ServiceContext.from_defaults(llm=llm)\n    doc = Document(text=img_desc)\n    index = VectorStoreIndex.from_documents([doc], service_context=service_context)\n    chatmemory = ChatMemoryBuffer.from_defaults(token_limit=<span class=\"hljs-number\">1500</span>)\n    \n    chat_engine = index.as_chat_engine(\n        chat_mode=<span class=\"hljs-string\">\"context\"</span>,\n        system_prompt=(\n            <span class=\"hljs-string\">f\"You are a chatbot, able to have normal interactions, as well as talk. \"</span>\n            <span class=\"hljs-string\">\"You always answer in great detail and are polite. Your responses always descriptive. \"</span>\n            <span class=\"hljs-string\">\"Your job is to talk about an image the user has uploaded. Image description: {img_desc}.\"</span>\n        ),\n        verbose=<span class=\"hljs-literal\">True</span>,\n        memory=chatmemory\n    )\n    <span class=\"hljs-keyword\">return</span> chat_engine</span></pre>\n <p>\n  The\n  <code class=\"cw qe qf qg qh b\">\n   create_chat_engine\n  </code>\n  function builds the infrastructure for our app's conversation capabilities. It starts by instantiating a PaLM object with the provided API key, setting up the service context, and creating a document with the image description. This document is then indexed to prepare it for Llamaindex\u2019s context chat engine. Finally, the chat engine is configured with a prompt that instructs the AI on how to engage in the conversation, referencing the image description and defining the chatbot's behavior.\n </p>\n <p>\n  <strong>\n   5. User Interaction and Message Handling\n  </strong>\n </p>\n <p>\n  The application ensures an engaging and controlled user experience by limiting the number of messages to 20 per session in the demo version. If this limit is reached, it gracefully notifies the user and disables further input to manage resources effectively.\n </p>\n <pre><span class=\"ql om gt qh b bf qm qn l qo qp\" id=\"b5f9\"><span class=\"hljs-keyword\">if</span> message_count &amp;gt;= <span class=\"hljs-number\">20</span>:\n    st.error(<span class=\"hljs-string\">\"Notice: The maximum message limit for this demo version has been reached.\"</span>)\n    <span class=\"hljs-comment\"># Disabling the uploader and input by not displaying them</span>\n    image_uploader_placeholder = st.empty()  <span class=\"hljs-comment\"># Placeholder for the uploader</span>\n    chat_input_placeholder = st.empty()      <span class=\"hljs-comment\"># Placeholder for the chat input</span></span></pre>\n <p>\n  However, when the message count is within the limit, the application provides a clear chat option and handles the image upload process. Upon uploading, it immediately processes the image to get a caption, sets up the chat engine, and updates the user interface to reflect the successful upload.\n </p>\n <pre><span class=\"ql om gt qh b bf qm qn l qo qp\" id=\"621b\"><span class=\"hljs-keyword\">else</span>:\n    <span class=\"hljs-comment\"># Add a clear chat button</span>\n    <span class=\"hljs-keyword\">if</span> st.button(<span class=\"hljs-string\">\"Clear Chat\"</span>):\n        clear_chat()\n\n    <span class=\"hljs-comment\"># Image upload section</span>\n    image_file = st.file_uploader(<span class=\"hljs-string\">\"Upload an image\"</span>, <span class=\"hljs-built_in\">type</span>=[<span class=\"hljs-string\">\"jpg\"</span>, <span class=\"hljs-string\">\"jpeg\"</span>, <span class=\"hljs-string\">\"png\"</span>], key=<span class=\"hljs-string\">\"uploaded_image\"</span>, on_change=on_image_upload)\n    <span class=\"hljs-comment\"># ...code for image upload and display</span></span></pre>\n <p>\n  For each user input, the message is added to the chat history, and the chat engine is queried for a response. The app ensures that each message \u2014 whether from the user or the assistant \u2014 is displayed in the chat interface, maintaining a coherent conversation flow.\n </p>\n <pre><span class=\"ql om gt qh b bf qm qn l qo qp\" id=\"db4a\"><span class=\"hljs-comment\"># ...code for handling user input and displaying chat history</span>\n\n<span class=\"hljs-comment\"># Call the chat engine to get the response if an image has been uploaded</span>\n<span class=\"hljs-keyword\">if</span> image_file <span class=\"hljs-keyword\">and</span> user_input:\n    <span class=\"hljs-keyword\">try</span>:\n        <span class=\"hljs-keyword\">with</span> st.spinner(<span class=\"hljs-string\">'Waiting for the chat engine to respond...'</span>):\n            <span class=\"hljs-comment\"># Get the response from your chat engine</span>\n            response = chat_engine.chat(user_input)\n        <span class=\"hljs-comment\"># ...code for appending and displaying the assistant's response</span>\n    <span class=\"hljs-keyword\">except</span> Exception <span class=\"hljs-keyword\">as</span> e:\n        st.error(<span class=\"hljs-string\">f'An error occurred.'</span>)\n        <span class=\"hljs-comment\"># ...exception handling code</span></span></pre>\n <h1>\n  Wrapping Up\n </h1>\n <p>\n  This app is the foundation, a springboard for more complex visual-language applications. The potential is limitless, and your insights can shape its future. I invite you to dive into the demo, tinker with the code, and join me in pushing the envelope of what AI can do.\n </p>\n <p>\n  <a href=\"https://github.com/AI-ANK/PaLM-Kosmos-Vision\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Link to GitHub Repo\n  </a>\n </p>\n <p>\n  <a href=\"https://www.linkedin.com/in/harshadsuryawanshi/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Connect with Me on LinkedIn\n  </a>\n </p>\n <p>\n  <a href=\"https://www.linkedin.com/posts/harshadsuryawanshi_ai-computervision-streamlit-activity-7126698614541680640-y5kB\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LinkedIn Post\n  </a>\n  :\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 12872, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4bf5fad7-81d8-42a0-8759-88e33e229195": {"__data__": {"id_": "4bf5fad7-81d8-42a0-8759-88e33e229195", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf.html", "file_name": "building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf.html", "file_type": "text/html", "file_size": 8785, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf.html", "file_name": "building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf.html", "file_type": "text/html", "file_size": 8785, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "17e175992d30d72337fa7cc9b2c6db479714d442597cc0899a1a1786630204e2", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h1>\n  Introduction\n </h1>\n <p>\n  We are seeing a huge wave of developers building Retrieval Augmented Generation (RAG) applications. The RAG tech stack generally contains a retrieval pipeline, LLM and prompt, among which LLM is accessible and developers are comfortable with prompt customization. However, developers new to search and index often need extensive help to build an effective\n  <strong>\n   retrieval\n  </strong>\n  pipeline. A production-ready retrieval pipeline typically consists of the following components:\n </p>\n <ul>\n  <li>\n   Document loader that parses and splits the long text\n  </li>\n  <li>\n   Embedding model serving as core indexing component\n  </li>\n  <li>\n   A vector database that stores the vector embeddings\n  </li>\n  <li>\n   Advanced components to future optimize retrieval quality, such as re-ranker model to judge semantic similarity better\n  </li>\n </ul>\n <p>\n  It\u2019s challenging to operate this complex tech stack. It involves managing software package dependencies, hosting services in Kubernetes clusters, and monitoring the performance of ML models. The high DevOps cost distracts developers from the most critical part of the user experience of RAG applications: prompt engineering, answer generation, and user interface.\n </p>\n <p>\n  While experienced search infrastructure engineers may still manage a complicated tech stack for its flexibility, Zilliz believes that most RAG developers could benefit from a retrieval API service that is user-friendly and allows for lighter customization.\n </p>\n <p>\n  Integrating\n  <strong>\n   Zilliz Cloud Pipelines\n  </strong>\n  and\n  <strong>\n   LlamaIndex\n  </strong>\n  brings a new approach to solving this problem. Zilliz Cloud Pipelines is a fully managed, scalable retrieval service. LlamaIndex is a flexible RAG framework that provides libraries and tools for organizing business logics such as retrieval and prompt engineering. The API service of Zilliz Cloud Pipelines is abstracted as a ManagedIndex in LlamaIndex. RAG developers using\n  <strong>\n   <em class=\"px\">\n    ZillizCloudPipelineIndex\n   </em>\n  </strong>\n  can easily scale the app from one user to millions of users without the hassle of setting up and maintaining the complex retrieval tech stack. It hides the technical complexity behind a few function calls, so that developers can focus on the core user experience of their RAG apps.\n </p>\n <p>\n  In this blog, we show how to use\n  <em class=\"px\">\n   ZillizCloudPipelineIndex\n  </em>\n  to build a high quality RAG chatbot. The chatbot is scalable and supports multi-tenancy through metadata filtering.\n </p>\n <h1>\n  Set up\n </h1>\n <p>\n  Since Zilliz Cloud Pipelines is an API service, first you need to set up a\n  <a href=\"https://cloud.zilliz.com/signup?utm_source=referral&amp;utm_medium=partner&amp;utm_campaign=2024-01-18_blog_zcp-llamaindex_llamaindex&amp;utm_content=llamaindex\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Zilliz Cloud\n  </a>\n  account and create a free serverless cluster.\n </p>\n <p>\n  Now you can construct\n  <strong>\n   <em class=\"px\">\n    ZillizCloudPipelineIndex\n   </em>\n  </strong>\n  and get the handler to index docs and query later.\n </p>\n <pre><span class=\"qk np gt qh b bf ql qm l qn qo\" id=\"2d65\">from llama_index.indices import ZillizCloudPipelineIndex\n\nzcp_index = ZillizCloudPipelineIndex(\n    project_id=\"<span class=\"hljs-symbol\">&amp;lt;</span>YOUR_ZILLIZ_PROJECT_ID<span class=\"hljs-symbol\">&amp;gt;</span>\",\n    cluster_id=\"<span class=\"hljs-symbol\">&amp;lt;</span>YOUR_ZILLIZ_CLUSTER_ID<span class=\"hljs-symbol\">&amp;gt;</span>\",\n    token=\"<span class=\"hljs-symbol\">&amp;lt;</span>YOUR_ZILLIZ_API_KEY<span class=\"hljs-symbol\">&amp;gt;</span>\",\n)\nzcp_index.create_pipelines(metadata_schema={\"user_id\": \"VarChar\", \"version\": \"VarChar\"})</span></pre>\n <p>\n  You can copy the Project ID, Cluster ID and API Key from your Zilliz account as follows:\n </p>\n <h1>\n  Ingest Documents\n </h1>\n <p>\n  Suppose your application has multiple users, and you would like to tag each user\u2019s document to provide isolation. Your application logic can be implemented as follows. For simplicity, here we demo ingesting public documents. Currently, Zilliz Cloud Pipelines\n  <a href=\"https://docs.zilliz.com/docs/run-pipelines#run-an-ingestion-pipeline?utm_source=referral&amp;utm_medium=partner&amp;utm_campaign=2024-01-18_blog_zcp-llamaindex_llamaindex\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   supports\n  </a>\n  documents stored and managed in AWS S3 and Google Cloud Storage. Local document upload will also be supported soon.\n </p>\n <pre><span class=\"qk np gt qh b bf ql qm l qn qo\" id=\"583d\"># user1 ingests a document, it is technical documentation <span class=\"hljs-keyword\">for</span> v2<span class=\"hljs-number\">.3</span> version. \nzcp_index.insert_doc_url(\n    url=<span class=\"hljs-string\">\"https://publicdataset.zillizcloud.com/milvus_doc.md\"</span>,\n    metadata={<span class=\"hljs-string\">\"user_id\"</span>: <span class=\"hljs-string\">\"user1\"</span>, <span class=\"hljs-string\">\"version\"</span>: <span class=\"hljs-string\">\"2.3\"</span>},\n)\n# user2 ingests a document, it is technical documentation <span class=\"hljs-keyword\">for</span> v2<span class=\"hljs-number\">.2</span> version. \nzcp_index.insert_doc_url(\n    url=<span class=\"hljs-string\">\"https://publicdataset.zillizcloud.com/milvus_doc_22.md\"</span>,\n    metadata={<span class=\"hljs-string\">\"user_id\"</span>: <span class=\"hljs-string\">\"user2\"</span>, <span class=\"hljs-string\">\"version\"</span>: <span class=\"hljs-string\">\"2.2\"</span>},\n)</span></pre>\n <h1>\n  Query\n </h1>\n <p>\n  To conduct semantic search with\n  <em class=\"px\">\n   ZillizCloudPipelineIndex\n  </em>\n  , you can use it\n  <em class=\"px\">\n   as_query_engine()\n  </em>\n  by specifying a few parameters:\n </p>\n <ul>\n  <li>\n   search_top_k: How many text nodes/chunks to retrieve. Optional, defaults to DEFAULT_SIMILARITY_TOP_K (2).\n  </li>\n  <li>\n   filters: Metadata filters. Optional, defaults to None. In this example, we set the filter to only retrieve docs of a specific user, to provide user-level data isolation.\n  </li>\n  <li>\n   output_metadata: What metadata fields to return with the retrieved text node. Optional, defaults to [].\n  </li>\n </ul>\n <pre><span class=\"qk np gt qh b bf ql qm l qn qo\" id=\"8ac9\"><span class=\"hljs-comment\"># Query the documents in ZillizCloudPipelineIndex</span>\n<span class=\"hljs-keyword\">from</span> llama_index.vector_stores.types <span class=\"hljs-keyword\">import</span> ExactMatchFilter, MetadataFilters\n\nquery_engine_for_user1 = zcp_index.as_query_engine(\n    search_top_k=<span class=\"hljs-number\">3</span>,\n    filters=MetadataFilters(\n        filters=[\n            ExactMatchFilter(key=<span class=\"hljs-string\">\"user_id\"</span>, value=<span class=\"hljs-string\">\"user1\"</span>)\n        ]  <span class=\"hljs-comment\"># The query would only search from documents of user1.</span>\n    ),\n    output_metadata=[<span class=\"hljs-string\">\"user_id\"</span>, <span class=\"hljs-string\">\"version\"</span>], <span class=\"hljs-comment\"># output these tags together with document text</span>\n)\n\nquestion = <span class=\"hljs-string\">\"Can users delete entities by complex boolean expressions?\"</span>\n<span class=\"hljs-comment\"># The chatbot will only answer with the retrieved information from user1's documents</span>\nanswer = query_engine_for_user1.query(question)</span></pre>\n <p>\n  Thanks to the abstraction of LlamaIndex and Zilliz Cloud Pipelines, with just 30 lines of code, we can demo a RAG service that supports multi-tenancy. Most importantly, this simple RAG app could easily scale to serving millions of users without changing any code.\n </p>\n <h1>\n  What to do next?\n </h1>\n <p>\n  You can check out the official\n  <a href=\"https://docs.llamaindex.ai/en/stable/examples/managed/zcpDemo.html#managed-index-with-zilliz-cloud-pipelines\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaIndex documentation\n  </a>\n  to learn about advanced customization of\n  <em class=\"px\">\n   ZillizCloudPipelineIndex\n  </em>\n  . Please ask questions at\n  <a href=\"https://discord.gg/8uyFbECzPX\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Zilliz user group\n  </a>\n  or\n  <a href=\"https://discord.com/invite/eN6D2HQ4aX\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaIndex discord\n  </a>\n  if you have any questions. Zilliz Cloud Pipelines will soon support local file upload and more choices of embedding and re-ranker models. Get a free\n  <a href=\"https://cloud.zilliz.com/signup?utm_source=referral&amp;utm_medium=partner&amp;utm_campaign=2024-01-18_blog_zcp-llamaindex_llamaindex&amp;utm_content=llamaindex\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Zilliz Cloud\n  </a>\n  account and stay tuned for more updates!\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 8780, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7603167c-80bd-4ae6-ba83-be688bcc6ff2": {"__data__": {"id_": "7603167c-80bd-4ae6-ba83-be688bcc6ff2", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-the-data-framework-for-llms-bca068e89e0e.html", "file_name": "building-the-data-framework-for-llms-bca068e89e0e.html", "file_type": "text/html", "file_size": 13028, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/building-the-data-framework-for-llms-bca068e89e0e.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-the-data-framework-for-llms-bca068e89e0e.html", "file_name": "building-the-data-framework-for-llms-bca068e89e0e.html", "file_type": "text/html", "file_size": 13028, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "5d32cc09c3e0a26afcd544bf5583ec13d28a1580ba066e3c2453fe222e81fc09", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Today is an exciting day for LlamaIndex, and a big milestone in my personal journey with generative AI. I\u2019ve followed generative models for most of my academic/professional career \u2014 from\n  <a href=\"https://scholar.google.com/citations?user=8JjemawAAAAJ&amp;hl=en\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   my research on GANs/sensor compression\n  </a>\n  to following\n  <a href=\"https://arxiv.org/abs/1706.03762\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Transformers\n  </a>\n  /\n  <a href=\"https://openai.com/blog/gpt-3-apps\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   GPT\n  </a>\n  <a href=\"https://openai.com/blog/chatgpt\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   developments\n  </a>\n  . It became increasingly clear that as these models got bigger/better, they were evolving from knowledge generators to intelligent engines that could reason/act over new information.\n </p>\n <p>\n  I formalized some of these key intuitions more concretely:\n </p>\n <ul>\n  <li>\n   LLMs are fantastic reasoning engines, capable of question-answering, summarization, planning, and more. They had the promise of becoming the \u201cneural\u201d compute unit at the core of a new age of AI-enabled software.\n  </li>\n  <li>\n   Yet, LLMs inherently have no awareness of your own data.\n  </li>\n  <li>\n   No one really knew the best practices for feeding your data into the LLM. Models had limited context windows and were expensive to finetune.\n  </li>\n </ul>\n <p>\n  If we could offer a toolkit to help set up the data architecture for LLM apps, then we could enable anyone to build LLM-powered knowledge workers and transform the way that software is written over private data. LLM-enabled software requires new infrastructure tooling over your data and has significant implications for the modern software data stack.\n </p>\n <p>\n  Determined to tackle this challenge, I built GPT Index (which we later rebranded to LlamaIndex), an initial exploratory effort to organize and retrieve information using LLMs. (\n  <a href=\"https://twitter.com/jerryjliu0/status/1590192512639332353?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   first Tweet is here\n  </a>\n  !)\n </p>\n <p>\n  It happened at the perfect time. Since last November, there has been an explosion in developer interest in building applications on top of LLMs. Most developers were figuring out ways to leverage the reasoning capabilities of LLMs on top of their own private data. Just two months in, I joined forces with Simon Suo, a brilliant AI technologist and my former colleague, and we evolved LlamaIndex from an exploratory project into a comprehensive framework designed to connect a user\u2019s private data with LLMs. It gained recognition within the AI community, captivating the attention of hackers, developers, and industry experts alike. In just six months, the project garnered an impressive following, with\n  <a href=\"https://github.com/jerryjliu/llama_index\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   16K Github Stars\n  </a>\n  ,\n  <a href=\"https://twitter.com/llama_index\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   20K Twitter followers\n  </a>\n  ,\n  <a href=\"https://pypi.org/project/llama-index/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   200K monthly downloads\n  </a>\n  , and\n  <a href=\"https://discord.gg/dGcwcsnxhU\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   6K active Discord users\n  </a>\n  . Companies like Instabase, Front, and Uber started experimenting with LlamaIndex on top of their data.\n </p>\n <p>\n  Some initial stacks started to emerge \u2014 for instance a common paradigm for building QA systems and chatbots was using a simple retrieval mechanism (top-k lookup from a vector database) with an LLM. LlamaIndex became viewed as a\n  <a href=\"https://medium.com/cowboy-ventures/the-new-infra-stack-for-generative-ai-9db8f294dc3f\" rel=\"noopener\">\n   critical\n  </a>\n  <a href=\"https://www.unusual.vc/post/devtools-for-language-models\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   data\n  </a>\n  <a href=\"https://www.madrona.com/foundation-models/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   orchestration\n  </a>\n  <a href=\"https://foundationcapital.com/foundation-model-ops-powering-the-next-wave-of-generative-ai-apps/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   component\n  </a>\n  of the emerging LLM software landscape.\n </p>\n <p>\n  Yet, it became clear that there were still significant technical challenges in the space of LLMs and data, and no one had the right answers. Even with the capable toolkit that we\u2019ve developed, we were just starting to scratch the surface on unlocking value from data.\n </p>\n <p>\n  We are thrilled to share that LlamaIndex has secured $8.5 million in seed funding, led by Greylock, to help propel these efforts further. We\u2019re excited to work with Jerry Chen, Saam Motamedi, and Jason Risch on the Greylock team. Joining us in this exciting journey are Jack Altman (CEO of Lattice), Lenny Rachitsky (Lenny\u2019s Newsletter), Mathilde Collin (CEO of Front), Raquel Urtasun (CEO of Waabi), Joey Gonzalez (Berkeley), and many others. Their belief in our vision and the impact of LlamaIndex on the future of AI fuels our passion in solving these data + AI problems.\n </p>\n <h1>\n  <strong>\n   Why LlamaIndex?\n  </strong>\n </h1>\n <p>\n  Calling an LLM API is easy. Setting up a software system that can extract insights from your private data is harder.\n </p>\n <p>\n  LlamaIndex is the advanced data framework for your LLM applications. It encompasses essential features allowing you to both manage and query your data.\n </p>\n <ul>\n  <li>\n   <strong>\n    Data Management:\n   </strong>\n   Data ingestion, data parsing/slicing, data storage/indexing.\n  </li>\n  <li>\n   <strong>\n    Data Querying:\n   </strong>\n   Data retrieval, response synthesis, multi-step interactions over your data.\n  </li>\n </ul>\n <p>\n  LlamaIndex allows you to seamlessly integrate individual or enterprise data, including files, workplace apps, and databases, with LLM applications. We also offer an extensive array of integrations with other storage providers and downstream applications.\n </p>\n <ul>\n  <li>\n   100+ data loaders\n  </li>\n  <li>\n   13+ vector database providers\n  </li>\n  <li>\n   Integrations with observability and experimentation frameworks (e.g. prompt tracking and system tracing)\n  </li>\n  <li>\n   Integrations as a\n   <a href=\"https://github.com/openai/chatgpt-retrieval-plugin/blob/main/datastore/providers/llama_datastore.py\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    ChatGPT Retrieval Plugin\n   </a>\n   or with\n   <a href=\"https://github.com/poe-platform/poe-protocol/tree/main/llama_poe\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Poe\n   </a>\n  </li>\n </ul>\n <p>\n  The end result is that you can build a variety of amazing knowledge-intensive LLM applications. This ranges from a search engine over your data, to chatbot-style interfaces, to structured analytics helpers, to autonomous knowledge agents.\n </p>\n <h1>\n  <strong>\n   What\u2019s next?\n  </strong>\n </h1>\n <p>\n  There are\n  <em class=\"ps\">\n   so\n  </em>\n  many things that we want to do to more fully realize our vision of unlocking LLM capabilities on top of your data. We\u2019ll broadly break this down into two categories: 1) our continued commitment to the open-source developer community, and 2) solving the data problem at scale for enterprises.\n </p>\n <h2>\n  <strong>\n   Build the best open source data framework and developer community\n  </strong>\n </h2>\n <p>\n  At a high-level, we want to continue iterating on our core feature capabilities, improving reliability, and satisfy both the needs of beginner and advanced users.\n </p>\n <ul>\n  <li>\n   <strong>\n    Handle complex queries:\n   </strong>\n   We want to continue advancing the idea of \u201cquerying your data\u201d, whether it\u2019s through leveraging agent-style interactions for data retrieval and synthesis or program synthesis/DSL.\n  </li>\n  <li>\n   <strong>\n    Multi-modal data management:\n   </strong>\n   The future of foundation models is multimodal, not just contained to LLMs. There are many types of semi-structured data (e.g. semi-structured data like JSONs, yaml files) as well as \u201ccomplex\u201d unstructured data (audio, images, video) that we\u2019d love to have native support for.\n  </li>\n  <li>\n   <strong>\n    Better evaluation of LLM data systems:\n   </strong>\n   Properly evaluating LLM calls is already tricky (how do you best evaluate the quality of a generated output? Some\n   <a href=\"https://github.com/openai/evals\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    libraries\n   </a>\n   for handling this). This becomes even more tricky when you chain LLM calls within an overall data system. We want to invest efforts into this area to provide greater transparency to our users.\n  </li>\n  <li>\n   <strong>\n    Optimization of Latency/Cost:\n   </strong>\n   Users are faced with a plethora of choices when it comes to building a data-driven LLM app: the choice of LLM model, embedding model, vector database, etc. They must choose in accordance to a variety of factors, from latency and cost to privacy.\n  </li>\n  <li>\n   <strong>\n    Ease of use for both beginner users and advanced users:\n   </strong>\n   Our goal is to make the utilization of LLM capabilities accessible and user-friendly for individuals at all skill levels. We will develop clear tutorials, examples, and tools to simplify the learning curve and convey the value of all of our features.\n  </li>\n </ul>\n <h2>\n  <strong>\n   Solving the data problem at scale for Enterprises\n  </strong>\n </h2>\n <p>\n  As we\u2019re iterating on the open-source project, we also want to identify the surrounding pain points in being able to build and deploy data-powered LLM apps to production. Our solution to this will build upon the success of our open-source project and be a natural evolution to the enterprise setting.\n </p>\n <ul>\n  <li>\n   <strong>\n    Production-ready data ingestion and management:\n   </strong>\n   We want to handle data updates, data consistency, and scalability to larger volumes of data parsing. We also want to continue expanding on the right storage abstractions for multi-modal data.\n  </li>\n  <li>\n   <strong>\n    Scale to Large Data Volumes:\n   </strong>\n   Enterprises will typically have orders of magnitude more data than an individual. We want to invest in hosted infrastructure/deployment solutions around our core package so that you don\u2019t have to.\n  </li>\n  <li>\n   <strong>\n    Domain-specific LLM solutions:\n   </strong>\n   We want to offer packaged solutions to enable users to easily build LLM apps in different domains, from healthcare to finance to legal.\n  </li>\n </ul>\n <p>\n  If you\u2019re building LLM apps in the enterprise setting, we\u2019d love to chat and learn more about pain points + desired features! Check out our\n  <a href=\"https://docs.google.com/forms/d/1lzXIE9G07D9eoK7MBUpRuN-PdBYGA7nWVqcIvNtN9mQ/edit#responses\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   form here\n  </a>\n  .\n </p>\n <h1>\n  <strong>\n   Join the Llama Gang! \ud83e\udd99\n  </strong>\n </h1>\n <p>\n  Join the Llama(Index) gang as we embark on this journey to solve problems at the intersection of LLMs and data. We are not just building tools for ML practitioners/researchers; the emerging LLM + data architecture stacks have implications for\n  <em class=\"ps\">\n   all\n  </em>\n  of software development. As a result, we are operating at the intersection of incredibly fun and challenging problems from a variety of different fields:\n </p>\n <ul>\n  <li>\n   Foundation Model Development\n  </li>\n  <li>\n   Information Retrieval + Recommendation Systems\n  </li>\n  <li>\n   Data Systems\n  </li>\n  <li>\n   MLOps\n  </li>\n  <li>\n   DevOps\n  </li>\n </ul>\n <p>\n  Interested in checking out the project?\n </p>\n <ul>\n  <li>\n   Find our project on\n   <a href=\"https://github.com/jerryjliu/llama_index\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Github\n   </a>\n   and check out our\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n  </li>\n  <li>\n   Check out our brand new landing page:\n   <a href=\"https://llamaindex.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    https://llamaindex.ai\n   </a>\n  </li>\n  <li>\n   Join our\n   <a href=\"https://discord.gg/dGcwcsnxhU\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Discord\n   </a>\n   or Follow our\n   <a href=\"https://twitter.com/llama_index\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Twitter\n   </a>\n  </li>\n </ul>\n <p>\n  Also, we\u2019re hiring!\n </p>\n <ul>\n  <li>\n   We\u2019re looking for founding engineers \u2014 experience in one or more of AI, data systems, and full-stack/front-end is nice to have but not a requirement.\n  </li>\n  <li>\n   If you\u2019re interested,\n   <a href=\"https://docs.google.com/forms/d/e/1FAIpQLScpSqZvTincCsspY5CyY_9gAGXnQfTS7HQvsgVQccncCJ7x5w/viewform?usp=sf_link\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    fill out our form here\n   </a>\n   .\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 12974, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b27e9bc5-89de-4fee-aaaf-8bd4a02091b6": {"__data__": {"id_": "b27e9bc5-89de-4fee-aaaf-8bd4a02091b6", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud.html", "file_name": "case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud.html", "file_type": "text/html", "file_size": 6199, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud.html", "file_name": "case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud.html", "file_type": "text/html", "file_size": 6199, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "db9176a943fc7ba0eba98c6db6369371f7cefea227456d55cebceb4ecb8918e3", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  The Challenge: Streamlining AI Development\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Scaleport AI specializes in transforming emerging AI technology into tangible business results. They possess deep expertise in deploying AI across key industries such as Legal, eCommerce, Real Estate, and Finance, providing tailored generative AI solutions for production applications.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Before adopting LlamaCloud and LlamaIndex, Scaleport AI faced several challenges:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Long development timelines for creating technical prototypes\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Difficulty in demonstrating tangible value to clients during the sales process\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Complex setup requirements for ingestion pipelines and data processing\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Suboptimal OCR performance, as existing solutions were not meeting the required accuracy and efficiency standards\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  The Solution: LlamaCloud's Comprehensive AI Development Platform\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Scaleport AI turned to LlamaCloud to address these challenges. LlamaCloud offered:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Centralized Knowledge Interface:\n   </strong>\n   Simplified data management and reduced time spent on data wrangling.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    LlamaParse:\n   </strong>\n   Outperformed existing OCR solutions, offering superior accuracy and efficiency.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Advanced Indexing and Retrieval:\n   </strong>\n   Enabled flexible integration with various data sources, enhancing data management and accessibility.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Rapid Prototyping and Easy Production Deployments:\n   </strong>\n   LlamaCloud provides an intuitive UI for rapid prototyping and a seamless transition from UI to code for full-scale development.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  The Results: Accelerated Development and Enhanced Client Engagement\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  LlamaCloud delivered remarkable improvements for\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"http://scaleport.ai/\" rel=\"noreferrer noopener\">\n   Scaleport.ai\n  </a>\n  :\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Accelerated Development Timelines:\n   </strong>\n   The team could build technical prototypes during the scoping phase, demonstrating tangible value instantly. This improved client engagement and sales outcomes.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Enhanced OCR Performance:\n   </strong>\n   LlamaParse outperformed GPT-4 vision on several OCR tasks, providing superior accuracy and efficiency.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Flexible Data Handling:\n   </strong>\n   LlamaCloud's integration with data sources and advanced indexing and retrieval capabilities allowed for quick delivery of high-quality results.\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Teemu Lahdenper\u00e4, CTO of Scaleport AI, shared his experience:\n </p>\n <blockquote class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  \"LlamaCloud has really sped up our development timelines - whether it's prototyping or production deployments. Before LlamaCloud, building even a simple application took forever because we needed to write our own abstractions for everything. When building an app for a client, a LOT of the work is building the ingestion pipelines. Doing that stuff with LlamaCloud and LlamaParse is remarkably simpler.\n </blockquote>\n <blockquote class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  This in turn has really helped our sales outcomes since we can show tangible value instantly. We've also seen great results with LlamaParse!\n </blockquote>\n <blockquote class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Specifically, we spent about 50-60% less development hours for one of our clients than we did for an equivalent application prior to LlamaCloud. The main time savings were around Llamaparse; not having to build a custom ingestion pipeline and having the indexing sorted. This helps our margins as well\u201d\n </blockquote>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Conclusion: A Game-Changer for AI Development\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  LlamaCloud has proven to be a game-changer for\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"http://scaleport.ai/\" rel=\"noreferrer noopener\">\n   Scaleport.ai\n  </a>\n  , enabling them to develop apps faster and enhance their overall AI application performance. This has accelerated their sales process! By leveraging LlamaCloud's comprehensive suite of tools,\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"http://scaleport.ai/\" rel=\"noreferrer noopener\">\n   Scaleport.ai\n  </a>\n  has positioned itself at the forefront of AI solution providers, ready to meet the evolving needs of their clients with speed, flexibility, and cutting-edge technology.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Want to see what LlamaCloud can do for you?\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://cloud.llamaindex.ai\" rel=\"noreferrer noopener\">\n   Sign up for LlamaCloud\n  </a>\n  and\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSdehUJJB4NIYfrPIKoFdF4j8kyfnLhMSH_qYJI_WGQbDWD25A/viewform\" rel=\"noreferrer noopener\">\n   get on the waitlist\n  </a>\n  for full access!\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 6195, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ef7ab3d9-ad12-44f8-81e5-a733fe37ed70": {"__data__": {"id_": "ef7ab3d9-ad12-44f8-81e5-a733fe37ed70", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex.html", "file_name": "case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex.html", "file_type": "text/html", "file_size": 6724, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex.html", "file_name": "case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex.html", "file_type": "text/html", "file_size": 6724, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "b4e685fd963f76e37461e933b0e9d47b2b3baad56c815a8ce8010dc08f7f4ffc", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  What is Lyzr?\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.lyzr.ai/\" rel=\"noreferrer noopener\">\n   Lyzr\n  </a>\n  is a full-stack agent framework that specializes in building fully autonomous AI agents for enterprises. Their focus is on achieving Organizational General Intelligence (OGI) by harnessing agent data. Lyzr offers pre-built agents like\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.lyzr.ai/jazon/\" rel=\"noreferrer noopener\">\n   Jazon\n  </a>\n  , an AI sales development representative, and\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.lyzr.ai/skott/\" rel=\"noreferrer noopener\">\n   Skott\n  </a>\n  , an AI content marketing agent, as well as a no-code builder for custom agent creation. Their platform enables organizations to build, deploy, and manage AI agents that can handle complex tasks and workflows autonomously.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  How does LlamaIndex help?\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  LlamaIndex plays a crucial role in Lyzr's technology stack:\n </p>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Context augmentation\n   </strong>\n   : LlamaIndex components supply essential context to Lyzr's agents, creating Retrieval-Augmented Generation (RAG) systems that enable them to perform focused and effective work.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Custom data access\n   </strong>\n   : LlamaIndex\u2019s data connectors are the preferred way for Lyzr agents to access customer-specific information. LlamaIndex\u2019s huge library of connectors means they can connect no matter where the customer stores their data.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Flexible retrieval\n   </strong>\n   : Lyzr uses LlamaIndex's customizable retrieval methods to optimize performance for different use cases. Lyzr\u2019s AutoRAG system determines the optimal retrieval model, chunk size, and other parameters based on input data and use case and passes those to LlamaIndex.\n  </li>\n </ol>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  What have the results been like?\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The integration of LlamaIndex into Lyzr's framework has contributed to significant growth and improved performance:\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   1. Rapid revenue growth\n  </strong>\n  : Lyzr's annual recurring revenue jumped from around $100,000 to about $1.5 million in less than 60 days.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   2. Enhanced agent accuracy\n  </strong>\n  : LlamaIndex's advanced capabilities and accurate RAG have led to highly accurate agents with very low error rates, providing a great alternative to OpenAI\u2019s Assistant API.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   3. Scalability\n  </strong>\n  : The flexibility provided by LlamaIndex has allowed Lyzr to sustain its growth and expand its agent offerings.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  What do customers think?\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Customer reception of Lyzr's LlamaIndex-powered agents has been overwhelmingly positive:\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   1. High adoption rate\n  </strong>\n  : 75% of Lyzr's customers use two or more AI agents including custom workflow agents, indicating strong adoption of Lyzr Agent Framework.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   2. Customer Persona\n  </strong>\n  : Lyzr\u2019s \u2018fully autonomous\u2019 AI agents seem to have captured customer\u2019s imagination with SaaS CTOs being the primary adopter of Lyzr AI Agents to automate their backend workflows in a more reliable, secure and predictable manner.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   3. Positive testimonials\n  </strong>\n  : Customers like SurePeople love Lyzr:\n </p>\n <blockquote class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  \"SurePeople is delighted to announce our partnership with Lyzr.ai, a key player in fortifying the scalability, security, and future-readiness of our AI infrastructure. Thanks to their versatile Agents, we're empowered to operate at the forefront of innovation, underpinned by a robust framework that bolsters our AI applications. In an ever-evolving landscape of artificial intelligence, Lyzr.ai's Agents ensure we remain at the cutting edge. Additionally, our collaboration has been enriched by their exceptionally skilled and cooperative team.\" \u2013 Niko Drakoulis, CEO of SurePeople\n </blockquote>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  What's next for Lyzr?\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Lyzr has ambitious plans for the future, building on their success with LlamaIndex, including new agents such as\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.lyzr.ai/kathy/\" rel=\"noreferrer noopener\">\n   Kathy\n  </a>\n  , an AI competitor analyst, and Diane, an AI HR agent, with several others in the pipeline. They\u2019re also developing a framework called Lyzr AgentMesh to enable interaction between these different AI agents, creating a cohesive AI-driven workforce.\n </p>\n <blockquote class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  \u201cWe are extremely thankful to Jerry and team for bringing LlamaIndex to the AI community. You guys have saved countless hours of tackling data retrieval challenges for us and many other builders in this space.\u201d - Siva Surendira, Founder, Lyzr AI.\n </blockquote>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  By continuing to use LlamaIndex as their RAG partner, Lyzr is well-positioned to expand its offerings and further establish itself as a leader in autonomous AI agent technology for enterprises.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Want to see what LlamaCloud can do for you?\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://cloud.llamaindex.ai\" rel=\"noreferrer noopener\">\n   Sign up for LlamaCloud\n  </a>\n  and\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSdehUJJB4NIYfrPIKoFdF4j8kyfnLhMSH_qYJI_WGQbDWD25A/viewform\" rel=\"noreferrer noopener\">\n   get on the waitlist\n  </a>\n  for full access!\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 6699, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eccad25c-8678-4c75-be21-1b172414e304": {"__data__": {"id_": "eccad25c-8678-4c75-be21-1b172414e304", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c.html", "file_name": "chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c.html", "file_type": "text/html", "file_size": 4649, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c.html", "file_name": "chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c.html", "file_type": "text/html", "file_size": 4649, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "b2274e0eca102b0efeac06d7883733f851ac5ca1ec6ea2778b012e15b5f3dbf7", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  It\u2019s official: as of today, ChatGPT\u2019s knowledge cutoff is 2 years old.\n </p>\n <blockquote class=\"twitter-tweet\">\n  <p dir=\"ltr\" lang=\"en\">\n   Happy 2nd birthday to ChatGPT's knowledge cutoff! \ud83c\udf82\n   <a href=\"https://t.co/O1cgRPSP3l\">\n    pic.twitter.com/O1cgRPSP3l\n   </a>\n  </p>\n  \u2014 Yi Ding -- prod/acc (@yi_ding)\n  <a href=\"https://twitter.com/yi_ding/status/1697589370222711081?ref_src=twsrc%5Etfw\">\n   September 1, 2023\n  </a>\n </blockquote>\n <h1>\n  Why doesn\u2019t OpenAI just update it?\n </h1>\n <p>\n  There are some fundamental reasons for this: training new LLMs is an expensive \u2014 at least tens of millions of dollars \u2014 and not guaranteed process. Cleaning new data sets for training is also expensive.\n </p>\n <h1>\n  What should I do if I\u2019m building an application that needs more recent data?\n </h1>\n <p>\n  You may be tempted to just send ChatGPT the entire wikipedia pages for 2022 and 2023:\n  <a href=\"https://en.wikipedia.org/wiki/2022\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://en.wikipedia.org/wiki/2022\n  </a>\n  You\u2019ll soon run into two limits: 1. there is a limit on the number of words you can send to a large language model (LLM). This is called the \u201ccontext window.\u201d 2. LLM APIs charge you by the word, so the more you send it, the more expensive your API calls become.\n </p>\n <p>\n  The standard technique is one called \u201cRetrieval Augmented Generation\u201d or RAG. What it is, boiled down very simply, is a process of searching for the right context, giving that context to the LLM, and then getting better results back.\n </p>\n <blockquote class=\"twitter-tweet\">\n  <p dir=\"ltr\" lang=\"en\">\n   What\u2019s Retrieval Augmented Generation? Search, Give, Get.\n   For those of us coming from a traditional software development background RAG can sound intimidating, but it really is a simple concept:\n   Search for the relevant data\n   Give the data to GPT\n   Get a better response\n   Of course,\u2026\n  </p>\n  \u2014 Yi Ding -- prod/acc (@yi_ding)\n  <a href=\"https://twitter.com/yi_ding/status/1684765549929332736?ref_src=twsrc%5Etfw\">\n   July 28, 2023\n  </a>\n </blockquote>\n <p>\n  At\n  <a href=\"https://llamaindex.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaIndex\n  </a>\n  we are the RAG experts, but there is a whole community of open source projects that are tackling this problem. We have integrated with over 20 open source vector databases and there are other open source tools like LangChain, Semantic Kernel, DSPy, Axilla and others (put your favorites in the comments!) that are attacking the problem in different ways.\n </p>\n <p>\n  Another technique is called fine tuning. Here, you essentially create a new custom model on top of an existing LLM. While LlamaIndex does support fine tuning, it often requires much more work and data:\n </p>\n <blockquote class=\"twitter-tweet\">\n  <p dir=\"ltr\" lang=\"en\">\n   We are big fans of fine tuning and custom models but knowing when to use RAG and when to use fine tuning, and how to use them in combination, is essential.\n   Watch this space!\n   <a href=\"https://t.co/vTpWauhj3C\">\n    https://t.co/vTpWauhj3C\n   </a>\n  </p>\n  \u2014 LlamaIndex \ud83e\udd99 (@llama_index)\n  <a href=\"https://twitter.com/llama_index/status/1692570383201812710?ref_src=twsrc%5Etfw\">\n   August 18, 2023\n  </a>\n </blockquote>\n <h1>\n  What if I don\u2019t need more recent data?\n </h1>\n <p>\n  That\u2019s totally OK! Not every application needs data that\u2019s more recent than 2021. Before LlamaIndex, I worked on an open source reading education tool, and phonics have definitely not changed in the last two years. If you\u2019re building something to write bedtime stories (\u2764\ufe0f Kidgeni\n  <a href=\"https://kidgeni.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://kidgeni.com/\n  </a>\n  ) or raps (check out TextFX!\n  <a href=\"https://textfx.withgoogle.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://textfx.withgoogle.com/\n  </a>\n  ) your application\n </p>\n <h1>\n  What if I just want to use ChatGPT with more recent information?\n </h1>\n <p>\n  There are a lot of chatbots that use Retrieval Augmented Generation currently. A few of the ones I\u2019ve personally tried are Metaphor\n  <a href=\"https://metaphor.systems/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://metaphor.systems/\n  </a>\n  , Perplexity\n  <a href=\"https://www.perplexity.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://www.perplexity.ai/\n  </a>\n  and Medisearch\n  <a href=\"https://medisearch.io/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://medisearch.io/\n  </a>\n  , and of course Google Bard and BingGPT.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4596, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43c11e43-a9e1-4e20-a578-35a234992fc0": {"__data__": {"id_": "43c11e43-a9e1-4e20-a578-35a234992fc0", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.html", "file_name": "combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.html", "file_type": "text/html", "file_size": 21543, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.html", "file_name": "combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.html", "file_type": "text/html", "file_size": 21543, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "ec3d95e1a3543b655893c0c726a060c2093d3c18cb7806149125297dbede1367", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h1>\n  Summary\n </h1>\n <p>\n  In this article, we showcase a powerful new query engine (\n  <code class=\"cw ow ox oy oz b\">\n   SQLAutoVectorQueryEngine\n  </code>\n  ) in LlamaIndex that can leverage both a SQL database as well as a vector store to fulfill complex natural language queries over a combination of structured and unstructured data. This query engine can leverage the expressivity of SQL over structured data, and join it with unstructured context from a vector database. We showcase this query engine on a few examples and show that it can handle queries that make use of both structured/unstructured data, or either.\n </p>\n <p>\n  Check out the full guide here:\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/query_engine/SQLAutoVectorQueryEngine.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://gpt-index.readthedocs.io/en/latest/examples/query_engine/SQLAutoVectorQueryEngine.html\n  </a>\n  .\n </p>\n <h1>\n  Context\n </h1>\n <p>\n  Data lakes in enterprises typically encompass both\n  <strong>\n   structured\n  </strong>\n  and\n  <strong>\n   unstructured\n  </strong>\n  data. Structured data is typically stored in a tabular format in SQL databases, organized into tables with predefined schemas and relationships between entities. On the other hand, unstructured data found in data lakes lacks a predefined structure and does not fit neatly into traditional databases. This type of data includes text documents, but also other multimodal formats such as audio recordings, videos, and more.\n </p>\n <p>\n  Large Language Models (LLMs) have the ability to extract insights from both structured and unstructured data. There have been some initial tooling and stacks that have emerged for tackling both types of data:\n </p>\n <ul>\n  <li>\n   <strong>\n    Text-to-SQL (Structured data):\n   </strong>\n   Given a collection of tabular schemas, we convert\n   natural language into a SQL statement which can then be executed against the database.\n  </li>\n  <li>\n   <strong>\n    Semantic Search with a Vector Database (Unstructured Data):\n   </strong>\n   Store unstructured documents along with their embeddings in a vector database (e.g. Pinecone, Chroma, Milvus, Weaviate, etc.). During query-time, fetch the relevant documents by embedding similarity, and then put into the LLM input prompt to synthesize a response.\n  </li>\n </ul>\n <p>\n  Each of these stacks solves particular use cases.\n </p>\n <h2>\n  Text-to-SQL Over Structured Data\n </h2>\n <p>\n  In the structured setting, SQL is an extremely expressive language for operating over tabular data \u2014 in the case of analytics, you can get aggregations, join information across multiple tables, sort by timestamp, and much more. Using the LLM to convert natural language to SQL can be thought as a program synthesis \u201ccheat code\u201d \u2014 just let the LLM compile to the right SQL query, and let the SQL engine on the database handle the rest!\n </p>\n <p>\n  <strong>\n   Use Case:\n  </strong>\n  Text-to-SQL queries are well-suited for analytics use cases where the answer can be found by executing a SQL statement. They are not suited for cases where you\u2019d need more detail than what is found in a structured table, or if you\u2019d need more sophisticated ways of determining relevance to the query beyond simple constructs like\n  <code class=\"cw ow ox oy oz b\">\n   WHERE\n  </code>\n  conditions.\n </p>\n <p>\n  <strong>\n   Example queries suited for Text-to-SQL:\n  </strong>\n </p>\n <ul>\n  <li>\n   \u201cWhat is the average population of cities in North America\u201d?\n  </li>\n  <li>\n   \u201cWhat are the largest cities and populations in each respective continent?\u201d\n  </li>\n </ul>\n <h2>\n  Semantic Search over Unstructured Data\n </h2>\n <p>\n  In the unstructured setting, the behavior for retrieval-augmented generation systems is to first perform retrieval and then synthesis. During retrieval, we first look up the most relevant documents to the query by embedding similarity. Some vector stores support being able to handle additional metadata filters for retrieval. We can choose to manually specify the set of required filters, or have the LLM \u201cinfer\u201d what the query string and metadata filters should be (see our\n  <a href=\"https://github.com/jerryjliu/llama_index/blob/main/docs/examples/vector_stores/pinecone_auto_retriever.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   auto-retrieval modules\n  </a>\n  in LlamaIndex or LangChain\u2019s\n  <a href=\"https://python.langchain.com/en/latest/modules/indexes/retrievers/examples/self_query.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   self-query module\n  </a>\n  ).\n </p>\n <p>\n  <strong>\n   Use Case:\n  </strong>\n  Retrieval Augmented Generation is well suited for queries where the answer can be obtained within some sections of unstructured text data. Most existing vector stores (e.g. Pinecone, Chroma) do not offer a SQL-like interface; hence they are less suited for queries that involve aggregations, joins, sums, etc.\n </p>\n <p>\n  <strong>\n   Example queries suited for Retrieval Augmented Generation\n  </strong>\n </p>\n <ul>\n  <li>\n   \u201cTell me about the historical museums in Berlin\u201d\n  </li>\n  <li>\n   \u201cWhat does Jordan ask from Nick on behalf of Gatsby?\u201d\n  </li>\n </ul>\n <h2>\n  Combining These Two Systems\n </h2>\n <p>\n  For some queries, we may want to make use of knowledge in\n  <strong>\n   both structured tables as well as vector databases/document stores\n  </strong>\n  in order to give the best answer to the query. Ideally this can give us the best of both worlds: the analytics capabilities over structured data, and semantic understanding over unstructured data.\n </p>\n <p>\n  Here\u2019s an example use case:\n </p>\n <ul>\n  <li>\n   You have access to a collection of articles about different cities, stored in a vector database\n  </li>\n  <li>\n   You also have access to a structured table containing statistics for each city.\n  </li>\n </ul>\n <p>\n  Given this data collection, let\u2019s take an example query: \u201cTell me about the arts and culture of the city with the highest population.\u201d\n </p>\n <p>\n  The \u201cproper\u201d way to answer this question is roughly as follows:\n </p>\n <ul>\n  <li>\n   Query the structured table for the city with the highest population.\n  </li>\n </ul>\n <pre><span class=\"ql nb gt oz b bf qm qn l qo qp\" id=\"3ee1\">SELECT city, population FROM city_stats ORDER BY population DESC LIMIT 1</span></pre>\n <ul>\n  <li>\n   Convert the original question into a more detailed question: \u201cTell me about the arts and culture of Tokyo.\u201d\n  </li>\n  <li>\n   Ask the new question over your vector database.\n  </li>\n  <li>\n   Use the original question + intermediate queries/responses to SQL db and vector db to synthesize the answer.\n  </li>\n </ul>\n <p>\n  Let\u2019s think about some of the high-level implications of such a sequence:\n </p>\n <ul>\n  <li>\n   Instead of doing embedding search (and optionally metadata filters) to retrieve relevant context, we want to somehow have a SQL query as a first \u201cretrieval\u201d step.\n  </li>\n  <li>\n   We want to make sure that we can somehow \u201cjoin\u201d the results from the SQL query with the context stored in the vector database. There is no existing language to \u201cjoin\u201d information between a SQL and vector database. We will have to implement this behavior ourselves.\n  </li>\n  <li>\n   Neither data source can answer this question on its own. The structured table only contains population information. The vector database contains city information but no easy way to query for the city with the maximum population.\n  </li>\n </ul>\n <h1>\n  A Query Engine to Combine Structured Analytics and Semantic Search\n </h1>\n <p>\n  We have created a brand-new query engine (\n  <code class=\"cw ow ox oy oz b\">\n   SQLAutoVectorQueryEngine\n  </code>\n  ) that can query, join, sequence, and combine both structured data from both your SQL database and unstructured data from your vector database in order to synthesize the final answer.\n </p>\n <p>\n  The\n  <code class=\"cw ow ox oy oz b\">\n   SQLAutoVectorQueryEngine\n  </code>\n  is initialized through passing in a SQL query engine (\n  <code class=\"cw ow ox oy oz b\">\n   GPTNLStructStoreQueryEngine\n  </code>\n  ) as well as a query engine that uses our vector store\n  <a href=\"https://github.com/jerryjliu/llama_index/blob/main/docs/examples/vector_stores/pinecone_auto_retriever.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   auto-retriever module\n  </a>\n  (\n  <code class=\"cw ow ox oy oz b\">\n   VectorIndexAutoRetriever\n  </code>\n  ). Both the SQL query engine and vector query engines are wrapped as \u201cTool\u201d objects containing a\n  <code class=\"cw ow ox oy oz b\">\n   name\n  </code>\n  and\n  <code class=\"cw ow ox oy oz b\">\n   description\n  </code>\n  field.\n </p>\n <blockquote>\n  <p class=\"ny nz qt oa b ob pa od oe of pb oh oi oj pc ol om on pd op oq or pe ot ou ov gm bj\" id=\"ccd6\">\n   <em class=\"gt\">\n    Reminder: the\n   </em>\n   <code class=\"cw ow ox oy oz b\">\n    <em class=\"gt\">\n     VectorIndexAutoRetriever\n    </em>\n   </code>\n   <em class=\"gt\">\n    takes in a natural language query as input. Given some knowledge of the metadata schema of the vector database, the auto retriever first\n   </em>\n   infers\n   <em class=\"gt\">\n    the other necessary query parameters to pass in (e.g. top-k value, and metadata filters), and executes a query against the vector database with all the query parameters.\n   </em>\n  </p>\n </blockquote>\n <figure>\n  <figcaption class=\"rd fe re qu qv rf rg be b bf z dt\">\n   Diagram of the flow for SQLAutoVectorQueryEngine\n  </figcaption>\n </figure>\n <p>\n  During query-time, we run the following steps:\n </p>\n <ol>\n  <li>\n   A selector prompt (similarly used in our\n   <code class=\"cw ow ox oy oz b\">\n    <a href=\"https://gpt-index.readthedocs.io/en/latest/reference/query/query_engines/router_query_engine.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n     RouterQueryEngine\n    </a>\n   </code>\n   , see\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/query_engine/RouterQueryEngine.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    guide\n   </a>\n   ) first chooses whether we should query the SQL database or the vector database. If it chooses to use the vector query engine, then the rest of the function execution is the same as querying the\n   <code class=\"cw ow ox oy oz b\">\n    RetrieverQueryEngine\n   </code>\n   with\n   <code class=\"cw ow ox oy oz b\">\n    VectorIndexAutoRetriever\n   </code>\n   .\n  </li>\n  <li>\n   If it chooses to query the SQL database, it will execute a text-to-SQL query operation against the database, and (optionally) synthesize a natural language output.\n  </li>\n  <li>\n   A\n   <strong>\n    query transformation\n   </strong>\n   is run, to convert the original question into a more detailed question given the results from the SQL query. For instance if the original question is \u201cTell me about the arts and culture of the city with the highest population.\u201d, and the SQL query returns Tokyo as the city with the highest population, then the new query is \u201cTell me about the arts and culture of Tokyo.\u201d The one exception is if the SQL query itself is enough to answer the original question; if it is, then function execution returns with the SQL query as the response.\n  </li>\n  <li>\n   The new query is then run through through the vector store query engine, which performs retrieval from the vector store and then LLM response synthesis. We enforce using a\n   <code class=\"cw ow ox oy oz b\">\n    VectorIndexAutoRetriever\n   </code>\n   module. This allows us to automatically infer the right query parameters (query string, top k, metadata filters), given the result of the SQL query. For instance, with the example above, we may infer the query to be something like\n   <code class=\"cw ow ox oy oz b\">\n    query_str=\"arts and culture\"\n   </code>\n   and\n   <code class=\"cw ow ox oy oz b\">\n    filters={\"title\": \"Tokyo\"}\n   </code>\n   .\n  </li>\n  <li>\n   The original question, SQL query, SQL response, vector store query, and vector store response are combined into a prompt to synthesize the final answer.\n  </li>\n </ol>\n <p>\n  Taking a step back, here are some general comments about this approach:\n </p>\n <ul>\n  <li>\n   Using our auto-retrieval module is our way of\n   <em class=\"qt\">\n    simulating\n   </em>\n   a join between the SQL database and vector database. We effectively use the results from our SQL query to determine the parameters to query the vector database with.\n  </li>\n  <li>\n   This also implies that there doesn\u2019t need to be an explicit mapping between the items in the SQL database and the metadata in the vector database, since we can rely on the LLM being able come up with the right query for different items. It would be interesting to model explicit relationships between structured tables and document store metadata though; that way we don\u2019t need to spend an extra LLM call in the auto-retrieval step inferring the right metadata filters.\n  </li>\n </ul>\n <h1>\n  Experiments\n </h1>\n <p>\n  So how well does this work? It works surprisingly well across a broad range of queries, from queries that can leverage both structured data and unstructured data to queries that are specific to a structured data collection or unstructured data collection.\n </p>\n <h2>\n  Setup\n </h2>\n <p>\n  Our experiment setup is very simple. We have a SQL table called\n  <code class=\"cw ow ox oy oz b\">\n   city_stats\n  </code>\n  which contains the city, population, and country of three different cities: Toronto, Tokyo, and Berlin.\n </p>\n <p>\n  We also use a Pinecone index to store Wikipedia articles corresponding to the three cities. Each article is chunked up and stored as a separate \u201cNode\u201d object; each chunk also contains a\n  <code class=\"cw ow ox oy oz b\">\n   title\n  </code>\n  metadata attribute containing the city name.\n </p>\n <p>\n  We then derive the\n  <code class=\"cw ow ox oy oz b\">\n   VectorIndexAutoRetriever\n  </code>\n  and\n  <code class=\"cw ow ox oy oz b\">\n   RetrieverQueryEngine\n  </code>\n  from the Pinecone vector index.\n </p>\n <pre><span class=\"ql nb gt oz b bf qm qn l qo qp\" id=\"ceda\"><span class=\"hljs-keyword\">from</span> llama_index.indices.vector_store.retrievers <span class=\"hljs-keyword\">import</span> VectorIndexAutoRetriever\n<span class=\"hljs-keyword\">from</span> llama_index.vector_stores.types <span class=\"hljs-keyword\">import</span> MetadataInfo, VectorStoreInfo\n<span class=\"hljs-keyword\">from</span> llama_index.query_engine.retriever_query_engine <span class=\"hljs-keyword\">import</span> RetrieverQueryEngine\n\n\nvector_store_info = VectorStoreInfo(\n    content_info=<span class=\"hljs-string\">'articles about different cities'</span>,\n    metadata_info=[\n        MetadataInfo(\n            name=<span class=\"hljs-string\">'city'</span>, \n            <span class=\"hljs-built_in\">type</span>=<span class=\"hljs-string\">'str'</span>, \n            description=<span class=\"hljs-string\">'The name of the city'</span>),\n    ]\n)\nvector_auto_retriever = VectorIndexAutoRetriever(vector_index, vector_store_info=vector_store_info)\n\nretriever_query_engine = RetrieverQueryEngine.from_args(\n    vector_auto_retriever, service_context=service_context\n)</span></pre>\n <p>\n  You can also get the SQL query engine as follows\n </p>\n <pre><span class=\"ql nb gt oz b bf qm qn l qo qp\" id=\"6fc8\">sql_query_engine = sql_index.as_query_engine()</span></pre>\n <p>\n  Both the SQL query engine and vector query engine can be wrapped as\n  <code class=\"cw ow ox oy oz b\">\n   QueryEngineTool\n  </code>\n  objects.\n </p>\n <pre><span class=\"ql nb gt oz b bf qm qn l qo qp\" id=\"0ec3\">sql_tool = QueryEngineTool.from_defaults(\n    query_engine=sql_query_engine,\n    description=(\n        'Useful for translating a natural language query into a SQL query over a table containing: '\n        'city_stats, containing the population/country of each city'\n    )\n)\nvector_tool = QueryEngineTool.from_defaults(\n    query_engine=query_engine,\n    description=f'Useful for answering semantic questions about different cities',\n)</span></pre>\n <p>\n  Finally, we can define our\n  <code class=\"cw ow ox oy oz b\">\n   SQLAutoVectorQueryEngine\n  </code>\n </p>\n <pre><span class=\"ql nb gt oz b bf qm qn l qo qp\" id=\"4360\">query_engine = SQLAutoVectorQueryEngine(\n    sql_tool,\n    vector_tool,\n    service_context=service_context\n)</span></pre>\n <h2>\n  Results\n </h2>\n <p>\n  We run some example queries.\n </p>\n <p>\n  <strong>\n   Query 1\n  </strong>\n </p>\n <pre><span class=\"ql nb gt oz b bf qm qn l qo qp\" id=\"33a8\">query_engine.query(\n  'Tell me about the arts and culture of the city with the highest population'\n)</span></pre>\n <p>\n  Intermediate steps:\n </p>\n <p>\n  Final Response:\n </p>\n <pre><span class=\"ql nb gt oz b bf qm qn l qo qp\" id=\"4e6d\">Tokyo is the city with the highest population, with 13.96 million people. It is a vibrant city with a rich culture and a wide variety of art forms. From traditional Japanese art such as calligraphy and woodblock prints to modern art galleries and museums, Tokyo has something for everyone. There are also many festivals and events throughout the year that celebrate the city's culture and art. Some popular festivals and events in Tokyo include the Sann\u014d Festival at Hie Shrine, the Sanja Festival at Asakusa Shrine, and the biennial Kanda Festivals. These events often feature parades with elaborately decorated floats and thousands of people. Additionally, an enormous fireworks display over the Sumida River takes place annually on the last Saturday of July, attracting over a million viewers. During spring, when cherry blossoms bloom, many residents gather in Ueno Park, Inokashira Park, and the Shinjuku Gyoen National Garden for picnics under the blossoms.</span></pre>\n <p>\n  This query runs through the full flow of the\n  <code class=\"cw ow ox oy oz b\">\n   SQLAutoVectorQueryEngine\n  </code>\n  . It first queries the SQL database for the city with the highest population (\u201cTokyo\u201d), and then queries the vector database with the new query. The results are combined into a final response.\n </p>\n <p>\n  <strong>\n   Query 2\n  </strong>\n </p>\n <pre><span class=\"ql nb gt oz b bf qm qn l qo qp\" id=\"8832\">query_engine.query(\"Tell me about the history of Berlin\")</span></pre>\n <p>\n  Intermediate Steps:\n </p>\n <p>\n  Final Response:\n </p>\n <pre><span class=\"ql nb gt oz b bf qm qn l qo qp\" id=\"82a0\">Berlin's history dates back to the early 13th century when it was founded as a small settlement. In 1618, the Margraviate of Brandenburg entered into a personal union with the Duchy of Prussia, and in 1701, they formed the Kingdom of Prussia with Berlin as its capital. The city grew and merged with neighboring cities, becoming a center of the Enlightenment under the rule of Frederick the Great in the 18th century.\n\nThe Industrial Revolution in the 19th century transformed Berlin, expanding its economy, population, and infrastructure. In 1871, it became the capital of the newly founded German Empire. The early 20th century saw Berlin as a hub for the German Expressionist movement and a major world capital known for its contributions to science, technology, arts, and other fields.\n\nIn 1933, Adolf Hitler and the Nazi Party came to power, leading to a decline in Berlin's Jewish community and the city's involvement in World War II. After the war, Berlin was divided into East and West Berlin, with the former under Soviet control and the latter under the control of the United States, United Kingdom, and France. The Berlin Wall was built in 1961, physically and ideologically dividing the city until its fall in 1989. Following the reunification of Germany in 1990, Berlin once again became the capital of a unified Germany and has since continued to grow and develop as a major global city.</span></pre>\n <p>\n  This query only requires the vector database and not the SQL database. The initial selector correctly identifies that we should just query the vector database and return the result.\n </p>\n <p>\n  <strong>\n   Query 3\n  </strong>\n </p>\n <pre><span class=\"ql nb gt oz b bf qm qn l qo qp\" id=\"da77\">query_engine.query('Can you give me the country corresponding to each city?')</span></pre>\n <p>\n  Intermediate Steps\n </p>\n <p>\n  Final Response\n </p>\n <pre><span class=\"ql nb gt oz b bf qm qn l qo qp\" id=\"27de\"> Toronto is in Canada, Tokyo is in Japan, and Berlin is in Germany.</span></pre>\n <p>\n  This query can be answered by just querying the SQL database, it does not need additional information from the vector database. The query transform step correctly identifies \u201cNone\u201d as the followup question, indicating that the original question has been answered.\n </p>\n <h1>\n  Conclusion\n </h1>\n <p>\n  So far, the stacks around LLMs + unstructured data and LLMs + structured data have largely been separate. We\u2019re excited about how combining LLMs on top of both structured and unstructured data can unlock new retrieval/query capabilities in novel and interesting ways!\n </p>\n <p>\n  We\u2019d love for you to try out the\n  <code class=\"cw ow ox oy oz b\">\n   SQLAutoVectorQueryEngine\n  </code>\n  and let us know what you think.\n </p>\n <p>\n  The full notebook walkthrough can be found\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/query_engine/SQLAutoVectorQueryEngine.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   in this guide\n  </a>\n  (\n  <a href=\"https://github.com/jerryjliu/llama_index/blob/main/docs/examples/query_engine/SQLAutoVectorQueryEngine.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   associated notebook\n  </a>\n  ).\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 21445, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "093dd541-ece9-485b-b41d-4ae80b733e86": {"__data__": {"id_": "093dd541-ece9-485b-b41d-4ae80b733e86", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191.html", "file_name": "create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191.html", "file_type": "text/html", "file_size": 4343, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191.html", "file_name": "create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191.html", "file_type": "text/html", "file_size": 4343, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "ec76452fef823a094db37eba231c494f0701dc89847801fce1bfae82930a6be7", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Introducing\n  <code class=\"cw ny nz oa ob b\">\n   <a href=\"https://www.npmjs.com/package/create-llama\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    create-llama\n   </a>\n  </code>\n  , the easiest way to get started with LlamaIndex!\n </p>\n <p>\n  <em class=\"od\">\n   Update 2023\u201311\u201320: we now have a\n  </em>\n  <a href=\"/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <em class=\"od\">\n    guide to deploying your create-llama apps\n   </em>\n  </a>\n  <em class=\"od\">\n   !\n  </em>\n </p>\n <p>\n  Want to use the power of LlamaIndex to load, index and chat with your data using LLMs like GPT-4? It just got a lot easier! We\u2019ve created a simple to use command-line tool that will generate a full-stack app just for you \u2014 just bring your own data! To get started, run:\n </p>\n <pre><span class=\"oq or gt ob b bf os ot l ou ov\" id=\"192c\">npx create-llama</span></pre>\n <p>\n  The app will then ask you a series of questions about what kind of app you want. You\u2019ll need to supply your own\n  <a href=\"https://platform.openai.com/api-keys\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   OpenAI API key\n  </a>\n  (or you can customize it to use a different LLM), and make a few decisions.\n </p>\n <h1>\n  How does it get my data?\n </h1>\n <p>\n  The generated app has a\n  <code class=\"cw ny nz oa ob b\">\n   data\n  </code>\n  folder where you can put as many files as you want; the app will automatically index them at build time and after that you can quickly chat with them. If you\u2019re using LlamaIndex.TS as the back-end (see below), you\u2019ll be able to ingest PDF, text, CSV, Markdown, Word and HTML files. If you\u2019re using the Python backend, you can read even more types, including audio and video files!\n </p>\n <h1>\n  Technical details\n </h1>\n <p>\n  The front-end it generates is a Next.js application, with your choice of\n  <a href=\"https://ui.shadcn.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   shadcn/ui\n  </a>\n  or vanilla HTML and CSS for styling.\n </p>\n <p>\n  For the back-end, you have 3 options:\n </p>\n <ul>\n  <li>\n   <strong>\n    Next.js\n   </strong>\n   : if you select this option, you\u2019ll have a full stack Next.js application that you can deploy to a host like\n   <a href=\"https://vercel.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Vercel\n   </a>\n   in just a few clicks. This uses\n   <a href=\"https://ts.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaIndex.TS\n   </a>\n   , our TypeScript library.\n  </li>\n  <li>\n   <strong>\n    Express\n   </strong>\n   : if you want a more traditional Node.js application you can generate an Express backend. This also uses LlamaIndex.TS.\n  </li>\n  <li>\n   <strong>\n    Python FastAPI\n   </strong>\n   : if you select this option you\u2019ll get a backend powered by the\n   <a href=\"https://pypi.org/project/llama-index/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    llama-index python package\n   </a>\n   , which you can deploy to a service like\n   <a href=\"https://render.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Render\n   </a>\n   or\n   <a href=\"https://fly.io/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    fly.io\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  There are a couple of other questions you\u2019ll be asked:\n </p>\n <ul>\n  <li>\n   Streaming or non-streaming: if you\u2019re not sure, you\u2019ll probably want a streaming backend.\n  </li>\n  <li>\n   <code class=\"cw ny nz oa ob b\">\n    SimpleChatEngine\n   </code>\n   or\n   <code class=\"cw ny nz oa ob b\">\n    ContextChatEngine\n   </code>\n   : the ContextChatEngine is the one that uses your data. If you just want to chat with GPT, you can use the\n   <code class=\"cw ny nz oa ob b\">\n    SimpleChatEngine\n   </code>\n   .\n  </li>\n </ul>\n <h1>\n  Go forth and customize!\n </h1>\n <p>\n  Once you\u2019ve got your app up and running, you can customize it to your heart\u2019s content! By default, for cost reasons, the app will use GPT-3.5-Turbo. If you\u2019d like to use GPT-4 you can configure that by modifying the file\n  <code class=\"cw ny nz oa ob b\">\n   app/api/chat/llamaindex-stream.ts\n  </code>\n  (in the Next.js backend) or you can configure it to use a different LLM entirely! LlamaIndex has integrations with dozens of LLMs, both APIs and local.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4310, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ebeacf6-ead5-42b9-8080-9a1316efd4d6": {"__data__": {"id_": "1ebeacf6-ead5-42b9-8080-9a1316efd4d6", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/customizing-property-graph-index-in-llamaindex.html", "file_name": "customizing-property-graph-index-in-llamaindex.html", "file_type": "text/html", "file_size": 30133, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/customizing-property-graph-index-in-llamaindex.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/customizing-property-graph-index-in-llamaindex.html", "file_name": "customizing-property-graph-index-in-llamaindex.html", "file_type": "text/html", "file_size": 30133, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "0390773b6bbfa862d9a52bfccbfe7aa9048d900a79b53dd73cd7a60e346e007c", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <paragraphxlarge class=\"Text_text__zPO0D Text_text-size-28__dLWMG\">\n  Learn how to implement entity deduplication and custom retrieval methods to increase GraphRAG accuracy\n </paragraphxlarge>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <em>\n   This is a guest post by Neo4J\n  </em>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms\" rel=\"noreferrer noopener\">\n   property graph index\n  </a>\n  is an excellent addition to LlamaIndex and an upgrade from the previous knowledge graph integration. First, the data representation is slightly different. In the previous integration, the graph was represented with triples, but now we have a proper property graph integration where nodes have labels and optionally node properties.\n </p>\n <figure>\n  <figcaption>\n   Example of a property graph model.\n  </figcaption>\n </figure>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Each node is assigned a label indicating its type, such as Person, Organization, Project, or Department. Nodes and relationships may also store node properties for other relevant details, such as the date of birth or project start and end date, as shown in this example.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Second, the property graph index is designed to be modular, so you can use one or multiple (custom) knowledge graph constructors as well as retrievers, making it an incredible tool to build your first knowledge graph or customize the implementation for your specific needs.\n </p>\n <figure>\n  <figcaption>\n   Property graph workflow\n  </figcaption>\n </figure>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The image illustrates the property graph integration within the LlamaIndex , beginning with documents being passed to graph constructors. These constructors are modular components responsible for extracting structured information, which is then stored in a knowledge graph. The graph can be built using various or custom modules, highlighting the system\u2019s flexibility to adapt to different data sources or extraction needs.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Graph retrievers then access the knowledge graph to retrieve data. This stage is also modular, allowing for the use of multiple retrievers or custom solutions designed to query specific types of data or relationships within the graph. Finally, the retrieved data is used by a LLM to generate an answer, representing the output or the insight derived from the process. This flow emphasizes a highly adaptable and scalable system where each component can be independently modified or replaced to enhance the overall functionality or to tailor it to specific requirements.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In this blog post you will learn how to:\n </p>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Construct a knowledge graph using a schema-guided extraction\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Perform entity deduplication using a combination of text embedding and word similarity techniques\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Design a custom graph retriever\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Finally, you will implement a question answering flow using the custom retriever\n  </li>\n </ol>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The code is available on\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/tomasonjo/blogs/blob/master/llm/llama_index_neo4j_custom_retriever.ipynb\" rel=\"noreferrer noopener\">\n   GitHub\n  </a>\n  .\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Environment setup\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In this blog post, we will use Neo4j as the underlying graph store. The easiest way is to get started is to a free instance on\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://neo4j.com/cloud/platform/aura-graph-database/\" rel=\"noreferrer noopener\">\n   Neo4j Aura\n  </a>\n  , which offers cloud instances of the Neo4j database. Alternatively, you can also set up a local instance of the Neo4j database by downloading the\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://neo4j.com/download/\" rel=\"noreferrer noopener\">\n   Neo4j Desktop\n  </a>\n  application and creating a local database instance.\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index.graph_stores.neo4j <span class=\"hljs-keyword\">import</span> Neo4jPGStore\n\nusername=<span class=\"hljs-string\">\"neo4j\"</span>\npassword=<span class=\"hljs-string\">\"stump-inlet-student\"</span>\nurl=<span class=\"hljs-string\">\"bolt://52.201.215.224:7687\"</span>\n\ngraph_store = Neo4jPGStore(\n    username=username,\n    password=password,\n    url=url,\n)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Additionally, you will require a working OpenAI API key.\n </p>\n <pre><code><span class=\"hljs-keyword\">import</span> os\n\nos.environ[<span class=\"hljs-string\">\"OPENAI_API_KEY\"</span>] = <span class=\"hljs-string\">\"sk-\"</span></code></pre>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Dataset\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In this blog post, we will use a\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.diffbot.com/solutions/news-monitoring/\" rel=\"noreferrer noopener\">\n   sample news article dataset fetched from Diffbot\n  </a>\n  , which I\u2019ve made available on\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/tomasonjo/blog-datasets/blob/main/news_articles.csv\" rel=\"noreferrer noopener\">\n   GitHub for easier access\n  </a>\n  .\n </p>\n <figure>\n  <figcaption>\n   Sample records from the dataset.\n  </figcaption>\n </figure>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Since the property graph index operates with documents, we have to wrap the text from the news as LlamaIndex documents.\n </p>\n <pre><code><span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\">as</span> pd\n<span class=\"hljs-keyword\">from</span> llama_index.core <span class=\"hljs-keyword\">import</span> Document\n\nnews = pd.read_csv(\n  <span class=\"hljs-string\">\"https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/news_articles.csv\"</span>)\ndocuments = [Document(text=<span class=\"hljs-string\">f\"<span class=\"hljs-subst\">{row[<span class=\"hljs-string\">'title'</span>]}</span>: <span class=\"hljs-subst\">{row[<span class=\"hljs-string\">'text'</span>]}</span>\"</span>) <span class=\"hljs-keyword\">for</span> i, row <span class=\"hljs-keyword\">in</span> news.iterrows()]</code></pre>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Graph construction\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  As mentioned, LlamaIndex provides multiple\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/module_guides/indexing/lpg_index_guide/#construction\" rel=\"noreferrer noopener\">\n   out-of-the-box graph constructors\n  </a>\n  . In this example, we will use the\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/module_guides/indexing/lpg_index_guide/#schemallmpathextractor\" rel=\"noreferrer noopener\">\n   SchemaLLMPathExtractor\n  </a>\n  , which allows us to define the schema of the graph structure we want to extract from documents.\n </p>\n <figure>\n  <figcaption>\n   Schema-guided graph structure extraction.\n  </figcaption>\n </figure>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We begin by defining the types of nodes and relationships we want the LLM to extract.\n </p>\n <pre><code>entities = <span class=\"hljs-type\">Literal</span>[<span class=\"hljs-string\">\"PERSON\"</span>, <span class=\"hljs-string\">\"LOCATION\"</span>, <span class=\"hljs-string\">\"ORGANIZATION\"</span>, <span class=\"hljs-string\">\"PRODUCT\"</span>, <span class=\"hljs-string\">\"EVENT\"</span>]\nrelations = <span class=\"hljs-type\">Literal</span>[\n    <span class=\"hljs-string\">\"SUPPLIER_OF\"</span>,\n    <span class=\"hljs-string\">\"COMPETITOR\"</span>,\n    <span class=\"hljs-string\">\"PARTNERSHIP\"</span>,\n    <span class=\"hljs-string\">\"ACQUISITION\"</span>,\n    <span class=\"hljs-string\">\"WORKS_AT\"</span>,\n    <span class=\"hljs-string\">\"SUBSIDIARY\"</span>,\n    <span class=\"hljs-string\">\"BOARD_MEMBER\"</span>,\n    <span class=\"hljs-string\">\"CEO\"</span>,\n    <span class=\"hljs-string\">\"PROVIDES\"</span>,\n    <span class=\"hljs-string\">\"HAS_EVENT\"</span>,\n    <span class=\"hljs-string\">\"IN_LOCATION\"</span>,\n]</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  As you can see, we are focusing our graph extraction around people and organizations. Next, we will specify the relationships associated with each node label.\n </p>\n <pre><code><span class=\"hljs-comment\"># define which entities can have which relations</span>\nvalidation_schema = {\n    <span class=\"hljs-string\">\"Person\"</span>: [<span class=\"hljs-string\">\"WORKS_AT\"</span>, <span class=\"hljs-string\">\"BOARD_MEMBER\"</span>, <span class=\"hljs-string\">\"CEO\"</span>, <span class=\"hljs-string\">\"HAS_EVENT\"</span>],\n    <span class=\"hljs-string\">\"Organization\"</span>: [\n        <span class=\"hljs-string\">\"SUPPLIER_OF\"</span>,\n        <span class=\"hljs-string\">\"COMPETITOR\"</span>,\n        <span class=\"hljs-string\">\"PARTNERSHIP\"</span>,\n        <span class=\"hljs-string\">\"ACQUISITION\"</span>,\n        <span class=\"hljs-string\">\"WORKS_AT\"</span>,\n        <span class=\"hljs-string\">\"SUBSIDIARY\"</span>,\n        <span class=\"hljs-string\">\"BOARD_MEMBER\"</span>,\n        <span class=\"hljs-string\">\"CEO\"</span>,\n        <span class=\"hljs-string\">\"PROVIDES\"</span>,\n        <span class=\"hljs-string\">\"HAS_EVENT\"</span>,\n        <span class=\"hljs-string\">\"IN_LOCATION\"</span>,\n    ],\n    <span class=\"hljs-string\">\"Product\"</span>: [<span class=\"hljs-string\">\"PROVIDES\"</span>],\n    <span class=\"hljs-string\">\"Event\"</span>: [<span class=\"hljs-string\">\"HAS_EVENT\"</span>, <span class=\"hljs-string\">\"IN_LOCATION\"</span>],\n    <span class=\"hljs-string\">\"Location\"</span>: [<span class=\"hljs-string\">\"HAPPENED_AT\"</span>, <span class=\"hljs-string\">\"IN_LOCATION\"</span>],\n}</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  For example, a person can have the following relationships:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   WORKS_AT\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   BOARD_MEMBER\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   CEO\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   HAS_EVENT\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The schema is quite specific except for the EVENT node label, which is slightly more ambiguous and allows the LLM to capture various types of information.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Now that we have defined the graph schema, we can input it into the\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   SchemaLLMPathExtractor\n  </code>\n  and use it to construct a graph.\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index.core <span class=\"hljs-keyword\">import</span> PropertyGraphIndex\n\nkg_extractor = SchemaLLMPathExtractor(\n    llm=llm,\n    possible_entities=entities,\n    possible_relations=relations,\n    kg_validation_schema=validation_schema,\n    <span class=\"hljs-comment\"># if false, allows for values outside of the schema</span>\n    <span class=\"hljs-comment\"># useful for using the schema as a suggestion</span>\n    strict=<span class=\"hljs-literal\">True</span>,\n)\n\nNUMBER_OF_ARTICLES = <span class=\"hljs-number\">250</span>\n\nindex = PropertyGraphIndex.from_documents(\n    documents[:NUMBER_OF_ARTICLES],\n    kg_extractors=[kg_extractor],\n    llm=llm,\n    embed_model=embed_model,\n    property_graph_store=graph_store,\n    show_progress=<span class=\"hljs-literal\">True</span>,\n)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  This code extracts graph information from 250 news articles, but you can adjust the number how you see fit. There are 2500 articles in total.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <em>\n   Note that extracting 250 articles takes about 7 minutes with GPT-4o. However, you can accelerate the process by employing parallelization through the\n  </em>\n  <em>\n   <code class=\"SanityPortableText_inlineCode__cI85z\">\n    num_workers\n   </code>\n  </em>\n  <em>\n   parameter.\n  </em>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We can visualize a small subgraph to inspect what was stored.\n </p>\n <figure>\n  <figcaption>\n   Text chunks are blue, while entity nodes are all the rest.\n  </figcaption>\n </figure>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The constructed graph contains both text chunks (blue), which contain text and embeddings. If an entity was mentioned in the text chunk, there is a\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   MENTIONS\n  </code>\n  relationships between the text chunk and entity. Additionally, entities can have relationships to other entities.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Entity deduplication\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Entity deduplication or disambiguation is an important but often overlooked step in graph construction. Essentially, it is a cleaning step where you try to match multiple nodes that represent a single entity and merge them together into a single node for better graph structural integrity.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  For example, in our constructed graph I could find some examples that could be merged.\n </p>\n <figure>\n  <figcaption>\n   Potential entity duplicates.\n  </figcaption>\n </figure>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We will use a combination of text embedding similarity and word distance to find potential duplicates. We start by defining the vector index on our entities in the graph.\n </p>\n <pre><code>graph_store.structured_query(<span class=\"hljs-string\">\"\"\"\nCREATE VECTOR INDEX entity IF NOT EXISTS\nFOR (m:`__Entity__`)\nON m.embedding\nOPTIONS {indexConfig: {\n `vector.dimensions`: 1536,\n `vector.similarity_function`: 'cosine'\n}}\n\"\"\"</span>)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The next Cypher query finds duplicates and is quite involved and I took me, Michael Hunger, and Eric Monk a couple of hours to perfect it.\n </p>\n <pre><code>similarity_threshold = <span class=\"hljs-number\">0.9</span>\nword_edit_distance = <span class=\"hljs-number\">5</span>\ndata = graph_store.structured_query(<span class=\"hljs-string\">\"\"\"\nMATCH (e:__Entity__)\nCALL {\n  WITH e\n  CALL db.index.vector.queryNodes('entity', 10, e.embedding)\n  YIELD node, score\n  WITH node, score\n  WHERE score &gt; toFLoat($cutoff)\n      AND (toLower(node.name) CONTAINS toLower(e.name) OR toLower(e.name) CONTAINS toLower(node.name)\n           OR apoc.text.distance(toLower(node.name), toLower(e.name)) &lt; $distance)\n      AND labels(e) = labels(node)\n  WITH node, score\n  ORDER BY node.name\n  RETURN collect(node) AS nodes\n}\nWITH distinct nodes\nWHERE size(nodes) &gt; 1\nWITH collect([n in nodes | n.name]) AS results\nUNWIND range(0, size(results)-1, 1) as index\nWITH results, index, results[index] as result\nWITH apoc.coll.sort(reduce(acc = result, index2 IN range(0, size(results)-1, 1) |\n        CASE WHEN index &lt;&gt; index2 AND\n            size(apoc.coll.intersection(acc, results[index2])) &gt; 0\n            THEN apoc.coll.union(acc, results[index2])\n            ELSE acc\n        END\n)) as combinedResult\nWITH distinct(combinedResult) as combinedResult\n// extra filtering\nWITH collect(combinedResult) as allCombinedResults\nUNWIND range(0, size(allCombinedResults)-1, 1) as combinedResultIndex\nWITH allCombinedResults[combinedResultIndex] as combinedResult, combinedResultIndex, allCombinedResults\nWHERE NOT any(x IN range(0,size(allCombinedResults)-1,1) \n    WHERE x &lt;&gt; combinedResultIndex\n    AND apoc.coll.containsAll(allCombinedResults[x], combinedResult)\n)\nRETURN combinedResult  \n\"\"\"</span>, param_map={<span class=\"hljs-string\">'cutoff'</span>: similarity_threshold, <span class=\"hljs-string\">'distance'</span>: word_edit_distance})\n<span class=\"hljs-keyword\">for</span> row <span class=\"hljs-keyword\">in</span> data:\n    <span class=\"hljs-built_in\">print</span>(row)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Without getting into too many details, we use a combination of text embeddings and word distance to find potential duplicates in our graph. You can tune\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   similarity_threshold\n  </code>\n  and\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   word_distance\n  </code>\n  to find the best combination that detects as many duplicates without too much false positives. Unfortunately, entity disambiguation is a hard problem and there are no perfect solutions. With this approach, we get quite good results, but there are some false positives in there as well:\n </p>\n <pre><code>[<span class=\"hljs-string\">'1963 AFL Draft'</span>, <span class=\"hljs-string\">'1963 NFL Draft'</span>]\n[<span class=\"hljs-string\">'June 14, 2023'</span>, <span class=\"hljs-string\">'June 15 2023'</span>]\n[<span class=\"hljs-string\">'BTC Halving'</span>, <span class=\"hljs-string\">'BTC Halving 2016'</span>, <span class=\"hljs-string\">'BTC Halving 2020'</span>, <span class=\"hljs-string\">'BTC Halving 2024'</span>, <span class=\"hljs-string\">'Bitcoin Halving'</span>, <span class=\"hljs-string\">'Bitcoin Halving 2024'</span>]</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  It is up to you to tweak the dials, and maybe add some manual exceptions before merging duplicate nodes.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Implementing a custom retriever\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Great, we have constructed a knowledge graph based on the news dataset. Now, let\u2019s examine our retriever options. At the moment, there are\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/module_guides/indexing/lpg_index_guide/#retrieval-and-querying\" rel=\"noreferrer noopener\">\n   four existing retrievers available\n  </a>\n  :\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/module_guides/indexing/lpg_index_guide/#default-llmsynonymretriever\" rel=\"noreferrer noopener\">\n    LLMSynonymRetriever\n   </a>\n   : takes the query, and tries to generate keywords and synonyms to retrieve nodes (and therefore the paths connected to those nodes).\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/module_guides/indexing/lpg_index_guide/#default-if-supported-vectorcontextretriever\" rel=\"noreferrer noopener\">\n    VectorContextRetriever\n   </a>\n   : retrieves nodes based on their vector similarity, and then fetches the paths connected to those nodes\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/module_guides/indexing/lpg_index_guide/#texttocypherretriever\" rel=\"noreferrer noopener\">\n    TextToCypherRetriever\n   </a>\n   : uses a graph store schema, your query, and a prompt template in order to generate and execute a cypher query\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/module_guides/indexing/lpg_index_guide/#cyphertemplateretriever\" rel=\"noreferrer noopener\">\n    CypherTemplateRetriever\n   </a>\n   : Rather than letting the LLM have free-range of generating any cypher statement, we can instead provide a cypher template and have the LLM fill in the parameters.\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Additionally, implementing a custom retriever is straightforward, so that is exactly what we will do here. Our custom retriever will first identify entities in the input query and then execute the VectorContextRetriever for each identified entity separately.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  First, we will define the entity extraction model and prompt.\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> pydantic <span class=\"hljs-keyword\">import</span> BaseModel\n<span class=\"hljs-keyword\">from</span> typing <span class=\"hljs-keyword\">import</span> <span class=\"hljs-type\">Optional</span>, <span class=\"hljs-type\">List</span>\n\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">Entities</span>(<span class=\"hljs-title class_ inherited__\">BaseModel</span>):\n    <span class=\"hljs-string\">\"\"\"List of named entities in the text such as names of people, organizations, concepts, and locations\"\"\"</span>\n    names: <span class=\"hljs-type\">Optional</span>[<span class=\"hljs-type\">List</span>[<span class=\"hljs-built_in\">str</span>]]\n\n\nprompt_template_entities = <span class=\"hljs-string\">\"\"\"\nExtract all named entities such as names of people, organizations, concepts, and locations\nfrom the following text:\n{text}\n\"\"\"</span></code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Now we can progress to the custom retriever implementation.\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> typing <span class=\"hljs-keyword\">import</span> <span class=\"hljs-type\">Any</span>, <span class=\"hljs-type\">Optional</span>\n\n<span class=\"hljs-keyword\">from</span> llama_index.core.embeddings <span class=\"hljs-keyword\">import</span> BaseEmbedding\n<span class=\"hljs-keyword\">from</span> llama_index.core.retrievers <span class=\"hljs-keyword\">import</span> CustomPGRetriever, VectorContextRetriever\n<span class=\"hljs-keyword\">from</span> llama_index.core.vector_stores.types <span class=\"hljs-keyword\">import</span> VectorStore\n<span class=\"hljs-keyword\">from</span> llama_index.program.openai <span class=\"hljs-keyword\">import</span> OpenAIPydanticProgram\n\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">MyCustomRetriever</span>(<span class=\"hljs-title class_ inherited__\">CustomPGRetriever</span>):\n    <span class=\"hljs-string\">\"\"\"Custom retriever with entity detection.\"\"\"</span>\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">init</span>(<span class=\"hljs-params\">\n        self,\n        <span class=\"hljs-comment\">## vector context retriever params</span>\n        embed_model: <span class=\"hljs-type\">Optional</span>[BaseEmbedding] = <span class=\"hljs-literal\">None</span>,\n        vector_store: <span class=\"hljs-type\">Optional</span>[VectorStore] = <span class=\"hljs-literal\">None</span>,\n        similarity_top_k: <span class=\"hljs-built_in\">int</span> = <span class=\"hljs-number\">4</span>,\n        path_depth: <span class=\"hljs-built_in\">int</span> = <span class=\"hljs-number\">1</span>,\n        include_text: <span class=\"hljs-built_in\">bool</span> = <span class=\"hljs-literal\">True</span>,\n        **kwargs: <span class=\"hljs-type\">Any</span>,\n    </span>) -&gt; <span class=\"hljs-literal\">None</span>:\n        <span class=\"hljs-string\">\"\"\"Uses any kwargs passed in from class constructor.\"\"\"</span>\n        self.entity_extraction = OpenAIPydanticProgram.from_defaults(\n            output_cls=Entities, prompt_template_str=prompt_template_entities\n        )\n        self.vector_retriever = VectorContextRetriever(\n            self.graph_store,\n            include_text=self.include_text,\n            embed_model=embed_model,\n            similarity_top_k=similarity_top_k,\n            path_depth=path_depth,\n        )\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">custom_retrieve</span>(<span class=\"hljs-params\">self, query_str: <span class=\"hljs-built_in\">str</span></span>) -&gt; <span class=\"hljs-built_in\">str</span>:\n        <span class=\"hljs-string\">\"\"\"Define custom retriever with entity detection.\n\n        Could return `str`, `TextNode`, `NodeWithScore`, or a list of those.\n        \"\"\"</span>\n        entities = self.entity_extraction(text=query_str).names\n        result_nodes = []\n        <span class=\"hljs-keyword\">if</span> entities:\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Detected entities: <span class=\"hljs-subst\">{entities}</span>\"</span>)\n            <span class=\"hljs-keyword\">for</span> entity <span class=\"hljs-keyword\">in</span> entities:\n                result_nodes.extend(self.vector_retriever.retrieve(entity))\n        <span class=\"hljs-keyword\">else</span>:\n            result_nodes.extend(self.vector_retriever.retrieve(query_str))\n        final_text = <span class=\"hljs-string\">\"\\n\\n\"</span>.join(\n            [n.get_content(metadata_mode=<span class=\"hljs-string\">\"llm\"</span>) <span class=\"hljs-keyword\">for</span> n <span class=\"hljs-keyword\">in</span> result_nodes]\n        )\n        <span class=\"hljs-keyword\">return</span> final_text</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   MyCustomRetriever\n  </code>\n  class has only two methods. You can use the\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   init\n  </code>\n  method to instantiate any functions or classes you will be using in the retriever. In this example, we instantiate the entity detection OpenAI program along with the vector context retriever.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   custom_retrieve\n  </code>\n  method is called during retrieval. In our custom retriever implementation, we first identify any relevant entities in the text. If any entities are found, we iterate and execute the vector context retriever for each entity. On the other hand, if no entities are identified we pass the entire input to the vector context retriever.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  As you can observe, you can easily customize the retriever for your use-case by incorporating existing retrievers or starting from scratch as you can easily execute Cypher statements by using the\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   structured_query\n  </code>\n  method of the graph store.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Question-answering flow\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Let\u2019s wrap it up by using the custom retriever to answer an example question. We need to pass the retriever to the\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   RetrieverQueryEngine\n  </code>\n  .\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index.core.query_engine <span class=\"hljs-keyword\">import</span> RetrieverQueryEngine\n\ncustom_sub_retriever = MyCustomRetriever(\n    index.property_graph_store,\n    include_text=<span class=\"hljs-literal\">True</span>,\n    vector_store=index.vector_store,\n    embed_model=embed_model\n)\n\nquery_engine = RetrieverQueryEngine.from_args(\n    index.as_retriever(sub_retrievers=[custom_sub_retriever]), llm=llm\n)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Let\u2019s test it out!\n </p>\n <pre><code>response = query_engine.query(\n    <span class=\"hljs-string\">\"What do you know about Maliek Collins or Darragh O\u2019Brien?\"</span>\n)\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-built_in\">str</span>(response))\n<span class=\"hljs-comment\"># Detected entities: ['Maliek Collins', \"Darragh O'Brien\"]</span>\n<span class=\"hljs-comment\"># Maliek Collins is a defensive tackle who has played for the Dallas Cowboys, Las Vegas Raiders, and Houston Texans. Recently, he signed a two-year contract extension with the Houston Texans worth $23 million, including a $20 million guarantee. This new deal represents a raise from his previous contract, where he earned $17 million with $8.5 million guaranteed. Collins is expected to be a key piece in the Texans' defensive line and fit well into their 4-3 alignment.</span>\n<span class=\"hljs-comment\"># Darragh O\u2019Brien is the Minister for Housing and has been involved in the State\u2019s industrial relations process and the Government. He was recently involved in a debate in the D\u00e1il regarding the pay and working conditions of retained firefighters, which led to a heated exchange and almost resulted in the suspension of the session. O\u2019Brien expressed confidence that the dispute could be resolved and encouraged unions to re-engage with the industrial relations process.</span></code></pre>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Summary\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In this blog post, we\u2019ve explored the intricacies of customizing the property graph index within LlamaIndex, focusing on implementing entity deduplication and designing custom retrieval methods to enhance GraphRAG accuracy. The property graph index allows for a modular and flexible approach, utilizing various graph constructors and retrievers to tailor the implementation to your specific needs. Whether you\u2019re building your first knowledge graph or optimizing for a unique dataset, these customizable components offer a powerful toolkit. We invite you to test out the property graph index integration to see how they can elevate your knowledge graph projects.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  As always, the code is available on\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/tomasonjo/blogs/blob/master/llm/llama_index_neo4j_custom_retriever.ipynb\" rel=\"noreferrer noopener\">\n   GitHub\n  </a>\n  .\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 30109, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a56f2b2d-eb47-487d-b108-690cae3ec9f6": {"__data__": {"id_": "a56f2b2d-eb47-487d-b108-690cae3ec9f6", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/data-agents-eed797d7972f.html", "file_name": "data-agents-eed797d7972f.html", "file_type": "text/html", "file_size": 32843, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/data-agents-eed797d7972f.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/data-agents-eed797d7972f.html", "file_name": "data-agents-eed797d7972f.html", "file_type": "text/html", "file_size": 32843, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "14256eb33cb86681debec0901a21e0e36a03e740b9a7fe069b7ba0bd62d75ff1", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Today we\u2019re incredibly excited to announce the launch of a big new capability within LlamaIndex:\n  <strong>\n   Data Agents\n  </strong>\n  .\n </p>\n <p>\n  Data Agents are LLM-powered knowledge workers that can intelligently perform various tasks over your data, in both a \u201cread\u201d and \u201cwrite\u201d function. They are capable of the following:\n </p>\n <ul>\n  <li>\n   Perform automated search and retrieval over different types of data \u2014 unstructured, semi-structured, and structured.\n  </li>\n  <li>\n   Calling any external service API in a structured fashion. They can either process the response immediately, or index/cache this data for future use.\n  </li>\n  <li>\n   Storing conversation history.\n  </li>\n  <li>\n   Using all of the above to fulfill both simple and complex data tasks.\n  </li>\n </ul>\n <p>\n  We\u2019ve worked hard to provide abstractions, services, and guides on both the agents side and tools side in order to build data agents. Today\u2019s launch consists of the following key components:\n </p>\n <ul>\n  <li>\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/agents/root.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     General Agent/Tool Abstractions\n    </strong>\n   </a>\n   <strong>\n    :\n   </strong>\n   a set of abstractions to build agent loops, and to have those loops interact with tools according to a structured API definition.\n  </li>\n  <li>\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/tools/root.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     LlamaHub Tool Repository\n    </strong>\n   </a>\n   <strong>\n    :\n   </strong>\n   A\n   <a href=\"https://llamahub.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    brand-new section within LlamaHub\n   </a>\n   that consists of 15+ Tools (e.g. Google Calendar, Notion, SQL, OpenAPI) that can be connected. Opening to\n   <a href=\"https://github.com/emptycrown/llama-hub\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    community contributions\n   </a>\n   !\n  </li>\n </ul>\n <p>\n  See below for full details.\n  <strong>\n   We show you how to build a Gmail agent that\u2019s able to automatically create/send emails in &lt;10 lines of code!\n  </strong>\n </p>\n <h1>\n  Context\n </h1>\n <p>\n  Our core mission at LlamaIndex is to unlock the full capabilities of LLMs over your external sources of data. It provides a set of tools to both define \u201cstate\u201d (how to parse/structure your data), and \u201ccompute\u201d (how to query your data). Up until now, our framework has primarily focused on search and retrieval use case. We have an incredible suite of tools and capabilities that not only allow you to create the basic RAG stack around a vector database + top-k retrieval, but also offer much greater functionality\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/query_modules/query_engine/root.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   beyond that\n  </a>\n  .\n </p>\n <p>\n  A lot of that technology used to lie in our query engines. Our goal was to increase the capability of query engines to answer a wide range of different queries. In order to do this, we had to improve the \u201creasoning\u201d capabilities of these query engines. As a result some of our existing query capabilities contain \u201cagent-like\u201d components: we have query engines capable of chain-of-thought reasoning, query decomposition, and routing. In the process, users had the option of choosing from a spectrum of query engines that had more constrained reasoning capabilities to less constrained capabilities.\n </p>\n <p>\n  But there was a huge opportunity for LLMs to have an even richer set of interactions with data; they should be capable of general reasoning over any set of tools, whether from a database or an API. They should also be capable of both \u201cread\u201d and \u201cwrite\u201d capabilities \u2014 the ability to not only understand state but also modify it. As a result they should be able to do more than search and retrieval from a static knowledge source.\n </p>\n <p>\n  Some existing\n  <a href=\"https://openai.com/blog/chatgpt-plugins\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   services\n  </a>\n  ,\n  <a href=\"https://python.langchain.com/docs/modules/agents/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   toolkits\n  </a>\n  , and\n  <a href=\"https://arxiv.org/abs/2302.04761\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   research\n  </a>\n  <a href=\"https://arxiv.org/abs/2210.03629\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   papers\n  </a>\n  have already demonstrated the possibilities of LLM-powered \u201cagents\u201d that can interact with the external environment. Using these existing approaches as inspiration, we saw an opportunity to build a principled series of abstractions enabling anyone to build knowledge workers over their data.\n </p>\n <h1>\n  Core Components of Data Agents\n </h1>\n <p>\n  Building a data agent requires the following core components:\n </p>\n <ul>\n  <li>\n   A reasoning loop\n  </li>\n  <li>\n   Tool abstractions\n  </li>\n </ul>\n <p>\n  At a high-level, a data agent is provided with a set of APIs, or Tools, to interact with. These APIs can return information about the world, or perform an action that modifies state. Each Tool exposes a request/response interface. The request is a set of structured parameters, and the response can be any format (at least conceptually, in most cases the response here is a text string of some form).\n </p>\n <p>\n  Given an input task, the data agent uses a\n  <strong>\n   reasoning loop\n  </strong>\n  to decide which tools to use, in which sequence, and the parameters to call each tool. The \u201cloop\u201d can conceptually be very simple (a one-step tool selection process), or complex (a multi-step selection process, where a multitude of tools are picked at each step).\n </p>\n <p>\n  These components are described in more detail below.\n </p>\n <h2>\n  Agent Abstraction + Reasoning Loop\n </h2>\n <p>\n  We have support for the following agents:\n </p>\n <ul>\n  <li>\n   OpenAI Function agent (built on top of the OpenAI Function API)\n  </li>\n  <li>\n   a ReAct agent (which works across any chat/text completion endpoint).\n  </li>\n </ul>\n <p>\n  You can use them as the following:\n </p>\n <pre><span class=\"qs ow gt qp b bf qt qu l qv qw\" id=\"d7cf\"><span class=\"hljs-keyword\">from</span> llama_index.agent <span class=\"hljs-keyword\">import</span> OpenAIAgent, ReActAgent\n<span class=\"hljs-keyword\">from</span> llama_index.llms <span class=\"hljs-keyword\">import</span> OpenAI\n\n<span class=\"hljs-comment\"># import and define tools</span>\n...\n<span class=\"hljs-comment\"># initialize llm</span>\nllm = OpenAI(model=<span class=\"hljs-string\">\"gpt-3.5-turbo-0613\"</span>)\n<span class=\"hljs-comment\"># initialize openai agent</span>\nagent = OpenAIAgent.from_tools(tools, llm=llm, verbose=<span class=\"hljs-literal\">True</span>)\n<span class=\"hljs-comment\"># initialize ReAct agent</span>\nagent = ReActAgent.from_tools(tools, llm=llm, verbose=<span class=\"hljs-literal\">True</span>)\n<span class=\"hljs-comment\"># use agent</span>\nresponse = agent.chat(<span class=\"hljs-string\">\"What is (121 * 3) + 42?\"</span>)</span></pre>\n <p>\n  Each agent takes in a set of Tools. The details behind our tool abstractions are provided below. Each agent also supports two main methods for taking in an input task \u2014\n  <code class=\"cw qx qy qz qp b\">\n   chat\n  </code>\n  and\n  <code class=\"cw qx qy qz qp b\">\n   query\n  </code>\n  . Note that these are the core methods used in our\n  <code class=\"cw qx qy qz qp b\">\n   ChatEngine\n  </code>\n  and\n  <code class=\"cw qx qy qz qp b\">\n   QueryEngine\n  </code>\n  respectively. In fact that our base agent class (\n  <code class=\"cw qx qy qz qp b\">\n   BaseAgent\n  </code>\n  ) simply inherits from\n  <code class=\"cw qx qy qz qp b\">\n   BaseChatEngine\n  </code>\n  and\n  <code class=\"cw qx qy qz qp b\">\n   BaseQueryEngine\n  </code>\n  .\n  <code class=\"cw qx qy qz qp b\">\n   chat\n  </code>\n  allows the agent to utilize previously stored conversation history, whereas\n  <code class=\"cw qx qy qz qp b\">\n   query\n  </code>\n  is a stateless call - history/state is not preserved over time.\n </p>\n <p>\n  The reasoning loop depends on the type of agent. The OpenAI agent calls the OpenAI function API in a while loop, since the tool decision logic is baked into the function API. Given an input prompt and previous chat history (which includes previous function calls), the function API will decide whether to make another function call (pick a Tool), or return an assistant message. If the API returns a function call, then we are responsible for executing the function and passing in a function message in the chat history. If the API returns an assistant message, then the loop is complete (we assume the task is solved).\n </p>\n <p>\n  The ReAct agent uses general text completion endpoints, so it can be used with any LLM. A text completion endpoint has a simple input str \u2192 output str format, which means that the reasoning logic must be encoded in the prompt. The ReAct agent uses an input prompt inspired by the ReAct paper (and adapted into other versions), in order to decide which tool to pick. It looks something like this:\n </p>\n <pre><span class=\"qs ow gt qp b bf qt qu l qv qw\" id=\"c804\">...\nYou have access to the following tools:\n{tool_desc}\n\nTo answer the question, please use the following format.\n\n```\nThought: I need to use a tool to help me answer the question.\nAction: tool name (one of {tool_names})\nAction Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"text\": \"hello world\", \"num_beams\": 5}})\n```\nPlease use a valid JSON format for the action input. Do NOT do this {{'text': 'hello world', 'num_beams': 5}}.\n\nIf this format is used, you will receive a response in the following format:\n\n```\nObservation: tool response\n```\n...</span></pre>\n <p>\n  We implement ReAct natively over chat prompts; the reasoning loop is implemented as an alternating series of assistant and user messages. The Thought/Action/Action Input section is represented as an assistant message, and the Observation section is implemented as a user message.\n </p>\n <p>\n  <strong>\n   Note:\n  </strong>\n  the ReAct prompt expects not only the name of the tool to pick, but also the parameters to fill in the tool in a JSON format. This makes the output not dissimilar from the output of the OpenAI Function API \u2014 the main difference is that in the case of the function API, the tool-picking logic is baked into the API itself (through a finetuned model), whereas here it is elicited through explicit prompting.\n </p>\n <h2>\n  Tool Abstractions\n </h2>\n <p>\n  Having proper tool abstractions is at the core of building data agents. Defining a set of Tools is similar to defining any API interface, with the exception that these Tools are meant for agent rather than human use. We allow users to define both a single Tool as well as a \u201cToolSpec\u201d containing a series of functions under the hood.\n </p>\n <p>\n  We describe the base tool abstraction, as well as how you can easily define tools over existing query engines, other tools.\n </p>\n <p>\n  <strong>\n   Base Tool Abstraction\n  </strong>\n </p>\n <p>\n  The base tool defines a very generic interface. The\n  <code class=\"cw qx qy qz qp b\">\n   __call__\n  </code>\n  function can take in any series of arguments, and return a generic\n  <code class=\"cw qx qy qz qp b\">\n   ToolOutput\n  </code>\n  container that can capture any response. A tool also has metadata containing its name, description, and function schema.\n </p>\n <pre><span class=\"qs ow gt qp b bf qt qu l qv qw\" id=\"9ffe\"><span class=\"hljs-variable\">@dataclass</span>\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">ToolMetadata</span>:\n    <span class=\"hljs-symbol\">description:</span> str\n    <span class=\"hljs-symbol\">name:</span> <span class=\"hljs-title class_\">Optional</span>[str] = <span class=\"hljs-title class_\">None</span>\n    <span class=\"hljs-symbol\">fn_schema:</span> <span class=\"hljs-title class_\">Optional</span>[<span class=\"hljs-title class_\">Type</span>[<span class=\"hljs-title class_\">BaseModel</span>]] = <span class=\"hljs-title class_\">DefaultToolFnSchema</span>\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">BaseTool</span>:\n    <span class=\"hljs-variable\">@property</span>\n    <span class=\"hljs-variable\">@abstractmethod</span>\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">metadata</span>(<span class=\"hljs-params\"><span class=\"hljs-variable language_\">self</span></span>) -&amp;gt; <span class=\"hljs-title class_\">ToolMetadata</span>:\n        pass\n    <span class=\"hljs-variable\">@abstractmethod</span>\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__call__</span>(<span class=\"hljs-params\"><span class=\"hljs-variable language_\">self</span>, <span class=\"hljs-symbol\">input:</span> <span class=\"hljs-title class_\">Any</span></span>) -&amp;gt; <span class=\"hljs-title class_\">ToolOutput</span>:\n        pass</span></pre>\n <p>\n  <strong>\n   Function Tool\n  </strong>\n </p>\n <p>\n  A function tool allows users to easily convert any function into a Tool. It takes in a user-defined function (that can take in any inputs/outputs), and wraps it into a tool interface. It can also \u201cauto-infer\u201d the function schema if it isn\u2019t specified beforehand.\n </p>\n <p>\n  Our\n  <code class=\"cw qx qy qz qp b\">\n   ToolSpec\n  </code>\n  classes make use of this\n  <code class=\"cw qx qy qz qp b\">\n   FunctionTool\n  </code>\n  abstraction to convert functions defined in the tool spec into a set of agent tools (see below).\n </p>\n <p>\n  Here\u2019s a trivial example of defining a FunctionTool.\n </p>\n <pre><span class=\"qs ow gt qp b bf qt qu l qv qw\" id=\"d7f8\">from llama_index.tools.function_tool import FunctionTool\n\ndef <span class=\"hljs-title function_\">multiply</span><span class=\"hljs-params\">(a: <span class=\"hljs-type\">int</span>, b: <span class=\"hljs-type\">int</span>)</span> -&amp;gt; <span class=\"hljs-type\">int</span>:\n    <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"Multiple two integers and returns the result integer\"</span><span class=\"hljs-string\">\"\"</span>\n    <span class=\"hljs-keyword\">return</span> a * b\nmultiply_tool = FunctionTool.from_defaults(fn=multiply)</span></pre>\n <p>\n  <strong>\n   QueryEngineTool\n  </strong>\n </p>\n <p>\n  Of course, we also provide Tool abstractions to wrap our existing query engines. This provides a seamless transition from working on query engines to working on agents. Our query engines can be thought of \u201cconstrained\u201d agents meant for the read/write setting and centered around retrieval purposes. These query engines can be used in an overall agent setting.\n </p>\n <pre><span class=\"qs ow gt qp b bf qt qu l qv qw\" id=\"1349\"><span class=\"hljs-keyword\">from</span> llama_index.tools <span class=\"hljs-keyword\">import</span> QueryEngineTool\n\nquery_engine_tools = [\n    QueryEngineTool(\n        query_engine=query_engine, \n        metadata=ToolMetadata(\n            name=<span class=\"hljs-string\">'&amp;lt;tool_name&amp;gt;'</span>, \n            description=<span class=\"hljs-string\">\"Queries over X data source.\"</span>\n        )\n    ),\n ...\n]</span></pre>\n <p>\n  <strong>\n   Tool Specs\n  </strong>\n </p>\n <p>\n  A\n  <strong>\n   tool spec\n  </strong>\n  is a Python class that represents a full API specification that an agent can interact with, and a tool spec can be converted into a list of tools that an agent can be initialized with.\n </p>\n <p>\n  This class allows users to define entire services, not just single tools that perform individual tasks. Each tool spec may contain read/write endpoints that allow an agent to interact with a service in meaningful ways. For instance, a Slack tool spec could allow the user to both read existing messages and channels (\n  <code class=\"cw qx qy qz qp b\">\n   load_data\n  </code>\n  ,\n  <code class=\"cw qx qy qz qp b\">\n   fetch_channels\n  </code>\n  ) as well as write messages (\n  <code class=\"cw qx qy qz qp b\">\n   send_message\n  </code>\n  ). It would be roughly defined as the following:\n </p>\n <pre><span class=\"qs ow gt qp b bf qt qu l qv qw\" id=\"424d\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">SlackToolSpec</span>(<span class=\"hljs-title class_ inherited__\">BaseToolSpec</span>):\n    <span class=\"hljs-string\">\"\"\"Slack tool spec.\"\"\"</span>\n    spec_functions = [<span class=\"hljs-string\">\"load_data\"</span>, <span class=\"hljs-string\">\"send_message\"</span>, <span class=\"hljs-string\">\"fetch_channels\"</span>]\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">load_data</span>(<span class=\"hljs-params\">\n          self,\n          channel_ids: <span class=\"hljs-type\">List</span>[<span class=\"hljs-built_in\">str</span>],\n          reverse_chronological: <span class=\"hljs-built_in\">bool</span> = <span class=\"hljs-literal\">True</span>,\n      </span>) -&amp;gt; <span class=\"hljs-type\">List</span>[Document]:\n          <span class=\"hljs-string\">\"\"\"Load data from the input directory.\"\"\"</span>\n          ...\n      <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">send_message</span>(<span class=\"hljs-params\">\n          self,\n          channel_id: <span class=\"hljs-built_in\">str</span>,\n          message: <span class=\"hljs-built_in\">str</span>,\n      </span>) -&amp;gt; <span class=\"hljs-literal\">None</span>:\n          <span class=\"hljs-string\">\"\"\"Send a message to a channel given the channel ID.\"\"\"</span>\n          ...\n      <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">fetch_channels</span>(<span class=\"hljs-params\">\n          self,\n      </span>) -&amp;gt; <span class=\"hljs-type\">List</span>[<span class=\"hljs-built_in\">str</span>]:\n          <span class=\"hljs-string\">\"\"\"Fetch a list of relevant channels.\"\"\"</span>\n          ...</span></pre>\n <p>\n  If a tool spec is initialized, it can be converted into a list of tools that can be fed into an agent with\n  <code class=\"cw qx qy qz qp b\">\n   to_tool_list\n  </code>\n  . For instance,\n </p>\n <pre><span class=\"qs ow gt qp b bf qt qu l qv qw\" id=\"ec2b\">tool_spec = SlackToolSpec()\n# initialize openai agent\nagent = OpenAIAgent.from_tools(tool_spec.to_tool_list(), llm=llm, verbose=True)</span></pre>\n <p>\n  Defining a tool spec is not that different than defining a Python class. Each function becomes converted into a tool, and by default the docstring for each function gets used as the tool description (though you can customize names/description in\n  <code class=\"cw qx qy qz qp b\">\n   to_tool_list(func_to_metadata_mapping=...)\n  </code>\n  .\n </p>\n <p>\n  We also made the intentional choice that the input arguments and return types can be anything. The primary reason is to preserve the generality of the tool interface for subsequent iterations of agents. Even if current iterations of agents expect tool outputs to be in string format, that may change in the future, and we didn\u2019t want to arbitrarily restrict the types of tool interface.\n </p>\n <h1>\n  LlamaHub Tool Repository\n </h1>\n <p>\n  A huge component of our launch is a brand-new addition to\n  <a href=\"https://llamahub.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaHub\n  </a>\n  : a Tool Repository. The Tool Repository consists of\n  <strong>\n   15+ Tool Specs\n  </strong>\n  that an agent can use. These tool specs represent an initial curated list of services that an agent can interact with and enrich its capability to perform different actions.\n </p>\n <p>\n  Among others, they include the following specs:\n </p>\n <ul>\n  <li>\n   Gmail Spec\n  </li>\n  <li>\n   Zapier Spec\n  </li>\n  <li>\n   Google Calendar Spec\n  </li>\n  <li>\n   OpenAPI Spec\n  </li>\n  <li>\n   SQL + Vector Database Spec\n  </li>\n </ul>\n <p>\n  We also provide a list of\n  <strong>\n   utility tools\n  </strong>\n  that help to abstract away pain points when designing agents to interact with different API services that return large amounts of data.\n </p>\n <p>\n  For instance, our Gmail Tool Spec allows an agent to search existing emails, create drafts, update drafts, and send emails. Our Zapier Spec allows an agent to perform any natural language query to Zapier through their\n  <a href=\"https://nla.zapier.com/start/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Natural Language Actions\n  </a>\n  interface.\n </p>\n <p>\n  Best of all, you don\u2019t need to spend a lot of time figuring out how to use these tools \u2014 we have\n  <a href=\"https://github.com/emptycrown/llama-hub/tree/main/llama_hub/tools/notebooks\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    10+ notebooks\n   </strong>\n  </a>\n  showing how you can build agents for each service, or even build agents that use a combination of services (e.g. Gmail, Google Calendar, and Search).\n </p>\n <h2>\n  Example Walkthrough\n </h2>\n <p>\n  Let\u2019s take a look at a few examples! We initialize an OpenAIAgent with the Gmail Spec. As mentioned above, the spec consists of tools to search emails, create/update drafts, and send emails.\n </p>\n <p>\n  Now let\u2019s give the agent a sequence of commands so that it can create an email draft, make a few edits to it, and then send it off.\n </p>\n <p>\n  First, let\u2019s create an initial email draft. Note that the agent chooses the\n  <code class=\"cw qx qy qz qp b\">\n   create_draft\n  </code>\n  tool, which takes in the \u201cto\u201d, \u201csubject\u201d, and \u201cmessage\u201d parameters. The agent is able to infer the parameters simultaneously while picking the tool.\n </p>\n <p>\n  Next, let\u2019s update the draft with a slight modification:\n </p>\n <p>\n  Next, let\u2019s show the current state of the draft.\n </p>\n <p>\n  Finally, let\u2019s send the email!\n </p>\n <p>\n  This is a good start, but this is just the beginning. We are actively working on contributing more tools to this repository, and we\u2019re also opening this up to community contributions. If you\u2019re interested in contributing a Tool to LlamaHub, please feel free to open a PR in this repo.\n </p>\n <h2>\n  Utility Tools\n </h2>\n <p>\n  Oftentimes, directly querying an API can return a massive volume of data, which on its own may overflow the context window of the LLM (or at the very least unnecessarily increase the number of tokens that you are using).\n </p>\n <p>\n  To tackle this, we\u2019ve provided an initial set of \u201cutility tools\u201d in the core LlamaIndex repo \u2014 utility tools are not conceptually tied to a given service (e.g. Gmail, Notion), but rather can augment the capabilities of existing Tools. In this particular case, utility tools help to abstract away common patterns of needing to cache/index and query data that\u2019s returned from any API request.\n </p>\n <p>\n  Let\u2019s walk through our two main utility tools below.\n </p>\n <p>\n  <strong>\n   OnDemandLoaderTool\n  </strong>\n </p>\n <p>\n  This tool turns any existing LlamaIndex data loader (\n  <code class=\"cw qx qy qz qp b\">\n   BaseReader\n  </code>\n  class) into a tool that an agent can use. The tool can be called with all the parameters needed to trigger\n  <code class=\"cw qx qy qz qp b\">\n   load_data\n  </code>\n  from the data loader, along with a natural language query string. During execution, we first load data from the data loader, index it (for instance with a vector store), and then query it \u201con-demand\u201d. All three of these steps happen in a single tool call.\n </p>\n <p>\n  Oftentimes this can be preferable to figuring out how to load and index API data yourself. While this may allow for data reusability, oftentimes users just need an ad-hoc index to abstract away prompt window limitations for any API call.\n </p>\n <p>\n  A usage example is given below:\n </p>\n <pre><span class=\"qs ow gt qp b bf qt qu l qv qw\" id=\"3359\"><span class=\"hljs-keyword\">from</span> llama_hub.wikipedia.base <span class=\"hljs-keyword\">import</span> WikipediaReader\n<span class=\"hljs-keyword\">from</span> llama_index.tools.on_demand_loader_tool <span class=\"hljs-keyword\">import</span> OnDemandLoaderTool\n\ntool = OnDemandLoaderTool.from_defaults(\n reader,\n name=<span class=\"hljs-string\">\"Wikipedia Tool\"</span>,\n description=<span class=\"hljs-string\">\"A tool for loading data and querying articles from Wikipedia\"</span>\n)</span></pre>\n <p>\n  <strong>\n   LoadAndSearchToolSpec\n  </strong>\n </p>\n <p>\n  The LoadAndSearchToolSpec takes in any existing Tool as input. As a tool spec, it implements\n  <code class=\"cw qx qy qz qp b\">\n   to_tool_list\n  </code>\n  , and when that function is called, two tools are returned: a\n  <code class=\"cw qx qy qz qp b\">\n   load\n  </code>\n  tool and then a\n  <code class=\"cw qx qy qz qp b\">\n   search\n  </code>\n  tool.\n </p>\n <p>\n  The\n  <code class=\"cw qx qy qz qp b\">\n   load\n  </code>\n  Tool execution would call the underlying Tool, and the index the output (by default with a vector index). The\n  <code class=\"cw qx qy qz qp b\">\n   search\n  </code>\n  Tool execution would take in a query string as input and call the underlying index.\n </p>\n <p>\n  This is helpful for any API endpoint that will by default return large volumes of data \u2014 for instance our WikipediaToolSpec will by default return entire Wikipedia pages, which will easily overflow most LLM context windows.\n </p>\n <p>\n  Example usage is shown below:\n </p>\n <pre><span class=\"qs ow gt qp b bf qt qu l qv qw\" id=\"acf7\"><span class=\"hljs-keyword\">from</span> llama_hub.tools.wikipedia.base <span class=\"hljs-keyword\">import</span> WikipediaToolSpec\n<span class=\"hljs-keyword\">from</span> llama_index.tools.tool_spec.load_and_search.base <span class=\"hljs-keyword\">import</span> LoadAndSearchToolSpec\n\nwiki_spec = WikipediaToolSpec()\n<span class=\"hljs-comment\"># Get the search wikipedia tool</span>\ntool = wiki_spec.to_tool_list()[<span class=\"hljs-number\">1</span>]\n<span class=\"hljs-comment\"># Create the Agent with load/search tools</span>\nagent = OpenAIAgent.from_tools(\n LoadAndSearchToolSpec.from_defaults(\n    tool\n ).to_tool_list(), verbose=<span class=\"hljs-literal\">True</span>\n)</span></pre>\n <p>\n  This is the output when we run an input prompt\n </p>\n <pre><span class=\"qs ow gt qp b bf qt qu l qv qw\" id=\"13c2\">agent.chat('what is the capital of poland')</span></pre>\n <p>\n  Output:\n </p>\n <pre><span class=\"qs ow gt qp b bf qt qu l qv qw\" id=\"2bda\">=== <span class=\"hljs-title class_\">Calling</span> <span class=\"hljs-title class_\">Function</span> ===\n<span class=\"hljs-title class_\">Calling</span> <span class=\"hljs-attr\">function</span>: search_data <span class=\"hljs-keyword\">with</span> <span class=\"hljs-attr\">args</span>: {\n  <span class=\"hljs-string\">\"query\"</span>: <span class=\"hljs-string\">\"capital of Poland\"</span>\n}\n<span class=\"hljs-title class_\">Got</span> <span class=\"hljs-attr\">output</span>: <span class=\"hljs-title class_\">Content</span> loaded! <span class=\"hljs-title class_\">You</span> can now search the information using read_search_data\n========================\n=== <span class=\"hljs-title class_\">Calling</span> <span class=\"hljs-title class_\">Function</span> ===\n<span class=\"hljs-title class_\">Calling</span> <span class=\"hljs-attr\">function</span>: read_search_data <span class=\"hljs-keyword\">with</span> <span class=\"hljs-attr\">args</span>: {\n  <span class=\"hljs-string\">\"query\"</span>: <span class=\"hljs-string\">\"What is the capital of Poland?\"</span>\n}\n<span class=\"hljs-title class_\">Got</span> <span class=\"hljs-attr\">output</span>: \n<span class=\"hljs-title class_\">The</span> capital <span class=\"hljs-keyword\">of</span> <span class=\"hljs-title class_\">Poland</span> is <span class=\"hljs-title class_\">Warsaw</span>.\n========================\n<span class=\"hljs-title class_\">AgentChatResponse</span>(response=<span class=\"hljs-string\">'The capital of Poland is Warsaw.'</span>, sources=[])</span></pre>\n <p>\n  Note that the agent figures out that it first needs to first call the \u201cload\u201d tool (denoted by the original name of the tool, \u201csearch_data\u201d). This load tool will load the Wikipedia page and index under the hood. The output just mentions that the \u201ccontent is loaded, and tells the agent that the next step is to use\n  <code class=\"cw qx qy qz qp b\">\n   read_search_data\n  </code>\n  . The agent then reasons that it needs to call the\n  <code class=\"cw qx qy qz qp b\">\n   read_search_data\n  </code>\n  tool, which will query the index for the right answer.\n </p>\n <h1>\n  FAQ\n </h1>\n <p>\n  <strong>\n   Should I use Data Agents for search and retrieval, or continue to use Query Engines?\n  </strong>\n </p>\n <p>\n  Short answer: both are possible. Query engines give you the ability to define your own workflows over your data, in both a constrained reasoning fashion as well as unconstrained fashion. For instance, you may want to define a specific workflow over text-to-SQL with our\n  <code class=\"cw qx qy qz qp b\">\n   NLStructStoreQueryEngine\n  </code>\n  (constrained), or a router module to decide between semantic search or summarization (less constrained), or use our\n  <code class=\"cw qx qy qz qp b\">\n   SubQuestionQueryEngine\n  </code>\n  to decompose a question among sub-documents (even less constrained).\n </p>\n <p>\n  By default, agent loops are unconstrained, and can theoretically reason over any set of tools that you give them. This means that you can get out-of-the-box advanced search/retrieval capabilities \u2014 for instance, in our OpenAI cookbook we show that you can get joint text-to-SQL capabilities by simply providing a SQL query engine and Vector Store Query engine as tools. But on the other hand, agents built in this fashion can be quite unreliable (see our blog post for more insights). If you are using agents for search/retrieval, be mindful of the 1) LLM you pick, and the 2) set of tools you pick too.\n </p>\n <p>\n  <strong>\n   How are LlamaIndex data agents different than existing agent frameworks (LangChain, Hugging Face, etc.)?\n  </strong>\n </p>\n <p>\n  Most of these core concepts are not new. Our overall design has taken inspiration from popular tools and frameworks for building agents. But in our \u201cdata agents\u201d design, we\u2019ve tried our best to answer the following key questions well:\n </p>\n <ul>\n  <li>\n   How do we effectively index/query and retrieve data beforehand?\n  </li>\n  <li>\n   How do we effectively index/query and retrieve data on the fly?\n  </li>\n  <li>\n   How do we design API interfaces for read/writes that are simultaneously rich (can take in structured inputs), but also easy for agents to understand?\n  </li>\n  <li>\n   How do we properly get sources in citations?\n  </li>\n </ul>\n <p>\n  Our goal with data agents is to create automated knowledge workers that can reason over and interact with data. Our core toolkit provides the foundations for properly indexing, retrieving, and querying data \u2014 these can be easily integrated as tools. We provide some additional tool abstractions to handle the cases where you want to \u201ccache\u201d API outputs on the fly (see above). Finally, we provide principled tool abstractions and design principles so that agents can interface with external services in a structured manner.\n </p>\n <p>\n  <strong>\n   Can I use Tools with LangChain agents?\n  </strong>\n  You can easily use any of our tools with LangChain agents as well.\n </p>\n <pre><span class=\"qs ow gt qp b bf qt qu l qv qw\" id=\"0804\">tools = tool_spec.to_tool_list()\nlangchain_tools = [t.to_langchain_tool() <span class=\"hljs-keyword\">for</span> t <span class=\"hljs-keyword\">in</span> tools]</span></pre>\n <p>\n  See our\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/tools/usage_pattern.html#using-with-langchain\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   tools usage guide\n  </a>\n  for more details!\n </p>\n <h1>\n  Conclusion\n </h1>\n <p>\n  In summary, today we launched two key items: Data Agent components (incl. agent reasoning loop and tool abstractions) and the LlamaHub Tool repository.\n </p>\n <h2>\n  Resources\n </h2>\n <p>\n  We\u2019ve written a comprehensive section in the docs \u2014 take a look here:\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/agents/root.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/agents/root.html\n  </a>\n </p>\n <p>\n  Take a look at our LlamaHub Tools section:\n  <a href=\"https://llamahub.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://llamahub.ai/\n  </a>\n </p>\n <p>\n  Notebook Tutorials for LlamaHub Tools:\n  <a href=\"https://github.com/emptycrown/llama-hub/tree/main/llama_hub/tools/notebooks\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://github.com/emptycrown/llama-hub/tree/main/llama_hub/tools/notebooks\n  </a>\n </p>\n <p>\n  If you have questions, please hop on our Discord:\n  <a href=\"https://discord.gg/dGcwcsnxhU\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://discord.gg/dGcwcsnxhU\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 32688, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45d275d9-debd-445a-aae0-6fc18f97f963": {"__data__": {"id_": "45d275d9-debd-445a-aae0-6fc18f97f963", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/data-agents-zapier-nla-67146395ce1.html", "file_name": "data-agents-zapier-nla-67146395ce1.html", "file_type": "text/html", "file_size": 1249, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/data-agents-zapier-nla-67146395ce1.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/data-agents-zapier-nla-67146395ce1.html", "file_name": "data-agents-zapier-nla-67146395ce1.html", "file_type": "text/html", "file_size": 1249, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "3bf712da96abb1ce5edbb763f1dca86488386fcc893fd55f0494604d82d38ae5", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <blockquote>\n  <p class=\"nd ne nf ng b nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob gm bj\" id=\"75a3\">\n   Joint blog by LlamaIndex team &amp; Zapier NLA team\n  </p>\n </blockquote>\n <p>\n  Wouldn\u2019t it be great to have a personal assistant that can\n  <strong>\n   access your data\n  </strong>\n  and\n  <strong>\n   perform tasks for you\n  </strong>\n  ?\n </p>\n <p>\n  Introducing LlamaIndex data agents, now more powerful with Zapier NLA.\n  <strong>\n   Within 5 lines of code\n  </strong>\n  , you can access the 5,000+ third party apps and over 30,000 actions on Zapier.\n </p>\n <pre><span class=\"ol om gt oi b bf on oo l op oq\" id=\"94d5\"><span class=\"hljs-keyword\">from</span> llama_hub.tools.zapier.base <span class=\"hljs-keyword\">import</span> ZapierToolSpec\n<span class=\"hljs-keyword\">from</span> llama_index.agent <span class=\"hljs-keyword\">import</span> OpenAIAgent\n\nzapier_spec = ZapierToolSpec(api_key=<span class=\"hljs-string\">\"sk-ak-your-key\"</span>)\nagent = OpenAIAgent.from_tools(zapier_spec.to_tool_list(), verbose=<span class=\"hljs-literal\">True</span>)\n\nagent.chat(<span class=\"hljs-string\">'Can you summarize the unread emails and send it to me on Slack?'</span>)</span></pre>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1246, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1298488c-751b-4e4b-8609-0fc822bbeda4": {"__data__": {"id_": "1298488c-751b-4e4b-8609-0fc822bbeda4", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.html", "file_name": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.html", "file_type": "text/html", "file_size": 38052, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.html", "file_name": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.html", "file_type": "text/html", "file_size": 38052, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "3b294b4e2110534ce96a43eea55308c7fa35298aa5f277c62e13e979349dd3b6", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h1>\n  Summary\n </h1>\n <p>\n  In this article, we compare how well LLM-powered agents with different degrees of complexity perform over practical data tasks (financial analysis). We compare the performance of agents with more\n  <em class=\"ow\">\n   complex, unrestrained\n  </em>\n  interaction behavior (ReAct) with agents that contain\n  <em class=\"ow\">\n   simpler, more constrained\n  </em>\n  interactions (routing). We specifically analyze how much complexity can be added to the agent layer vs. the tool layer.\n </p>\n <p>\n  We find that the choice of the language model matters a lot. ReAct agents that are powered by \u201cdumber\u201d models (in a tongue-in-cheek fashion we are referring to any non GPT-4 model as \u201cdumb\u201d) struggle to return relevant results over data. We find that constraining agent interaction behavior, and giving them access to more tools that can more explicitly perform complex actions, can help improve query performance over these less sophisticated LLMs. In contrast, more sophisticated models (GPT-4) can more reliably utilize the ReAct loop to execute a variety of complex data queries.\n </p>\n <p>\n  This blog post is quite detailed; we provide a\n  <em class=\"ow\">\n   lot\n  </em>\n  of experiments and results below. Best of all, you can run this all yourself with our\n  <a href=\"https://colab.research.google.com/drive/1uP38k4nr8OPmXbY4dLoKKQW0F29WtNuY?usp=sharing\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   example notebook\n  </a>\n  !\n </p>\n <h1>\n  Overview of Agents\n </h1>\n <p>\n  Building LLM-powered agents have gotten increasingly popular in the past few months. Frameworks like\n  <a href=\"https://github.com/hwchase17/langchain\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LangChain\n  </a>\n  have made it much easier to create these agents according to a set of common abstractions.\n </p>\n <p>\n  At a high-level, an \u201cagent\u201d is essentially an automated decision engine, that can be used to interact with an external environment. The core agent loop looks something like the following:\n </p>\n <ol>\n  <li>\n   The agent has access to a set of \u201ctools\u201d, which are generic functions that it can perform. It has an awareness of each tool through some attached metadata, and it can call each tool (either as a function call or structured API).\n  </li>\n  <li>\n   User feeds in a natural language input to the agent.\n  </li>\n  <li>\n   Given the input, the agent\n   <strong>\n    interacts with the set of tools\n   </strong>\n   in some fashion and returns the response.\n  </li>\n </ol>\n <p>\n  There\u2019s a variety of ways to perform\n  <strong>\n   agent-tool interaction.\n  </strong>\n </p>\n <ul>\n  <li>\n   The most popular is probably\n   <a href=\"https://arxiv.org/abs/2210.03629\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    ReAct\n   </a>\n   : the agent reasons over the next action, constructs an action command, executes the action. It repeats these steps in an iterative loop until the task is complete.\n  </li>\n  <li>\n   There are other interaction modes too. Recently there was a paper on\n   <a href=\"https://arxiv.org/pdf/2305.04091.pdf\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Plan-and-solve Prompting\n   </a>\n   , which generates a plan beforehand (to decompose a complex task into simpler ones). Before ReAct there have also been related techniques on\n   <a href=\"https://arxiv.org/abs/2210.03350\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Self-Ask\n   </a>\n   and\n   <a href=\"https://arxiv.org/abs/2201.11903\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Chain of Thought Prompting\n   </a>\n   .\n  </li>\n </ul>\n <h2>\n  \u201cComplex\u201d vs. \u201cSimple\u201d Agent Interaction Techniques\n </h2>\n <p>\n  We classify techniques like ReAct are more\n  <em class=\"ow\">\n   complex and unconstrained:\n  </em>\n  this is because they perform iterative reasoning and also break the input into smaller steps. Complicated agent interaction loops allow for more\n  <em class=\"ow\">\n   freedom of behavior,\n  </em>\n  and\n  create an increased burden on the LLM being used. The pro of complex interaction frameworks is that they can be more general and handle a broader class of queries over simple tools. The con is that if the LLM is not up to par, then these frameworks are prone to making mistakes; unconstrained behavior can lead to unexpected results.\n </p>\n <p>\n  On the other end of the spectrum, you can imagine a\n  <em class=\"ow\">\n   simple and constrained\n  </em>\n  agent interaction mechanism, where the agent does one-step selection of the underlying tool to use, and returns the response from the tool. The agent essentially just acts as a router from the query to Tool. There are no steps to break down the question into smaller ones, and no iterative chain-of-thought loops. The pro here is that the model will likely make fewer errors. The con here is that the interaction technique allows for less freedom and imposes more constraints on behavior.\n </p>\n <h2>\n  Investigating Agent Interaction Techniques for Data Querying\n </h2>\n <p>\n  We at LlamaIndex are interested in how agents can help augment data tasks. More specifically, we are interested in how agents can help perform complex user queries over a diverse range of data sources. This includes not only asking questions over a single document, but being able to synthesize insights across multiple documents and return that to the user.\n </p>\n <p>\n  LlamaIndex query engines can be used as Tools within an agent construct to query your data (we provide\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/how_to/integrations/using_with_langchain.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   seamless integrations with LangChain\n  </a>\n  ). These Tools can vary in complexity. For instance, a\n  <em class=\"ow\">\n   simple\n  </em>\n  Tool could be our\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/how_to/integrations/vector_stores.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   vector store query engine\n  </a>\n  , which does top-k embedding retrieval from a vector store. A more\n  <em class=\"ow\">\n   advanced\n  </em>\n  tool could be a query engine over our graph data structure, which can be setup to\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/use_cases/queries.html#compare-contrast-queries\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   explicitly provide compare/contrast capabilities\n  </a>\n  over any subset of documents. The tool itself can contain \u201cagent-like\u201d decision-making capabilities under the hood. LlamaIndex provides a variety of modules around\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/query_engine/RouterQueryEngine.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   routing\n  </a>\n  ,\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/how_to/query/query_transformations.html#single-step-query-decomposition\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   query decomposition\n  </a>\n  , and\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/query_engine/sub_question_query_engine.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   multi-part query planning\n  </a>\n  .\n </p>\n <p>\n  In this blog post, we are interested in comparing the following approaches to designing agents and tools to see which approach can provide good answers to different user queries in a robust fashion:\n </p>\n <ul>\n  <li>\n   more\n   <em class=\"ow\">\n    complex and unconstrained\n   </em>\n   agent interaction (ReAct) over a set of\n   <em class=\"ow\">\n    simple\n   </em>\n   Tools\n  </li>\n  <li>\n   more\n   <em class=\"ow\">\n    simple and constrained\n   </em>\n   agent interaction (simple routing) that uses more\n   <em class=\"ow\">\n    complex\n   </em>\n   Tools\n  </li>\n </ul>\n <figure>\n  <figcaption class=\"qp fe qq qb qc qr qs be b bf z dt\">\n   Complex Agents with Simple Tools, Simple Agents with Complex Tools\n  </figcaption>\n </figure>\n <p>\n  Essentially what we are interested in is how much complexity can be pushed to the agent interaction layer vs. being left in the Tool layer. We explore the following concrete example: let\u2019s say the user query is to compare/contrast two different documents (a relatively complex query). If the set of Tools are all just vector indices over different documents, could the agent interaction loop figure out how to execute that query reliably against the vector indices? On the other hand, if we push the complexity down to the Tool layer, then we could\n  <em class=\"ow\">\n   explicitly\n  </em>\n  have a Tool that can perform \u201ccompare/contrast\u201d over your Documents. Then the burden on the agent is to simply call this Tool instead of interacting with a set of other tools in a more complex fashion.\n </p>\n <h2>\n  High-Level Findings\n </h2>\n <p>\n  The high-level finding is that\n  <strong>\n   less sophisticated agents need more constraints.\n  </strong>\n  More specifically, we found that using a GPT-3 powered agent in a ReAct loop did not provide good results over complex queries; it was not able to figure out the proper interaction pattern over the provided set of Tools in order to surface the results. Instead, by adding more constraints to the agent behavior and providing more sophistication in the Tool itself, we were able to get a GPT-3 agent to produce better results.\n </p>\n <p>\n  <strong>\n   Smarter agents require fewer constraints.\n  </strong>\n  We did find that GPT-4 agents with ReAct were able to provide better query results than GPT-3 agents when presented with a set of simple Tools over the data. This implies that more powerful agents may not need as many tools to \u201cexplicitly\u201d perform tasks when much of that logic can be handled in the agent interaction loop.\n </p>\n <h1>\n  Setup\n </h1>\n <p>\n  Our data consists of three Uber 10-Q filings (quarterly financial reports) in 2022: March, June, and September. We wish to execute different queries over this data; the bulk of these queries are around comparing different bits of information between these documents.\n </p>\n <pre><span class=\"qx nb gt qu b bf qy qz l ra rb\" id=\"78ce\">march_2022 = SimpleDirectoryReader(input_files=[\"../data/10q/uber_10q_march_2022.pdf\"]).load_data()\njune_2022 = SimpleDirectoryReader(input_files=[\"../data/10q/uber_10q_june_2022.pdf\"]).load_data()\nsept_2022 = SimpleDirectoryReader(input_files=[\"../data/10q/uber_10q_sept_2022.pdf\"]).load_data()</span></pre>\n <p>\n  We use LlamaIndex to define a vector index over each document, which just stores the document chunks + embeddings in a vector store. We can then query each vector index using a simple\n  <code class=\"cw rc rd re qu b\">\n   QueryEngine\n  </code>\n  . We create a tool for each of these\n  <code class=\"cw rc rd re qu b\">\n   QueryEngine\n  </code>\n  objects.\n </p>\n <pre><span class=\"qx nb gt qu b bf qy qz l ra rb\" id=\"7ecb\"># define indices\nmarch_index = GPTVectorStoreIndex.from_documents(march_2022)\njune_index = GPTVectorStoreIndex.from_documents(june_2022)\nsept_index = GPTVectorStoreIndex.from_documents(sept_2022)\n\n# define query engine\nmarch_engine = march_index.as_query_engine(similarity_top_k=3)\njune_engine = june_index.as_query_engine(similarity_top_k=3)\nsept_engine = sept_index.as_query_engine(similarity_top_k=3)</span></pre>\n <p>\n  We also define a\n  <code class=\"cw rc rd re qu b\">\n   ComposableGraph\n  </code>\n  over these three documents. The composable graph roughly follows the\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/use_cases/queries.html#compare-contrast-queries\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   guide described here\n  </a>\n  . This graph is explicitly setup to perform compare/contrast queries over these three documents.\n </p>\n <pre><span class=\"qx nb gt qu b bf qy qz l ra rb\" id=\"a152\">graph = ComposableGraph.from_indices(\n    GPTListIndex,\n    children_indices=[march_index, june_index, sept_index],\n    index_summaries=[\n        \"Provides information about Uber quarterly financials ending March 2022\",\n        \"Provides information about Uber quarterly financials ending June 2022\",\n        \"Provides information about Uber quarterly financials ending September 2022\"\n    ]\n)</span></pre>\n <p>\n  The graph can be queried with a\n  <code class=\"cw rc rd re qu b\">\n   ComposableGraphQueryEngine\n  </code>\n  :\n </p>\n <pre><span class=\"qx nb gt qu b bf qy qz l ra rb\" id=\"9190\"><span class=\"hljs-comment\"># define decompose_transform</span>\ndecompose_transform = DecomposeQueryTransform(verbose=<span class=\"hljs-literal\">True</span>)\n\n<span class=\"hljs-comment\"># define custom query engines</span>\ncustom_query_engines = {}\n<span class=\"hljs-keyword\">for</span> index <span class=\"hljs-keyword\">in</span> [march_index, june_index, sept_index]:\n    query_engine = index.as_query_engine(service_context=service_context)\n    query_engine = TransformQueryEngine(\n        query_engine,\n        query_transform=decompose_transform,\n        transform_extra_info={<span class=\"hljs-string\">'index_summary'</span>: index.index_struct.summary},\n    )\n    custom_query_engines[index.index_id] = query_engine\n\ncustom_query_engines[graph.root_id] = graph.root_index.as_query_engine(\n    service_context=service_context,\n    streaming=<span class=\"hljs-literal\">True</span>,\n)\n\n<span class=\"hljs-comment\"># define graph</span>\ng_engine = graph.as_query_engine(\n    custom_query_engines=custom_query_engines\n)</span></pre>\n <p>\n  We try the following agent setups:\n </p>\n <ul>\n  <li>\n   <strong>\n    GPT-3 ReAct agent:\n   </strong>\n   A zero-shot GPT-3 ReAct agent with three Tools: each Tool corresponds to the vector index over a 10-Q filing.\n  </li>\n  <li>\n   <strong>\n    GPT-4 ReAct agent:\n   </strong>\n   Same as above but using GPT-4 instead.\n  </li>\n  <li>\n   <strong>\n    Simple Router agent:\n   </strong>\n   A simple router \u201cagent\u201d with four Tools: the three Tools listed above + the\n   <code class=\"cw rc rd re qu b\">\n    ComposableGraphQueryEngine\n   </code>\n   explicitly setup to perform compare/contrast queries.\n  </li>\n </ul>\n <p>\n  The code snippets for initializing these agents are below. For the simple router agent, we use the native\n  <code class=\"cw rc rd re qu b\">\n   RouterQueryEngine\n  </code>\n  within LlamaIndex, though you should also be able to achieve similar results in LangChain through either the zero-shot agent (with tweaked settings) or the router chain.\n </p>\n <h2>\n  <strong>\n   GPT-3/GPT-4 ReAct Agent Setup\n  </strong>\n </h2>\n <pre><span class=\"qx nb gt qu b bf qy qz l ra rb\" id=\"4d83\"><span class=\"hljs-comment\"># initializing zero-shot ReAct agent</span>\n\nuber_config_sept = IndexToolConfig(\n    query_engine=sept_engine, \n    name=<span class=\"hljs-string\">f\"Uber 10Q September 2022\"</span>,\n    description=<span class=\"hljs-string\">f\"Provides information about Uber quarterly financials ending September 2022\"</span>,\n    tool_kwargs={<span class=\"hljs-string\">\"return_direct\"</span>: <span class=\"hljs-literal\">False</span>}\n)\nuber_config_june = IndexToolConfig(\n    query_engine=june_engine, \n    name=<span class=\"hljs-string\">f\"Uber 10Q June 2022\"</span>,\n    description=<span class=\"hljs-string\">f\"Provides information about Uber quarterly financials ending June 2022\"</span>,\n    tool_kwargs={<span class=\"hljs-string\">\"return_direct\"</span>: <span class=\"hljs-literal\">False</span>}\n)\nuber_config_march = IndexToolConfig(\n    query_engine=march_engine, \n    name=<span class=\"hljs-string\">f\"Uber 10Q March 2022\"</span>,\n    description=<span class=\"hljs-string\">f\"Provides information about Uber quarterly financials ending March 2022\"</span>,\n    tool_kwargs={<span class=\"hljs-string\">\"return_direct\"</span>: <span class=\"hljs-literal\">False</span>}\n)\n\ntoolkit = LlamaToolkit(\n    index_configs=[uber_config_sept, uber_config_june, uber_config_march],\n)\n\n<span class=\"hljs-comment\"># this is a light wrapper around `initialize_agent` in langchain (which defaults to zero-shot)</span>\nagent_chain = create_llama_agent(\n    toolkit,\n    llm, <span class=\"hljs-comment\"># can be GPT-3 or GPT-4 </span>\n    verbose=<span class=\"hljs-literal\">True</span>\n)</span></pre>\n <h2>\n  Simple Router Agent Setup\n </h2>\n <pre><span class=\"qx nb gt qu b bf qy qz l ra rb\" id=\"15e1\">\nquery_tool_sept = QueryEngineTool.from_defaults(\n    query_engine=sept_engine,\n    description=f\"Provides information about Uber quarterly financials ending September 2022\",\n)\nquery_tool_june = QueryEngineTool.from_defaults(\n    query_engine=june_engine,\n    description=f\"Provides information about Uber quarterly financials ending June 2022\",\n)\nquery_tool_march = QueryEngineTool.from_defaults(\n    query_engine=march_engine,\n    description=f\"Provides information about Uber quarterly financials ending March 2022\",\n)\nquery_tool_graph = QueryEngineTool.from_defaults(\n    query_engine=g_engine,\n    description=f\"Provides comparisons between Uber financials across quarters in 2022. Can be used to answer \"\n                 \"any questions that require analysis across multiple quarters.\",\n)\n\n# our \"router\" query engine is effectively a simple agent that can only perform routing\nquery_engine = RouterQueryEngine(\n    selector=LLMSingleSelector.from_defaults(),\n    query_engine_tools=[\n        query_tool_sept,\n        query_tool_june,\n        query_tool_march,\n        query_tool_graph\n    ]\n)</span></pre>\n <p>\n  Now that we\u2019ve described the setup, let\u2019s take a look at the results below!\n </p>\n <h1>\n  Findings and Experiments\n </h1>\n <p>\n  At a high-level, we find using GPT-3 in ReAct agents produces suboptimal results over these queries. They tend to exhibit the following characteristics:\n </p>\n <ul>\n  <li>\n   <strong>\n    Unpredictability in the set of chosen tools:\n   </strong>\n   The set of tools chosen can differ even if the questions are semantically similar, leading to variability in the responses.\n  </li>\n  <li>\n   <strong>\n    Lack of coverage in the set of chosen tools:\n   </strong>\n   Oftentimes we expect that a given question is able to make use of all three 10-Q statements, but only a subset of them are picked.\n  </li>\n  <li>\n   <strong>\n    Erroneous chain-of-thought processing:\n   </strong>\n   Sometimes the agent uses tools throughout the CoT process that are irrelevant to the question.\n  </li>\n </ul>\n <p>\n  In contrast, we find that GPT-4 ReAct agents provide answers that are more relevant, predictable, and exhibit fewer errors in intermediate results.\n </p>\n <p>\n  Finally, we find that using a simpler routing-only GPT-3 agent with access to an explicit \u201ccompare/contrast\u201d tool allows the agent to perform better.\n </p>\n <p>\n  As a reminder, full results are in the notebook:\n  <a href=\"https://colab.research.google.com/drive/1uP38k4nr8OPmXbY4dLoKKQW0F29WtNuY?usp=sharing\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://colab.research.google.com/drive/1uP38k4nr8OPmXbY4dLoKKQW0F29WtNuY?usp=sharing\n  </a>\n </p>\n <h2>\n  GPT-3 ReAct Agent Results\n </h2>\n <p>\n  <strong>\n   Query 1\n  </strong>\n </p>\n <pre><span class=\"qx nb gt qu b bf qy qz l ra rb\" id=\"19d6\">agent_chain.run(input=\"Analyze Uber revenue growth over the last few quarters\")</span></pre>\n <p>\n  Response:\n </p>\n <p>\n  We see that only the September 10-Q filing is chosen to answer the question. The September 10-Q does contain some information about revenue growth compared to the same period in 2021, but that doesn\u2019t explicitly answer the question, which is about revenue growth the past few quarters.\n </p>\n <p>\n  <strong>\n   Query 2\n  </strong>\n </p>\n <pre><span class=\"qx nb gt qu b bf qy qz l ra rb\" id=\"8ad0\">agent_chain.run(input=\"Analyze changes in risk factors for Uber\")</span></pre>\n <p>\n  Response:\n </p>\n <p>\n  The September and June 10-Q filings are chosen, but not March. Moreover, the answer is vague and doesn\u2019t provide much detail regarding concrete risk factors for Uber (and also mentions that the risk factors \u201chave changed over the past three quarters\u201d even though it\u2019s only using two Tools).\n </p>\n <p>\n  <strong>\n   Query 3\n  </strong>\n </p>\n <p>\n  In this query, we more explicitly showcase how slight changes in prompts can induce different chain-of-thought paths through different Tools, and as a result produce different answers.\n </p>\n <pre><span class=\"qx nb gt qu b bf qy qz l ra rb\" id=\"0f43\"># Prompt variation 1 \nagent_chain.run(input=\"Analyze Uber revenue growth and risk factors over time\")</span></pre>\n <p>\n  Response:\n </p>\n <pre><span class=\"qx nb gt qu b bf qy qz l ra rb\" id=\"7806\"># Prompt variation 2\nagent_chain.run(input=\"Analyze Uber revenue growth and risk factors over quarters\")</span></pre>\n <p>\n  The main difference between these two queries is \u201cover time\u201d versus \u201cover quarters.\u201d As we can see, not only are the selected Tools different between the two variations, but the inputs are different as well \u2014 in the first it\u2019s \u201cfinancials\u201d, and in the second it\u2019s \u201cRevenue growth and risk factors.\u201d\n </p>\n <p>\n  Since the Tool input in the first variant is unrelated to the question, the answer is similarly vague: \u201cUber\u2019s revenue growth and risk factors can be analyzed by comparing the financials\u2026\u201d\n </p>\n <p>\n  <strong>\n   Query 4:\n  </strong>\n </p>\n <p>\n  Here instead of asking a compare/contrast question let\u2019s just ask a question about a given statement.\n </p>\n <pre><span class=\"qx nb gt qu b bf qy qz l ra rb\" id=\"603f\">agent_chain.run(input=\"How much cash did Uber have in sept 2022?\")</span></pre>\n <p>\n  We see that the agent makes two errors 1) it is not able to supply an action input to each Tool, and 2) ends up looking through the June and March filings which are irrelevant to the question.\n </p>\n <h2>\n  GPT-4 ReAct Agent Results\n </h2>\n <p>\n  GPT-4 ReAct agents perform a lot better than GPT-3 agents. They comprehensively go through the set of available Tools, and provide much more detailed observation extraction and response synthesis.\n </p>\n <p>\n  We won\u2019t go through all of these examples, but they can be found in the example notebook!\n </p>\n <p>\n  <strong>\n   Query 1:\n  </strong>\n </p>\n <pre><span class=\"qx nb gt qu b bf qy qz l ra rb\" id=\"24a6\">agent_chain_gpt4.run(input=\"Analyze Uber revenue growth over the last few quarters\")</span></pre>\n <p>\n  Response:\n </p>\n <p>\n  Unlike the GPT-3 agent, here the GPT-4 agent at least goes through every filing and synthesizes the result.\n </p>\n <p>\n  <strong>\n   Query 2\n  </strong>\n </p>\n <pre><span class=\"qx nb gt qu b bf qy qz l ra rb\" id=\"4716\">agent_chain_gpt4.run(input=\"Analyze changes in risk factors for Uber\")</span></pre>\n <p>\n  Response:\n </p>\n <p>\n  Here the GPT-4 agent still only looks at September and June (and skips March), but the response is way more detailed, and references concrete facts within the report.\n </p>\n <h2>\n  GPT-3 Router Agent Results\n </h2>\n <p>\n  Reminder: the router agent doesn\u2019t do any CoT and has additional access to our ComposableGraph query engine, which can explicitly perform compare/contrast queries. Let\u2019s take a look at how this agent does.\n </p>\n <p>\n  <strong>\n   Query 1\n  </strong>\n </p>\n <pre><span class=\"qx nb gt qu b bf qy qz l ra rb\" id=\"2e7e\">response = query_engine.query(\"Analyze Uber revenue growth over the last few quarters\")</span></pre>\n <p>\n  Response (intermediate steps):\n </p>\n <pre><span class=\"qx nb gt qu b bf qy qz l ra rb\" id=\"9a3e\">Selecting query engine 3: Provides comparisons between Uber financials across quarters in 2022. Can be used to answer any questions that require analysis across multiple quarters..\n&gt; Current query: Analyze Uber revenue growth over the last few quarters\n&gt; New query:  What was Uber's revenue growth from the last quarter ending March 2022 compared to the previous quarter?\n&gt; Current query: Analyze Uber revenue growth over the last few quarters\n&gt; New query:  What was Uber's revenue growth from the last quarter ending June 2022 compared to the previous quarter?\n&gt; Current query: Analyze Uber revenue growth over the last few quarters\n&gt; New query:  What was Uber's revenue growth from the last few quarters ending September 2022?\n</span></pre>\n <p>\n  Final Response\n </p>\n <pre><span class=\"qx nb gt qu b bf qy qz l ra rb\" id=\"66e2\">Based on the limited context information provided, we can only analyze Uber's revenue growth for two specific quarters:\n\n1. For the quarter ending March 2022, Uber's revenue growth increased compared to the first quarter of 2021. However, the exact percentage of growth is not provided.\n\n2. For the quarter ending September 2022, Uber's revenue growth was 72%.\n\nUnfortunately, we do not have enough data to analyze the revenue growth for the quarters ending June 2022 or to compare it to the previous quarter. To provide a more comprehensive analysis, additional data on Uber's revenue growth for other quarters would be needed.</span></pre>\n <p>\n  To be fair, this response does not cover the month of June. But the fault of that lies with the implementation of the Tool itself rather than the agent interaction step. The agent\u2019s only job is to pick the right Tool to solve the task at hand, and it does that job correctly; it decides to choose the Tool corresponding to our\n  <code class=\"cw rc rd re qu b\">\n   ComposableGraphQueryEngine\n  </code>\n  abstraction (which can perform compare/contrast queries).\n </p>\n <p>\n  <strong>\n   Query 2\n  </strong>\n </p>\n <pre><span class=\"qx nb gt qu b bf qy qz l ra rb\" id=\"e2bd\">response = query_engine.query(\"Analyze changes in risk factors for Uber\")</span></pre>\n <p>\n  Response (intermediate steps):\n </p>\n <pre><span class=\"qx nb gt qu b bf qy qz l ra rb\" id=\"58e3\"><span class=\"hljs-variable constant_\">INFO</span><span class=\"hljs-symbol\">:llama_index</span>.query_engine.<span class=\"hljs-symbol\">router_query_engine:</span><span class=\"hljs-title class_\">Selecting</span> query engine <span class=\"hljs-number\">3</span>: <span class=\"hljs-title class_\">Provides</span> comparisons between <span class=\"hljs-title class_\">Uber</span> financials across quarters <span class=\"hljs-keyword\">in</span> <span class=\"hljs-number\">2022</span>. <span class=\"hljs-title class_\">Can</span> be used to answer any questions that <span class=\"hljs-keyword\">require</span> analysis across multiple quarters..\n<span class=\"hljs-title class_\">Selecting</span> query engine <span class=\"hljs-number\">3</span>: <span class=\"hljs-title class_\">Provides</span> comparisons between <span class=\"hljs-title class_\">Uber</span> financials across quarters <span class=\"hljs-keyword\">in</span> <span class=\"hljs-number\">2022</span>. <span class=\"hljs-title class_\">Can</span> be used to answer any questions that <span class=\"hljs-keyword\">require</span> analysis across multiple quarters..\n&amp;gt; <span class=\"hljs-title class_\">Current</span> <span class=\"hljs-symbol\">query:</span> <span class=\"hljs-title class_\">Analyze</span> changes <span class=\"hljs-keyword\">in</span> risk factors <span class=\"hljs-keyword\">for</span> <span class=\"hljs-title class_\">Uber</span>\n&amp;gt; <span class=\"hljs-title class_\">New</span> <span class=\"hljs-symbol\">query:</span>  <span class=\"hljs-title class_\">What</span> are the risk factors <span class=\"hljs-keyword\">for</span> <span class=\"hljs-title class_\">Uber</span> <span class=\"hljs-keyword\">in</span> the quarter ending <span class=\"hljs-title class_\">March</span> <span class=\"hljs-number\">2022</span>?\n&amp;gt; <span class=\"hljs-title class_\">Current</span> <span class=\"hljs-symbol\">query:</span> <span class=\"hljs-title class_\">Analyze</span> changes <span class=\"hljs-keyword\">in</span> risk factors <span class=\"hljs-keyword\">for</span> <span class=\"hljs-title class_\">Uber</span>\n&amp;gt; <span class=\"hljs-title class_\">New</span> <span class=\"hljs-symbol\">query:</span>  <span class=\"hljs-title class_\">What</span> are the risk factors <span class=\"hljs-keyword\">for</span> <span class=\"hljs-title class_\">Uber</span> <span class=\"hljs-keyword\">in</span> the quarter ending <span class=\"hljs-title class_\">June</span> <span class=\"hljs-number\">2022</span>?\n&amp;gt; <span class=\"hljs-title class_\">Current</span> <span class=\"hljs-symbol\">query:</span> <span class=\"hljs-title class_\">Analyze</span> changes <span class=\"hljs-keyword\">in</span> risk factors <span class=\"hljs-keyword\">for</span> <span class=\"hljs-title class_\">Uber</span>\n&amp;gt; <span class=\"hljs-title class_\">New</span> <span class=\"hljs-symbol\">query:</span>  <span class=\"hljs-title class_\">What</span> are the risk factors <span class=\"hljs-keyword\">for</span> <span class=\"hljs-title class_\">Uber</span> <span class=\"hljs-keyword\">in</span> the quarter ending <span class=\"hljs-title class_\">September</span> <span class=\"hljs-number\">2022</span>?</span></pre>\n <p>\n  Final Response\n </p>\n <pre><span class=\"qx nb gt qu b bf qy qz l ra rb\" id=\"65a0\">From the context information provided, we can analyze the changes in risk factors <span class=\"hljs-keyword\">for</span> Uber across the three quarters as follows:\n\n<span class=\"hljs-number\">1.</span> COVID<span class=\"hljs-number\">-19</span> pandemic: The ongoing pandemic remains a constant risk factor across all quarters, with its impact on driver <span class=\"hljs-keyword\">and</span> courier supply constraints, consumer demand, <span class=\"hljs-keyword\">and</span> government restrictions. However, the specific concerns related to the pandemic may evolve, such as the emergence of <span class=\"hljs-keyword\">new</span> virus variants <span class=\"hljs-keyword\">or</span> changes in vaccination rates.\n\n<span class=\"hljs-number\">2.</span> Driver <span class=\"hljs-keyword\">and</span> courier supply constraints: This risk factor is present in the quarter ending March <span class=\"hljs-number\">2022</span> but is <span class=\"hljs-keyword\">not</span> explicitly mentioned in the subsequent quarters. However, it may still be an underlying concern, especially <span class=\"hljs-keyword\">if</span> the pandemic continues to impact driver availability.\n\n<span class=\"hljs-number\">3.</span> Changes in consumer demand: This risk factor is mentioned in the quarter ending March <span class=\"hljs-number\">2022</span>, with an imbalance between Mobility <span class=\"hljs-keyword\">and</span> Delivery services. It is <span class=\"hljs-keyword\">not</span> explicitly mentioned in the subsequent quarters, but fluctuations in revenue due to changes in Gross Bookings <span class=\"hljs-keyword\">and</span> Mobility Trip volumes in the quarter ending June <span class=\"hljs-number\">2022</span> could be related to changes in consumer demand.\n\n<span class=\"hljs-number\">4.</span> Government restrictions <span class=\"hljs-keyword\">and</span> regulations: This risk factor is present in the quarter ending March <span class=\"hljs-number\">2022</span>, with the impact of restrictions on Mobility offerings. In the quarter ending June <span class=\"hljs-number\">2022</span>, the focus shifts to the potential impact of regulatory changes <span class=\"hljs-keyword\">and</span> legal claims, such as driver classification in the UK. In the quarter ending September <span class=\"hljs-number\">2022</span>, the risk factor of driver classification as employees, workers, <span class=\"hljs-keyword\">or</span> quasi-employees becomes more prominent.\n\n<span class=\"hljs-number\">5.</span> Uncertainty around future developments: This risk factor is mentioned in the quarter ending March <span class=\"hljs-number\">2022</span> but is <span class=\"hljs-keyword\">not</span> explicitly mentioned in the subsequent quarters. However, it may still be an underlying concern, especially with the ongoing impact of COVID<span class=\"hljs-number\">-19</span> <span class=\"hljs-keyword\">and</span> potential permanent changes to end-users<span class=\"hljs-number\">'</span> behavior.\n\n<span class=\"hljs-number\">6.</span> Accounting pronouncements: This risk factor is introduced in the quarter ending June <span class=\"hljs-number\">2022</span>, with the adoption <span class=\"hljs-keyword\">and</span> impact of recently issued accounting pronouncements potentially affecting Uber<span class=\"hljs-number\">'</span>s consolidated financial statements.\n\n<span class=\"hljs-number\">7.</span> Integration <span class=\"hljs-keyword\">and</span> performance of acquired businesses: This risk factor is introduced in the quarter ending June <span class=\"hljs-number\">2022</span>, with the acquisition of Transplace contributing to the increase in Freight revenue.\n\n<span class=\"hljs-number\">8.</span> Competition: This risk factor is introduced in the quarter ending September <span class=\"hljs-number\">2022</span>, highlighting the highly competitive nature of the mobility, delivery, <span class=\"hljs-keyword\">and</span> logistics industries, <span class=\"hljs-keyword\">and</span> the need to lower fares, offer incentives, <span class=\"hljs-keyword\">and</span> promotions to remain competitive.\n\n<span class=\"hljs-number\">9.</span> Significant losses <span class=\"hljs-keyword\">and</span> increasing operating expenses: This risk factor is introduced in the quarter ending September <span class=\"hljs-number\">2022</span>, emphasizing the financial challenges Uber faces in achieving profitability.</span></pre>\n <p>\n  The job of the agent is once again simple: it chooses the graph query engine that is optimized to perform \u201ccompare/contrast\u201d queries. The result is much more detailed than the result obtained from the GPT-3 ReAct agent (and is comparable to the GPT-4 ReAct agent). It contains a detailed analysis of all risk factors in bullet points, and offers a comparison across time for all bullet points.\n </p>\n <p>\n  <strong>\n   Query 3\n  </strong>\n </p>\n <pre><span class=\"qx nb gt qu b bf qy qz l ra rb\" id=\"ef96\">response = query_engine.query(\"Analyze Uber revenue growth and risk factors over the last few quarters\")</span></pre>\n <p>\n  Response (intermediate steps):\n </p>\n <pre><span class=\"qx nb gt qu b bf qy qz l ra rb\" id=\"8dab\">&gt; Current query: Analyze Uber revenue growth and risk factors over quarters\n&gt; New query:  What is Uber's revenue growth and risk factors for the quarter ending March 2022?\n&gt; Current query: Analyze Uber revenue growth and risk factors over quarters\n&gt; New query:  What is Uber's revenue growth and risk factors for the quarter ending March 2022?\n&gt; Current query: Analyze Uber revenue growth and risk factors over quarters\n&gt; New query:  What is Uber's revenue growth and risk factors for the quarter ending June 2022?\n&gt; Current query: Analyze Uber revenue growth and risk factors over quarters\n&gt; New query:  What is Uber's revenue growth and risk factors for the quarter ending June 2022?\n&gt; Current query: Analyze Uber revenue growth and risk factors over quarters\n&gt; New query:  What were Uber's total revenues for the quarter ending September 2022?\n&gt; Current query: Analyze Uber revenue growth and risk factors over quarters\n&gt; New query:  What were Uber's total revenues for the quarter ending September 2022?</span></pre>\n <p>\n  Final Response\n </p>\n <pre><span class=\"qx nb gt qu b bf qy qz l ra rb\" id=\"c81b\">Uber<span class=\"hljs-string\">'s revenue growth has been impressive over the past few quarters, with the quarter ending March 2022 showing a 136% year-over-year growth and the quarter ending June 2022 showing a 105% year-over-year growth. The revenue for the quarter ending June 2022 was $8,343 million. \n\nThe risk factors for Uber'</span>s business have been largely related to the ongoing impacts of the COVID-<span class=\"hljs-number\">19</span> pandemic, including reduced <span class=\"hljs-keyword\">global</span> demand <span class=\"hljs-keyword\">for</span> Mobility rides, supply constraints, <span class=\"hljs-keyword\">and</span> potential permanent changes to end-user behavior. These risk factors have been present in both the quarter ending March <span class=\"hljs-number\">2022</span> <span class=\"hljs-keyword\">and</span> the quarter ending June <span class=\"hljs-number\">2022</span>, though the specific risks have varied slightly.</span></pre>\n <h1>\n  Concluding Thoughts\n </h1>\n <p>\n  ReAct-based agents offer a powerful, general reasoning loop, and have the potential to solve complex tasks over your data. But they tend to only work reliably with more powerful language models such as GPT-4. Less sophisticated models (e.g. GPT-3) will make more unpredictable and erroneous decisions, leading to subpar query performance over your data sources.\n </p>\n <p>\n  Agents implemented with \u201cdumber\u201d models need more interaction constraints in order to make more reliable, less erroneous decisions. We find that if we explicitly constrain the agent interface and push the complexity down to the Tool layer, we can still create agents that offer good performance over your data.\n </p>\n <p>\n  Of course, this is just an initial analysis and there\u2019s a few caveats/limitations:\n </p>\n <ul>\n  <li>\n   You may be able to \u201cprompt hack\u201d the default ReAct loop to get more consistent results, and we did not try that.\n  </li>\n  <li>\n   We only tested this over a set of three financial documents. There\u2019s a lot more work that needs to be done if we want to test this out on thousands of docs.\n  </li>\n  <li>\n   We only compared GPT-3 and GPT-4, there\u2019s so many more models to compare/benchmark, e.g ChatGPT, any open-source model, Anthropic Claude, etc.\n  </li>\n  <li>\n   We did not test out other agent interaction patterns besides ReAct: \u201cplan and solve\u201d agents (though we do have similar formulations in LlamaIndex), AutoGPT-like task management, and more.\n  </li>\n </ul>\n <p>\n  Whether you\u2019ve run into similar findings or you disagree with our analysis, let us know! We\u2019d love to facilitate this discussion on our\n  <a href=\"https://discord.gg/dGcwcsnxhU\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Discord\n  </a>\n  .\n </p>\n <h2>\n  Notebook Walkthrough\n </h2>\n <p>\n  You can find the full notebook walkthrough here:\n  <a href=\"https://colab.research.google.com/drive/1uP38k4nr8OPmXbY4dLoKKQW0F29WtNuY?usp=sharing\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://colab.research.google.com/drive/1uP38k4nr8OPmXbY4dLoKKQW0F29WtNuY?usp=sharing\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 37923, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58797dbc-24cc-4e0c-911a-c75a82476b66": {"__data__": {"id_": "58797dbc-24cc-4e0c-911a-c75a82476b66", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.html", "file_name": "easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.html", "file_type": "text/html", "file_size": 20138, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.html", "file_name": "easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.html", "file_type": "text/html", "file_size": 20138, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "5b20e24d12c344d9469d3e66539504ad9924c8cf02c63061f64aece86c17c79a", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  <a href=\"https://ai.meta.com/llama/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Llama 2\n  </a>\n  is a huge milestone in the advancement of open-source LLMs. The biggest model and its finetuned variants sit at the top of the\n  <a href=\"https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Hugging Face Open LLM Leaderboard\n  </a>\n  . Multiple benchmarks show that it is approaching GPT-3.5 (or in some cases even surpassing it) in terms of performance. All of this means that open-source LLMs are an increasingly viable and reliable option for use in complex LLM applications, from RAG systems to agents.\n </p>\n <h1>\n  Context: Llama-2\u20137B is Not Good at Text-to-SQL\n </h1>\n <p>\n  A downside of the smallest Llama 2 model (7B parameters), however, is that it\u2019s not very good at generating SQL, making it impractical for structured analytics use cases. As an example, we tried prompting Llama 2 to generate the correct SQL statement given the following prompt template:\n </p>\n <pre><span class=\"py os gt pv b bf pz qa l qb qc\" id=\"08e7\">You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. \n\nYou must output the SQL query that answers the question.\n\n### Input:\n{input}\n\n### Context:\n{context}\n\n### Response:</span></pre>\n <p>\n  Here we plugged in a sample entry from the\n  <a href=\"https://huggingface.co/datasets/b-mc2/sql-create-context\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   sql-create-context dataset\n  </a>\n  .\n </p>\n <pre><span class=\"py os gt pv b bf pz qa l qb qc\" id=\"72dd\">input: In 1981 which team picked overall 148?\ncontext: CREATE TABLE table_name_8 (team VARCHAR, year VARCHAR, overall_pick VARCHAR)</span></pre>\n <p>\n  Meanwhile, here is the generated output vs. correct output:\n </p>\n <pre><span class=\"py os gt pv b bf pz qa l qb qc\" id=\"416a\">Generated output: SELECT * FROM `table_name_8` WHERE <span class=\"hljs-string\">'1980'</span> = YEAR AND TEAM = <span class=\"hljs-string\">\"Boston Celtics\"</span> ORDER BY OVERALL_PICK DESC LIMIT <span class=\"hljs-number\">1</span>;\n\nCorrect output: SELECT team FROM table_name_8 WHERE year = <span class=\"hljs-number\">1981</span> AND overall_pick = <span class=\"hljs-string\">\"148\"</span></span></pre>\n <p>\n  This is clearly not ideal. Unlike ChatGPT and GPT-4, Llama 2 does not reliably produce well-formatted and correct SQL outputs.\n </p>\n <p>\n  This is exactly where fine-tuning comes in \u2014 given a proper corpus of text-to-SQL data, we can teach Llama 2 to be better at generating SQL outputs from natural language. At a high-level, fine-tuning involves modifying the weights of the model in some capacity. There are different ways to finetune models, from updating all parameters of the network, to a subset of the parameters, to only finetuning additional parameters (e.g.\n  <a href=\"https://arxiv.org/abs/2106.09685\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   how LoRA works\n  </a>\n  ).\n </p>\n <p>\n  Once the model is finetuned, it can still be plugged into a downstream LLM application. That is exactly what this tutorial aims to show. It is a step more involved than our existing tutorials which have primarily focused on \u201cin-context learning\u201d and \u201cretrieval-augmentation\u201d use cases \u2014 freezing the model itself but focusing on the orchestration of data into the input prompt. Finetuning can have a high learning curve and also require a lot of compute. This tutorial makes it as easy as possible to get started.\n </p>\n <h1>\n  Tutorial Overview\n </h1>\n <p>\n  In this tutorial, we show you how you can finetune Llama 2 on a text-to-SQL dataset, and then use it for structured analytics against any SQL database using the capabilities of\n  <a href=\"https://github.com/jerryjliu/llama_index\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaIndex\n  </a>\n  .\n </p>\n <p>\n  Here is the stack that we use:\n </p>\n <ul>\n  <li>\n   <code class=\"cw qg qh qi pv b\">\n    <a href=\"https://huggingface.co/datasets/b-mc2/sql-create-context\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n     b-mc2/sql-create-context\n    </a>\n   </code>\n   <a href=\"https://huggingface.co/datasets/b-mc2/sql-create-context\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    from Hugging Face datasets\n   </a>\n   as the training dataset\n  </li>\n  <li>\n   <a href=\"https://github.com/openlm-research/open_llama\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    OpenLLaMa\n   </a>\n   <code class=\"cw qg qh qi pv b\">\n    open_llama_7b_v2\n   </code>\n   as the base model\n  </li>\n  <li>\n   <a href=\"https://github.com/huggingface/peft\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    PEFT for efficient finetuning\n   </a>\n  </li>\n  <li>\n   <a href=\"https://modal.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Modal\n   </a>\n   for handling all cloud compute/orchestration for finetuning. And also for the excellent reference\n   <a href=\"https://github.com/modal-labs/doppel-bot\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    doppel-bot repo\n   </a>\n   .\n  </li>\n  <li>\n   <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaIndex\n   </a>\n   for text-to-SQL inference against any SQL database.\n  </li>\n </ul>\n <p>\n  Special mention to the awesome\n  <a href=\"https://www.anyscale.com/blog/fine-tuning-llama-2-a-comprehensive-case-study-for-tailoring-models-to-unique-applications\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Llama 2 tutorial from Anyscale that helped to inspire this project\n  </a>\n  .\n </p>\n <p>\n  All of our materials can be found in our Github repo:\n  <a href=\"https://github.com/run-llama/modal_finetune_sql\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://github.com/run-llama/modal_finetune_sql\n  </a>\n  (again emphasizing that this is adapted from\n  <a href=\"https://github.com/modal-labs/doppel-bot\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   doppel-bot\n  </a>\n  ). Also, the full tutorial can be found in our\n  <a href=\"https://github.com/run-llama/modal_finetune_sql/blob/main/tutorial.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Jupyter notebook guide\n  </a>\n  . Make sure to check it out!\n </p>\n <p>\n  As mentioned above, performing finetuning does require quite a few steps. Our goal is to make this as straightforward as possible to follow and use out of the box. We don\u2019t cover all the nitty gritty detailsof Modal, PEFT, the finetuning procedure itself, etc. but we do give a rough overview.\n </p>\n <p>\n  There are also certainly higher-level APIs that we could\u2019ve used (e.g. OpenAI, Lamini) in order to achieve this task. There\u2019s plenty of room for followup tutorials to cover these topics!\n </p>\n <h2>\n  Step 1: Loading Training Data for Finetuning LLaMa\n </h2>\n <p>\n  The first step here is to open up the\n  <a href=\"https://github.com/run-llama/modal_finetune_sql/blob/main/tutorial.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Jupyter notebook\n  </a>\n  . The notebook is organized into a series of runnable scripts that each perform the steps needed to load data.\n </p>\n <p>\n  Our code uses Modal for every step of the orchestration, and Modal is best used on top of the Python scripts themselves. That is why a lot of these cells don\u2019t contain Python blocks of their own.\n </p>\n <p>\n  First we use Modal to load in the\n  <code class=\"cw qg qh qi pv b\">\n   b-mc2/sql-create-context\n  </code>\n  dataset. This is a simple task that just loads in the dataset and formats it into a\n  <code class=\"cw qg qh qi pv b\">\n   .jsonl\n  </code>\n  file.\n </p>\n <pre><span class=\"py os gt pv b bf pz qa l qb qc\" id=\"9cb1\">modal run src.load_data_sql --data-dir \"data_sql\"</span></pre>\n <p>\n  As we can see, under the hood the task is quite straightforward:\n </p>\n <pre><span class=\"py os gt pv b bf pz qa l qb qc\" id=\"47a3\"><span class=\"hljs-comment\"># Modal stubs allow our function to run remotely</span>\n<span class=\"hljs-meta\">@stub.function(<span class=\"hljs-params\">\n    retries=Retries(<span class=\"hljs-params\">\n        max_retries=<span class=\"hljs-number\">3</span>,\n        initial_delay=<span class=\"hljs-number\">5.0</span>,\n        backoff_coefficient=<span class=\"hljs-number\">2.0</span>,\n    </span>),\n    timeout=<span class=\"hljs-number\">60</span> * <span class=\"hljs-number\">60</span> * <span class=\"hljs-number\">2</span>,\n    network_file_systems={VOL_MOUNT_PATH.as_posix(): output_vol},\n    cloud=<span class=\"hljs-string\">\"gcp\"</span>,\n</span>)</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">load_data_sql</span>(<span class=\"hljs-params\">data_dir: <span class=\"hljs-built_in\">str</span> = <span class=\"hljs-string\">\"data_sql\"</span></span>):\n    <span class=\"hljs-keyword\">from</span> datasets <span class=\"hljs-keyword\">import</span> load_dataset\n\n    dataset = load_dataset(<span class=\"hljs-string\">\"b-mc2/sql-create-context\"</span>)\n\n    dataset_splits = {<span class=\"hljs-string\">\"train\"</span>: dataset[<span class=\"hljs-string\">\"train\"</span>]}\n    out_path = get_data_path(data_dir)\n\n    out_path.parent.mkdir(parents=<span class=\"hljs-literal\">True</span>, exist_ok=<span class=\"hljs-literal\">True</span>)\n\n    <span class=\"hljs-keyword\">for</span> key, ds <span class=\"hljs-keyword\">in</span> dataset_splits.items():\n        <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(out_path, <span class=\"hljs-string\">\"w\"</span>) <span class=\"hljs-keyword\">as</span> f:\n            <span class=\"hljs-keyword\">for</span> item <span class=\"hljs-keyword\">in</span> ds:\n                newitem = {\n                    <span class=\"hljs-string\">\"input\"</span>: item[<span class=\"hljs-string\">\"question\"</span>],\n                    <span class=\"hljs-string\">\"context\"</span>: item[<span class=\"hljs-string\">\"context\"</span>],\n                    <span class=\"hljs-string\">\"output\"</span>: item[<span class=\"hljs-string\">\"answer\"</span>],\n                }\n                f.write(json.dumps(newitem) + <span class=\"hljs-string\">\"\\n\"</span>)</span></pre>\n <h2>\n  Step 2: Run Finetuning Script\n </h2>\n <p>\n  The next step is to run our finetuning script on the parsed dataset.\n </p>\n <pre><span class=\"py os gt pv b bf pz qa l qb qc\" id=\"6964\">modal run src.finetune_sql --data-dir \"data_sql\" --model-dir \"model_sql\"</span></pre>\n <p>\n  The finetuning script performs the following steps.\n </p>\n <p>\n  <strong>\n   Splits the dataset into training and validation splits\n  </strong>\n </p>\n <pre><span class=\"py os gt pv b bf pz qa l qb qc\" id=\"596b\">train_val = data[\"train\"].train_test_split(test_size=val_set_size, shuffle=True, seed=42)\ntrain_data = train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\nval_data = train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)</span></pre>\n <p>\n  <strong>\n   Formats each split into tuples of (input prompt, label):\n  </strong>\n  The input query and context are formatted into the same input prompt. The input prompt is then tokenized, and the labels are set to the exact same as the input prompt \u2014 this allows the model to train on next-token prediction.\n </p>\n <pre><span class=\"py os gt pv b bf pz qa l qb qc\" id=\"22f3\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">generate_and_tokenize_prompt</span>(<span class=\"hljs-params\">data_point</span>):\n  full_prompt = generate_prompt_sql(\n      data_point[<span class=\"hljs-string\">\"input\"</span>],\n      data_point[<span class=\"hljs-string\">\"context\"</span>],\n      data_point[<span class=\"hljs-string\">\"output\"</span>],\n  )\n  tokenized_full_prompt = tokenize(full_prompt)\n  <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> train_on_inputs:\n      <span class=\"hljs-keyword\">raise</span> NotImplementedError(<span class=\"hljs-string\">\"not implemented yet\"</span>)\n  <span class=\"hljs-keyword\">return</span> tokenized_full_prompt</span></pre>\n <p>\n  The input prompt is the exact same as what was given at the top of this blog.\n </p>\n <p>\n  When the finetuning script is run, the model is saved in the remote cloud directory specified by model_dir (which is set to a default value if not specified).\n </p>\n <h2>\n  Step 3: Evaluation\n </h2>\n <p>\n  The model has been finetuned and can be served from the cloud. We can run some basic evaluations using sample data from sql-create-context to compare the performance of the finetuned model vs. the baseline Llama 2 model.\n </p>\n <pre><span class=\"py os gt pv b bf pz qa l qb qc\" id=\"a768\">modal run src.eval_sql::main</span></pre>\n <p>\n  The results demonstrate a massive improvement for the finetuned model:\n </p>\n <pre><span class=\"py os gt pv b bf pz qa l qb qc\" id=\"9cf8\">Input <span class=\"hljs-number\">1</span>: {<span class=\"hljs-string\">'input'</span>: <span class=\"hljs-string\">'Which region (year) has Abigail at number 7, Sophia at number 1 and Aaliyah at number 5?'</span>, <span class=\"hljs-string\">'context'</span>: <span class=\"hljs-string\">'CREATE TABLE table_name_12 (region__year_ VARCHAR, no_5 VARCHAR, no_7 VARCHAR, no_1 VARCHAR)'</span>, <span class=\"hljs-string\">'output'</span>: <span class=\"hljs-string\">'SELECT region__year_ FROM table_name_12 WHERE no_7 = \"abigail\" AND no_1 = \"sophia\" AND\nno_5 = \"aaliyah\"'</span>}\nOutput <span class=\"hljs-number\">1</span> (finetuned model): SELECT region__year_ FROM table_name_12 WHERE no_7 = <span class=\"hljs-string\">\"abigail\"</span> AND no_1 = <span class=\"hljs-string\">\"aaliyah\"</span> AND no_5 = <span class=\"hljs-string\">\"sophia\"</span>\nOutput <span class=\"hljs-number\">1</span> (base model): SELECT * FROM table_name_12 WHERE region__year = <span class=\"hljs-string\">'2018'</span> AND no_5 = <span class=\"hljs-string\">'Abigail'</span> AND no_7 = <span class=\"hljs-string\">'Sophia'</span> AND no_1 = <span class=\"hljs-string\">'Aaliyah'</span>;\n\n\nInput <span class=\"hljs-number\">2</span>: {<span class=\"hljs-string\">'input'</span>: <span class=\"hljs-string\">'Name the result/games for 54741'</span>, <span class=\"hljs-string\">'context'</span>: <span class=\"hljs-string\">'CREATE TABLE table_21436373_11 (result_games VARCHAR, attendance VARCHAR)'</span>, <span class=\"hljs-string\">'output'</span>: <span class=\"hljs-string\">'SELECT result_games FROM table_21436373_11 WHERE attendance = 54741'</span>}\nOutput <span class=\"hljs-number\">2</span> (finetuned model): SELECT result_games FROM table_21436373_11 WHERE attendance = <span class=\"hljs-string\">\"54741\"</span>\nOutput <span class=\"hljs-number\">2</span> (base model): SELECT * FROM table_21436373_11 WHERE result_games = <span class=\"hljs-string\">'name'</span> AND attendance &amp;gt; <span class=\"hljs-number\">0</span>;</span></pre>\n <p>\n  Whereas the base model produces wrongly formatted outputs, or incorrect SQL statements,\n </p>\n <p>\n  the finetuned model is able to produce outputs that are much closer to that of the expected output.\n </p>\n <h2>\n  Step 4: Integrating the Finetuned Model with LlamaIndex\n </h2>\n <p>\n  We can now use this model in LlamaIndex for text-to-SQL over any database.\n </p>\n <p>\n  We first define a test SQL database that we can then use to test the inference capabilities of the model.\n </p>\n <p>\n  We create a toy\n  <code class=\"cw qg qh qi pv b\">\n   city_stats\n  </code>\n  table that contains city name, population, and country information, and populate it with a few sample cities.\n </p>\n <pre><span class=\"py os gt pv b bf pz qa l qb qc\" id=\"f910\">db_file = <span class=\"hljs-string\">\"cities.db\"</span>\nengine = <span class=\"hljs-title function_\">create_engine</span>(f<span class=\"hljs-string\">\"sqlite:///{db_file}\"</span>)\nmetadata_obj = <span class=\"hljs-title class_\">MetaData</span>()\n# create city <span class=\"hljs-variable constant_\">SQL</span> table\ntable_name = <span class=\"hljs-string\">\"city_stats\"</span>\ncity_stats_table = <span class=\"hljs-title class_\">Table</span>(\n    table_name,\n    metadata_obj,\n    <span class=\"hljs-title class_\">Column</span>(<span class=\"hljs-string\">\"city_name\"</span>, <span class=\"hljs-title class_\">String</span>(<span class=\"hljs-number\">16</span>), primary_key=<span class=\"hljs-title class_\">True</span>),\n    <span class=\"hljs-title class_\">Column</span>(<span class=\"hljs-string\">\"population\"</span>, <span class=\"hljs-title class_\">Integer</span>),\n    <span class=\"hljs-title class_\">Column</span>(<span class=\"hljs-string\">\"country\"</span>, <span class=\"hljs-title class_\">String</span>(<span class=\"hljs-number\">16</span>), nullable=<span class=\"hljs-title class_\">False</span>),\n)\nmetadata_obj.<span class=\"hljs-title function_\">create_all</span>(engine)</span></pre>\n <p>\n  This is stored in a\n  <code class=\"cw qg qh qi pv b\">\n   cities.db\n  </code>\n  file.\n </p>\n <p>\n  We can then use Modal to load both the finetuned model and this database file into the\n  <code class=\"cw qg qh qi pv b\">\n   NLSQLTableQueryEngine\n  </code>\n  in LlamaIndex - this query engine allows users easily start performing text-to-SQL over a given database.\n </p>\n <pre><span class=\"py os gt pv b bf pz qa l qb qc\" id=\"4eb3\">modal run src.inference_sql_llamaindex::main --query <span class=\"hljs-string\">\"Which city has the highest population?\"</span> --sqlite-file-path <span class=\"hljs-string\">\"nbs/cities.db\"</span> --model-dir <span class=\"hljs-string\">\"model_sql\"</span> --use-finetuned-model True</span></pre>\n <p>\n  We get a response like the following:\n </p>\n <pre><span class=\"py os gt pv b bf pz qa l qb qc\" id=\"ef94\"><span class=\"hljs-variable constant_\">SQL</span> <span class=\"hljs-title class_\">Query</span>: <span class=\"hljs-variable constant_\">SELECT</span> <span class=\"hljs-title function_\">MAX</span>(population) <span class=\"hljs-variable constant_\">FROM</span> city_stats <span class=\"hljs-variable constant_\">WHERE</span> country = <span class=\"hljs-string\">\"United States\"</span>\n<span class=\"hljs-title class_\">Response</span>: [(<span class=\"hljs-number\">2679000</span>,)]</span></pre>\n <h1>\n  Conclusion\n </h1>\n <p>\n  And that\u2019s basically it! This tutorial provides a very high-level way for you to get started finetuning a Llama 2 model on generating SQL statements, and showcases end-to-end how you can plug it into your text-to-SQL workflows with LlamaIndex.\n </p>\n <p>\n  <strong>\n   Resources\n  </strong>\n </p>\n <p>\n  For the sake of completeness we\u2019re linking all of our resources again here.\n </p>\n <p>\n  Tutorial repo:\n  <a href=\"https://github.com/run-llama/modal_finetune_sql\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://github.com/run-llama/modal_finetune_sql\n  </a>\n  (adapted from\n  <a href=\"https://modal.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   doppel-bot\n  </a>\n  ).\n </p>\n <p>\n  <a href=\"https://github.com/run-llama/modal_finetune_sql/blob/main/tutorial.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Jupyter notebook guide\n  </a>\n  .\n </p>\n <p>\n  Stack:\n </p>\n <ul>\n  <li>\n   <code class=\"cw qg qh qi pv b\">\n    [b-mc2/sql-create-context\n   </code>\n   from Hugging Face datasets](\n   <a href=\"https://huggingface.co/datasets/b-mc2/sql-create-context\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    https://huggingface.co/datasets/b-mc2/sql-create-context\n   </a>\n   )\n  </li>\n  <li>\n   <a href=\"https://github.com/openlm-research/open_llama\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    OpenLLaMa\n   </a>\n  </li>\n  <li>\n   <a href=\"https://github.com/huggingface/peft\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    PEFT\n   </a>\n  </li>\n  <li>\n   <a href=\"https://modal.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Modal\n   </a>\n   (+\n   <a href=\"https://github.com/modal-labs/doppel-bot\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    doppel-bot repo\n   </a>\n   ).\n  </li>\n  <li>\n   <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaIndex\n   </a>\n  </li>\n </ul>\n <p>\n  Special mention:\n  <a href=\"https://www.anyscale.com/blog/fine-tuning-llama-2-a-comprehensive-case-study-for-tailoring-models-to-unique-applications\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Llama 2 tutorial from Anyscale\n  </a>\n  .\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 20107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10abc301-d5d8-4f47-98f0-448a326fae47": {"__data__": {"id_": "10abc301-d5d8-4f47-98f0-448a326fae47", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.html", "file_name": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.html", "file_type": "text/html", "file_size": 34831, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.html", "file_name": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.html", "file_type": "text/html", "file_size": 34831, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "10fcc17d09c9359d561b7fda943a6bc421cdb1374a1f2f27b5fbd1cdae26b2d8", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  In this article I wanted to share the process of adding new data loaders to LlamaIndex. First we\u2019ll look at what LlamaIndex is and try a simple example of providing additional context to an LLM query using a simple CSV loader. Then we look at how easy it is to add a new loader for graph databases to LlamaIndex. And lastly we try that new loader and another loader for GraphQL APIs that I added in practice and see how their extra context can help an LLM answer questions better.\n </p>\n <h1>\n  Background/Context\n </h1>\n <p>\n  I was listening to the\n  <a href=\"https://medium.com/llamaindex-blog/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595\" rel=\"noopener\">\n   \"This Week in ML\" (twiml) Podcast\n  </a>\n  where\n  <a href=\"https://medium.com/u/e76da1c45ef7?source=post_page-----bcaecec262d7--------------------------------\" rel=\"noopener\" target=\"_blank\">\n   Jerry Liu\n  </a>\n  from LlamaIndex (previously GPT-Index) explained the ideas behind the library to enrich query contexts to LLMs with data from any number of sources.\n </p>\n <p>\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/index.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaIndex\n  </a>\n  is a toolkit to augment LLMs with your own (private) data using in-context learning. It takes care of selecting the right context to retrieve from large knowledge bases. To achieve that it utilizes a number of connectors or loaders (from\n  <a href=\"https://llamahub.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaHub\n  </a>\n  ) and data structures (indices) to efficiently provide the pre-processed data as\n  <code class=\"cw pz qa qb qc b\">\n   Documents\n  </code>\n  .\n </p>\n <p>\n  Each type of index stores documents in different ways, e.g via embeddings for vector search, as a simple list or graph or tree structure. Those indices are used as query interface to the LLM, transparently embedding the relevant context.\n </p>\n <p>\n  Besides the higher quality response from the LLM, you get also the documents returned that have been used to construct the answer. LlamaIndex also allows chain of thought reasoning, compare/contrast queries, and natural language querying of databases.\n </p>\n <p>\n  See also this presentation from Jerry:\n </p>\n <p>\n  All the code for the blog post is available in this\n  <a href=\"https://colab.research.google.com/drive/1NUrIoiOh692LaQkBHEmnD-5IuLBpBqGJ#scrollTo=JN4gqQF-NRwj\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Colab Notebook\n  </a>\n  .\n </p>\n <h1>\n  Using a Basic CSV Loader\n </h1>\n <p>\n  Here is an example of using a basic CSV loader to provide documents for LlamaIndex.\n </p>\n <p>\n  In our Notebook we download the\n  <code class=\"cw pz qa qb qc b\">\n   countries.csv\n  </code>\n  via the\n  <a href=\"https://annexare.github.io/Countries/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Countries List Project\n  </a>\n  (MIT) (\n  <a href=\"https://raw.githubusercontent.com/annexare/Countries/master/dist/countries.csv\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   raw source\n  </a>\n  ).\n </p>\n <p>\n  Our dependencies are\n  <code class=\"cw pz qa qb qc b\">\n   llama-index\n  </code>\n  and\n  <code class=\"cw pz qa qb qc b\">\n   python-dotenv\n  </code>\n  .\n </p>\n <pre><span class=\"qv oj gt qc b bf qw qx l qy qz\" id=\"883e\">!pip install llama-index==0.6.19 python-dotenv</span></pre>\n <p>\n  We need to provide our OpenAI-api key, to avoid accidentally leaking it in the notebook, I uploaded an\n  <code class=\"cw pz qa qb qc b\">\n   openai.env\n  </code>\n  file and use the\n  <code class=\"cw pz qa qb qc b\">\n   dotenv\n  </code>\n  library to load the contents as environment variables.\n </p>\n <p>\n  In the next step we load the env file and prepare the OpenAI\n  <code class=\"cw pz qa qb qc b\">\n   ChatGPTLLMPredictor\n  </code>\n  (using\n  <code class=\"cw pz qa qb qc b\">\n   gpt-3.5-turbo\n  </code>\n  by default) and add it to the\n  <code class=\"cw pz qa qb qc b\">\n   ServiceContext\n  </code>\n  .\n </p>\n <pre><span class=\"qv oj gt qc b bf qw qx l qy qz\" id=\"cbfa\"><span class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\">from</span> pathlib <span class=\"hljs-keyword\">import</span> Path\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> GPTVectorStoreIndex, SimpleDirectoryReader, ServiceContext, GPTListIndex\n<span class=\"hljs-keyword\">from</span> llama_index.llm_predictor.chatgpt <span class=\"hljs-keyword\">import</span> ChatGPTLLMPredictor\n<span class=\"hljs-keyword\">from</span> dotenv <span class=\"hljs-keyword\">import</span> load_dotenv\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> download_loader\n\nload_dotenv(<span class=\"hljs-string\">\"openai.env\"</span>)\n\nllm_predictor = ChatGPTLLMPredictor()\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)</span></pre>\n <p>\n  Now we can use the loader to load the CSV and turn it into documents, create an an GPT Index (\n  <code class=\"cw pz qa qb qc b\">\n   VectorStoreIndex\n  </code>\n  in this case), which LlamaIndex can then use to retrieve the relevant information to pass along in the context to the LLM.\n </p>\n <p>\n  Initializing CSV Loader and GPTVectorStoreIndex\n </p>\n <pre><span class=\"qv oj gt qc b bf qw qx l qy qz\" id=\"2441\">SimpleCSVReader = download_loader(<span class=\"hljs-string\">\"SimpleCSVReader\"</span>)\nloader = SimpleCSVReader(concat_rows=<span class=\"hljs-literal\">False</span>)\ndocuments = loader.load_data(file=Path(<span class=\"hljs-string\">'./countries.csv'</span>))\n\n<span class=\"hljs-built_in\">print</span>(documents)\nindex = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)</span></pre>\n <p>\n  Documents from the CSV Loader\n </p>\n <pre><span class=\"qv oj gt qc b bf qw qx l qy qz\" id=\"f675\">[Document(text='country, capital, type', doc_id='67c30c68-7d9f-4906-945b-9affc96f95d2', embedding=None, doc_hash='3a506ebea9c04655b51406d79fdf5e3a87c3d8ff5b5387aace3e5a79711a21b8', extra_info=None),\nDocument(text='Abkhazia, Sukhumi, countryCapital', doc_id='6e6be4b5-051f-48e0-8774-6d48e0444785', embedding=None, doc_hash='ea387d0eab94cc6c59f98c473ac1f0ee64093901673b43e1c0d163bbc203026e', extra_info=None),\n...]</span></pre>\n <p>\n  The CSV loader didn\u2019t create one Document per CSV row by default, but only one for the whole document, but you could configure it so that it turned the CSV into one document per row.\n </p>\n <p>\n  LlamaIndex supports much more involved setups of different kinds of indexes, allows to chain them and even conditionally select one or the other. Here we just do the bare minimum to demonstrate our loaders.\n </p>\n <p>\n  After setting up the indices with the appropriate loaders, and connected indexes, we now can use the index as an LLM query engine and execute our user query.\n </p>\n <p>\n  To demonstrate that the LLM still is able to use its world knowledge, we can ask in a mix of English (System), German (Question) and French (requested Answer).\n </p>\n <pre><span class=\"qv oj gt qc b bf qw qx l qy qz\" id=\"8764\">queryEngine = index.as_query_engine()\n\nqueryEngine.query(\"\"\"\nProvide the answer in French.\nQuestion: Was ist die Hauptstadt von Albanien?\n\"\"\")</span></pre>\n <p>\n  As you can see in the response below it doesn\u2019t just answer our question correctly in French\n  <code class=\"cw pz qa qb qc b\">\n   La capitale de l\u2019Albanie est Tirana.\n  </code>\n  , but also provides which documents it used to generate the answer.\n </p>\n <pre><span class=\"qv oj gt qc b bf qw qx l qy qz\" id=\"e5fc\">Response(response=\"La capitale de l'Albanie est Tirana.\", \nsource_nodes=[NodeWithScore(node=Node(text='              <span class=\"hljs-symbol\">&amp;lt;</span>td<span class=\"hljs-symbol\">&amp;gt;</span>Albania<span class=\"hljs-symbol\">&amp;lt;</span>/td<span class=\"hljs-symbol\">&amp;gt;</span>', doc_id='3decbee1-98cc-4650-a071-ed25cd3e00d5', embedding=None, doc_hash='7d9d85082095471a9663690742d2d49fc37b2ec37cc5acf4e99e006a68a17742', extra_info=None, \nnode_info={'start': 0, 'end': 30, '_node_type': <span class=\"hljs-symbol\">&amp;lt;</span>NodeType.TEXT: '1'<span class=\"hljs-symbol\">&amp;gt;</span>}, \nrelationships={<span class=\"hljs-symbol\">&amp;lt;</span>DocumentRelationship.SOURCE: '1'<span class=\"hljs-symbol\">&amp;gt;</span>: '7b6c861f-2c2f-4905-a047-edfc25f7df19'}), score=0.7926356007369129), \nNodeWithScore(node=Node(text='              <span class=\"hljs-symbol\">&amp;lt;</span>td<span class=\"hljs-symbol\">&amp;gt;</span>Algiers<span class=\"hljs-symbol\">&amp;lt;</span>/td<span class=\"hljs-symbol\">&amp;gt;</span>', doc_id='8111b737-9f45-4855-8cd8-f958d4eb0ccd', embedding=None, doc_hash='8570a02a057a6ebbd0aff6d3f63c9f29a0ee858a81d913298d31b025101d1e44', \nextra_info=None, node_info={'start': 0, 'end': 30, '_node_type': <span class=\"hljs-symbol\">&amp;lt;</span>NodeType.TEXT: '1'<span class=\"hljs-symbol\">&amp;gt;</span>}, relationships={<span class=\"hljs-symbol\">&amp;lt;</span>DocumentRelationship.SOURCE: '1'<span class=\"hljs-symbol\">&amp;gt;</span>: '22e11ac6-8375-4d0c-91c6-4750fc63a375'}), score=0.7877589022795918)], extra_info={'3decbee1-98cc-4650-a071-ed25cd3e00d5': None, '8111b737-9f45-4855-8cd8-f958d4eb0ccd': None})</span></pre>\n <h1>\n  LlamaIndex Loaders\n </h1>\n <p>\n  The number of existing data sources in\n  <a href=\"https://llamahub.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaHub\n  </a>\n  is impressive, I counted 100+ integrations in\n  <a href=\"https://github.com/emptycrown/llama-hub\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   the repository\n  </a>\n  . You can find anything from Google docs, to GitHub, to relational databases.\n </p>\n <figure>\n  <figcaption class=\"ri fe rj ra rb rk rl be b bf z dt\">\n   LlamaHub, screenshot by Author\n  </figcaption>\n </figure>\n <p>\n  But I was missing two of my favorite technologies: GraphQL - the API query language open sourced by Facebook and Graph databases like Neo4j, the best way to store and manage large amounts of connected data, for example in Knowledge Graphs.\n </p>\n <blockquote>\n  <p class=\"nm nn rp no b hr np nq nr hu ns nt nu nv nw nx ny nz oa ob oc od oe of og oh gm bj\" id=\"0ca1\">\n   So I thought: \"How hard can it be to add them :)\"\n  </p>\n </blockquote>\n <h1>\n  Adding the new loaders\n </h1>\n <p>\n  Adding new loaders is really straightforward. There is a script in the llama-hub repository to help with adding a new loader. Running\n  <code class=\"cw pz qa qb qc b\">\n   ./add-loader.sh &lt;folder&gt;\n  </code>\n  added the skeleton files.\n </p>\n <p>\n  To get familiar with the existing implementations I looked at the\n  <a href=\"https://github.com/emptycrown/llama-hub/tree/main/llama_hub/database\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Databases (relational)\n  </a>\n  and\n  <a href=\"https://github.com/emptycrown/llama-hub/tree/main/llama_hub/mongo\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   MongoDB integrations\n  </a>\n  , the former for the Graph Database and the latter for the GraphQL.\n </p>\n <p>\n  It was easy enough, we only needed the requirements for our loader, implement the\n  <code class=\"cw pz qa qb qc b\">\n   base.py\n  </code>\n  with an straightforward API and a\n  <code class=\"cw pz qa qb qc b\">\n   README.md`\n  </code>\n  with an explanation and a code example.\n </p>\n <p>\n  The main difference my loaders have from the existing ones, is that they don\u2019t use hard-coded field names for extracting the relevant value from the query result, but instead turn the result into YAML.\n </p>\n <p>\n  I picked YAML not because I like it, but because it was closest to a textual representation of a nested tree of key-value pairs that a user would write as nested bullet lists.\n </p>\n <p>\n  Below is the example code for the Graph Database implementation (the GraphQL one is similar).\n </p>\n <h1>\n  Adding the Graph Database Loader\n </h1>\n <p>\n  I added the requirements for the\n  <code class=\"cw pz qa qb qc b\">\n   neo4j\n  </code>\n  dependency, a Cypher query language over Bolt protocol python driver, that also works with Memgraph and AWS Neptune.\n </p>\n <p>\n  Then I added the code for\n  <code class=\"cw pz qa qb qc b\">\n   <em class=\"rp\">\n    __init__\n   </em>\n  </code>\n  to take in a database server URI, database name and credentials to connect and create a driver instance.\n </p>\n <p>\n  The\n  <code class=\"cw pz qa qb qc b\">\n   load_data\n  </code>\n  method takes in the query to run and optional parameters. It\u2019s implemented by calling the driver\u2019s\n  <code class=\"cw pz qa qb qc b\">\n   execute_query\n  </code>\n  method.\n </p>\n <p>\n  Each row of results is mapped into a LlamaIndex\n  <code class=\"cw pz qa qb qc b\">\n   Document\n  </code>\n  with the\n  <code class=\"cw pz qa qb qc b\">\n   text\n  </code>\n  being the YAML representation of the results.\n </p>\n <pre><span class=\"qv oj gt qc b bf qw qx l qy qz\" id=\"5445\"><span class=\"hljs-string\">\"\"\"Graph Database Cypher Reader.\"\"\"</span>\n\n<span class=\"hljs-keyword\">from</span> typing <span class=\"hljs-keyword\">import</span> <span class=\"hljs-type\">Dict</span>, <span class=\"hljs-type\">List</span>, <span class=\"hljs-type\">Optional</span>\n\n<span class=\"hljs-keyword\">from</span> llama_index.readers.base <span class=\"hljs-keyword\">import</span> BaseReader\n<span class=\"hljs-keyword\">from</span> llama_index.readers.schema.base <span class=\"hljs-keyword\">import</span> Document\n\n<span class=\"hljs-keyword\">import</span> yaml\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">GraphDBCypherReader</span>(<span class=\"hljs-title class_ inherited__\">BaseReader</span>):\n    <span class=\"hljs-string\">\"\"\"Graph database Cypher reader.\n\n    Combines all Cypher query results into the Document type used by LlamaIndex.\n\n    Args:\n        uri (str): Graph Database URI\n        username (str): Username\n        password (str): Password\n\n    \"\"\"</span>\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">\n        self,\n        uri: <span class=\"hljs-built_in\">str</span>,\n        username: <span class=\"hljs-built_in\">str</span>,\n        password: <span class=\"hljs-built_in\">str</span>,\n        database: <span class=\"hljs-built_in\">str</span>\n    </span>) -&amp;gt; <span class=\"hljs-literal\">None</span>:\n        <span class=\"hljs-string\">\"\"\"Initialize with parameters.\"\"\"</span>\n        <span class=\"hljs-keyword\">try</span>:\n            <span class=\"hljs-keyword\">from</span> neo4j <span class=\"hljs-keyword\">import</span> GraphDatabase, basic_auth\n\n        <span class=\"hljs-keyword\">except</span> ImportError:\n            <span class=\"hljs-keyword\">raise</span> ImportError(\n                <span class=\"hljs-string\">\"`neo4j` package not found, please run `pip install neo4j`\"</span>\n            )\n        <span class=\"hljs-keyword\">if</span> uri:\n            <span class=\"hljs-keyword\">if</span> uri <span class=\"hljs-keyword\">is</span> <span class=\"hljs-literal\">None</span>:\n                <span class=\"hljs-keyword\">raise</span> ValueError(<span class=\"hljs-string\">\"`uri` must be provided.\"</span>)\n            self.client = GraphDatabase.driver(uri=uri, auth=basic_auth(username, password))\n            self.database = database\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">load_data</span>(<span class=\"hljs-params\">\n        self, query: <span class=\"hljs-built_in\">str</span>, parameters: <span class=\"hljs-type\">Optional</span>[<span class=\"hljs-type\">Dict</span>] = <span class=\"hljs-literal\">None</span>\n    </span>) -&amp;gt; <span class=\"hljs-type\">List</span>[Document]:\n        <span class=\"hljs-string\">\"\"\"Run the Cypher with optional parameters and turn results into documents\n\n        Args:\n            query (str): Graph Cypher query string.\n            parameters (Optional[Dict]): optional query parameters.\n\n        Returns:\n            List[Document]: A list of documents.\n\n        \"\"\"</span>\n        <span class=\"hljs-keyword\">if</span> parameters <span class=\"hljs-keyword\">is</span> <span class=\"hljs-literal\">None</span>:\n            parameters = {}\n\n        records, summary, keys = self.client.execute_query(query, parameters, database_ = self.database)\n\n        documents = [Document(yaml.dump(entry.data())) <span class=\"hljs-keyword\">for</span> entry <span class=\"hljs-keyword\">in</span> records]\n\n        <span class=\"hljs-keyword\">return</span> documents</span></pre>\n <p>\n  You\u2019re now ready to start using the data loader. If you want to start using this in your code, simply import `GraphDBCypherReader` from the relevant file and follow the steps below.\n </p>\n <p>\n  If you wish to submit the loader on LlamaHub, the process is fairly straightforward. After adding an example to the readme which uses an always-on demo server with StackOverflow data, I was ready to create a\n  <a href=\"https://github.com/emptycrown/llama-hub/pull/266\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   pull request\n  </a>\n  . After a short discussion the PR was quickly merged.\n </p>\n <p>\n  Thanks a lot Jerry for the smooth experience.\n </p>\n <p>\n  Now let\u2019s see how to use our two loaders.\n </p>\n <h1>\n  Using the Graph Database Loader\n </h1>\n <p>\n  The GraphDB Cypher loader, connects to graph databases, which are specialized databases that store data not in tables but in entities (\n  <em class=\"rp\">\n   Nodes\n  </em>\n  ) and their\n  <em class=\"rp\">\n   Relationships\n  </em>\n  . Because they are schema free, you can store real-world knowledge without compromising on richness.\n </p>\n <figure>\n  <figcaption class=\"ri fe rj ra rb rk rl be b bf z dt\">\n   Image for \u201cNetwork Graph\u201d generated by Midjourney by Author\n  </figcaption>\n </figure>\n <p>\n  Relationships can also hold attributes, which can represent time, weights, costs or whatever defines the concrete relationship. Any node can have as many or as few attributes or relationships as needed.\n </p>\n <blockquote>\n  <p class=\"nm nn rp no b hr np nq nr hu ns nt nu nv nw nx ny nz oa ob oc od oe of og oh gm bj\" id=\"dfcf\">\n   To query a graph database you can use the\n   <code class=\"cw pz qa qb qc b\">\n    <em class=\"gt\">\n     Cypher\n    </em>\n   </code>\n   query language, a pattern based language that expresses those relationships in visual ascii-art patterns. You encircle nodes in parentheses\n   <code class=\"cw pz qa qb qc b\">\n    <em class=\"gt\">\n     ()\n    </em>\n   </code>\n   and draw relationships as arrows\n   <code class=\"cw pz qa qb qc b\">\n    <em class=\"gt\">\n     --&gt;\n    </em>\n   </code>\n   with additional constraints put in square brackets. Otherwise Cypher provides many features known from SQL and also supports many graph operations as well as handling data structures like nested documents, of lists and dicts.\n  </p>\n </blockquote>\n <p>\n  Let\u2019s use a movie graph database and ask the LLM a question about\n  <strong>\n   <em class=\"rp\">\n    common action movie plots\n   </em>\n  </strong>\n  .\n </p>\n <p>\n  Setting up the\n  <code class=\"cw pz qa qb qc b\">\n   ServiceContext\n  </code>\n  and the\n  <code class=\"cw pz qa qb qc b\">\n   ChatGPTLLMPredictor\n  </code>\n  is the same as before.\n </p>\n <p>\n  Then we get the\n  <code class=\"cw pz qa qb qc b\">\n   GraphDBCypherReader\n  </code>\n  and connect it to our database (with an small example movie graph from\n  <a href=\"https://themoviedb.org\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   TheMovieDB\n  </a>\n  with permission).\n </p>\n <pre><span class=\"qv oj gt qc b bf qw qx l qy qz\" id=\"7048\">GraphDBCypherReader = download_loader('GraphDBCypherReader')\n\nreader = GraphDBCypherReader(uri = \"neo4j+s://demo.neo4jlabs.com\", \\\n    username = \"recommendations\", password = \"recommendations\", database = \"recommendations\")</span></pre>\n <p>\n  Then we define our query to the graph database with a parameter of year that allows us to pick more recent movies. When loading the data, each row of results should turn into one\n  <code class=\"cw pz qa qb qc b\">\n   Document\n  </code>\n  where the\n  <code class=\"cw pz qa qb qc b\">\n   text\n  </code>\n  property of the document is the YAML representation of the row.\n </p>\n <pre><span class=\"qv oj gt qc b bf qw qx l qy qz\" id=\"b882\">query = <span class=\"hljs-string\">\"\"\"\n    MATCH (m:Movie)-[rel:ACTED_IN|DIRECTED|IN_GENRE]-(other)\n    WHERE $year &amp;lt; m.year and m.imdbRating &amp;gt; $rating\n    WITH m, type(rel) as relation, collect(other.name) as names\n    RETURN m.title as title, m.year as year, m.plot as plot, relation, names\n    ORDER BY m.year ASC\n\"\"\"</span>\n\ndocuments = reader.load_data(query, parameters = {<span class=\"hljs-string\">\"year\"</span>:<span class=\"hljs-number\">1990</span>,<span class=\"hljs-string\">\"rating\"</span>:<span class=\"hljs-number\">8</span>})\nindex = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-built_in\">len</span>(documents))\n<span class=\"hljs-built_in\">print</span>(documents[<span class=\"hljs-number\">0</span>:<span class=\"hljs-number\">5</span>])</span></pre>\n <p>\n  The output will look similar to the following:\n </p>\n <pre><span class=\"qv oj gt qc b bf qw qx l qy qz\" id=\"8e7a\">829\n[Document(text='names:\\n- Saifei He\\n- Li Gong\\n- Jingwu Ma\\n- Cuifen Cao\\nplot: A young woman becomes the fourth wife of a wealthy lord, and must learn to live\\n  with the strict rules and tensions within the household.\\nrelation: ACTED_IN\\ntitle: Raise the Red Lantern (Da hong deng long gao gao gua)\\nyear: 1991\\n', doc_id='782d9a63-251b-4bb8-aa3d-5d8f6d1fb5d2', embedding=None, doc_hash='f9fd966bc5f2234e94d09efebd3be008db8c891f8666c1a364abf7812f5d7a1c', extra_info=None), Document(text='names:\\n- Yimou Zhang\\nplot: A young woman becomes the fourth wife of a wealthy lord, and must learn to live\\n  with the strict rules and tensions within the household.\\nrelation: DIRECTED\\ntitle: Raise the Red Lantern (Da hong deng long gao gao gua)\\nyear: 1991\\n', doc_id='2e13caf6-b9cf-4263-a264-7121bc77d1ee', embedding=None, doc_hash='e1f340ed1fac2f1b8d6076cfc2c9e9cb0109d5d11e5dcdbf3a467332f5995cb1', extra_info=None), ...]</span></pre>\n <p>\n  Now we can use our\n  <code class=\"cw pz qa qb qc b\">\n   index\n  </code>\n  to run a LLM query to answer the questions we wanted to pose.\n </p>\n <pre><span class=\"qv oj gt qc b bf qw qx l qy qz\" id=\"cd31\">queryEngine= index.as_query_engine()\n\nqueryEngine.query(\"\"\"\nWhat are the most common plots in action movies?\n\"\"\")</span></pre>\n <p>\n  The answer shows that the LLM can utilize the inputs, understands the genre \"action movies\" and can summarize their plots. Here is its answer.\n </p>\n <blockquote>\n  <p class=\"nm nn rp no b hr np nq nr hu ns nt nu nv nw nx ny nz oa ob oc od oe of og oh gm bj\" id=\"4d79\">\n   Based on the given context information, it appears that the most common plots in action movies are heists and battles against controlling forces. However, it is important to note that this conclusion is based on a limited sample size and may not be representative of all action movies.\n  </p>\n </blockquote>\n <pre><span class=\"qv oj gt qc b bf qw qx l qy qz\" id=\"2ccf\">Response(response='Based on the given context information, it appears that the most common plots in action movies are heists and battles against controlling forces. However, it is important to note that this conclusion is based on a limited sample size and may not be representative of all action movies.',\n\n\nsource_nodes=[NodeWithScore(node=Node(text='names:\\n- Action\\n- Crime\\n- Thriller\\nplot: A group of professional bank robbers start to feel the heat from police when\\n  they unknowingly leave a clue at their latest heist.\\nrelation: IN_GENRE\\ntitle: Heat\\nyear: 1995\\n', doc_id='bb117618-1cce-4cec-bd9b-8645ab0b50a3', embedding=None, doc_hash='4d493a9f33eb7a1c071756f61e1975ae5c313ecd42243f81a8827919a618468b', extra_info=None, node_info={'start': 0, 'end': 215, '_node_type': <span class=\"hljs-symbol\">&amp;lt;</span>NodeType.TEXT: '1'<span class=\"hljs-symbol\">&amp;gt;</span>}, relationships={<span class=\"hljs-symbol\">&amp;lt;</span>DocumentRelationship.SOURCE: '1'<span class=\"hljs-symbol\">&amp;gt;</span>: 'dbfffdae-d88c-49e2-9d6b-83dad427a3f3'}), score=0.8247381316731472), NodeWithScore(node=Node(text='names:\\n- Thriller\\n- Sci-Fi\\n- Action\\nplot: A computer hacker learns from mysterious rebels about the true nature of his\\n  reality and his role in the war against its controllers.\\nrelation: IN_GENRE\\ntitle: Matrix, The\\nyear: 1999\\n', doc_id='c4893c61-32ee-4d05-b559-1f65a5197e5e', embedding=None, doc_hash='0b6a080bf712548099c5c8c1b033884a38742c73dc23d420ac2e677e7ece82f4', extra_info=None, node_info={'start': 0, 'end': 227, '_node_type': <span class=\"hljs-symbol\">&amp;lt;</span>NodeType.TEXT: '1'<span class=\"hljs-symbol\">&amp;gt;</span>}, relationships={<span class=\"hljs-symbol\">&amp;lt;</span>DocumentRelationship.SOURCE: '1'<span class=\"hljs-symbol\">&amp;gt;</span>: '6c8dea11-1371-4f5a-a1a1-7f517f027008'}), score=0.8220633045996049)], extra_info={'bb117618-1cce-4cec-bd9b-8645ab0b50a3': None, 'c4893c61-32ee-4d05-b559-1f65a5197e5e': None})</span></pre>\n <h1>\n  Using the GraphQL Loader\n </h1>\n <p>\n  The GraphQL loader is similarly easy to use.\n </p>\n <p>\n  <a href=\"https://graphql.org\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   GraphQL\n  </a>\n  is not a database query language, but an API query language that is based on strict schema expressed in \"type definitions\". There you express your entities, their attributes (fields) both for scalar datatypes as well as object datatypes pointing to other entities.\n </p>\n <figure>\n  <figcaption class=\"ri fe rj ra rb rk rl be b bf z dt\">\n   What is GraphQL from GraphQL.org, Screenshot by Author\n  </figcaption>\n </figure>\n <p>\n  GraphQL itself is a tree based query language, that expresses a nested structure of data that you want to fetch starting from a root query. The fields of every entity returned from that query can be selected and for object fields you can further select fields from the referred entity and so on, almost ad-infinitum (API-Limits apply).\n </p>\n <p>\n  There are a number of GraphQL libraries, most notably the JavaScript reference implementation, but also\n  <code class=\"cw pz qa qb qc b\">\n   gql\n  </code>\n  for python, and also integrations with databases like Hasura, Prisma or the\n  <a href=\"https://neo4j.com/product/graphql-library/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Neo4j-GraphQL-Library\n  </a>\n  . Several larger projects now provide GraphQL APIs including GitHub, Spotify, Twitter.\n </p>\n <p>\n  The demo is similar to our first one. We use a public GraphQL endpoint (\n  <a href=\"https://countries.trevorblades.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://countries.trevorblades.com/\n  </a>\n  ), that provides a structure of continent\u2192country\u2192capital. (\n  <a href=\"https://github.com/trevorblades/countries\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Licensed under MIT\n  </a>\n  )\n </p>\n <p>\n  A subset of the type-definition is here.\n </p>\n <pre><span class=\"qv oj gt qc b bf qw qx l qy qz\" id=\"841d\">type <span class=\"hljs-title class_\">Query</span> {\n    <span class=\"hljs-title function_\">continent</span>(<span class=\"hljs-attr\">code</span>: <span class=\"hljs-variable constant_\">ID</span>!): <span class=\"hljs-title class_\">Continent</span>\n    <span class=\"hljs-title function_\">continents</span>(<span class=\"hljs-attr\">filter</span>: <span class=\"hljs-title class_\">ContinentFilterInput</span> = {}): [<span class=\"hljs-title class_\">Continent</span>!]!\n    <span class=\"hljs-title function_\">countries</span>(<span class=\"hljs-attr\">filter</span>: <span class=\"hljs-title class_\">CountryFilterInput</span> = {}): [<span class=\"hljs-title class_\">Country</span>!]!\n    <span class=\"hljs-title function_\">country</span>(<span class=\"hljs-attr\">code</span>: <span class=\"hljs-variable constant_\">ID</span>!): <span class=\"hljs-title class_\">Country</span>\n    <span class=\"hljs-title function_\">language</span>(<span class=\"hljs-attr\">code</span>: <span class=\"hljs-variable constant_\">ID</span>!): <span class=\"hljs-title class_\">Language</span>\n    <span class=\"hljs-title function_\">languages</span>(<span class=\"hljs-attr\">filter</span>: <span class=\"hljs-title class_\">LanguageFilterInput</span> = {}): [<span class=\"hljs-title class_\">Language</span>!]!\n}\n\ntype <span class=\"hljs-title class_\">Continent</span> {\n    <span class=\"hljs-attr\">code</span>: <span class=\"hljs-variable constant_\">ID</span>!\n    <span class=\"hljs-attr\">countries</span>: [<span class=\"hljs-title class_\">Country</span>!]!\n    <span class=\"hljs-attr\">name</span>: <span class=\"hljs-title class_\">String</span>!\n}\n\ntype <span class=\"hljs-title class_\">Country</span> {\n    <span class=\"hljs-attr\">awsRegion</span>: <span class=\"hljs-title class_\">String</span>!\n    <span class=\"hljs-attr\">capital</span>: <span class=\"hljs-title class_\">String</span>\n    <span class=\"hljs-attr\">code</span>: <span class=\"hljs-variable constant_\">ID</span>!\n    <span class=\"hljs-attr\">continent</span>: <span class=\"hljs-title class_\">Continent</span>!\n    <span class=\"hljs-attr\">currencies</span>: [<span class=\"hljs-title class_\">String</span>!]!\n    <span class=\"hljs-attr\">currency</span>: <span class=\"hljs-title class_\">String</span>\n    <span class=\"hljs-attr\">emoji</span>: <span class=\"hljs-title class_\">String</span>!\n    <span class=\"hljs-attr\">emojiU</span>: <span class=\"hljs-title class_\">String</span>!\n    <span class=\"hljs-attr\">languages</span>: [<span class=\"hljs-title class_\">Language</span>!]!\n    <span class=\"hljs-title function_\">name</span>(<span class=\"hljs-attr\">lang</span>: <span class=\"hljs-title class_\">String</span>): <span class=\"hljs-title class_\">String</span>!\n    <span class=\"hljs-attr\">native</span>: <span class=\"hljs-title class_\">String</span>!\n    <span class=\"hljs-attr\">phone</span>: <span class=\"hljs-title class_\">String</span>!\n    <span class=\"hljs-attr\">phones</span>: [<span class=\"hljs-title class_\">String</span>!]!\n    <span class=\"hljs-attr\">states</span>: [<span class=\"hljs-title class_\">State</span>!]!\n    <span class=\"hljs-attr\">subdivisions</span>: [<span class=\"hljs-title class_\">Subdivision</span>!]!\n}\n...</span></pre>\n <p>\n  In our demo, we again define the\n  <code class=\"cw pz qa qb qc b\">\n   ServiceContext\n  </code>\n  with the\n  <code class=\"cw pz qa qb qc b\">\n   ChatGPTLLMPredictor\n  </code>\n  as before. Then we get the\n  <code class=\"cw pz qa qb qc b\">\n   GraphQLReader\n  </code>\n  loader and point it to the URL of the endpoint. You can also provide additional HTTP-Headers, e.g. for authentication.\n </p>\n <pre><span class=\"qv oj gt qc b bf qw qx l qy qz\" id=\"6aa9\">GraphQLReader = download_loader('GraphQLReader')\nreader = GraphQLReader(uri = \"https://countries.trevorblades.com/\", headers = {})</span></pre>\n <pre><span class=\"qv oj gt qc b bf qw qx l qy qz\" id=\"469b\">query = <span class=\"hljs-string\">\"\"\"\nquery getContinents {\n  continents {\n    name\n    countries {\n      name\n      capital\n    }\n  }\n}\n\"\"\"</span>\ndocuments = reader.load_data(query, variables = {})\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-built_in\">len</span>(documents))\n<span class=\"hljs-built_in\">print</span>(documents)</span></pre>\n <p>\n  We see that it finds 7 continents with countries and capitals, each of the root results (continent) is turned into a document\n </p>\n <pre><span class=\"qv oj gt qc b bf qw qx l qy qz\" id=\"6167\">7\n[Document(text='countries:\\n- capital: Luanda\\n  name: Angola\\n- capital: Ouagadougou\\n  name: Burkina Faso\\n- capital: Bujumbura\\n  name: Burundi\\n- capital: Porto-Novo\\n  name: Benin\\n- capital: Gaborone\\n  name: Botswana\\n- capital: Kinshasa\\n  name: Democratic Republic of the Congo\\n- capital: Bangui\\n  name: Central African Republic\\n....',doc_id='b82fec36-5e82-4246-b7ab-f590bf6741ab', embedding=None, doc_hash='a4caa760423d6ca861b9332f386add3c449f1683168391ae10f7f73a691a2240', extra_info=None)]</span></pre>\n <p>\n  Again we stress the LLM only a little bit by asking it in German, \"Which capitals are in North America\".\n </p>\n <pre><span class=\"qv oj gt qc b bf qw qx l qy qz\" id=\"ec08\">index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\nqueryEngine= index.as_query_engine()\n\nresponse = queryEngine.query(\"\"\"\nQuestion: Welche Hauptst\u00e4dte liegen in Nordamerika?\nAnswer:\n\"\"\")\n\nresponse.response</span></pre>\n <p>\n  I was surprised, as I had only expected a hand-full of countries and cities. But we get 27 countries that are in North America. This shows how our perception is skewed by the western worldview.\n </p>\n <pre><span class=\"qv oj gt qc b bf qw qx l qy qz\" id=\"69d9\">Die Hauptst\u00e4dte, die in Nordamerika liegen, sind Ottawa, San Jos\\xE9, Havana, Willemstad, Roseau, Santo Domingo, St. George's, Nuuk, Guatemala City, Tegucigalpa, Port-au-Prince, Kingston, Basseterre, George Town, Castries, Marigot, Fort-de-France, Plymouth, Mexico City, Managua, Panama City, Saint-Pierre, San Juan, San Salvador, Philipsburg, Cockburn Town, Port of Spain, Washington D.C., Kingstown und Road Town.</span></pre>\n <p>\n  We could also flip the GraphQL query around and then get 250 countries with their respective capitals and continents.\n </p>\n <pre><span class=\"qv oj gt qc b bf qw qx l qy qz\" id=\"e20a\">query = <span class=\"hljs-string\">\"\"\"\nquery getCountries {\n  countries {\n    name\n    capital\n    continent {\n        name\n    }\n  }\n}\n\"\"\"</span>\ndocuments = reader.load_data(query, variables = {})\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-built_in\">len</span>(documents))\n<span class=\"hljs-built_in\">print</span>(documents)</span></pre>\n <p>\n  Both document lists should work equally well, but let\u2019s see.\n </p>\n <p>\n  This time the answer from the LLM was much more limited. I\u2019m not sure if that was because the index fed the LLM fewer documents to pick from.\n </p>\n <pre><span class=\"qv oj gt qc b bf qw qx l qy qz\" id=\"a495\">index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\nqueryEngine= index.as_query_engine()\n\nresponse = queryEngine.query(\"\"\"\nQuestion: Which capitals are in North America?\nAnswer:\n\"\"\")\n\nresponse.response</span></pre>\n <pre><span class=\"qv oj gt qc b bf qw qx l qy qz\" id=\"8cc0\">Washington D.C. and Mexico City are in North America.</span></pre>\n <h1>\n  Conclusion\n </h1>\n <p>\n  It was really smooth to add new data loaders to LlamaHub, thanks a lot to\n  <a href=\"https://medium.com/u/e76da1c45ef7?source=post_page-----bcaecec262d7--------------------------------\" rel=\"noopener\" target=\"_blank\">\n   Jerry Liu\n  </a>\n  for making it so easy. Please let me know what you\u2019re doing with these loaders and if you have any feedback.\n </p>\n <p>\n  If I find time in the next weeks I also want to look into the\n  <code class=\"cw pz qa qb qc b\">\n   KnowledgeGraphIndex\n  </code>\n  and see if my graph database loader can nicely populate that one.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 34794, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf9e1e55-3c01-4ad7-9212-ab5716ef2216": {"__data__": {"id_": "cf9e1e55-3c01-4ad7-9212-ab5716ef2216", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.html", "file_name": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.html", "file_type": "text/html", "file_size": 10471, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.html", "file_name": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.html", "file_type": "text/html", "file_size": 10471, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "8db6207f4ac01f95c1a6219b51d1fab3e23377112f2ed36da62a0133fd28725f", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  A few days ago, we published a blog on\n  <a href=\"/multi-modal-rag-621de7525fea\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Multi-Modal RAG\n  </a>\n  (Retrieval-Augmented Generation) and our latest (still in beta) abstractions to help enable and simplify building them. In this post, we now go over the important topic of how one can sensibly evaluate Multi-Modal RAG systems.\n </p>\n <p>\n  A natural starting point is to consider how evaluation was done in traditional, text-only RAG and then ask ourselves how this ought to be modified to suit the multi-modal scenario (e.g., in text-only RAG, we use an LLM, but in multi-modal RAG we require a Large Multi-Modal Model or LMM for short). This is exactly what we\u2019ll do next and as you\u2019ll see, the overarching evaluation framework stays the same as it was in the text-only RAG, requiring only a few additions and modifications in order to make it more multi-modal appropriate.\n </p>\n <h1>\n  Primer: Multi-Modal RAG vs Text-Only RAG\n </h1>\n <figure>\n  <figcaption class=\"pl fe pm ox oy pn po be b bf z dt\">\n   Illustration of text-only RAG versus multi-modal RAG. In multi-modal RAG, images modality can show up in the user query, the retrieved context, as well as the final answer.\n  </figcaption>\n </figure>\n <p>\n  Let\u2019s consider the main differences between multi-modal and text-only RAG. Below are two tables that describe the RAG build considerations as well as query-time pipeline and compares and contrasts multi-modal and text-only cases against them.\n </p>\n <figure>\n  <figcaption class=\"pl fe pm ox oy pn po be b bf z dt\">\n   Table 1: Build considerations for RAG systems and how they differ text-only versus multi-modal scenarios.\n  </figcaption>\n </figure>\n <figure>\n  <figcaption class=\"pl fe pm ox oy pn po be b bf z dt\">\n   Table 2: The pipeline for querying a RAG and how they differ text-only versus multi-modal scenarios.\n  </figcaption>\n </figure>\n <h1>\n  Evaluation Of Text-Only RAG\n </h1>\n <p>\n  For text-only RAG, the standard approach is to separately consider the evaluation of two stages: Retrieval and Generation.\n </p>\n <p>\n  <strong>\n   Retriever Evaluation:\n  </strong>\n  are the retrieved documents relevant to the user query?\n </p>\n <p>\n  Some of the more popular metrics for retrieval evaluation include\n  <strong>\n   recall, hit rate\n  </strong>\n  ,\n  <strong>\n   mean reciprocal rank, mean average precision, and normalized discounted cumulative gain.\n  </strong>\n  The first two of these metrics recall and hit rate, don\u2019t consider the position (or ranking) of the relevant documents, whereas all the others do in their own respective ways.\n </p>\n <p>\n  <strong>\n   Generator Evaluation:\n  </strong>\n  does the response use the retrieved documents to sufficiently answer the user query?\n </p>\n <p>\n  In abstractive question-answering systems, like the kinds we\u2019re talking about in this blog, measuring the generated response is made more tricky due to the fact that there isn\u2019t just one way to sufficiently answer a query in written language \u2014 there\u2019s plenty!\n </p>\n <p>\n  So, in this case, our measurement relies on subjective judgement, which can be performed by humans, though this is costly and unscalable. An alternative approach, is to use an LLM judge to measure things like\n  <em class=\"pw\">\n   relevancy\n  </em>\n  and\n  <em class=\"pw\">\n   faithfulness\n  </em>\n  .\n </p>\n <ul>\n  <li>\n   Relevancy: considers textual context and evaluates how much the generated response matches the query.\n  </li>\n  <li>\n   Faithfulness: evaluates how much the generated response matches the retrieved textual context.\n  </li>\n </ul>\n <p>\n  For both of these, the retrieved context as well as the query and generated response are passed to the LLM judge. (This pattern of using an LLM to judge the responses has been termed by some researchers of the space as LLM-As-A-Judge (\n  <a href=\"https://arxiv.org/abs/2306.05685\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Zheng et al., 2023\n  </a>\n  ).)\n </p>\n <p>\n  Currently, the\n  <code class=\"cw qf qg qh qi b\">\n   llama-index (v0.9.2)\n  </code>\n  library supports hit-rate and mean reciprocal rank for retrieval evaluation, as well as relevancy, faithfulness and a few others for generator evaluation. (Check out our Evaluation guides in our\n  <a href=\"https://docs.llamaindex.ai/en/stable/optimizing/evaluation/evaluation.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   docs\n  </a>\n  !).\n </p>\n <h1>\n  Evaluation Of Multi-Modal RAG\n </h1>\n <p>\n  For the multi-modal case, the evaluation can (and should) still be carried out with respect to the different stages of retrieval and generation.\n </p>\n <p>\n  <strong>\n   Separating Out Retrieval Evaluation For Text and Image Modalities\n  </strong>\n </p>\n <p>\n  Now that retrieved documents can come in two forms, it would seem most sensible to consider computing the usual retrieval evaluation metrics separately for images and text. In this way, you have more knowledge as to which aspect of the multi-modal retriever is working well and what isn\u2019t. One can then apply a desired weighting scheme to establish a single aggregated retrieval score per metric.\n </p>\n <p>\n  Hit Rate Mean Reciprocal Rank Text 0.95 0.88 Images 0.88 0.75\n </p>\n <figure>\n  <figcaption class=\"pl fe pm ox oy pn po be b bf z dt\">\n   Table 3: Retrieval evaluation in multi-modal scenario.\n  </figcaption>\n </figure>\n <p>\n  <strong>\n   Using Multi-Modal LLMs For Generator Evaluations (LMM-As-A-Judge)\n  </strong>\n </p>\n <p>\n  Multi-modal models (i.e., LMMs) like OpenAI\u2019s GPT-4V or open-source alternatives like LLaVA are able to take in both input and image context to produce an answer the user query. As in text-only RAG, we are also concerned about the \u201crelevancy\u201d and \u201cfaithfulness\u201d of these generated answers. But in order to be able to compute such metrics in the multi-modal case, we would need a judge model that is also able to take in the context images and text data. Thus, in the multi-modal case, we adopt the LMM-As-A-Judge pattern in order to compute relevancy and faithfulness as well as other related metrics!\n </p>\n <ul>\n  <li>\n   Relevancy (multi-modal): considers\n   <strong>\n    textual and visual context\n   </strong>\n   and evaluates how much the generated response matches the query.\n  </li>\n  <li>\n   Faithfulness (multi-modal): evaluates how much the generated response matches the retrieved\n   <strong>\n    textual and visual context\n   </strong>\n   .\n  </li>\n </ul>\n <p>\n  If you want to test these out, then you\u2019re in luck as we\u2019ve recently released our beta Multi-Modal Evaluator abstractions! See the code snippet below for how one can use these abstractions to perform their respective evaluations on a generated response to a given query.\n </p>\n <pre><span class=\"qn oa gt qi b bf qo qp l qq qr\" id=\"cde8\"><span class=\"hljs-keyword\">from</span> llama_index.evaluation.multi_modal <span class=\"hljs-keyword\">import</span> (\n\tMultiModalRelevancyEvaluator,\n\tMultiModalFaithfulnessEvaluator\n)\n<span class=\"hljs-keyword\">from</span> llama_index.multi_modal_llm <span class=\"hljs-keyword\">import</span> OpenAIMultiModal\n\nrelevancy_judge = MultiModalRelevancyEvaluator(\n    multi_modal_llm=OpenAIMultiModal(\n        model=<span class=\"hljs-string\">\"gpt-4-vision-preview\"</span>,\n        max_new_tokens=<span class=\"hljs-number\">300</span>,\n    )\n)\n\nfaithfulness_judge = MultiModalRelevancyEvaluator(\n    multi_modal_llm=OpenAIMultiModal(\n        model=<span class=\"hljs-string\">\"gpt-4-vision-preview\"</span>,\n        max_new_tokens=<span class=\"hljs-number\">300</span>,\n    )\n)\n\n<span class=\"hljs-comment\"># Generated response to a query and its retrieved context information</span>\nquery = ...\nresponse = ...\ncontexts = ...  <span class=\"hljs-comment\"># retrieved text contexts</span>\nimage_paths = ...  <span class=\"hljs-comment\"># retrieved image contexts</span>\n\n<span class=\"hljs-comment\"># Evaluations</span>\nrelevancy_eval = relevancy_judge.evaluate(\n query=query,\n response=response,\n contexts=contexts,\n image_paths=image_paths\n)\n\nfaithfulness_eval = faithfulness_judge.evaluate(\n query=query,\n response=response,\n contexts=contexts,\n image_paths=image_paths\n)</span></pre>\n <h1>\n  A Few Important Remarks\n </h1>\n <p>\n  First, it is worth mentioning that using LLMs or LMMs to judge generated responses has its drawbacks. These judges are generative models themselves and can suffer from hallucinations and other inconsistencies. Though studies have shown that strong LLMs can align to human judgments at a relatively high rate (\n  <a href=\"https://arxiv.org/abs/2306.05685\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Zheng et al., 2023\n  </a>\n  ), using them in production systems should be handled with higher standards of care. At time of writing, there has no been study to show that strong LMMs can also align well to human judgements.\n </p>\n <p>\n  Secondly, the evaluation of a generator touches mostly on the evaluation of its knowledge and reasoning capabilities. There are other important dimensions on which to evaluate LLMs and LMMs, including Alignment and Safety \u2014 see\n  <a href=\"https://arxiv.org/pdf/2310.19736.pdf\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Evaluating LMMs: A Comprehensive Survey\n  </a>\n  for more information.\n </p>\n <h1>\n  Go forth and evaluate\n </h1>\n <p>\n  In this post, we covered how evaluation can be performed on multi-modal RAG systems. We believe that separating out the retrieval evaluations per modalities for increased visibility as well as the LMM-As-A-Judge represent a sensible extension of the evaluation framework for text-only RAG. We encourage you to check out our practical notebook guides as well as docs for more information on how you can not only build Multi-Modal RAGs but also adequately evaluate them!\n </p>\n <ul>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/evaluation/multi_modal/multi_modal_rag_evaluation.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook guide for evaluating Multi-Modal RAG systems with LlamaIndex\n   </a>\n  </li>\n  <li>\n   <a href=\"/multi-modal-rag-621de7525fea\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Intro to Multi-Modal RAG\n   </a>\n  </li>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/models/multi_modal.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs/guides on Multi-Modal Abstractions\n   </a>\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 10436, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ab33830-349c-4cfd-a9df-27023e133c9c": {"__data__": {"id_": "2ab33830-349c-4cfd-a9df-27023e133c9c", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.html", "file_name": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.html", "file_type": "text/html", "file_size": 17554, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.html", "file_name": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.html", "file_type": "text/html", "file_size": 17554, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "af11b5eda5d181a6a54b7e41dd8190f7aa7626c3a45a98e1ef42af317064cd87", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h1>\n  Introduction\n </h1>\n <p>\n  Retrieval-augmented generation (RAG) has introduced an innovative approach that fuses the extensive retrieval capabilities of search systems with the LLM. When implementing a RAG system, one critical parameter that governs the system\u2019s efficiency and performance is the\n  <code class=\"cw pk pl pm pn b\">\n   chunk_size\n  </code>\n  . How does one discern the optimal chunk size for seamless retrieval? This is where LlamaIndex\n  <code class=\"cw pk pl pm pn b\">\n   Response Evaluation\n  </code>\n  comes in handy. In this blog post, we'll guide you through the steps to determine the best\n  <code class=\"cw pk pl pm pn b\">\n   chunk size\n  </code>\n  using LlamaIndex\u2019s\n  <code class=\"cw pk pl pm pn b\">\n   Response Evaluation\n  </code>\n  module. If you're unfamiliar with the\n  <code class=\"cw pk pl pm pn b\">\n   Response\n  </code>\n  Evaluation module, we recommend reviewing its\n  <a href=\"https://docs.llamaindex.ai/en/latest/core_modules/supporting_modules/evaluation/modules.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   documentation\n  </a>\n  before proceeding.\n </p>\n <h1>\n  Why Chunk Size Matters\n </h1>\n <p>\n  Choosing the right\n  <code class=\"cw pk pl pm pn b\">\n   chunk_size\n  </code>\n  is a critical decision that can influence the efficiency and accuracy of a RAG system in several ways:\n </p>\n <ol>\n  <li>\n   <strong>\n    Relevance and Granularity\n   </strong>\n   : A small\n   <code class=\"cw pk pl pm pn b\">\n    chunk_size\n   </code>\n   , like 128, yields more granular chunks. This granularity, however, presents a risk: vital information might not be among the top retrieved chunks, especially if the\n   <code class=\"cw pk pl pm pn b\">\n    similarity_top_k\n   </code>\n   setting is as restrictive as 2. Conversely, a chunk size of 512 is likely to encompass all necessary information within the top chunks, ensuring that answers to queries are readily available. To navigate this, we employ the Faithfulness and Relevancy metrics. These measure the absence of \u2018hallucinations\u2019 and the \u2018relevancy\u2019 of responses based on the query and the retrieved contexts respectively.\n  </li>\n  <li>\n   <strong>\n    Response Generation Time\n   </strong>\n   : As the\n   <code class=\"cw pk pl pm pn b\">\n    chunk_size\n   </code>\n   increases, so does the volume of information directed into the LLM to generate an answer. While this can ensure a more comprehensive context, it might also slow down the system. Ensuring that the added depth doesn't compromise the system's responsiveness is crucial.\n  </li>\n </ol>\n <p>\n  In essence, determining the optimal\n  <code class=\"cw pk pl pm pn b\">\n   chunk_size\n  </code>\n  is about striking a balance: capturing all essential information without sacrificing speed. It's vital to undergo thorough testing with various sizes to find a configuration that suits the specific use case and dataset.\n </p>\n <p>\n  For a practical evaluation in choosing the right\n  <code class=\"cw pk pl pm pn b\">\n   chunk_size\n  </code>\n  , you can access and run the following setup on this\n  <a href=\"https://colab.research.google.com/drive/1LPvJyEON6btMpubYdwySfNs0FuNR9nza?usp=sharing\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    Google Colab Notebook\n   </strong>\n  </a>\n  .\n </p>\n <h1>\n  Setup\n </h1>\n <p>\n  Before embarking on the experiment, we need to ensure all requisite modules are imported:\n </p>\n <pre><span class=\"qk np gt pn b bf ql qm l qn qo\" id=\"c77c\"><span class=\"hljs-keyword\">import</span> nest_asyncio\n\nnest_asyncio.apply()\n\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> (\n    SimpleDirectoryReader,\n    VectorStoreIndex,\n    ServiceContext,\n)\n<span class=\"hljs-keyword\">from</span> llama_index.evaluation <span class=\"hljs-keyword\">import</span> (\n    DatasetGenerator,\n    FaithfulnessEvaluator,\n    RelevancyEvaluator\n)\n<span class=\"hljs-keyword\">from</span> llama_index.llms <span class=\"hljs-keyword\">import</span> OpenAI\n\n<span class=\"hljs-keyword\">import</span> openai\n<span class=\"hljs-keyword\">import</span> time\nopenai.api_key = <span class=\"hljs-string\">'OPENAI-API-KEY'</span></span></pre>\n <h1>\n  Download Data\n </h1>\n <p>\n  We\u2019ll be using the Uber 10K SEC Filings for 2021 for this experiment.\n </p>\n <pre><span class=\"qk np gt pn b bf ql qm l qn qo\" id=\"b979\">!mkdir -p 'data/10k/'\n!wget 'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'</span></pre>\n <h1>\n  Load Data\n </h1>\n <p>\n  Let\u2019s load our document.\n </p>\n <pre><span class=\"qk np gt pn b bf ql qm l qn qo\" id=\"e95c\">documents = SimpleDirectoryReader(\"./data/10k/\").load_data()</span></pre>\n <h1>\n  Question Generation\n </h1>\n <p>\n  To select the right\n  <code class=\"cw pk pl pm pn b\">\n   chunk_size\n  </code>\n  , we'll compute metrics like Average Response time, Faithfulness, and Relevancy for various\n  <code class=\"cw pk pl pm pn b\">\n   chunk_sizes\n  </code>\n  . The\n  <code class=\"cw pk pl pm pn b\">\n   DatasetGenerator\n  </code>\n  will help us generate questions from the documents.\n </p>\n <pre><span class=\"qk np gt pn b bf ql qm l qn qo\" id=\"ad22\">data_generator = DatasetGenerator.from_documents(documents)\neval_questions = data_generator.generate_questions_from_nodes()</span></pre>\n <h1>\n  Setting Up Evaluators\n </h1>\n <p>\n  We are setting up the GPT-4 model to serve as the backbone for evaluating the responses generated during the experiment. Two evaluators,\n  <code class=\"cw pk pl pm pn b\">\n   FaithfulnessEvaluator\n  </code>\n  and\n  <code class=\"cw pk pl pm pn b\">\n   RelevancyEvaluator\n  </code>\n  , are initialised with the\n  <code class=\"cw pk pl pm pn b\">\n   service_context\n  </code>\n  .\n </p>\n <ol>\n  <li>\n   <strong>\n    Faithfulness Evaluator\n   </strong>\n   \u2014 It is useful for measuring if the response was hallucinated and measures if the response from a query engine matches any source nodes.\n  </li>\n  <li>\n   <strong>\n    Relevancy Evaluator\n   </strong>\n   \u2014 It is useful for measuring if the query was actually answered by the response and measures if the response + source nodes match the query.\n  </li>\n </ol>\n <pre><span class=\"qk np gt pn b bf ql qm l qn qo\" id=\"d485\"># We will use GPT-4 for evaluating the responses\ngpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n\n# Define service context for GPT-4 for evaluation\nservice_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n\n# Define Faithfulness and Relevancy Evaluators which are based on GPT-4\nfaithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\nrelevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4)</span></pre>\n <h1>\n  Response Evaluation For A Chunk Size\n </h1>\n <p>\n  We evaluate each chunk_size based on 3 metrics.\n </p>\n <ol>\n  <li>\n   Average Response Time.\n  </li>\n  <li>\n   Average Faithfulness.\n  </li>\n  <li>\n   Average Relevancy.\n  </li>\n </ol>\n <p>\n  Here\u2019s a function,\n  <code class=\"cw pk pl pm pn b\">\n   evaluate_response_time_and_accuracy\n  </code>\n  , that does just that which has:\n </p>\n <ol>\n  <li>\n   VectorIndex Creation.\n  </li>\n  <li>\n   Building the Query Engine.\n  </li>\n  <li>\n   Metrics Calculation.\n  </li>\n </ol>\n <pre><span class=\"qk np gt pn b bf ql qm l qn qo\" id=\"8a18\"><span class=\"hljs-comment\"># Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size</span>\n<span class=\"hljs-comment\"># We use GPT-3.5-Turbo to generate response and GPT-4 to evaluate it.</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">evaluate_response_time_and_accuracy</span>(<span class=\"hljs-params\">chunk_size, eval_questions</span>):\n    <span class=\"hljs-string\">\"\"\"\n    Evaluate the average response time, faithfulness, and relevancy of responses generated by GPT-3.5-turbo for a given chunk size.\n    \n    Parameters:\n    chunk_size (int): The size of data chunks being processed.\n    \n    Returns:\n    tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.\n    \"\"\"</span>\n\n    total_response_time = <span class=\"hljs-number\">0</span>\n    total_faithfulness = <span class=\"hljs-number\">0</span>\n    total_relevancy = <span class=\"hljs-number\">0</span>\n\n    <span class=\"hljs-comment\"># create vector index</span>\n    llm = OpenAI(model=<span class=\"hljs-string\">\"gpt-3.5-turbo\"</span>)\n    service_context = ServiceContext.from_defaults(llm=llm, chunk_size=chunk_size)\n    vector_index = VectorStoreIndex.from_documents(\n        eval_documents, service_context=service_context\n    )\n    <span class=\"hljs-comment\"># build query engine</span>\n    query_engine = vector_index.as_query_engine()\n    num_questions = <span class=\"hljs-built_in\">len</span>(eval_questions)\n\n    <span class=\"hljs-comment\"># Iterate over each question in eval_questions to compute metrics.</span>\n    <span class=\"hljs-comment\"># While BatchEvalRunner can be used for faster evaluations (see: https://docs.llamaindex.ai/en/latest/examples/evaluation/batch_eval.html),</span>\n    <span class=\"hljs-comment\"># we're using a loop here to specifically measure response time for different chunk sizes.</span>\n    <span class=\"hljs-keyword\">for</span> question <span class=\"hljs-keyword\">in</span> eval_questions:\n        start_time = time.time()\n        response_vector = query_engine.query(question)\n        elapsed_time = time.time() - start_time\n        \n        faithfulness_result = faithfulness_gpt4.evaluate_response(\n            response=response_vector\n        ).passing\n        \n        relevancy_result = relevancy_gpt4.evaluate_response(\n            query=question, response=response_vector\n        ).passing\n\n        total_response_time += elapsed_time\n        total_faithfulness += faithfulness_result\n        total_relevancy += relevancy_result\n\n    average_response_time = total_response_time / num_questions\n    average_faithfulness = total_faithfulness / num_questions\n    average_relevancy = total_relevancy / num_questions\n\n    <span class=\"hljs-keyword\">return</span> average_response_time, average_faithfulness, average_relevancy</span></pre>\n <h1>\n  Testing Across Different Chunk Sizes\n </h1>\n <p>\n  We\u2019ll evaluate a range of chunk sizes to identify which offers the most promising metrics.\n </p>\n <pre><span class=\"qk np gt pn b bf ql qm l qn qo\" id=\"262a\">chunk_sizes = [<span class=\"hljs-number\">128</span>, <span class=\"hljs-number\">256</span>, <span class=\"hljs-number\">512</span>, <span class=\"hljs-number\">1024</span>, <span class=\"hljs-number\">2048</span>]\n\n<span class=\"hljs-keyword\">for</span> chunk_size <span class=\"hljs-keyword\">in</span> chunk_sizes:\n  avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size, eval_questions)\n  <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Chunk size <span class=\"hljs-subst\">{chunk_size}</span> - Average Response time: <span class=\"hljs-subst\">{avg_response_time:<span class=\"hljs-number\">.2</span>f}</span>s, Average Faithfulness: <span class=\"hljs-subst\">{avg_faithfulness:<span class=\"hljs-number\">.2</span>f}</span>, Average Relevancy: <span class=\"hljs-subst\">{avg_relevancy:<span class=\"hljs-number\">.2</span>f}</span>\"</span>)</span></pre>\n <h1>\n  Bringing It All Together\n </h1>\n <p>\n  Let\u2019s compile the processes:\n </p>\n <pre><span class=\"qk np gt pn b bf ql qm l qn qo\" id=\"ba5d\"><span class=\"hljs-keyword\">import</span> nest_asyncio\n\nnest_asyncio.apply()\n\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> (\n    SimpleDirectoryReader,\n    VectorStoreIndex,\n    ServiceContext,\n)\n<span class=\"hljs-keyword\">from</span> llama_index.evaluation <span class=\"hljs-keyword\">import</span> (\n    DatasetGenerator,\n    FaithfulnessEvaluator,\n    RelevancyEvaluator\n)\n<span class=\"hljs-keyword\">from</span> llama_index.llms <span class=\"hljs-keyword\">import</span> OpenAI\n\n<span class=\"hljs-keyword\">import</span> openai\n<span class=\"hljs-keyword\">import</span> time\n\nopenai.api_key = <span class=\"hljs-string\">'OPENAI-API-KEY'</span>\n\n<span class=\"hljs-comment\"># Download Data</span>\n!mkdir -p <span class=\"hljs-string\">'data/10k/'</span>\n!wget <span class=\"hljs-string\">'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/10k/uber_2021.pdf'</span> -O <span class=\"hljs-string\">'data/10k/uber_2021.pdf'</span>\n\n<span class=\"hljs-comment\"># Load Data</span>\nreader = SimpleDirectoryReader(<span class=\"hljs-string\">\"./data/10k/\"</span>)\ndocuments = reader.load_data()\n\n<span class=\"hljs-comment\"># To evaluate for each chunk size, we will first generate a set of 40 questions from first 20 pages.</span>\neval_documents = documents[:<span class=\"hljs-number\">20</span>]\ndata_generator = DatasetGenerator.from_documents()\neval_questions = data_generator.generate_questions_from_nodes(num = <span class=\"hljs-number\">20</span>)\n\n<span class=\"hljs-comment\"># We will use GPT-4 for evaluating the responses</span>\ngpt4 = OpenAI(temperature=<span class=\"hljs-number\">0</span>, model=<span class=\"hljs-string\">\"gpt-4\"</span>)\n\n<span class=\"hljs-comment\"># Define service context for GPT-4 for evaluation</span>\nservice_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n\n<span class=\"hljs-comment\"># Define Faithfulness and Relevancy Evaluators which are based on GPT-4</span>\nfaithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\nrelevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4)\n\n<span class=\"hljs-comment\"># Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">evaluate_response_time_and_accuracy</span>(<span class=\"hljs-params\">chunk_size</span>):\n    total_response_time = <span class=\"hljs-number\">0</span>\n    total_faithfulness = <span class=\"hljs-number\">0</span>\n    total_relevancy = <span class=\"hljs-number\">0</span>\n\n    <span class=\"hljs-comment\"># create vector index</span>\n    llm = OpenAI(model=<span class=\"hljs-string\">\"gpt-3.5-turbo\"</span>)\n    service_context = ServiceContext.from_defaults(llm=llm, chunk_size=chunk_size)\n    vector_index = VectorStoreIndex.from_documents(\n        eval_documents, service_context=service_context\n    )\n\n    query_engine = vector_index.as_query_engine()\n    num_questions = <span class=\"hljs-built_in\">len</span>(eval_questions)\n\n    <span class=\"hljs-keyword\">for</span> question <span class=\"hljs-keyword\">in</span> eval_questions:\n        start_time = time.time()\n        response_vector = query_engine.query(question)\n        elapsed_time = time.time() - start_time\n        \n        faithfulness_result = faithfulness_gpt4.evaluate_response(\n            response=response_vector\n        ).passing\n        \n        relevancy_result = relevancy_gpt4.evaluate_response(\n            query=question, response=response_vector\n        ).passing\n\n        total_response_time += elapsed_time\n        total_faithfulness += faithfulness_result\n        total_relevancy += relevancy_result\n\n    average_response_time = total_response_time / num_questions\n    average_faithfulness = total_faithfulness / num_questions\n    average_relevancy = total_relevancy / num_questions\n\n    <span class=\"hljs-keyword\">return</span> average_response_time, average_faithfulness, average_relevancy\n\n<span class=\"hljs-comment\"># Iterate over different chunk sizes to evaluate the metrics to help fix the chunk size.</span>\n<span class=\"hljs-keyword\">for</span> chunk_size <span class=\"hljs-keyword\">in</span> [<span class=\"hljs-number\">128</span>, <span class=\"hljs-number\">256</span>, <span class=\"hljs-number\">512</span>, <span class=\"hljs-number\">1024</span>, <span class=\"hljs-number\">2048</span>]\n  avg_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size)\n  <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Chunk size <span class=\"hljs-subst\">{chunk_size}</span> - Average Response time: <span class=\"hljs-subst\">{avg_time:<span class=\"hljs-number\">.2</span>f}</span>s, Average Faithfulness: <span class=\"hljs-subst\">{avg_faithfulness:<span class=\"hljs-number\">.2</span>f}</span>, Average Relevancy: <span class=\"hljs-subst\">{avg_relevancy:<span class=\"hljs-number\">.2</span>f}</span>\"</span>)</span></pre>\n <h1>\n  Result\n </h1>\n <p>\n  The above table illustrates that as the chunk size increases, there is a minor uptick in the Average Response Time. Interestingly, the Average Faithfulness seems to reach its zenith at\n  <code class=\"cw pk pl pm pn b\">\n   chunk_size\n  </code>\n  of 1024, whereas Average Relevancy shows a consistent improvement with larger chunk sizes, also peaking at 1024. This suggests that a chunk size of 1024 might strike an optimal balance between response time and the quality of the responses, measured in terms of faithfulness and relevancy.\n </p>\n <h1>\n  Conclusion\n </h1>\n <p>\n  Identifying the best chunk size for a RAG system is as much about intuition as it is empirical evidence. With LlamaIndex\u2019s\n  <code class=\"cw pk pl pm pn b\">\n   Response Evaluation\n  </code>\n  module, you can experiment with various sizes and base your decisions on concrete data. When building a RAG system, always remember that\n  <code class=\"cw pk pl pm pn b\">\n   chunk_size\n  </code>\n  is a pivotal parameter. Invest the time to meticulously evaluate and adjust your chunk size for unmatched results.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 17525, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1fb1c0dd-614c-4cce-a613-fa39c6634069": {"__data__": {"id_": "1fb1c0dd-614c-4cce-a613-fa39c6634069", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.html", "file_name": "fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.html", "file_type": "text/html", "file_size": 13624, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.html", "file_name": "fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.html", "file_type": "text/html", "file_size": 13624, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "fc5a50fcd9707c0146040b8916947dc2bd0a20f73c7f38a45011f2db127e71ed", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  We\u2019ve added capabilities in LlamaIndex allowing you to fine-tune a linear adapter on top of embeddings produced from\n  <em class=\"nx\">\n   any\n  </em>\n  model (\n  <code class=\"cw ny nz oa ob b\">\n   sentence_transformers\n  </code>\n  , OpenAI, and more).\n </p>\n <p>\n  This allows you to transform your embedding representations into a new latent space that\u2019s optimized for retrieval over your specific data and queries. This can lead to small increases in retrieval performance that in turn translate to better performing RAG systems.\n </p>\n <p>\n  A nice bonus: you do\n  <em class=\"nx\">\n   not\n  </em>\n  need to re-embed your documents by using this adapter! Simply transform the query instead.\n </p>\n <p>\n  We have a\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding_adapter.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   full end-to-end guide\n  </a>\n  showing how you can generate a synthetic dataset, fine-tune the linear adapter, and evaluate its performance.\n </p>\n <h1>\n  Context\n </h1>\n <p>\n  The concept of fine-tuning your embedding model is powerful. In fact, we were inspired to both add a\n  <a href=\"https://github.com/run-llama/finetune-embedding\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   full example repository\n  </a>\n  /\n  <a href=\"https://medium.com/llamaindex-blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971\" rel=\"noopener\">\n   blog post\n  </a>\n  as well as\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   native abstractions in LlamaIndex\n  </a>\n  showing how you can fine-tune a sentence_transformers model over any unstructured text corpus (with our\n  <code class=\"cw ny nz oa ob b\">\n   SentenceTransformersFinetuneEngine\n  </code>\n  ).\n </p>\n <p>\n  However, this approach has some limitations:\n </p>\n <ul>\n  <li>\n   The\n   <code class=\"cw ny nz oa ob b\">\n    SentenceTransformersFinetuneEngine\n   </code>\n   is limited to fine-tuning\n   <code class=\"cw ny nz oa ob b\">\n    sentence_transformers\n   </code>\n   models.\n  </li>\n  <li>\n   After finetuning the embedding model, you will need to re-embed your document corpus.\n  </li>\n </ul>\n <p>\n  During our\n  <a href=\"https://www.youtube.com/watch?v=mndiDJ5k26A\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Finetuning + RAG webinar\n  </a>\n  last Friday, Jo (Vespa) mentioned the exact same problem: fine-tuning the embeddings model requires you to reindex your documents. However, his work with Vespa\n  <a href=\"https://blog.vespa.ai/tailoring-frozen-embeddings-with-vespa/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   explored the concept of \u201cfreezing\u201d document embeddings using a foundation model\n  </a>\n  , and instead training a transformation on the query embedding.\n </p>\n <p>\n  This inspired us to explore a similar embedding fine-tuning approach that was simultaneously more general but also allowed us to freeze existing document embeddings.\n </p>\n <h1>\n  Approach\n </h1>\n <p>\n  Our brand-new\n  <code class=\"cw ny nz oa ob b\">\n   EmbeddingAdapterFinetuneEngine\n  </code>\n  fine-tunes a\n  <strong>\n   linear adapter\n  </strong>\n  on top of query embeddings produced by any model. The\n  <strong>\n   linear adapter\n  </strong>\n  is simply a linear transformation that specifically transforms the query embedding\n  <em class=\"nx\">\n   while keeping document embeddings fixed\n  </em>\n  .\n </p>\n <p>\n  The linear adapter can be used on top of any existing embeddings model: SBERT embeddings, OpenAI embeddings, Cohere embeddings, and more. As a result you can just plug this in on top of any embedding model that you\u2019re already using!\n </p>\n <p>\n  Since document embeddings are unchanged, this means that you can always fine-tune this linear adapter\n  <em class=\"nx\">\n   after\n  </em>\n  you\u2019ve generated embeddings for your documents. You can choose to arbitrarily re-train this adapter on top of changing data distributions, without needing to re-embed all your documents.\n </p>\n <h2>\n  Technical Details\n </h2>\n <p>\n  As mentioned above, the linear adapter simply performs a linear transformation on top of the query embedding while keeping the Document embeddings fixed (with a weight matrix W + bias term b):\n </p>\n <p>\n  And that\u2019s it! If document embeddings can be represented as a (n x d) matrix D, where n is number of documents and d is the embedding dimension, then embedding similarity is just measured by\n </p>\n <p>\n  The linear adapter is trained using a similar loss term as the\n  <code class=\"cw ny nz oa ob b\">\n   MultipleNegativesRankingLoss\n  </code>\n  function in\n  <code class=\"cw ny nz oa ob b\">\n   sentence_transformers\n  </code>\n  \u2014 given a batch of positive (question, context) examples, the function uses cross-entropy loss under the hood to penalize the ground-truth (question, context) pairs for being far apart and swapped pairs for being too close.\n </p>\n <p>\n  <strong>\n   Additional Notes:\n  </strong>\n  We ended up writing the bulk of this fine-tuning logic in plain PyTorch, but taking heavy inspiration from the\n  <code class=\"cw ny nz oa ob b\">\n   sentence_transformers\n  </code>\n  <a href=\"https://github.com/UKPLab/sentence-transformers\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   source code\n  </a>\n  . We couldn\u2019t use sentence_transformers directly since we take in embeddings as inputs rather than raw text. You can take a look at some of our training code here.\n </p>\n <h1>\n  Notebook Walkthrough\n </h1>\n <p>\n  In this notebook walkthrough, we follow a similar set of steps as our\n  <a href=\"https://medium.com/llamaindex-blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971\" rel=\"noopener\">\n   previous blog post on embedding fine-tuning\n  </a>\n  :\n </p>\n <ol>\n  <li>\n   Generate a synthetic question-context dataset for both training and evaluation.\n  </li>\n  <li>\n   Fine-tuning our linear adapter on top of an existing model (e.g. SBERT)\n  </li>\n  <li>\n   Getting the embedding model, and evaluating it.\n  </li>\n </ol>\n <p>\n  As with the previous post, we use the UBER and LYFT 10K as example data. We use Lyft to generate our training dataset and Uber to generate our evaluation dataset.\n </p>\n <p>\n  The full guide is here:\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding_adapter.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://gpt-index.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding_adapter.html\n  </a>\n </p>\n <h2>\n  Generate a Synthetic Dataset for Trraining and Evaluation\n </h2>\n <p>\n  We use our helper abstractions,\n  <code class=\"cw ny nz oa ob b\">\n   generate_qa_embedding_pairs\n  </code>\n  , to generate our training and evaluation dataset. This function takes in any set of text nodes (chunks) and generates a structured dataset containing (question, context) pairs.\n </p>\n <pre><span class=\"rc ow gt ob b bf rd re l rf rg\" id=\"af1d\">from llama_index.finetuning import (\n    generate_qa_embedding_pairs,\n    EmbeddingQAFinetuneDataset,\n)\n\n# generate\ntrain_dataset = generate_qa_embedding_pairs(train_nodes)\nval_dataset = generate_qa_embedding_pairs(val_nodes)\n\n# save\ntrain_dataset.save_json(\"train_dataset.json\")\nval_dataset.save_json(\"val_dataset.json\")\n\n# load \ntrain_dataset = EmbeddingQAFinetuneDataset.from_json(\"train_dataset.json\")\nval_dataset = EmbeddingQAFinetuneDataset.from_json(\"val_dataset.json\")</span></pre>\n <h2>\n  Fine-tuning our Linear Adapter\n </h2>\n <p>\n  We then fine-tune our linear adapter on top of an existing embedding model. We import our new\n  <code class=\"cw ny nz oa ob b\">\n   EmbeddingAdapterFinetuneEngine\n  </code>\n  abstraction, which takes in an existing embedding model and a set of training parameters.\n </p>\n <p>\n  In this example we use the\n  <code class=\"cw ny nz oa ob b\">\n   bge-small-en\n  </code>\n  sentence-transformers model, but we can also use any embedding model in LlamaIndex/LangChain.\n </p>\n <pre><span class=\"rc ow gt ob b bf rd re l rf rg\" id=\"518c\"><span class=\"hljs-keyword\">from</span> llama_index.finetuning <span class=\"hljs-keyword\">import</span> EmbeddingAdapterFinetuneEngine\n<span class=\"hljs-keyword\">from</span> llama_index.embeddings <span class=\"hljs-keyword\">import</span> resolve_embed_model\n<span class=\"hljs-keyword\">import</span> torch\n\nbase_embed_model = resolve_embed_model(<span class=\"hljs-string\">\"local:BAAI/bge-small-en\"</span>)\n<span class=\"hljs-comment\"># alternative: use OpenAI</span>\n<span class=\"hljs-comment\"># from llama_index.embeddings import OpenAIEmbedding</span>\n<span class=\"hljs-comment\"># openai = OpenAIEmbedding()</span>\n\nfinetune_engine = EmbeddingAdapterFinetuneEngine(\n    train_dataset,\n    base_embed_model,\n    model_output_path=<span class=\"hljs-string\">\"&amp;lt;model_output_path&amp;gt;\"</span>,\n    epochs=<span class=\"hljs-number\">4</span>,\n    verbose=<span class=\"hljs-literal\">True</span>,\n    <span class=\"hljs-comment\"># can optionally pass along any parameters that go into `train_model`</span>\n    <span class=\"hljs-comment\"># optimizer_class=torch.optim.SGD,</span>\n    <span class=\"hljs-comment\"># optimizer_params={\"lr\": 0.01}</span>\n)</span></pre>\n <p>\n  We can then call fine-tune to kick off the fine-tuning job. Training a linear model is quite straightforward and doesn\u2019t require heavy machinery \u2014 this can easily run on a Macbook.\n </p>\n <pre><span class=\"rc ow gt ob b bf rd re l rf rg\" id=\"a496\">finetune_engine.finetune()</span></pre>\n <h2>\n  Getting the Embedding Model, and Evaluating it\n </h2>\n <p>\n  Once the fine-tuning job is then, we can then fetch our embedding model.\n </p>\n <p>\n  We can either directly fetch it from our\n  <code class=\"cw ny nz oa ob b\">\n   finetune_engine\n  </code>\n  , or import our new\n  <code class=\"cw ny nz oa ob b\">\n   LinearAdapterEmbeddingModel\n  </code>\n  and construct it in a more manual fashion.\n </p>\n <p>\n  Option 1:\n </p>\n <pre><span class=\"rc ow gt ob b bf rd re l rf rg\" id=\"89c4\">embed_model = finetune_engine.get_finetuned_model()</span></pre>\n <p>\n  Option 2:\n </p>\n <pre><span class=\"rc ow gt ob b bf rd re l rf rg\" id=\"718d\"><span class=\"hljs-keyword\">from</span> llama_index.embeddings <span class=\"hljs-keyword\">import</span> LinearAdapterEmbeddingModel\n\nembed_model = LinearAdapterEmbeddingModel(base_embed_model, <span class=\"hljs-string\">\"&amp;lt;model_output_path&amp;gt;\"</span>)</span></pre>\n <p>\n  The next step is to evaluate it. We compare the fine-tuned model against the base model, as well as against\n  <code class=\"cw ny nz oa ob b\">\n   text-embedding-ada-002\n  </code>\n  .\n </p>\n <p>\n  We evaluate with two ranking metrics:\n </p>\n <ul>\n  <li>\n   <strong>\n    Hit-rate metric:\n   </strong>\n   For each (query, context) pair, we retrieve the top-k documents with the query. It\u2019s a\n   <em class=\"nx\">\n    hit\n   </em>\n   if the results contain the ground-truth context.\n  </li>\n  <li>\n   <strong>\n    Mean Reciprocal Rank\n   </strong>\n   : A slightly more granular ranking metric that looks at the \u201creciprocal rank\u201d of the ground-truth context in the top-k retrieved set. The reciprocal rank is defined as 1/rank. Of course, if the results don\u2019t contain the context, then the reciprocal rank is 0.\n  </li>\n </ul>\n <p>\n  Some additional comments:\n </p>\n <ul>\n  <li>\n   We ran with 4 epochs over the Lyft documents\n  </li>\n  <li>\n   We used Adam as an optimizer with the default learning rate (we tried SGD and it didn\u2019t work as well)\n  </li>\n </ul>\n <p>\n  <strong>\n   Results\n  </strong>\n </p>\n <figure>\n  <figcaption class=\"or fe os od oe ot ou be b bf z dt\">\n   Quantiative metrics (hit-rate and MRR) for ada, bge, and our fine-tuned model\n  </figcaption>\n </figure>\n <p>\n  In terms of hit-rate, the base model gets 78.7% hit-rate on the validation dataset, and the fine-tuned model gets 79.8%. In the meantime\n  <code class=\"cw ny nz oa ob b\">\n   text-embedding-ada-002\n  </code>\n  gets 87.0%.\n </p>\n <p>\n  In terms of MRR, the base model gets 64.3%, and the fine-tuned model gets 66%.\n  <code class=\"cw ny nz oa ob b\">\n   text-embedding-ada-002\n  </code>\n  gets 68.4%.\n </p>\n <p>\n  There is some performance bump from the fine-tuned model, though admittedly it is small \u2014 it is smaller than the performance bump gained through fine-tuning sentence_transformers directly on the latest dataset.\n </p>\n <p>\n  That said, a performance bump is still a performance bump, and it\u2019s very cheap for you to spin up and try yourself! So you can decide whether or not this would make sense for you.\n </p>\n <h1>\n  Conclusion\n </h1>\n <p>\n  We created a brand-new module in LlamaIndex that allows you fine-tune a linear adapter on top of any embedding model.\n </p>\n <p>\n  It can help you eke out some marginal improvement in retrieval metrics; importantly, it allows you to keep document embeddings fixed and only transform the query.\n </p>\n <h2>\n  Resources\n </h2>\n <p>\n  Guide:\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding_adapter.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://gpt-index.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding_adapter.html\n  </a>\n </p>\n <p>\n  Training code (if you want to take a look for yourself):\n  <a href=\"https://github.com/jerryjliu/llama_index/blob/main/llama_index/finetuning/embeddings/adapter_utils.py\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://github.com/jerryjliu/llama_index/blob/main/llama_index/finetuning/embeddings/adapter_utils.py\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 13587, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5133220-b12b-46a4-bbea-dcdc289b1280": {"__data__": {"id_": "e5133220-b12b-46a4-bbea-dcdc289b1280", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.html", "file_name": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.html", "file_type": "text/html", "file_size": 15144, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.html", "file_name": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.html", "file_type": "text/html", "file_size": 15144, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "c50a7cfb5e9442b43c1e44ef6e08a6943c70d7b43f3bf78e4c1eccad6d2579eb", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  <strong>\n   UPDATE 9/10/2023:\n  </strong>\n  We\u2019ve included embedding finetuning abstractions into the LlamaIndex repo, so this repo is technically outdated! Please check out our\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/end_to_end_tutorials/finetuning.html#finetuning-embeddings-for-better-retrieval-performance\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   embedding fine-tuning guides\n  </a>\n  in the core documentation.\n </p>\n <p>\n  We\u2019ve created a\n  <a href=\"https://github.com/run-llama/finetune-embedding\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   comprehensive, end-to-end guide\n  </a>\n  showing you how to fine-tune an embedding model to improve performance of Retrieval Augmented Generation (RAG) systems over any unstructured text corpus (no labels required!).\n </p>\n <p>\n  The result is a\n  <a href=\"https://github.com/run-llama/finetune-embedding/blob/main/evaluate.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    5\u201310% performance increase in retrieval evaluation metrics\n   </strong>\n  </a>\n  \u2014 our finetuned\n  <code class=\"cw nz oa ob oc b\">\n   bge\n  </code>\n  model almost reaches\n  <code class=\"cw nz oa ob oc b\">\n   text-embedding-ada-002\n  </code>\n  levels of retrieval performance in terms of hit rate. This enables more accurate retrieval which leads to better RAG systems as a whole.\n </p>\n <p>\n  This tutorial is helpful to\n  <em class=\"od\">\n   anyone\n  </em>\n  building RAG systems:\n </p>\n <ul>\n  <li>\n   If you\u2019re new to finetuning, no problem! We have\n   <a href=\"https://github.com/run-llama/finetune-embedding#steps-for-running\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    step by step notebooks\n   </a>\n   walking through the key steps. Simply substitute the file links for your own data, and just run every cell.\n  </li>\n  <li>\n   Finetuning embedding models is lightweight and doesn\u2019t require a GPU. These notebooks were tested on an M2 Macbook Pro.\n  </li>\n </ul>\n <p>\n  <strong>\n   Resources\n  </strong>\n </p>\n <ul>\n  <li>\n   Repo:\n   <a href=\"https://github.com/run-llama/finetune-embedding\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    https://github.com/run-llama/finetune-embedding\n   </a>\n  </li>\n  <li>\n   Notebooks:\n   <a href=\"https://github.com/run-llama/finetune-embedding/blob/main/generate_dataset.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Dataset Generation\n   </a>\n   ,\n   <a href=\"https://github.com/run-llama/finetune-embedding/blob/main/finetune.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Finetuning\n   </a>\n   ,\n   <a href=\"https://github.com/run-llama/finetune-embedding/blob/main/evaluate.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Evaluation\n   </a>\n  </li>\n </ul>\n <h1>\n  Background/Context\n </h1>\n <h2>\n  <strong>\n   The Current RAG Stack\n  </strong>\n </h2>\n <p>\n  RAG is a popular paradigm for connecting Large Language Models (LLMs) with an external source of data that was not present in its training corpus. It pairs a\n  <strong>\n   retrieval model\n  </strong>\n  over a knowledge bank with the LLM through its input prompt space. RAG stacks typically look like the following:\n </p>\n <ul>\n  <li>\n   <strong>\n    Indexing\n   </strong>\n   : Prepare a corpus of unstructured text, parse/chunk it. Then\n   <em class=\"od\">\n    embed\n   </em>\n   each chunk and put in a vector database.\n  </li>\n  <li>\n   <strong>\n    Query-time:\n   </strong>\n   <em class=\"od\">\n    Retrieve\n   </em>\n   context from the vector db using top-k embedding similarity lookup, and stuff context into the LLM input space.\n  </li>\n </ul>\n <p>\n  (Of course RAG can be much more advanced than this, and LlamaIndex provides tools for both\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/getting_started/concepts.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   simple and advanced RAG\n  </a>\n  )\n </p>\n <p>\n  Unfortunately RAG is easy to prototype by cobbling together the different components, but hard to productionize. The simple stack has many failure modes and oftentimes the issue lies with bad retrieval \u2014 if the returned context is irrelevant to the query, then the capability of the LLM is irrelevant; the answer will always be bad.\n </p>\n <h2>\n  <strong>\n   How Can We Make Retrieval Better?\n  </strong>\n </h2>\n <p>\n  We can try more sophisticated retrieval algorithms (e.g. hybrid search, reranking).\n </p>\n <p>\n  An\n  <a href=\"https://twitter.com/jerryjliu0/status/1692931028963221929?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   insight\n  </a>\n  from our recent\n  <a href=\"https://www.youtube.com/watch?v=Zj5RCweUHIk\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   production RAG webinar\n  </a>\n  , however, is that the embeddings themselves may not live in an optimal latent space for your data. Embeddings generated by pre-trained models may be close/far from each other based on the pre-training objective, but may not completely align with your own retrieval objective. For instance, if you\u2019re building search over ML ArXiv papers, you may want the embeddings to align semantically with specific ML concepts (e.g. \u201cLLMs\u201d, \u201cNLP\u201d) and not filler words \u201cThis paper is\u2026\u201d).\n </p>\n <p>\n  Finetuning is a way to solve that. The concept of finetuning has become increasingly popular in the LLM space, with\n  <a href=\"https://github.com/artidoro/qlora\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   technological advancements\n  </a>\n  as well as\n  <a href=\"https://platform.openai.com/docs/guides/fine-tuning\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   easy-to-use services\n  </a>\n  .\n </p>\n <p>\n  In this tutorial, we focus on\n  <strong>\n   finetuning the embedding model.\n  </strong>\n  We show how finetuning the embedding model can lead to better retrieval performance.\n </p>\n <h2>\n  Challenges/Considerations\n </h2>\n <p>\n  When you finetune embeddings, you need training examples. In the case of embeddings, this typically means that you have both \u201cpositive\u201d and \u201cnegative\u201d examples \u2014 pairs of texts that should be close to each other and far from each other.\n </p>\n <p>\n  An issue is that we don\u2019t have these positive or negative examples apriori. Given a dataset of unstructured text, is it possible to\n  <strong>\n   automatically\n  </strong>\n  generate these example pairs?\n </p>\n <p>\n  With LlamaIndex you can! We use LlamaIndex modules to automatically generate a set of questions from unstructured text chunks. These (question, chunk) pairs are then used as positive examples as training signals for the model (negative examples are randomly sampled across other chunks).\n </p>\n <p>\n  The next section shows a full walkthrough across all of our modules.\n </p>\n <h1>\n  Walkthrough\n </h1>\n <p>\n  At a high-level, we do the following:\n </p>\n <ol>\n  <li>\n   Generating synthetic dataset for training and evaluation (\n   <a href=\"https://github.com/run-llama/finetune-embedding/blob/main/generate_dataset.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   )\n  </li>\n  <li>\n   Finetuning an opensource embedding model (\n   <a href=\"https://github.com/run-llama/finetune-embedding/blob/main/finetune.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   )\n  </li>\n  <li>\n   Evaluating the embedding model (\n   <a href=\"https://github.com/run-llama/finetune-embedding/blob/main/evaluate.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   )\n  </li>\n </ol>\n <h2>\n  Generating synthetic dataset for training and evaluation\n </h2>\n <p>\n  The key idea here is that we can leverage an LLM to generate hypothetical questions that are best answered by a given piece of context. This allows us to generate synthetic positive pairs of (query, relevant documents) in a scalable way without requiring human labellers.\n </p>\n <p>\n  More concretely, we first process the given documents into a corpus of text chunks. We do this with the\n  <code class=\"cw nz oa ob oc b\">\n   SimpleNodeParser\n  </code>\n  module in LlamaIndex:\n </p>\n <pre><span class=\"ra pf gt oc b bf rb rc l rd re\" id=\"03a6\">parser = <span class=\"hljs-title class_\">SimpleNodeParser</span>()\nnodes = parser.get_nodes_from_documents(docs, show_progress=verbose)\ncorpus = {\n  node.<span class=\"hljs-symbol\">node_id:</span> node.get_content(metadata_mode=<span class=\"hljs-title class_\">MetadataMode</span>.<span class=\"hljs-variable constant_\">NONE</span>) \n  <span class=\"hljs-keyword\">for</span> node <span class=\"hljs-keyword\">in</span> nodes\n}</span></pre>\n <p>\n  Then for each text chunk, we use LLM to generate a few hypothetical questions that can be answered with information form that text chunk. The example prompt is shown below as well.\n </p>\n <pre><span class=\"ra pf gt oc b bf rb rc l rd re\" id=\"92cb\">prompt_template = prompt_template <span class=\"hljs-keyword\">or</span> <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\\\n  Context information is below.\n  \n  ---------------------\n  {context_str}\n  ---------------------\n  \n  Given the context information and not prior knowledge.\n  generate only questions based on the below query.\n  \n  You are a Teacher/ Professor. Your task is to setup \\\n  {num_questions_per_chunk} questions for an upcoming \\\n  quiz/examination. The questions should be diverse in nature \\\n  across the document. Restrict the questions to the \\\n  context information provided.\"</span>\n  <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n\n# for a given node, extract questions (do this over all nodes in outer loop)\nquery = prompt_template.format(context_str=text, num_questions_per_chunk=num_questions_per_chunk)\nresponse = llm.complete(query)\n\nresult = str(response).strip().split(\"</span>\\n<span class=\"hljs-string\">\")\nquestions = [\n    re.sub(r\"</span>^\\d+[\\).\\s]<span class=\"hljs-string\">\", \"</span><span class=\"hljs-string\">\", question).strip() for question in result\n]\nquestions = [question for question in questions if len(question) &amp;gt; 0]\n</span></span></pre>\n <p>\n  Finally, we collect all pairs of questions and text chunks as the dataset. Example query, chunk, and mapping is shown below.\n </p>\n <pre><span class=\"ra pf gt oc b bf rb rc l rd re\" id=\"c535\">\n# example query\nf331640a-b407-4028-8db8-4b8db691dd34: \"What is the market value of Lyft's common stock held by non-affiliates as of June 30, 2021, based on the closing sales price of the Class A common stock on that date?\"\n\n# example corpus\nd5554f3e-cdaf-41d7-ac49-8f0ffe3f5759:\"UNITED STATESSECURITIES AND...\"\n\n# example mapping\nf331640a-b407-4028-8db8-4b8db691dd34: d5554f3e-cdaf-41d7-ac49-8f0ffe3f5759</span></pre>\n <h2>\n  Finetuning an opensource embedding model\n </h2>\n <p>\n  We leverage the high-level model fitting API from\n  <code class=\"cw nz oa ob oc b\">\n   sentencetransformers\n  </code>\n  to very easily setup a training process.\n </p>\n <p>\n  We use\n  <code class=\"cw nz oa ob oc b\">\n   MultipleNegativesRankingLoss\n  </code>\n  as the training object and\n  <code class=\"cw nz oa ob oc b\">\n   InformationRetrievalEvaluator\n  </code>\n  as the evaluator during training. Also, we use\n  <code class=\"cw nz oa ob oc b\">\n   <a href=\"https://huggingface.co/BAAI/bge-small-en\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    BAAI/bge-small-en\n   </a>\n  </code>\n  on Hugging Face as the base model and train for a small number of epochs.\n </p>\n <pre><span class=\"ra pf gt oc b bf rb rc l rd re\" id=\"2b6b\"><span class=\"hljs-comment\"># define model</span>\nmodel_id = <span class=\"hljs-string\">\"BAAI/bge-small-en\"</span>\nmodel = SentenceTransformer(model_id)\n\n...\n\n<span class=\"hljs-comment\"># define loss</span>\n<span class=\"hljs-keyword\">from</span> sentence_transformers <span class=\"hljs-keyword\">import</span> losses\nloss = losses.MultipleNegativesRankingLoss(model)\n\n<span class=\"hljs-comment\"># define evaluator</span>\n<span class=\"hljs-keyword\">from</span> sentence_transformers.evaluation <span class=\"hljs-keyword\">import</span> InformationRetrievalEvaluator\n<span class=\"hljs-comment\"># define over validation dataset</span>\n...\nevaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs)\n\n<span class=\"hljs-comment\"># run training</span>\n...\nmodel.fit(\n    train_objectives=[(loader, loss)],\n    epochs=EPOCHS,\n    warmup_steps=warmup_steps,\n    output_path=<span class=\"hljs-string\">'exp_finetune'</span>,\n    show_progress_bar=<span class=\"hljs-literal\">True</span>,\n    evaluator=evaluator, \n    evaluation_steps=<span class=\"hljs-number\">50</span>,\n)</span></pre>\n <h2>\n  Evaluating the embedding model\n </h2>\n <p>\n  We compare the finetuned model against the base model, as well as the OpenAI embedding model\n  <code class=\"cw nz oa ob oc b\">\n   text-embedding-ada-002\n  </code>\n  .\n </p>\n <p>\n  We evaluate with two main metrics:\n </p>\n <ul>\n  <li>\n   <strong>\n    Hit-rate metric:\n   </strong>\n   For each (query, relevant_doc) pair, we retrieve the top-k documents with the query. It\u2019s a\n   <em class=\"od\">\n    hit\n   </em>\n   if the results contain relevant_doc.\n  </li>\n  <li>\n   <code class=\"cw nz oa ob oc b\">\n    InformationRetrievalEvaluator\n   </code>\n   from sentence_transformers. This provides a comprehensive suite of metrics such as cosine similarity accuracy, precision, recall at different top-k values.\n  </li>\n </ul>\n <p>\n  <strong>\n   Results\n  </strong>\n </p>\n <p>\n  In terms of hit-rate metric, the base model gets 78% hit-rate on the validation dataset, and the fine-tuned model gets 84%.\n  <code class=\"cw nz oa ob oc b\">\n   text-embedding-ada-002\n  </code>\n  gets 87%, which means that our fine-tuned model is only 3% off!\n </p>\n <figure>\n  <figcaption class=\"pa fe pb om on pc pd be b bf z dt\">\n   Hit-rate for `text-embedding-ada-002`, base model, finetuned model\n  </figcaption>\n </figure>\n <p>\n  The InformationRetrievalEvaluator shows a similar improvement across an entire suite of metrics. The fine-tuned model increases evaluation metrics by 5\u201310% compared to the base-model.\n </p>\n <figure>\n  <figcaption class=\"pa fe pb om on pc pd be b bf z dt\">\n   Evaluation suite from `InformationRetrievalEvaluator`\n  </figcaption>\n </figure>\n <h1>\n  Conclusion\n </h1>\n <p>\n  We successfully finetuned an embedding model over unlabeled, unstructured data to give better retrieval performance for downstream RAG systems. We show a 5\u201310% improvement across all metrics!\n </p>\n <p>\n  <strong>\n   Resources\n  </strong>\n </p>\n <p>\n  (copied from intro)\n </p>\n <ul>\n  <li>\n   Repo:\n   <a href=\"https://github.com/run-llama/finetune-embedding\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    https://github.com/run-llama/finetune-embedding\n   </a>\n  </li>\n  <li>\n   Notebooks:\n   <a href=\"https://github.com/run-llama/finetune-embedding/blob/main/generate_dataset.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Dataset Generation\n   </a>\n   ,\n   <a href=\"https://github.com/run-llama/finetune-embedding/blob/main/finetune.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Finetuning\n   </a>\n   ,\n   <a href=\"https://github.com/run-llama/finetune-embedding/blob/main/evaluate.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Evaluation\n   </a>\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 15095, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cb09bc93-1d03-429b-bf6b-157bf6d0e135": {"__data__": {"id_": "cb09bc93-1d03-429b-bf6b-157bf6d0e135", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.html", "file_name": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.html", "file_type": "text/html", "file_size": 27896, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.html", "file_name": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.html", "file_type": "text/html", "file_size": 27896, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "b55b0b4c394a6d30a901e020006797f9f5a3f9e41d9690ae0352f5c4ff66f831", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h1>\n  Introduction\n </h1>\n <p>\n  GPT-4V has amazed us with its ability to analyze images and even generate website code from visuals.\n </p>\n <p>\n  This blog post investigates GPT-4V\u2019s proficiency in interpreting bar charts, scatter plots, and tables. We aim to assess whether specific questioning and chain of thought prompting can yield better responses compared to broader inquiries. Our demonstration seeks to determine if GPT-4V can exceed these known limitations with precise questioning and systematic reasoning techniques.\n </p>\n <p>\n  We observed in these experiments that asking specific questions, rather than general ones, yields better answers. Let\u2019s delve into these experiments.\n </p>\n <p>\n  You can also follow along with this blog post in our\n  <a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/multi_modal/gpt4v_experiments_cot.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Google Colab Notebook\n  </a>\n  .\n </p>\n <blockquote>\n  <p class=\"oh oi po oj b ok pf om on oo pg oq or os ph ou ov ow pi oy oz pa pj pc pd pe gm bj\" id=\"2203\">\n   N\n   <!-- -->\n   OTE: This blog post aims to inform the community about GPT-4V\u2019s performance, though the results might not be universally applicable. We strongly advise conducting tests with similar questions on your own dataset before drawing conclusions.\n  </p>\n </blockquote>\n <h1>\n  Experiment:\n </h1>\n <p>\n  What we\u2019ll do is take an image of each of a bar chart, scatter plot, and table and analyze them by asking three types of questions.\n </p>\n <ol>\n  <li>\n   General Question: Simply ask, \u201cAnalyse the image.\u201d\n  </li>\n  <li>\n   Specific Question: Specific Question the performance of a certain category by providing more details.\n  </li>\n  <li>\n   Chain of Thought Prompting: Use a step-by-step reasoning method to walk through the analysis.\n  </li>\n </ol>\n <p>\n  These guidelines aim to test how different questioning techniques might improve the precision of the information we gather from the images.\n </p>\n <h1>\n  Data for experiments:\n </h1>\n <p>\n  To test out we selected the following images from\n  <a href=\"https://arxiv.org/pdf/2307.09288.pdf\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Llama2\n  </a>\n  and\n  <a href=\"https://arxiv.org/pdf/2310.06825.pdf\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   MistralAI\n  </a>\n  papers.\n </p>\n <ol>\n  <li>\n   Violation percentage of safety with different LLMs across categories. (Llama2 paper)\n  </li>\n  <li>\n   Llama2 vs Mistral model performances across various NLP tasks.(Mistral paper)\n  </li>\n  <li>\n   Performances of different LLMs across various NLP tasks. (Llama2 paper)\n  </li>\n </ol>\n <p>\n  Let\u2019s go through each of these data points one by one.\n </p>\n <h1>\n  Image 1 \u2014 Violation percentage of safety with different LLMs across categories.\n </h1>\n <h1>\n  General Question:\n </h1>\n <p>\n  <strong>\n   Question:\n  </strong>\n  Analyse the Image.\n </p>\n <p>\n  <strong>\n   Answer:\n  </strong>\n </p>\n <blockquote>\n  <p class=\"oh oi po oj b ok pf om on oo pg oq or os ph ou ov ow pi oy oz pa pj pc pd pe gm bj\" id=\"b825\">\n   The image you\u2019ve provided is a bar chart displaying data grouped into three categories of online violations: hateful and harmful, illicit and criminal activity, and unqualified advice. Each of these categories has a number in parentheses, indicating the sample size of reported incidents for each type of violation (395, 728, and 311, respectively). The x-axis of the chart specifies various social media platforms or services, such as \u201cVideo sharing\u201d, \u201cSocial networking\u201d, \u201cGaming\u201d, \u201cDating\u201d, \u201cForums &amp; boards\u201d, \u201cCommercial Websites\u201d, \u201cMedia sharing\u201d, \u201cP2P/File sharing\u201d, \u201cWiki\u201d, and \u201cOther\u201d. It appears to measure how often these categories of violations occur on each type of platform. The y-axis measures the percentage of violations reported, ranging from 0% to 60%. Each platform/service has three bars corresponding to the three violation categories, showing their respective percentages. The bars also have error bars, which typically represent the variability of the data, such as standard deviation, standard error, or confidence interval, indicating the precision of the estimates. The chart has a legend indicating the color corresponding to each of the three categories of violations. This visual representation helps to compare the prevalence of different types of violations across the different types of online services. Without specific numbers, it\u2019s not possible to provide exact percentages, but we can observe trends, such as: \u2014 \u201cForums &amp; boards\u201d and \u201cSocial networking\u201d platforms have notably higher percentages across all three types of violations compared to other platforms. \u2014 \u201cCommercial Websites\u201d seem to have lower percentages of reported hateful and harmful activities and unqualified advice but higher percentages of illicit and criminal activities when compared to other platforms like \u201cWiki\u201d or \u201cP2P/File sharing\u201d. \u2014 \u201cGaming\u201d appears to have a moderate percentage of hateful and harmful violations, lower levels of illicit and criminal activity, and relatively higher levels of unqualified advice. Overall, this chart is a visual tool that likely aims to inform about the prevalence of certain types of online violations across different digital platforms.\n  </p>\n </blockquote>\n <h2>\n  Observation:\n </h2>\n <p>\n  As you can see though the categories hateful and harmful, illicit and criminal activity, and unqualified advice but it hallucinated with x-axis values with \u2014 \u201cVideo sharing\u201d, \u201cSocial networking\u201d, \u201cGaming\u201d, \u201cDating\u201d, \u201cForums &amp; boards\u201d, \u201cCommercial Websites\u201d, \u201cMedia sharing\u201d, \u201cP2P/File sharing\u201d, \u201cWiki\u201d, and \u201cOther\u201d.\n </p>\n <h1>\n  Specific Questions:\n </h1>\n <p>\n  <strong>\n   Question:\n  </strong>\n  Which model among llama2 and vicuna models does better in terms of violation percentages in Hateful and harmful category.\n </p>\n <p>\n  <strong>\n   Answer:\n  </strong>\n </p>\n <blockquote>\n  <p class=\"oh oi po oj b ok pf om on oo pg oq or os ph ou ov ow pi oy oz pa pj pc pd pe gm bj\" id=\"a020\">\n   From the provided bar chart, we can analyze the violation percentage for both the Llama2 and Vicuna models in the \u201cHateful and harmful\u201d category. To determine which model does better (i.e., has a lower violation percentage), you would look at the height of the bars corresponding to each model within that category. In the \u201cHateful and harmful\u201d category (which is the first group of bars on the left), you can compare the blue bar (representing Llama2) to the light blue bar (representing Vicuna). The model with the shorter bar in this category will have a lower violation percentage, and hence, better performance with respect to minimizing hateful and harmful content. Please note that I cannot physically see the chart, so I\u2019m unable to specify which model has the lower violation percentage. If you provide the actual percentages or describe the relative bar lengths for Llama2 and Vicuna in the \u201cHateful and harmful\u201d category, I could then tell you which model performs better in that respect.\n  </p>\n </blockquote>\n <h2>\n  <strong>\n   Observation:\n  </strong>\n </h2>\n <p>\n  It failed to accurately capture the information, mistakenly identifying the light blue bar as representing Vicuna when, in fact, it is the light blue bar that represents Llama2.\n </p>\n <p>\n  Let us give more information on the question and see if it can improve the answer.\n </p>\n <p>\n  <strong>\n   Question:\n  </strong>\n  In the image provided to you depicts about the violation rate performance of various AI models across Hateful and harmful, Illicit and criminal activity, Unqualified advice categories. Hateful and harmful category is in first column. Bars with light blue are with Llama2 model and dark blue are with Vicuna models. With this information, Can you compare about Llama2 and Vicuna models in Hateful and harmful category.\n </p>\n <p>\n  <strong>\n   Answer:\n  </strong>\n </p>\n <blockquote>\n  <p class=\"oh oi po oj b ok pf om on oo pg oq or os ph ou ov ow pi oy oz pa pj pc pd pe gm bj\" id=\"3578\">\n   In the image you provided, which shows a bar graph for the violation rate performance of various AI models in the Hateful and harmful category, we can analyze the light blue bars that represent the Llama2 model and the dark blue bars that represent the Vicuna model. Based on the visual information given, I can compare the two models within this category by observing the heights of the light blue bars (Llama2) versus the heights of the dark blue bars (Vicuna) for each subsection within the category. A lower bar indicates a lower violation rate, which could be interpreted as better performance in minimizing violations for the given criteria. Since I can\u2019t give you the exact numbers or percentages, I would describe their relative performances. It appears that for some subsections, the Llama2 bars are shorter than the Vicuna bars, suggesting that the Llama2 model could have a lower violation rate in those areas. Conversely, in other subsections, the Vicuna bars might be shorter than the Llama2 bars, indicating a lower violation rate for the Vicuna model in those areas. The exact subsections where one model outperforms the other would depend on their relative bar heights, which should be compared individually for the given information.\n  </p>\n </blockquote>\n <h2>\n  <strong>\n   Observation:\n  </strong>\n </h2>\n <p>\n  It did answer the question correctly.\n </p>\n <h1>\n  Chain Of Thought Prompting:\n </h1>\n <p>\n  <strong>\n   Question:\n  </strong>\n  Based on the image provided. Follow the steps and answer the query \u2014 which model among llama2 and vicuna does better in terms of violation percentages in \u2018Hateful and harmful\u2019.\n </p>\n <p>\n  Examine the Image: Look at the mentioned category in the query in the Image.\n </p>\n <p>\n  Identify Relevant Data: Note the violation percentages.\n </p>\n <p>\n  Evaluate: Compare if there is any comparison required as per the query.\n </p>\n <p>\n  Draw a Conclusion: Now draw the conclusion based on the whole data.\n </p>\n <p>\n  <strong>\n   Answer:\n  </strong>\n </p>\n <blockquote>\n  <p class=\"oh oi po oj b ok pf om on oo pg oq or os ph ou ov ow pi oy oz pa pj pc pd pe gm bj\" id=\"edbb\">\n   Examine the Image: The image is a bar chart depicting violation percentages in three categories: \u201cHateful and harmful,\u201d \u201cIllicit and criminal activity,\u201d and \u201cUnqualified advice.\u201d Each category has two bars next to each other representing two models named \u201cllama2\u201d and \u201cvicuna.\u201d Identify Relevant Data: We are specifically interested in the \u201cHateful and harmful\u201d category, which is the first set of bars on the left. The two bars indicate the violation percentages for \u201cllama2\u201d and \u201cvicuna.\u201d Evaluate: By visually inspecting the bar corresponding to \u201cHateful and harmful,\u201d we see that the blue bar (indicating \u201cllama2\u201d) has a lower percentage than the red bar (indicating \u201cvicuna\u201d). The exact percentages are not clear, but we can discern the relative performance between the two. Draw a Conclusion: From the data visible, \u201cllama2\u201d has a lower violation percentage than \u201cvicuna\u201d in the \u201cHateful and harmful\u201d category, thereby doing better in that respect according to the given image.\n  </p>\n </blockquote>\n <h2>\n  Observation:\n </h2>\n <p>\n  With chain of thought prompting it did hallicunate with bar colours but answered correctly saying Llama2 has lower violation compared to vicuna in Hateful and harmful though for a section Llama2 has higher violation compared to vicuna.\n </p>\n <h1>\n  Image2 \u2014 Llama2 vs Mistral model performances across various NLP tasks.\n </h1>\n <h1>\n  General Question:\n </h1>\n <p>\n  <strong>\n   Question:\n  </strong>\n  Analyze the Image.\n </p>\n <p>\n  <strong>\n   Answer:\n  </strong>\n </p>\n <blockquote>\n  <p class=\"oh oi po oj b ok pf om on oo pg oq or os ph ou ov ow pi oy oz pa pj pc pd pe gm bj\" id=\"2870\">\n   The image appears to be a set of four line graphs that compare the performance of different natural language processing (NLP) models across four different metrics: Natural Language Understanding (NLU), Reasoning, Knowledge, and Commonsense. The models compared are LLaMA 2, LLaMA 13B, Mistral, and GPT-3. Each graph plots the performance metric (on the y-axis) against the model size measured in billions of effective parameters (on the x-axis). In general, these plots demonstrate that as the model size increases, the performance on each metric improves. Here are specific observations for each graph: 1. Natural Language Understanding (Top left): All models show an increase in NLU performance as the model size increases. Mistral is depicted as the highest-performing model at each size benchmark. 2. Reasoning (Top right): Again, larger models perform better on reasoning tasks, with Mistral consistently outperforming the other models mentioned. 3. Knowledge (Bottom left): Similar to the NLU and reasoning tasks, the trend is for better performance with larger models, with Mistral leading across the different sizes. 4. Commonsense (Bottom right): This graph follows the same trend with the Mistral model performing better at each size level compared to the other models. Based on these graphs, Mistral appears to be the top-performing model in all four metrics, suggesting that it might be more efficient or effective than LLaMA 2, LLaMA 13B, and GPT-3, at least within the scope of the parameters and tasks represented here. On a technical note, it\u2019s important to mention that the y-axis shows different scales for the metrics (ranging from 45\u201375% for NLU, Knowledge, and Commonsense, and from 50\u201372% for Reasoning), which suggests that these tasks might have different levels of difficulty or different benchmarks for success.\n  </p>\n </blockquote>\n <h2>\n  Observation:\n </h2>\n <p>\n  It did answer the query but hallucinated with the NLU task which is the MMLU task and assumed Mistral is available across all different model parameters.\n </p>\n <h1>\n  Specific Questions:\n </h1>\n <p>\n  <strong>\n   Question:\n  </strong>\n  How well does mistral model compared to llama2 model?\n </p>\n <p>\n  <strong>\n   Answer:\n  </strong>\n </p>\n <blockquote>\n  <p class=\"oh oi po oj b ok pf om on oo pg oq or os ph ou ov ow pi oy oz pa pj pc pd pe gm bj\" id=\"2674\">\n   The image you\u2019ve provided contains four graphs, each plotting different performance metrics for evaluating language models. It compares two models: Mistral and LLaMA2 across four different aspects \u2014 MMLU (Multi-tasking multiple-choice), Reasoning, Knowledge, and Commonsense. In each graph, the performance metric is plotted on the y-axis while the effective model size in billion parameters is plotted on the x-axis. Here\u2019s a summary based on the trends observed in the graphs: 1. **MMLU (Top Left Graph)**: LLaMA2 shows a steeper performance curve as the model size increases, starting at around 61% for the smallest size and reaching upwards of 66% for the largest model. Mistral also shows performance gains with model size but plateaus earlier, reaching a level just shy of 62%. 2. **Reasoning (Top Right Graph)**: In the reasoning task, LLaMA2 again has a steeper improvement curve, starting from about 61% and surpassing 72% for the largest model. Mistral, while improving, seems to plateau near 70% for the largest model. 3. **Knowledge (Bottom Left Graph)**: This graph reflects a similar trend to the previous ones, with LLaMA2 beginning at a lower performance around 46% and eclipsing 52%. Mistral starts higher at around 48% and appears to plateau near 52%. 4. **Commonsense (Bottom Right Graph)**: Here, LLaMA2 starts its performance at approximately 62% and reaches just above 66%. Mistral seems to start at a slightly higher point than LLaMA2 but ends at a similar level to LLaMA2\u2019s largest model. Overall, the LLaMA2 model appears to show a greater degree of improvement in performance as the model size increases compared to Mistral across these metrics. Meanwhile, Mistral starts at a higher performance for some metrics but tends to plateau earlier, suggesting that LLaMA2 may scale better with size in terms of performance gains.\n  </p>\n </blockquote>\n <h2>\n  Observation:\n </h2>\n <p>\n  Incorrect answers and percentages are not accurate enough and again assumed mistral is available across all parameter models.\n </p>\n <p>\n  <strong>\n   Question:\n  </strong>\n  Assuming mistral is available in 7B series. How well does mistral model compared to llama2 model?\n </p>\n <p>\n  <strong>\n   Answer:\n  </strong>\n </p>\n <blockquote>\n  <p class=\"oh oi po oj b ok pf om on oo pg oq or os ph ou ov ow pi oy oz pa pj pc pd pe gm bj\" id=\"7660\">\n   This image appears to be a set of four graphs, each comparing the performance of three different language models on various tasks: Mistral, LLaMA-2, and a hypothetical \u201cSize 7B\u201d model. The graphs are labeled with task-specific performance metrics: \u201cMLM U\u201d, \u201cReasoning\u201d, \u201cKnowledge\u201d, and \u201cCommonsense\u201d. The x-axes on the graphs represent model size in terms of the number of parameters, with three points that likely correspond to the sizes of the models being compared. The y-axes represent performance as a percentage, which could mean accuracy, precision, recall, or another relevant performance metric depending on the specific task. The graphs appear to show that Mistral performs better than LLaMA-2 and the Size 7B model across all metrics. This indicates that within the context of these measurements and tasks, Mistral is a stronger model. The exact nature of the tasks or what \u201cMLM U\u201d, \u201cReasoning\u201d, \u201cKnowledge\u201d, and \u201cCommonsense\u201d specifically refer to are not detailed in the image, but they likely correspond to standard NLP tasks designed to test understanding of language, ability to reason, knowledge recall, and commonsense reasoning, respectively. The performance improvements are depicted as increasingly significant with larger model sizes.\n  </p>\n </blockquote>\n <h2>\n  Observation:\n </h2>\n <p>\n  Now by giving the detail that mistral is available in 7B series, it is able to answer correctly.\n </p>\n <h1>\n  Chain of thought prompting:\n </h1>\n <p>\n  <strong>\n   Question:\n  </strong>\n  Based on the image provided. Follow the steps and answer the query \u2014 Assuming mistral is available in 7B series. How well does mistral model compared to llama2 model?.\n </p>\n <p>\n  Examine the Image: Look at the mentioned category in the query in the Image.\n </p>\n <p>\n  Identify Relevant Data: Note the respective percentages.\n </p>\n <p>\n  Evaluate: Compare if there is any comparision required as per the query.\n </p>\n <p>\n  Draw a Conclusion: Now draw the conclusion based on the whole data.\n </p>\n <p>\n  <strong>\n   Answer:\n  </strong>\n </p>\n <blockquote>\n  <p class=\"oh oi po oj b ok pf om on oo pg oq or os ph ou ov ow pi oy oz pa pj pc pd pe gm bj\" id=\"02f0\">\n   Examine the Image: The image contains four graphs, each graph compares the performance of three different models \u2014 Llama 2, Mistral, and an unnamed third model \u2014 across different metrics: mAP@L (mean Average Precision at L), Reason@L (presumably a reasoning score at L), Knowledge@L, and Comprehension@L. Each graph shows performance as a function of model size (in terms of billion parameters). Identify Relevant Data: We need to focus on the Mistral and Llama 2 models across all four graphs to extract the relevant data. For mAP@L: \u2014 Llama 2 reaches above 65% when reaching 70 billion parameters. \u2014 Mistral starts at around 5 billion parameters with about 48% and reaches almost 65% by the time it gets to 34 billion parameters. For Reason@L: \u2014 Llama 2 reaches about 68% when reaching 70 billion parameters. \u2014 Mistral starts at around 5 billion parameters with about 52% and reaches almost 68% by the time it gets to 34 billion parameters. For Knowledge@L: \u2014 Llama 2 starts at about 45% with 1.3 billion parameters and reaches just slightly under 60% by 70 billion parameters. \u2014 Mistral is not plotted in this graph, so no direct comparison can be made. For Comprehension@L: \u2014 Llama 2 starts just above 40% with 1.3 billion parameters and reaches just under 60% by 70 billion parameters. \u2014 Mistral starts around 13 billion parameters with about 52% and continues above 62% by 34 billion parameters. Evaluate: From the graphs, we can see that Mistral and Llama 2 are very comparable in performance in the metrics where both are shown. However, Mistral is not displayed on the Knowledge@L graph, so no comparison can be made for that particular metric. Draw a Conclusion: In the graphs where both models can be compared (mAP@L, Reason@L, and Comprehension@L), Mistral appears to perform comparably to Llama 2, reaching similar levels of performance at smaller parameter sizes relative to the plotted sizes for Llama 2. This suggests that Mistral could be more parameter-efficient. However, since there is no data for Mistral in the Knowledge@L graph, we cannot draw a complete comparison\n  </p>\n </blockquote>\n <h2>\n  Observation:\n </h2>\n <p>\n  There is a hallucination with a number of model parameters and percentage points though the final conclusion is partially correct.\n </p>\n <h1>\n  Image 3 \u2014 Performances of different LLMs across various NLP tasks.\n </h1>\n <h1>\n  General Question:\n </h1>\n <p>\n  <strong>\n   Question:\n  </strong>\n  Analyze the Image.\n </p>\n <p>\n  <strong>\n   Answer:\n  </strong>\n </p>\n <blockquote>\n  <p class=\"oh oi po oj b ok pf om on oo pg oq or os ph ou ov ow pi oy oz pa pj pc pd pe gm bj\" id=\"0b3d\">\n   The image appears to be a table containing numerical data, which seems to be a comparative analysis of various models across multiple parameters or tests. Each row represents a different model or configuration as indicated by names such as \u201cMPT\u201d and \u201cFalcon\u201d followed by a size specification like \u201c70B,\u201d \u201c7B,\u201d or some other parameter. The columns are labeled with test names or evaluation metrics, such as \u201cSize,\u201d \u201cAvg,\u201d \u201cAQuA-RAT,\u201d \u201cLogiQA,\u201d \u201cL-SAT-AR,\u201d \u201cL-SAT-IC,\u201d \u201cL-SAT-RC,\u201d \u201cSAT-en (w/o Psg.),\u201d and \u201cSAT-math.\u201d The data is likely related to performance scores of these models on these tests, where higher numbers probably indicate better performance. Without additional context, it is difficult to provide a comprehensive analysis of this data, but it seems clear that it is intended to provide a performance comparison between different models on various tasks, possibly in the field of machine learning or artificial intelligence evaluations, where such models are typically assessed on reasoning, comprehension, or problem-solving capabilities. The \u201c70B\u201d and \u201c7B\u201d demarcations could refer to the size of the model in terms of the number of parameters, commonly used in assessing language models. A deeper evaluation would require further insights into the specific nature of these tests and models, along with the intended use-case for which they were being compared.\n  </p>\n </blockquote>\n <h2>\n  <strong>\n   Observation:\n  </strong>\n </h2>\n <p>\n  It did not analyze the image specifically but understood the overall data present in the image to some extent.\n </p>\n <h1>\n  Specific Questions:\n </h1>\n <p>\n  <strong>\n   Question:\n  </strong>\n  which model has higher performance in SAT-en?\n </p>\n <p>\n  <strong>\n   Answer:\n  </strong>\n </p>\n <blockquote>\n  <p class=\"oh oi po oj b ok pf om on oo pg oq or os ph ou ov ow pi oy oz pa pj pc pd pe gm bj\" id=\"0084\">\n   In the table you\u2019ve provided, the models are compared based on their performance on several benchmarks, including SAT-en (SAT Analogies, or \u201cSAT\u201d in the table). To determine which model has higher performance specifically on the SAT-en benchmark, you\u2019ll need to look at the corresponding column. In the SAT-en column (second from the right), the two models with the highest scores are: \u2014 LLaMA1 65B: with a score of 57.9 \u2014 LLaMA2 70B: with a score of 63.4 Between these two, the LLaMA2 model with 70 billion parameters shows the higher performance on the SAT-en benchmark with a score of 63.4.\n  </p>\n </blockquote>\n <h2>\n  Observation:\n </h2>\n <p>\n  It did answer correctly but the numbers are being hallucinated.\n </p>\n <p>\n  <strong>\n   Question:\n  </strong>\n  which model has higher performance in SAT-en in 7B series models?\n </p>\n <p>\n  <strong>\n   Answer:\n  </strong>\n </p>\n <blockquote>\n  <p class=\"oh oi po oj b ok pf om on oo pg oq or os ph ou ov ow pi oy oz pa pj pc pd pe gm bj\" id=\"47ea\">\n   The table you\u2019ve provided shows performance benchmark scores for different model sizes across various AI models such as MPT, Falcon, and LLaMA on different tasks, such as Aqua-RAT, LogiQA, LastAR, SAT-en, and SAT-math. For the SAT-en task specifically, you asked which model in the 7B series has the highest performance. To find out, we need to look at the column labeled \u201cSAT-en (w/o Ps_8).\u201d In the 7B series of models, here are the scores: \u2014 MPT 7B: 37.1 \u2014 Falcon 7B: 37.3 \u2014 LLaMA 7B: 63.9 \u2014 Model2 7B: 37.4 The LLaMA 7B model outperforms the other 7B models on the SAT-en (w/o Ps_8) task with a score of 63.9.\n  </p>\n </blockquote>\n <h2>\n  <strong>\n   Observation:\n  </strong>\n </h2>\n <p>\n  Incorrect answer and recognized Llama series of models and values incorrectly.\n </p>\n <h1>\n  Chain of thought prompting:\n </h1>\n <p>\n  <strong>\n   Question:\n  </strong>\n  Based on the image provided. Follow the steps and answer the query \u2014 which model has higher performance in SAT-en in 7B series models?\n </p>\n <p>\n  Examine the Image: Look at the mentioned category in the query in the Image.\n </p>\n <p>\n  Identify Relevant Data: Note the respective percentages.\n </p>\n <p>\n  Evaluate: Compare if there is any comparison required as per the query.\n </p>\n <p>\n  Draw a Conclusion: Now draw the conclusion based on the whole data.\n </p>\n <p>\n  <strong>\n   Answer:\n  </strong>\n </p>\n <blockquote>\n  <p class=\"oh oi po oj b ok pf om on oo pg oq or os ph ou ov ow pi oy oz pa pj pc pd pe gm bj\" id=\"f44d\">\n   To answer which model has higher performance in SAT-en in the 7B series models, I will follow the provided steps: Examine the Image: The categories in the image include model names and sizes, and performance scores for various tasks, including the SAT-en category. Identify Relevant Data: The relevant data for the SAT-en category are the percentages listed under it for each 7B series model. Evaluate: I will compare the SAT-en percentages of each 7B series model. Draw a Conclusion: The SAT-en scores for the 7B series models are as follows: \u2014 MPT 7B: 63.1% \u2014 Falcon 7B: 73.4% \u2014 LLama 1 7B: No data present for this category. \u2014 LLama 2 7B: 76.6% Based on the data, the LLama 2 7B model has the highest SAT-en performance among the 7B series models with a score of 76.6%.\n  </p>\n </blockquote>\n <h2>\n  Observation:\n </h2>\n <p>\n  With chain of thought prompting we are able to get the right conclusion though it should be noted that it picked up wrong values.\n </p>\n <h1>\n  Final Observations:\n </h1>\n <p>\n  Observations made based on experiments on Hallucination and correctness. (Please note that these observations are specific to the images used and cannot be generalized, as they vary depending on the images.)\n </p>\n <h1>\n  Summary\n </h1>\n <p>\n  In this blog post, we have showcased experiments ranging from general inquiries to systematic questions and chain of thought prompting techniques and observed Hallucination and correctness metrics.\n </p>\n <p>\n  However, it should be noted that the outputs from GPT-4V can be somewhat inconsistent, and the levels of hallucination are slightly elevated. Therefore, repeating the same experiment could result in different answers, particularly with generalized questions.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 27513, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fcd4f869-7e6b-4140-8092-7f0fded29c81": {"__data__": {"id_": "fcd4f869-7e6b-4140-8092-7f0fded29c81", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.html", "file_name": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.html", "file_type": "text/html", "file_size": 19580, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.html", "file_name": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.html", "file_type": "text/html", "file_size": 19580, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "a0e6c7ee213a82414547672ab6b78d80f5f7d8e5a0bac4c7b1fbfdf5aef7cdc6", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  In this article, we\u2019ll dive deep into the world of LLM app development and take a closer look at my journey of building the Streamlit LLM hackathon-winning app\n  <a href=\"https://finsight-report.streamlit.app/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   FinSight \u2014 Financial Insights At Your Fingertips\n  </a>\n  . This article covers the entire process from ideation to execution, along with code snippets and snapshots.\n </p>\n <h1>\n  Introduction\n </h1>\n <h2>\n  A use case for LLMs in finance\n </h2>\n <p>\n  One fascinating use case for LLMs in finance is to use them on company annual\n  reports (10-K form). These reports are publicly available information that\n  pretty much every portfolio manager, financial analyst, and shareholder uses\n  regularly to make informed decisions.\n </p>\n <p>\n  However reading, understanding, and assessing these reports, especially for\n  multiple companies can be tedious and time-consuming. Hence, using LLMs on\n  annual reports to extract insights and summarize would solve a lot of problems\n  and save valuable time.\n </p>\n <p>\n  When the\n  <a href=\"https://www.linkedin.com/posts/vishwasgowda217_llm-hackathon-streamlit-activity-7115398433573666816-1y72?utm_source=share&amp;utm_medium=member_desktop\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Streamlit LLM Hackathon\n  </a>\n  was, announced I thought this was the best time to explore this idea. And\n  that\u2019s how\n  <a href=\"https://finsight-report.streamlit.app/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   FinSight\n  </a>\n  came into existence.\n </p>\n <h2>\n  How does FinSight work?\n </h2>\n <figure>\n  <figcaption>\n   A small Demonstration\n  </figcaption>\n </figure>\n <p>\n  FinSight has two main features called Annual Report Analyzer and Finance\n  Metric Review, but for this blog post, we will be concentrating on the former.\n </p>\n <p>\n  Annual Report Analyzer is a RAG(Retrieval Augmented Generation) based feature,\n  which means that the LLM will be generating insights based on the information\n  in a knowledge base (which in this case is a company\u2019s annual report). Here\u2019s\n  how it works behind the scenes:\n </p>\n <figure>\n  <figcaption>\n   RAG pipeline for Annual Report Analyzer\n  </figcaption>\n </figure>\n <p>\n  While this is a basic representation of the architecture, we will be doing a\n  deep dive into the importance of each of these components and how they work.\n </p>\n <h1>\n  Setup\n </h1>\n <p>\n  In case you want to refer the code to the app:\n  <a href=\"https://github.com/vishwasg217/finsight\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Repo\n  </a>\n </p>\n <p>\n  We will use\n  <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaIndex\n  </a>\n  to build the knowledge base and to query it using an LLM (gpt-4 is the best\n  suited). LlamaIndex is a simple, flexible data framework for connecting custom\n  data sources to large language models.\n </p>\n <p>\n  For the front end,\n  <a href=\"https://streamlit.io/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Streamlit\n  </a>\n  is the most convenient tool to build and share web apps.\n </p>\n <ol>\n  <li>\n   Clone Repository\n  </li>\n </ol>\n <pre><span>git clone https://github.com/vishwasg217/finsight.gitcd finsight</span></pre>\n <p>\n  2. Setup Virtual Environment\n </p>\n <pre><span># For macOS and Linux:python3 -m venv venv# For Windows:python -m venv venv</span></pre>\n <p>\n  3. Activate Virtual Environment\n </p>\n <pre><span># For macOS and Linux:source venv/bin/activate# For Windows:.\\venv\\Scripts\\activate</span></pre>\n <p>\n  4. Install Required Dependencies:\n </p>\n <pre><span>pip install -r requirements.txt</span></pre>\n <p>\n  5. Set up the Environment Variables:\n </p>\n <pre><span># create directorymkdir .streamlit# create toml filetouch .streamlit/secrets.toml</span></pre>\n <p>\n  You can get your API keys here:\n  <a href=\"https://www.alphavantage.co/support/#api-key\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   AlphaVantage\n  </a>\n  ,\n  <a href=\"https://openai.com/blog/openai-api\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   OpenAI\n  </a>\n  ,\n </p>\n <pre><span># Add the following API keysav_api_key = \"ALPHA_VANTAGE API KEY\"openai_api_key = \"OPEN AI API KEY\"</span></pre>\n <h1>\n  Document Loading, Indexing, and Storage\n </h1>\n <p>\n  Although LlamaIndex has its own set of data connectors to read PDFs, we still\n  need to write a small function\n  <code>\n   process_pdf()\n  </code>\n  to load the PDFs since we are doing it through\n  Streamlit.\n </p>\n <pre><span>from pypdf import PdfReaderfrom llama_index.schema import Documentdef process_pdf(pdf):    file = PdfReader(pdf)    text = \"\"    for page in file.pages:        text += str(page.extract_text())            doc = Document(text=text)    return [doc]</span></pre>\n <p>\n  The next step is to ingest, index, and store this document in a vector\n  database. In this case, we will use FAISS DB, as we require in an in-memory\n  vector database. FAISS is also very convenient to use. Hence, we write a\n  function called\n  <code>\n   get_vector_index()\n  </code>\n  to do exactly that.\n </p>\n <p>\n  In case you\u2019re interested in checking out other vector DB options, you read\n  can\n  <a href=\"https://gpt-index.readthedocs.io/en/stable/core_modules/data_modules/storage/vector_stores.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   this\n  </a>\n  .\n </p>\n <pre><span>from llama_index.llms import OpenAIfrom llama_index import VectorStoreIndex, ServiceContext, StorageContextfrom llama_index.vector_stores import FaissVectorStoredef get_vector_index(documents):    llm = OpenAI(OPENAI_API_KEY)    faiss_index = faiss.IndexFlatL2(d)    vector_store = FaissVectorStore(faiss_index=faiss_index)    storage_context = StorageContext.from_defaults(vector_store=vector_store)    service_context = ServiceContext.from_defaults(llm=llm)     index = VectorStoreIndex.from_documents(documents,         service_context=service_context,        storage_context=storage_context    )       return index</span></pre>\n <p>\n  <code>\n   ServiceContext()\n  </code>\n  and\n  <code>\n   StorageContext()\n  </code>\n  are used to\n  set the configurations for the vector store. Using\n  <code>\n   VectorStoreIndex.from_documents()\n  </code>\n  we ingest, index, and store the document as vector embeddings in the FAISS DB.\n </p>\n <pre><span># Calling the functions through streamlit frontendimport streamlit as stif \"index\" not in st.session_state:  st.session_state.index = Noneif \"process_doc\" not in st.session_state:        st.session_state.process_doc = Falseif st.sidebar.button(\"Process Document\"):        with st.spinner(\"Processing Document...\"):            documents = process_pdf(pdfs)            st.session_state.index = get_vector_index(documents)            st.session_state.process_doc = True  st.toast(\"Document Processsed!\")</span></pre>\n <h1>\n  Query Tools and Engines\n </h1>\n <p>\n  Now that we have our knowledge base ready, it\u2019s time to build a mechanism to\n  query it.\n </p>\n <pre><span>index = get_vector_index(documents)engine = index.as_query_engine()query = \"How has Microsoft performed in this fiscal year?\"response = engine(query)</span></pre>\n <p>\n  Ideally, the above code should have been enough to query and synthesize a\n  response from the information in the vector DB. However, the response wouldn't\n  be comprehensive and detailed enough, especially for such open-ended\n  questions. We need to develop a better mechanism that allows us to break down\n  a query into more detailed questions and retrieve context from multiple parts\n  of the vector DB.\n </p>\n <pre><span>def get_query_engine(engine):    query_engine_tools = [        QueryEngineTool(            query_engine=engine,            metadata=ToolMetadata(                name=\"Annual Report\",                description=f\"Provides information about the company from its annual report.\",            ),        ),    ]    s_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools)    return s_engineindex = get_vector_index(documents)engine = index.as_query_engine()s_engine = get_query_engine(engine)</span></pre>\n <p>\n  Let\u2019s break the above function down. The\n  <code>\n   QueryEngineTool\n  </code>\n  module wraps around the\n  <code>\n   engine\n  </code>\n  and\n  helps provide context and metadata to the engine. This is especially useful\n  when you have more than one engine and you want to provide context to the LLM\n  as to which one to use for a given query.\n </p>\n <p>\n  Here\u2019s what that would look like:\n </p>\n <pre><span># example for multiple query engine toolsquery_engine_tools = [    QueryEngineTool(        query_engine=sept_engine,        metadata=ToolMetadata(            name=\"sept_22\",            description=\"Provides information about Uber quarterly financials ending September 2022\",        ),    ),    QueryEngineTool(        query_engine=june_engine,        metadata=ToolMetadata(            name=\"june_22\",            description=\"Provides information about Uber quarterly financials ending June 2022\",        ),    )]</span></pre>\n <p>\n  You can read more about the tools available in LlamaIndex\n  <a href=\"https://docs.llamaindex.ai/en/stable/core_modules/agent_modules/tools/root.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   here\n  </a>\n  .\n </p>\n <p>\n  However, we\u2019re currently sticking to just one QueryEnginerTool for now.\n </p>\n <p>\n  The\n  <code>\n   SubQuestionQueryEngine\n  </code>\n  module breaks down a complex query\n  into many sub-questions and their target query engine for execution. After\n  executing all sub-questions, all responses are gathered and sent to a response\n  synthesizer to produce the final response. Using this module is essential\n  because generating insights from annual reports requires complex queries that\n  need to retrieve information from multiple nodes within the vector DB.\n </p>\n <figure>\n  <figcaption>\n   SubQuestionQueryEngine at work\n  </figcaption>\n </figure>\n <h1>\n  Prompt Engineering\n </h1>\n <p>\n  Prompt engineering is essential to the entire process mainly for two reasons:\n </p>\n <ol>\n  <li>\n   To provide clarity to the agent as to what it needs to retrieve from the\n    vector DB by writing precise and relevant queries\n  </li>\n  <li>\n   And then control the quality of the output generated from the retrieved\n    context by providing a structure and description for the output to be\n    generated.\n  </li>\n </ol>\n <p>\n  Both these points are handled by using\n  <code>\n   PromptTemplate\n  </code>\n  and\n  <code>\n   PydanticOutputParser\n  </code>\n  module in\n  <code>\n   langchain\n  </code>\n  .\n </p>\n <p>\n  Using the\n  <code>\n   PydanticOutputParser\n  </code>\n  we write the description for the\n  different sections of the insights to be generated. After having a few\n  conversations with finance experts, I concluded generating insights for these\n  4 sections: different sections: Fiscal Year Highlights, Strategic Outlook and\n  Future Direction, Risk Management, Innovation and R&amp;D. Now let\u2019s write the\n  <code>\n   pydantic\n  </code>\n  class for these sections:\n </p>\n <pre><span>from pydantic import BaseModel, Fieldclass FiscalYearHighlights(BaseModel):    performance_highlights: str = Field(..., description=\"Key performance metrics and financial stats over the fiscal year.\")    major_events: str = Field(..., description=\"Highlight of significant events, acquisitions, or strategic shifts that occurred during the year.\")    challenges_encountered: str = Field(..., description=\"Challenges the company faced during the year and, if and how they managed or overcame them.\")class StrategyOutlookFutureDirection(BaseModel):    strategic_initiatives: str = Field(..., description=\"The company's primary objectives and growth strategies for the upcoming years.\")    market_outlook: str = Field(..., description=\"Insights into the broader market, competitive landscape, and industry trends the company anticipates.\")class RiskManagement(BaseModel):    risk_factors: str = Field(..., description=\"Primary risks the company acknowledges.\")    risk_mitigation: str = Field(..., description=\"Strategies for managing these risks.\")class InnovationRnD(BaseModel):    r_and_d_activities: str = Field(..., description=\"Overview of the company's focus on research and development, major achievements, or breakthroughs.\")    innovation_focus: str = Field(..., description=\"Mention of new technologies, patents, or areas of research the company is diving into.\")</span></pre>\n <p>\n  <strong>\n   Note: These sections and their description are for generic use cases. They\n    can be changed to suit your particular needs.\n  </strong>\n </p>\n <p>\n  These pydantic classes will provide the format and description for each\n  section to the prompt. So let\u2019s write a function that allows us to plug in any\n  pydantic class to a prompt:\n </p>\n <pre><span>from langchain.prompts import PromptTemplatefrom langchain.output_parsers import PydanticOutputParserprompt_template = \"\"\"You are given the task of generating insights for {section} from the annual report of the company. Given below is the output format, which has the subsections.Must use bullet points.Always use $ symbol for money values, and round it off to millions or billions accordinglyIncase you don't have enough info you can just write: No information available---{output_format}---\"\"\"def report_insights(engine, section, pydantic_model):    parser = PydanticOutputParser(pydantic_object=pydantic_model)    prompt_template = PromptTemplate(        template=prompt_template,        input_variables=[\"section\"],        partial_variables={\"output_format\": parser.get_format_instructions()}    )    formatted_input = prompt_template.format(section=section)    response = engine.query(formatted_input)    parsed_response = parser.parse(response.response)    return parsed_response</span></pre>\n <p>\n  <code>\n   PromptTemplate\n  </code>\n  plugs in all the values such as\n  <code>\n   section\n  </code>\n  and\n  <code>\n   output_format\n  </code>\n  into the prompt template.\n  <code>\n   PydanticOutputParser\n  </code>\n  converts the pydantic class into a format\n  that is readable to the LLM. The response generated will be in string format,\n  hence we use the\n  <code>\n   parser.parse()\n  </code>\n  function to parse the response\n  and get a structured output.\n </p>\n <pre><span># calling the function in streamlit frontendif st.session_state.process_doc:    if st.button(\"Analyze Report\"):        engine = get_query_engine(st.session_state.index.as_query_engine(similarity_top_k=3))        with st.status(\"**Analyzing Report...**\"):            st.write(\"Fiscal Year Highlights...\")            st.session_state.fiscal_year_highlights = report_insights(engine, \"Fiscal Year Highlights\", FiscalYearHighlights)            st.write(\"Strategy Outlook and Future Direction...\")            st.session_state.strategy_outlook_future_direction = report_insights(engine, \"Strategy Outlook and Future Direction\", StrategyOutlookFutureDirection)            st.write(\"Risk Management...\")            st.session_state.risk_management = report_insights(engine, \"Risk Management\", RiskManagement)                        st.write(\"Innovation and R&amp;D...\")            st.session_state.innovation_and_rd = report_insights(engine, \"Innovation and R&amp;D\", InnovationRnD)# displaying the generated insights  if st.session_state.fiscal_year_highlights:                with tab1:            st.write(\"## Fiscal Year Highlights\")            st.write(\"### Performance Highlights\")            st.write(st.session_state.fiscal_year_highlights.performance_highlights)            st.write(\"### Major Events\")            st.write(st.session_state.fiscal_year_highlights.major_events)            st.write(\"### Challenges Encountered\")            st.write(st.session_state.fiscal_year_highlights.challenges_encountered)            st.write(\"### Milestone Achievements\")            st.write(str(st.session_state.fiscal_year_highlights.milestone_achievements))    if st.session_state.strategy_outlook_future_direction:        with tab2:            st.write(\"## Strategy Outlook and Future Direction\")            st.write(\"### Strategic Initiatives\")            st.write(st.session_state.strategy_outlook_future_direction.strategic_initiatives)            st.write(\"### Market Outlook\")            st.write(st.session_state.strategy_outlook_future_direction.market_outlook)            st.write(\"### Product Roadmap\")            st.write(st.session_state.strategy_outlook_future_direction.product_roadmap)    if st.session_state.risk_management:        with tab3:            st.write(\"## Risk Management\")            st.write(\"### Risk Factors\")            st.write(st.session_state.risk_management.risk_factors)            st.write(\"### Risk Mitigation\")            st.write(st.session_state.risk_management.risk_mitigation)    if st.session_state.innovation_and_rd:        with tab4:            st.write(\"## Innovation and R&amp;D\")            st.write(\"### R&amp;D Activities\")            st.write(st.session_state.innovation_and_rd.r_and_d_activities)            st.write(\"### Innovation Focus\")            st.write(st.session_state.innovation_and_rd.innovation_focus)</span></pre>\n <p>\n  You can find the complete code Annual Report Analyzer\n  <a href=\"https://github.com/vishwasg217/finsight/blob/main/src/pages/2_%F0%9F%97%82%EF%B8%8F_Annual_Report_Analyzer.py\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   here\n  </a>\n </p>\n <h1>\n  Upcoming Features\n </h1>\n <ol>\n  <li>\n   Select and Store Insights: I\u2019ve been working on a feature that allows the\n    user to select any insight needed and also save it into the user\u2019s account\n  </li>\n  <li>\n   Adding more profession-specific insights: Currently, the insight works well\n    for generic purposes. However, different professions use annual reports\n    differently, so naturally I need to create a different set of insights based\n    on the user\u2019s use case.\n  </li>\n  <li>\n   <code>\n    PandasQueryEngine\n   </code>\n   Module for querying financial statements:\n    Using this module, the LLM will be able to extract better insights from\n    financial statements which are typically in a structured format.\n  </li>\n </ol>\n <h1>\n  Conclusion\n </h1>\n <p>\n  In summary, FinSight\u2019s Annual Report Analyzer makes financial analysis easier\n  and more insightful by harnessing the power of LLMs. It\u2019s a valuable tool for\n  portfolio managers, financial analysts, and shareholders, saving time and\n  improving decision-making. While the core pipeline remains consistent, note\n  that our deployed app code might evolve to incorporate upgrades and enhanced\n  features, ensuring ongoing improvements.\n </p>\n <p>\n  Big thanks to\n  <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaIndex\n  </a>\n  for helping me make FinSight a reality. No other framework is as advanced in\n  making RAG-based tools.\n </p>\n <p>\n  If you like what you\u2019ve read, please do leave a clap for me, and also show\n  some love to\n  <a href=\"https://finsight-report.streamlit.app/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   FinSight\n  </a>\n  . You can check out the GitHub repo\n  <a href=\"https://github.com/vishwasg217/finsight\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   here\n  </a>\n  .\n </p>\n <p>\n  You connect with me on\n  <a href=\"https://www.linkedin.com/feed/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LinkedIn\n  </a>\n  and\n  <a href=\"https://twitter.com/VishwasAiTech\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Twitter\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 19543, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7e016859-6da8-439e-859d-93470687a784": {"__data__": {"id_": "7e016859-6da8-439e-859d-93470687a784", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa.html", "file_name": "how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa.html", "file_type": "text/html", "file_size": 16248, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa.html", "file_name": "how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa.html", "file_type": "text/html", "file_size": 16248, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "0a290a27ee0674ef21d53a1d5a6f0b8cd5cc951da2ca74d53f928f661eca3157", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Agents are autonomous systems that can execute end-to-end tasks without much or fewer instructions. These agents are capable of solving tasks related to questions and answering, using tools to achieve a desired behavior, or even planning tasks.\n </p>\n <p>\n  In this article, we will explore a few capabilities of LlamaIndex.TS\u2019s built-in agents to achieve a set of goals. We have two parts:\n </p>\n <ol>\n  <li>\n   <a href=\"#cb81\" rel=\"noopener ugc nofollow\">\n    Building a simple agent for calculation.\n   </a>\n  </li>\n  <li>\n   <a href=\"#b49b\" rel=\"noopener ugc nofollow\">\n    Using agents with your personal/private data to answer questions.\n   </a>\n  </li>\n </ol>\n <p>\n  All the full code examples will be at the end of the post.\n </p>\n <h1>\n  Setup\n </h1>\n <p>\n  To start you need to have\n  <code class=\"cw px py pz qa b\">\n   llamaindex\n  </code>\n  package installed on your node environment setup and an OpenAI key.\n </p>\n <p>\n  To install the package:\n </p>\n <pre><span class=\"qj ov gt qa b bf qk ql l qm qn\" id=\"2a73\">npm install llamaindex </span></pre>\n <p>\n  To set up the OpenAI key you can set your environment variable:\n </p>\n <pre><span class=\"qj ov gt qa b bf qk ql l qm qn\" id=\"c23a\">export OPENAI_API_KEY=sk-***************</span></pre>\n <h1>\n  Agent for calculations\n </h1>\n <p>\n  The first agent will be responsible for getting a user input of numbers and based on this selecting the right tools to achieve the task\n </p>\n <h2>\n  Import the classes:\n </h2>\n <pre><span class=\"qj ov gt qa b bf qk ql l qm qn\" id=\"6fcd\">import { FunctionTool, OpenAIAgent } from \"llamaindex\";</span></pre>\n <h2>\n  Function Tool\n </h2>\n <p>\n  The first step will be creating tools that the agent will have access to, we will be creating an\n  <code class=\"cw px py pz qa b\">\n   add\n  </code>\n  function and a\n  <code class=\"cw px py pz qa b\">\n   multiply\n  </code>\n  function.\n </p>\n <p>\n  OpenAI provides us a function calling API which we can send our function arguments and get back a response\n </p>\n <p>\n  We start by creating two functions:\n </p>\n <pre><span class=\"qj ov gt qa b bf qk ql l qm qn\" id=\"07d7\">// Define <span class=\"hljs-selector-tag\">a</span> function <span class=\"hljs-selector-tag\">to</span> sum two numbers\nfunction sum({ <span class=\"hljs-selector-tag\">a</span>, <span class=\"hljs-selector-tag\">b</span> }: { <span class=\"hljs-selector-tag\">a</span>: number; <span class=\"hljs-selector-tag\">b</span>: number }): number {\n  return <span class=\"hljs-selector-tag\">a</span> + <span class=\"hljs-selector-tag\">b</span>;\n}\n\n// Define <span class=\"hljs-selector-tag\">a</span> function <span class=\"hljs-selector-tag\">to</span> multiply two numbers\nfunction multiply({ <span class=\"hljs-selector-tag\">a</span>, <span class=\"hljs-selector-tag\">b</span> }: { <span class=\"hljs-selector-tag\">a</span>: number; <span class=\"hljs-selector-tag\">b</span>: number }): number {\n  return <span class=\"hljs-selector-tag\">a</span> * <span class=\"hljs-selector-tag\">b</span>;\n}</span></pre>\n <p>\n  Now we can set up the\n  <code class=\"cw px py pz qa b\">\n   FunctionTool\n  </code>\n  class which will be given to the agent, this class requires the properties of the function and metadata for the tool, helping the Large Language Models (LLMs) to identify which tool the LLM should use and the parameters.\n </p>\n <pre><span class=\"qj ov gt qa b bf qk ql l qm qn\" id=\"5234\"><span class=\"hljs-comment\">// Sum properties to give to the LLM</span>\n<span class=\"hljs-keyword\">const</span> sumJSON = {\n  <span class=\"hljs-attr\">type</span>: <span class=\"hljs-string\">\"object\"</span>,\n  <span class=\"hljs-attr\">properties</span>: {\n    <span class=\"hljs-attr\">a</span>: {\n      <span class=\"hljs-attr\">type</span>: <span class=\"hljs-string\">\"number\"</span>,\n      <span class=\"hljs-attr\">description</span>: <span class=\"hljs-string\">\"The first number\"</span>,\n    },\n    <span class=\"hljs-attr\">b</span>: {\n      <span class=\"hljs-attr\">type</span>: <span class=\"hljs-string\">\"number\"</span>,\n      <span class=\"hljs-attr\">description</span>: <span class=\"hljs-string\">\"The second number\"</span>,\n    },\n  },\n  <span class=\"hljs-attr\">required</span>: [<span class=\"hljs-string\">\"a\"</span>, <span class=\"hljs-string\">\"b\"</span>],\n};\n\n<span class=\"hljs-comment\">// Multiply properties to give to the LLM</span>\n<span class=\"hljs-keyword\">const</span> multiplyJSON = {\n  <span class=\"hljs-attr\">type</span>: <span class=\"hljs-string\">\"object\"</span>,\n  <span class=\"hljs-attr\">properties</span>: {\n    <span class=\"hljs-attr\">a</span>: {\n      <span class=\"hljs-attr\">type</span>: <span class=\"hljs-string\">\"number\"</span>,\n      <span class=\"hljs-attr\">description</span>: <span class=\"hljs-string\">\"The number to multiply\"</span>,\n    },\n    <span class=\"hljs-attr\">b</span>: {\n      <span class=\"hljs-attr\">type</span>: <span class=\"hljs-string\">\"number\"</span>,\n      <span class=\"hljs-attr\">description</span>: <span class=\"hljs-string\">\"The multiplier\"</span>,\n    },\n  },\n  <span class=\"hljs-attr\">required</span>: [<span class=\"hljs-string\">\"a\"</span>, <span class=\"hljs-string\">\"b\"</span>],\n};\n\n<span class=\"hljs-comment\">// Create sum function tool</span>\n<span class=\"hljs-keyword\">const</span> sumFunctionTool = <span class=\"hljs-keyword\">new</span> <span class=\"hljs-title class_\">FunctionTool</span>(sum, {\n  <span class=\"hljs-attr\">name</span>: <span class=\"hljs-string\">\"sum\"</span>,\n  <span class=\"hljs-attr\">description</span>: <span class=\"hljs-string\">\"Use this function to sum two numbers\"</span>,\n  <span class=\"hljs-attr\">parameters</span>: sumJSON,\n});\n\n<span class=\"hljs-comment\">// Creat multiply function tool</span>\n<span class=\"hljs-keyword\">const</span> multiplyFunctionTool = <span class=\"hljs-keyword\">new</span> <span class=\"hljs-title class_\">FunctionTool</span>(multiply, {\n  <span class=\"hljs-attr\">name</span>: <span class=\"hljs-string\">\"multiply\"</span>,\n  <span class=\"hljs-attr\">description</span>: <span class=\"hljs-string\">\"Use this function to multiply two numbers\"</span>,\n  <span class=\"hljs-attr\">parameters</span>: multiplyJSON,\n});</span></pre>\n <h2>\n  Chat with Agent\n </h2>\n <p>\n  Now we have the tools to give to the agent, we can set up the agent:\n </p>\n <pre><span class=\"qj ov gt qa b bf qk ql l qm qn\" id=\"510f\"><span class=\"hljs-comment\">// Setup the agent with the respective tools</span>\n<span class=\"hljs-keyword\">const</span> agent = <span class=\"hljs-keyword\">new</span> <span class=\"hljs-title class_\">OpenAIAgent</span>({\n  <span class=\"hljs-attr\">tools</span>: [sumFunctionTool, multiplyFunctionTool],\n  <span class=\"hljs-attr\">verbose</span>: <span class=\"hljs-literal\">true</span>,\n});</span></pre>\n <p>\n  And then ask a question:\n </p>\n <pre><span class=\"qj ov gt qa b bf qk ql l qm qn\" id=\"74a4\"><span class=\"hljs-comment\">// Chat with LLM</span>\n<span class=\"hljs-keyword\">const</span> response = <span class=\"hljs-keyword\">await</span> agent.<span class=\"hljs-title function_\">chat</span>({\n  <span class=\"hljs-attr\">message</span>: <span class=\"hljs-string\">\"How much is 5 + 5? then multiply by 2\"</span>,\n});\n\n<span class=\"hljs-comment\">// Agent output</span>\n<span class=\"hljs-variable language_\">console</span>.<span class=\"hljs-title function_\">log</span>(<span class=\"hljs-title class_\">String</span>(response));</span></pre>\n <p>\n  Now the agent will choose the right tools to achieve the desired task using the functions provided by you. Then you should see an output as:\n </p>\n <pre><span class=\"qj ov gt qa b bf qk ql l qm qn\" id=\"e0a4\">=== <span class=\"hljs-title class_\">Calling</span> <span class=\"hljs-title class_\">Function</span> ===\n<span class=\"hljs-title class_\">Calling</span> <span class=\"hljs-attr\">function</span>: sum <span class=\"hljs-keyword\">with</span> <span class=\"hljs-attr\">args</span>: {\n  <span class=\"hljs-string\">\"a\"</span>: <span class=\"hljs-number\">5</span>,\n  <span class=\"hljs-string\">\"b\"</span>: <span class=\"hljs-number\">5</span>\n}\n<span class=\"hljs-title class_\">Got</span> output <span class=\"hljs-number\">10</span>\n==========================\n=== <span class=\"hljs-title class_\">Calling</span> <span class=\"hljs-title class_\">Function</span> ===\n<span class=\"hljs-title class_\">Calling</span> <span class=\"hljs-attr\">function</span>: multiply <span class=\"hljs-keyword\">with</span> <span class=\"hljs-attr\">args</span>: {\n  <span class=\"hljs-string\">\"a\"</span>: <span class=\"hljs-number\">10</span>,\n  <span class=\"hljs-string\">\"b\"</span>: <span class=\"hljs-number\">2</span>\n}\n<span class=\"hljs-title class_\">Got</span> output <span class=\"hljs-number\">20</span>\n==========================\n<span class=\"hljs-title class_\">The</span> result <span class=\"hljs-keyword\">of</span> adding <span class=\"hljs-number\">5</span> and <span class=\"hljs-number\">5</span> is <span class=\"hljs-number\">10.</span> <span class=\"hljs-title class_\">When</span> you multiply <span class=\"hljs-number\">10</span> by <span class=\"hljs-number\">2</span>, the result is <span class=\"hljs-number\">20.</span></span></pre>\n <h1>\n  Using Agents with your documents\n </h1>\n <p>\n  The second agent will be responsible for going through a set of Dan Abramov essays and answering questions based on the available data.\n </p>\n <p>\n  Firstly, we will import the necessary classes and functions\n </p>\n <h2>\n  Import the classes and functions\n </h2>\n <pre><span class=\"qj ov gt qa b bf qk ql l qm qn\" id=\"5d79\">import {\n  OpenAIAgent,\n  SimpleDirectoryReader,\n  VectorStoreIndex,\n  SummaryIndex,\n  QueryEngineTool,\n} from \"llamaindex\";</span></pre>\n <h2>\n  Loading Documents\n </h2>\n <p>\n  Now we will load the documents and insert them into a local vector store index which will be responsible for storing the documents and allowing the agent to query the most relevant data for the task and a summarize vector index which can better help on tasks summary related.\n </p>\n <pre><span class=\"qj ov gt qa b bf qk ql l qm qn\" id=\"d20e\"><span class=\"hljs-comment\">// Load the documents</span>\n<span class=\"hljs-keyword\">const</span> documents = <span class=\"hljs-keyword\">await</span> <span class=\"hljs-keyword\">new</span> <span class=\"hljs-title class_\">SimpleDirectoryReader</span>().<span class=\"hljs-title function_\">loadData</span>({\n  <span class=\"hljs-attr\">directoryPath</span>: <span class=\"hljs-string\">\"node_modules/llamaindex/examples\"</span>,\n});\n\n<span class=\"hljs-comment\">// Create a vector index from the documents</span>\n<span class=\"hljs-keyword\">const</span> vectorIndex = <span class=\"hljs-keyword\">await</span> <span class=\"hljs-title class_\">VectorStoreIndex</span>.<span class=\"hljs-title function_\">fromDocuments</span>(documents);\n<span class=\"hljs-keyword\">const</span> summaryIndex = <span class=\"hljs-keyword\">await</span> <span class=\"hljs-title class_\">SummaryIndex</span>.<span class=\"hljs-title function_\">fromDocuments</span>(documents)</span></pre>\n <h2>\n  Creating the Query Engine Tool\n </h2>\n <p>\n  Now we will create the tooling that allows the Agents to access the Vector Index:\n </p>\n <pre><span class=\"qj ov gt qa b bf qk ql l qm qn\" id=\"6fb3\"><span class=\"hljs-comment\">// Create a query engine from the vector index</span>\n<span class=\"hljs-keyword\">const</span> abramovQueryEngine = vectorIndex.<span class=\"hljs-title function_\">asQueryEngine</span>();\n<span class=\"hljs-keyword\">const</span> abramovSummaryEngine = summaryIndex.<span class=\"hljs-title function_\">asQueryEngine</span>();\n\n<span class=\"hljs-comment\">// Create a QueryEngineTool with the vector engine</span>\n<span class=\"hljs-keyword\">const</span> vectorEngineTool = <span class=\"hljs-keyword\">new</span> <span class=\"hljs-title class_\">QueryEngineTool</span>({\n  <span class=\"hljs-attr\">queryEngine</span>: abramovQueryEngine,\n  <span class=\"hljs-attr\">metadata</span>: {\n    <span class=\"hljs-attr\">name</span>: <span class=\"hljs-string\">\"abramov_query_engine\"</span>,\n    <span class=\"hljs-attr\">description</span>: <span class=\"hljs-string\">\"Use this engine to answer specific questions about Abramov\"</span>,\n  },\n});\n\n<span class=\"hljs-comment\">// Create a QueryEngineTool with the summary engine</span>\n<span class=\"hljs-keyword\">const</span> summaryEngineTool = <span class=\"hljs-keyword\">new</span> <span class=\"hljs-title class_\">QueryEngineTool</span>({\n  <span class=\"hljs-attr\">queryEngine</span>: abramovSummaryEngine,\n  <span class=\"hljs-attr\">metadata</span>: {\n    <span class=\"hljs-attr\">name</span>: <span class=\"hljs-string\">\"abramov_summary_engine\"</span>,\n    <span class=\"hljs-attr\">description</span>: <span class=\"hljs-string\">\"Use this engine to generate summaries about Abramov\"</span>,\n  },\n});</span></pre>\n <h2>\n  Creating the OpenAI Agent\n </h2>\n <p>\n  Now we can create and provide the necessary tools for the agent\n </p>\n <pre><span class=\"qj ov gt qa b bf qk ql l qm qn\" id=\"5be4\"><span class=\"hljs-comment\">// Setup the agent </span>\n<span class=\"hljs-keyword\">const</span> agent = <span class=\"hljs-keyword\">new</span> <span class=\"hljs-title class_\">OpenAIAgent</span>({\n  <span class=\"hljs-attr\">tools</span>: [vectorEngineTool, summaryEngineTool],\n  <span class=\"hljs-attr\">verbose</span>: <span class=\"hljs-literal\">true</span>,\n});</span></pre>\n <h2>\n  Chat with Agent\n </h2>\n <p>\n  Now you can chat with the agent about Dan Abramov and it will select the right tools to achieve the goal.\n </p>\n <pre><span class=\"qj ov gt qa b bf qk ql l qm qn\" id=\"9e79\">\n<span class=\"hljs-comment\">// Chat with Agent</span>\n<span class=\"hljs-keyword\">const</span> response = <span class=\"hljs-keyword\">await</span> agent.<span class=\"hljs-title function_\">chat</span>({\n  <span class=\"hljs-attr\">message</span>: <span class=\"hljs-string\">\"Where he worked in his 20s?\"</span>,\n});\n\n<span class=\"hljs-comment\">// Log output</span>\n<span class=\"hljs-variable language_\">console</span>.<span class=\"hljs-title function_\">log</span>(<span class=\"hljs-title class_\">String</span>(response));</span></pre>\n <p>\n  The tool output\n </p>\n <pre><span class=\"qj ov gt qa b bf qk ql l qm qn\" id=\"d61b\">=== Calling Function ===\nCalling <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span>: <span class=\"hljs-title\">abramov_query_engine</span> <span class=\"hljs-title\">with</span> <span class=\"hljs-title\">args</span>: </span>{\n  <span class=\"hljs-string\">\"query\"</span>: <span class=\"hljs-string\">\"work experience in 20s\"</span>\n}\nGot output The individual had their first job <span class=\"hljs-keyword\">as</span> a software developer in their <span class=\"hljs-number\">20</span>s. They worked <span class=\"hljs-keyword\">for</span> a Russian-American outsourcing company <span class=\"hljs-keyword\">and</span> their salary was $<span class=\"hljs-number\">18</span>k/year.\n==========================\nIn his <span class=\"hljs-number\">20</span>s, Abramov worked <span class=\"hljs-keyword\">as</span> a software developer <span class=\"hljs-keyword\">for</span> a Russian-American outsourcing company. His salary during that time was $<span class=\"hljs-number\">18</span>,<span class=\"hljs-number\">000</span> per year.</span></pre>\n <h1>\n  Conclusion\n </h1>\n <p>\n  Autonomous agents are powerful when talking about creating workflows and automation that can take your business to the next level or make your life easier.\n </p>\n <p>\n  There is still a long way to go before they are fully autonomous but the arrival of LLMs is allowing the first steps of these reasoning and decision-making engines\n </p>\n <p>\n  Do you already use agents in your day-to-day? and wanna discuss agents or help with agents? Reach me on\n  <a href=\"https://twitter.com/manelferreira_\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Twitter\n  </a>\n </p>\n <h1>\n  References\n </h1>\n <p>\n  Code example:\n  <a href=\"https://github.com/EmanuelCampos/agents-typescript-example\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://github.com/EmanuelCampos/agents-typescript-example\n  </a>\n </p>\n <p>\n  LLamaIndexTS Documentation:\n  <a href=\"https://ts.llamaindex.ai/modules/agent/openai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://ts.llamaindex.ai/modules/agent/openai\n  </a>\n </p>\n <p>\n  <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://www.llamaindex.ai/\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 16245, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "015332b0-8eea-4d0a-8af8-d3ec7fdc0849": {"__data__": {"id_": "015332b0-8eea-4d0a-8af8-d3ec7fdc0849", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.html", "file_name": "how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.html", "file_type": "text/html", "file_size": 8975, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.html", "file_name": "how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.html", "file_type": "text/html", "file_size": 8975, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "5fdf499ea48ad0ac25e4001ecca52ac0562dd180b016d02bac81a9aac364188d", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  ChatGPT, developed by OpenAI, has changed the way we interact online. Being a general purpose chatbot, ChatGPT is limited to answering generic queries. But it becomes even more useful if you can get it to answer your questions specific to your business. To do that, you need to train ChatGPT on your data.\n </p>\n <p>\n  <a href=\"https://www.thesamur.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   EmbedAI\n  </a>\n  is a no-code platform for creating AI chatbots trained on your business data. This includes data sourced from web pages, PDFs, Notion documents, or YouTube videos, allowing EmbedAI to adapt to a wide range of information sources.\n </p>\n <p>\n  In this blog post, we\u2019ll show you how we used\n  <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaIndex\n  </a>\n  with\n  <a href=\"https://thesamur.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   EmbedAI\n  </a>\n  to enable us to train ChatGPT on your own data, helping you create a customized and effective AI chatbot tailored for your business needs.\n </p>\n <h1>\n  Chat with your data use-cases\n </h1>\n <p>\n  There\u2019s a variety of ways that a chatbot trained on your data could be helpful, including:\n </p>\n <ol>\n  <li>\n   <strong>\n    Customer Support Bot\n   </strong>\n   : Manages frequently asked questions about a product, addressing customer support inquiries efficiently.\n  </li>\n  <li>\n   <strong>\n    Company Search Engine\n   </strong>\n   : Finds internal company documents and information fast, boosting workplace efficiency.\n  </li>\n  <li>\n   <strong>\n    Personalized Learning Assistant\n   </strong>\n   : Offers tailored educational support and study guidance based on specific course content.\n  </li>\n  <li>\n   <strong>\n    Technical Support assistant\n   </strong>\n   : Provides in-depth help for complex software issues, from troubleshooting to usage tips.\n  </li>\n  <li>\n   <strong>\n    Healthcare Assistant\n   </strong>\n   : Gives general health advice and information, based on medical literature and FAQs.\n  </li>\n  <li>\n   <strong>\n    Finance Chatbot\n   </strong>\n   : Assists with financial queries, offering advice on products, market trends, and investment strategies by training on financial data\n  </li>\n </ol>\n <p>\n  Let\u2019s delve into creating our own chat apps that integrate with various data sources like PDFs, Notion documents, videos, webpages, and more.\n </p>\n <h1>\n  Case 1: Custom ChatGPT for your site\n </h1>\n <p>\n  To train ChatGPT on your website content, we need to scrape the content from all the relevant webpages. The steps to do this are:\n </p>\n <ul>\n  <li>\n   Extract all the URLs from your website, such as from your sitemap\n  </li>\n  <li>\n   Include only relevant URLs which you need to train on\n  </li>\n  <li>\n   Use SimpleWebPageReader from\n   <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaIndex\n   </a>\n   to download the content from these URLs\n  </li>\n </ul>\n <p>\n  Here\u2019s some sample code to do that:\n </p>\n <p>\n  Once the data is ready, an AI chatbot can be trained on these documents by using LlamaIndex\u2019s VectorStoreIndex class.\n </p>\n <p>\n  To create a ChatGPT chatbot on your website without coding you can use EmbedAI as outlined below which uses LlamaIndex internally:\n </p>\n <h1>\n  Case 2: Custom ChatGPT for your PDF documents\n </h1>\n <p>\n  If your business specific data is stored in PDF documents and you wish to create a chatbot that can surface the information in them we can do that with LlamaIndex using the PDFMiner library. This time the steps are:\n </p>\n <ul>\n  <li>\n   Upload your PDFs and store them in the cloud\n  </li>\n  <li>\n   Install the PDFMiner library\n  </li>\n  <li>\n   Fetch the uploaded PDFs and extract the document text using LlamaIndex loader\n  </li>\n </ul>\n <p>\n  Here\u2019s the code for creating an AI chatbot trained on PDF documents with LlamaIndex\n </p>\n <p>\n  If you want to create a ChatGPT chatbot on your PDF content without coding you can use EmbedAI as in the demo below which uses LlamaIndex internally\n </p>\n <h1>\n  Case 3: Custom ChatGPT for your videos\n </h1>\n <p>\n  Often, valuable information is embedded in videos, which isn\u2019t as accessible for users searching for information. However, by training an AI chatbot with this content, it can become an incredibly rich resource for your users, significantly enhancing their experience.\n </p>\n <p>\n  Let\u2019s see how we can fetch the information from our youtube videos to train an AI chatbot using LlamaIndex. The steps are:\n </p>\n <ul>\n  <li>\n   Find your Channel ID\n  </li>\n  <li>\n   Install\n   <code class=\"cw qe qf qg qh b\">\n    scrapetube\n   </code>\n   and pass it your channel ID to get your list of videos\n  </li>\n  <li>\n   Install the Youtube transcript api and pass the video URLs from above to LlamaIndex loader to get a list of documents\n  </li>\n </ul>\n <p>\n  The code looks like this:\n </p>\n <p>\n  Now you can train an AI chatbot on these documents by using SimpleVectorIndex from LlamaIndex to create a ChatGPT bot trained on your youtube videos, and as before, you can use EmbedAI to create a chatbot with no code.\n </p>\n <h1>\n  Case 4: Custom ChatGPT for Notion\n </h1>\n <p>\n  In many modern companies, a significant portion of their content is stored in Notion. As this content grows, quickly locating specific information becomes increasingly challenging. To address this, we can develop a chatbot for Notion to streamline the process of finding the necessary information.\n </p>\n <p>\n  Steps to prepare the data:\n </p>\n <ul>\n  <li>\n   Fetch an access token from Notion following\n   <a href=\"https://www.notion.so/help/create-integrations-with-the-notion-api\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    their instructions\n   </a>\n  </li>\n  <li>\n   Using the Notion API, parse data from Notion and generate LlamaIndex documents\n  </li>\n  <li>\n   Train a chatbot on these using VectorStoreIndex\n  </li>\n </ul>\n <p>\n  If you prefer a No-code way to train a chatbot on your Notion documents, you can use EmbedAI as in the demo below which uses LlamaIndex internally:\n </p>\n <p>\n  This doesn\u2019t stop here. With EmbedAI, you can connect data from even more sources like Google Docs, Shopify or even use Zapier to connect with 6000+ tools and chat with their data. You can achieve this by choosing your specific data connector from\n  <a href=\"https://llamahub.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaHub\n  </a>\n </p>\n <h1>\n  Challenges while building EmbedAI\n </h1>\n <ul>\n  <li>\n   In EmbedAI, while connecting with a data source like Notion, the data can keep changing regularly which needs to be auto-refreshed. So the data needs a periodic refresh to add new documents or edit existing documents which needs to be handled internally. Likewise, when indexing website data it can be refreshed regularly. LlamaIndex makes it easy to handle these scenarios. LlamaIndex has a\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/ingestion/redis_ingestion_pipeline.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    guide to handling continuous ingestion\n   </a>\n   .\n  </li>\n  <li>\n   Querying over tabular data in EmbedAI is a major issue when dealing with PDF content containing tables. Naive chunking can give sub-optimal results and even hallucinations. LlamaIndex provides\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/query_engine/sec_tables/tesla_10q_table.html#joint-tabular-semantic-qa-over-tesla-10k\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    a guide on how to deal with PDFs containing both text and tables\n   </a>\n   and achieve optimal results while querying.\n  </li>\n  <li>\n   Shopify integration in EmbedAI needed hybrid search, as we needed to search not only on product description but also on product metadata. Thus a combination of semantic search and keyword search is needed to obtain optimal results. LlamaIndex provides a simple framework to build a hybrid search application, such as in\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/vector_stores/PineconeIndexDemo-Hybrid.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    this example\n   </a>\n   .\n  </li>\n </ul>\n <h1>\n  Custom trained chatbots can help your business\n </h1>\n <p>\n  Training ChatGPT with your own data provides a significant advantage for your business. From enhancing customer support with bots trained on specific product knowledge to creating sophisticated company search engines, the applications are as diverse as they are impactful. LlamaIndex provides a lot of abstractions to help with building a custom chatbot trained on your data, and we use them heavily at EmbedAI. For those seeking a no-code solution to develop an AI chatbot tailored to their data, starting with EmbedAI is a straightforward option and we encourage you to\n  <a href=\"https://www.thesamur.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   try it out\n  </a>\n  .\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 8956, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19be6fb5-5454-421a-a25c-65cd25babeb4": {"__data__": {"id_": "19be6fb5-5454-421a-a25c-65cd25babeb4", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.html", "file_name": "improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.html", "file_type": "text/html", "file_size": 8911, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.html", "file_name": "improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.html", "file_type": "text/html", "file_size": 8911, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "a29cd2c197f62871cfb1d980cd3811497b1c31c2f5e697faf0efe539b69c09ba", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h2>\n  Introduction\n </h2>\n <p>\n  Large Language Models (LLMs) improve performance by accessing external data for background knowledge tasks related. However, existing approaches require costly modifications during LM\u2019s pre-training or integrating the data store after the model has been trained. On the downside, both strategies lead to suboptimal performance.\n </p>\n <p>\n  To address this problem an\n  <strong>\n   AI Research team at Meta\n  </strong>\n  has proposed a method called\n  <a href=\"https://arxiv.org/pdf/2310.01352.pdf\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    RA-DIT: RETRIEVAL-AUGMENTED DUAL INSTRUCTION TUNING\n   </strong>\n  </a>\n  that allows any LLM to be upgraded to include retrieval features.\n </p>\n <p>\n  In this blog post, we will explore RA-DIT capabilities to have better performance on Retrieval Augmentation Generation (RAG) through building the dataset and fine-tuning the models.\n </p>\n <p>\n  The RA-DIT approach involves two distinct fine-tuning steps:\n </p>\n <ol>\n  <li>\n   Update a pre-trained LM to better use retrieved information.\n  </li>\n  <li>\n   Update the retriever to return more relevant results\n  </li>\n </ol>\n <h2>\n  How it works\n </h2>\n <p>\n  The RA-DIT approach separately fine-tunes the LLM and the retriever. The LLM is updated to maximize the probability of the correct answer given the retrieval-augmented instructions, while the retriever is updated to minimize how much the document is semantically similar (relevant) to the query.\n </p>\n <p>\n  Below we are going through each step from generating the fine-tuning dataset, fine-tuning the language model for better predictions, and refining the retrieval search process.\n </p>\n <h2>\n  Fine-tuning Dataset\n </h2>\n <p>\n  The fine-tuning dataset is tailored to enhance the language model\u2019s ability to leverage knowledge and boost its contextual awareness during prediction generation. Generating Q/A pairs, summarizing data, and incorporating chain-of-thought reasoning can lead to improved results when integrated with the models.\n </p>\n <p>\n  Following our\n  <a href=\"https://docs.llamaindex.ai/en/stable/examples/finetuning/knowledge/finetune_retrieval_aug.html#fine-tuning-with-retrieval-augmentation\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LamaIndex implementation\n  </a>\n  , we retrieve the top_k nodes, generate Q/A pairs from the documents, and then augment the data. We use the Q/A pairs through the QueryResponseDataset module, which returns a (query, response) pair for the fine-tuning dataset. While the retrieval fine-tuning data set is created on Q/A pairs data.\n </p>\n <h2>\n  Language Model Fine-tuning\n </h2>\n <p>\n  With our fine-tuning dataset in hand, we can refine our LLM to achieve two main benefits: Adapt the LLM to better utilization of relevant background knowledge and train the LLM to produce accurate predictions even with incorrectly retrieved chunks, empowering the model to rely on its own knowledge.\n </p>\n <h2>\n  Retriever Fine-tuning\n </h2>\n <p>\n  The retriever is fine-tuned using the LM-Supervised Retrieval (LSR) method. In this approach, the LLM assesses the information fetched by the retriever. If the LLM finds the information misaligned with the given query, it sends feedback to the retriever. Using this feedback, the retriever refines its search process, ensuring it fetches data that the LLM can effectively use. This collaboration enhances the overall quality of the answers provided.\n </p>\n <h2>\n  Evaluation\n </h2>\n <p>\n  To assess the suggested method, the authors employed specific datasets and metrics. Let\u2019s delve into each of these to grasp the experimental results better.\n </p>\n <h2>\n  Metrics\n </h2>\n <p>\n  An \u201cexact match\u201d (EM) metric was used to measure how closely the model\u2019s prediction matches the ground truth answer.\n </p>\n <h2>\n  Dataset\n </h2>\n <p>\n  The methodology was tested on two distinct tasks:\n </p>\n <ol>\n  <li>\n   Knowledge-intensive tasks.\n  </li>\n  <li>\n   Commonsense reasoning.\n  </li>\n </ol>\n <p>\n  Let\u2019s explore the datasets utilized for both of these tasks.\n </p>\n <h2>\n  Knowledge-intensive dataset\n </h2>\n <p>\n  For knowledge-intensive tasks the selected datasets predominantly focus on the model\u2019s capacity to access, understand, and relay deep and specific knowledge. They encompass questions rooted in facts, general trivia, and complex domain-specific queries;\n </p>\n <p>\n  The datasets used are MMLU, Natural Questions (NQ), TriviaQA, and a subset of tasks from the KILT benchmark.\n </p>\n <h2>\n  <strong>\n   Commonsense reasoning dataset\n  </strong>\n </h2>\n <p>\n  Commonsense reasoning datasets challenge the model\u2019s ability to reason and infer based on general knowledge and everyday scenarios. They contain questions and scenarios that typically don\u2019t rely on deep domain knowledge but rather on intuitive and general world understanding.\n </p>\n <p>\n  The datasets used are BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC-E, ARC-C, OBQA.\n </p>\n <p>\n  For a better understanding of how these datasets were utilized you can check the\n  <a href=\"https://arxiv.org/pdf/2310.01352.pdf\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   paper\n  </a>\n  for better understanding.\n </p>\n <h2>\n  Results\n </h2>\n <p>\n  In a comparative analysis of model performance on knowledge-intensive (Table 2 below) and commonsense reasoning tasks (Table 3 below), three models were considered:\n </p>\n <ul>\n  <li>\n   LLAMA 65B\n  </li>\n  <li>\n   LLAMA 65B REPLUG (only retrieval augmentation)\n  </li>\n  <li>\n   RA-DIT 65B\n  </li>\n </ul>\n <h2>\n  Knowledge Intensive Tasks\n </h2>\n <p>\n  Evaluations are conducted in 0-shot, 5-shot, and 64-shot fine-tuning settings.\n </p>\n <p>\n  <strong>\n   0-shot Analysis:\n  </strong>\n </p>\n <ul>\n  <li>\n   RA-DIT 65B demonstrated superior performance with an average EM score of 50.5 across all tasks.\n  </li>\n  <li>\n   It outperformed LLAMA 65B REPlug (43.1 average) and significantly surpassed LLAMA 65B (32.9 average).\n  </li>\n </ul>\n <p>\n  <strong>\n   5-shot Analysis:\n  </strong>\n </p>\n <ul>\n  <li>\n   RA-DIT 65B maintained its lead with an average EM score of 55.2.\n  </li>\n  <li>\n   LLAMA 65B REPlug followed closely with 52.7, while LLAMA 65B achieved an average of 45.0.\n  </li>\n </ul>\n <p>\n  In a separate evaluation for 64-shot fine-tuning, two models were analyzed: ATLAS and RA-DIT 65B.\n </p>\n <p>\n  <strong>\n   64-shot Fine-tuning:\n  </strong>\n </p>\n <ul>\n  <li>\n   RA-DIT 65B achieved an average performance of 60.9 across all tasks, slightly surpassing ATLAS, which obtained an average score of 56.8\n  </li>\n </ul>\n <h2>\n  Commonsense reasoning\n </h2>\n <p>\n  <em class=\"qb\">\n   RA-DIT 65B\n  </em>\n  was benchmarked in order to evaluate the impact of retrieval-augmented instruction tuning on the LLMs parametric knowledge and reasoning capabilities.\n </p>\n <p>\n  In this experiment without retrieval augmentation,\n  <em class=\"qb\">\n   RA-DIT\n  </em>\n  showed improvements over base\n  <em class=\"qb\">\n   LLAMA\n  </em>\n  65B models on 7 of 8 evaluation datasets, indicating that the parametric knowledge and reasoning capabilities of the LLM component are in general preserved.\n </p>\n <p>\n  In summary, RA-DIT 65B consistently delivered great results, surpassing its competitors in multiple scenarios, underscoring its proficiency and aptitude in knowledge-intensive tasks while showing that the parametric knowledge and reasoning capabilities of the LLM are still preserved.\n </p>\n <h2>\n  Conclusion\n </h2>\n <p>\n  The RA-DIT approach provides a structured method to enhance how Large Language Models utilize external data. Through the dual fine-tuning of both the model and the retriever, we target better accuracy and context-awareness in responses.\n </p>\n <p>\n  The incorporation of the LSR technique fosters a more efficient data retrieval process, ensuring that the generated answers are both relevant and informed, the final results show that RA-DIT surpasses un-tuned RALM approaches like REPLUG showing competitive results.\n </p>\n <p>\n  You can explore more about the LLamaIndex implementation at:\n  <a href=\"https://docs.llamaindex.ai/en/stable/examples/finetuning/knowledge/finetune_retrieval_aug.html#fine-tuning-with-retrieval-augmentation\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://docs.llamaindex.ai/en/stable/examples/finetuning/knowledge/finetune_retrieval_aug.html#fine-tuning-with-retrieval-augmentation\n  </a>\n </p>\n <h2>\n  References\n </h2>\n <p>\n  RA-DIT paper:\n  <a href=\"https://arxiv.org/abs/2310.01352\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://arxiv.org/abs/2310.01352\n  </a>\n </p>\n <p>\n  Connect with me on\n  <a href=\"https://twitter.com/manelferreira_\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Twitter\n  </a>\n  and\n  <a href=\"https://www.linkedin.com/in/emanuelcferreira/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Linkedin\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 8890, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0411a47-97c5-43ae-98c4-efb40e22c9bf": {"__data__": {"id_": "b0411a47-97c5-43ae-98c4-efb40e22c9bf", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.html", "file_name": "improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.html", "file_type": "text/html", "file_size": 20086, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.html", "file_name": "improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.html", "file_type": "text/html", "file_size": 20086, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "e592b0e0b7fba3a86eec671b54f27b2b66c6b0c4cd2736277aaf27316a739ad4", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h1>\n  Introduction:\n </h1>\n <p>\n  Achieving an efficient Retrieval-Augmented-Generation (RAG) pipeline is heavily dependent on robust retrieval performance. As we explored in our previous\n  <a href=\"https://medium.com/llamaindex-blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83\" rel=\"noopener\">\n   blog post\n  </a>\n  , rerankers have a significant impact on boosting retrieval performance. But what if we could take it a step further? What if our reranker was not just any reranker, but one tuned specifically to our domain or dataset? Could this specialization enhance the retrieval performance even more?\n </p>\n <p>\n  To answer these questions, we turn to CohereAI\u2019s beta release of fine-tuning reranker(Custom reranker) models. By integrating these with LlamaIndex, we now offer the ability to build your very own Cohere custom reranker using our streamlined process.\n </p>\n <p>\n  In this blog post, we\u2019ll guide you through the steps to create a Cohere custom reranker with LlamaIndex and evaluate the retrieval performance.\n </p>\n <p>\n  For a hands-on walkthrough, you can follow the tutorial on\n  <a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/finetuning/rerankers/cohere_custom_reranker.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Google Colab Notebook\n  </a>\n  .\n </p>\n <p>\n  Let\u2019s start fine-tuning a Cohere reranker (custom reranker) with LlamaIndex.\n </p>\n <blockquote>\n  <p class=\"oi oj pp ok b ol ph on oo op pi or os ot pj ov ow ox pk oz pa pb pl pd pe pf gm bj\" id=\"44a2\">\n   N\n   <!-- -->\n   OTE: This is a guide for fine-tuning a Cohere reranker (custom reranker). The results presented at the end of this tutorial are unique to the chosen dataset and parameters. We suggest experimenting with your dataset and various parameters before deciding to incorporate it into your RAG pipeline.\n  </p>\n </blockquote>\n <h1>\n  Setting Up the Environment\n </h1>\n <pre><span class=\"pz nl gt pw b bf qa qb l qc qd\" id=\"5f24\">!pip install llama-index cohere pypdf</span></pre>\n <h1>\n  Setting Up the Keys\n </h1>\n <pre><span class=\"pz nl gt pw b bf qa qb l qc qd\" id=\"9827\">openai_api_key = 'YOUR OPENAI API KEY'\ncohere_api_key = 'YOUR COHEREAI API KEY'\n\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\nos.environ[\"COHERE_API_KEY\"] = cohere_api_key</span></pre>\n <h1>\n  Download the Data\n </h1>\n <p>\n  We will use Lyft 2021 10K SEC Filings for training and Uber 2021 10K SEC Filings for evaluation.\n </p>\n <pre><span class=\"pz nl gt pw b bf qa qb l qc qd\" id=\"726f\">!mkdir -p 'data/10k/'\n!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'\n!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/lyft_2021.pdf' -O 'data/10k/lyft_2021.pdf'</span></pre>\n <h1>\n  Load the Data\n </h1>\n <pre><span class=\"pz nl gt pw b bf qa qb l qc qd\" id=\"bb7a\">lyft_docs = SimpleDirectoryReader(input_files=['./data/10k/lyft_2021.pdf']).load_data()\nuber_docs = SimpleDirectoryReader(input_files=['./data/10k/uber_2021.pdf']).load_data()</span></pre>\n <h1>\n  Data Curation\n </h1>\n <p>\n  <strong>\n   Create Nodes.\n  </strong>\n </p>\n <p>\n  The\n  <a href=\"https://docs.cohere.com/docs/rerank-models\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   documentation\n  </a>\n  mentions that Query + Relevant Passage/ Query + Hard Negatives should be less than 510 tokens. To accommodate that we limit\n  <code class=\"cw qe qf qg pw b\">\n   chunk_size\n  </code>\n  to 400 tokens. (Each chunk will eventually be treated as a Relevant Passage/ Hard Negative)\n </p>\n <pre><span class=\"pz nl gt pw b bf qa qb l qc qd\" id=\"e284\"># Limit chunk size to 400\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=400)\n\n# Create nodes\nlyft_nodes = node_parser.get_nodes_from_documents(lyft_docs)\nuber_nodes = node_parser.get_nodes_from_documents(uber_docs)</span></pre>\n <p>\n  We will use gpt-4 to create questions from chunks.\n </p>\n <pre><span class=\"pz nl gt pw b bf qa qb l qc qd\" id=\"47d3\">llm = OpenAI(api_key=openai_api_key, temperature=0, model='gpt-4')</span></pre>\n <p>\n  Prompt to generate questions from each Node/ chunk.\n </p>\n <pre><span class=\"pz nl gt pw b bf qa qb l qc qd\" id=\"7066\"># Prompt to generate questions\nqa_generate_prompt_tmpl = \"\"\"\\\nContext information is below.\n\n---------------------\n{context_str}\n---------------------\n\nGiven the context information and not prior knowledge.\ngenerate only questions based on the below query.\n\nYou are a Professor. Your task is to setup \\\n{num_questions_per_chunk} questions for an upcoming \\\nquiz/examination. The questions should be diverse in nature \\\nacross the document. The questions should not contain options, not start with Q1/ Q2. \\\nRestrict the questions to the context information provided.\\\n\"\"\"</span></pre>\n <p>\n  It expects a minimum of 256 (Query + Relevant passage) pairs with or without hard negatives for training and 64 pairs for validation. Please note that the validation is optional.\n </p>\n <p>\n  <strong>\n   Training:\n  </strong>\n  We use the first 256 nodes from Lyft for creating training pairs.\n </p>\n <p>\n  <strong>\n   Validation:\n  </strong>\n  We will use the next 64 nodes from Lyft for validation.\n </p>\n <p>\n  <strong>\n   Testing:\n  </strong>\n  We will use the first 150 nodes from Uber.\n </p>\n <pre><span class=\"pz nl gt pw b bf qa qb l qc qd\" id=\"7d01\"><span class=\"hljs-comment\"># Training dataset</span>\nqa_dataset_lyft_train = generate_question_context_pairs(\n    lyft_nodes[:<span class=\"hljs-number\">256</span>], llm=llm, num_questions_per_chunk=<span class=\"hljs-number\">1</span>, qa_generate_prompt_tmpl=qa_generate_prompt_tmpl\n)\n\n<span class=\"hljs-comment\"># Save [Optional]</span>\nqa_dataset_lyft_train.save_json(<span class=\"hljs-string\">\"lyft_train_dataset.json\"</span>)\n\n<span class=\"hljs-comment\"># Validation dataset</span>\nqa_dataset_lyft_val = generate_question_context_pairs(\n    lyft_nodes[<span class=\"hljs-number\">257</span>:<span class=\"hljs-number\">321</span>], llm=llm, num_questions_per_chunk=<span class=\"hljs-number\">1</span>, qa_generate_prompt_tmpl=qa_generate_prompt_tmpl\n)\n\n<span class=\"hljs-comment\"># Save [Optional]</span>\nqa_dataset_lyft_val.save_json(<span class=\"hljs-string\">\"lyft_val_dataset.json\"</span>)\n\n<span class=\"hljs-comment\"># Testing dataset</span>\nqa_dataset_uber_val = generate_question_context_pairs(\n    uber_nodes[:<span class=\"hljs-number\">150</span>], llm=llm, num_questions_per_chunk=<span class=\"hljs-number\">1</span>, qa_generate_prompt_tmpl=qa_generate_prompt_tmpl\n)\n\n<span class=\"hljs-comment\"># Save [Optional]</span>\nqa_dataset_uber_val.save_json(<span class=\"hljs-string\">\"uber_val_dataset.json\"</span>)</span></pre>\n <p>\n  Now that we have compiled questions from each chunk, we will format the data according to the specifications required for training and validation.\n </p>\n <h1>\n  Data Format and Requirements\n </h1>\n <p>\n  For both training and validation, it currently accepts data in the format of triplets, every row should have the following\n </p>\n <p>\n  <strong>\n   query:\n  </strong>\n  This represents the question or target.\n </p>\n <p>\n  <strong>\n   relevant_passages:\n  </strong>\n  This represents a list of documents or passages that contain information that answers the query. For every query, there must be at least one relevant_passage\n </p>\n <p>\n  <strong>\n   hard_negatives:\n  </strong>\n  This represents chunks or passages that don\u2019t contain answers for the query. It should be noted that Hard negatives are optional but providing at least ~5 hard negatives will lead to meaningful improvement.\n </p>\n <p>\n  You can check the\n  <a href=\"https://docs.cohere.com/docs/rerank-models\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   documentation\n  </a>\n  for more details.\n </p>\n <p>\n  We need to have an embedding model for creating hard negatives with a cosine similarity approach.\n </p>\n <pre><span class=\"pz nl gt pw b bf qa qb l qc qd\" id=\"05a9\"># Initialize the Cohere embedding model which we use it for creating Hard Negatives.\nembed_model = CohereEmbedding(\n    cohere_api_key=cohere_api_key,\n    model_name=\"embed-english-v3.0\",\n    input_type=\"search_document\",\n)</span></pre>\n <p>\n  Let\u2019s create 3 datasets.\n </p>\n <ol>\n  <li>\n   Dataset without hard negatives.\n  </li>\n  <li>\n   Dataset with hard negatives selected at random.\n  </li>\n  <li>\n   Dataset with hard negatives selected based on cosine similarity.\n  </li>\n </ol>\n <pre><span class=\"pz nl gt pw b bf qa qb l qc qd\" id=\"b74a\"># Train and val datasets without hard negatives.\ngenerate_cohere_reranker_finetuning_dataset(\n    qa_dataset_lyft_train,\n    finetune_dataset_file_name = \"train.jsonl\"\n)\n\ngenerate_cohere_reranker_finetuning_dataset(\n    qa_dataset_lyft_val,\n    finetune_dataset_file_name = \"val.jsonl\"\n)\n\n# Train and val datasets with hard negatives selected at random.\ngenerate_cohere_reranker_finetuning_dataset(\n    qa_dataset_lyft_train,\n    num_negatives = 5,\n    hard_negatives_gen_method = \"random\",\n    finetune_dataset_file_name = \"train_5_random.jsonl\",\n    embed_model = embed_model,\n)\n\ngenerate_cohere_reranker_finetuning_dataset(\n    qa_dataset_lyft_val,\n    num_negatives = 5,\n    hard_negatives_gen_method = \"random\",\n    finetune_dataset_file_name = \"val_5_random.jsonl\",\n    embed_model = embed_model,\n)\n\n# Train and val datasets with hard negatives selected based on cosine similarity.\ngenerate_cohere_reranker_finetuning_dataset(\n    qa_dataset_lyft_train,\n    num_negatives = 5,\n    hard_negatives_gen_method = \"cosine_similarity\",\n    finetune_dataset_file_name = \"train_5_cosine_similarity.jsonl\",\n    embed_model = embed_model,\n)\n\ngenerate_cohere_reranker_finetuning_dataset(\n    qa_dataset_lyft_val,\n    num_negatives = 5,\n    hard_negatives_gen_method = \"cosine_similarity\",\n    finetune_dataset_file_name = \"val_5_cosine_similarity.jsonl\",\n    embed_model = embed_model,\n)</span></pre>\n <h1>\n  Fine-tuning Reranker (Custom Reranker)\n </h1>\n <p>\n  With our training and validation datasets ready, we\u2019re set to proceed with the training process. Be aware that this training is expected to take approximately 25 to 45 minutes.\n </p>\n <pre><span class=\"pz nl gt pw b bf qa qb l qc qd\" id=\"0413\"># Reranker model with 0 hard negatives.\nfinetune_model_no_hard_negatives = CohereRerankerFinetuneEngine(\n    train_file_name=\"train.jsonl\",\n    val_file_name=\"val.jsonl\",\n    model_name=\"lyft_reranker_0_hard_negatives1\",\n    model_type=\"RERANK\",\n    base_model=\"english\",\n    api_key = cohere_api_key\n)\nfinetune_model_no_hard_negatives.finetune()\n\n# Reranker model with 5 hard negatives selected at random\nfinetune_model_random_hard_negatives = CohereRerankerFinetuneEngine(\n    train_file_name=\"train_5_random.jsonl\",\n    val_file_name=\"val_5_random.jsonl\",\n    model_name=\"lyft_reranker_5_random_hard_negatives1\",\n    model_type=\"RERANK\",\n    base_model=\"english\",\n)\nfinetune_model_random_hard_negatives.finetune()\n\n# Reranker model with 5 hard negatives selected based on cosine similarity\nfinetune_model_cosine_hard_negatives = CohereRerankerFinetuneEngine(\n    train_file_name=\"train_5_cosine_similarity.jsonl\",\n    val_file_name=\"val_5_cosine_similarity.jsonl\",\n    model_name=\"lyft_reranker_5_cosine_hard_negatives1\",\n    model_type=\"RERANK\",\n    base_model=\"english\",\n)\nfinetune_model_cosine_hard_negatives.finetune()</span></pre>\n <p>\n  Once the jobs are submitted, you can check the training status in the\n  <code class=\"cw qe qf qg pw b\">\n   models\n  </code>\n  section of the\n  <a href=\"https://dashboard.cohere.com/models\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   dashboard\n  </a>\n  . You can check the status of the job in the dashboard and you should see an image something similar to the following one.\n </p>\n <p>\n  You then need to get the Cohere Reranker model for testing.\n </p>\n <pre><span class=\"pz nl gt pw b bf qa qb l qc qd\" id=\"ee22\">reranker_base = CohereRerank(top_n=<span class=\"hljs-number\">5</span>)\nreranker_model_0 = finetune_model_no_hard_negatives.get_finetuned_model(\n    top_n=<span class=\"hljs-number\">5</span>\n)\nreranker_model_5_random = (\n    finetune_model_random_hard_negatives.get_finetuned_model(top_n=<span class=\"hljs-number\">5</span>)\n)\nreranker_model_5_cosine = (\n    finetune_model_cosine_hard_negatives.get_finetuned_model(top_n=<span class=\"hljs-number\">5</span>)\n)</span></pre>\n <h1>\n  Testing\n </h1>\n <p>\n  We will conduct tests on the first 150 nodes from Uber using the following different rerankers.\n </p>\n <ol>\n  <li>\n   Without Reranker.\n  </li>\n  <li>\n   Cohere Reranker.\n  </li>\n  <li>\n   Fine-tuned reranker (Custom reranker) without hard negatives.\n  </li>\n  <li>\n   Fine-tuned reranker (Custom reranker) with hard negatives selected at random.\n  </li>\n  <li>\n   Fine-tuned reranker (Custom reranker) with hard negatives selected based on cosine similarity.\n  </li>\n </ol>\n <p>\n  Let\u2019s define the rerankers.\n </p>\n <pre><span class=\"pz nl gt pw b bf qa qb l qc qd\" id=\"dade\">RERANKERS = {\n    \"WithoutReranker\": \"None\",\n    \"CohereRerank\": reranker_base,\n    \"CohereRerank_0\": reranker_model_0,\n    \"CohereRerank_5_random\": reranker_model_5_random,\n    \"CohereRerank_5_cosine\": reranker_model_5_cosine,\n}</span></pre>\n <p>\n  Create an Index and Retriever for evaluation purposes.\n </p>\n <pre><span class=\"pz nl gt pw b bf qa qb l qc qd\" id=\"4724\"># Initialize the Cohere embedding model, `input_type` is different for indexing and retrieval.\nindex_embed_model = CohereEmbedding(\n    cohere_api_key=cohere_api_key,\n    model_name=\"embed-english-v3.0\",\n    input_type=\"search_document\",\n)\n\nquery_embed_model = CohereEmbedding(\n    cohere_api_key=cohere_api_key,\n    model_name=\"embed-english-v3.0\",\n    input_type=\"search_query\",\n)\n\nservice_context_index = ServiceContext.from_defaults(llm=None, embed_model=index_embed_model)\nservice_context_query = ServiceContext.from_defaults(llm=None, embed_model=query_embed_model)\n\nvector_index = VectorStoreIndex(uber_nodes[:150], service_context=service_context_index)\nvector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=10, service_context=service_context_query)</span></pre>\n <p>\n  Define a function to display the results\n </p>\n <pre><span class=\"pz nl gt pw b bf qa qb l qc qd\" id=\"c645\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">display_results</span>(<span class=\"hljs-params\">embedding_name, reranker_name, eval_results</span>):\n    <span class=\"hljs-string\">\"\"\"Display results from evaluate.\"\"\"</span>\n\n    metric_dicts = []\n    <span class=\"hljs-keyword\">for</span> eval_result <span class=\"hljs-keyword\">in</span> eval_results:\n        metric_dict = eval_result.metric_vals_dict\n        metric_dicts.append(metric_dict)\n\n    full_df = pd.DataFrame(metric_dicts)\n\n    hit_rate = full_df[<span class=\"hljs-string\">\"hit_rate\"</span>].mean()\n    mrr = full_df[<span class=\"hljs-string\">\"mrr\"</span>].mean()\n\n    metric_df = pd.DataFrame(\n        {<span class=\"hljs-string\">\"Embedding\"</span>: [embedding_name], <span class=\"hljs-string\">\"Reranker\"</span>: [reranker_name], <span class=\"hljs-string\">\"hit_rate\"</span>: [hit_rate], <span class=\"hljs-string\">\"mrr\"</span>: [mrr]}\n    )\n\n    <span class=\"hljs-keyword\">return</span> metric_df</span></pre>\n <p>\n  Loop over different rerankers and evaluate retrieval performance using Custom Retriever.\n </p>\n <pre><span class=\"pz nl gt pw b bf qa qb l qc qd\" id=\"d3cd\">results_df = pd.DataFrame()\n\nembed_name = <span class=\"hljs-string\">'CohereEmbedding'</span>\n\n<span class=\"hljs-comment\"># Loop over rerankers</span>\n<span class=\"hljs-keyword\">for</span> rerank_name, reranker <span class=\"hljs-keyword\">in</span> RERANKERS.items():\n\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Running Evaluation for Reranker: <span class=\"hljs-subst\">{rerank_name}</span>\"</span>)\n\n    <span class=\"hljs-comment\"># Define Retriever</span>\n    <span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">CustomRetriever</span>(<span class=\"hljs-title class_ inherited__\">BaseRetriever</span>):\n        <span class=\"hljs-string\">\"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"</span>\n\n        <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">\n            self,\n            vector_retriever: VectorIndexRetriever,\n        </span>) -&amp;gt; <span class=\"hljs-literal\">None</span>:\n            <span class=\"hljs-string\">\"\"\"Init params.\"\"\"</span>\n\n            self._vector_retriever = vector_retriever\n\n        <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">_retrieve</span>(<span class=\"hljs-params\">self, query_bundle: QueryBundle</span>) -&amp;gt; <span class=\"hljs-type\">List</span>[NodeWithScore]:\n            <span class=\"hljs-string\">\"\"\"Retrieve nodes given query.\"\"\"</span>\n\n            retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n\n            <span class=\"hljs-keyword\">if</span> reranker != <span class=\"hljs-string\">'None'</span>:\n                retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n            <span class=\"hljs-keyword\">else</span>:\n                retrieved_nodes = retrieved_nodes[:<span class=\"hljs-number\">5</span>]\n\n            <span class=\"hljs-keyword\">return</span> retrieved_nodes\n\n        <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">_aretrieve</span>(<span class=\"hljs-params\">self, query_bundle: QueryBundle</span>) -&amp;gt; <span class=\"hljs-type\">List</span>[NodeWithScore]:\n            <span class=\"hljs-string\">\"\"\"Asynchronously retrieve nodes given query.\n            \"\"\"</span>\n            <span class=\"hljs-keyword\">return</span> self._retrieve(query_bundle)\n\n        <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">aretrieve</span>(<span class=\"hljs-params\">self, str_or_query_bundle: QueryType</span>) -&amp;gt; <span class=\"hljs-type\">List</span>[NodeWithScore]:\n            <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">isinstance</span>(str_or_query_bundle, <span class=\"hljs-built_in\">str</span>):\n                str_or_query_bundle = QueryBundle(str_or_query_bundle)\n            <span class=\"hljs-keyword\">return</span> <span class=\"hljs-keyword\">await</span> self._aretrieve(str_or_query_bundle)\n\n    custom_retriever = CustomRetriever(vector_retriever)\n\n    retriever_evaluator = RetrieverEvaluator.from_metric_names(\n        [<span class=\"hljs-string\">\"mrr\"</span>, <span class=\"hljs-string\">\"hit_rate\"</span>], retriever=custom_retriever\n    )\n    eval_results = <span class=\"hljs-keyword\">await</span> retriever_evaluator.aevaluate_dataset(qa_dataset_uber_val)\n\n    current_df = display_results(embed_name, rerank_name, eval_results)\n    results_df = pd.concat([results_df, current_df], ignore_index=<span class=\"hljs-literal\">True</span>)</span></pre>\n <h1>\n  Results:\n </h1>\n <p>\n  From the above table (1- without reranker, 2 \u2014 with base cohere reranker, 3\u20135: Fine-tuned rerankers (Custom rerankers)), we can see that the Fine-tuned rerankers (custom rerankers) have resulted in performance improvements. It\u2019s crucial to note that the choice of the optimal number of hard negatives, as well as the decision between random or cosine sampling, should be grounded in empirical evidence. This guide offers a structured approach for improving retrieval systems through the fine-tuning of the Cohere re-ranker.\n </p>\n <h1>\n  Summary:\n </h1>\n <p>\n  In this blog post, we\u2019ve demonstrated fine-tuning a Cohere reranker (custom reranker) using LlamaIndex, which has improved retrieval performance metrics. We eagerly anticipate the community\u2019s use of these abilities to boost their retrieval efficiency within RAG pipelines. Additionally, there is room for advancement in selecting hard negatives, and we invite the community to contribute.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 20061, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d4a76c0d-8011-4cfa-ba89-4bd18f779538": {"__data__": {"id_": "d4a76c0d-8011-4cfa-ba89-4bd18f779538", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/improving-vector-search-reranking-with-postgresml-and-llamaindex.html", "file_name": "improving-vector-search-reranking-with-postgresml-and-llamaindex.html", "file_type": "text/html", "file_size": 16145, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/improving-vector-search-reranking-with-postgresml-and-llamaindex.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/improving-vector-search-reranking-with-postgresml-and-llamaindex.html", "file_name": "improving-vector-search-reranking-with-postgresml-and-llamaindex.html", "file_type": "text/html", "file_size": 16145, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "1cd1f21cc33eab2280192a688cc2cd399139d33908422525fdf1298cf97b15b1", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Search and Reranking: Improving Result Relevance\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Search systems typically employ two main methods: keyword and semantic. Keyword search matches exact query terms to indexed database content, while semantic search uses NLP and machine learning to understand query context and intent. Many effective systems combine both approaches for optimal results.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  After initial retrieval, reranking can further improve result relevance. Traditional reranking relies on historical user interaction data, but this approach struggles with new content and requires substantial data to train effectively. An advanced alternative is using cross-encoders, which directly compare query-result pairs for similarity.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Cross-encoders directly compare two pieces of text and compute a similarity score. Unlike traditional semantic search methods, we cannot precompute embeddings for cross-encoders and reuse them later. Instead, we must run the cross-encoder for every pair of texts we want to compare, making this method computationally expensive and impractical for large-scale searches. However, it is highly effective for reranking a subset of our dataset because it excels at evaluating new, unseen data without the need for extensive user interaction data for fine-tuning.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Cross-encoders complement and enhance traditional reranking systems by addressing their limitations in deep text analysis, particularly for novel or highly specific content. They do not rely on large datasets of user interactions for training (though such data can still be beneficial) and are adept at handling new and previously unseen data. This makes cross-encoders an excellent choice for enhancing the relevance of search results in a reranking context.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Implementing Reranking\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We are going to implement a simple reranking example using LlamaIndex and the PostgresML managed index. For more info on the PostgresML managed index. Check out our announcement with LlamaIndex:\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/simplify-your-rag-application-architecture-with-llamaindex-postgresml\" rel=\"noreferrer noopener\">\n   Simplify your RAG application architecture with LlamaIndex + PostgresML\n  </a>\n  .\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Install the required dependencies to get started:\n </p>\n <pre><code>pip install llama_index llama-index-indices-managed-postgresml</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We will be using the Paul Graham dataset which can be downloaded with curl:\n </p>\n <pre><code><span class=\"hljs-built_in\">mkdir</span> data\n\ncurl -o data/paul_graham_essay.txt https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The PostgresML Managed Index will handle storing, splitting, embedding, and querying our documents. All we need is a database connection string. If you haven\u2019t already,\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://postgresml.org/signup\" rel=\"noreferrer noopener\">\n   create your PostgresML account\n  </a>\n  . You\u2019ll get $100 in free credits when you complete your profile.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Set the PGML_DATABASE_URL environment variable:\n </p>\n <pre><code><span class=\"hljs-built_in\">export</span> PGML_DATABASE_URL=<span class=\"hljs-string\">\"{YOUR_CONNCECTION_STRING}\"</span></code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Let\u2019s create our index:\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index.core.readers <span class=\"hljs-keyword\">import</span> SimpleDirectoryReader\n<span class=\"hljs-keyword\">from</span> llama_index.indices.managed.postgresml <span class=\"hljs-keyword\">import</span> PostgresMLIndex\n\n\ndocuments = SimpleDirectoryReader(<span class=\"hljs-string\">\"data\"</span>).load_data()\nindex = PostgresMLIndex.from_documents(\n    documents, collection_name=<span class=\"hljs-string\">\"llama-index-rerank-example\"</span>\n)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Note the collection_name is used to uniquely identify the index you are working with.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Here we are using the SimpleDirectoryReader to load in the documents and then we construct the PostgresMLIndex from those documents.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  This workflow does not require document preprocessing. Instead, the documents are sent directly to PostgresML where they are stored, split, and embedded per the pipeline specification. This is a unique quality of using the PostgresML managed index.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Now let\u2019s search! We can perform semantic search and get the top 2 results by creating a retriever from our index.\n </p>\n <pre><code>retriever = index.as_retriever(limit=<span class=\"hljs-number\">2</span>)\ndocs = retriever.retrieve(<span class=\"hljs-string\">\"What did the author do as a child?\"</span>)\n<span class=\"hljs-keyword\">for</span> doc <span class=\"hljs-keyword\">in</span> docs:\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"---------\"</span>)\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Id: <span class=\"hljs-subst\">{doc.id_}</span>\"</span>)\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Score: <span class=\"hljs-subst\">{doc.score}</span>\"</span>)\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Text: <span class=\"hljs-subst\">{doc.text}</span>\"</span>)\n</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Doing this we get:\n </p>\n <pre><code>---------\n\nId: de01b7e1-95f8-4aa0-b4ec-45ef64816e0e\n\nScore: 0.7793415653313153\n\nText: Wow, I thought, there's an audience. If I write something and put it on the web, anyone can read it. That may seem obvious now, but it was surprising then. In the print era there was a narrow channel to readers, guarded by fierce monsters known as editors. The only way to get an audience for anything you wrote was to get it published as a book, or in a newspaper or magazine. Now anyone could publish anything.\n\n\n\nThis had been possible in principle since 1993, but not many people had realized it yet. I had been intimately involved with building the infrastructure of the web for most of that time, and a writer as well, and it had taken me 8 years to realize it. Even then it took me several years to understand the implications. It meant there would be a whole new generation of essays. [11]\n\n\n\nIn the print era, the channel for publishing essays had been vanishingly small. Except for a few officially anointed thinkers who went to the right parties in New York, the only people allowed to publish essays were specialists writing about their specialties. There were so many essays that had never been written, because there had been no way to publish them. Now they could be, and I was going to write them. [12]\n\n\n\nI've worked on several different things, but to the extent there was a turning point where I figured out what to work on, it was when I started publishing essays online. From then on I knew that whatever else I did, I'd always write essays too.\n\n\n\n---------\n\nId: de01b7e1-95f8-4aa0-b4ec-45ef64816e0e\n\nScore: 0.7770352826735559\n\nText: Asterix comics begin by zooming in on a tiny corner of Roman Gaul that turns out not to be controlled by the Romans. You can do something similar on a map of New York City: if you zoom in on the Upper East Side, there's a tiny corner that's not rich, or at least wasn't in 1993. It's called Yorkville, and that was my new home. Now I was a New York artist \u2014 in the strictly technical sense of making paintings and living in New York.\n\n\n\nI was nervous about money, because I could sense that Interleaf was on the way down. Freelance Lisp hacking work was very rare, and I didn't want to have to program in another language, which in those days would have meant C++ if I was lucky. So with my unerring nose for financial opportunity, I decided to write another book on Lisp. This would be a popular book, the sort of book that could be used as a textbook. I imagined myself living frugally off the royalties and spending all my time painting. (The painting on the cover of this book, ANSI Common Lisp, is one that I painted around this time.)\n\n\n\nThe best thing about New York for me was the presence of Idelle and Julian Weber. Idelle Weber was a painter, one of the early photorealists, and I'd taken her painting class at Harvard. I've never known a teacher more beloved by her students. Large numbers of former students kept in touch with her, including me. After I moved to New York I became her de facto studio assistant.</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  These aren\u2019t bad results, but they aren\u2019t perfect. Let\u2019s try reranking with a cross-encoder.\n </p>\n <pre><code>retriever = index.as_retriever(\n    limit=<span class=\"hljs-number\">2</span>,\n    rerank={\n        <span class=\"hljs-string\">\"model\"</span>: <span class=\"hljs-string\">\"mixedbread-ai/mxbai-rerank-base-v1\"</span>,\n        <span class=\"hljs-string\">\"num_documents_to_rerank\"</span>: <span class=\"hljs-number\">100</span>\n    }\n)\ndocs = retriever.retrieve(<span class=\"hljs-string\">\"What did the author do as a child?\"</span>)\n<span class=\"hljs-keyword\">for</span> doc <span class=\"hljs-keyword\">in</span> docs:\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"---------\"</span>)\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Id: <span class=\"hljs-subst\">{doc.id_}</span>\"</span>)\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Score: <span class=\"hljs-subst\">{doc.score}</span>\"</span>)\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Text: <span class=\"hljs-subst\">{doc.text}</span>\"</span>)\n</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Here, we configure our retriever to return the top two documents, but this time, we add a rerank parameter to use the mixedbread-ai/mxbai-rerank-base-v1 model. This means our initial semantic search will return 100 results, which will then be reranked by the mixedbread-ai/mxbai-rerank-base-v1 model, and only the top two results will be presented.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Running this outputs:\n </p>\n <pre><code>Id: de01b7e1-95f8-4aa0-b4ec-45ef64816e0e\nScore: 0.17803585529327393\nText: What I Worked On\n\nFebruary 2021\n\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines \u2014 CPU, disk drives, printer, card reader \u2014 sitting up on a raised floor under bright fluorescent lights.\n\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\n\n\n---------\nId: de01b7e1-95f8-4aa0-b4ec-45ef64816e0e\nScore: 0.1057136133313179\nText: I wanted not just to build things, but to build things that would last.\n\nIn this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where he was in grad school. One day I went to visit the Carnegie Institute, where I'd spent a lot of time as a kid. While looking at a painting there I realized something that might seem obvious, but was a big surprise to me. There, right on the wall, was something you could make that would last. Paintings didn't become obsolete. Some of the best ones were hundreds of years old.\n\nAnd moreover this was something you could make a living doing. Not as easily as you could by writing software, of course, but I thought if you were really industrious and lived really cheaply, it had to be possible to make enough to survive. And as an artist you could be truly independent. You wouldn't have a boss, or even need to get research funding.\n\nI had always liked looking at paintings. Could I make them? I had no idea. I'd never imagined it was even possible. I knew intellectually that people made art \u2014 that it didn't just appear spontaneously \u2014 but it was as if the people who made it were a different species. They either lived long ago or were mysterious geniuses doing strange things in profiles in Life magazine. The idea of actually being able to make art, to put that verb before that noun, seemed almost miraculous.\n</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  These are much better results! We can see that the top document has the answer to the user\u2019s question. Notice that we did not have to specify a third party API to use for reranking. Once again, PostgresML handles the reranking using cross-encoders in the database.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We can use re-ranking directly in RAG:\n </p>\n <pre><code>query_engine = index.as_query_engine(\n    streaming=<span class=\"hljs-literal\">True</span>,\n    vector_search_limit=<span class=\"hljs-number\">2</span>,\n    vector_search_rerank={\n        <span class=\"hljs-string\">\"model\"</span>: <span class=\"hljs-string\">\"mixedbread-ai/mxbai-rerank-base-v1\"</span>,\n        <span class=\"hljs-string\">\"num_documents_to_rerank\"</span>: <span class=\"hljs-number\">100</span>,\n    },\n)\nresults = query_engine.query(<span class=\"hljs-string\">\"What did the author do as a child?\"</span>)\n<span class=\"hljs-keyword\">for</span> text <span class=\"hljs-keyword\">in</span> results.response_gen:\n    <span class=\"hljs-built_in\">print</span>(text, end=<span class=\"hljs-string\">\"\"</span>, flush=<span class=\"hljs-literal\">True</span>)\n</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Running this outputs:\n </p>\n <pre><code>Based on the context information, as a child, the author worked on writing (writing short stories) and programming (on the IBM 1401 using Fortran) outside of school.</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  That is the exact answer we wanted!\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Reranking Leads to Better Results\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Search can be complicated. Reranking with cross-encoders improves search by comparing text pairs and effectively handling new data. Implementing reranking with LlamaIndex and PostgresML improves search results, providing more precise answers in retrieval-augmented generation applications.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  To get started with PostgresML and LlamaIndex, you can follow the PostgresML intro\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://postgresml.org/docs/introduction/getting-started/\" rel=\"noreferrer noopener\">\n   guide\n  </a>\n  to setup your account, and use the examples above with your own data.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 16118, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad700478-309e-467d-bfcc-15ed542fa03e": {"__data__": {"id_": "ad700478-309e-467d-bfcc-15ed542fa03e", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-airbyte-sources-within-llamaindex-42209071722f.html", "file_name": "introducing-airbyte-sources-within-llamaindex-42209071722f.html", "file_type": "text/html", "file_size": 12780, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/introducing-airbyte-sources-within-llamaindex-42209071722f.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-airbyte-sources-within-llamaindex-42209071722f.html", "file_name": "introducing-airbyte-sources-within-llamaindex-42209071722f.html", "file_type": "text/html", "file_size": 12780, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "405e51e240f1ff561fc6608637d4da7032589bc4bfecb72e112ddf5ca6093ee9", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Authored by Joe Reuter, Software Engineer at Airbyte\n </p>\n <p>\n  (cross-posted from the Airbyte blog; check it out\n  <a href=\"https://airbyte.com/blog/introducing-airbyte-sources-within-llamaindex\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   here\n  </a>\n  !)\n </p>\n <h1>\n  Content\n </h1>\n <p>\n  It\u2019s now possible to utilize the Airbyte sources for\n  <a href=\"https://llamahub.ai/l/airbyte_gong\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Gong\n  </a>\n  ,\n  <a href=\"https://llamahub.ai/l/airbyte_hubspot\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Hubspot\n  </a>\n  ,\n  <a href=\"https://llamahub.ai/l/airbyte_salesforce\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Salesforce\n  </a>\n  ,\n  <a href=\"https://llamahub.ai/l/airbyte_shopify\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Shopify\n  </a>\n  ,\n  <a href=\"https://llamahub.ai/l/airbyte_stripe\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Stripe\n  </a>\n  ,\n  <a href=\"https://llamahub.ai/l/airbyte_typeform\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Typeform\n  </a>\n  and\n  <a href=\"https://llamahub.ai/l/airbyte_zendesk_support\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Zendesk Support\n  </a>\n  directly within your LlamaIndex-based application, implemented as\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/data_modules/connector/usage_pattern.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   data loaders\n  </a>\n  .\n </p>\n <p>\n  For example, to load the Stripe invoices for a user, you can use the AirbyteStripeLoader. Installing it is super simple, when you have LlamaIndex installed locally you only need to install the source you are interested in, and you are ready to go:\n </p>\n <pre><span class=\"pt on gt pq b bf pu pv l pw px\" id=\"4887\">pip install airbyte-source-stripe\npip install llama-hub</span></pre>\n <p>\n  After that, simply download the loader and pass in configuration and the stream you want to load:\n </p>\n <pre><span class=\"pt on gt pq b bf pu pv l pw px\" id=\"b8cd\"><span class=\"hljs-keyword\">from</span> llama_hub.airbyte_stripe.base <span class=\"hljs-keyword\">import</span> AirbyteStripeReader\n\nconfig = {\n  <span class=\"hljs-string\">\"client_secret\"</span>: <span class=\"hljs-string\">\"&amp;lt;secret key&amp;gt;\"</span>,\n  <span class=\"hljs-string\">\"account_id\"</span>: <span class=\"hljs-string\">\"&amp;lt;account id&amp;gt;\"</span>,\n  <span class=\"hljs-string\">\"start_date\"</span>: <span class=\"hljs-string\">\"&amp;lt;date from which to start retrieving records from in ISO format, e.g. 2020\u201310\u201320T00:00:00Z&amp;gt;\"</span>\n}\nreader = AirbyteStripeReader(config=config)\ndocuments = reader.load_data(stream_name=<span class=\"hljs-string\">\"invoices\"</span>)</span></pre>\n <h1>\n  Why does this matter?\n </h1>\n <p>\n  This is the beginning of making Airbyte\u2019s\n  <a href=\"http://docs.airbyte.com/integrations\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   300+ sources\n  </a>\n  available as data loaders in LlamaHub.\n </p>\n <p>\n  Airbyte can move data from just about any source to your warehouse or vector database to power your LLM use case (check out this\n  <a href=\"https://airbyte.com/tutorials/chat-with-your-data-using-openai-pinecone-airbyte-and-langchain\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   tutorial\n  </a>\n  for setting up such a data pipeline!). This is normally done by using Airbyte Cloud or a local Airbyte instance, setting up a connection, and running it on a schedule (or via API trigger) to make sure your data stays fresh.\n </p>\n <p>\n  But if you are just getting started and are running everything locally, using a full Airbyte instance (including the UI, scheduling service, scale-out capabilities, etc..) may be overkill.\n </p>\n <p>\n  With this release, it\u2019s easier than ever to run any Python-based source in LlamaIndex directly within your Python runtime \u2014 no need to spin up an Airbyte instance or make API calls to Airbyte Cloud.\n </p>\n <h1>\n  Moving between hosted and embedded Airbyte\n </h1>\n <p>\n  Since the same code is running under the hood, every Airbyte-built loader is compatible with the respective source in the Airbyte service. This means it\u2019s trivial to lift your embedded loading pipeline into your self-hosted Airbyte installation or your Airbyte Cloud instance. The schema of the loader configuration object and that of the output records is 100% compatible.\n </p>\n <p>\n  Running syncs on hosted Airbyte means:\n </p>\n <ul>\n  <li>\n   UI to keep track of running pipelines\n  </li>\n  <li>\n   Event notifications including alerting on failing syncs or running post-sync operations\n  </li>\n  <li>\n   Easily running pipelines on a schedule\n  </li>\n  <li>\n   Scale-out capabilities\n  </li>\n  <li>\n   API to power programmatic use cases\n  </li>\n  <li>\n   Out-of-the-box state management of your connections\n  </li>\n  <li>\n   Support\n  </li>\n  <li>\n   And more\n  </li>\n </ul>\n <p>\n  Running syncs with LlamaIndex loaders means:\n </p>\n <ul>\n  <li>\n   No overhead for running yet another service\n  </li>\n  <li>\n   Full control over timing and pipeline execution\n  </li>\n </ul>\n <h1>\n  Combining Airbyte loaders with indices and query engines\n </h1>\n <p>\n  As Airbyte loaders are behaving like regular loaders, they can easily be combined with all LlamaIndex utilities to build powerful LLM-based applications:\n </p>\n <pre><span class=\"pt on gt pq b bf pu pv l pw px\" id=\"cd31\">relevant_keys = [\"customer_name\", \"total\", \"currency\"]\nreader = AirbyteStripeReader(\n    config=strip_config,\n    record_handler=lambda record, id: Document(\n        doc_id=id,\n        text=record.data[\"description\"] or \"\",\n        extra_info={\n            key: record.data[key] for key in relevant_keys if key in record.data\n        },\n    ),\n)\n\nindex = ListIndex.from_documents(reader.load_data(stream_name=\"invoices\"))\nquery_engine = index.as_query_engine()\nquestion = input(\"What do you want to know about your customers?\")\nprint(query_engine.query(question))</span></pre>\n <h1>\n  Incremental loads\n </h1>\n <p>\n  Since your python application is basically acting as the Airbyte platform, you have full control over how the \u201csync\u201d is executed. For example you can still benefit from\n  <a href=\"https://glossary.airbyte.com/term/incremental-synchronization/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   incremental syncs\n  </a>\n  if your stream supports it by accessing the \u201clast_state\u201d property of the loader. This allows you to load only documents that changed since the last time you loaded, allowing you to update an existing vector database effectively:\n </p>\n <pre><span class=\"pt on gt pq b bf pu pv l pw px\" id=\"1720\">import airbyte_cdk.models.airbyte_protocol import AirbyteMessage\nwith open('stripe_sync_checkpoint.json', 'w') as file:\n  file.write(reader.last_state.json())\n\n# later\nwith open('stripe_sync_checkpoint.json', 'r') as file:\n  current_state = AirbyteStateMessage.parse_raw(file.read())\nnew_docs = reader.load_data(stream_name=\"invoices\", state=current_state)</span></pre>\n <h1>\n  Mapping Airbyte records to LlamaIndex documents\n </h1>\n <p>\n  By default, each record gets mapped to a Document as part of the loader, with all the various fields in the record becoming a part of the `extra_info` property of the Document (the `extra_info` represents structured metadata for each document) . The text portion of the document is set to the JSON representation of the record. By default, any metadata defined on the Document will be concatenated with the text in downstream modules, so all the fields in the record will be used for embedding and synthesis purposes within a LlamaIndex app. You can pass in a record handler to customize this behavior to build the text part of a record depending on the data:\n </p>\n <pre><span class=\"pt on gt pq b bf pu pv l pw px\" id=\"69a2\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">handle_record</span>(<span class=\"hljs-params\">record, <span class=\"hljs-built_in\">id</span></span>):\n  <span class=\"hljs-keyword\">return</span> Document(doc_id=<span class=\"hljs-built_in\">id</span>, text=record.data[<span class=\"hljs-string\">\"title\"</span>], extra_info=record.data)\nreader = AirbyteGongReader(config=gong_config, record_handler=handle_record)</span></pre>\n <h1>\n  Custom sources\n </h1>\n <p>\n  For now, the following Airbyte sources are available as pip packages (with more to come):\n </p>\n <ul>\n  <li>\n   <a href=\"https://llamahub.ai/l/airbyte_gong\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Gong\n   </a>\n   pip install airbyte-source-gong\n  </li>\n  <li>\n   <a href=\"https://llamahub.ai/l/airbyte_hubspot\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Hubspot\n   </a>\n   pip install airbyte-source-hubspot\n  </li>\n  <li>\n   <a href=\"https://llamahub.ai/l/airbyte_salesforce\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Salesforce\n   </a>\n   pip install airbyte-source-salesforce\n  </li>\n  <li>\n   <a href=\"https://llamahub.ai/l/airbyte_shopify\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Shopify\n   </a>\n   pip install airbyte-source-shopify\n  </li>\n  <li>\n   <a href=\"https://llamahub.ai/l/airbyte_stripe\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Stripe\n   </a>\n   pip install airbyte-source-stripe\n  </li>\n  <li>\n   <a href=\"https://llamahub.ai/l/airbyte_typeform\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Typeform\n   </a>\n   pip install airbyte-source-typeform\n  </li>\n  <li>\n   <a href=\"https://llamahub.ai/l/airbyte_zendesk_support\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Zendesk Support\n   </a>\n   pip install airbyte-source-zendesk-support\n  </li>\n </ul>\n <p>\n  However, if you have implemented your own custom Airbyte sources, it\u2019s also possible to integrate them by using the AirbyteCDKReader base class that works with the Source interface of the Airbyte CDK:\n </p>\n <pre><span class=\"pt on gt pq b bf pu pv l pw px\" id=\"f16a\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> download_loader\n<span class=\"hljs-keyword\">from</span> my_source.source <span class=\"hljs-keyword\">import</span> MyCustomSource <span class=\"hljs-comment\"># plug in your own source here</span>\nAirbyteCDKReader = download_loader(AirbyteCDKReader)\nconfig = {\n  <span class=\"hljs-comment\"># your custom configuration</span>\n}\nreader = AirbyteCDKReader(source_class=MyCustomSource, config=config)\ndocuments = reader.load_data(stream_name=<span class=\"hljs-string\">\"my-stream\"</span>)</span></pre>\n <p>\n  You can also install sources from the main Airbyte repository by installing directly via git \u2014 for example, to fetch the Github source, simply run\n </p>\n <pre><span class=\"pt on gt pq b bf pu pv l pw px\" id=\"a8d0\">pip install <span class=\"hljs-string\">\"source_github@git+https://github.com/airbytehq/airbyte.git@master#subdirectory=airbyte-integrations/connectors/source-github\"</span></span></pre>\n <p>\n  After that, the source is available to be plucked into the AirbyteCDKLoader:\n </p>\n <pre><span class=\"pt on gt pq b bf pu pv l pw px\" id=\"3d30\"><span class=\"hljs-keyword\">from</span> source_github.source <span class=\"hljs-keyword\">import</span> SourceGithub\nissues_loader = AirbyteCDKReader(source_class=SourceGithub, config=config)\ndocuments = reader.load_data(stream_name=<span class=\"hljs-string\">\"issues\"</span>)</span></pre>\n <p>\n  Check out\n  <a href=\"https://docs.airbyte.com/connector-development/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   the connector development documentation\n  </a>\n  for how to get started writing your own sources \u2014 it\u2019s easy to get started with them and will allow you to move from local embedded loaders to using a hosted Airbyte instance seamlessly depending on your needs.\n </p>\n <h1>\n  Any questions? We would love to hear from you\n </h1>\n <p>\n  If you are interested in leveraging Airbyte to ship data to your LLM-based applications,\n  <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSduobMZwbqiFlPxsWDG-hrBw6NLYMDu_7zRfo4j7AsaO1QtfQ/viewform?usp=sf_link&amp;_gl=1*m4v6ic*_ga*MTM4ODAyNjg4NS4xNjY5ODkyNDQ1*_ga_HDBMVFQGBH*MTY5MjM2MzY0Ni45NS4xLjE2OTIzNjU2NDUuMC4wLjA.\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   please take a moment\n  </a>\n  to fill out our survey so we can make sure to prioritize the most important features.\n </p>\n <p>\n  If you have questions or are interested in other existing sources being exposed as loaders this way, do not hesitate to reach out on our\n  <a href=\"https://airbyte.com/community/community\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   community slack channel\n  </a>\n  or in the\n  <a href=\"https://discord.com/channels/1059199217496772688/1100459663696334968\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   integrations channel\n  </a>\n  on the LlamaIndex discord server.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 12749, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c18d9bd8-763e-4a8c-b32f-2e8c87b5c3be": {"__data__": {"id_": "c18d9bd8-763e-4a8c-b32f-2e8c87b5c3be", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html", "file_name": "introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html", "file_type": "text/html", "file_size": 18790, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html", "file_name": "introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html", "file_type": "text/html", "file_size": 18790, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "2ac1c499694039b98eb7b69f4e24372dfa4329f1807d344883100680d94f3c79", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We're excited to announce the alpha release of\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   llama-agents\n  </code>\n  , a new open-source framework designed to simplify the process of building, iterating, and deploying multi-agent AI systems and turn your agents into production microservices. Whether you're working on complex question-answering systems, collaborative AI assistants, or distributed AI workflows, llama-agents provides the tools and structure you need to bring your ideas to life.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Key Features of llama-agents\n </h2>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Distributed Service Oriented Architecture:\n   </strong>\n   every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Communication via standardized API interfaces:\n   </strong>\n   interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Define agentic and explicit orchestration flows:\n   </strong>\n   developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an \u201cagentic orchestrator\u201d that decides which agents are relevant to the task.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Ease of deployment:\n   </strong>\n   launch, scale and monitor each agent and your control plane independently.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Scalability and resource management:\n   </strong>\n   use our built-in observability tools to monitor the quality and performance of the system and each individual agent service\n  </li>\n </ol>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Let's dive into how you can start using llama-agents to build your own multi-agent systems.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Getting Started with llama-agents\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  First, install the framework using pip:\n </p>\n <pre><code>pip install llama-agents llama-index-agent-openai</code></pre>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  Basic System Setup\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Here's a simple example of how to set up a basic multi-agent system using llama-agents. First we\u2019ll bring in our dependencies and set up our control plane, which contains our LLM-powered orchestrator\n </p>\n <pre><code><span class=\"hljs-keyword\">import</span> dotenv\ndotenv.load_dotenv() <span class=\"hljs-comment\"># our .env file defines OPENAI_API_KEY</span>\n<span class=\"hljs-keyword\">from</span> llama_agents <span class=\"hljs-keyword\">import</span> (\n    AgentService,\n    ControlPlaneServer,\n    SimpleMessageQueue,\n    AgentOrchestrator,\n)\n<span class=\"hljs-keyword\">from</span> llama_index.core.agent <span class=\"hljs-keyword\">import</span> FunctionCallingAgentWorker\n<span class=\"hljs-keyword\">from</span> llama_index.core.tools <span class=\"hljs-keyword\">import</span> FunctionTool\n<span class=\"hljs-keyword\">from</span> llama_index.llms.openai <span class=\"hljs-keyword\">import</span> OpenAI\n<span class=\"hljs-keyword\">import</span> logging\n\n<span class=\"hljs-comment\"># turn on logging so we can see the system working</span>\nlogging.getLogger(<span class=\"hljs-string\">\"llama_agents\"</span>).setLevel(logging.INFO)\n\n<span class=\"hljs-comment\"># Set up the message queue and control plane</span>\nmessage_queue = SimpleMessageQueue()\ncontrol_plane = ControlPlaneServer(\n    message_queue=message_queue,\n    orchestrator=AgentOrchestrator(llm=OpenAI()),\n)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Next we create our tools using LlamaIndex\u2019s existing abstractions, provide those tools to an agent, and turn that agent into an independent microservice:\n </p>\n <pre><code><span class=\"hljs-comment\"># create a tool</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">get_the_secret_fact</span>() -&gt; <span class=\"hljs-built_in\">str</span>:\n    <span class=\"hljs-string\">\"\"\"Returns the secret fact.\"\"\"</span>\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"The secret fact is: A baby llama is called a 'Cria'.\"</span>\n\ntool = FunctionTool.from_defaults(fn=get_the_secret_fact)\n\n<span class=\"hljs-comment\"># Define an agent</span>\nworker = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())\nagent = worker.as_agent()\n\n<span class=\"hljs-comment\"># Create an agent service</span>\nagent_service = AgentService(\n    agent=agent,\n    message_queue=message_queue,\n    description=<span class=\"hljs-string\">\"General purpose assistant\"</span>,\n    service_name=<span class=\"hljs-string\">\"assistant\"</span>,\n)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Finally we launch the service and the control plane. Note that here we\u2019re using a helper function to run a single query through the system and then exit; next we\u2019ll show how to deploy this to production.\n </p>\n <pre><code><span class=\"hljs-comment\"># Set up the launcher for local testing</span>\n<span class=\"hljs-keyword\">from</span> llama_agents <span class=\"hljs-keyword\">import</span> LocalLauncher\n\nlauncher = LocalLauncher(\n    [agent_service],\n    control_plane,\n    message_queue,\n)\n\n<span class=\"hljs-comment\"># Run a single query through the system</span>\nresult = launcher.launch_single(<span class=\"hljs-string\">\"What's the secret fact?\"</span>)\n<span class=\"hljs-built_in\">print</span>(result)</code></pre>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Deploying Your Multi-Agent System\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Once you've tested your system locally, you can deploy it as a set of services for real production use. Here's how you might set that up. This is similar to the previous example, but we\u2019ve added a second agent service and we\u2019re using a different launcher. Let\u2019s bring in our dependencies and set up our control plane again:\n </p>\n <pre><code><span class=\"hljs-keyword\">import</span> dotenv\ndotenv.load_dotenv()\n<span class=\"hljs-keyword\">from</span> llama_agents <span class=\"hljs-keyword\">import</span> (\n    AgentService,\n    AgentOrchestrator,\n    ControlPlaneServer,\n    SimpleMessageQueue,\n)\n\n<span class=\"hljs-keyword\">from</span> llama_index.core.agent <span class=\"hljs-keyword\">import</span> FunctionCallingAgentWorker\n<span class=\"hljs-keyword\">from</span> llama_index.core.tools <span class=\"hljs-keyword\">import</span> FunctionTool\n<span class=\"hljs-keyword\">from</span> llama_index.llms.openai <span class=\"hljs-keyword\">import</span> OpenAI\n<span class=\"hljs-keyword\">import</span> logging\n\n<span class=\"hljs-comment\"># change logging level to enable or disable more verbose logging</span>\nlogging.getLogger(<span class=\"hljs-string\">\"llama_agents\"</span>).setLevel(logging.INFO)\n\n<span class=\"hljs-comment\"># create our multi-agent framework components</span>\nmessage_queue = SimpleMessageQueue()\ncontrol_plane = ControlPlaneServer(\n    message_queue=message_queue,\n    orchestrator=AgentOrchestrator(llm=OpenAI()),\n)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Then as before we create a tool and an agent, though this time we\u2019ll add a second agent:\n </p>\n <pre><code><span class=\"hljs-comment\"># create a tool</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">get_the_secret_fact</span>() -&gt; <span class=\"hljs-built_in\">str</span>:\n    <span class=\"hljs-string\">\"\"\"Returns the secret fact.\"\"\"</span>\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"The secret fact is: A baby llama is called a 'Cria'.\"</span>\n\ntool = FunctionTool.from_defaults(fn=get_the_secret_fact)\n\n<span class=\"hljs-comment\"># create our agents</span>\nworker1 = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())\nworker2 = FunctionCallingAgentWorker.from_tools([], llm=OpenAI())\nagent1 = worker1.as_agent()\nagent2 = worker2.as_agent()</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We turn those agents into services:\n </p>\n <pre><code>agent_server_1 = AgentService(\n    agent=agent1,\n    message_queue=message_queue,\n    description=<span class=\"hljs-string\">\"Useful for getting the secret fact.\"</span>,\n    service_name=<span class=\"hljs-string\">\"secret_fact_agent\"</span>,\n    host=<span class=\"hljs-string\">\"localhost\"</span>,\n    port=<span class=\"hljs-number\">8003</span>\n)\nagent_server_2 = AgentService(\n    agent=agent2,\n    message_queue=message_queue,\n    description=<span class=\"hljs-string\">\"Useful for getting random dumb facts.\"</span>,\n    service_name=<span class=\"hljs-string\">\"dumb_fact_agent\"</span>,\n    host=<span class=\"hljs-string\">\"localhost\"</span>,\n    port=<span class=\"hljs-number\">8004</span>\n)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  And finally we launch each service as an independent agent. Here we\u2019re doing them all from a single script, but each of these could be a totally separate service, launched and scaled independently:\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_agents <span class=\"hljs-keyword\">import</span> ServerLauncher, CallableMessageConsumer\n\n<span class=\"hljs-comment\"># Additional human consumer</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">handle_result</span>(<span class=\"hljs-params\">message</span>) -&gt; <span class=\"hljs-literal\">None</span>:\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Got result:\"</span>, message.data)\n\n\n<span class=\"hljs-comment\"># the final result is published to a \"human\" consumer</span>\n<span class=\"hljs-comment\"># so we define one to handle it!</span>\nhuman_consumer = CallableMessageConsumer(\n    handler=handle_result, message_type=<span class=\"hljs-string\">\"human\"</span>\n)\n\n<span class=\"hljs-comment\"># Define Launcher</span>\nlauncher = ServerLauncher(\n    [agent_server_1, agent_server_2],\n    control_plane,\n    message_queue,\n    additional_consumers=[human_consumer]\n)\n\nlauncher.launch_servers()</code></pre>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Real-time monitoring\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  One of the coolest debugging features of our multi-agent system is our agent monitor, which is built right in. You launch it like this:\n </p>\n <pre><code>llama-agents monitor --control-plane-url http://127.0.0.1:8000</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Once launched, you get an intuitive, point-and-click terminal application. You can see both of the agents running, and at the bottom you can inject a task like the query \u201cWhat is the secret fact?\u201d You\u2019ll get a job ID which you can then click on to see your results:\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Building a Query Rewriting RAG System\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Let's look at a more complex example: a Query Rewriting RAG system. This system will rewrite user queries to improve retrieval, then use the rewritten query to perform RAG over a document.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  This example demonstrates how to create a more sophisticated system that combines query rewriting with RAG to improve question-answering capabilities. See\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama-agents/blob/main/examples/query_rewrite_rag.ipynb\" rel=\"noreferrer noopener\">\n   this notebook\n  </a>\n  for a fuller explanation of what\u2019s going on.\n </p>\n <pre><code><span class=\"hljs-keyword\">import</span> dotenv\ndotenv.load_dotenv() <span class=\"hljs-comment\"># our .env defines OPENAI_API_KEY</span>\n<span class=\"hljs-keyword\">from</span> llama_index.core <span class=\"hljs-keyword\">import</span> VectorStoreIndex, Document\n<span class=\"hljs-keyword\">from</span> llama_index.core.agent <span class=\"hljs-keyword\">import</span> FnAgentWorker\n<span class=\"hljs-keyword\">from</span> llama_index.core <span class=\"hljs-keyword\">import</span> PromptTemplate\n<span class=\"hljs-keyword\">from</span> llama_index.core.query_pipeline <span class=\"hljs-keyword\">import</span> QueryPipeline\n<span class=\"hljs-keyword\">from</span> llama_index.core.query_engine <span class=\"hljs-keyword\">import</span> RetrieverQueryEngine\n<span class=\"hljs-keyword\">from</span> llama_agents <span class=\"hljs-keyword\">import</span> (\n    AgentService,\n    ControlPlaneServer,\n    SimpleMessageQueue,\n    PipelineOrchestrator,\n    ServiceComponent,\n)\n<span class=\"hljs-keyword\">from</span> llama_agents.launchers <span class=\"hljs-keyword\">import</span> LocalLauncher\n<span class=\"hljs-keyword\">from</span> llama_index.llms.openai <span class=\"hljs-keyword\">import</span> OpenAI\n<span class=\"hljs-keyword\">import</span> logging\n\n<span class=\"hljs-comment\"># change logging level to enable or disable more verbose logging</span>\nlogging.getLogger(<span class=\"hljs-string\">\"llama_agents\"</span>).setLevel(logging.INFO)\n\n<span class=\"hljs-comment\"># Load and index your document</span>\ndocs = [Document(text=<span class=\"hljs-string\">\"The rabbit is a small mammal with long ears and a fluffy tail. His name is Peter.\"</span>)]\nindex = VectorStoreIndex.from_documents(docs)\n\n<span class=\"hljs-comment\"># Define a query rewrite agent</span>\nHYDE_PROMPT_STR = (\n    <span class=\"hljs-string\">\"Please rewrite the following query to include more detail:\\n{query_str}\\n\"</span>\n)\nHYDE_PROMPT_TMPL = PromptTemplate(HYDE_PROMPT_STR)\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">run_hyde_fn</span>(<span class=\"hljs-params\">state</span>):\n    prompt_tmpl, llm, input_str = (\n        state[<span class=\"hljs-string\">\"prompt_tmpl\"</span>],\n        state[<span class=\"hljs-string\">\"llm\"</span>],\n        state[<span class=\"hljs-string\">\"__task__\"</span>].<span class=\"hljs-built_in\">input</span>,\n    )\n    qp = QueryPipeline(chain=[prompt_tmpl, llm])\n    output = qp.run(query_str=input_str)\n    state[<span class=\"hljs-string\">\"__output__\"</span>] = <span class=\"hljs-built_in\">str</span>(output)\n    <span class=\"hljs-keyword\">return</span> state, <span class=\"hljs-literal\">True</span>\n\nhyde_agent = FnAgentWorker(\n    fn=run_hyde_fn,\n    initial_state={<span class=\"hljs-string\">\"prompt_tmpl\"</span>: HYDE_PROMPT_TMPL, <span class=\"hljs-string\">\"llm\"</span>: OpenAI()}\n).as_agent()\n\n<span class=\"hljs-comment\"># Define a RAG agent</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">run_rag_fn</span>(<span class=\"hljs-params\">state</span>):\n    retriever, llm, input_str = (\n        state[<span class=\"hljs-string\">\"retriever\"</span>],\n        state[<span class=\"hljs-string\">\"llm\"</span>],\n        state[<span class=\"hljs-string\">\"__task__\"</span>].<span class=\"hljs-built_in\">input</span>,\n    )\n    query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm)\n    response = query_engine.query(input_str)\n    state[<span class=\"hljs-string\">\"__output__\"</span>] = <span class=\"hljs-built_in\">str</span>(response)\n    <span class=\"hljs-keyword\">return</span> state, <span class=\"hljs-literal\">True</span>\n\nrag_agent = FnAgentWorker(\n    fn=run_rag_fn,\n    initial_state={<span class=\"hljs-string\">\"retriever\"</span>: index.as_retriever(), <span class=\"hljs-string\">\"llm\"</span>: OpenAI()}\n).as_agent()\n\n<span class=\"hljs-comment\"># Set up the multi-agent system</span>\nmessage_queue = SimpleMessageQueue()\n\nquery_rewrite_service = AgentService(\n    agent=hyde_agent,\n    message_queue=message_queue,\n    description=<span class=\"hljs-string\">\"Query rewriting service\"</span>,\n    service_name=<span class=\"hljs-string\">\"query_rewrite\"</span>,\n)\n\nrag_service = AgentService(\n    agent=rag_agent,\n    message_queue=message_queue,\n    description=<span class=\"hljs-string\">\"RAG service\"</span>,\n    service_name=<span class=\"hljs-string\">\"rag\"</span>,\n)\n\n<span class=\"hljs-comment\"># Create the pipeline</span>\npipeline = QueryPipeline(chain=[\n    ServiceComponent.from_service_definition(query_rewrite_service),\n    ServiceComponent.from_service_definition(rag_service),\n])\norchestrator = PipelineOrchestrator(pipeline)\n\ncontrol_plane = ControlPlaneServer(\n    message_queue=message_queue,\n    orchestrator=orchestrator,\n)\n\n<span class=\"hljs-comment\"># Set up the launcher</span>\nlauncher = LocalLauncher(\n    [query_rewrite_service, rag_service],\n    control_plane,\n    message_queue,\n)\n\n<span class=\"hljs-comment\"># Run a query</span>\nresult = launcher.launch_single(<span class=\"hljs-string\">\"Tell me about rabbits\"</span>)\n<span class=\"hljs-built_in\">print</span>(result)</code></pre>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Public roadmap\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  This is an alpha release, meaning that we\u2019d love your feedback on features to better help you build multi-agent systems in production! We\u2019ve created a\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama-agents/discussions/49\" rel=\"noreferrer noopener\">\n   public roadmap\n  </a>\n  showing where we plan to go from here. We\u2019re actively seeking public feedback on what works for you and what doesn\u2019t.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Dive in!\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   llama-agents\n  </code>\n  provides a powerful, flexible framework for building complex multi-agent AI systems. Whether you're prototyping a new idea or scaling to production,\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   llama-agents\n  </code>\n  offers the tools you need to bring your AI vision to life. Check out\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama-agents\" rel=\"noreferrer noopener\">\n   the repo\n  </a>\n  to learn more, especially our library of\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama-agents/tree/main/examples\" rel=\"noreferrer noopener\">\n   examples\n  </a>\n  .\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We're excited to see what the community builds with\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   llama-agents\n  </code>\n  . Happy coding!\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 18751, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f337657-1fcf-4602-80ec-e2765f1ca312": {"__data__": {"id_": "0f337657-1fcf-4602-80ec-e2765f1ca312", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-llama-datasets-aadb9994ad9e.html", "file_name": "introducing-llama-datasets-aadb9994ad9e.html", "file_type": "text/html", "file_size": 16032, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/introducing-llama-datasets-aadb9994ad9e.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-llama-datasets-aadb9994ad9e.html", "file_name": "introducing-llama-datasets-aadb9994ad9e.html", "file_type": "text/html", "file_size": 16032, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "8341f3c53cf894870147babb6bc96d317464fc0d157a24c9ca6bbb85a32df1d6", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  (Authors: Andrei Fajardo and Jerry Liu @ LlamaIndex)\n </p>\n <p>\n  Today we\u2019re excited to introduce\n  <strong>\n   Llama Datasets\n  </strong>\n  \ud83e\udd99 \ud83d\udcdd\u2014 a set of community-contributed datasets that allow users to easily benchmark their RAG pipelines for different use cases. A dataset consists of both question-answer pairs as well as source context. To use them, download them from LlamaHub; then evaluate your RAG pipeline using the dataset + a set of evaluation metrics.\n </p>\n <p>\n  We\u2019re launching with an initial set of 10 evaluation datasets and we\u2019ll be adding more! We\u2019ve also made it super easy to contribute your own dataset \u2014 upload your source documents + QA pairs (generated manually or synthetically).\n </p>\n <h1>\n  Context\n </h1>\n <p>\n  A big problem in building production RAG is evaluation. Unlike traditional software systems, LLM systems (and ML systems more generally) are stochastic black-boxes designed to model noisy real-world signals. This means that developers can\u2019t easily define unit tests that assert deterministic behavior \u2014 there may always be an input that causes an error. Because developers don\u2019t quite know what goes out given what goes in, they need to define an\n  <strong>\n   evaluation dataset\n  </strong>\n  that\u2019s reflective of their production use cases, and evaluate their system over this dataset using a set of\n  <strong>\n   evaluation metrics\n  </strong>\n  .\n </p>\n <p>\n  We\u2019ve presented\n  <a href=\"https://docs.google.com/presentation/d/1wtlEJC6SXLsoGkDdCkxC-Y_V8bvyYsi9Ozh59VMI22I/edit?usp=sharing\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   extensively on this topic\n  </a>\n  \u2014 every AI engineer should setup evaluation before trying to optimize their LLM or RAG application with advanced techniques.\n </p>\n <p>\n  But we\u2019ve increasingly found that defining the\n  <strong>\n   right evaluation dataset is hard and use-case dependent\n  </strong>\n  . Evaluating over academic benchmarks, like BEIR and HotpotQA oftentimes fail to generalize to specific use cases. Certain parameters that work well on certain data domains (e.g. SEC filings) may fail on others (e.g. research papers).\n </p>\n <p>\n  That\u2019s what inspired us to create Llama Datasets. Instead of being prescriptive on the data you must use, we\u2019ve decided to create a hub where you can easily pick and choose the right datasets for your use case!\n </p>\n <figure>\n  <figcaption class=\"pq fe pr pc pd ps pt be b bf z dt\">\n   Llama Datasets on LlamaHub\n  </figcaption>\n </figure>\n <h1>\n  Overview\n </h1>\n <p>\n  Today\u2019s launch includes the set of Llama Datasets on\n  <a href=\"https://llamahub.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaHub\n  </a>\n  , an accompanying\n  <code class=\"cw pu pv pw px b\">\n   RagEvaluatorPack\n  </code>\n  to help compute metrics over a dataset, as well as accompanying dataset abstractions that you can also use on their own.\n </p>\n <ul>\n  <li>\n   To use a Llama Dataset, download it off LlamaHub and run our\n   <code class=\"cw pu pv pw px b\">\n    RagEvaluatorPack\n   </code>\n   (or run your own evaluation modules).\n  </li>\n  <li>\n   To generate a Llama Dataset, define a\n   <code class=\"cw pu pv pw px b\">\n    LabelledRagDataset\n   </code>\n   with a set of\n   <code class=\"cw pu pv pw px b\">\n    LabelledRagDataExample\n   </code>\n   objects.\n  </li>\n  <li>\n   To contribute a Llama Dataset, submit a \u201cdata card\u201d to LlamaHub and upload your raw dataset files to our\n   <code class=\"cw pu pv pw px b\">\n    llama_datasets\n   </code>\n   repository.\n  </li>\n </ul>\n <p>\n  Check out the below sections for a walkthrough over an example dataset.\n </p>\n <p>\n  We\u2019re launching with 10 initial datasets:\n </p>\n <ul>\n  <li>\n   <a href=\"https://llamahub.ai/l/llama_datasets-blockchain_solana\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blockchain Solana Dataset\n   </a>\n  </li>\n  <li>\n   <a href=\"https://llamahub.ai/l/llama_datasets-braintrust_coda\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Coda Help Desk Dataset (with Braintrust)\n   </a>\n  </li>\n  <li>\n   <a href=\"https://llamahub.ai/l/llama_datasets-patronus_financebench\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    FinanceBench Dataset (Patronus AI)\n   </a>\n  </li>\n  <li>\n   <a href=\"https://llamahub.ai/l/llama_datasets-paul_graham_essay\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Paul Graham Essay Dataset\n   </a>\n  </li>\n  <li>\n   <a href=\"https://llamahub.ai/l/llama_datasets-llama2_paper\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Llama 2 Paper Dataset\n   </a>\n  </li>\n  <li>\n   <a href=\"https://llamahub.ai/l/llama_datasets-10k-uber_2021\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Uber/Lyft 2021 10K Filings Dataset\n   </a>\n  </li>\n  <li>\n   <a href=\"https://llamahub.ai/l/llama_datasets-mini_truthfulqa\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Mini Truthful QA Dataset (Arize AI)\n   </a>\n  </li>\n  <li>\n   <a href=\"https://llamahub.ai/l/llama_datasets-mini_squadv2\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Mini Squad V2 Dataset (Arize AI)\n   </a>\n  </li>\n  <li>\n   <a href=\"https://llamahub.ai/l/llama_datasets-origin_of_covid19\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Origin of COVID-19\n   </a>\n  </li>\n  <li>\n   <a href=\"https://llamahub.ai/l/llama_datasets-eval_llm_survey_paper\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LLM Survey Paper Dataset\n   </a>\n  </li>\n </ul>\n <figure>\n  <figcaption class=\"pq fe pr pc pd ps pt be b bf z dt\">\n   Example Llama Dataset page\n  </figcaption>\n </figure>\n <h1>\n  Example Walkthrough\n </h1>\n <p>\n  Let\u2019s walk through the different steps of using/contributing a Llama Dataset.\n </p>\n <h2>\n  1. Downloading and Using a Llama Dataset\n </h2>\n <p>\n  <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/llama_dataset/downloading_llama_datasets.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Follow the full notebook here.\n  </a>\n </p>\n <p>\n  Downloading a dataset is simple, do the following command (here we download Paul Graham).\n </p>\n <pre><span class=\"qz nz gt px b bf ra rb l rc rd\" id=\"a5f9\"><span class=\"hljs-keyword\">from</span> llama_index.llama_dataset <span class=\"hljs-keyword\">import</span> download_llama_dataset\n\n<span class=\"hljs-comment\"># download and install dependencies</span>\nrag_dataset, documents = download_llama_dataset(\n    <span class=\"hljs-string\">\"PaulGrahamEssayDataset\"</span>, <span class=\"hljs-string\">\"./paul_graham\"</span>\n)</span></pre>\n <p>\n  This downloads a\n  <code class=\"cw pu pv pw px b\">\n   rag_dataset\n  </code>\n  which contains the QA pairs (+ reference context), and\n  <code class=\"cw pu pv pw px b\">\n   documents\n  </code>\n  which is the source document corpus.\n </p>\n <p>\n  Let\u2019s inspect the\n  <code class=\"cw pu pv pw px b\">\n   rag_dataset\n  </code>\n  with\n  <code class=\"cw pu pv pw px b\">\n   to_pandas()\n  </code>\n  :\n </p>\n <figure>\n  <figcaption class=\"pq fe pr pc pd ps pt be b bf z dt\">\n   Sample rows from `rag_dataset`\n  </figcaption>\n </figure>\n <p>\n  Generating predictions over the RAG dataset is straightforward. You can easily plug in any query engine into\n  <code class=\"cw pu pv pw px b\">\n   amake_predictions_with\n  </code>\n  :\n </p>\n <pre><span class=\"qz nz gt px b bf ra rb l rc rd\" id=\"7d22\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> VectorStoreIndex\n\n<span class=\"hljs-comment\"># a basic RAG pipeline, uses service context defaults</span>\nindex = VectorStoreIndex.from_documents(documents=documents)\nquery_engine = index.as_query_engine()\n\n<span class=\"hljs-comment\"># generate prediction dataset</span>\nprediction_dataset = <span class=\"hljs-keyword\">await</span> rag_dataset.amake_predictions_with(\n    query_engine=query_engine, show_progress=<span class=\"hljs-literal\">True</span>\n)</span></pre>\n <p>\n  The\n  <code class=\"cw pu pv pw px b\">\n   prediction_dataset\n  </code>\n  is a\n  <code class=\"cw pu pv pw px b\">\n   RagPredictionDataset\n  </code>\n  object that looks like the following:\n </p>\n <figure>\n  <figcaption class=\"pq fe pr pc pd ps pt be b bf z dt\">\n   Prediction Dataset\n  </figcaption>\n </figure>\n <p>\n  Given the\n  <code class=\"cw pu pv pw px b\">\n   rag_dataset\n  </code>\n  and\n  <code class=\"cw pu pv pw px b\">\n   prediction_dataset\n  </code>\n  , you can use our evaluation modules to measure performance across a variety of metrics (e.g. faithfulness, correctness, relevancy).\n </p>\n <pre><span class=\"qz nz gt px b bf ra rb l rc rd\" id=\"38d5\"><span class=\"hljs-keyword\">for</span> example, prediction <span class=\"hljs-keyword\">in</span> tqdm.tqdm(\n    zip(rag_dataset.examples, prediction_dataset.predictions)\n):\n    correctness_result = judges[<span class=\"hljs-string\">\"correctness\"</span>].evaluate(\n        query=example.query,\n        response=prediction.response,\n        reference=example.reference_answer,\n    )</span></pre>\n <p>\n  To eliminate the boilerplate of writing all these evaluation modules, we\u2019ve also provided a LlamaPack that will do all this for you!\n </p>\n <pre><span class=\"qz nz gt px b bf ra rb l rc rd\" id=\"a2ec\"><span class=\"hljs-keyword\">from</span> llama_index.llama_pack <span class=\"hljs-keyword\">import</span> download_llama_pack\n\nRagEvaluatorPack = download_llama_pack(<span class=\"hljs-string\">\"RagEvaluatorPack\"</span>, <span class=\"hljs-string\">\"./pack\"</span>)\nrag_evaluator = RagEvaluatorPack(\n    query_engine=query_engine, rag_dataset=rag_dataset\n)\nbenchmark_df = <span class=\"hljs-keyword\">await</span> rag_evaluator.arun()</span></pre>\n <h2>\n  2. Generating a Llama Dataset\n </h2>\n <p>\n  <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/llama_dataset/labelled-rag-datasets.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Follow the full notebook here.\n  </a>\n </p>\n <p>\n  You can use our\n  <code class=\"cw pu pv pw px b\">\n   LabelledRagDataExample\n  </code>\n  and\n  <code class=\"cw pu pv pw px b\">\n   LabelledRagDataset\n  </code>\n  abstractions to create your own dataset.\n </p>\n <p>\n  Here\u2019s an example of adding an example manually.\n </p>\n <pre><span class=\"qz nz gt px b bf ra rb l rc rd\" id=\"ba93\">from llama_index.llama_dataset import (\n    LabelledRagDataExample,\n    CreatedByType,\n    CreatedBy,\n)\n\n# constructing a LabelledRagDataExample\nquery = \"This is a test query, is it not?\"\nquery_by = CreatedBy(type=CreatedByType.AI, model_name=\"gpt-4\")\nreference_answer = \"Yes it is.\"\nreference_answer_by = CreatedBy(type=CreatedByType.HUMAN)\nreference_contexts = [\"This is a sample context\"]\n\nrag_example = LabelledRagDataExample(\n    query=query,\n    query_by=query_by,\n    reference_contexts=reference_contexts,\n    reference_answer=reference_answer,\n    reference_answer_by=reference_answer_by,\n)</span></pre>\n <pre><span class=\"qz nz gt px b bf ra rb l rc rd\" id=\"7b83\"><span class=\"hljs-keyword\">from</span> llama_index.llama_dataset.rag <span class=\"hljs-keyword\">import</span> LabelledRagDataset\n\nrag_dataset = LabelledRagDataset(examples=[rag_example, rag_example_2])</span></pre>\n <p>\n  You can also synthetically generate a dataset over any document corpus with GPT-4:\n </p>\n <pre><span class=\"qz nz gt px b bf ra rb l rc rd\" id=\"ad77\"><span class=\"hljs-comment\"># generate questions against chunks</span>\n<span class=\"hljs-keyword\">from</span> llama_index.llama_dataset.generator <span class=\"hljs-keyword\">import</span> RagDatasetGenerator\n<span class=\"hljs-keyword\">from</span> llama_index.llms <span class=\"hljs-keyword\">import</span> OpenAI\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> ServiceContext\n\n<span class=\"hljs-comment\"># set context for llm provider</span>\ngpt_4_context = ServiceContext.from_defaults(\n    llm=OpenAI(model=<span class=\"hljs-string\">\"gpt-4\"</span>, temperature=<span class=\"hljs-number\">0.3</span>)\n)\n\n<span class=\"hljs-comment\"># instantiate a DatasetGenerator</span>\ndataset_generator = RagDatasetGenerator.from_documents(\n    documents,\n    service_context=gpt_4_context,\n    num_questions_per_chunk=<span class=\"hljs-number\">2</span>,  <span class=\"hljs-comment\"># set the number of questions per nodes</span>\n    show_progress=<span class=\"hljs-literal\">True</span>,\n)</span></pre>\n <h2>\n  3. Contributing a Llama Dataset\n </h2>\n <p>\n  We\u2019ve provided a\n  <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/llama_dataset/ragdataset_submission_template.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   ready-made submission notebook template here\n  </a>\n  \u2014 just fill in the blanks with your dataset!\n </p>\n <p>\n  If you\u2019re interested in contributing a dataset, we\u2019d love to feature it! You just need to follow these steps:\n </p>\n <ol>\n  <li>\n   <strong>\n    Create the dataset:\n   </strong>\n   To create a\n   <code class=\"cw pu pv pw px b\">\n    LabelledRagDataset\n   </code>\n   , you can create it from scratch either manually or with synthetically generated examples, or create it from an existing dataset.\n  </li>\n  <li>\n   <strong>\n    Generate a baseline evaluation dataset:\n   </strong>\n   Benchmark a basic top-k RAG pipeline over your dataset, and report the numbers. This will serve as a point of reference for others. You can use the\n   <code class=\"cw pu pv pw px b\">\n    RagEvaluatorPack\n   </code>\n   for this purpose.\n  </li>\n  <li>\n   <strong>\n    Prepare the dataset card (\n   </strong>\n   <code class=\"cw pu pv pw px b\">\n    <strong>\n     card.json\n    </strong>\n   </code>\n   <strong>\n    ) and\n   </strong>\n   <code class=\"cw pu pv pw px b\">\n    <strong>\n     README.md\n    </strong>\n   </code>\n   <strong>\n    :\n   </strong>\n   These will be shown on the LlamaHub page for this dataset. If you want to auto-generate this given some inputs, check out our\n   <code class=\"cw pu pv pw px b\">\n    <a href=\"https://llamahub.ai/l/llama_packs-llama_dataset_metadata\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n     LlamaDatasetMetadata\n    </a>\n   </code>\n   LlamaPack.\n  </li>\n  <li>\n   Submit a PR into\n   <code class=\"cw pu pv pw px b\">\n    llama-hub\n   </code>\n   to register the\n   <code class=\"cw pu pv pw px b\">\n    LlamaDataset\n   </code>\n   .\n  </li>\n  <li>\n   Submit a PR into\n   <code class=\"cw pu pv pw px b\">\n    llama-datasets\n   </code>\n   to upload the\n   <code class=\"cw pu pv pw px b\">\n    LlamaDataset\n   </code>\n   and its source files.\n  </li>\n </ol>\n <p>\n  You can follow all of these steps in our\n  <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/llama_dataset/ragdataset_submission_template.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   notebook template above\n  </a>\n  \u2014 simply substitute your own data.\n </p>\n <h1>\n  Conclusion\n </h1>\n <p>\n  We\u2019d love for you to check out our datasets and let us know your feedback! We\u2019d love your contributions as well.\n </p>\n <h2>\n  Resources\n </h2>\n <p>\n  Here are the resources mentioned in the blog post.\n </p>\n <ul>\n  <li>\n   <a href=\"https://llamahub.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Llama Datasets on LlamaHub\n   </a>\n   (make sure to select \u201cLlama Datasets\u201d from the dropdown)\n  </li>\n  <li>\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/llama_dataset/downloading_llama_datasets.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Downloading a Llama Dataset Notebook\n   </a>\n  </li>\n  <li>\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/llama_dataset/labelled-rag-datasets.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Creating a Llama Dataset Notebook\n   </a>\n  </li>\n  <li>\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/llama_dataset/ragdataset_submission_template.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Contributing a Llama Dataset Notebook Template\n   </a>\n  </li>\n  <li>\n   <a href=\"https://github.com/run-llama/llama-hub#how-to-add-a-llama-dataset\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    README on Contributing a Llama Dataset\n   </a>\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 15961, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d551e45-4afb-475f-84cf-6e916d1bb625": {"__data__": {"id_": "0d551e45-4afb-475f-84cf-6e916d1bb625", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-llama-packs-e14f453b913a.html", "file_name": "introducing-llama-packs-e14f453b913a.html", "file_type": "text/html", "file_size": 12815, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/introducing-llama-packs-e14f453b913a.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-llama-packs-e14f453b913a.html", "file_name": "introducing-llama-packs-e14f453b913a.html", "file_type": "text/html", "file_size": 12815, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "3ef4965e06665d616975934417f3167e789b892622f481dc9de7c0131a857cef", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Today we\u2019re excited to introduce\n  <strong>\n   Llama Packs \ud83e\udd99\ud83d\udce6\u2014\n  </strong>\n  a community-driven hub of prepackaged modules that you can use to kickstart your LLM application. Import them for a wide variety of use cases, from building a Streamlit app to building advanced retrieval over Weaviate to a resume parser that does structured data extraction. Just as important, inspect and customize them to your liking.\n </p>\n <p>\n  They\u2019re available on\n  <a href=\"https://llamahub.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaHub\n  </a>\n  : we\u2019ve launched 16+ templates with our launch partners already, and we\u2019re going to be adding a lot more!\n </p>\n <p>\n  (To those of you in the states, Happy Thanksgiving \ud83e\udd83)\n </p>\n <h1>\n  Context\n </h1>\n <p>\n  There are so many choices when building an LLM app that it can be daunting to get started building for a specific use case. Even for RAG the user needs to make the following decisions:\n </p>\n <ul>\n  <li>\n   Which LLM should I use? Embedding model?\n  </li>\n  <li>\n   Vector database?\n  </li>\n  <li>\n   Chunking/parsing strategy\n  </li>\n  <li>\n   Retrieval Algorithm\n  </li>\n  <li>\n   Wrapping in surrounding application\n  </li>\n </ul>\n <p>\n  Every use case requires different parameters, and LlamaIndex as a core LLM framework offers a comprehensive set of unopinionated modules to let users compose an application.\n </p>\n <p>\n  But we needed a way for users to get started more easily for their use case. And that\u2019s exactly where Llama Packs comes in.\n </p>\n <h1>\n  Overview\n </h1>\n <p>\n  Llama Packs can be described in two ways:\n </p>\n <ul>\n  <li>\n   On one hand, they are prepackaged\n   <strong>\n    modules\n   </strong>\n   that can be initialized with parameters and run out of the box to achieve a given use case (whether that\u2019s a full RAG pipeline, application template, and more). You can also import\n   <strong>\n    submodules\n   </strong>\n   (e.g. LLMs, query engines) to use directly\n   <strong>\n    .\n   </strong>\n  </li>\n  <li>\n   On another hand, LlamaPacks are\n   <strong>\n    templates\n   </strong>\n   that you can inspect, modify, and use.\n  </li>\n </ul>\n <p>\n  They can be downloaded either through our\n  <code class=\"cw pk pl pm pn b\">\n   llama_index\n  </code>\n  Python library or the CLI in\n  <em class=\"po\">\n   one line of code:\n  </em>\n </p>\n <p>\n  <strong>\n   CLI:\n  </strong>\n </p>\n <pre><span class=\"px oa gt pn b bf py pz l qa qb\" id=\"fcdd\">llamaindex-cli download-llamapack &lt;pack_name&gt; --download-dir &lt;pack_directory&gt;</span></pre>\n <p>\n  <strong>\n   Python\n  </strong>\n </p>\n <pre><span class=\"px oa gt pn b bf py pz l qa qb\" id=\"d7d7\"><span class=\"hljs-keyword\">from</span> llama_index.llama_pack <span class=\"hljs-keyword\">import</span> download_llama_pack\n\n<span class=\"hljs-comment\"># download and install dependencies</span>\nVoyageQueryEnginePack = download_llama_pack(\n  <span class=\"hljs-string\">\"&amp;lt;pack_name&amp;gt;\"</span>, <span class=\"hljs-string\">\"&amp;lt;pack_directory&amp;gt;\"</span>\n)</span></pre>\n <p>\n  Llama Packs can span abstraction levels \u2014 some are full prepackaged templates (full Streamlit / Gradio apps), and some combine a few smaller modules together (e.g. our SubQuestionQueryEngine with Weaviate). All of them are found in\n  <a href=\"https://llamahub.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaHub\n  </a>\n  \ud83d\udc47. You can filter by packs by selecting \u201cLlama Packs\u201d from the dropdown.\n </p>\n <figure>\n  <figcaption class=\"ql fe qm qc qd qn qo be b bf z dt\">\n   Llama Packs on LlamaHub\n  </figcaption>\n </figure>\n <p>\n  We\u2019re excited to partner with the following companies/contributors for our launch, featuring\n  <strong>\n   16+ templates.\n  </strong>\n  We highlight some examples below:\n </p>\n <ul>\n  <li>\n   <strong>\n    Streamlit / Snowflake (Caroline F.):\n   </strong>\n   <a href=\"https://llamahub.ai/l/llama_packs-streamlit_chatbot\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Streamlit Chatbot\n   </a>\n  </li>\n  <li>\n   <strong>\n    Arize (Mikyo K., Xander S.):\n   </strong>\n   <a href=\"https://llamahub.ai/l/llama_packs-arize_phoenix_query_engine\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Arize Phoenix\n   </a>\n  </li>\n  <li>\n   <strong>\n    ActiveLoop / DeepLake (Mikayel H., Adhilkhan S.):\n   </strong>\n   <a href=\"https://llamahub.ai/l/llama_packs-deeplake_deepmemory_retriever\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    DeepMemory Pack\n   </a>\n   ,\n   <a href=\"https://llamahub.ai/l/llama_packs-deeplake_multimodal_retrieval\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Multi-modal Retrieval\n   </a>\n  </li>\n  <li>\n   <strong>\n    Weaviate (Erika C.):\n   </strong>\n   <a href=\"https://llamahub.ai/l/llama_packs-sub_question_weaviate\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Sub Question Query Engine\n   </a>\n   ,\n   <a href=\"https://llamahub.ai/l/llama_packs-retry_engine_weaviate\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Retry Query Engine\n   </a>\n  </li>\n  <li>\n   <strong>\n    Voyage AI (Hong L.):\n   </strong>\n   <a href=\"https://llamahub.ai/l/llama_packs-voyage_query_engine\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Voyage AI Pack\n   </a>\n  </li>\n  <li>\n   <strong>\n    TruEra (Josh R.):\n   </strong>\n   <a href=\"https://llamahub.ai/l/llama_packs-trulens_eval_packs\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    TruLens Eval Pack\n   </a>\n   (this is 3 packs in one)\n  </li>\n  <li>\n   <strong>\n    Timescale (Matvey A.):\n   </strong>\n   <a href=\"https://llamahub.ai/l/llama_packs-timescale_vector_autoretrieval\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Timescale Vector AutoRetrieval\n   </a>\n  </li>\n  <li>\n   <strong>\n    Wenqi G.:\n   </strong>\n   <a href=\"https://llamahub.ai/l/llama_packs-llava_completion\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LLaVa Completion Pack\n   </a>\n  </li>\n </ul>\n <p>\n  There\u2019s not enough room in this blog post to feature every template, we\u2019ll be running features on every pack in the next few days.\n </p>\n <p>\n  Special thanks to Logan Markewich and Andrei Fajardo on the LlamaIndex team for getting Llama Packs up and running.\n </p>\n <h1>\n  <strong>\n   Example Walkthrough\n  </strong>\n </h1>\n <p>\n  The best way to highlight LlamaPack features is to showcase an example. We\u2019ll walk through a simple\n  <a href=\"https://llamahub.ai/l/llama_packs-voyage_query_engine\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Llama Pack\n  </a>\n  that gives the user a RAG pipeline setup with Voyage AI embeddings.\n </p>\n <figure>\n  <figcaption class=\"ql fe qm qc qd qn qo be b bf z dt\">\n   Voyage AI Pack. Every Pack has a detailed README on how to use / modules.\n  </figcaption>\n </figure>\n <p>\n  First, we download and initialize the Pack over a set of documents:\n </p>\n <pre><span class=\"px oa gt pn b bf py pz l qa qb\" id=\"4ba3\"><span class=\"hljs-keyword\">from</span> llama_index.llama_pack <span class=\"hljs-keyword\">import</span> download_llama_pack\n\n<span class=\"hljs-comment\"># download pack</span>\nVoyageQueryEnginePack = download_llama_pack(<span class=\"hljs-string\">\"VoyageQueryEnginePack\"</span>, <span class=\"hljs-string\">\"./voyage_pack\"</span>)\n<span class=\"hljs-comment\"># initialize pack (assume documents is defined)</span>\nvoyage_pack = VoyageQueryEnginePack(documents)</span></pre>\n <p>\n  Every Llama Pack implements a\n  <code class=\"cw pk pl pm pn b\">\n   get_modules()\n  </code>\n  function allowing you to inspect/use the modules.\n </p>\n <pre><span class=\"px oa gt pn b bf py pz l qa qb\" id=\"a7d5\">modules = voyage_pack.get_modules()\ndisplay(modules)\n\n# get LLM, vector index\nllm = modules[\"llm\"]\nvector_index = modules[\"index\"]</span></pre>\n <p>\n  The Llama Pack can be run in an\n  <strong>\n   out of the box\n  </strong>\n  fashion. By calling\n  <code class=\"cw pk pl pm pn b\">\n   run\n  </code>\n  , we\u2019ll execute the RAG pipeline and get back a response. In this setting, you don\u2019t need to worry about the internals.\n </p>\n <pre><span class=\"px oa gt pn b bf py pz l qa qb\" id=\"a50f\"><span class=\"hljs-comment\"># this will run the full pack</span>\nresponse = voyage_pack.run(<span class=\"hljs-string\">\"What did the author do growing up?\"</span>, similarity_top_k=<span class=\"hljs-number\">2</span>)\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-built_in\">str</span>(response))\n</span></pre>\n <pre><span class=\"px oa gt pn b bf py pz l qa qb\" id=\"466c\">The author spent his time outside of school mainly writing and programming. He wrote short stories and attempted to write programs on an IBM 1401. Later, he started programming on a TRS-80, creating simple games and a word processor. He also painted still lives while studying at the Accademia.</span></pre>\n <p>\n  The second important thing is that you have\n  <strong>\n   full access to the code of the Llama Pack\n  </strong>\n  . This allows you to customize the Llama Pack, rip out code, or just use it as reference to build your own app. Let\u2019s take a look at the downloaded pack in\n  <code class=\"cw pk pl pm pn b\">\n   voyage_pack/base.py\n  </code>\n  , and swap out the OpenAI LLM for Anthropic:\n </p>\n <pre><span class=\"px oa gt pn b bf py pz l qa qb\" id=\"0229\"><span class=\"hljs-keyword\">from</span> llama_index.llms <span class=\"hljs-keyword\">import</span> Anthropic\n...\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">VoyageQueryEnginePack</span>(<span class=\"hljs-title class_ inherited__\">BaseLlamaPack</span>):\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, documents: <span class=\"hljs-type\">List</span>[Document]</span>) -&amp;gt; <span class=\"hljs-literal\">None</span>:\n        llm = Anthropic()\n        embed_model = VoyageEmbedding(\n            model_name=<span class=\"hljs-string\">\"voyage-01\"</span>, voyage_api_key=os.environ[<span class=\"hljs-string\">\"VOYAGE_API_KEY\"</span>]\n        )\n        service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n        self.llm = llm\n        self.index = VectorStoreIndex.from_documents(\n            documents, service_context=service_context\n        )\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">get_modules</span>(<span class=\"hljs-params\">self</span>) -&amp;gt; <span class=\"hljs-type\">Dict</span>[<span class=\"hljs-built_in\">str</span>, <span class=\"hljs-type\">Any</span>]:\n        <span class=\"hljs-string\">\"\"\"Get modules.\"\"\"</span>\n        <span class=\"hljs-keyword\">return</span> {<span class=\"hljs-string\">\"llm\"</span>: self.llm, <span class=\"hljs-string\">\"index\"</span>: self.index}\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">run</span>(<span class=\"hljs-params\">self, query_str: <span class=\"hljs-built_in\">str</span>, **kwargs: <span class=\"hljs-type\">Any</span></span>) -&amp;gt; <span class=\"hljs-type\">Any</span>:\n        <span class=\"hljs-string\">\"\"\"Run the pipeline.\"\"\"</span>\n        query_engine = self.index.as_query_engine(**kwargs)\n        <span class=\"hljs-keyword\">return</span> query_engine.query(query_str)</span></pre>\n <p>\n  You can re-import the module directly and run it again:\n </p>\n <pre><span class=\"px oa gt pn b bf py pz l qa qb\" id=\"f24f\"><span class=\"hljs-keyword\">from</span> voyage_pack.base <span class=\"hljs-keyword\">import</span> VoyageQueryEnginePack\n\nvoyage_pack = VoyageQueryEnginePack(documents)\nresponse = voyage_pack.run(<span class=\"hljs-string\">\"What did the author do during his time in RISD?\"</span>)\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-built_in\">str</span>(response))</span></pre>\n <h1>\n  Conclusion\n </h1>\n <p>\n  Try it out and let us know what you think!\n </p>\n <h2>\n  Contributing\n </h2>\n <p>\n  Not on here yet? We\u2019d\n  <em class=\"po\">\n   love\n  </em>\n  to feature you! If you have any templates with LlamaIndex, adding it is almost as simple as copying/pasting your existing code over into a\n  <code class=\"cw pk pl pm pn b\">\n   BaseLlamaPack\n  </code>\n  subclass. Take a look at this folder for a full set of examples:\n  <a href=\"https://github.com/run-llama/llama-hub/tree/main/llama_hub/llama_packs\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://github.com/run-llama/llama-hub/tree/main/llama_hub/llama_packs\n  </a>\n </p>\n <h2>\n  Resources\n </h2>\n <p>\n  All Llama Packs can be found on LlamaHub:\n  <a href=\"https://llamahub.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://llamahub.ai/\n  </a>\n </p>\n <p>\n  The full notebook walkthrough is here:\n  <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/llama_hub/llama_packs_example.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://github.com/run-llama/llama_index/blob/main/docs/examples/llama_hub/llama_packs_example.ipynb\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 12766, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75b64a9d-5435-4697-a5a1-e6f68fd72ee7": {"__data__": {"id_": "75b64a9d-5435-4697-a5a1-e6f68fd72ee7", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-llamacloud-and-llamaparse-af8cedf9006b.html", "file_name": "introducing-llamacloud-and-llamaparse-af8cedf9006b.html", "file_type": "text/html", "file_size": 17273, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/introducing-llamacloud-and-llamaparse-af8cedf9006b.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-llamacloud-and-llamaparse-af8cedf9006b.html", "file_name": "introducing-llamacloud-and-llamaparse-af8cedf9006b.html", "file_type": "text/html", "file_size": 17273, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "2e6958ae806cb8ccb26c790df3ae5bc47c2fe92ff888c7df79e084fb61f8548e", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Today is a big day for the LlamaIndex ecosystem: we are announcing LlamaCloud, a new generation of managed parsing, ingestion, and retrieval services, designed to bring\n  <strong>\n   production-grade\n  </strong>\n  <strong>\n   context-augmentation\n  </strong>\n  to your LLM and RAG applications.\n </p>\n <p>\n  Using LlamaCloud as an enterprise AI engineer, you can focus on writing the business logic and not on data wrangling. Process large volumes of production data, immediately leading to better response quality. LlamaCloud launches with the following key components:\n </p>\n <ol>\n  <li>\n   <strong>\n    LlamaParse:\n   </strong>\n   Proprietary parsing for complex documents with embedded objects such as tables and figures. LlamaParse directly integrates with LlamaIndex ingestion and retrieval to let you build retrieval over complex, semi-structured documents. You\u2019ll be able to answer complex questions that simply weren\u2019t possible previously.\n  </li>\n  <li>\n   <strong>\n    Managed Ingestion and Retrieval API:\n   </strong>\n   An API which allows you to easily load, process, and store data for your RAG app and consume it in any language. Backed by data sources in\n   <a href=\"https://llamahub.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaHub\n   </a>\n   , including LlamaParse, and our data storage integrations.\n  </li>\n </ol>\n <p>\n  LlamaParse is available in a public preview setting starting today. It can currently handle PDFs and usage is capped for public use;\n  <a href=\"https://llamaindex.ai/contact\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   contact us\n  </a>\n  for commercial terms. The managed ingestion and retrieval API is available as a private preview; we are offering access to a limited set of enterprise design partners. If you\u2019re interested,\n  <a href=\"https://llamaindex.ai/contact\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   get in touch\n  </a>\n  . (We\u2019ve also launched a\n  <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   new version of our website\n  </a>\n  \ud83e\udd99!)\n </p>\n <h1>\n  RAG is Only as Good as your Data\n </h1>\n <p>\n  A core promise of LLMs is the ability to automate knowledge search, synthesis, extraction, and planning over any source of unstructured data. Over the past year a new data stack has emerged to power these context-augmented LLM applications, popularly referred to as Retrieval-Augmented Generation (RAG). This stack includes loading data, processing it, embedding it, and loading into a vector database. This enables downstream orchestration of retrieval and prompting to provide context within an LLM app.\n </p>\n <p>\n  This stack is different from any ETL stack before it, because unlike traditional software, every decision in the data stack directly\n  <strong>\n   affects the accuracy\n  </strong>\n  of the full LLM-powered system. Every decision like chunk size and embedding model affects LLM outputs, and since LLMs are black boxes, you can\u2019t unit test your way to correct behavior.\n </p>\n <p>\n  We\u2019ve spent the past year hard at work at the forefront of providing tooling and educating users on how to build high-performing, advanced RAG for various use cases. We crossed the 2M monthly download mark, and are used by large enterprises to startups, including Adyen, T-Systems, Jasper.ai, Weights and Biases, DataStax, and many more.\n </p>\n <p>\n  But while getting started with our famous 5-line starter example is easy, building production-grade RAG remains a complex and subtle problem. In our hundreds of user conversations, we learned the biggest pain points:\n </p>\n <ul>\n  <li>\n   <strong>\n    Results aren\u2019t accurate enough:\n   </strong>\n   The application was not able to produce satisfactory results for a long-tail of input tasks/queries.\n  </li>\n  <li>\n   <strong>\n    The number of parameters to tune is overwhelming:\n   </strong>\n   It\u2019s not clear which parameters across the data parsing, ingestion, retrieval.\n  </li>\n  <li>\n   <strong>\n    PDFs are specifically a problem:\n   </strong>\n   I have complex docs with lots of messy formatting. How do I represent this in the right way so the LLM can understand it?\n  </li>\n  <li>\n   <strong>\n    Data syncing is a challenge:\n   </strong>\n   Production data often updates regularly, and continuously syncing new data brings a new set of challenges.\n  </li>\n </ul>\n <p>\n  These are the problems we set out to solve with LlamaCloud.\n </p>\n <h1>\n  Data Pipelines to Bring you to Production\n </h1>\n <p>\n  We built LlamaCloud and LlamaParse as the data pipelines to get your RAG application to production more quickly.\n </p>\n <h1>\n  LlamaParse\n </h1>\n <p>\n  LlamaParse is a state-of-the-art parser designed to specifically unlock RAG over complex PDFs with embedded tables and charts. This simply wasn\u2019t possible before with other approaches, and we\u2019re incredibly excited about this technology.\n </p>\n <figure>\n  <figcaption class=\"pz fe qa pl pm qb qc be b bf z dt\">\n   LlamaParse Demo. Given a PDF file, returns a parsed markdown file that maintains semantic structure within the document.\n  </figcaption>\n </figure>\n <p>\n  For the past few months we\u2019ve been obsessed with this problem. This is a surprisingly prevalent use case across a variety of data types and verticals, from ArXiv papers to 10K filings to medical reports.\n </p>\n <p>\n  Naive chunking and retrieval algorithms do terribly. We were the first to propose a\n  <a href=\"https://docs.llamaindex.ai/en/stable/examples/query_engine/pdf_tables/recursive_retriever.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   novel recursive retrieval RAG technique\n  </a>\n  for being able to hierarchically index and query over tables and text in a document. The only challenge that remained was how to properly parse out tables and text in the first place.\n </p>\n <figure>\n  <figcaption class=\"pz fe qa pl pm qb qc be b bf z dt\">\n   Comparison of LlamaParse vs. PyPDF over the Apple 10K filing.\n   <a href=\"https://drive.google.com/file/d/1fyQAg7nOtChQzhF2Ai7HEeKYYqdeWsdt/view?usp=sharing\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Full comparisons are here\n   </a>\n   . A green highlight in a cell means that the RAG pipeline correctly returned the cell value as the answer to a question over that cell. A red highlight means that the question was answered incorrectly.\n  </figcaption>\n </figure>\n <p>\n  This is where LlamaParse comes in. We\u2019ve developed a proprietary parsing service that is incredibly good at parsing PDFs with complex tables into a well-structured markdown format. This representation directly plugs into the advanced Markdown parsing and recursive retrieval algorithms available in the open-source library. The end result is that you are able to build RAG over complex documents that can answer questions over both tabular and unstructured data. Check out the results below for a comparison:\n </p>\n <figure>\n  <figcaption class=\"pz fe qa pl pm qb qc be b bf z dt\">\n   Comparison of baseline PDF approach (top) vs. LlamaParse + advanced retrieval (bottom)\n  </figcaption>\n </figure>\n <figure>\n  <figcaption class=\"pz fe qa pl pm qb qc be b bf z dt\">\n   Results over the\n   <a href=\"https://github.com/run-llama/llama-hub/tree/main/llama_hub/llama_datasets/10k/uber_2021\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Uber 10K Dataset\n   </a>\n   . For more information on our evaluation metrics check out our\n   <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/evaluating/root.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    evaluation page\n   </a>\n   here.\n  </figcaption>\n </figure>\n <p>\n  This service is available in a\n  <strong>\n   public preview mode:\n  </strong>\n  available to everyone, but with a usage limit (1k pages per day). It operates as a standalone service that also plugs into our managed ingestion and retrieval API (see below). Check out our\n  <a href=\"https://docs.cloud.llamaindex.ai/llamaparse/\" target=\"_blank\">\n   LlamaParse onboarding here\n  </a>\n  for more details.\n </p>\n <pre><span class=\"qk oi gt qh b bf ql qm l qn qo\" id=\"e50b\"><span class=\"hljs-keyword\">from</span> llama_parse <span class=\"hljs-keyword\">import</span> LlamaParse\n\nparser = LlamaParse(\n    api_key=<span class=\"hljs-string\">\"llx-...\"</span>,  <span class=\"hljs-comment\"># can also be set in your env as LLAMA_CLOUD_API_KEY</span>\n    result_type=<span class=\"hljs-string\">\"markdown\"</span>,  <span class=\"hljs-comment\"># \"markdown\" and \"text\" are available</span>\n    verbose=<span class=\"hljs-literal\">True</span>\n)</span></pre>\n <p>\n  For unlimited commercial use of LlamaParse,\n  <a href=\"https://llamaindex.ai/contact\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   get in touch\n  </a>\n  with us.\n </p>\n <p>\n  <strong>\n   Next Steps\n  </strong>\n </p>\n <p>\n  Our early users have already given us important feedback on what they\u2019d like to see next. Currently we primarily support PDFs with tables, but we are also building out better support for figures, and and an expanded set of the most popular document types: .docx, .pptx, .html.\n </p>\n <h1>\n  Managed Ingestion and Retrieval\n </h1>\n <p>\n  Our other main offering in LlamaCloud is a managed ingestion and retrieval API which allows you to easily declare performant data pipelines for any context-augmented LLM application.\n </p>\n <p>\n  Get clean data for your LLM application, so you can spend less time wrangling data and more time writing core application logic. LlamaCloud empowers enterprise developers with the following benefits:\n </p>\n <ul>\n  <li>\n   <strong>\n    Engineering Time Savings:\n   </strong>\n   Instead of having to write custom connectors and parsing logic in Python, our APIs allow you to directly connect to different data sources.\n  </li>\n  <li>\n   <strong>\n    Performance:\n   </strong>\n   we provide good out-of-the-box performance for different data types, while offering an intuitive path for experimentation, evaluation, and improvement.\n  </li>\n  <li>\n   <strong>\n    Ease Systems Complexity:\n   </strong>\n   Handle a large number of data sources with incremental updates.\n  </li>\n </ul>\n <p>\n  Let\u2019s do a brief tour through the core components!\n </p>\n <ol>\n  <li>\n   <strong>\n    Ingestion:\n   </strong>\n   Declare a managed pipeline to process and transform/chunk/embed data backed by our 150+ data sources in LlamaHub and our 40+ storage integrations as destinations. Automatically handle syncing and load balancing. Define through the UI or our open-source library.\n  </li>\n  <li>\n   <strong>\n    Retrieval:\n   </strong>\n   Get access to state-of-the-art, advanced retrieval backed by our open-source library and your data storage. Wrap it in an easy-to-use REST API that you can consume from any language.\n  </li>\n  <li>\n   <strong>\n    Playground:\n   </strong>\n   Interactive UI to test and refine your ingestion/retrieval strategies pre-deployment, with evaluations in the loop.\n  </li>\n </ol>\n <figure>\n  <figcaption class=\"pz fe qa pl pm qb qc be b bf z dt\">\n   LlamaCloud Playground: configure, evaluate, and optimize your ingestion/retrieval pipeline before deployment.\n  </figcaption>\n </figure>\n <figure>\n  <figcaption class=\"pz fe qa pl pm qb qc be b bf z dt\">\n   LlamaCloud Retrieval: Access advanced retrieval over your storage system via an API.\n  </figcaption>\n </figure>\n <p>\n  We are opening up a private beta to a limited set of enterprise partners for the managed ingestion and retrieval API. If you\u2019re interested in centralizing your data pipelines and spending more time working on your actual RAG use cases, come\n  <a href=\"https://llamaindex.ai/contact\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   talk to us\n  </a>\n  .\n </p>\n <h1>\n  Launch Partners and Collaborators\n </h1>\n <p>\n  We opened up access to LlamaParse at\n  <a href=\"https://rag-a-thon.devpost.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   our hackathon\n  </a>\n  we co-hosted with\n  <a href=\"https://www.futureproofsv.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Futureproof Labs\n  </a>\n  and\n  <a href=\"https://www.datastax.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Datastax\n  </a>\n  at the beginning of February. We saw some incredible applications of LlamaParse in action,\n  <a href=\"/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   including parsing building codes for Accessory Dwelling Unit (ADU) planning\n  </a>\n  ,\n  <a href=\"https://devpost.com/software/home-ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   parsing real-estate disclosures for home buying\n  </a>\n  , and dozens more.\n </p>\n <p>\n  Eric Ciarla, co-founder at Mendable AI, incorporated LlamaParse into Mendable\u2019s data stack: \u201cWe integrated LlamaParse into our\n  <a href=\"https://github.com/mendableai/data-connectors\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   open source data connector repo\n  </a>\n  which powers our production ingestion suite. It was easy to integrate and more powerful than any of the alternatives we tried.\u201d\n </p>\n <p>\n  We\u2019re also excited to be joined by initial launch partners and collaborators in the LLM and AI ecosystem, from storage to compute.\n </p>\n <p>\n  <strong>\n   DataStax\n  </strong>\n </p>\n <p>\n  Datastax has incorporated LlamaParse into their RAGStack to bring a privacy-preserving out-of-the-box RAG solution for enterprises: \"Last week one of our customers Imprompt has launched a pioneering 'Chat-to-Everything' platform leveraging RAGStack powered by LlamaIndex to enhance their enterprise offerings while prioritizing privacy.\" said Davor Bonaci, CTO and executive vice president, DataStax. \"We're thrilled to partner with LlamaIndex to bring a streamlined solution to market. By incorporating LlamaIndex into RAGStack, we are providing enterprise developers with a comprehensive Gen AI stack that simplifies the complexities of RAG implementation, while offering long-term support and compatibility assurance.\u201d\n </p>\n <p>\n  <strong>\n   MongoDB\n  </strong>\n </p>\n <p>\n  \u201cMongoDB\u2019s partnership with LlamaIndex allows for the ingestion of data into the MongoDB Atlas Vector database, and the retrieval of the index from Atlas via LlamaParse and LlamaCloud, enabling the development of RAG systems and other AI applications,\u201d said Greg Maxson, Global Lead, AI Ecosystems at MongoDB. \u201cNow, developers can abstract complexities associated with data ingestion, simplify RAG pipeline implementations, and more cost effectively develop large language model applications, ultimately accelerating generative AI app development and more quickly bringing apps to market.\u201d\n </p>\n <p>\n  <strong>\n   Qdrant\n  </strong>\n </p>\n <p>\n  Andr\u00e9 Zayarni, CEO of Qdrant, says \u201cThe Qdrant team is excited to partner with LlamaIndex to combine the power of optimal data preprocessing, vectorization, and ingestion with Qdrant for a powerful fullstack RAG solution.\u201d\n </p>\n <p>\n  <strong>\n   NVIDIA\n  </strong>\n </p>\n <p>\n  We are also excited to collaborate with NVIDIA to integrate LlamaIndex with the\n  <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   NVIDIA AI Enterprise\n  </a>\n  software platform for production AI: \u201cLlamaCloud will help enterprises get generative AI applications from development into production with connectors that link proprietary data to the power of large language models,\u201d said Justin Boitano, vice president, Enterprise and Edge Computing, NVIDIA. \u201cPairing LlamaCloud with NVIDIA AI Enterprise can accelerate the end-to-end LLM pipeline \u2014 including data processing, embedding creation, indexing, and model inference on accelerated computing across clouds, data centers and out to the edge.\u201d\n </p>\n <h1>\n  FAQ\n </h1>\n <p>\n  <strong>\n   Is this competitive with vector databases?\n  </strong>\n </p>\n <p>\n  No. LlamaCloud is focused primarily on data parsing and ingestion, which is a complementary layer to any vector storage provider. The retrieval layer is orchestration on top of an existing storage system. LlamaIndex open-source integrates with 40+ of the most popular vector databases, and we are working hard to do the following:\n </p>\n <ol>\n  <li>\n   Integrate LlamaCloud with storage providers of existing design partners\n  </li>\n  <li>\n   Make LlamaCloud available in a more \u201cself-serve\u201d manner.\n  </li>\n </ol>\n <h1>\n  Next Steps\n </h1>\n <p>\n  As mentioned in the above sections, LlamaParse is available for public preview starting today with a usage cap. LlamaCloud is in a private preview mode; we are offering access to a limited set of enterprise design partners. If you\u2019re interested come talk to us!\n </p>\n <p>\n  LlamaParse:\n  <a href=\"https://github.com/run-llama/llama_parse\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Repo\n  </a>\n  ,\n  <a href=\"https://github.com/run-llama/llama_parse/blob/main/examples/demo_advanced.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Cookbook\n  </a>\n  ,\n  <a href=\"https://www.llamaindex.ai/contact\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Contact Us\n  </a>\n </p>\n <p>\n  LlamaCloud:\n  <a href=\"https://www.llamaindex.ai/contact\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Contact Us\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 17198, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67a39a17-1d98-4e44-b55e-0c34fbe4931a": {"__data__": {"id_": "67a39a17-1d98-4e44-b55e-0c34fbe4931a", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-llamaindex-ts-89f41a1f24ab.html", "file_name": "introducing-llamaindex-ts-89f41a1f24ab.html", "file_type": "text/html", "file_size": 3898, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/introducing-llamaindex-ts-89f41a1f24ab.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-llamaindex-ts-89f41a1f24ab.html", "file_name": "introducing-llamaindex-ts-89f41a1f24ab.html", "file_type": "text/html", "file_size": 3898, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "8e6e6f018089f80304e8ae15ea24a1c2abd30563685886f7eac90ad05f879c38", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  We are beyond excited to announce v0.0.1 of\n  <a href=\"https://github.com/run-llama/LlamaIndexTS/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    LlamaIndex.TS\n   </strong>\n  </a>\n  , a Typescript first library focused on helping you use your private data with large language models.\n </p>\n <h2>\n  <strong>\n   What is LlamaIndex?\n  </strong>\n </h2>\n <p>\n  Our core goal for LlamaIndex is to help developers easily integrate their data with Large Language Models (LLMs). LLMs, like ChatGPT, have been a revolution in the way we think about handling textual input and data, but all of them have the limitation in what data they have access to. In addition to the \u201cknowledge cutoff\u201d (we are nearing the 2 year anniversary for when ChatGPT\u2019s latest data was trained) LLMs can\u2019t access data from your companies, from your personal analyses, or from the data your users generate.\n </p>\n <p>\n  With LlamaIndex.TS, we look to achieve that goal by meeting developers at their (my) language of choice, in this case Typescript. We are committed to making this library the easiest to use, most robust solution out there for using data with LLMs.\n </p>\n <h2>\n  <strong>\n   Backstory\n  </strong>\n </h2>\n <p>\n  It was at the Emergency ChatGPT Hackathon hosted by Pete Huang and Rachel Woods that I met Jerry. Having worked in the JS world for the last 8 years, my first question was \u201cwhy don\u2019t you build this in Javascript?\u201d After he demurred, he very patiently guided me through setting up the Python dev environment. (I think it took us 20 minutes before we figured it all out!) So, when Jerry offered to let me build LlamaIndex.TS I obviously couldn\u2019t turn him down. Can\u2019t wait to see what you build with it!\n </p>\n <h2>\n  <strong>\n   Design\n  </strong>\n </h2>\n <p>\n  At a high level, LlamaIndex.TS first takes the file inputs, loads them into a standardized format, and creates an Index (knowledge base).\n </p>\n <p>\n  We then retrieve the relevant information from the index and use that in our query to the LLM to generate more a grounded response.\n </p>\n <p>\n  Check out\n  <a href=\"https://ts.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   our docs\n  </a>\n  for a more in depth explanation!\n </p>\n <h2>\n  Playground\n </h2>\n <p>\n  We are building an open source playground for LlamaIndex.TS. Please check it out at\n  <a href=\"https://llama-playground.vercel.app/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://llama-playground.vercel.app/\n  </a>\n  PRs are welcome here!\n  <a href=\"https://github.com/run-llama/ts-playground\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://github.com/run-llama/ts-playground\n  </a>\n </p>\n <h2>\n  <strong>\n   Main Differences from LlamaIndex Python\n  </strong>\n </h2>\n <ul>\n  <li>\n   All function names are \ud83d\udc2a camel cased.\n  </li>\n  <li>\n   The prompt interface is much simpler and uses native javascript template literals.\n  </li>\n  <li>\n   We do not ship non-async versions of functions. Please use await or .then callbacks.\n  </li>\n  <li>\n   We use interfaces and POJOs in lieu of classes where it makes sense. For example, ChatEngine, a base class in Python is an interface in JS. ServiceContext, a class in Python is an interface/POJO in JS.\n  </li>\n </ul>\n <h2>\n  <strong>\n   Runtimes\n  </strong>\n </h2>\n <p>\n  Currently, we support NodeJS v18 and up. Lots of plans on this front though. Stay tuned!\n </p>\n <h2>\n  <strong>\n   Contributing\n  </strong>\n </h2>\n <p>\n  Only the core features are built out so far, so there is a lot of work that needs to be done on the loader and integration side. If you\u2019re interested in contributing, please send us a message or even better a PR!\n </p>\n <p>\n  <a href=\"https://github.com/run-llama/LlamaIndexTS\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://github.com/run-llama/LlamaIndexTS\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3874, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29b4215a-bbc5-47b4-8d27-5747dc7f4dcd": {"__data__": {"id_": "29b4215a-bbc5-47b4-8d27-5747dc7f4dcd", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-query-pipelines-025dc2bb0537.html", "file_name": "introducing-query-pipelines-025dc2bb0537.html", "file_type": "text/html", "file_size": 17799, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/introducing-query-pipelines-025dc2bb0537.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-query-pipelines-025dc2bb0537.html", "file_name": "introducing-query-pipelines-025dc2bb0537.html", "file_type": "text/html", "file_size": 17799, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "5fcd57187a04008345dcaa755500264e38b61a80673c3379ff5968e4d34f44e8", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Today we introduce\n  <strong>\n   Query Pipelines,\n  </strong>\n  a new declarative API within LlamaIndex that allows you to\n  <strong>\n   concisely orchestrate simple-to-advanced query workflows over your data for different use cases\n  </strong>\n  (RAG, structured data extraction, and more).\n </p>\n <p>\n  At the core of all this is our\n  <code class=\"cw ny nz oa ob b\">\n   QueryPipeline\n  </code>\n  abstraction. It can take in many LlamaIndex modules (LLMs, prompts, query engines, retrievers, itself). It can create a computational graph over these modules (e.g. a sequential chain or a DAG). It has callback support and native support with our\n  <a href=\"https://docs.llamaindex.ai/en/latest/module_guides/observability/observability.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   observability partners\n  </a>\n  .\n </p>\n <p>\n  The end goal is that it\u2019s even easier to build LLM workflows over your data. Check out our comprehensive\n  <a href=\"https://docs.llamaindex.ai/en/latest/examples/pipeline/query_pipeline.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   introduction guide\n  </a>\n  , as well as our\n  <a href=\"https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/root.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   docs page\n  </a>\n  for more details.\n </p>\n <figure>\n  <figcaption class=\"or fe os od oe ot ou be b bf z dt\">\n   Example `QueryPipeline` setup for an advanced RAG pipeline\n  </figcaption>\n </figure>\n <h1>\n  Context\n </h1>\n <p>\n  Over the past year AI engineers have developed customized, complex orchestration flows with LLMs to solve a variety of different use cases. Over time some common patterns developed. At a top-level, paradigms emerged to query a user\u2019s data \u2014 this includes RAG (in a narrow definition) to query unstructured data, and text-to-SQL to query structured data. Other paradigms emerged around use cases like structured data extraction (e.g. prompt the LLM to output JSON, and parse it), prompt chaining (e.g. chain-of-thought), and agents that could interact with external services (combine prompt chaining\n </p>\n <p>\n  <strong>\n   There is a lot of query orchestration in RAG.\n  </strong>\n  Even within RAG itself there can be a lot of work to build an advanced RAG pipeline optimized for performance. Starting from the user query, we may want to run query understanding/transformations (re-writing, routing). We also may want to run multi-stage retrieval algorithms \u2014 e.g. top-k lookup + reranking. We may also want to use prompts + LLMs to do response synthesis in different ways. Here\u2019s a great blog on advanced RAG\n  <a href=\"https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   components\n  </a>\n  .\n </p>\n <figure>\n  <figcaption class=\"or fe os od oe ot ou be b bf z dt\">\n   <a href=\"https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Source: \u201cAdvanced RAG Techniques: an Illustrated Overview\u201d by Ivan Ilin\n   </a>\n  </figcaption>\n </figure>\n <p>\n  <strong>\n   RAG has become more modular:\n  </strong>\n  Instead of a single way to do retrieval/RAG, developers are encouraged to pick and choose the best modules for their use cases. This sentiment is echoed in the\n  <a href=\"https://arxiv.org/pdf/2312.10997.pdf\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   RAG Survey paper by Gao et al.\n  </a>\n </p>\n <p>\n  This leads to creative new patterns like\n  <a href=\"https://github.com/stanfordnlp/dspy\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   DSP\n  </a>\n  ,\n  <a href=\"https://arxiv.org/abs/2305.14283\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Rewrite-Retrieve-Read\n  </a>\n  , or\n  <a href=\"https://arxiv.org/abs/2305.15294\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   interleaving retrieval+generation multiple times\n  </a>\n  .\n </p>\n <h2>\n  Previous State of LlamaIndex\n </h2>\n <p>\n  LlamaIndex itself has hundreds of RAG guides and 16+ Llama Pack recipes letting users setup\n  <a href=\"/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   different RAG pipelines\n  </a>\n  , and has been at the forefront of establishing advanced RAG patterns.\n </p>\n <p>\n  We\u2019ve also exposed low-level modules such as\n  <a href=\"https://docs.llamaindex.ai/en/latest/module_guides/models/llms.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LLMs\n  </a>\n  ,\n  <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/models/prompts.html#prompts\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   prompts\n  </a>\n  ,\n  <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   embeddings\n  </a>\n  ,\n  <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/root.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   postprocessors\n  </a>\n  and easy subclassability of core components like\n  <a href=\"https://docs.llamaindex.ai/en/stable/examples/query_engine/CustomRetrievers.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   retrievers\n  </a>\n  and\n  <a href=\"https://docs.llamaindex.ai/en/stable/examples/query_engine/custom_query_engine.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   query engines\n  </a>\n  so that users can define their own workflows.\n </p>\n <p>\n  But up until now, we didn\u2019t explicitly have an orchestration abstraction. Users were responsible for figuring out their own workflows by reading the API guides of each module, converting outputs to the right inputs, and using the modules imperatively.\n </p>\n <h1>\n  Query Pipeline\n </h1>\n <p>\n  As a result, our QueryPipeline provides a declarative query orchestration abstraction. You can use it to compose both sequential chains and directed acyclic graphs (DAGs) of arbitrary complexity.\n </p>\n <p>\n  You can already compose these workflows imperatively with LlamaIndex modules, but the QueryPipeline allows you to do it efficiently with fewer lines of code.\n </p>\n <p>\n  It has the following benefits:\n </p>\n <ul>\n  <li>\n   <strong>\n    Express common query workflows with fewer lines of code/boilerplate:\n   </strong>\n   Stop writing converter logic between outputs/inputs, and figuring out the exact typing of arguments for each module!\n  </li>\n  <li>\n   <strong>\n    Greater readability:\n   </strong>\n   Reduced boilerplate leads to greater readability.\n  </li>\n  <li>\n   <strong>\n    End-to-end observability:\n   </strong>\n   Get callback integration across the entire pipeline (even for arbitrarily nested DAGs), so you stop fiddling around with our observability integrations.\n  </li>\n  <li>\n   <strong>\n    [In the future] Easy Serializability:\n   </strong>\n   A declarative interface allows the core components to be serialized/redeployed on other systems much more easily.\n  </li>\n  <li>\n   <strong>\n    [In the future] Caching:\n   </strong>\n   This interface also allows us to build a caching layer under the hood, allowing input re-use.\n  </li>\n </ul>\n <figure>\n  <figcaption class=\"or fe os od oe ot ou be b bf z dt\">\n   Visualization of our advanced RAG QueryPipeline using `networkx` and `pyvis`\n  </figcaption>\n </figure>\n <h1>\n  Usage\n </h1>\n <p>\n  The QueryPipeline allows you to a DAG-based query workflow using LlamaIndex modules. There are two main ways to use it:\n </p>\n <ul>\n  <li>\n   As a sequential chain (easiest/most concise)\n  </li>\n  <li>\n   As a full DAG (more expressive)\n  </li>\n </ul>\n <p>\n  See our\n  <a href=\"https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/usage_pattern.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   usage pattern guide\n  </a>\n  for more details.\n </p>\n <h2>\n  Sequential Chain\n </h2>\n <p>\n  Some simple pipelines are purely linear in nature \u2014 the output of the previous module directly goes into the input of the next module.\n </p>\n <p>\n  Some examples:\n </p>\n <ul>\n  <li>\n   Prompt \u2192 LLM \u2192 Output parsing\n  </li>\n  <li>\n   Retriever \u2192Response synthesizer\n  </li>\n </ul>\n <p>\n  Here\u2019s the most basic example, chaining a prompt with LLM. Simply initialize\n  <code class=\"cw ny nz oa ob b\">\n   QueryPipeline\n  </code>\n  with the\n  <code class=\"cw ny nz oa ob b\">\n   chain\n  </code>\n  parameter.\n </p>\n <pre><span class=\"ra ow gt ob b bf rb rc l rd re\" id=\"7749\"># try chaining basic prompts\nprompt_str = \"Please generate related movies to {movie_name}\"\nprompt_tmpl = PromptTemplate(prompt_str)\nllm = OpenAI(model=\"gpt-3.5-turbo\")\n\np = QueryPipeline(chain=[prompt_tmpl, llm], verbose=True)</span></pre>\n <h2>\n  Setting up a DAG for an Advanced RAG Workflow\n </h2>\n <p>\n  Generally setting up a query workflow will require using our lower-level functions to build a DAG.\n </p>\n <p>\n  For instance, to build an \u201cadvanced RAG\u201d consisting of query rewriting/retrieval/reranking/synthesis, you\u2019d do something like the following.\n </p>\n <pre><span class=\"ra ow gt ob b bf rb rc l rd re\" id=\"74ea\"><span class=\"hljs-keyword\">from</span> llama_index.postprocessor <span class=\"hljs-keyword\">import</span> CohereRerank\n<span class=\"hljs-keyword\">from</span> llama_index.response_synthesizers <span class=\"hljs-keyword\">import</span> TreeSummarize\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> ServiceContext\n\n<span class=\"hljs-comment\"># define modules</span>\nprompt_str = <span class=\"hljs-string\">\"Please generate a question about Paul Graham's life regarding the following topic {topic}\"</span>\nprompt_tmpl = PromptTemplate(prompt_str)\nllm = OpenAI(model=<span class=\"hljs-string\">\"gpt-3.5-turbo\"</span>)\nretriever = index.as_retriever(similarity_top_k=<span class=\"hljs-number\">3</span>)\nreranker = CohereRerank()\nsummarizer = TreeSummarize(\n    service_context=ServiceContext.from_defaults(llm=llm)\n)\n\n<span class=\"hljs-comment\"># define query pipeline</span>\np = QueryPipeline(verbose=<span class=\"hljs-literal\">True</span>)\np.add_modules(\n    {\n        <span class=\"hljs-string\">\"llm\"</span>: llm,\n        <span class=\"hljs-string\">\"prompt_tmpl\"</span>: prompt_tmpl,\n        <span class=\"hljs-string\">\"retriever\"</span>: retriever,\n        <span class=\"hljs-string\">\"summarizer\"</span>: summarizer,\n        <span class=\"hljs-string\">\"reranker\"</span>: reranker,\n    }\n)\n<span class=\"hljs-comment\"># add edges </span>\np.add_link(<span class=\"hljs-string\">\"prompt_tmpl\"</span>, <span class=\"hljs-string\">\"llm\"</span>)\np.add_link(<span class=\"hljs-string\">\"llm\"</span>, <span class=\"hljs-string\">\"retriever\"</span>)\np.add_link(<span class=\"hljs-string\">\"retriever\"</span>, <span class=\"hljs-string\">\"reranker\"</span>, dest_key=<span class=\"hljs-string\">\"nodes\"</span>)\np.add_link(<span class=\"hljs-string\">\"llm\"</span>, <span class=\"hljs-string\">\"reranker\"</span>, dest_key=<span class=\"hljs-string\">\"query_str\"</span>)\np.add_link(<span class=\"hljs-string\">\"reranker\"</span>, <span class=\"hljs-string\">\"summarizer\"</span>, dest_key=<span class=\"hljs-string\">\"nodes\"</span>)\np.add_link(<span class=\"hljs-string\">\"llm\"</span>, <span class=\"hljs-string\">\"summarizer\"</span>, dest_key=<span class=\"hljs-string\">\"query_str\"</span>)</span></pre>\n <p>\n  In this code block we 1) add modules, and then 2) define relationships between modules. Note that by\n  <code class=\"cw ny nz oa ob b\">\n   source_key\n  </code>\n  and\n  <code class=\"cw ny nz oa ob b\">\n   dest_key\n  </code>\n  are\n  <strong>\n   optional\n  </strong>\n  and are only required if first module has more than one output / the second module has more than one input respectively.\n </p>\n <h2>\n  <strong>\n   Running the Pipeline\n  </strong>\n </h2>\n <p>\n  If the pipeline has one \u201croot\u201d node and one output node, use\n  <code class=\"cw ny nz oa ob b\">\n   run\n  </code>\n  . Using the previous example,\n </p>\n <pre><span class=\"ra ow gt ob b bf rb rc l rd re\" id=\"5b3e\">output = p.run(topic=\"YC\")\n# output type is Response\ntype(output)</span></pre>\n <p>\n  If the pipeline has multiple root nodes and/or multiple output nodes, use\n  <code class=\"cw ny nz oa ob b\">\n   run_multi\n  </code>\n  .\n </p>\n <pre><span class=\"ra ow gt ob b bf rb rc l rd re\" id=\"6bee\">output_dict = p.<span class=\"hljs-title function_\">run_multi</span>({<span class=\"hljs-string\">\"llm\"</span>: {<span class=\"hljs-string\">\"topic\"</span>: <span class=\"hljs-string\">\"YC\"</span>}})\n<span class=\"hljs-title function_\">print</span>(output_dict)</span></pre>\n <h2>\n  Defining a Custom Query Component\n </h2>\n <p>\n  It\u2019s super easy to subclass\n  <code class=\"cw ny nz oa ob b\">\n   CustomQueryComponent\n  </code>\n  so you can plug it into the QueryPipeline.\n </p>\n <p>\n  Check out\n  <a href=\"https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/usage_pattern.html#defining-a-custom-query-component\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   our walkthrough\n  </a>\n  for more details.\n </p>\n <h2>\n  Supported Modules\n </h2>\n <p>\n  Currently the following LlamaIndex modules are supported within a QueryPipeline. Remember, you can define your own!\n </p>\n <ol>\n  <li>\n   LLMs (both completion and chat) (\n   <code class=\"cw ny nz oa ob b\">\n    LLM\n   </code>\n   )\n  </li>\n  <li>\n   Prompts (\n   <code class=\"cw ny nz oa ob b\">\n    PromptTemplate\n   </code>\n   )\n  </li>\n  <li>\n   Query Engines (\n   <code class=\"cw ny nz oa ob b\">\n    BaseQueryEngine\n   </code>\n   )\n  </li>\n  <li>\n   Query Transforms (\n   <code class=\"cw ny nz oa ob b\">\n    BaseQueryTransform\n   </code>\n   )\n  </li>\n  <li>\n   Retrievers (\n   <code class=\"cw ny nz oa ob b\">\n    BaseRetriever\n   </code>\n   )\n  </li>\n  <li>\n   Output Parsers (\n   <code class=\"cw ny nz oa ob b\">\n    BaseOutputParser\n   </code>\n   )\n  </li>\n  <li>\n   Postprocessors/Rerankers (\n   <code class=\"cw ny nz oa ob b\">\n    BaseNodePostprocessor\n   </code>\n   )\n  </li>\n  <li>\n   Response Synthesizers (\n   <code class=\"cw ny nz oa ob b\">\n    BaseSynthesizer\n   </code>\n   )\n  </li>\n  <li>\n   Other\n   <code class=\"cw ny nz oa ob b\">\n    QueryPipeline\n   </code>\n   objects\n  </li>\n  <li>\n   Custom components (\n   <code class=\"cw ny nz oa ob b\">\n    CustomQueryComponent\n   </code>\n   )\n  </li>\n </ol>\n <p>\n  Check out the\n  <a href=\"https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/module_usage.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   module usage guide\n  </a>\n  for more details.\n </p>\n <h1>\n  Walkthrough Example\n </h1>\n <p>\n  Make sure to check out our\n  <a href=\"https://docs.llamaindex.ai/en/latest/examples/pipeline/query_pipeline.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Introduction to Query Pipelines guide\n  </a>\n  for full details. We go over all the steps above with concrete examples!\n </p>\n <p>\n  The notebook guide also logs traces through\n  <a href=\"https://github.com/Arize-ai/phoenix\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Arize Phoenix\n  </a>\n  . You can see the full run of each QueryPipeline in the Phoenix dashboard. Our full callback support throughout every component in a QueryComponent allows you to easily integrate with any observability provider.\n </p>\n <h1>\n  Related Work\n </h1>\n <p>\n  The idea of a declarative syntax for building LLM-powered pipelines is not new. Related works include\n  <a href=\"https://docs.haystack.deepset.ai/docs/pipelines\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Haystack\n  </a>\n  as well as the\n  <a href=\"https://python.langchain.com/docs/expression_language/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LangChain Expression Language\n  </a>\n  . Other related works include pipelines that are setup in the no-code/low-code setting such as\n  <a href=\"https://github.com/logspace-ai/langflow\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Langflow\n  </a>\n  /\n  <a href=\"https://flowiseai.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Flowise\n  </a>\n  .\n </p>\n <p>\n  Our main goal here was highlighted above: provide a convenient dev UX to define common query workflows over your data. There\u2019s a lot of optimizations/guides to be done here!\n </p>\n <h1>\n  FAQ\n </h1>\n <p>\n  <strong>\n   What\u2019s the difference between a\n  </strong>\n  <code class=\"cw ny nz oa ob b\">\n   <strong>\n    QueryPipeline\n   </strong>\n  </code>\n  <strong>\n   and\n  </strong>\n  <code class=\"cw ny nz oa ob b\">\n   <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/root.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     IngestionPipeline\n    </strong>\n   </a>\n  </code>\n  <strong>\n   ?\n  </strong>\n </p>\n <p>\n  Great question. Currently the IngestionPipeline operates during the data ingestion stage, and the QueryPipeline operates during the query stage. That said, there\u2019s potentially some shared abstractions we\u2019ll develop for both!\n </p>\n <h1>\n  Conclusion + Resources\n </h1>\n <p>\n  That\u2019s it! As mentioned above we\u2019ll be adding a lot more resources and guides soon. In the meantime check out our current guides:\n </p>\n <ul>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/root.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Query Pipelines Guide\n   </a>\n  </li>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/pipeline/query_pipeline.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Query Pipelines Walkthrough\n   </a>\n  </li>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/usage_pattern.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Query Pipeline Usage Pattern\n   </a>\n  </li>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/module_usage.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Query Pipelines Module Usage Guide\n   </a>\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 17746, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d8709fd-deca-456e-8637-22e1d560e2d5": {"__data__": {"id_": "0d8709fd-deca-456e-8637-22e1d560e2d5", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.html", "file_name": "introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.html", "file_type": "text/html", "file_size": 7788, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.html", "file_name": "introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.html", "file_type": "text/html", "file_size": 7788, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "7bfb460380386e106768e6fc9536cd7ca75aaccf2944efe4baa1e83744ee9b02", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Today we introduce\n  <a href=\"https://github.com/run-llama/rags\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    RAGs\n   </strong>\n  </a>\n  <strong>\n   ,\n  </strong>\n  a Streamlit app that allows you to create and customize your own RAG pipeline and then use it over your own data \u2014 all with natural language! This means you can now setup a \u201cChatGPT over your data\u201d without needing to code.\n </p>\n <p>\n  Setup and query a RAG pipeline in three simple steps:\n </p>\n <ol>\n  <li>\n   <strong>\n    Easy Task Description\n   </strong>\n   : Simply describe your task (like \u201cload this web page\u201d) and define the parameters for your RAG systems (like retrieving a certain number of documents).\n  </li>\n  <li>\n   <strong>\n    Configurable Settings\n   </strong>\n   : Dive into the configuration view to see and alter the automatically generated parameters, such as top-k retrieval, summarization options, and more.\n  </li>\n  <li>\n   <strong>\n    Interactive RAG Agent:\n   </strong>\n   Once set up, you can interact with your RAG agent, asking questions and getting responses based on your data.\n  </li>\n </ol>\n <p>\n  The app is designed for both\n  <strong>\n   less-technical and technical users:\n  </strong>\n  if you\u2019re less-technical, you still need to clone the repo and pip install it, but you don\u2019t need to worry about what\u2019s going on under the hood. On the other hand, if you are technical, you can inspect and customize specific parameter settings (e.g. top-k, data).\n </p>\n <figure>\n  <figcaption class=\"ov fe ow oh oi ox oy be b bf z dt\">\n   Home page for RAGs\n  </figcaption>\n </figure>\n <h1>\n  Detailed Overview\n </h1>\n <p>\n  The app contains the following sections, corresponding to the steps listed above.\n </p>\n <h2>\n  [1] \ud83c\udfe0 Home Page\n </h2>\n <p>\n  This is the section where you build a RAG pipeline by instructing the \u201cbuilder agent\u201d. Typically to setup a RAG pipeline you need the following components:\n </p>\n <ol>\n  <li>\n   <strong>\n    Describe the dataset\n   </strong>\n   : Currently we support either\n   <em class=\"qr\">\n    a single local file\n   </em>\n   or a\n   <em class=\"qr\">\n    web page\n   </em>\n   . We\u2019re open to suggestions here!\n  </li>\n  <li>\n   <strong>\n    Define the Task\n   </strong>\n   : Your description here initializes the \u201csystem prompt\u201d of the LLM powering the RAG pipeline.\n  </li>\n  <li>\n   <strong>\n    Set RAG Parameters\n   </strong>\n   : Configure typical RAG setup parameters, such as top-k retrieval, chunk size, and summarization options. See below for the full list of parameters.\n  </li>\n </ol>\n <h2>\n  [2] \u2699\ufe0f RAG Config: Tailoring Your Experience\n </h2>\n <p>\n  After setting up the basics, you move to the RAG Config section. This part of the app provides an intuitive UI where you can:\n </p>\n <ul>\n  <li>\n   <strong>\n    View Generated Parameters:\n   </strong>\n   The builder agent suggests parameters based on your initial setup.\n  </li>\n  <li>\n   <strong>\n    Edit and Customize\n   </strong>\n   : You have complete freedom to tweak these settings, ensuring the RAG agent behaves exactly as you need.\n  </li>\n  <li>\n   <strong>\n    Update the Agent:\n   </strong>\n   Any changes you make can be instantly applied by hitting the \u201cUpdate Agent\u201d button.\n  </li>\n </ul>\n <p>\n  This is the current set of parameters:\n </p>\n <ul>\n  <li>\n   System Prompt\n  </li>\n  <li>\n   Include Summarization: whether to also add a summarization tool (instead of only doing top-k retrieval.)\n  </li>\n  <li>\n   Top-K\n  </li>\n  <li>\n   Chunk Size\n  </li>\n  <li>\n   Embed Model\n  </li>\n  <li>\n   LLM\n  </li>\n </ul>\n <h2>\n  [3] \ud83e\udd16 Generated RAG Agent: Interacting with Your Data\n </h2>\n <p>\n  The final piece of the RAGs experience is the Generated RAG Agent section. Here\u2019s what you can expect:\n </p>\n <ul>\n  <li>\n   <strong>\n    Interactive Chatbot Interface:\n   </strong>\n   Just like ChatGPT, engage in conversations with your RAG agent.\n  </li>\n  <li>\n   <strong>\n    Data-Driven Responses:\n   </strong>\n   The agent utilizes top-k vector search and optional summarization tools to answer your queries based on the underlying data.\n  </li>\n  <li>\n   <strong>\n    Seamless Integration:\n   </strong>\n   The agent dynamically picks the right tools to fulfill your queries, ensuring a smooth and intelligent interaction with your dataset.\n  </li>\n </ul>\n <h1>\n  Architecture\n </h1>\n <p>\n  We\u2019ll cover the architecture in more detail in followups. At a high-level:\n </p>\n <ul>\n  <li>\n   We have a\n   <strong>\n    builder agent\n   </strong>\n   equipped with\n   <strong>\n    builder tools \u2014\n   </strong>\n   tools necessary to construct a RAG pipeline.\n  </li>\n  <li>\n   The builder agent will use these tools to set the configuration state. At the end of the initial conversational flow these parameters are then used to initialize the RAG agent.\n  </li>\n </ul>\n <h1>\n  Let\u2019s Walk through an Example!\n </h1>\n <h2>\n  Installation and Setup\n </h2>\n <p>\n  Getting RAGs up and running is straightforward:\n </p>\n <ol>\n  <li>\n   Clone the RAGs project and navigate to the\n   <code class=\"cw qt qu qv qw b\">\n    rags\n   </code>\n   project folder:\n   <a href=\"https://github.com/run-llama/rags\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    https://github.com/run-llama/rags\n   </a>\n  </li>\n  <li>\n   Install the required packages:\n  </li>\n </ol>\n <pre><span class=\"ra pa gt qw b bf rb rc l rd re\" id=\"bd91\">pip install -r requirements.txt</span></pre>\n <p>\n  3. Launch the app:\n </p>\n <pre><span class=\"ra pa gt qw b bf rb rc l rd re\" id=\"8543\">streamlit run 1_\ud83c\udfe0_Home.py</span></pre>\n <h2>\n  Build the RAG Agent\n </h2>\n <p>\n  In the below diagram we show a sequence of commands to \u201cbuild\u201d a RAG pipeline.\n </p>\n <ol>\n  <li>\n   Say that you want to build a chatbot\n  </li>\n  <li>\n   Define the dataset (here it\u2019s a web page, can also be a local file)\n  </li>\n  <li>\n   Define the task\n  </li>\n  <li>\n   Define params (chunk size 512, top-k = 3)\n  </li>\n </ol>\n <figure>\n  <figcaption class=\"ov fe ow oh oi ox oy be b bf z dt\">\n   Screenshot of the home page \u2014 build a RAG agent\n  </figcaption>\n </figure>\n <figure>\n  <figcaption class=\"ov fe ow oh oi ox oy be b bf z dt\">\n   Followup questions to set parameters and build the RAG agent\n  </figcaption>\n </figure>\n <h2>\n  View the Configuration\n </h2>\n <p>\n  We can see the generated configuration in the below page, and view/edit them as necessary!\n </p>\n <p>\n  E.g. we can set\n  <code class=\"cw qt qu qv qw b\">\n   include_summarization\n  </code>\n  to True.\n </p>\n <h2>\n  Test It Out\n </h2>\n <p>\n  Now we can ask questions! We can ask both specific questions as well as summarization questions.\n </p>\n <p>\n  This uses both the vector search and summarization tools to answer the requisite questions.\n </p>\n <figure>\n  <figcaption class=\"ov fe ow oh oi ox oy be b bf z dt\">\n   Question about a specific detail (performs vector search)\n  </figcaption>\n </figure>\n <figure>\n  <figcaption class=\"ov fe ow oh oi ox oy be b bf z dt\">\n   Summarization question over the entire document\n  </figcaption>\n </figure>\n <h1>\n  Conclusion\n </h1>\n <p>\n  In general RAGs is an initial take towards a world where LLM applications are built by and powered by natural language. Let us know your thoughts and feedback!\n </p>\n <h2>\n  Resources\n </h2>\n <p>\n  RAGs repo:\n  <a href=\"https://github.com/run-llama/rags\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://github.com/run-llama/rags\n  </a>\n </p>\n <h2>\n  Contributions and Support\n </h2>\n <p>\n  We\u2019re committed to improving and expanding RAGs. If you encounter any issues or have suggestions, feel free to file a Github issue or join our\n  <a href=\"https://discord.gg/dGcwcsnxhU\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Discord community\n  </a>\n  .\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 7726, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45b64eb6-f8fd-4097-9e2e-1cf2b25b01d7": {"__data__": {"id_": "45b64eb6-f8fd-4097-9e2e-1cf2b25b01d7", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41.html", "file_name": "introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41.html", "file_type": "text/html", "file_size": 4155, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41.html", "file_name": "introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41.html", "file_type": "text/html", "file_size": 4155, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "78a77bf175c9d1930752916347e0af150fdd1b50994cba1d8481c5bb3de884a3", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Want to try out retrieval-augmented generation (RAG) without writing a line of code? We got you covered! Introducing the new\n  <code class=\"cw ny nz oa ob b\">\n   llamaindex-cli\n  </code>\n  tool, installed when you\n  <code class=\"cw ny nz oa ob b\">\n   pip install llama-index\n  </code>\n  ! It uses\n  <a href=\"https://docs.llamaindex.ai/en/stable/examples/vector_stores/ChromaIndexDemo.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Chroma\n  </a>\n  under the hood, so you\u2019ll need to\n  <code class=\"cw ny nz oa ob b\">\n   pip install chromadb\n  </code>\n  as well.\n </p>\n <h1>\n  How to use it\n </h1>\n <ol>\n  <li>\n   <strong>\n    Set the\n   </strong>\n   <code class=\"cw ny nz oa ob b\">\n    <strong>\n     OPENAI_API_KEY\n    </strong>\n   </code>\n   <strong>\n    environment variable:\n   </strong>\n   By default, this tool uses OpenAI\u2019s API. As such, you\u2019ll need to ensure the OpenAI API Key is set under the\n   <code class=\"cw ny nz oa ob b\">\n    OPENAI_API_KEY\n   </code>\n   environment variable whenever you use the tool.\n  </li>\n </ol>\n <pre><span class=\"pr oe gt ob b bf ps pt l pu pv\" id=\"d38c\">$ export OPENAI_API_KEY=&amp;lt;api_key&amp;gt;</span></pre>\n <p>\n  <strong>\n   2. Ingest some files:\n  </strong>\n  Now, you need to point the tool at some local files that it can ingest into the local vector database. For this example, we\u2019ll ingest the LlamaIndex\n  <code class=\"cw ny nz oa ob b\">\n   README.md\n  </code>\n  file:\n </p>\n <pre><span class=\"pr oe gt ob b bf ps pt l pu pv\" id=\"fd33\">$ llamaindex-cli rag --files \"./README.md\"</span></pre>\n <p>\n  You can only specify a file glob pattern such as\n </p>\n <pre><span class=\"pr oe gt ob b bf ps pt l pu pv\" id=\"5dcd\">$ llamaindex-cli rag --files \"./docs/**/*.rst\"</span></pre>\n <p>\n  <strong>\n   3. Ask a Question\n  </strong>\n  : You can now start asking questions about any of the documents you\u2019d ingested in the prior step:\n </p>\n <pre><span class=\"pr oe gt ob b bf ps pt l pu pv\" id=\"a33a\">$ llamaindex-cli rag --question <span class=\"hljs-string\">\"What is LlamaIndex?\"</span> \nLlamaIndex is a data framework that helps in ingesting, structuring, and accessing <span class=\"hljs-keyword\">private</span> or domain-specific data <span class=\"hljs-keyword\">for</span> LLM-based applications. It provides tools such as data connectors to ingest data from various sources, data indexes to structure the data, and engines <span class=\"hljs-keyword\">for</span> natural language access to the data. LlamaIndex follows a Retrieval-Augmented <span class=\"hljs-title function_\">Generation</span> <span class=\"hljs-params\">(RAG)</span> approach, where it retrieves information from data sources, adds it to the question as context, and then asks the LLM to generate an answer based on the enriched prompt. This approach overcomes the limitations of fine-tuning LLMs and provides a more cost-effective, up-to-date, and trustworthy solution <span class=\"hljs-keyword\">for</span> data augmentation. LlamaIndex is designed <span class=\"hljs-keyword\">for</span> both beginner and advanced users, with a high-level API <span class=\"hljs-keyword\">for</span> easy usage and lower-level APIs <span class=\"hljs-keyword\">for</span> customization and extension.</span></pre>\n <p>\n  <strong>\n   4. Open a Chat REPL\n  </strong>\n  : You can even open a chat interface within your terminal! Just run\n  <code class=\"cw ny nz oa ob b\">\n   llamaindex-cli rag --chat\n  </code>\n  and start asking questions about the files you\u2019ve ingested.\n </p>\n <h1>\n  Customize it to your heart\u2019s content!\n </h1>\n <p>\n  You can customize\n  <code class=\"cw ny nz oa ob b\">\n   llamaindex-cli\n  </code>\n  to use any LLM model, even local models like Mixtral 8x7b through\n  <a href=\"https://docs.llamaindex.ai/en/stable/examples/llm/ollama.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Ollama\n  </a>\n  , and you can build more advanced query and retrieval techniques.\n  <a href=\"https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Check the documentation\n  </a>\n  for details on how to get started.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4140, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5da260ff-81af-437d-bbc7-e67593307b87": {"__data__": {"id_": "5da260ff-81af-437d-bbc7-e67593307b87", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms.html", "file_name": "introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms.html", "file_type": "text/html", "file_size": 14933, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms.html", "file_name": "introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms.html", "file_type": "text/html", "file_size": 14933, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "4c4cd4fd3ac4dfcbf7eceea7d576b9feab428c4cfbe2a438d332f83fed644868", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We're thrilled to announce a new feature in LlamaIndex that expands our knowledge graph capabilities to be more flexible, extendible, and robust. Introducing the Property Graph Index!\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Why Property Graphs?\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Traditional knowledge graph representations like knowledge triples (subject, predicate, object) are limited in expressiveness. They lack the ability to:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Assign labels and properties to nodes and relationships\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Represent text nodes as vector embeddings\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Perform both vector and symbolic retrieval\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Our existing\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   KnowledgeGraphIndex\n  </code>\n  was burdened with these limitations, as well as general limitations on the architecture of the index itself.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The Property Graph Index solves these issues. By using a labeled property graph representation, it enables far richer modeling, storage and querying of your knowledge graph.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  With Property Graphs, you can:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Categorize nodes and relationships into types with associated metadata\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Treat your graph as a superset of a vector database for hybrid search\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Express complex queries using the Cypher graph query language\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  This makes Property Graphs a powerful and flexible choice for building knowledge graphs with LLMs.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Constructing Your Graph\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The Property Graph Index offers several ways to extract a knowledge graph from your data, and you can combine as many as you want:\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   1. Schema-Guided Extraction\n  </strong>\n  : Define allowed entity types, relationship types, and their connections in a schema. The LLM will only extract graph data that conforms to this schema.\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index.indices.property_graph <span class=\"hljs-keyword\">import</span> SchemaLLMPathExtractor\n\nentities = <span class=\"hljs-type\">Literal</span>[<span class=\"hljs-string\">\"PERSON\"</span>, <span class=\"hljs-string\">\"PLACE\"</span>, <span class=\"hljs-string\">\"THING\"</span>]\nrelations = <span class=\"hljs-type\">Literal</span>[<span class=\"hljs-string\">\"PART_OF\"</span>, <span class=\"hljs-string\">\"HAS\"</span>, <span class=\"hljs-string\">\"IS_A\"</span>]\nschema = {\n    <span class=\"hljs-string\">\"PERSON\"</span>: [<span class=\"hljs-string\">\"PART_OF\"</span>, <span class=\"hljs-string\">\"HAS\"</span>, <span class=\"hljs-string\">\"IS_A\"</span>],\n    <span class=\"hljs-string\">\"PLACE\"</span>: [<span class=\"hljs-string\">\"PART_OF\"</span>, <span class=\"hljs-string\">\"HAS\"</span>], \n    <span class=\"hljs-string\">\"THING\"</span>: [<span class=\"hljs-string\">\"IS_A\"</span>],\n}\n\nkg_extractor = SchemaLLMPathExtractor(\n  llm=llm, \n  possible_entities=entities, \n  possible_relations=relations, \n  kg_validation_schema=schema,\n  strict=<span class=\"hljs-literal\">True</span>,  <span class=\"hljs-comment\"># if false, allows values outside of spec</span>\n)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   2. Implicit Extraction\n  </strong>\n  : Use LlamaIndex constructs to specify relationships between nodes in your data. The graph will be built based on the\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   node.relationships\n  </code>\n  attribute. For example, when running a document through a node parser, the\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   PREVIOUS\n  </code>\n  ,\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   NEXT\n  </code>\n  and\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   SOURCE\n  </code>\n  relationships will be captured.\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index.core.indices.property_graph <span class=\"hljs-keyword\">import</span> ImplicitPathExtractor\n\nkg_extractor = ImplicitPathExtractor()</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   3. Free-Form Extraction\n  </strong>\n  : Let the LLM infer the entities, relationship types and schema directly from your data in a free-form manner. (This is similar to how the\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   KnowledgeGraphIndex\n  </code>\n  works today.)\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index.core.indices.property_graph <span class=\"hljs-keyword\">import</span> SimpleLLMPathExtractor\n\nkg_extractor = SimpleLLMPathExtractor(llm=llm)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Mix and match these extraction approaches for fine-grained control over your graph structure.\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index.core <span class=\"hljs-keyword\">import</span> PropertyGraphIndex\n\nindex = PropertyGraphIndex.from_documents(docs, kg_extractors=[...])</code></pre>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  Embeddings\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  By default, all graph nodes are embedded. While some graph databases support embeddings natively, you can also specify and use any vector store from LlamaIndex on top of your graph database.\n </p>\n <pre><code>index = PropertyGraphIndex(..., vector_store=vector_store)</code></pre>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Querying Your Graph\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The Property Graph Index supports a wide variety of querying techniques that can be combined and run concurrently.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   1. Keyword/Synonym-Based Retrieval\n  </strong>\n  : Expand your query into relevant keywords and synonyms and find matching nodes.\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index.core.indices.property_graph <span class=\"hljs-keyword\">import</span> LLMSynonymRetriever\n\nsub_retriever = LLMSynonymRetriever(index.property_graph_store, llm=llm)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   2. Vector Similarity\n  </strong>\n  : Retrieve nodes based on the similarity of their vector representations to your query.\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index.core.indices.property_graph <span class=\"hljs-keyword\">import</span> VectorContextRetriever\n\nsub_retriever = VectorContextRetriever(\n  index.property_graph_store, \n  vector_store=index.vector_store,\n  embed_model=embed_model,\n)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   3. Cypher Queries\n  </strong>\n  : Use the expressive Cypher graph query language to specify complex graph patterns and traverse multiple relationships.\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index.core.indices.property_graph <span class=\"hljs-keyword\">import</span> CypherTemplateRetriever\n<span class=\"hljs-keyword\">from</span> llama_index.core.bridge.pydantic <span class=\"hljs-keyword\">import</span> BaseModel, Field\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">Params</span>(<span class=\"hljs-title class_ inherited__\">BaseModel</span>):\n \u201c\u201d\u201dParameters <span class=\"hljs-keyword\">for</span> a cypher query.\u201d\u201d\u201d\n names: <span class=\"hljs-built_in\">list</span>[<span class=\"hljs-built_in\">str</span>] = Field(description=\u201dA <span class=\"hljs-built_in\">list</span> of possible entity names <span class=\"hljs-keyword\">or</span> keywords related to the query.\u201d)\n \ncypher_query = <span class=\"hljs-string\">\"\"\"\n   MATCH (c:Chunk)-[:MENTIONS]-&gt;(o) \n   WHERE o.name IN $names\n   RETURN c.text, o.name, o.label;\n\"\"\"</span>\n   \nsub_retriever = CypherTemplateRetriever(\n index.property_graph_store, \n Params, \n cypher_query,\n llm=llm,\n)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Instead of providing a template, you can also let the LLM write the entire cypher query based on context from the query and database:\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index.core.indices.property_graph <span class=\"hljs-keyword\">import</span> TextToCypherRetriever\n\nsub_retriever = TextToCypherRetriever(index.property_graph_store, llm=llm)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   4. Custom Graph Traversal\n  </strong>\n  : Define your own graph traversal logic by subclassing key retriever components.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  These retrievers can be combined and composed for hybrid search that leverages both the graph structure and vector representations of nodes.\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index.indices.property_graph <span class=\"hljs-keyword\">import</span> VectorContextRetriever, LLMSynonymRetriever\n\nvector_retriever = VectorContextRetriever(index.property_graph_store, embed_model=embed_model)  \nsynonym_retriever = LLMSynonymRetriever(index.property_graph_store, llm=llm)\n\nretriever = index.as_retriever(sub_retrievers=[vector_retriever, synonym_retriever])</code></pre>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Using the Property Graph Store\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Under the hood, the Property Graph Index uses a\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   PropertyGraphStore\n  </code>\n  abstraction to store and retrieve graph data. You can also use this store directly for lower-level control.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The store supports:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Inserting and updating nodes, relationships and properties\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Querying nodes by ID or properties\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Retrieving relationship paths from a starting node\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Executing Cypher queries (if the backing store supports it)\n  </li>\n </ul>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index.graph_stores.neo4j <span class=\"hljs-keyword\">import</span> Neo4jPGStore\n\ngraph_store = Neo4jPGStore(\n    username=<span class=\"hljs-string\">\"neo4j\"</span>,\n    password=<span class=\"hljs-string\">\"password\"</span>,\n    url=<span class=\"hljs-string\">\"bolt://localhost:7687\"</span>,\n)\n\n<span class=\"hljs-comment\"># insert nodes</span>\nnodes = [\n    EntityNode(name=<span class=\"hljs-string\">\"llama\"</span>, label=<span class=\"hljs-string\">\"ANIMAL\"</span>, properties={<span class=\"hljs-string\">\"key\"</span>: <span class=\"hljs-string\">\"value\"</span>}),\n    EntityNode(name=<span class=\"hljs-string\">\"index\"</span>, label=<span class=\"hljs-string\">\"THING\"</span>, properties={<span class=\"hljs-string\">\"key\"</span>: <span class=\"hljs-string\">\"value\"</span>}), \n]\ngraph_store.upsert_nodes(nodes)\n\n<span class=\"hljs-comment\"># insert relationships  </span>\nrelations = [\n    Relation(\n        label=<span class=\"hljs-string\">\"HAS\"</span>,\n        source_id=nodes[<span class=\"hljs-number\">0</span>].<span class=\"hljs-built_in\">id</span>, \n        target_id=nodes[<span class=\"hljs-number\">1</span>].<span class=\"hljs-built_in\">id</span>,\n    )\n]\ngraph_store.upsert_relations(relations)\n\n<span class=\"hljs-comment\"># query nodes</span>\nllama_node = graph_store.get(properties={<span class=\"hljs-string\">\"name\"</span>: <span class=\"hljs-string\">\"llama\"</span>})[<span class=\"hljs-number\">0</span>]\n\n<span class=\"hljs-comment\"># get relationship paths  </span>\npaths = graph_store.get_rel_map([llama_node], depth=<span class=\"hljs-number\">1</span>)\n\n<span class=\"hljs-comment\"># run Cypher query</span>\nresults = graph_store.structured_query(<span class=\"hljs-string\">\"MATCH (n) RETURN n LIMIT 10\"</span>)  </code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Several backing stores are supported, including in-memory, disk-based, and Neo4j.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Learn More\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/module_guides/indexing/lpg_index_guide/\" rel=\"noreferrer noopener\">\n    Property Graph Index Overview\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/examples/property_graph/property_graph_basic/\" rel=\"noreferrer noopener\">\n    Basic Usage Notebook\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/examples/property_graph/property_graph_advanced/\" rel=\"noreferrer noopener\">\n    Advanced Usage with Neo4j\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/examples/property_graph/property_graph_neo4j/\" rel=\"noreferrer noopener\">\n    Using the Property Graph Store Directly\n   </a>\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  A huge thanks to our partners at\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://neo4j.com/\" rel=\"noreferrer noopener\">\n   Neo4j\n  </a>\n  for their collaboration on this launch, especially\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.linkedin.com/in/tomazb/\" rel=\"noreferrer noopener\">\n   Tomaz Bratanic\n  </a>\n  for the detailed integration guide and design guidance.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We can't wait to see what you build with the new Property Graph Index! As always, feel free to join our\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"http://discord.gg/dGcwcsnxhU\" rel=\"noreferrer noopener\">\n   Discord\n  </a>\n  to share your projects, ask questions, and get support from the community.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Happy building!\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The LlamaIndex Team\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 14916, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6f6bbd07-b237-4787-a3ce-fab279884a70": {"__data__": {"id_": "6f6bbd07-b237-4787-a3ce-fab279884a70", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb.html", "file_name": "join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb.html", "file_type": "text/html", "file_size": 6164, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb.html", "file_name": "join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb.html", "file_type": "text/html", "file_size": 6164, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "1f8c57cca64b2a5ee151ce335521d84e4f5a1d0877ebb8f9c48fc2890ba752e5", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <blockquote>\n  <p class=\"nc nd ne nf b ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa gm bj\" id=\"26a5\">\n   Master LlamaIndex with our course developed in collaboration with Activeloop, TowardsAI, &amp; Intel. Learn to apply advanced retrieval across industries in 40+ lessons. This is a guest post from Activeloop.\n  </p>\n </blockquote>\n <p>\n  LlamaIndex is proud to collaborate with Activeloop, Towards AI, and the Intel Disruptor Initiative to offer a free course on \u201c\n  <a href=\"https://learn.activeloop.ai/courses/rag\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Advanced Retrieval Augmented Generation for Production\n  </a>\n  ,\u201d a part of the Gen AI 360: Foundational Model Certification series. This comprehensive course takes a hands-on approach to applying RAG techniques across various industries, including legal, biomedical, healthcare, e-commerce, and finance.\n </p>\n <p>\n  The free course is designed for practical learning and invites participants to tackle real business challenges, such as developing a multi-modal AI shopping assistant. The course has over 40 lessons, 7 interactive projects, and 2 hours of video content, including from LlamaIndex CEO Jerry Liu. In 20+ hours of learning, the curriculum is geared towards enabling GenAI tinkerers, professionals, and executives to apply LlamaIndex and Deep Lake,\n  <a href=\"http://activeloop.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Activeloop\n  </a>\n  \u2019s database for AI in production. Participants who complete the course will be awarded a certificate at no cost.\n </p>\n <blockquote>\n  <p class=\"or os gt be ot ou ov ow ox oy oz oa dt\" id=\"1013\">\n   In the rapidly evolving business landscape, leveraging Retrieval Augmented Generation (RAG) tools like Llamalndex &amp; Deep Lake by Activeloop is essential for enterprises seeking a competitive edge in GenAI. This course is tailored to quickly upskill your team in GenAI workflows, emphasizing the integration of Activeloop\u2019s advanced features like Deep Memory with Llamalndex for unmatched retrieval accuracy. It\u2019s a strategic investment to enhance your team\u2019s capabilities, ensuring your enterprise stays at the forefront of AI innovation\n  </p>\n  <p class=\"or os gt be ot ou ov ow ox oy oz oa dt\" id=\"529d\">\n   - Jerry Liu, CEO &amp; Co-Founder, LlamaIndex\n  </p>\n </blockquote>\n <h1>\n  Highlighted projects across several industries:\n </h1>\n <ul>\n  <li>\n   <strong>\n    Healthcare:\n   </strong>\n   Advanced RAG for Pill Searching. Combine cutting-edge NLP and computer vision techniques to build an AI app that recognizes pills from images, and lists their side effects and instructions to use. This project offers hands-on experience with the latest AI technologies like Segment Anything or GPT-4-vision.\n  </li>\n  <li>\n   <strong>\n    Legal:\n   </strong>\n   Patent Generation and Search Engine. Gain practical knowledge in constructing a system like PatentPT, which incorporates a fine-tuned LLM to search or create patents. Learn how to build a meta-agent to smartly route user inquiries, ensuring a fluid chat experience with a database of 8 million USPTO patents for comprehensive retrieval and generation capabilities.\n  </li>\n  <li>\n   <strong>\n    E-commerce:\n   </strong>\n   AI-Powered Shopping Assistant for Outfit Recommendations. Build a multi-modal AI assistant that curates outfit suggestions for any occasion, weather, or budget!\n  </li>\n </ul>\n <h1>\n  What will I learn?\n </h1>\n <ul>\n  <li>\n   <strong>\n    Challenges with Naive RAG:\n   </strong>\n   We\u2019ll address common issues such as low precision, recall, and suboptimal response generation. Strategies for refining data processing, enhancing embedding models, refining retrieval algorithms, and optimizing prompt usage will be explored to improve RAG system performance.\n  </li>\n  <li>\n   <strong>\n    Advanced RAG with LlamaIndex:\n   </strong>\n   Delve into basic and advanced RAG methods using LlamaIndex. The course covers the essential aspects of LlamaIndex required for RAG application development, complemented by Activeloop\u2019s Deep Memory module, which natively integrates seamlessly with LlamaIndex to enhance retrieval accuracy by an average of 22%. Topics will range from small to large-scale retrieval, handling structured and unstructured data, and querying techniques.\n  </li>\n  <li>\n   <strong>\n    RAG Agents:\n   </strong>\n   This module focuses on the creation and application of RAG agents with LlamaIndex, including advanced querying, summarizing databases, and designing AI assistants using various APIs.\n  </li>\n </ul>\n <p>\n  Production-grade apps: Learn to fine-tune the LlamaIndex RAG pipeline for professional deployment, evaluate RAG systems crafted with LlamaIndex, and ensure your models\u2019 observability and effectiveness.\n </p>\n <figure>\n  <figcaption class=\"qr fe qs ob oc qt qu be b bf z dt\">\n   Here\u2019s a brief introduction to the course by Louis Bouchard from TowardsAI team.\n  </figcaption>\n </figure>\n <h1>\n  Who Should Enroll?\n </h1>\n <p>\n  This course is ideal for aspiring AI professionals, executives, and enthusiasts eager to apply AI in practical scenarios. Whether you want to enhance your organization\u2019s AI capabilities or expand your knowledge, this course offers valuable hands-on experience. A basic understanding of coding and Python is recommended.\n </p>\n <h1>\n  Complimentary Free Trial of Deep Lake\n </h1>\n <p>\n  As a part of the course, all course takers can redeem a free extended trial of one month for the Activeloop Starter and Growth plans by redeeming the\n  <strong>\n   <em class=\"ne\">\n    GENAI360LLAMA\n   </em>\n  </strong>\n  promo code at checkout. Check out the following video to\n  <a href=\"https://www.loom.com/share/72f563abfe2544a587dd77cd34ece135?sid=f5079a31-ad13-4b9f-a030-429b27a8703c\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   learn more\n  </a>\n  .\n </p>\n <p>\n  Join thousands of AI engineers in mastering master\n  <a href=\"http://learn.activeloop.ai/courses/rag\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Retrieval Augmented Generation with LlamaIndex\n  </a>\n  . Enroll for free today!\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 6141, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9252e368-47eb-4114-922a-b9098ed2a1e4": {"__data__": {"id_": "9252e368-47eb-4114-922a-b9098ed2a1e4", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/launching-the-first-genai-native-document-parsing-platform.html", "file_name": "launching-the-first-genai-native-document-parsing-platform.html", "file_type": "text/html", "file_size": 9532, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/launching-the-first-genai-native-document-parsing-platform.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/launching-the-first-genai-native-document-parsing-platform.html", "file_name": "launching-the-first-genai-native-document-parsing-platform.html", "file_type": "text/html", "file_size": 9532, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "d70b6f53f93ec60d63eb4b5302605fb4b2499d2f7931ff94438b92f042d35db7", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Our mission at LlamaIndex is to connect the world\u2019s data to the power of LLMs, and today we\u2019re pleased to announce our latest big step towards that goal with the world\u2019s first GenAI-native document parsing platform, LlamaParse.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We launched the first public version of LlamaParse\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b\" rel=\"noreferrer noopener\">\n   3 weeks ago\n  </a>\n  and the response has been huge with well over 2,000 users parsing over 1 million pages! We\u2019ve been hard at work releasing hundreds of bug fixes and new features since then, and today we\u2019re releasing a game-changing new feature,\n  <strong>\n   GenAI-powered parsing instructions\n  </strong>\n  .\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Using LLMs for world-class parsing\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The key insight behind parsing instructions is that\n  <strong>\n   you know what kind of documents you have\n  </strong>\n  , so you already know what kind of output you want. Why make the parser guess when an LLM-enabled parser can take simple, natural-language instructions from you and provide radically better parsing results?\n </p>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  Example 1: rich table support\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Since we first released LlamaParse it has featured\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_parse/blob/main/examples/demo_advanced.ipynb\" rel=\"noreferrer noopener\">\n   industry-leading table extraction\n  </a>\n  capabilities. Under the hood, this has been using LLM intelligence since the start. It seamlessly integrates with the advanced indexing/retrieval capabilities that the open-source framework offers, enabling users to build state-of-the-art document RAG. Now with JSON mode (see below) and parsing instructions, you can take this even further.\n </p>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  Example 2: parsing comic books\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Parsing translated manga presents a particular challenge for a parser since a regular parser interprets the panels as cells in a table, and the reading order is right-to-left even though the book is in English, as shown in this extract from \"The manga guide to calculus\", by Hiroyuki Kojima:\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Using LlamaParse, you can give the parser plain, English-language instructions on what to do:\n </p>\n <pre><code>The provided document is a manga comic book. \nMost pages do NOT have title. It does not contain tables. \nTry to reconstruct the dialogue happening in a cohesive way.</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  (You can see the full code in our\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://colab.research.google.com/drive/1dO2cwDCXjj9pS9yQDZ2vjg-0b5sRXQYo\" rel=\"noreferrer noopener\">\n   demonstration notebook\n  </a>\n  , including what it looks like to parse this without the instructions)\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The result is a perfect parse!\n </p>\n <pre><code># The Asagake Times\n\nSanda-Cho Distributor\n\nA newspaper distributor?\n\nDo I have the wrong map?</code></pre>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  Example 3: mathematical equations\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Another challenging format for parsing is complex mathematical equations (by coincidence, the manga we picked as an example is all about how to do mathematics):\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  To parse this, we take the same instructions as before and add one sentence:\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   Output any math equation in LATEX markdown (between $$)\n  </code>\n  . The result of parsing is clear LaTeX instructions, which render the equations perfectly:\n </p>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  Anything an LLM can do, our parser can do\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  You can use this kind of natural-language instruction to do all sorts of advanced pre-processing on your documents \u2014 simplify language, include sentiment analysis, translate them to another language! We can\u2019t wait to see what you do with the power of LlamaParse.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  JSON mode\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Parsing instructions are definitely the headline feature, but we have dozens of other features new to LlamaParse since launch. A standout is JSON mode, a rich programmatic format perfect for when you want more precision about exactly what you want to parse out. JSON mode\u2019s output includes\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   the full structure of the document that was parsed\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   tables, text and headings marked\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   tables are available as CSV and JSON\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   images are marked and available for extraction (see below)\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   a wealth of metadata about each node\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  If you are building a custom RAG strategy JSON mode gives you everything you need to build it. Check out our\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_parse/blob/main/examples/demo_json.ipynb\" rel=\"noreferrer noopener\">\n   JSON mode examples!\n  </a>\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Image extraction\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  One of the best features of JSON mode is image extraction: every page that contains images comes with a list of images, marked up with metadata including their size and position on the page, and you can\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://api.cloud.llamaindex.ai/docs#/parsing/get_job_image_result_api_parsing_job__job_id__result_image__name__get\" rel=\"noreferrer noopener\">\n   retrieve these images directly\n  </a>\n  and\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_parse/blob/main/examples/demo_json.ipynb\" rel=\"noreferrer noopener\">\n   include them in your indexing\n  </a>\n  to extract even more information from your complex, image-heavy documents.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Expanded document types\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We launched LlamaParse with exceptional support for PDFs, and we have continued to expand its capability every day. We\u2019ve also added support for a large array of document types:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Microsoft Word (.doc, .docx)\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Microsoft PowerPoint (.pptx)\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Rich Text Format (.rtf)\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Apple Pages (.pages)\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Apple Keynote (.key)\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   ePub books (.epub)\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   And dozens more!\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  All of these document types \u201cjust work\u201d without any additional work on your part, and we are constantly expanding the list of supported file types. Check out this\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://colab.research.google.com/drive/1B5OlhHU8ewppuWf_d4dhZJW2vdYiqZ95?usp=sharing\" rel=\"noreferrer noopener\">\n   demo notebook\n  </a>\n  where we demonstrate parsing a PowerPoint file.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  And one more thing\u2026 unlimited parsing!\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The huge demand for LlamaParse has included many people asking to go beyond our free daily limits via paid plans, and we\u2019re happy to answer those requests. Our pricing is simple:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   7000 pages/week are free\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Additional pages are $0.003/page, or $3 per 1000 pages\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Maximum size for one document is 750 pages\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  And of course we retain our generous free tier of 1000 pages/day.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The public version of LlamaParse is a hosted service. If you want to extend LlamaParse capabilities to build advanced document RAG, or wish to deploy LlamaParse in a private cloud,\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/contact\" rel=\"noreferrer noopener\">\n   get in touch.\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 9505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4d5e3eb-45fb-4b5e-8bfb-ccecefd2ab7b": {"__data__": {"id_": "b4d5e3eb-45fb-4b5e-8bfb-ccecefd2ab7b", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llama-index-prem-ai-join-forces-51702fecedec.html", "file_name": "llama-index-prem-ai-join-forces-51702fecedec.html", "file_type": "text/html", "file_size": 9535, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llama-index-prem-ai-join-forces-51702fecedec.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llama-index-prem-ai-join-forces-51702fecedec.html", "file_name": "llama-index-prem-ai-join-forces-51702fecedec.html", "file_type": "text/html", "file_size": 9535, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "bee41dba5bb7b39b81185c11de7381ffafc26e06e6cc9e79e844e3160590b657", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  <strong>\n   Co-authors:\n  </strong>\n  Simone Giacomelli (co-founder at Prem), Jerry Liu (co-founder/CEO at LlamaIndex)\n </p>\n <p>\n  We\u2019re pleased to share the successful integration of Prem App and Llama Index, a union that brings a new level of privacy to AI development. Prem\u2019s self-hosting AI models and Llama\u2019s versatile data framework enhances the ability to build AI applications in a customizable and flexible manner.\n </p>\n <h2>\n  Integration Details\n </h2>\n <p>\n  By combining Prem\u2019s self-hosting AI models with Llama Index\u2019s data framework, developers can now connect custom data sources to large language models easily. This simplifies the process of data ingestion, indexing, and querying, streamlining the overall AI development cycle.\n </p>\n <h2>\n  Getting Started\n </h2>\n <p>\n  To leverage this integration, simply download the Prem App and connect your data sources through the Llama Index platform. This allows you to self-host your AI models with Prem App and utilize Llama Index\u2019s capabilities to manage your data efficiently. This integration, therefore, significantly boosts AI application development, giving developers greater control and flexibility over their projects.\n </p>\n <h1>\n  Getting Started\n </h1>\n <h2>\n  Install Prem\n </h2>\n <p>\n  You can run Prem in two different ways:\n </p>\n <ul>\n  <li>\n   MacOS: go to\n   <a href=\"https://premai.io\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    https://premai.io\n   </a>\n   and download Prem App.\n  </li>\n  <li>\n   Server: run the installer script:\n   <code class=\"cw qm qn qo qp b\">\n    wget -q &lt;https://get.prem.ninja/install.sh&gt; -O install.sh; sudo bash ./install.sh\n   </code>\n  </li>\n </ul>\n <h2>\n  Run the services in the GUI\n </h2>\n <p>\n  When the UI is up and running, you can see all the services available. With just one click you can download the service you are interested in. In the background, the docker image associated with the service will be downloaded based on your hardware requirements.\n </p>\n <p>\n  While waiting for the download to be completed, read more about the service, in the detail view. Just click on the card and you will be redirected to the service page. Each service page is packaged with some general info as well as complete documentation giving more details into the model exposed. When the download has been completed, just click on Open and the service will start. You can interact with the service from the playground or from APIs.\n </p>\n <p>\n  You can check the port on which the service is running from the service detail view.\n </p>\n <h1>\n  Start Building Your App\n </h1>\n <p>\n  In this quick tutorial will show you how to build a simple Talk to your Data use case using Prem landing page content.\n </p>\n <p>\n  In order to achieve that we will need to run three services:\n </p>\n <ul>\n  <li>\n   Redis: we will use Redis as a vector store to store the embeddings.\n  </li>\n  <li>\n   Vicuna 7B Q4: we will use Vicuna in order to generate a proper response for the user based on the most similar document we get using Redis similarity search\n  </li>\n  <li>\n   All MiniLM L6 V2: we will use sentence transformers in order to generate the embeddings out of our documents.\n  </li>\n </ul>\n <p>\n  If all the services necessary are running, you will see a similar interface as the one beyond.\n </p>\n <p>\n  You can now start integrating the services using Llama Index library. In the following code snippets, we will show you how you can build a simple talk to your data use case using Prem and Llama Index.\n </p>\n <ol>\n  <li>\n   Import all necessary dependencies and assign a random string to\n   <code class=\"cw qm qn qo qp b\">\n    OPENAI_API_KEY\n   </code>\n   environment variable.\n  </li>\n </ol>\n <pre><span class=\"qy om gt qp b bf qz ra l rb rc\" id=\"13b7\"><span class=\"hljs-keyword\">import</span> os\n\n<span class=\"hljs-keyword\">from</span> llama_index.vector_stores <span class=\"hljs-keyword\">import</span> RedisVectorStore\n<span class=\"hljs-keyword\">from</span> llama_index.storage.storage_context <span class=\"hljs-keyword\">import</span> StorageContext\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> ListIndex, LLMPredictor, Document\n\n<span class=\"hljs-keyword\">from</span> langchain.chat_models <span class=\"hljs-keyword\">import</span> ChatOpenAI\n<span class=\"hljs-keyword\">from</span> langchain.embeddings.openai <span class=\"hljs-keyword\">import</span> OpenAIEmbeddings\n\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> LangchainEmbedding, ServiceContext\n\nos.environ[<span class=\"hljs-string\">\"OPENAI_API_KEY\"</span>] = <span class=\"hljs-string\">\"random-string\"</span></span></pre>\n <p>\n  2. Load the Data / Create some Documents. In this example, I am using Prem landing page content creating manually some documents.\n </p>\n <pre><span class=\"qy om gt qp b bf qz ra l rb rc\" id=\"428c\">doc1 = Document(text=\"Prem is an easy to use open source AI platform. With Prem you can quickly build privacy preserving AI applications.\")\ndoc2 = Document(text=\"\"\"\nPrem App\n\nAn intuitive desktop application designed to effortlessly deploy and self-host Open-Source AI models without exposing sensitive data to third-party.\n\n\"\"\")\ndoc3 = Document(text=\"\"\"\nPrem Benefits\n\nEffortless Integration\nSeamlessly implement machine learning models with the user-friendly interface of OpenAI's API.\n\nReady for the Real World\nBypass the complexities of inference optimizations. Prem's got you covered.\n\nRapid Iterations, Instant Results\nDevelop, test, and deploy your models in just minutes.\n\nPrivacy Above All\nYour keys, your models. We ensure end-to-end encryption.\n\nComprehensive Documentation\nDive into our rich resources and learn how to make the most of Prem.\n\nPreserve Your Anonymity\nMake payments with Bitcoin and Cryptocurrency. It's a permissionless infrastructure, designed for you.\n\"\"\")</span></pre>\n <p>\n  3. Instantiate the LLMs connecting to the running services.\n </p>\n <pre><span class=\"qy om gt qp b bf qz ra l rb rc\" id=\"98fb\"># Instantiate a llm predictor using Langchain pointing to vicuna-7b-q4 service\nllm_predictor = LLMPredictor(llm=ChatOpenAI(openai_api_base=\"http://localhost:8111/api/v1\", max_tokens=128))\n\n# Instantiate the embeddings object using Langchain pointing to all-MiniLM-L6-v2 service\nembeddings = OpenAIEmbeddings(openai_api_base=\"http://localhost:8444/api/v1\")\nembed_model = LangchainEmbedding(embeddings)\n\n# define a service context using the embeddings and llm defined above.\nservice_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor=llm_predictor)</span></pre>\n <p>\n  4. Configure the Vector Store\n </p>\n <pre><span class=\"qy om gt qp b bf qz ra l rb rc\" id=\"bb1a\"># instantiate the vectorstore connecting to Redis service\nvector_store = RedisVectorStore(\n    index_name=\"prem_landing\",\n    index_prefix=\"llama\",\n    redis_url=\"redis://localhost:6379\",\n    overwrite=True\n)\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)</span></pre>\n <p>\n  5. Index the documents\n </p>\n <pre><span class=\"qy om gt qp b bf qz ra l rb rc\" id=\"56cd\">index = ListIndex.from_documents([doc1, doc2, doc3], storage_context=storage_context)</span></pre>\n <p>\n  6. Perform an example query\n </p>\n <pre><span class=\"qy om gt qp b bf qz ra l rb rc\" id=\"0147\">query_engine = index.as_query_engine(\n    retriever_mode=<span class=\"hljs-string\">\"embedding\"</span>, \n    verbose=<span class=\"hljs-literal\">True</span>, \n    service_context=service_context\n)\nresponse = query_engine.query(<span class=\"hljs-string\">\"What are Prem benefits?\"</span>)\n<span class=\"hljs-built_in\">print</span>(response)</span></pre>\n <pre><span class=\"qy om gt qp b bf qz ra l rb rc\" id=\"0f6c\">The benefits of using Prem include: Effortless Integration, Ready for the Real World, Rapid Iterations, Instant Results, Privacy Above All, Comprehensive Documentation, Preserve Your Anonymity, and an intuitive desktop application designed to effortlessly deploy and self-host Open-Source AI models without exposing sensitive data to third-party.</span></pre>\n <p>\n  And Done \ud83c\udf89 You are now using Prem with Llama Index.\n </p>\n <h1>\n  More Information\n </h1>\n <p>\n  Check out our documentation at:\n  <a href=\"https://github.com/premai-io/prem-app\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://github.com/premai-io/prem-app\n  </a>\n </p>\n <p>\n  Check out a simple talk to your data notebook with Llama Index:\n  <a href=\"https://github.com/premAI-io/prem-daemon/blob/main/resources/notebooks/llama_index.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://github.com/premAI-io/prem-daemon/blob/main/resources/notebooks/llama_index.ipynb\n  </a>\n </p>\n <p>\n  Checkout our YouTube tutorials\n </p>\n <ul>\n  <li>\n   Getting Started with Prem:\n   <a href=\"https://www.youtube.com/watch?v=XixH46Ysl5A\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    https://www.youtube.com/watch?v=XixH46Ysl5A\n   </a>\n  </li>\n  <li>\n   Deploy Prem in your Paperspace instance:\n   <a href=\"https://www.youtube.com/watch?v=aW8t6wouwx0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    https://www.youtube.com/watch?v=aW8t6wouwx0\n   </a>\n  </li>\n </ul>\n <h1>\n  Join Us\n </h1>\n <p>\n  Our partnership is based on a shared understanding that the future of AI is open, composable, and privacy-centric.\n </p>\n <p>\n  <a href=\"https://discord.com/invite/kpKk6vYVAn\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Join us on this journey\n  </a>\n  !\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 9519, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ba8e8f5-55df-4d69-9171-68459d3b96eb": {"__data__": {"id_": "7ba8e8f5-55df-4d69-9171-68459d3b96eb", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamacloud-built-for-enterprise-llm-app-builders.html", "file_name": "llamacloud-built-for-enterprise-llm-app-builders.html", "file_type": "text/html", "file_size": 10150, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamacloud-built-for-enterprise-llm-app-builders.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamacloud-built-for-enterprise-llm-app-builders.html", "file_name": "llamacloud-built-for-enterprise-llm-app-builders.html", "file_type": "text/html", "file_size": 10150, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "633606a74dfe5853c6ee3c7f6dd0762b8c8c583e10495c9d42c739968cc5e76f", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  RAG is only as Good as your Data\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Building production-ready LLM applications is hard. We've been chatting with hundreds of users, ranging from Fortune 500 enterprises to pre-seed startups and here's what they tell us they struggle with:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Data Quality Issues\n   </strong>\n   : Most companies deal with large sets of complex, heterogeneous documents. Think PDFs with messy formatting, images, tables across multiple pages, different languages - the list goes on. Ensuring high-quality data input is crucial. \"Garbage in, garbage out\" holds especially true for LLM applications.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Scalability Hurdles\n   </strong>\n   : Each new data source requires significant engineering hours for custom parsing and tuning. Keeping data sources in sync isn't easy either.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Accuracy Concerns\n   </strong>\n   : Bad retrievals and hallucinations are common problems when LLMs interact with enterprise data, leading to unreliable outputs.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Configuration Overload:\n   </strong>\n   Fine-tuning LLM applications involves numerous parameters and often requires deep technical expertise, making iterative improvement a daunting task.\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  As developers shift from prototypes towards building production applications - complex orchestration is needed and they want to centralize their abstractions for managing their data. They want a unified interface for processing and retrieving over their diverse sources of data.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  To address these difficulties, we soft-launched LlamaCloud and made LlamaParse widely available a few months ago to bring production-grade context-augmentation to your LLM and RAG applications. LlamaParse can already support 50+ languages and 100+ document formats. The adoption has been incredible - we have grown to tens of thousands of active users for LlamaParse who have processed tens of million pages! Here\u2019s an example from Dean Barr, Applied AI Lead at Carlyle:\n </p>\n <blockquote class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  As an AI Applied Data Scientist who was granted one of the first ML patents in the U.S., and who is building cutting-edge AI capabilities at one of the world's largest Private Equity Funds, I can confidently say that LlamaParse from LlamaIndex is currently the best technology I have seen for parsing complex document structures for Enterprise RAG pipelines. Its ability to preserve nested tables, extract challenging spatial layouts, and images is key to maintaining data integrity in advanced RAG and agentic model building.\n </blockquote>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  The Rise of Centralized Knowledge Management\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We have designed LlamaCloud to cater to the need of\n  <strong>\n   production-grade\n  </strong>\n  <strong>\n   context-augmentation\n  </strong>\n  for your LLM and RAG applications. Let's take a tour of what LlamaCloud brings to the table:\n </p>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    LlamaParse\n   </strong>\n   : Our state-of-the-art parser that turns complex documents with tables and charts into LLM-friendly formats. You can learn more about\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.cloud.llamaindex.ai/llamaparse/getting_started\" rel=\"noreferrer noopener\">\n    LlamaParse here\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Managed Ingestion\n   </strong>\n   : Connect to enterprise data sources and your choice of data sinks with ease. We support\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.cloud.llamaindex.ai/llamacloud/data_sources\" rel=\"noreferrer noopener\">\n    multiple data sources\n   </a>\n   and are adding more. LlamaCloud provides default parsing configurations for generating vector embeddings, while also allowing deep customization for specific applications.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Advanced Retrieval\n   </strong>\n   : LlamaCloud allows basic semantic search retrieval as well as advanced techniques like hybrid search, reranking, and metadata filtering to improve the accuracy of the retrieval. This provides the necessary configurability to build end to end RAG over complex documents.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    LlamaCloud Playground\n   </strong>\n   : An interactive UI to test and refine your ingestion and retrieval strategies before deployment.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Scalability and Security\n   </strong>\n   : Handle large volumes of production data. Compliance certifications as well as deployment options are available based on your security needs.\n  </li>\n </ol>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  This video gives a detailed walk through of LlamaCloud:\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Our customers tell us that LlamaCloud enables developers to spend less time setting up and iterating on their data pipelines for LLM use cases, allowing them to iterate through the LLM application development lifecycle much more quickly. Here\u2019s what Teemu Lahdenpera, CTO at\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"http://Scaleport.ai\" rel=\"noreferrer noopener\">\n   Scaleport.ai\n  </a>\n  had to say:\n </p>\n <blockquote class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  LlamaCloud has really sped up our development timelines. Getting to technical prototypes quickly allows us to show tangible value instantly, improving our sales outcomes. When needed, switching from the LlamaCloud UI to code has been really seamless. The configurable parsing and retrieval features have significantly improved our response accuracy.\n </blockquote>\n <blockquote class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We've also seen great results with LlamaParse and found it outperforming GPT-4 vision on some OCR tasks!\n </blockquote>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Try it yourself\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We\u2019ve opened up an official waitlist for LlamaCloud. Here's how you can get involved:\n </p>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Join the LlamaCloud Waitlist\n   </strong>\n   :\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSdehUJJB4NIYfrPIKoFdF4j8kyfnLhMSH_qYJI_WGQbDWD25A/viewform\" rel=\"noreferrer noopener\">\n    Sign up here\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Get in Touch\n   </strong>\n   : Have questions? Want to discuss unlimited commercial use?\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/contact\" rel=\"noreferrer noopener\">\n    Contact us\n   </a>\n   and let's chat! Note: we support private deployments for a select number of enterprises\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Stay Updated\n   </strong>\n   : Follow us on\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index\" rel=\"noreferrer noopener\">\n    Twitter\n   </a>\n   and join our\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://discord.gg/dGcwcsnxhU\" rel=\"noreferrer noopener\">\n    Discord community\n   </a>\n   to stay in the loop.\n  </li>\n </ol>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In the meantime, anyone can create an account at\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://cloud.llamaindex.ai/\" rel=\"noreferrer noopener\">\n   https://cloud.llamaindex.ai/\n  </a>\n  . While you\u2019re waiting for official LlamaCloud access, anyone can immediately start using our LlamaParse APIs.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We\u2019re shipping a\n  <em>\n   lot\n  </em>\n  of features in the next few weeks. We look forward to seeing the context-augmented LLM applications that you can build on top of LlamaCloud! \ud83d\ude80\ud83e\udd99\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   FAQ\n  </strong>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   Have you got some examples of how to use LlamaCloud?\n  </strong>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We sure do! One of the strengths of LlamaCloud is how easily the endpoints integrate into your existing code. Our\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llamacloud-demo\" rel=\"noreferrer noopener\">\n   llamacloud-demo repo\n  </a>\n  has lots of examples from\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llamacloud-demo/blob/main/examples/getting_started.ipynb\" rel=\"noreferrer noopener\">\n   getting started\n  </a>\n  to\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llamacloud-demo/blob/main/examples/batch_eval.ipynb\" rel=\"noreferrer noopener\">\n   running evaluations\n  </a>\n  .\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   Is this competitive with vector databases?\n  </strong>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  No. LlamaCloud is focused primarily on data parsing and ingestion, which is a complementary layer to any vector storage provider. The retrieval layer is orchestration on top of an existing storage system. LlamaIndex open-source integrates with 40+ of the most popular vector databases, and we are integrating LlamaCloud with storage providers based on customer requests.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 10133, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eaf39a01-bafe-4c52-85af-37d412ae586f": {"__data__": {"id_": "eaf39a01-bafe-4c52-85af-37d412ae586f", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.html", "file_name": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.html", "file_type": "text/html", "file_size": 23681, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.html", "file_name": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.html", "file_type": "text/html", "file_size": 23681, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "f84474a265815f58e348abdd29799b7d7b57ac059fa306dfe6f44f5980ae0cfc", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  A few months ago, we launched LlamaIndex 0.6.0, which included a massive rewrite of our codebase to make our library more modular, customizable, and accessible to both beginner and advanced users:\n </p>\n <ul>\n  <li>\n   We created modular storage abstractions (data, indices), and compute abstractions (retrievers, query engines).\n  </li>\n  <li>\n   We created a lower-level API where users could use our modules (retrievers, query engines) independently and customize it as part of a larger system.\n  </li>\n </ul>\n <p>\n  Today, we\u2019re excited to launch LlamaIndex 0.7.0. Our latest release continues the theme of improving modularity/customizability at the lower level to enable\n  <strong>\n   bottoms-up development of LLM applications over your data.\n  </strong>\n  You now have even more control over using key abstractions: the LLM, our response synthesizer, and our Document and Node objects.\n </p>\n <ul>\n  <li>\n   We\u2019ve created\n   <strong>\n    standalone LLM abstractions\n   </strong>\n   (OpenAI, HuggingFace, PaLM).\n  </li>\n  <li>\n   We\u2019ve made our\n   <strong>\n    response synthesis module an independent module\n   </strong>\n   you can use completely independently of the rest of our abstractions \u2014 get rid of the prompt boilerplate of trying to figure out how to fit context within a context window.\n  </li>\n  <li>\n   We\u2019ve added\n   <strong>\n    extensive metadata management capabilities\n   </strong>\n   to our Document/Node objects \u2014 now you have complete control over context you decide to inject into your documents.\n  </li>\n </ul>\n <p>\n  Below, we describe each section more in detail. We also outline a full list of breaking changes at the bottom.\n </p>\n <h1>\n  Standalone LLM Abstractions\n </h1>\n <p>\n  We\u2019ve created standalone LLM abstractions for OpenAI, HuggingFace, and PaLM. These abstractions can be used on their own, or as part of an existing LlamaIndex system (query engines, retrievers).\n </p>\n <h2>\n  High-level Motivation\n </h2>\n <p>\n  We did this for multiple reasons:\n </p>\n <ul>\n  <li>\n   Cleaner abstractions in the codebase. Before, our\n   <code class=\"cw qm qn qo qp b\">\n    LLMPredictor\n   </code>\n   class had a ton of leaky abstractions with the underlying LangChain LLM class. This made our LLM abstractions hard to reason about, and hard to customize.\n  </li>\n  <li>\n   Slightly cleaner dev UX. Before, if you wanted to customize the default LLM (for instance, use \u201ctext-davinci-003\u201d, you had to import the correct LangChain class, wrap it in our LLMPredictor, and then pass it to ServiceContext. Now it\u2019s easy to just import our LLM abstraction (which is natively documented with our docs) and plug it into ServiceContext. Of course, you can still use LangChain\u2019s LLMs if you wish.\n  </li>\n  <li>\n   Conducive to bottoms-up development: it makes sense to play around with these LLM modules independently before plugging them in as part of a larger system. It\u2019s reflective of our bigger push in 0.7.0 to let users compose their own workflows.\n  </li>\n </ul>\n <h2>\n  <strong>\n   Using on their own\n  </strong>\n </h2>\n <p>\n  Our LLM abstractions support both\n  <code class=\"cw qm qn qo qp b\">\n   complete\n  </code>\n  and\n  <code class=\"cw qm qn qo qp b\">\n   chat\n  </code>\n  endpoints. The main difference is that\n  <code class=\"cw qm qn qo qp b\">\n   complete\n  </code>\n  is designed to take in a simple string input, and output a\n  <code class=\"cw qm qn qo qp b\">\n   CompletionResponse\n  </code>\n  (containing text output + additional fields).\n  <code class=\"cw qm qn qo qp b\">\n   chat\n  </code>\n  takes in a\n  <code class=\"cw qm qn qo qp b\">\n   ChatMessage\n  </code>\n  and outputs a\n  <code class=\"cw qm qn qo qp b\">\n   ChatResponse\n  </code>\n  (containing a chat message + additional fields).\n </p>\n <p>\n  These LLM endpoints also natively support streaming via\n  <code class=\"cw qm qn qo qp b\">\n   stream_complete\n  </code>\n  and\n  <code class=\"cw qm qn qo qp b\">\n   stream_chat\n  </code>\n  .\n </p>\n <p>\n  Here\u2019s on how you can use the LLM abstractions on their own:\n </p>\n <pre><span class=\"qy ov gt qp b bf qz ra l rb rc\" id=\"127a\"><span class=\"hljs-keyword\">from</span> llama_index.llms <span class=\"hljs-keyword\">import</span> OpenAI\n\n<span class=\"hljs-comment\"># using complete endpoint</span>\nresp = OpenAI().complete(<span class=\"hljs-string\">'Paul Graham is '</span>)\n<span class=\"hljs-built_in\">print</span>(resp)\n<span class=\"hljs-comment\"># get raw object</span>\nresp_raw = resp.raw\n<span class=\"hljs-comment\"># using chat endpoint</span>\n<span class=\"hljs-keyword\">from</span> llama_index.llms <span class=\"hljs-keyword\">import</span> ChatMessage, OpenAI\nmessages = [\n    ChatMessage(role=<span class=\"hljs-string\">'system'</span>, content=<span class=\"hljs-string\">'You are a pirate with a colorful personality'</span>),\n    ChatMessage(role=<span class=\"hljs-string\">'user'</span>, content=<span class=\"hljs-string\">'What is your name'</span>)\n]\nresp = OpenAI().chat(messages)\n<span class=\"hljs-built_in\">print</span>(resp)\n<span class=\"hljs-comment\"># get raw object</span>\nresp_raw = resp.raw\n<span class=\"hljs-comment\"># using streaming endpoint</span>\n<span class=\"hljs-keyword\">from</span> llama_index.llms <span class=\"hljs-keyword\">import</span> OpenAI\nllm = OpenAI()\nresp = llm.stream_complete(<span class=\"hljs-string\">'Paul Graham is '</span>)\n<span class=\"hljs-keyword\">for</span> delta <span class=\"hljs-keyword\">in</span> resp:\n    <span class=\"hljs-built_in\">print</span>(delta, end=<span class=\"hljs-string\">''</span>)</span></pre>\n <p>\n  Here\u2019s how you can use the LLM abstractions as part of an overall LlamaIndex system.\n </p>\n <pre><span class=\"qy ov gt qp b bf qz ra l rb rc\" id=\"289e\"><span class=\"hljs-keyword\">from</span> llama_index.llms <span class=\"hljs-keyword\">import</span> OpenAI\n<span class=\"hljs-keyword\">from</span> llama_index.indices.service_context <span class=\"hljs-keyword\">import</span> ServiceContext\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> VectorStoreIndex\n\nllm = OpenAI(model=<span class=\"hljs-string\">'gpt-3.5-turbo'</span>, temperature=<span class=\"hljs-number\">0</span>)\nservice_context = ServiceContext.from_defaults(llm=llm)\nindex = VectorStoreIndex.from_documents(docs, service_context=service_context)\nresponse = index.as_query_engine().query(<span class=\"hljs-string\">\"&amp;lt;question&amp;gt;\"</span>)</span></pre>\n <p>\n  Note: Our top-level\n  <code class=\"cw qm qn qo qp b\">\n   LLMPredictor\n  </code>\n  still exists but is less user-facing (and we might deprecate in the future). Also, you can still use LangChain LLMs through our\n  <code class=\"cw qm qn qo qp b\">\n   LangChainLLM\n  </code>\n  class.\n </p>\n <h2>\n  <strong>\n   Resources\n  </strong>\n </h2>\n <p>\n  All of our notebooks have by default been updated to use our native OpenAI LLM integration. Here\u2019s some resources to show both the LLM abstraction on its own as well as how it can be used in the overall system:\n </p>\n <ul>\n  <li>\n   <a href=\"https://github.com/jerryjliu/llama_index/blob/main/docs/examples/llm/openai.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    OpenAI LLM\n   </a>\n  </li>\n  <li>\n   <a href=\"https://github.com/jerryjliu/llama_index/blob/main/docs/examples/llm/llm_predictor.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Using LLM in LLMPredictor\n   </a>\n  </li>\n  <li>\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html#example-changing-the-underlying-llm\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Changing LLM within Index/Query Engine\n   </a>\n  </li>\n  <li>\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html#example-using-a-custom-llm-model-advanced\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Defining a custom LLM Model\n   </a>\n  </li>\n </ul>\n <h1>\n  Standalone Response Synthesis Modules\n </h1>\n <h2>\n  <strong>\n   Context\n  </strong>\n </h2>\n <p>\n  In any RAG system, there is retrieval and there is synthesis. The responsibility of the synthesis component is to take in incoming context as input, and synthesize a response using the LLM.\n </p>\n <p>\n  Fundamentally, the synthesis module needs to synthesize a response over\n  <strong>\n   any\n  </strong>\n  context list, regardless of how long that context list is. This is essentially \u201cboilerplate\u201d that an LLM developer /\n  <a href=\"https://www.latent.space/p/ai-engineer\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   \u201cAI engineer\u201d\n  </a>\n  must write.\n </p>\n <p>\n  We had this as an internal abstraction in LlamaIndex before (as a\n  <code class=\"cw qm qn qo qp b\">\n   ResponseSynthesizer\n  </code>\n  ), but the external-facing UX was unfriendly to users. The actual piece that gathered responses (the\n  <code class=\"cw qm qn qo qp b\">\n   ResponseBuilder\n  </code>\n  ) was hard to customize, and the\n  <code class=\"cw qm qn qo qp b\">\n   ResponseSynthesizer\n  </code>\n  itself was adding an extra unnecessary layer.\n </p>\n <p>\n  Now we have a set of standalone modules that you can easily import. Previously, when you set the\n  <code class=\"cw qm qn qo qp b\">\n   response_mode\n  </code>\n  in the query engine, these were being setup for you. Now they are more directly available and user-facing.\n </p>\n <p>\n  Here\u2019s a list of all the new\n  <code class=\"cw qm qn qo qp b\">\n   Response Synthesiszer\n  </code>\n  modules available from\n  <code class=\"cw qm qn qo qp b\">\n   llama_index.response_synthesizer\n  </code>\n  :\n </p>\n <ul>\n  <li>\n   <code class=\"cw qm qn qo qp b\">\n    Refine\n   </code>\n   - Query an LLM, sending each text chunk individually. After the first LLM call, the existing answer is also sent to the LLM for updating and refinement using the next text chunk.\n  </li>\n  <li>\n   <code class=\"cw qm qn qo qp b\">\n    Accumulate\n   </code>\n   - Query an LLM with the same prompt across multiple text chunks, and return a formatted list of responses\n  </li>\n  <li>\n   <code class=\"cw qm qn qo qp b\">\n    Compact\n   </code>\n   - The same as\n   <code class=\"cw qm qn qo qp b\">\n    Refine\n   </code>\n   , but puts as much text as possible into each LLM call\n  </li>\n  <li>\n   <code class=\"cw qm qn qo qp b\">\n    CompactAndAccumulate\n   </code>\n   - The same as\n   <code class=\"cw qm qn qo qp b\">\n    Accumulate\n   </code>\n   , but puts as much text as possible\n  </li>\n  <li>\n   <code class=\"cw qm qn qo qp b\">\n    TreeSummarize\n   </code>\n   - Create a bottom-up summary from the provided text chunks, and return the root summary\n  </li>\n  <li>\n   <code class=\"cw qm qn qo qp b\">\n    SimpleSummarize\n   </code>\n   - Combine and truncate all text chunks, and summarize in a single LLM call\n  </li>\n </ul>\n <h2>\n  <strong>\n   Usage\n  </strong>\n </h2>\n <p>\n  As detailed above, you can directly set a response synthesizer in a query engine, or let the\n  <code class=\"cw qm qn qo qp b\">\n   response_mode\n  </code>\n  fetch the relevant response synthesizer.\n </p>\n <p>\n  Furthermore though, you can directly call and use these synthesizers as low level modules. Here\u2019s a small example:\n </p>\n <pre><span class=\"qy ov gt qp b bf qz ra l rb rc\" id=\"950d\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> ServiceContext\n<span class=\"hljs-keyword\">from</span> llama_index.response_synthesizers <span class=\"hljs-keyword\">import</span> CompactAndRefine\n\n<span class=\"hljs-comment\"># you can also configure the text_qa_template, refine_template, </span>\n<span class=\"hljs-comment\"># and streaming toggle from here</span>\nresponse_synthesizer = CompactAndRefine(\n  service_context=service_context.from_defaults()\n)\nresponse = response_synthesizer.get_response(\n <span class=\"hljs-string\">\"What skills does Bob have?\"</span>,\n  text_chunks=[<span class=\"hljs-string\">\" ...\"</span>]  <span class=\"hljs-comment\"># here would be text, hopefully about Bob's skills</span>\n)</span></pre>\n <h2>\n  Resources\n </h2>\n <p>\n  Here are some additional notebooks showing how to use\n  <code class=\"cw qm qn qo qp b\">\n   get_response_synthesizer\n  </code>\n  :\n </p>\n <ul>\n  <li>\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/guides/primer/usage_pattern.html#low-level-api\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Low-level API Usage Pattern\n   </a>\n  </li>\n  <li>\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/query_engine/CustomRetrievers.html#plugin-retriever-into-query-engine\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Custom Retrievers\n   </a>\n  </li>\n </ul>\n <h1>\n  Metadata Management Capabilities\n </h1>\n <p>\n  If you want to have good performance in any LLM application over your data (including a RAG pipeline), you need to make sure that your documents actually contain relevant context for the query. One way to do this is to add proper metadata, both at the document-level and after the documents have been parsed into text chunks (into Nodes).\n </p>\n <p>\n  We allow you to define metadata fields within a Document, customize the ID, and also customize the metadata text/format for LLM usage and embedding usage.\n </p>\n <p>\n  <strong>\n   Defining Metadata Fields\n  </strong>\n </p>\n <pre><span class=\"qy ov gt qp b bf qz ra l rb rc\" id=\"95fa\">document = Document(\n    text='text', \n    metadata={\n        'filename': '<span class=\"hljs-symbol\">&amp;lt;</span>doc_file_name<span class=\"hljs-symbol\">&amp;gt;</span>', \n        'category': '<span class=\"hljs-symbol\">&amp;lt;</span>category<span class=\"hljs-symbol\">&amp;gt;</span>'\n    }\n)</span></pre>\n <p>\n  <strong>\n   Customizing the ID\n  </strong>\n </p>\n <p>\n  The ID of each document can be set multiple ways\n </p>\n <ul>\n  <li>\n   Within the constructor:\n   <code class=\"cw qm qn qo qp b\">\n    document = Document(text=\"text\", doc_id_=\"id\")\n   </code>\n  </li>\n  <li>\n   After constructing the object:\n   <code class=\"cw qm qn qo qp b\">\n    document.doc_id = \"id\"\n   </code>\n  </li>\n  <li>\n   Automatically using the\n   <code class=\"cw qm qn qo qp b\">\n    SimpleDirectoryReader\n   </code>\n   :\n   <code class=\"cw qm qn qo qp b\">\n    SimpleDirectoryReader(filename_as_id=True).load_data()\n   </code>\n  </li>\n </ul>\n <p>\n  <strong>\n   Customizing the Metadata Text for LLMs and Embeddings\n  </strong>\n </p>\n <p>\n  As seen above, you can set metadata containing useful information. By default, all the metadata will be seen by the embedding model and the LLM. However, sometimes you may want to only include data to bias embeddings, or only include data as extra information for the LLM!\n </p>\n <p>\n  With the new\n  <code class=\"cw qm qn qo qp b\">\n   Document\n  </code>\n  objects, you can configure what each metadata field is used for:\n </p>\n <pre><span class=\"qy ov gt qp b bf qz ra l rb rc\" id=\"62bd\">document = Document(\n    text='text', \n    metadata={\n        'filename': '<span class=\"hljs-symbol\">&amp;lt;</span>doc_file_name<span class=\"hljs-symbol\">&amp;gt;</span>', \n        'category': '<span class=\"hljs-symbol\">&amp;lt;</span>category<span class=\"hljs-symbol\">&amp;gt;</span>'\n    },\n    excluded_llm_metadata_keys=['filename', 'category'],\n    excluded_embed_metadata_keys=['filename']\n)</span></pre>\n <p>\n  <strong>\n   Customizing the Metadata Format Template\n  </strong>\n </p>\n <p>\n  When the metadata is inserted into the text, it follows a very specific format. This format is configurable at multiple levels:\n </p>\n <pre><span class=\"qy ov gt qp b bf qz ra l rb rc\" id=\"21c6\">from llama_index.schema import MetadataMode\n\ndocument = Document(\n  text='text',\n  metadata={\"key\": \"val\"},\n  metadata_seperator=\"::\",\n    metadata_template=\"{key}=<span class=\"hljs-symbol\">&amp;gt;</span>{value}\",\n    text_template=\"Metadata: {metadata_str}\\\\n-----\\\\nContent: {content}\"\n)\n# available modes are ALL, NONE, LLM, and EMBED\nprint(document.get_content(metadata_mode=MetadataMode.ALL))\n# output:\n# Metadata: key=<span class=\"hljs-symbol\">&amp;gt;</span>val\n# -----\n# text</span></pre>\n <p>\n  Please check out this guide for more\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_documents.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   details\n  </a>\n  !\n </p>\n <h1>\n  Full List of Breaking Changes\n </h1>\n <h2>\n  Response Synthesis + Node Postprocessors\n </h2>\n <p>\n  The\n  <code class=\"cw qm qn qo qp b\">\n   ResponseSynthesizer\n  </code>\n  object class has been removed, and replaced with\n  <code class=\"cw qm qn qo qp b\">\n   get_response_synthesizer\n  </code>\n  . In addition to this, node post processors are now handled by the query engine directly, and the old\n  <code class=\"cw qm qn qo qp b\">\n   SentenceEmbeddingOptimizer\n  </code>\n  has been switched to become a node post processor instance itself.\n </p>\n <p>\n  Here is an example of the required migration to use all moved features.\n </p>\n <p>\n  <strong>\n   Old\n  </strong>\n </p>\n <pre><span class=\"qy ov gt qp b bf qz ra l rb rc\" id=\"0837\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> (\n    VectorStoreIndex,\n    ResponseSynthesizer,\n)\n<span class=\"hljs-keyword\">from</span> llama_index.indices.postprocessor <span class=\"hljs-keyword\">import</span> SimilarityPostprocessor\n<span class=\"hljs-keyword\">from</span> llama_index.optimizers <span class=\"hljs-keyword\">import</span> SentenceEmbeddingOptimizer\n<span class=\"hljs-keyword\">from</span> llama_index.query_engine <span class=\"hljs-keyword\">import</span> RetrieverQueryEngine\n\ndocuments = ...\n<span class=\"hljs-comment\"># build index</span>\nindex = VectorStoreIndex.from_documents(documents)\n<span class=\"hljs-comment\"># configure retriever</span>\nretriever = index.as_retriever(\n   similarity_top_k=<span class=\"hljs-number\">3</span>\n)\n<span class=\"hljs-comment\"># configure response synthesizer</span>\nresponse_synthesizer = ResponseSynthesizer.from_args(\n   response_mode=<span class=\"hljs-string\">\"tree_summarize\"</span>,\n    node_postprocessors=[\n        SimilarityPostprocessor(similarity_cutoff=<span class=\"hljs-number\">0.7</span>),\n        SentenceEmbeddingOptimizer(percentile_cutoff=<span class=\"hljs-number\">0.5</span>)\n    ]\n)\n<span class=\"hljs-comment\"># assemble query engine</span>\nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    response_synthesizer=response_synthesizer,\n)</span></pre>\n <p>\n  <strong>\n   New\n  </strong>\n </p>\n <pre><span class=\"qy ov gt qp b bf qz ra l rb rc\" id=\"d66c\">from llama_index import (\n    VectorStoreIndex,\n    get_response_synthesizer,\n)\nfrom llama_index.indices.postprocessor import (\n    SimilarityPostprocessor,\n    SentenceEmbeddingOptimizer\n)\n\ndocuments = ...\n# build index\nindex = VectorStoreIndex.from_documents(documents)\n# configure response synthesizer\nresponse_synthesizer = get_response_synthesizer(\n   response_mode=\"tree_summarize\",\n)\n# assemble query engine\nquery_engine = index.as_query_engine(\n  similarity_top_k=3,\n    response_synthesizer=response_synthesizer,\n    node_postprocessors=[\n        SimilarityPostprocessor(similarity_cutoff=0.7),\n        SentenceEmbeddingOptimizer(percentile_cutoff=0.5)\n    ]\n)</span></pre>\n <h2>\n  LLM Predictor\n </h2>\n <p>\n  While introducing a new LLM abstraction, we cleaned up the LLM Predictor and removed several deprecated functionalities:\n </p>\n <ol>\n  <li>\n   Remove\n   <code class=\"cw qm qn qo qp b\">\n    ChatGPTLLMPredictor\n   </code>\n   and\n   <code class=\"cw qm qn qo qp b\">\n    HuggingFaceLLMPredictor\n   </code>\n   (use\n   <code class=\"cw qm qn qo qp b\">\n    OpenAI\n   </code>\n   and\n   <code class=\"cw qm qn qo qp b\">\n    HuggingFaceLLM\n   </code>\n   instead, see\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/how_to/customization/llms_migration_guide.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    migration guide\n   </a>\n   )\n  </li>\n  <li>\n   Remove support for setting\n   <code class=\"cw qm qn qo qp b\">\n    cache\n   </code>\n   via\n   <code class=\"cw qm qn qo qp b\">\n    LLMPredictor\n   </code>\n   constructor.\n  </li>\n  <li>\n   Removed\n   <code class=\"cw qm qn qo qp b\">\n    llama_index.token_counter.token_counter\n   </code>\n   module (see\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/how_to/callbacks/token_counting_migration.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    migration guide\n   </a>\n   ).\n  </li>\n </ol>\n <p>\n  Now, the LLM Predictor class is mostly a lightweight wrapper on top of the\n  <code class=\"cw qm qn qo qp b\">\n   LLM\n  </code>\n  abstraction that handles:\n </p>\n <ul>\n  <li>\n   conversion of prompts to the string or chat message input format expected by the LLM\n  </li>\n  <li>\n   logging of prompts and responses to a callback manager\n  </li>\n </ul>\n <p>\n  We advice users to configure the\n  <code class=\"cw qm qn qo qp b\">\n   llm\n  </code>\n  argument in\n  <code class=\"cw qm qn qo qp b\">\n   ServiceContext\n  </code>\n  directly (instead of creating LLM Predictor).\n </p>\n <h2>\n  Chat Engine\n </h2>\n <p>\n  We updated the\n  <code class=\"cw qm qn qo qp b\">\n   BaseChatEngine\n  </code>\n  interface to take in a\n  <code class=\"cw qm qn qo qp b\">\n   List[ChatMessage]]\n  </code>\n  for the\n  <code class=\"cw qm qn qo qp b\">\n   chat_history\n  </code>\n  instead of tuple of strings. This makes the data model consistent with the input/output of the\n  <code class=\"cw qm qn qo qp b\">\n   LLM\n  </code>\n  , also more flexibility to specify consecutive messages with the same role.\n </p>\n <p>\n  <strong>\n   Old\n  </strong>\n </p>\n <pre><span class=\"qy ov gt qp b bf qz ra l rb rc\" id=\"5a50\">engine = SimpleChatEngine.<span class=\"hljs-built_in\">from_defaults</span>(\n\tchat_history=[(<span class=\"hljs-string\">\"human message\"</span>, <span class=\"hljs-string\">\"assistant message\"</span>)],\n)\nresponse = engine.<span class=\"hljs-built_in\">chat</span>(<span class=\"hljs-string\">\"new human message\"</span>)</span></pre>\n <p>\n  <strong>\n   New\n  </strong>\n </p>\n <pre><span class=\"qy ov gt qp b bf qz ra l rb rc\" id=\"bdfc\">engine = SimpleChatEngine.from_defaults(\n    service_context=mock_service_context,\n    chat_history=[\n        ChatMessage(role=MessageRole.USER, content=\"human message\"),\n        ChatMessage(role=MessageRole.ASSISTANT, content=\"assistant message\"),\n    ],\n)\nresponse = engine.chat(\"new human message\")</span></pre>\n <p>\n  We also exposed\n  <code class=\"cw qm qn qo qp b\">\n   chat_history\n  </code>\n  state as a property and supported overriding\n  <code class=\"cw qm qn qo qp b\">\n   chat_history\n  </code>\n  in\n  <code class=\"cw qm qn qo qp b\">\n   chat\n  </code>\n  and\n  <code class=\"cw qm qn qo qp b\">\n   achat\n  </code>\n  endpoints.\n </p>\n <h2>\n  Prompt Helper\n </h2>\n <p>\n  We removed some previously deprecated arguments:\n  <code class=\"cw qm qn qo qp b\">\n   max_input_size\n  </code>\n  ,\n  <code class=\"cw qm qn qo qp b\">\n   embedding_limit\n  </code>\n  ,\n  <code class=\"cw qm qn qo qp b\">\n   max_chunk_overlap\n  </code>\n </p>\n <h1>\n  Conclusion\n </h1>\n <p>\n  At a high-level, we hope that these changes continue to enable bottoms-up development of LLM applications over your data. We first encourage you to play around with our new modules on their own to get a sense what they do and where they can be used. Once you\u2019re ready to use them in more advanced workflows, then you can figure out how to use our outer components to setup a sophisticated RAG pipeline.\n </p>\n <p>\n  As always, our\n  <a href=\"https://github.com/jerryjliu/llama_index\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   repo\n  </a>\n  is here and our\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   docs\n  </a>\n  are here. If you have thoughts/comments, don\u2019t hesitate to hop in our\n  <a href=\"https://discord.gg/dGcwcsnxhU\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Discord\n  </a>\n  !\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 23634, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e84ce598-1e29-4f9b-b8b8-d1a63678ac26": {"__data__": {"id_": "e84ce598-1e29-4f9b-b8b8-d1a63678ac26", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim.html", "file_name": "llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim.html", "file_type": "text/html", "file_size": 3079, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim.html", "file_name": "llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim.html", "file_type": "text/html", "file_size": 3079, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "e4ea5b1da0e94a0545a315b7fdfb296c598604d9b70d36357da9e7efd6825fa8", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Generative AI is rapidly transforming the global economy. Enterprises are increasingly looking to adopt generative AI to drive business transformation, but face challenges around protecting IP, ensuring security and compliance, and moving smoothly from proof of concept to production.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  That\u2019s why LlamaIndex is excited to announce that it is integrated with\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://nvidianews.nvidia.com/news/generative-ai-microservices-for-developers\" rel=\"noreferrer noopener\">\n   NVIDIA NIM inference microservices\n  </a>\n  to help enterprises seamlessly deploy generative AI at scale. NVIDIA NIM, part of the\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\" rel=\"noreferrer noopener\">\n   NVIDIA AI Enterprise\n  </a>\n  software platform, optimizes inference on more than two dozen popular AI models from NVIDIA and its partner ecosystem.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  LlamaIndex is an open-source tool for connecting your data to LLMs and extracting valuable insights. By integrating NVIDIA NIM runtimes with LlamaIndex\u2019s data connection capabilities, enterprises will be able to:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Connect generative AI models hosted using NVIDIA NIM to their own proprietary data sources, allowing them to generate accurate model outputs while keeping sensitive data secure\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Search across and extract insights from both structured and unstructured enterprise data to enhance the knowledge and accuracy of AI models\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Build data-enriched generative AI applications for use cases like enterprise search, question answering, analytics, and more\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  For developers, using NVIDIA NIM with LlamaIndex provides a seamless path from experimentation to production. They can access the NIM microservices from the newly launched NVIDIA API catalog to quickly build AI applications using industry-standard protocols, and then easily transition those applications to work on their self-hosted NVIDIA NIM instance for enhanced security, customization, and cost effectiveness at scale.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The combination of NVIDIA NIM for optimized model inference and LlamaIndex\u2019s data connection helps unlock the full potential of enterprise-level generative AI. We look forward to seeing the innovative applications that emerge from this integration as more organizations embrace AI to transform their business.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  You can expect an in-depth technical blog post about how to use NVIDIA NIM and LlamaIndex very soon!\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3072, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c71a83e3-f32a-4445-9ee3-cad21e9a0778": {"__data__": {"id_": "c71a83e3-f32a-4445-9ee3-cad21e9a0778", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-and-transformers-agents-67042ee1d8d6.html", "file_name": "llamaindex-and-transformers-agents-67042ee1d8d6.html", "file_type": "text/html", "file_size": 14762, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-and-transformers-agents-67042ee1d8d6.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-and-transformers-agents-67042ee1d8d6.html", "file_name": "llamaindex-and-transformers-agents-67042ee1d8d6.html", "file_type": "text/html", "file_size": 14762, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "fe74273127808f5e41607dcaaf0be0a7dcb49e31b7e7ab731970686a27af9f43", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h1>\n  Summary\n </h1>\n <p>\n  Agents are a popular use-case for Large Language Models (LLMs), typically provide a structure that enables LLMs to make decisions, use tools, and accomplish tasks. These agents can take many forms, like the fully-autonomous versions seen with\n  <a href=\"https://github.com/Significant-Gravitas/Auto-GPT\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Auto-GPT\n  </a>\n  , to more controlled implementations like\n  <a href=\"https://python.langchain.com/en/latest/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Langchain\n  </a>\n  Agents. With the recent release of\n  <a href=\"https://huggingface.co/docs/transformers/transformers_agents\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Transformers Agents\n  </a>\n  , we showcase how\n  <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaIndex\n  </a>\n  continues to be a useful tool for agents, by augmenting their existing image-generator tool. Using an vector index created from 10K\n  <a href=\"https://huggingface.co/datasets/poloclub/diffusiondb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   DiffusionDB\n  </a>\n  prompts, the Text2Image Prompt Assistant tool we created can re-write prompts to generate more beautiful images. Full source code is available in the\n  <a href=\"https://huggingface.co/spaces/llamaindex/text2image_prompt_assistant/tree/main\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Hugging Face Space for the tool\n  </a>\n  , and a\n  <a href=\"https://colab.research.google.com/drive/1r0t423LTkCYi5fGLrSfTdtC0DzKuU-zq?usp=sharing\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   colab notebook\n  </a>\n  is available as a usage walkthrough.\n </p>\n <h1>\n  Creating the Tool\n </h1>\n <p>\n  Transformers Agents come with a variety of per-configured tools that leverage the vast amounts of open-source models hosted on Hugging Face-Hub. Furthermore, additional tools can be created and shared by simply publishing a new Hugging Face Space with the proper tool setup.\n </p>\n <p>\n  To create a tool, your code simply needs a\n  <code class=\"cw pk pl pm pn b\">\n   tool_config.json\n  </code>\n  file that describes the tool, as well as a file containing the implementation of your tool. While the documentation was a little fuzzy for this part, we eventually were able to use\n  <a href=\"https://huggingface.co/huggingface-tools\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   the implementation of existing custom tools\n  </a>\n  as the framework for our own.\n </p>\n <p>\n  To enable LlamaIndex to write text-to-image prompts, we need a way to show the LLM what examples of good prompts look like. To do this, we indexed 10K random text-to-image prompts from DiffusionDB.\n </p>\n <pre><span class=\"pw nn gt pn b bf px py l pz qa\" id=\"1743\"><span class=\"hljs-keyword\">from</span> datasets <span class=\"hljs-keyword\">import</span> load_dataset\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> VectorStoreIndex, Document\n\n<span class=\"hljs-comment\"># downloads a LOT of data</span>\ndataset = load_dataset(<span class=\"hljs-string\">'poloclub/diffusiondb'</span>, <span class=\"hljs-string\">'2m_random_10k'</span>)\n\ndocuments = []\n<span class=\"hljs-keyword\">for</span> sample <span class=\"hljs-keyword\">in</span> dataset[<span class=\"hljs-string\">'train'</span>]:\n    documents.append(Document(sample[<span class=\"hljs-string\">'prompt'</span>]))\n\n<span class=\"hljs-comment\"># create index</span>\nindex = VectorStoreIndex.from_documents(documents)\n\n<span class=\"hljs-comment\"># store index</span>\nindex.storage_context.persist(persist_dir=<span class=\"hljs-string\">\"./storage\"</span>)</span></pre>\n <p>\n  To get LlamaIndex to write prompts using examples, we need to customize the prompt templates a bit. You can see the final prompt templates and how to use them below:\n </p>\n <pre><span class=\"pw nn gt pn b bf px py l pz qa\" id=\"83aa\">text_qa_template = <span class=\"hljs-title class_\">Prompt</span>(\n    <span class=\"hljs-string\">\"Examples of text-to-image prompts are below: \\n\"</span>\n    <span class=\"hljs-string\">\"---------------------\\n\"</span>\n    <span class=\"hljs-string\">\"{context_str}\"</span>\n    <span class=\"hljs-string\">\"\\n---------------------\\n\"</span>\n    <span class=\"hljs-string\">\"Given the existing examples of text-to-image prompts, \"</span>\n    <span class=\"hljs-string\">\"write a new text-to-image prompt in the style of the examples, \"</span>\n    <span class=\"hljs-string\">\"by re-wording the following prompt to match the style of the above examples: {query_str}\\n\"</span>\n)\n\n\nrefine_template = <span class=\"hljs-title class_\">Prompt</span>(\n    <span class=\"hljs-string\">\"The initial prompt is as follows: {query_str}\\n\"</span>\n    <span class=\"hljs-string\">\"We have provided an existing text-to-image prompt based on this query: {existing_answer}\\n\"</span>\n    <span class=\"hljs-string\">\"We have the opportunity to refine the existing prompt \"</span>\n    <span class=\"hljs-string\">\"(only if needed) with some more relevant examples of text-to-image prompts below.\\n\"</span>\n    <span class=\"hljs-string\">\"------------\\n\"</span>\n    <span class=\"hljs-string\">\"{context_msg}\\n\"</span>\n    <span class=\"hljs-string\">\"------------\\n\"</span>\n    <span class=\"hljs-string\">\"Given the new examples of text-to-image prompts, refine the existing text-to-image prompt to better \"</span>\n    <span class=\"hljs-string\">\"statisfy the required style. \"</span>\n    <span class=\"hljs-string\">\"If the context isn't useful, or the existing prompt is good enough, return the existing prompt.\"</span>\n)\n\nquery_engine = index.<span class=\"hljs-title function_\">as_query_engine</span>(\n    text_qa_template=text_qa_template, \n    refine_template=refine_template\n)\n\nresponse = query_engine.<span class=\"hljs-title function_\">query</span>(<span class=\"hljs-string\">\"Draw me a picture of a happy dog\"</span>)</span></pre>\n <h2>\n  Snag #1\n </h2>\n <p>\n  One main drawback of Transformers Agents currently is that they will only pick one tool to solve each prompt. So if we want to augment the image-generator tool, we need to replace it! In our tool implementation, we actually load the original image-generator tool and call it after running LlamaIndex to generate a new text-to-image prompt.\n </p>\n <h2>\n  Snag #2\n </h2>\n <p>\n  The next bump in our journey is how Hugging Face downloads tools from the space. Initially, it only downloading the\n  <code class=\"cw pk pl pm pn b\">\n   tool_config.json\n  </code>\n  file and the source code for the tool. But we also need to download the prompts we spent time indexing!\n </p>\n <p>\n  To get around this, during the\n  <code class=\"cw pk pl pm pn b\">\n   setup()\n  </code>\n  of the tool, we call\n  <code class=\"cw pk pl pm pn b\">\n   hf_hub_download()\n  </code>\n  to download the files we need to load the index.\n </p>\n <h2>\n  Back on Track\n </h2>\n <p>\n  With the index created and the general processes figured out, the actual tool implementation is fairly straightforward.\n </p>\n <pre><span class=\"pw nn gt pn b bf px py l pz qa\" id=\"8622\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">Text2ImagePromptAssistant</span>(<span class=\"hljs-title class_ inherited__\">Tool</span>):\n    \n    inputs = [<span class=\"hljs-string\">'text'</span>]\n    outputs = [<span class=\"hljs-string\">'image'</span>]\n    description = PROMPT_ASSISTANT_DESCRIPTION\n    \n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, *args, openai_api_key=<span class=\"hljs-string\">''</span>, model_name=<span class=\"hljs-string\">'text-davinci-003'</span>, temperature=<span class=\"hljs-number\">0.3</span>, verbose=<span class=\"hljs-literal\">False</span>, **hub_kwargs</span>):\n        <span class=\"hljs-built_in\">super</span>().__init__()\n        os.environ[<span class=\"hljs-string\">'OPENAI_API_KEY'</span>] = openai_api_key\n        <span class=\"hljs-keyword\">if</span> model_name == <span class=\"hljs-string\">'text-davinci-003'</span>:\n            llm = OpenAI(model_name=model_name, temperature=temperature)\n        <span class=\"hljs-keyword\">elif</span> model_name <span class=\"hljs-keyword\">in</span> (<span class=\"hljs-string\">'gpt-3.5-turbo'</span>, <span class=\"hljs-string\">'gpt-4'</span>):\n            llm = ChatOpenAI(model_name=model_name, temperature=temperature)\n        <span class=\"hljs-keyword\">else</span>:\n            <span class=\"hljs-keyword\">raise</span> ValueError(\n                <span class=\"hljs-string\">f\"<span class=\"hljs-subst\">{model_name}</span> is not supported, please choose one \"</span>\n                <span class=\"hljs-string\">\"of 'text-davinci-003', 'gpt-3.5-turbo', or 'gpt-4'.\"</span>\n            )\n        service_context = ServiceContext.from_defaults(llm_predictor=LLMPredictor(llm=llm))\n        set_global_service_context(service_context)\n        \n        self.storage_path = os.path.dirname(__file__)\n        self.verbose = verbose\n        self.hub_kwargs = hub_kwargs\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">setup</span>(<span class=\"hljs-params\">self</span>):\n        hf_hub_download(repo_id=<span class=\"hljs-string\">\"llamaindex/text2image_prompt_assistant\"</span>, filename=<span class=\"hljs-string\">\"storage/vector_store.json\"</span>, repo_type=<span class=\"hljs-string\">\"space\"</span>, local_dir=self.storage_path)\n        hf_hub_download(repo_id=<span class=\"hljs-string\">\"llamaindex/text2image_prompt_assistant\"</span>, filename=<span class=\"hljs-string\">\"storage/index_store.json\"</span>, repo_type=<span class=\"hljs-string\">\"space\"</span>, local_dir=self.storage_path)\n        hf_hub_download(repo_id=<span class=\"hljs-string\">\"llamaindex/text2image_prompt_assistant\"</span>, filename=<span class=\"hljs-string\">\"storage/docstore.json\"</span>, repo_type=<span class=\"hljs-string\">\"space\"</span>, local_dir=self.storage_path)\n        \n        self.index = load_index_from_storage(StorageContext.from_defaults(persist_dir=os.path.join(self.storage_path, <span class=\"hljs-string\">\"storage\"</span>)))\n        self.query_engine = self.index.as_query_engine(similarity_top_k=<span class=\"hljs-number\">5</span>, text_qa_template=text_qa_template, refine_template=refine_template)\n        \n        <span class=\"hljs-comment\"># setup the text-to-image tool too</span>\n        self.text2image = load_tool(<span class=\"hljs-string\">'huggingface-tools/text-to-image'</span>)\n        self.text2image.setup()\n\n        self.initialized = <span class=\"hljs-literal\">True</span>\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__call__</span>(<span class=\"hljs-params\">self, prompt</span>):\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> self.is_initialized:\n            self.setup()\n\n        better_prompt = <span class=\"hljs-built_in\">str</span>(self.query_engine.query(prompt)).strip()\n        \n        <span class=\"hljs-keyword\">if</span> self.verbose:\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">'==New prompt generated by LlamaIndex=='</span>, flush=<span class=\"hljs-literal\">True</span>)\n            <span class=\"hljs-built_in\">print</span>(better_prompt, <span class=\"hljs-string\">'\\n'</span>, flush=<span class=\"hljs-literal\">True</span>)\n\n        <span class=\"hljs-keyword\">return</span> self.text2image(better_prompt)</span></pre>\n <h1>\n  Running the Tool\n </h1>\n <p>\n  With the tool setup, we can now test it with an actual agent! For testing, we used an\n  <code class=\"cw pk pl pm pn b\">\n   OpenAIAgent\n  </code>\n  with the\n  <code class=\"cw pk pl pm pn b\">\n   text-davinci-003\n  </code>\n  model. When asked to draw a picture of a mountain, this is what we got:\n </p>\n <pre><span class=\"pw nn gt pn b bf px py l pz qa\" id=\"41b0\"><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span> OpenAiAgent\nagent = OpenAiAgent(model=<span class=\"hljs-string\">\"text-davinci-003\"</span>, api_key=<span class=\"hljs-string\">\"your_api_key\"</span>)\n\nagent.run(<span class=\"hljs-string\">\"Draw me a picture a mountain.\"</span>)</span></pre>\n <figure>\n  <figcaption class=\"qv fe qw qq qr qx qy be b bf z dt\">\n   The initial picture of mountains that the agent created.\n  </figcaption>\n </figure>\n <p>\n  As you can see, the picture looks alright. But, text-to-image prompts are somewhat of an art.\n </p>\n <p>\n  To use our new tool, we just need to replace the existing image-generator tool:\n </p>\n <pre><span class=\"pw nn gt pn b bf px py l pz qa\" id=\"0fd1\"><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span> load_tool\nprompt_assistant = load_tool(\n    <span class=\"hljs-string\">\"llamaindex/text2image_prompt_assistant\"</span>,\n    openai_api_key=<span class=\"hljs-string\">\"your_api_key\"</span>,\n    model_name=<span class=\"hljs-string\">'text-davinci-003'</span>,\n    temperature=<span class=\"hljs-number\">0.3</span>,  <span class=\"hljs-comment\"># increase or decrease this to control variation</span>\n    verbose=<span class=\"hljs-literal\">True</span>\n)\n\n<span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span> OpenAiAgent\nagent = OpenAiAgent(model=<span class=\"hljs-string\">\"text-davinci-003\"</span>, api_key=<span class=\"hljs-string\">\"your_api_key\"</span>)\n\n<span class=\"hljs-comment\"># replace the existing tool</span>\nagent.toolbox[<span class=\"hljs-string\">'image_generator'</span>] = prompt_assistant\n\nagent.run(<span class=\"hljs-string\">\"Draw me a picture a mountain.\"</span>)</span></pre>\n <p>\n  Using Our new LlamaIndex Prompt Assistant tool, we get a much more stylized result. In the terminal, we see the prompt was re-written as \u201ca majestic mountain peak, surrounded by lush greenery, with a stunning sunset in the background,\u201d which resulted in the following image:\n </p>\n <figure>\n  <figcaption class=\"qv fe qw qq qr qx qy be b bf z dt\">\n   Image generated by our Text2Image Prompt Assistant tool.\n  </figcaption>\n </figure>\n <p>\n  Looks great! With the temperature variable, we can control how varied the generated prompts become. With a temperature above zero, each prompt generated by LlamaIndex with the same agent prompt will be brand new!\n </p>\n <h1>\n  Conclusion\n </h1>\n <p>\n  In conclusion, we have demonstrated how LlamaIndex can be used to augment LLM agents, by implementing a Text2Image Prompt Assistant tool with a Transformers Agent. Using a vector database created from DiffusionDB, LlamaIndex can suggest better prompts when generating images.\n </p>\n <p>\n  Custom tools in Transformers Agents are easily distributed and shared using Hugging Face Spaces, and we are excited to see what other people build and share!\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 14757, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e5d98e6-d512-4b30-b531-d13042572fd3": {"__data__": {"id_": "3e5d98e6-d512-4b30-b531-d13042572fd3", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-and-weaviate-ba3ff1cbf5f4.html", "file_name": "llamaindex-and-weaviate-ba3ff1cbf5f4.html", "file_type": "text/html", "file_size": 9644, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-and-weaviate-ba3ff1cbf5f4.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-and-weaviate-ba3ff1cbf5f4.html", "file_name": "llamaindex-and-weaviate-ba3ff1cbf5f4.html", "file_type": "text/html", "file_size": 9644, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "49955bb0c3c828891d4bda70049037fbc761c15e630cf249857bb0bcc3cf1d1a", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  <strong>\n   Co-authors:\n  </strong>\n </p>\n <ul>\n  <li>\n   Jerry Liu (co-founder/CEO of LlamaIndex)\n  </li>\n  <li>\n   Erika Cardenas (Developer Advocate, Weaviate)\n  </li>\n </ul>\n <p>\n  While large language models (LLMs) like GPT-4 have impressive capabilities in generation and reasoning, they have limitations in terms of their ability to access and retrieve specific facts, figures, or contextually relevant information. A popular solution to this problem is setting up a retrieval-augmented generation (RAG) system: combine the language model with an external storage provider, and create an overall software system that can orchestrate the interactions with and between these components in order to create a \u201cchat with your data\u201d experience.\n </p>\n <p>\n  The combination of Weaviate and LlamaIndex provide the critical components needed to easily setup a powerful and reliable RAG stack, so that you can easily deliver powerful LLM-enabled experiences over your data, such as search engines, chatbots, and more. First, we can use Weaviate as the vector database that acts as the external storage provider. Next, we can use a powerful data framework such as LlamaIndex to help with data management and orchestration around Weaviate when building the LLM app.\n </p>\n <p>\n  In this blog post, we walk through an overview of LlamaIndex and some of the core data management and query modules. We then go through an initial demo notebook.\n </p>\n <p>\n  We\u2019re kicking off a new series to guide you on how to use LlamaIndex and Weaviate for your LLM applications.\n </p>\n <h1>\n  An Introduction to LlamaIndex\n </h1>\n <p>\n  LlamaIndex is a data framework for building LLM applications. It provides a comprehensive toolkit for ingestion, management, and querying of your external data so that you can use it with your LLM app.\n </p>\n <h2>\n  Data Ingestion\n </h2>\n <p>\n  On data ingestion, LlamaIndex offers connectors to 100+ data sources, ranging from different file formats (.pdf, .docx, .pptx) to APIs (Notion, Slack, Discord, etc.) to web scrapers (Beautiful Soup, Readability, etc.). These data connectors are primarily hosted on [LlamaHub](\n  <a href=\"https://llamahub.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://llamahub.ai/\n  </a>\n  ). This makes it easy for users to integrate data from their existing files and applications.\n </p>\n <h2>\n  Data Indexing\n </h2>\n <p>\n  Once the data is loaded, LlamaIndex offers the ability to index this data with a wide variety of data structures and storage integration options (including Weaviate). LlamaIndex supports indexing unstructured, semi-structured, and structured data. A standard way to index unstructured data is to split the source documents into text \u201cchunks\u201d, embed each chunk, and store each chunk/embedding in a vector database.\n </p>\n <h2>\n  Data Querying\n </h2>\n <p>\n  Once your data is ingested/stored, LlamaIndex provides the tools to define an advanced retrieval / query \u201cengine\u201d over your data. Our retriever constructs allow you to retrieve data from your knowledge base given an input prompt. A query engine construct allows you to define an interface that can take in an input prompt, and output a knowledge-augmented response \u2014 it can use retrieval and synthesis (LLM) modules under the hood.\n </p>\n <p>\n  Some examples of query engine \u201ctasks\u201d are given below, in rough order from easy to advanced:\n </p>\n <ul>\n  <li>\n   Semantic Search: Retrieve the top-k most similar items from the knowledge corpus by embedding similarity to the query, and synthesize a response over these contexts.\n  </li>\n  <li>\n   Structured Analytics: Convert natural language to a SQL query that can be executed\n  </li>\n  <li>\n   Query Decomposition over Documents: Break down a query into sub-questions, each over a subset of underlying documents. Each sub-question can be executed against its own query engine.\n  </li>\n </ul>\n <h1>\n  Demo Notebook Walkthrough\n </h1>\n <p>\n  Let\u2019s walk through a simple example of how LlamaIndex can be used with Weaviate to build a simple Question-Answering (QA) system over the Weaviate blogs!\n </p>\n <p>\n  The full code can be found in the\n  <a href=\"https://github.com/weaviate/recipes/blob/main/integrations/llamaindex/upload.py\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Weaviate recipes repo\n  </a>\n  .\n </p>\n <p>\n  The first step is to setup your Weaviate client. In this example, we connect to a local Weaviate instance through port\n  <code class=\"cw qn qo qp qq b\">\n   http://localhost:8080\n  </code>\n  :\n </p>\n <pre><span class=\"qz ov gt qq b bf ra rb l rc rd\" id=\"c677\"><span class=\"hljs-keyword\">import</span> weaviate\n# connect to your weaviate instance\nclient = weaviate.Client(<span class=\"hljs-string\">\"http://localhost:8080\"</span>)</span></pre>\n <p>\n  The next step is to ingest the Weaviate documentation and parse the documents into chunks. You can choose to use one of our many web page readers to scrape any website yourself \u2014 but luckily, the downloaded files are already readily available in the recipes repo.\n </p>\n <pre><span class=\"qz ov gt qq b bf ra rb l rc rd\" id=\"35ce\"><span class=\"hljs-keyword\">from</span> llama_index.node_parser <span class=\"hljs-keyword\">import</span> SimpleNodeParser\n<span class=\"hljs-comment\"># load the blogs in using the reader</span>\nblogs = SimpleDirectoryReader(<span class=\"hljs-string\">'./data'</span>).load_data()\n<span class=\"hljs-comment\"># chunk up the blog posts into nodes</span>\nparser = SimpleNodeParser()\nnodes = parser.get_nodes_from_documents(blogs)</span></pre>\n <p>\n  Here, we use the SimpleDirectoryReader to load in all documents from a given directory. We then use our\n  <code class=\"cw qn qo qp qq b\">\n   SimpleNodeParser\n  </code>\n  to chunk up the source documents into Node objects (text chunks).\n </p>\n <p>\n  The next step is to 1) define a\n  <code class=\"cw qn qo qp qq b\">\n   WeaviateVectorStore\n  </code>\n  , and 2) build a vector index over this vector store using LlamaIndex.\n </p>\n <pre><span class=\"qz ov gt qq b bf ra rb l rc rd\" id=\"90d3\"># construct vector store\nvector_store = WeaviateVectorStore(weaviate_client = client, index_name=\"BlogPost\", text_key=\"content\")\n# setting up the storage for the embeddings\nstorage_context = StorageContext.from_defaults(vector_store = vector_store)\n# set up the index\nindex = VectorStoreIndex(nodes, storage_context = storage_context)</span></pre>\n <p>\n  Our WeaviateVectorStore abstraction creates a central interface between our data abstractions and the Weaviate service. Note that the\n  <code class=\"cw qn qo qp qq b\">\n   VectorStoreIndex\n  </code>\n  is initialized from both the nodes and the storage context object containing the Weaviate vector store. During the initialization phase, the nodes are loaded into the vector store.\n </p>\n <p>\n  Finally, we can define a query engine on top of our index. This query engine will perform semantic search and response synthesis, and output an answer.\n </p>\n <pre><span class=\"qz ov gt qq b bf ra rb l rc rd\" id=\"b2e1\">\u200b\u200bquery_engine = index.as_query_engine()\nresponse = query_engine.query(<span class=\"hljs-string\">\"What is the intersection between LLMs and search?\"</span>)\n<span class=\"hljs-built_in\">print</span>(response)</span></pre>\n <p>\n  You should get an answer like the following:\n </p>\n <pre><span class=\"qz ov gt qq b bf ra rb l rc rd\" id=\"043b\">The intersection between LLMs and search is the ability to use LLMs to improve search capabilities, such as retrieval-augmented generation, query understanding, index construction, LLMs in re-ranking, and search result compression. LLMs can also be used to manage document updates, rank search results, and compress search results. LLMs can be used to prompt the language model to extract or formulate a question based on the prompt and then send that question to the search engine, or to prompt the model with a description of the search engine tool and how to use it with a special `[SEARCH]` token. LLMs can also be used to prompt the language model to rank search results according to their relevance with the query, and to classify the most likely answer span given a question and text passage as input.</span></pre>\n <h1>\n  Next Up in this Series\n </h1>\n <p>\n  This blog post shared an initial overview of the LlamaIndex and Weaviate integration. We covered an introduction to the toolkits offered in LlamaIndex and a notebook on how to build a simple QA engine over Weaviate\u2019s blog posts. Now that we have a baseline understanding, we will build on this by sharing more advanced guides soon. Stay tuned!\n </p>\n <h2>\n  What\u2019s next\n  <a href=\"https://weaviate.io/blog/llamaindex-and-weaviate#whats-next\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   \u200b\n  </a>\n </h2>\n <p>\n  Check out\n  <a href=\"https://weaviate.io/developers/weaviate/quickstart\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Getting Started with Weaviate\n  </a>\n  , and begin building amazing apps with Weaviate.\n </p>\n <p>\n  You can reach out to us on\n  <a href=\"https://weaviate.io/slack\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Slack\n  </a>\n  or\n  <a href=\"https://twitter.com/weaviate_io\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Twitter\n  </a>\n  , or\n  <a href=\"https://forum.weaviate.io/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   join the community forum\n  </a>\n  .\n </p>\n <p>\n  Weaviate is open source, and you can follow the project on\n  <a href=\"https://github.com/weaviate/weaviate\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   GitHub\n  </a>\n  . Don\u2019t forget to give us a \u2b50\ufe0f while you are there!\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 9603, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83e4306e-ad51-472d-8de8-5db4fe1e12e9": {"__data__": {"id_": "83e4306e-ad51-472d-8de8-5db4fe1e12e9", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.html", "file_name": "llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.html", "file_type": "text/html", "file_size": 9573, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.html", "file_name": "llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.html", "file_type": "text/html", "file_size": 9573, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "125159dff9a47594af61732245e01ed66002bd82a90131e08baeb0292958780a", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h1>\n  Introduction:\n </h1>\n <p>\n  In the world of IT and Software Development, knowledge transfer (KT) stands out as a big challenge. Whether it\u2019s new hires trying to understand their roles, folks on their notice periods aiming for a smooth handover, or the daily tasks of developers and product specialists adapting to ever-changing projects \u2014 the KT process often leads to stress and worry.\n </p>\n <p>\n  This gets more complicated with information spread out everywhere, the mix of new and old tech, and the fast pace of IT and Software Development projects. In this situation, broken bits of knowledge become the norm, causing delays, misunderstandings, and making learning harder.\n </p>\n <p>\n  But amidst these challenges, might there be a beacon of optimism shining through?\n </p>\n <p>\n  <a href=\"https://www.linkedin.com/in/vibhavagarwal5/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Vibhav\n  </a>\n  and I have developed a system that seamlessly organizes KT sessions. By leveraging personal images, we generate video explanations that are paired with individual code snippets, making the code far more comprehensible. Our innovative approach was recognized when we secured the First Prize at the Google Cloud, Searce, and LifeSight hackathon. With the combined strengths of LlamaIndex and D-ID, our aim is not just to consolidate information but also to simplify tasks and elevate the KT process. In doing so, we\u2019re transforming a daunting industry challenge into a straightforward and manageable endeavor.\n </p>\n <p>\n  Want to see how LlamaIndex plays a key role in this change?\n </p>\n <p>\n  Let\u2019s dive in together!\n </p>\n <h1>\n  Solution:\n </h1>\n <p>\n  The solution has four stages:\n </p>\n <h2>\n  Code Parsing:\n </h2>\n <ul>\n  <li>\n   Break down the code base into individual code snippets or blocks.\n  </li>\n </ul>\n <h2>\n  Summary and Explanation Generation with LlamaIndex:\n </h2>\n <ul>\n  <li>\n   Produce a comprehensive summary of the entire code base.\n  </li>\n  <li>\n   Create detailed explanations for each individual code block using LlamaIndex.\n  </li>\n </ul>\n <h2>\n  Video Creation with D-ID:\n </h2>\n <ul>\n  <li>\n   Generate videos using text-to-speech capabilities provided by D-ID.\n  </li>\n </ul>\n <h2>\n  Video-Code Integration:\n </h2>\n <ul>\n  <li>\n   Seamlessly stitch together the individual code blocks with their corresponding generated videos.\n  </li>\n </ul>\n <p>\n  Let\u2019s dive into each stage in detail.\n </p>\n <h2>\n  1. Code Parsing: Breaking Down the Code\n </h2>\n <figure>\n  <figcaption>\n   Code Parser\n  </figcaption>\n </figure>\n <p>\n  Understanding a code base starts with a high-level summary, but the true depth\n  lies in individual snippets or blocks. However, using entire code bases for\n  explanations can overwhelm language models like LLMs, causing them to either\n  exceed token limits or miss key details.\n </p>\n <p>\n  Our approach is simple yet efficient: break the code into digestible sections\n  like import statements, classes, initializer functions, and methods without\n  losing the code\u2019s flow. This segmentation is done through a dependency graph\n  approach, utilizing Python\u2019s\n  <code>\n   ast\n  </code>\n  library. By analyzing the code's structure, we can extract\n  classes, their docstrings, initializers, and other methods. This method not\n  only captures the essence of each segment but is also flexible, allowing for\n  further rules to extract additional code components.\n </p>\n <p>\n  The\n  <code>\n   code_parser\n  </code>\n  class embodies this strategy. It navigates the\n  code, distinguishing module-level functions from class-nested ones, and\n  arranges them systematically. The result? A granular yet comprehensive view of\n  the code, paving the way for precise and context-rich explanations.\n </p>\n <h2>\n  2. Summary and Explanation Generation with LlamaIndex\n </h2>\n <p>\n  <strong>\n   Producing a Comprehensive Summary:\n  </strong>\n </p>\n <p>\n  The initial step in understanding a code base is to grasp its overall essence.\n  This is achieved by generating a concise summary that gives a bird\u2019s-eye view\n  of the entire code. LlamaIndex\u2019s Summary\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/data_modules/index/index_guide.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Index\n  </a>\n  has been tailored for this exact task. In SummaryIndex, each block of code is\n  treated as a node. By inputting the structured blocks obtained from our code\n  parsing phase into SummaryIndex, we can produce a comprehensive snapshot that\n  serves as a summary of the entire code base.\n </p>\n <p>\n  <strong>\n   Detailed Explanations for Individual Code Blocks:\n  </strong>\n </p>\n <p>\n  With a general understanding established, the next step is to delve into the\n  finer details. Starting from import statements, progressing to functions, and\n  eventually diving into classes and initializer functions, every block gets its\n  due attention. Here, LlamaIndex\u2019s\n  <code>\n   accumulate\n  </code>\n  response mode is a valuable asset, providing in-depth\n  explanations for each block.\n </p>\n <p>\n  However, a challenge arises. While\n  <code>\n   accumulate\n  </code>\n  provides in-depth insights into each block, it can\n  occasionally miss the broader context offered by preceding blocks. To address\n  this limitation, we\u2019ve adopted a two-pronged approach. As depicted in the\n  subsequent architecture, we employ two SummaryIndices for this endeavor.\n </p>\n <ol>\n  <li>\n   We utilize the first SummaryIndex to generate a concise summary for each\n    block, treating each block as a Node in SummaryIndex.\n  </li>\n  <li>\n   For the second SummaaryIndex in the stack, we feed the summarized context\n    from one node into the next. This ensures every node benefits from the\n    context of its predecessor. We then harness the\n   <code>\n    accumulate\n   </code>\n   mode to provide detailed explanations, making\n    certain that every segment of the code is explained comprehensively,\n    preserving the broader perspective. The outcome? A deep, contextually rich\n    interpretation of each code section.\n  </li>\n </ol>\n <p>\n  Note: We utilized Google\u2019s PaLM API in conjunction with LlamaIndex to generate\n  summaries and explanations. Alternatively, models like GPT-3.5, GPT-4, or\n  other LLM\u2019s can be employed for this purpose.\n </p>\n <h2>\n  3. Video Creation with D-ID:\n </h2>\n <p>\n  After carefully crafting summaries and detailed explanations for each code\n  block, it\u2019s essential to convey this information in a captivating and\n  accessible manner. Videos, given their dynamic appeal, have the power to make\n  intricate code explanations clearer and more engaging. This is where D-ID\n  comes into play.\n </p>\n <p>\n  With the prowess of D-ID\u2019s cutting-edge technology, we\u2019re able to create\n  realistic videos where avatars \u2014 whether they\u2019re of us or another chosen\n  figure \u2014 articulate each code block. Now, what brings these avatars to life?\n  The answer lies in Microsoft\u2019s text-to-speech synthesizer. This tool takes our\n  detailed textual explanations and transforms them into natural, fluent speech.\n  Thus, with D-ID, we\u2019re not just generating video but also integrating audio,\n  culminating in a comprehensive and fluid video explanation.\n </p>\n <p>\n  To see this in action, let\u2019s take a look at a sample output.\n </p>\n <h2>\n  4. Video-Code Integration:\n </h2>\n <p>\n  After generating insightful videos with avatars elucidating the code and\n  having our individual code snippets ready, the next crucial step is to marry\n  these two elements. This fusion ensures that viewers receive an immersive\n  visual experience, where they can simultaneously watch the explanation and\n  observe the related code.\n </p>\n <p>\n  To achieve this, we employed the\n  <code>\n   carbon\n  </code>\n  library, which transforms our code snippets into visually\n  appealing images. These images, when presented side-by-side with our\n  explanatory videos, offer a clearer understanding of the code in focus. The\n  final touch is added with the\n  <code>\n   moviepy\n  </code>\n  library, which seamlessly\n  stitches the video and code images together, ensuring a smooth and integrated\n  visual flow. Below, you'll find a sample illustrating this compelling\n  combination.\n </p>\n <h1>\n  Final Automatic Knowledge Transfer (KT) Generated Video\n </h1>\n <p>\n  Following our detailed process, we\u2019ve crafted a KT video where Jerry explains\n  the ChatEngine code base of LlamaIndex. Watch the video below to see it all\n  come together!\n </p>\n <p>\n  Code Repository:\n  <a href=\"https://github.com/ravi03071991/KT_Generator\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://github.com/ravi03071991/KT_Generator\n  </a>\n </p>\n <h1>\n  Conclusion\n </h1>\n <p>\n  Through this post, we\u2019ve showcased the transformative potential of LlamaIndex\n  in creating Knowledge Transfer (KT) Videos for code bases. It\u2019s genuinely\n  remarkable to envision the advancements we\u2019re making in this space. The\n  methodology we\u2019ve adopted is language-neutral, allowing flexibility in\n  adapting to various code bases. With some tweaks to the code parsing phase, we\n  believe it\u2019s feasible to scale this to cover expansive code repositories\n  within organizations. Imagine a platform akin to YouTube, perhaps\n  <strong>\n   KodeTube(KT)\n  </strong>\n  , where an organization\u2019s entire codebase is\n  cataloged through explanatory videos. The horizon is bright with the\n  opportunities LlamaIndex brings, and we\u2019re thrilled about the journey ahead.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 9512, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a368acbf-5e8a-4f67-90fb-681d10d26379": {"__data__": {"id_": "a368acbf-5e8a-4f67-90fb-681d10d26379", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.html", "file_name": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.html", "file_type": "text/html", "file_size": 28117, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.html", "file_name": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.html", "file_type": "text/html", "file_size": 28117, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "b0968316e709257488eed7145e24313f73266550ff18cd724cec0c137e6fbe44", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h1>\n  Introduction\n </h1>\n <p>\n  Retrieving the appropriate chunks, nodes, or context is a critical aspect of building an efficient Retrieval-Augmented Generation (RAG) application. However, a vector or embedding-based search may not be effective for all types of user queries.\n </p>\n <p>\n  To address this,\n  <a href=\"https://weaviate.io/blog/hybrid-search-explained\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Hybrid search\n  </a>\n  combines both keyword-based methods (BM25) and vector (embedding) search techniques. Hybrid search has a specific parameter,\n  <code class=\"cw pq pr ps pt b\">\n   Alpha\n  </code>\n  to balance the weightage between keyword (BM25) and vector search in retrieving the right context for your RAG application. (alpha=0.0 - keyword search (BM25) and alpha=1.0 - vector search)\n </p>\n <p>\n  But here\u2019s where it gets interesting: fine-tuning Alpha isn\u2019t just a task; it\u2019s an art form. Achieving the ideal balance is crucial for unlocking the full potential of hybrid search. This involves adjusting different Alpha values for various types of user queries in your RAG system.\n </p>\n <p>\n  In this blog post, we will look into tuning Alpha within the Weaviate vector database using the\n  <code class=\"cw pq pr ps pt b\">\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/evaluation/retrieval/retriever_eval.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     Retrieval Evaluation\n    </strong>\n   </a>\n  </code>\n  module of LlamaIndex with and without rerankers with the help of Hit Rate and MRR metrics.\n </p>\n <p>\n  Before diving into the implementation, let\u2019s first understand the different query types and metrics we will be using in this article.\n </p>\n <h1>\n  Different User Query Types:\n </h1>\n <p>\n  User queries in an RAG application vary based on individual intent. For these diverse query types, it\u2019s essential to fine-tune the\n  <code class=\"cw pq pr ps pt b\">\n   <strong>\n    Alpha\n   </strong>\n  </code>\n  parameter. This process involves routing each user query to a specific\n  <code class=\"cw pq pr ps pt b\">\n   <strong>\n    Alpha\n   </strong>\n  </code>\n  value for effective retrieval and response synthesis.\n  <a href=\"https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-ai-search-outperforming-vector-search-with-hybrid/ba-p/3929167\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Microsoft\n  </a>\n  has identified various user query categories, and we have selected a few for tuning our hybrid search. Below are the different user query types we considered:\n </p>\n <ol>\n  <li>\n   <strong>\n    Web Search Queries:\n   </strong>\n   Brief queries similar to those typically inputted into search engines.\n  </li>\n  <li>\n   <strong>\n    Concept Seeking Queries:\n   </strong>\n   Abstract questions that necessitate detailed, multi-sentence answers.\n  </li>\n  <li>\n   <strong>\n    Fact Seeking Queries:\n   </strong>\n   Queries that have a single, definitive answer.\n  </li>\n  <li>\n   <strong>\n    Keyword Queries:\n   </strong>\n   Concise queries composed solely of crucial identifier words.\n  </li>\n  <li>\n   <strong>\n    Queries With Misspellings:\n   </strong>\n   Queries containing typos, transpositions, and common misspellings.\n  </li>\n  <li>\n   <strong>\n    Exact Sub-string Searches:\n   </strong>\n   Queries that exactly match sub-strings from the original context.\n  </li>\n </ol>\n <p>\n  Let\u2019s look at sample examples in each of these different user query types:\n </p>\n <ol>\n  <li>\n   <strong>\n    Web Search Queries\n   </strong>\n  </li>\n </ol>\n <blockquote>\n  <p class=\"om on qf oo b op pk or os ot pl ov ow ox pm oz pa pb pn pd pe pf po ph pi pj gm bj\" id=\"2417\">\n   <code class=\"cw pq pr ps pt b\">\n    <em class=\"gt\">\n     Transfer capabilities of LLaMA language model to non-English languages\n    </em>\n   </code>\n  </p>\n </blockquote>\n <p>\n  <strong>\n   2. Concept Seeking Queries\n  </strong>\n </p>\n <blockquote>\n  <p class=\"om on qf oo b op pk or os ot pl ov ow ox pm oz pa pb pn pd pe pf po ph pi pj gm bj\" id=\"698f\">\n   <code class=\"cw pq pr ps pt b\">\n    <em class=\"gt\">\n     What is the dual-encoder architecture used in recent works on dense retrievers?\n    </em>\n   </code>\n  </p>\n </blockquote>\n <p>\n  <strong>\n   3. Fact Seeking Queries\n  </strong>\n </p>\n <blockquote>\n  <p class=\"om on qf oo b op pk or os ot pl ov ow ox pm oz pa pb pn pd pe pf po ph pi pj gm bj\" id=\"c017\">\n   <code class=\"cw pq pr ps pt b\">\n    <em class=\"gt\">\n     What is the total number of propositions the English Wikipedia dump is segmented into in FACTOID WIKI?\n    </em>\n   </code>\n  </p>\n </blockquote>\n <p>\n  <strong>\n   4. Keyword Queries\n  </strong>\n </p>\n <blockquote>\n  <p class=\"om on qf oo b op pk or os ot pl ov ow ox pm oz pa pb pn pd pe pf po ph pi pj gm bj\" id=\"1497\">\n   <code class=\"cw pq pr ps pt b\">\n    <em class=\"gt\">\n     GTR retriever recall rate\n    </em>\n   </code>\n  </p>\n </blockquote>\n <p>\n  <strong>\n   5. Queries With Misspellings\n  </strong>\n </p>\n <blockquote>\n  <p class=\"om on qf oo b op pk or os ot pl ov ow ox pm oz pa pb pn pd pe pf po ph pi pj gm bj\" id=\"3608\">\n   <code class=\"cw pq pr ps pt b\">\n    <em class=\"gt\">\n     What is the advntage of prposition retrieval over sentnce or passage retrieval?\n    </em>\n   </code>\n  </p>\n </blockquote>\n <p>\n  <strong>\n   6. Exact Sub-string Searches\n  </strong>\n </p>\n <blockquote>\n  <p class=\"om on qf oo b op pk or os ot pl ov ow ox pm oz pa pb pn pd pe pf po ph pi pj gm bj\" id=\"1d9a\">\n   <code class=\"cw pq pr ps pt b\">\n    <em class=\"gt\">\n     first kwords for the GTR retriever. Finer-grained\n    </em>\n   </code>\n  </p>\n </blockquote>\n <h1>\n  Retrieval Evaluation Metrics:\n </h1>\n <p>\n  We will utilize Hit Rate and MRR metrics for retrieval evaluation. Let\u2019s get into understanding these metrics.\n </p>\n <p>\n  <strong>\n   Hit Rate:\n  </strong>\n </p>\n <p>\n  Hit Rate measures the proportion of queries for which the correct chunk/ context appears within the top-k results chunks/ contexts. Put simply, it evaluates how frequently our system correctly identifies the chunk within its top-k chunks.\n </p>\n <p>\n  <strong>\n   Mean Reciprocal Rank (MRR):\n  </strong>\n </p>\n <p>\n  MRR assesses a system\u2019s accuracy by taking into account the position of the highest-ranking relevant chunk/ context for each query. It calculates the average of the inverse of these positions across all queries. For instance, if the first relevant chunk/ context is at the top of the list, its reciprocal rank is 1. If it\u2019s the second item, the reciprocal rank becomes 1/2, and this pattern continues accordingly.\n </p>\n <p>\n  The remainder of this blog post is divided into two main sections:\n </p>\n <ol>\n  <li>\n   Implementing\n   <code class=\"cw pq pr ps pt b\">\n    <strong>\n     Alpha\n    </strong>\n   </code>\n   Tuning in Hybrid Search for Various Query Types.\n  </li>\n  <li>\n   Analyzing the results of two different document datasets:\n  </li>\n </ol>\n <ul>\n  <li>\n   <strong>\n    Indexing a Single Document:\n   </strong>\n   The\n   <a href=\"https://arxiv.org/pdf/2312.04511.pdf\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LLM Compiler Paper\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Indexing Three Documents:\n   </strong>\n   The\n   <a href=\"https://arxiv.org/pdf/2312.04511.pdf\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LLM Compiler\n   </a>\n   ,\n   <a href=\"https://arxiv.org/abs/2401.01055\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Llama Beyond English\n   </a>\n   , and\n   <a href=\"https://arxiv.org/abs/2312.06648\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Dense X Retrieval\n   </a>\n   Papers.\n  </li>\n </ul>\n <p>\n  You can also continue following along in the\n  <a href=\"https://colab.research.google.com/drive/1aiXqofZp7hSXuUdv2UGt_QoJa_liJDZ6?usp=sharing\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Google Colab Notebook\n  </a>\n  from this point forward.\n </p>\n <h1>\n  Implementation\n </h1>\n <p>\n  We will adopt a systematic approach to implement the experimental workflow, which involves the following steps:\n </p>\n <ol>\n  <li>\n   Data Download.\n  </li>\n  <li>\n   Data Loading.\n  </li>\n  <li>\n   Weaviate Client Setup.\n  </li>\n  <li>\n   Index Creation and Node Insertion.\n  </li>\n  <li>\n   Define LLM (GPT-4)\n  </li>\n  <li>\n   Define CohereAI Reranker.\n  </li>\n  <li>\n   Generation of Synthetic Queries for Various Query Types.\n  </li>\n  <li>\n   Define CustomRetriever.\n  </li>\n  <li>\n   Functions for Retrieval Evaluation and Metrics Calculation.\n  </li>\n  <li>\n   Conducting Retrieval Evaluation for Different Query Types and Alpha Values.\n  </li>\n </ol>\n <p>\n  Let\u2019s begin by defining some essential functions for our implementation.\n </p>\n <ol>\n  <li>\n   <code class=\"cw pq pr ps pt b\">\n    get_weaviate_client\n   </code>\n   - sets up weaviate client.\n  </li>\n  <li>\n   <code class=\"cw pq pr ps pt b\">\n    load_documents\n   </code>\n   - load the documents from the file path.\n  </li>\n  <li>\n   <code class=\"cw pq pr ps pt b\">\n    create_nodes\n   </code>\n   - create nodes by chunking the documents using a text splitter.\n  </li>\n  <li>\n   <code class=\"cw pq pr ps pt b\">\n    connect_index\n   </code>\n   - connect to weaviate index.\n  </li>\n  <li>\n   <code class=\"cw pq pr ps pt b\">\n    insert_nodes_index\n   </code>\n   - insert nodes into the index.\n  </li>\n </ol>\n <pre><span class=\"qp np gt pt b bf qq qr l qs qt\" id=\"3466\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">get_weaviate_client</span>(<span class=\"hljs-params\">api_key, url</span>):\n  auth_config = weaviate.AuthApiKey(api_key=api_key)\n\n  client = weaviate.Client(\n    url=url,\n    auth_client_secret=auth_config\n  )\n  <span class=\"hljs-keyword\">return</span> client\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">load_documents</span>(<span class=\"hljs-params\">file_path, num_pages=<span class=\"hljs-literal\">None</span></span>):\n  <span class=\"hljs-keyword\">if</span> num_pages:\n    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()[:num_pages]\n  <span class=\"hljs-keyword\">else</span>:\n    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n  <span class=\"hljs-keyword\">return</span> documents\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">create_nodes</span>(<span class=\"hljs-params\">documents, chunk_size=<span class=\"hljs-number\">512</span>, chunk_overlap=<span class=\"hljs-number\">0</span></span>):\n  node_parser = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n  nodes = node_parser.get_nodes_from_documents(documents)\n  <span class=\"hljs-keyword\">return</span> nodes\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">connect_index</span>(<span class=\"hljs-params\">weaviate_client</span>):\n  vector_store = WeaviateVectorStore(weaviate_client=weaviate_client)\n  storage_context = StorageContext.from_defaults(vector_store=vector_store)\n  index = VectorStoreIndex([], storage_context=storage_context)\n  <span class=\"hljs-keyword\">return</span> index\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">insert_nodes_index</span>(<span class=\"hljs-params\">index, nodes</span>):\n  index.insert_nodes(nodes)</span></pre>\n <ol>\n  <li>\n   <strong>\n    Download Data\n   </strong>\n  </li>\n </ol>\n <pre><span class=\"qp np gt pt b bf qq qr l qs qt\" id=\"c7c7\">!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2312.04511.pdf\" -O \"llm_compiler.pdf\"\n!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2401.01055.pdf\" -O \"llama_beyond_english.pdf\"\n!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2312.06648.pdf\" -O \"dense_x_retrieval.pdf\"</span></pre>\n <p>\n  2.\n  <strong>\n   Load Data\n  </strong>\n </p>\n <pre><span class=\"qp np gt pt b bf qq qr l qs qt\" id=\"3327\"># load documents, we will skip references and appendices from the papers.\ndocuments1 = load_documents(\"llm_compiler.pdf\", 12)\ndocuments2 = load_documents(\"dense_x_retrieval.pdf\", 9)\ndocuments3 = load_documents(\"llama_beyond_english.pdf\", 7)\n\n# create nodes\nnodes1 = create_nodes(documents1)\nnodes2 = create_nodes(documents2)\nnodes3 = create_nodes(documents3)</span></pre>\n <p>\n  3.\n  <strong>\n   Setup Weaviate Client\n  </strong>\n </p>\n <pre><span class=\"qp np gt pt b bf qq qr l qs qt\" id=\"548f\">url = 'cluster URL'\napi_key = 'your api key'\n\nclient = get_weaviate_client(api_key, url)</span></pre>\n <p>\n  4.\n  <strong>\n   Create an Index and Insert Nodes.\n  </strong>\n </p>\n <pre><span class=\"qp np gt pt b bf qq qr l qs qt\" id=\"5f1d\">index = connect_index(client)\n\ninsert_nodes_index(index, nodes1)</span></pre>\n <p>\n  5.\n  <strong>\n   Define LLM\n  </strong>\n </p>\n <pre><span class=\"qp np gt pt b bf qq qr l qs qt\" id=\"e9f5\"># Deing LLM for query generation\nllm = OpenAI(model='gpt-4', temperature=0.1)</span></pre>\n <p>\n  6.\n  <strong>\n   Create Synthetic Queries\n  </strong>\n </p>\n <p>\n  We will create queries as discussed earlier, check prompts for each of the query types in the notebook, and code for each type of query. Showing code snippet for reference.\n </p>\n <pre><span class=\"qp np gt pt b bf qq qr l qs qt\" id=\"c439\">queries = generate_question_context_pairs(\n    nodes, \n  llm=llm, \n  num_questions_per_chunk=<span class=\"hljs-number\">2</span>, \n  qa_generate_prompt_tmpl = qa_template\n)</span></pre>\n <p>\n  7.\n  <strong>\n   Define reranker\n  </strong>\n </p>\n <pre><span class=\"qp np gt pt b bf qq qr l qs qt\" id=\"72f4\">reranker = CohereRerank(api_key=os.environ['COHERE_API_KEY'], top_n=4)</span></pre>\n <p>\n  8.\n  <strong>\n   Define CustomRetriever\n  </strong>\n </p>\n <p>\n  We will define\n  <code class=\"cw pq pr ps pt b\">\n   CustomRetriever\n  </code>\n  class to perform retrieval operations with and without a reranker.\n </p>\n <pre><span class=\"qp np gt pt b bf qq qr l qs qt\" id=\"70a0\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">CustomRetriever</span>(<span class=\"hljs-title class_ inherited__\">BaseRetriever</span>):\n    <span class=\"hljs-string\">\"\"\"Custom retriever that performs hybrid search with and without reranker\"\"\"</span>\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">\n        self,\n        vector_retriever: VectorIndexRetriever,\n        reranker: CohereRerank\n    </span>) -&amp;gt; <span class=\"hljs-literal\">None</span>:\n        <span class=\"hljs-string\">\"\"\"Init params.\"\"\"</span>\n\n        self._vector_retriever = vector_retriever\n        self._reranker = reranker\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">_retrieve</span>(<span class=\"hljs-params\">self, query_bundle: QueryBundle</span>) -&amp;gt; <span class=\"hljs-type\">List</span>[NodeWithScore]:\n        <span class=\"hljs-string\">\"\"\"Retrieve nodes given query.\"\"\"</span>\n\n        retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n\n        <span class=\"hljs-keyword\">if</span> self._reranker != <span class=\"hljs-literal\">None</span>:\n            retrieved_nodes = self._reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n        <span class=\"hljs-keyword\">else</span>:\n            retrieved_nodes = retrieved_nodes[:<span class=\"hljs-number\">4</span>]\n\n        <span class=\"hljs-keyword\">return</span> retrieved_nodes\n\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">_aretrieve</span>(<span class=\"hljs-params\">self, query_bundle: QueryBundle</span>) -&amp;gt; <span class=\"hljs-type\">List</span>[NodeWithScore]:\n        <span class=\"hljs-string\">\"\"\"Asynchronously retrieve nodes given query.\n\n        Implemented by the user.\n\n        \"\"\"</span>\n        <span class=\"hljs-keyword\">return</span> self._retrieve(query_bundle)\n\n    <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">aretrieve</span>(<span class=\"hljs-params\">self, str_or_query_bundle: QueryType</span>) -&amp;gt; <span class=\"hljs-type\">List</span>[NodeWithScore]:\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">isinstance</span>(str_or_query_bundle, <span class=\"hljs-built_in\">str</span>):\n            str_or_query_bundle = QueryBundle(str_or_query_bundle)\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-keyword\">await</span> self._aretrieve(str_or_query_bundle)</span></pre>\n <p>\n  9.\n  <strong>\n   Define functions for retriever evaluation and metrics computation\n  </strong>\n </p>\n <p>\n  We will look into retriever performance for different\n  <code class=\"cw pq pr ps pt b\">\n   alpha\n  </code>\n  values with and without reranker.\n </p>\n <pre><span class=\"qp np gt pt b bf qq qr l qs qt\" id=\"2955\"><span class=\"hljs-comment\"># Alpha values and datasets to test</span>\nalpha_values = [<span class=\"hljs-number\">0.0</span>, <span class=\"hljs-number\">0.2</span>, <span class=\"hljs-number\">0.4</span>, <span class=\"hljs-number\">0.6</span>, <span class=\"hljs-number\">0.8</span>, <span class=\"hljs-number\">1.0</span>]\n\n<span class=\"hljs-comment\"># Function to evaluate retriever and return results</span>\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">evaluate_retriever</span>(<span class=\"hljs-params\">alpha, dataset, reranker=<span class=\"hljs-literal\">None</span></span>):\n    retriever = VectorIndexRetriever(index,\n                                     vector_store_query_mode=<span class=\"hljs-string\">\"hybrid\"</span>,\n                                     similarity_top_k=<span class=\"hljs-number\">10</span>,\n                                     alpha=alpha)\n    custom_retriever = CustomRetriever(retriever,\n                                       reranker)\n\n    retriever_evaluator = RetrieverEvaluator.from_metric_names([<span class=\"hljs-string\">\"mrr\"</span>, <span class=\"hljs-string\">\"hit_rate\"</span>], retriever=custom_retriever)\n    eval_results = <span class=\"hljs-keyword\">await</span> retriever_evaluator.aevaluate_dataset(dataset)\n    <span class=\"hljs-keyword\">return</span> eval_results\n\n<span class=\"hljs-comment\"># Function to calculate and store metrics</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">calculate_metrics</span>(<span class=\"hljs-params\">eval_results</span>):\n    metric_dicts = []\n    <span class=\"hljs-keyword\">for</span> eval_result <span class=\"hljs-keyword\">in</span> eval_results:\n        metric_dict = eval_result.metric_vals_dict\n        metric_dicts.append(metric_dict)\n\n    full_df = pd.DataFrame(metric_dicts)\n\n    hit_rate = full_df[<span class=\"hljs-string\">\"hit_rate\"</span>].mean()\n    mrr = full_df[<span class=\"hljs-string\">\"mrr\"</span>].mean()\n    <span class=\"hljs-keyword\">return</span> hit_rate, mrr</span></pre>\n <p>\n  <strong>\n   10. Retrieval Evaluation\n  </strong>\n </p>\n <p>\n  Here we do retrieval evaluation on different query types (datasets) and alpha values to understand which alpha will be suitable for which query type. You need to plug in the reranker accordingly to compute the retrieval evaluation with and without the reranker.\n </p>\n <pre><span class=\"qp np gt pt b bf qq qr l qs qt\" id=\"4b6f\"><span class=\"hljs-comment\"># Asynchronous function to loop over datasets and alpha values and evaluate</span>\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">main</span>():\n    results_df = pd.DataFrame(columns=[<span class=\"hljs-string\">'Dataset'</span>, <span class=\"hljs-string\">'Alpha'</span>, <span class=\"hljs-string\">'Hit Rate'</span>, <span class=\"hljs-string\">'MRR'</span>])\n\n    <span class=\"hljs-keyword\">for</span> dataset <span class=\"hljs-keyword\">in</span> datasets_single_document.keys():\n        <span class=\"hljs-keyword\">for</span> alpha <span class=\"hljs-keyword\">in</span> alpha_values:\n            eval_results = <span class=\"hljs-keyword\">await</span> evaluate_retriever(alpha, datasets_single_document[dataset])\n            hit_rate, mrr = calculate_metrics(eval_results)\n            new_row = pd.DataFrame({<span class=\"hljs-string\">'Dataset'</span>: [dataset], <span class=\"hljs-string\">'Alpha'</span>: [alpha], <span class=\"hljs-string\">'Hit Rate'</span>: [hit_rate], <span class=\"hljs-string\">'MRR'</span>: [mrr]})\n            results_df = pd.concat([results_df, new_row], ignore_index=<span class=\"hljs-literal\">True</span>)\n\n    <span class=\"hljs-comment\"># Determine the grid size for subplots</span>\n    num_rows = <span class=\"hljs-built_in\">len</span>(datasets_single_document) // <span class=\"hljs-number\">2</span> + <span class=\"hljs-built_in\">len</span>(datasets_single_document) % <span class=\"hljs-number\">2</span>\n    num_cols = <span class=\"hljs-number\">2</span>\n\n    <span class=\"hljs-comment\"># Plotting the results in a grid</span>\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(<span class=\"hljs-number\">12</span>, num_rows * <span class=\"hljs-number\">4</span>), squeeze=<span class=\"hljs-literal\">False</span>)  <span class=\"hljs-comment\"># Ensure axes is always 2D</span>\n\n    <span class=\"hljs-keyword\">for</span> i, dataset <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">enumerate</span>(datasets_single_document):\n        ax = axes[i // num_cols, i % num_cols]\n        dataset_df = results_df[results_df[<span class=\"hljs-string\">'Dataset'</span>] == dataset]\n        ax.plot(dataset_df[<span class=\"hljs-string\">'Alpha'</span>], dataset_df[<span class=\"hljs-string\">'Hit Rate'</span>], marker=<span class=\"hljs-string\">'o'</span>, label=<span class=\"hljs-string\">'Hit Rate'</span>)\n        ax.plot(dataset_df[<span class=\"hljs-string\">'Alpha'</span>], dataset_df[<span class=\"hljs-string\">'MRR'</span>], marker=<span class=\"hljs-string\">'o'</span>, linestyle=<span class=\"hljs-string\">'--'</span>, label=<span class=\"hljs-string\">'MRR'</span>)\n        ax.set_xlabel(<span class=\"hljs-string\">'Alpha'</span>)\n        ax.set_ylabel(<span class=\"hljs-string\">'Metric Value'</span>)\n        ax.set_title(<span class=\"hljs-string\">f'<span class=\"hljs-subst\">{dataset}</span>'</span>)\n        ax.legend()\n        ax.grid(<span class=\"hljs-literal\">True</span>)\n\n    <span class=\"hljs-comment\"># If the number of datasets is odd, remove the last (empty) subplot</span>\n    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(datasets_single_document) % num_cols != <span class=\"hljs-number\">0</span>:\n        fig.delaxes(axes[-<span class=\"hljs-number\">1</span>, -<span class=\"hljs-number\">1</span>])  <span class=\"hljs-comment\"># Remove the last subplot if not needed</span>\n\n    <span class=\"hljs-comment\"># Adjust layout to prevent overlap</span>\n    plt.tight_layout()\n    plt.show()\n\n<span class=\"hljs-comment\"># Run the main function</span>\nasyncio.run(main())</span></pre>\n <h1>\n  Analyze the results:\n </h1>\n <p>\n  Having completed the implementation phase, we now turn our attention to analyzing the outcomes. We conducted two sets of experiments: one on a single document and another on multiple documents. These experiments varied in alpha values, types of user queries, and the inclusion or exclusion of a reranker. The accompanying graphs display the results, focusing on the Hit Rate and MRR (Mean Reciprocal Rank) as retrieval evaluation metrics.\n </p>\n <blockquote>\n  <p class=\"om on qf oo b op pk or os ot pl ov ow ox pm oz pa pb pn pd pe pf po ph pi pj gm bj\" id=\"492d\">\n   P\n   <!-- -->\n   lease keep in mind that following observations are specific to the datasets used in our study. We encourage you to conduct the experiment with your own documents and draw your relevant observations and conclusions.\n  </p>\n </blockquote>\n <h2>\n  <strong>\n   With Single Document:\n  </strong>\n </h2>\n <p>\n  <strong>\n   Without Reranker:\n  </strong>\n </p>\n <p>\n  <strong>\n   With Reranker:\n  </strong>\n </p>\n <h2>\n  With Multiple Documents:\n </h2>\n <p>\n  <strong>\n   Without Reranker:\n  </strong>\n </p>\n <p>\n  <strong>\n   With Reranker:\n  </strong>\n </p>\n <h1>\n  Observations:\n </h1>\n <ol>\n  <li>\n   There is a boost in Hit Rate and MRR in single and multiple documents indexing with the help of a reranker. Time and again it proves using reranker is pretty useful in your RAG application.\n  </li>\n  <li>\n   Though most of the time hybrid search wins over keyword/ vector search, it should be carefully evaluated for different query types based on user queries in the RAG application.\n  </li>\n  <li>\n   The behavior is different when you index a single document and multiple documents, which suggests it\u2019s always better to tune alpha as you add documents into the index.\n  </li>\n  <li>\n   Let\u2019s look at a deeper analysis of different query types:\n  </li>\n </ol>\n <ul>\n  <li>\n   <strong>\n    Web Search Queries:\n   </strong>\n  </li>\n </ul>\n <p>\n  \u2014 MRR is higher with hybrid search with alpha=0.2/0.6 based on with/ without rerankers irrespective of single/ multiple documents indexing.\n </p>\n <p>\n  \u2014 The Hit rate is higher with alpha=1.0 for both single/ multiple documents indexing and with/ without rerankers.\n </p>\n <ul>\n  <li>\n   <strong>\n    Concept Seeking Queries:\n   </strong>\n  </li>\n </ul>\n <p>\n  \u2014 MRR and Hit Rate are higher with hybrid search (with different alpha values) in Multiple documents indexing.\n </p>\n <p>\n  \u2014 MRR and Hit Rate are higher at Alpha=0.0 indicating keyword search works better in Single document indexing. Should be noted that MRR has different behavior with and without reranking.\n </p>\n <ul>\n  <li>\n   <strong>\n    Fact Seeking Queries\n   </strong>\n  </li>\n </ul>\n <p>\n  \u2014 MRR and Hit Rate are higher with Hybrid search with/ without reranker in Multiple documents indexing.\n </p>\n <p>\n  \u2014 MRR and Hit Rate are higher with hybrid search with reranker and keyword search (alpha=0.0) is better without reranker in single documents indexing.\n </p>\n <ul>\n  <li>\n   <strong>\n    Keyword Queries\n   </strong>\n  </li>\n </ul>\n <p>\n  \u2014 MRR and Hit Rate are higher with Hybrid search with/ without reranker in Multiple documents indexing.\n </p>\n <p>\n  \u2014 MRR and Hit Rate are higher with hybrid search with reranker and keyword search is better without reranker in single documents indexing. (though MRR is slightly higher with alpha=0.2)\n </p>\n <ul>\n  <li>\n   <strong>\n    Queries With Misspellings\n   </strong>\n  </li>\n </ul>\n <p>\n  \u2014 MRR and Hit Rate are higher with Hybrid search with/ without reranker in single and multiple documents indexing. (Though in some cases hybrid search with alpha=1.0 wins).\n </p>\n <p>\n  \u2014 This also demonstrates that vector search performs better with misspelled queries, as keyword searches lose effectiveness in such cases.\n </p>\n <ul>\n  <li>\n   <strong>\n    Exact Sub-string Searches\n   </strong>\n  </li>\n </ul>\n <p>\n  \u2014 MRR and Hit Rate are higher with Keyword search with/ without reranker in Single documents indexing and without reranker in multiple documents indexing.\n </p>\n <p>\n  \u2014 MRR and Hit Rate are higher with Hybrid search (alpha=0.4) with reranker in multiple documents indexing.\n </p>\n <h1>\n  What\u2019s Next?\n </h1>\n <p>\n  In this blog post, we looked into the tuning of Alpha in a hybrid search system for a range of query types. It was interesting to see how the results varied when indexing either a single document or multiple documents. Going forward, you might consider experimenting with documents from diverse domains, employing different query lengths for various query types. Should you come across any noteworthy observations, we encourage you to share them with us in the comments. It would certainly be interesting to discuss these findings with the wider community.\n </p>\n <h1>\n  References:\n </h1>\n <ol>\n  <li>\n   <a href=\"https://weaviate.io/blog/hybrid-search-explained\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Hybrid Search Explained\n   </a>\n  </li>\n  <li>\n   <a href=\"https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-ai-search-outperforming-vector-search-with-hybrid/ba-p/3929167\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Azure AI Search: Outperforming vector search with hybrid retrieval and ranking capabilities\n   </a>\n  </li>\n </ol>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 28066, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d4312cad-4695-4e8f-84d4-793758a8c966": {"__data__": {"id_": "d4312cad-4695-4e8f-84d4-793758a8c966", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-gemini-8d7c3b9ea97e.html", "file_name": "llamaindex-gemini-8d7c3b9ea97e.html", "file_type": "text/html", "file_size": 16217, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-gemini-8d7c3b9ea97e.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-gemini-8d7c3b9ea97e.html", "file_name": "llamaindex-gemini-8d7c3b9ea97e.html", "file_type": "text/html", "file_size": 16217, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "328a86f2cc33c0c18358fd8ee5f34d574da128d495616d7bf1c8c45b3ce3f8a2", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  (co-authored by Jerry Liu, Haotian Zhang, Logan Markewich, and Laurie Voss @ LlamaIndex)\n </p>\n <p>\n  Today is Google\u2019s\n  <a href=\"https://blog.google/technology/ai/gemini-api-developers-cloud/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   public release\n  </a>\n  of its latest AI model, Gemini. We\u2019re excited to be a day 1 launch partner for Gemini, with support immediately available in LlamaIndex today!\n </p>\n <p>\n  As of 0.9.15, LlamaIndex offers full support for all currently\n  <strong>\n   released and upcoming Gemini models\n  </strong>\n  (Gemini Pro, Gemini Ultra). We support both a \u201ctext-only\u201d Gemini variant with a text-in/text-out format as well as a multimodal variant that takes in both text and images as input, and outputs text. We\u2019ve made some fundamental multi-modal abstraction changes to support the Gemini multi-modal interface, which allows users to input multiple images along with text. Our Gemini integrations are also\n  <strong>\n   feature-complete:\n  </strong>\n  they support (non-streaming, streaming), (sync, async), and (text completion, chat message) formats \u2014 8 combinations in total.\n </p>\n <p>\n  In addition, we also support the brand-new\n  <strong>\n   Semantic Retriever API,\n  </strong>\n  which bundles storage, embedding models, retrieval, and LLM in a RAG pipeline. We show you how it can be used on its own, or decomposed+bundled with LlamaIndex components to create advanced RAG pipelines.\n </p>\n <p>\n  Huge shoutout to the Google Labs and Semantic Retriever teams for helping us get setup with early access.\n </p>\n <ul>\n  <li>\n   <strong>\n    Google Labs:\n   </strong>\n   Mark McDonald, Josh Gordon, Arthur Soroken\n  </li>\n  <li>\n   <strong>\n    Semantic Retriever:\n   </strong>\n   Lawrence Tsang, Cher Hu\n  </li>\n </ul>\n <p>\n  The below sections contain a detailed walkthrough of both our brand-new Gemini and Semantic Retriever abstractions in LlamaIndex. If you don\u2019t want to read that now, make sure you bookmark our detailed notebook guides below!\n </p>\n <ul>\n  <li>\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/gemini.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Gemini (text-only) Guide\n   </a>\n  </li>\n  <li>\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/gemini.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Gemini (multi-modal) Guide\n   </a>\n  </li>\n  <li>\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/managed/GoogleDemo.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Semantic Retriever Guide\n   </a>\n  </li>\n </ul>\n <h1>\n  Gemini Release and Support\n </h1>\n <p>\n  There\u2019s been a ton of press around Gemini, which boasts\n  <a href=\"https://blog.google/technology/ai/google-gemini-ai/#performance\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   impressive performance\n  </a>\n  at a variety of benchmarks. The Ultra variants (which are not yet publicly available) outperform GPT-4 on benchmarks from MMLU to Big-Bench Hard to math and coding tasks. Their\n  <a href=\"https://www.youtube.com/watch?v=K4pX1VAxaAI\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   multimodal demos\n  </a>\n  demonstrate joint image/text understanding from domains like scientific paper understanding to literature review.\n </p>\n <p>\n  Let\u2019s walk through examples of using Gemini in LlamaIndex. We walk through both the text model (\n  <code class=\"cw px py pz qa b\">\n   from llama_index.llms import Gemini\n  </code>\n  ) as well as the multi-modal model (\n  <code class=\"cw px py pz qa b\">\n   from llama_index.multi_modal_llms.gemini import GeminiMultiModal\n  </code>\n  )\n </p>\n <h1>\n  Text Model\n </h1>\n <p>\n  <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/gemini.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Full Notebook Guide Here\n  </a>\n </p>\n <p>\n  We start with the text model. In the code snippet below, we show a bunch of different configurations, from completion to chat to streaming to async.\n </p>\n <pre><span class=\"qe ov gt qa b bf qf qg l qh qi\" id=\"9b32\">from llama_index.llms import Gemini\n\n# completion\nresp = Gemini().complete(\"Write a poem about a magic backpack\")\n# chat\nmessages = [\n    ChatMessage(role=\"user\", content=\"Hello friend!\"),\n    ChatMessage(role=\"assistant\", content=\"Yarr what is shakin' matey?\"),\n    ChatMessage(\n        role=\"user\", content=\"Help me decide what to have for dinner.\"\n    ),\n]\nresp = Gemini().chat(messages)\n# streaming (completion)\nllm = Gemini()\nresp = llm.stream_complete(\n    \"The story of Sourcrust, the bread creature, is really interesting. It all started when...\"\n)\n# streaming (chat)\nllm = Gemini()\nmessages = [\n    ChatMessage(role=\"user\", content=\"Hello friend!\"),\n    ChatMessage(role=\"assistant\", content=\"Yarr what is shakin' matey?\"),\n    ChatMessage(\n        role=\"user\", content=\"Help me decide what to have for dinner.\"\n    ),\n]\nresp = llm.stream_chat(messages)\n# async completion\nresp = await llm.acomplete(\"Llamas are famous for \")\nprint(resp)\n# async streaming (completion)\nresp = await llm.astream_complete(\"Llamas are famous for \")\nasync for chunk in resp:\n    print(chunk.text, end=\"\")</span></pre>\n <p>\n  The\n  <code class=\"cw px py pz qa b\">\n   Gemini\n  </code>\n  class of course has parameters that can be set. This includes\n  <code class=\"cw px py pz qa b\">\n   model_name\n  </code>\n  ,\n  <code class=\"cw px py pz qa b\">\n   temperature\n  </code>\n  ,\n  <code class=\"cw px py pz qa b\">\n   max_tokens\n  </code>\n  , and\n  <code class=\"cw px py pz qa b\">\n   generate_kwargs\n  </code>\n  .\n </p>\n <p>\n  As an example, you can do:\n </p>\n <pre><span class=\"qe ov gt qa b bf qf qg l qh qi\" id=\"4457\">llm = Gemini(model=\"models/gemini-ultra\")</span></pre>\n <h1>\n  Multi-modal Model\n </h1>\n <p>\n  <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/gemini.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Full Notebook Guide Here\n  </a>\n </p>\n <p>\n  In this notebook, we test out the\n  <code class=\"cw px py pz qa b\">\n   gemini-pro-vision\n  </code>\n  variant that features\n  <strong>\n   multi-modal inputs.\n  </strong>\n  It contains the following features:\n </p>\n <ul>\n  <li>\n   supports both\n   <code class=\"cw px py pz qa b\">\n    complete\n   </code>\n   and\n   <code class=\"cw px py pz qa b\">\n    chat\n   </code>\n   capabilities\n  </li>\n  <li>\n   supports streaming and async\n  </li>\n  <li>\n   Supports feeding in\n   <strong>\n    multiple images\n   </strong>\n   in addition to text in the completion endpoint\n  </li>\n  <li>\n   Future work: multi-turn chat interleaving text and images is supported within our abstraction, but is not yet enabled for gemini-pro-vision.\n  </li>\n </ul>\n <p>\n  Let\u2019s walk through a concrete example. Let\u2019s say we are given a picture of the\n  <a href=\"https://storage.googleapis.com/generativeai-downloads/data/scene.jpg\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   following scene\n  </a>\n  :\n </p>\n <figure>\n  <figcaption class=\"qk fe ql nx ny qm qn be b bf z dt\">\n   Scene from a street in New York City\n  </figcaption>\n </figure>\n <p>\n  We can then initialize our Gemini Vision model, and ask it a question: \u201cIdentify the city where this photo was taken\u201d:\n </p>\n <pre><span class=\"qe ov gt qa b bf qf qg l qh qi\" id=\"7451\">from llama_index.multi_modal_llms.gemini import GeminiMultiModal\nfrom llama_index.multi_modal_llms.generic_utils import (\n    load_image_urls,\n)\n\nimage_urls = [\n    \"<span class=\"hljs-symbol\">&amp;lt;</span>https://storage.googleapis.com/generativeai-downloads/data/scene.jpg<span class=\"hljs-symbol\">&amp;gt;</span>\",\n    # Add yours here!\n]\nimage_documents = load_image_urls(image_urls)\ngemini_pro = GeminiMultiModal(model=\"models/gemini-pro\")\ncomplete_response = gemini_pro.complete(\n    prompt=\"Identify the city where this photo was taken.\",\n    image_documents=image_documents,\n)</span></pre>\n <p>\n  Our response is the following:\n </p>\n <pre><span class=\"qe ov gt qa b bf qf qg l qh qi\" id=\"d6e8\">New York City</span></pre>\n <p>\n  We can insert multiple images too. Here\u2019s an example with an image of Messi and the Colosseum.\n </p>\n <pre><span class=\"qe ov gt qa b bf qf qg l qh qi\" id=\"f03d\">image_urls = [\n    \"<span class=\"hljs-symbol\">&amp;lt;</span>https://www.sportsnet.ca/wp-content/uploads/2023/11/CP1688996471-1040x572.jpg<span class=\"hljs-symbol\">&amp;gt;</span>\",\n    \"<span class=\"hljs-symbol\">&amp;lt;</span>https://res.cloudinary.com/hello-tickets/image/upload/c_limit,f_auto,q_auto,w_1920/v1640835927/o3pfl41q7m5bj8jardk0.jpg<span class=\"hljs-symbol\">&amp;gt;</span>\",\n]\nimage_documents_1 = load_image_urls(image_urls)\nresponse_multi = gemini_pro.complete(\n    prompt=\"is there any relationship between those images?\",\n    image_documents=image_documents_1,\n)\nprint(response_multi)</span></pre>\n <h1>\n  Multi-Modal Use Cases (Structured Outputs, RAG)\n </h1>\n <p>\n  <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/gemini.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Full Notebook Guide Here\n  </a>\n </p>\n <p>\n  We\u2019ve created extensive resources about different\n  <a href=\"https://docs.llamaindex.ai/en/latest/use_cases/multimodal.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   multi-modal use cases\n  </a>\n  , from structured output extraction to RAG.\n </p>\n <p>\n  Thanks to Haotian Zhang, we have examples for\n  <strong>\n   <em class=\"qo\">\n    both\n   </em>\n  </strong>\n  these use cases with Gemini. Please see our extensive notebook guides for more details. In the meantime here\u2019s the final results!\n </p>\n <p>\n  <strong>\n   Structured Data Extraction with Gemini Pro Vision\n  </strong>\n </p>\n <figure>\n  <figcaption class=\"qk fe ql nx ny qm qn be b bf z dt\">\n   Screenshot of a Google Maps Restaurant Listing\n  </figcaption>\n </figure>\n <p>\n  Output:\n </p>\n <pre><span class=\"qe ov gt qa b bf qf qg l qh qi\" id=\"83bc\">('restaurant', 'La Mar by Gaston Acurio')\n('food', 'South American')\n('location', '500 Brickell Key Dr, Miami, FL 33131')\n('category', 'Restaurant')\n('hours', 'Open \u22c5 Closes 11 PM')\n('price', 4.0)\n('rating', 4)\n('review', '4.4 (2,104)')\n('description', 'Chic waterfront find offering Peruvian &amp; fusion fare, plus bars for cocktails, ceviche &amp; anticucho.')\n('nearby_tourist_places', 'Brickell Key Park')</span></pre>\n <p>\n  <strong>\n   Multi-Modal RAG\n  </strong>\n </p>\n <p>\n  We run our structured output extractor on multiple restaurant images, index these nodes, and then ask a question \u201cRecommend a Orlando restaurant for me and its nearby tourist places\u201d\n </p>\n <pre><span class=\"qe ov gt qa b bf qf qg l qh qi\" id=\"dad2\">I recommend Mythos Restaurant in Orlando. It is an American restaurant located at 6000 Universal Blvd, Orlando, FL 32819, United States. It has a rating of 4 and a review score of 4.3 based on 2,115 reviews. The restaurant offers a mythic underwater-themed dining experience with a view of Universal Studios' Inland Sea. It is located near popular tourist places such as Universal's Islands of Adventure, Skull Island: Reign of Kong, The Wizarding World of Harry Potter, Jurassic Park River Adventure, Hollywood Rip Ride Rockit, and Universal Studios Florida.</span></pre>\n <h1>\n  Semantic Retriever\n </h1>\n <p>\n  The Generative Language Semantic Retriever offers specialized embedding models for high-quality retrieval, and a tuned LLM for producing grounded-output with safety settings.\n </p>\n <p>\n  It can be used out of the box (with our\n  <code class=\"cw px py pz qa b\">\n   GoogleIndex\n  </code>\n  ) or decomposed into different components (\n  <code class=\"cw px py pz qa b\">\n   GoogleVectorStore\n  </code>\n  and\n  <code class=\"cw px py pz qa b\">\n   GoogleTextSynthesizer\n  </code>\n  ) and combined with LlamaIndex abstractions!\n </p>\n <p>\n  Our full\n  <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/managed/GoogleDemo.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   semantic retriever notebook guide is here.\n  </a>\n </p>\n <h1>\n  Out of the Box Configuration\n </h1>\n <p>\n  You can use it out of the box with very few lines of setup. Simply define the index, insert nodes, and then get a query engine:\n </p>\n <pre><span class=\"qe ov gt qa b bf qf qg l qh qi\" id=\"ad30\"><span class=\"hljs-keyword\">from</span> llama_index.indices.managed.google.generativeai <span class=\"hljs-keyword\">import</span> GoogleIndex\n\nindex = GoogleIndex.from_corpus(corpus_id=<span class=\"hljs-string\">\"&amp;lt;corpus_id&amp;gt;\"</span>)\nindex.insert_documents(nodes)\nquery_engine = index.as_query_engine(...)\nresponse = query_engine.query(<span class=\"hljs-string\">\"&amp;lt;query&amp;gt;\"</span>)</span></pre>\n <p>\n  A cool feature here is that Google\u2019s query engine supports different\n  <strong>\n   answering styles\n  </strong>\n  as well as\n  <strong>\n   safety settings\n  </strong>\n  .\n </p>\n <p>\n  <strong>\n   Answering Styles:\n  </strong>\n </p>\n <ul>\n  <li>\n   ABSTRACTIVE (succinct but abstract)\n  </li>\n  <li>\n   EXTRACTIVE (brief and extractive)\n  </li>\n  <li>\n   VERBOSE (extra details)\n  </li>\n </ul>\n <p>\n  <strong>\n   Safety Settings\n  </strong>\n </p>\n <p>\n  You can specify safety settings in the query engine, which let you define guardrails on whether the answer is explicit in different settings. See the\n  <code class=\"cw px py pz qa b\">\n   <a href=\"https://github.com/google/generative-ai-python\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    generative-ai-python\n   </a>\n  </code>\n  library for more information.\n </p>\n <h1>\n  Decomposing into Different Components\n </h1>\n <p>\n  The\n  <code class=\"cw px py pz qa b\">\n   GoogleIndex\n  </code>\n  is built upon two components: a vector store (\n  <code class=\"cw px py pz qa b\">\n   GoogleVectorStore\n  </code>\n  ) and the response synthesizer (\n  <code class=\"cw px py pz qa b\">\n   GoogleTextSynthesizer\n  </code>\n  ). You can use these as modular components in conjunction with LlamaIndex abstractions to create\n  <strong>\n   advanced RAG\n  </strong>\n  .\n </p>\n <p>\n  The notebook guide highlights three\n  <strong>\n   advanced RAG use cases\n  </strong>\n  :\n </p>\n <ul>\n  <li>\n   <strong>\n    Google Retriever + Reranking:\n   </strong>\n   Use the Semantic Retriever to return relevant results, but then use our r\n   <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/root.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    eranking modules\n   </a>\n   to process/filter results before feeding it to response synthesis.\n  </li>\n  <li>\n   <strong>\n    Multi-Query + Google Retriever:\n   </strong>\n   Use our multi-query capabilities, like our\n   <code class=\"cw px py pz qa b\">\n    MultiStepQueryEngine\n   </code>\n   to break a complex question into multiple steps, and execute each step against the semantic retriever.\n  </li>\n  <li>\n   <strong>\n    HyDE + Google Retriever:\n   </strong>\n   HyDE is a popular query transformation technique that hallucinates an answer from a query, and uses the hallucinated answer for embedding lookup. Use that as a step before the retrieval step from the Semantic Retriever.\n  </li>\n </ul>\n <h1>\n  Conclusion\n </h1>\n <p>\n  There\u2019s a\n  <strong>\n   lot\n  </strong>\n  in here, and even then the blog post doesn\u2019t even cover half of what we\u2019ve released today.\n </p>\n <p>\n  Please please make sure to check out our extensive notebook guides! Linking the resources again below:\n </p>\n <ul>\n  <li>\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/gemini.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Gemini (text-only) Guide\n   </a>\n  </li>\n  <li>\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/gemini.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Gemini (multi-modal) Guide\n   </a>\n  </li>\n  <li>\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/managed/GoogleDemo.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Semantic Retriever Guide\n   </a>\n  </li>\n </ul>\n <p>\n  Again, huge shoutout to the Google teams and Haotian Zhang, Logan Markewich from the LlamaIndex team for putting together everything for this release.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 16170, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75c83f20-cfc9-4004-a2f1-3b5844923fdc": {"__data__": {"id_": "75c83f20-cfc9-4004-a2f1-3b5844923fdc", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.html", "file_name": "llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.html", "file_type": "text/html", "file_size": 17065, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.html", "file_name": "llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.html", "file_type": "text/html", "file_size": 17065, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "d01e8afcf24c399e302e87848d1108ce590055f4a70e238e8dca74e80a03af8a", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h1>\n  Introduction\n </h1>\n <p>\n  E-commerce platforms, such as Amazon and Walmart, are teeming with products that attract a multitude of reviews every single day. These reviews are crucial touchpoints that reflect consumer sentiments about products. But how can businesses sift through vast databases to derive meaningful insights from these reviews?\n </p>\n <p>\n  The answer lies in combining SQL with RAG (Retrieval Augmented Generation) through LlamaIndex.\n </p>\n <p>\n  Let\u2019s deep dive into this!\n </p>\n <h1>\n  Sample Dataset of Product Reviews\n </h1>\n <p>\n  For the purpose of this demonstration, we\u2019ve generated a sample dataset using GPT-4 that comprises reviews for three products: iPhone13, SamsungTV, and an Ergonomic Chair. Here\u2019s a sneak peek:\n </p>\n <ul>\n  <li>\n   iPhone13: \u201cAmazing battery life and camera quality. Best iPhone yet.\u201d\n  </li>\n  <li>\n   SamsungTV: \u201cImpressive picture clarity and vibrant colors. A top-notch TV.\u201d\n  </li>\n  <li>\n   Ergonomic Chair: \u201cFeels really comfortable even after long hours.\u201d\n  </li>\n </ul>\n <p>\n  Here is a sample dataset.\n </p>\n <pre><span class=\"qf no gt qc b bf qg qh l qi qj\" id=\"1d0d\">rows = [\n    # iPhone13 Reviews\n    {\"category\": \"Phone\", \"product_name\": \"Iphone13\", \"review\": \"The iPhone13 is a stellar leap forward. From its sleek design to the crystal-clear display, it screams luxury and functionality. Coupled with the enhanced battery life and an A15 chip, it's clear Apple has once again raised the bar in the smartphone industry.\"},\n    {\"category\": \"Phone\", \"product_name\": \"Iphone13\", \"review\": \"This model brings the brilliance of the ProMotion display, changing the dynamics of screen interaction. The rich colors, smooth transitions, and lag-free experience make daily tasks and gaming absolutely delightful.\"},\n    {\"category\": \"Phone\", \"product_name\": \"Iphone13\", \"review\": \"The 5G capabilities are the true game-changer. Streaming, downloading, or even regular browsing feels like a breeze. It's remarkable how seamless the integration feels, and it's obvious that Apple has invested a lot in refining the experience.\"},\n\n    # SamsungTV Reviews\n    {\"category\": \"TV\", \"product_name\": \"SamsungTV\", \"review\": \"Samsung's display technology has always been at the forefront, but with this TV, they've outdone themselves. Every visual is crisp, the colors are vibrant, and the depth of the blacks is simply mesmerizing. The smart features only add to the luxurious viewing experience.\"},\n    {\"category\": \"TV\", \"product_name\": \"SamsungTV\", \"review\": \"This isn't just a TV; it's a centerpiece for the living room. The ultra-slim bezels and the sleek design make it a visual treat even when it's turned off. And when it's on, the 4K resolution delivers a cinematic experience right at home.\"},\n    {\"category\": \"TV\", \"product_name\": \"SamsungTV\", \"review\": \"The sound quality, often an oversight in many TVs, matches the visual prowess. It creates an enveloping atmosphere that's hard to get without an external sound system. Combined with its user-friendly interface, it's the TV I've always dreamt of.\"},\n\n    # Ergonomic Chair Reviews\n    {\"category\": \"Furniture\", \"product_name\": \"Ergonomic Chair\", \"review\": \"Shifting to this ergonomic chair was a decision I wish I'd made earlier. Not only does it look sophisticated in its design, but the level of comfort is unparalleled. Long hours at the desk now feel less daunting, and my back is definitely grateful.\"},\n    {\"category\": \"Furniture\", \"product_name\": \"Ergonomic Chair\", \"review\": \"The meticulous craftsmanship of this chair is evident. Every component, from the armrests to the wheels, feels premium. The adjustability features mean I can tailor it to my needs, ensuring optimal posture and comfort throughout the day.\"},\n    {\"category\": \"Furniture\", \"product_name\": \"Ergonomic Chair\", \"review\": \"I was initially drawn to its aesthetic appeal, but the functional benefits have been profound. The breathable material ensures no discomfort even after prolonged use, and the robust build gives me confidence that it's a chair built to last.\"},\n]</span></pre>\n <h1>\n  Setting up an In-Memory Database\n </h1>\n <p>\n  To process our data, we\u2019re using an in-memory SQLite database. SQLAlchemy provides an efficient way to model, create, and interact with this database. Here\u2019s how our\n  <code class=\"cw qk ql qm qc b\">\n   product_reviews\n  </code>\n  table structure looks:\n </p>\n <ul>\n  <li>\n   <code class=\"cw qk ql qm qc b\">\n    id\n   </code>\n   (Integer, Primary Key)\n  </li>\n  <li>\n   <code class=\"cw qk ql qm qc b\">\n    category\n   </code>\n   (String)\n  </li>\n  <li>\n   <code class=\"cw qk ql qm qc b\">\n    product_name\n   </code>\n   (String)\n  </li>\n  <li>\n   <code class=\"cw qk ql qm qc b\">\n    review\n   </code>\n   (String, Not Null)\n  </li>\n </ul>\n <p>\n  Once we\u2019ve defined our table structure, we populate it with our sample dataset.\n </p>\n <pre><span class=\"qf no gt qc b bf qg qh l qi qj\" id=\"e17e\">engine = create_engine(<span class=\"hljs-string\">\"sqlite:///:memory:\"</span>)\nmetadata_obj = MetaData()\n\n<span class=\"hljs-comment\"># create product reviews SQL table</span>\ntable_name = <span class=\"hljs-string\">\"product_reviews\"</span>\ncity_stats_table = Table(\n    table_name,\n    metadata_obj,\n    Column(<span class=\"hljs-string\">\"id\"</span>, Integer(), primary_key=<span class=\"hljs-literal\">True</span>),\n    Column(<span class=\"hljs-string\">\"category\"</span>, String(<span class=\"hljs-number\">16</span>), primary_key=<span class=\"hljs-literal\">True</span>),\n    Column(<span class=\"hljs-string\">\"product_name\"</span>, Integer),\n    Column(<span class=\"hljs-string\">\"review\"</span>, String(<span class=\"hljs-number\">16</span>), nullable=<span class=\"hljs-literal\">False</span>)\n)\nmetadata_obj.create_all(engine)\n\nsql_database = SQLDatabase(engine, include_tables=[<span class=\"hljs-string\">\"product_reviews\"</span>])\n\n<span class=\"hljs-keyword\">for</span> row <span class=\"hljs-keyword\">in</span> rows:\n    stmt = insert(city_stats_table).values(**row)\n    <span class=\"hljs-keyword\">with</span> engine.connect() <span class=\"hljs-keyword\">as</span> connection:\n        cursor = connection.execute(stmt)\n        connection.commit()</span></pre>\n <h1>\n  Analysing Product Reviews \u2014 Text2SQL + RAG\n </h1>\n <p>\n  Deriving insights from data often requires intricate questioning.\n </p>\n <p>\n  SQL + RAG in LlamaIndex simplifies this by breaking it into a three-step process:\n </p>\n <ol>\n  <li>\n   <strong>\n    Decomposition of the Question:\n   </strong>\n  </li>\n </ol>\n <ul>\n  <li>\n   Primary Query Formation: Frame the main question in natural language to extract preliminary data from the SQL table.\n  </li>\n  <li>\n   Secondary Query Formation: Construct an auxiliary question to refine or interpret the results of the primary query.\n  </li>\n </ul>\n <p>\n  <strong>\n   2. Data Retrieval\n  </strong>\n  : Run the primary query using the Text2SQL LlamaIndex module to obtain the initial set of results.\n </p>\n <p>\n  <strong>\n   3. Final Answer Generation:\n  </strong>\n  Use List Index to further refine the results based on the secondary question, leading to the conclusive answer.\n </p>\n <p>\n  Let\u2019s start doing it step by step.\n </p>\n <h1>\n  Decomposing User Query into Two Phases\n </h1>\n <p>\n  When working with a relational database, it\u2019s often helpful to break down user queries into more manageable parts. This makes it easier to retrieve accurate data from our database and subsequently process or interpret this data to meet the user\u2019s needs. We\u2019ve designed an approach to decompose queries into two distinct questions by giving an example to\n  <code class=\"cw qk ql qm qc b\">\n   gpt-3.5-turbo\n  </code>\n  model to generate two distinct questions.\n </p>\n <p>\n  Let\u2019s apply this to the query \u201cGet the summary of reviews of Iphone13\u201d and our system would generate:\n </p>\n <ul>\n  <li>\n   Database Query: \u201cRetrieve reviews related to iPhone13 from the table.\u201d\n  </li>\n  <li>\n   Interpretation Query: \u201cSummarize the retrieved reviews.\u201d\n  </li>\n </ul>\n <p>\n  This approach ensures that we cater to both the data retrieval and data interpretation needs, resulting in more accurate and tailored responses to user queries.\n </p>\n <pre><span class=\"qf no gt qc b bf qg qh l qi qj\" id=\"4424\">def generate_questions(user_query: str) -<span class=\"hljs-symbol\">&amp;gt;</span> List[str]:\n  system_message = '''\n  You are given with Postgres table with the following columns.\n\n  city_name, population, country, reviews.\n\n  Your task is to decompose the given question into the following two questions.\n\n  1. Question in natural language that needs to be asked to retrieve results from the table.\n  2. Question that needs to be asked on the top of the result from the first question to provide the final answer.\n\n  Example:\n\n  Input:\n  How is the culture of countries whose population is more than 5000000\n\n  Output:\n  1. Get the reviews of countries whose population is more than 5000000\n  2. Provide the culture of countries\n  '''\n\n  messages = [\n      ChatMessage(role=\"system\", content=system_message),\n      ChatMessage(role=\"user\", content=user_query),\n  ]\n  generated_questions = llm.chat(messages).message.content.split('\\n')\n\n  return generated_questions\n\nuser_query = \"Get the summary of reviews of Iphone13\"\n\ntext_to_sql_query, rag_query = generate_questions(user_query)</span></pre>\n <h1>\n  Data Retrieval \u2014 Executing the Primary Query\n </h1>\n <p>\n  When we decompose a user\u2019s question into its constituent parts, the first step is to convert the \u201cDatabase Query in Natural Language\u201d into an actual SQL query that can be run against our database. In this section, we\u2019ll use the LlamaIndex\u2019s\n  <code class=\"cw qk ql qm qc b\">\n   NLSQLTableQueryEngine\n  </code>\n  to handle the conversion and execution of this SQL query.\n </p>\n <p>\n  <strong>\n   Setting up the NLSQLTableQueryEngine:\n  </strong>\n </p>\n <p>\n  The\n  <code class=\"cw qk ql qm qc b\">\n   NLSQLTableQueryEngine\n  </code>\n  is a powerful tool that takes natural language queries and converts them into SQL queries. We initiate this by providing the necessary details:\n </p>\n <ul>\n  <li>\n   <code class=\"cw qk ql qm qc b\">\n    sql_database\n   </code>\n   : This represents our SQL database connection details.\n  </li>\n  <li>\n   <code class=\"cw qk ql qm qc b\">\n    tables\n   </code>\n   : We specify which table(s) our query will be run against. In this scenario, we're targeting the\n   <code class=\"cw qk ql qm qc b\">\n    product_reviews\n   </code>\n   table.\n  </li>\n  <li>\n   <code class=\"cw qk ql qm qc b\">\n    synthesize_response\n   </code>\n   : When set to\n   <code class=\"cw qk ql qm qc b\">\n    False\n   </code>\n   , this ensures we receive raw SQL responses without additional synthesis.\n  </li>\n  <li>\n   <code class=\"cw qk ql qm qc b\">\n    service_context\n   </code>\n   : This is an optional parameter, which could be used to provide service-specific settings or plugins.\n  </li>\n </ul>\n <pre><span class=\"qf no gt qc b bf qg qh l qi qj\" id=\"b34a\">sql_query_engine = NLSQLTableQueryEngine(\n    sql_database=sql_database,\n    tables=[<span class=\"hljs-string\">\"product_reviews\"</span>],\n    synthesize_response=<span class=\"hljs-literal\">False</span>,\n    service_context=service_context\n)</span></pre>\n <p>\n  <strong>\n   Executing the natural language Query:\n  </strong>\n </p>\n <p>\n  After setting up the engine, the next step is executing our natural language query against it. The engine\u2019s\n  <code class=\"cw qk ql qm qc b\">\n   query()\n  </code>\n  method is used for this purpose.\n </p>\n <pre><span class=\"qf no gt qc b bf qg qh l qi qj\" id=\"e8a0\">sql_response = sql_query_engine.query(text_to_sql_query)</span></pre>\n <p>\n  <strong>\n   Processing the SQL Response:\n  </strong>\n </p>\n <p>\n  The result of our SQL query is usually a list of rows (with each row represented as a list of reviews). To make it more readable and usable for the third step of processing summarizing reviews, we convert this result into a single string.\n </p>\n <pre><span class=\"qf no gt qc b bf qg qh l qi qj\" id=\"1ff9\">sql_response_list = ast.literal_eval(sql_response.response)\ntext = [<span class=\"hljs-string\">' '</span>.join(t) <span class=\"hljs-keyword\">for</span> t <span class=\"hljs-keyword\">in</span> sql_response_list]\ntext = <span class=\"hljs-string\">' '</span>.join(text)</span></pre>\n <p>\n  You can check the generated SQL query in\n  <code class=\"cw qk ql qm qc b\">\n   sql_response.metadata[\"sql_query\"].\n  </code>\n </p>\n <p>\n  By following this process, we\u2019re able to seamlessly integrate natural language processing with SQL query execution. Let\u2019s go with the last step in this process for getting a summary of the reviews.\n </p>\n <h1>\n  Refining and Interpreting the reviews with ListIndex:\n </h1>\n <p>\n  After obtaining the primary set of results from the SQL query, there are often situations where further refinement or interpretation is required. This is where\n  <code class=\"cw qk ql qm qc b\">\n   ListIndex\n  </code>\n  from LlamaIndex plays a crucial role. It allows us to execute the secondary question on our obtained text data to get a refined answer.\n </p>\n <pre><span class=\"qf no gt qc b bf qg qh l qi qj\" id=\"86a3\">listindex = ListIndex([Document(text=text)])\nlist_query_engine = listindex.as_query_engine()\n\nresponse = list_query_engine.query(rag_query)\n\n<span class=\"hljs-built_in\">print</span>(response.response)</span></pre>\n <p>\n  Now let\u2019s wrap everything under a function and try out a few interesting examples:\n </p>\n <pre><span class=\"qf no gt qc b bf qg qh l qi qj\" id=\"1697\"><span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"Function to perform SQL+RAG\"</span><span class=\"hljs-string\">\"\"</span>\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">sql_rag</span>(<span class=\"hljs-params\"><span class=\"hljs-symbol\">user_query:</span> str</span>) -&amp;gt; <span class=\"hljs-symbol\">str:</span>\n  text_to_sql_query, rag_query = generate_questions(user_query)\n\n  sql_response = sql_query_engine.query(text_to_sql_query)\n\n  sql_response_list = ast.literal_eval(sql_response.response)\n\n  text = [<span class=\"hljs-string\">' '</span>.join(t) <span class=\"hljs-keyword\">for</span> t <span class=\"hljs-keyword\">in</span> sql_response_list]\n  text = <span class=\"hljs-string\">' '</span>.join(text)\n\n  listindex = <span class=\"hljs-title class_\">ListIndex</span>([<span class=\"hljs-title class_\">Document</span>(text=text)])\n  list_query_engine = listindex.as_query_engine()\n\n  summary = list_query_engine.query(rag_query)\n\n  <span class=\"hljs-keyword\">return</span> summary.response</span></pre>\n <h2>\n  Examples:\n </h2>\n <pre><span class=\"qf no gt qc b bf qg qh l qi qj\" id=\"9432\">sql_rag(\"How is the sentiment of SamsungTV product?\")</span></pre>\n <p>\n  <em class=\"rd\">\n   The sentiment of the reviews for the Samsung TV product is generally positive. Users express satisfaction with the picture clarity, vibrant colors, and stunning picture quality. They appreciate the smart features, user-friendly interface, and easy connectivity options. The sleek design and wall-mounting capability are also praised. The ambient mode, gaming mode, and HDR content are mentioned as standout features. Users find the remote control with voice command convenient and appreciate the regular software updates. However, some users mention that the sound quality could be better and suggest using an external audio system. Overall, the reviews indicate that the Samsung TV is considered a solid investment for quality viewing.\n  </em>\n </p>\n <pre><span class=\"qf no gt qc b bf qg qh l qi qj\" id=\"9063\">sql_rag(\"Are people happy with Ergonomic Chair?\")</span></pre>\n <p>\n  <em class=\"rd\">\n   The overall satisfaction of people with the Ergonomic Chair is high.\n  </em>\n </p>\n <p>\n  You can play around with the approach and dataset in the Google Colab Notebook \u2014\n  <a href=\"https://colab.research.google.com/drive/13le_rgEo-waW5ZWjWDEyUf64R6n_4Cez?usp=sharing\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   here\n  </a>\n  .\n </p>\n <h1>\n  Conclusion\n </h1>\n <p>\n  In the era of e-commerce, where user reviews dictate the success or failure of products, the ability to rapidly analyze and interpret vast swaths of textual data is paramount. LlamaIndex, through its ingenious integration of SQL and RAG, offers businesses a powerful tool to glean actionable insights from such datasets. By seamlessly blending structured SQL queries with the abstraction of natural language processing, we\u2019ve showcased a streamlined approach to transform vague user queries into precise, informative answers.\n </p>\n <p>\n  With this approach, businesses can now efficiently sift through mountains of reviews, extract the essence of user sentiments, and make informed decisions. Whether it\u2019s about gauging the overall sentiment for a product, understanding specific feature feedback, or even tracking the evolution of reviews over time, the Text2SQL+RAG methodology in LlamaIndex is the harbinger of a new age of data analytics.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 16990, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2394993-0632-41dc-9b85-fb6398ac127d": {"__data__": {"id_": "b2394993-0632-41dc-9b85-fb6398ac127d", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff.html", "file_name": "llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff.html", "file_type": "text/html", "file_size": 4196, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff.html", "file_name": "llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff.html", "file_type": "text/html", "file_size": 4196, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "a18719eb34c25302fb18b8f61b9b0530f090c75cd4348b8e0e7766eee55bd4e0", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Hi, I\u2019m Laurie, and today is my first day as VP of Developer Relations at LlamaIndex!\n </p>\n <p>\n  Quick background on me: I started my career 27 years ago as a web developer, founded a couple of companies including\n  <a href=\"https://www.npmjs.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   npm Inc.\n  </a>\n  , and have always been about talking to developers about the state of the development world and how we fit into it, whether that\u2019s on my\n  <a href=\"https://seldo.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   personal site\n  </a>\n  , in\n  <a href=\"https://www.youtube.com/watch?v=gChULw-uEjY&amp;ab_channel=JSConf\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   conference talks\n  </a>\n  , or in big\n  <a href=\"https://jamstack.org/survey/2022/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   community surveys\n  </a>\n  .\n </p>\n <p>\n  I wrote last week about why\n  <a href=\"https://seldo.com/posts/ai-ml-llms-and-the-future-of-software\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LLMs are the future of software\n  </a>\n  . To summarize that post: until now computers have been very good at ingesting, sorting, and transmitting data, but understanding what they were working with was beyond them. Very recently, with tools like\n  <a href=\"https://openai.com/research/gpt-4\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   GPT-4\n  </a>\n  and\n  <a href=\"https://ai.meta.com/llama/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Llama 2\n  </a>\n  that threshold has been crossed. Software can now read, summarize, and make novel connections within arbitrarily large sets of data. It can write software, it can use tools, it can generate text, images and music. A huge new set of capabilities have been unlocked.\n </p>\n <p>\n  The last time I saw a shift this big in the technological landscape was probably the original iPhone. Suddenly, everybody who was doing everything needed to also do it for mobile devices. You sell real estate? Now you do it with an app. You run a dating site? Now you do it with an app. There was a gigantic rush as everyone who did everything in software suddenly saw huge benefits from moving onto a new platform.\n </p>\n <p>\n  But the most interesting applications that platform shift enabled were the ones that couldn\u2019t have existed before. Uber relies on everyone \u2014 drivers as well as customers \u2014 already owning a piece of mobile hardware that can connect to the Internet and use GPS to locate them. Prior to the mobile revolution, that would have been an absurd business plan. After it happened, it was taken for granted. So in the same way, I\u2019m especially interested in seeing what the applications of LLMs are that were simply impossible before.\n </p>\n <p>\n  All of which is why I\u2019m delighted to be at a company at the center of the LLM universe. If you\u2019re new to LlamaIndex, it\u2019s a Python and JavaScript framework that lets you quickly put together totally customizable, production-class applications that use LLMs. We let you ingest data from dozens of sources and use any model you want, and we make it simple to link everything together.\n </p>\n <p>\n  The team at LlamaIndex is absolutely amazing and I am looking forward to working with Jerry, Simon and Yi. Yi will be focusing more on partnerships.\n </p>\n <p>\n  Want to see a demo? Our\n  <a href=\"https://www.secinsights.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   SEC Insights\n  </a>\n  app lets you ingest regulatory documents from major corporations and then ask questions about them. Want to dive and build your own? Get started in\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/getting_started/installation.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Python\n  </a>\n  or\n  <a href=\"https://ts.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   JavaScript\n  </a>\n  !\n </p>\n <p>\n  P.S. If you\u2019re wondering \u201cwhy an alpaca?\u201d it\u2019s because my\n  <a href=\"https://seldo.com/images/processed/floof-with-border.svg\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   personal mascot\n  </a>\n  is an alpaca and has been for years. Alpacas and llamas are a great match!\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4169, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec34043d-40a0-4bd1-9d3f-3c1d34b74d6f": {"__data__": {"id_": "ec34043d-40a0-4bd1-9d3f-3c1d34b74d6f", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.html", "file_name": "llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.html", "file_type": "text/html", "file_size": 26130, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.html", "file_name": "llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.html", "file_type": "text/html", "file_size": 26130, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "a3eab900960a3de1f2b0c04711592f525cf82796484bca03547eb624bc226c97", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  (co-authored by Jerry Liu, CEO of LlamaIndex, Jeffrey Wang, co-founder at Metaphor, and Adam Hoffman, Software Engineer at Hypotenuse Labs)\n </p>\n <p>\n  We\u2019re incredibly excited to launch an\n  <a href=\"https://llamahub.ai/l/tools-metaphor\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   integration\n  </a>\n  between LlamaIndex and\n  <a href=\"https://platform.metaphor.systems/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Metaphor\n  </a>\n  : combine the capabilities of LlamaIndex data agents with Metaphor as a\n  <em class=\"on\">\n   native\n  </em>\n  LLM search tool to enable knowledge workers capable of answering any question over any data, no matter how recent or complex.\n </p>\n <p>\n  We provide a deeper overview of Metaphor and the LlamaIndex integration below. We also walk through our\n  <a href=\"https://github.com/emptycrown/llama-hub/blob/main/llama_hub/tools/notebooks/metaphor.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   example notebook\n  </a>\n  to showcase how they can be combined.\n </p>\n <h1>\n  Background/Context\n </h1>\n <p>\n  State-of-the art large language models (LLMs) such as ChatGPT, GPT-4, Claude 2 have incredible reasoning capabilities that unlock a wide variety of use cases \u2014 from insight extraction to question-answering to general workflow automation. Yet they are limited in their abilities to retrieve contextually relevant information. A popular stack that has emerged is to setup a retrieval-augmented generation (RAG) system, which combines LLMs with external storage solutions over a static knowledge source. Frameworks such as LlamaIndex provide a variety of tools to setup both simple and complex RAG systems.\n </p>\n <p>\n  Yet even this is not the complete picture. LLMs should ideally be able to dynamically search and retrieve information from the external world, not just depend on a static source of knowledge. This would allow them to fulfill a more general set of tasks and not only perform search/retrieval, but perform actions as well.\n </p>\n <p>\n  To do this well, we need two core components:\n </p>\n <ul>\n  <li>\n   General abstractions that allow LLMs to intelligently perform various tasks over your data, in both a \u201cread\u201d and \u201cwrite\u201d fashion\n  </li>\n  <li>\n   A good search engine tailored for LLM use\n  </li>\n </ul>\n <p>\n  LlamaIndex\n  <a href=\"https://medium.com/llamaindex-blog/data-agents-eed797d7972f\" rel=\"noopener\">\n   data agent abstractions\n  </a>\n  help to satisfy the first core component. A complete data agent consists of both a reasoning loop as well as a set of Tools. These tools can be interfaces for search/retrieval or more generally any external API. Given a query, the agent will execute its reasoning loop and dynamically figure out the set of Tools it will need to fulfill the task at hand.\n </p>\n <p>\n  Data agents have access to a rich set of Tools offered on\n  <a href=\"https://llamahub.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaHub\n  </a>\n  \u2014 these range from Gmail API, to a SQL db API, to a basic tool in the form of Bing search. We\u2019ve shown that they are capable of e2e tasks from\n  <a href=\"https://github.com/emptycrown/llama-hub/blob/main/llama_hub/tools/notebooks/gmail.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   sending emails\n  </a>\n  ,\n  <a href=\"https://github.com/emptycrown/llama-hub/blob/main/llama_hub/tools/notebooks/google_calendar.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   scheduling meetings\n  </a>\n  , to automating\n  <a href=\"https://github.com/emptycrown/llama-hub/blob/main/llama_hub/tools/notebooks/shopify.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   custom support insight extraction\n  </a>\n  . Yet there has never been a tool tailored for LLM use.\n </p>\n <h1>\n  Overview of Metaphor\n </h1>\n <p>\n  The Metaphor API is designed to connect your LLM to the internet. It allows you to perform fully neural, highly semantic searches over the Internet and also get clean, HTML content from the results.\n </p>\n <p>\n  Metaphor was trained to predict links on the internet, given how people talk about things on the Internet. For example, someone might post about a great article they read like this:\n </p>\n <blockquote>\n  <p class=\"na nb on nc b nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx gm bj\" id=\"6c08\">\n   Found an amazing article I read about the history of Rome\u2019s architecture: [LINK]\n  </p>\n </blockquote>\n <p>\n  By training a model to predict these links given how people talk about them, the end result is a totally different way to search the internet \u2014\n  <em class=\"on\">\n   search as if you\u2019re about to share the link you want\n  </em>\n  . While a little unintuitive at first, searching this way can return extremely high quality results. But for the purposes of LlamaIndex, you won\u2019t need to worry about this because by default, queries will be converted into Metaphor prompts.\n </p>\n <p>\n  Why would you use Metaphor Search over Bing/Google? There are 3 main reasons:\n </p>\n <ul>\n  <li>\n   You can search fully semantically, for instance with feelings or complex descriptors.\n  </li>\n  <li>\n   You can search only for the type of entity that you want. Companies, articles, people.\n  </li>\n  <li>\n   You can find content that Google simply doesn\u2019t surface well, maybe because keywords aren\u2019t the right tool or maybe just because Google doesn\u2019t care about returning good results for that type of content.\n  </li>\n </ul>\n <p>\n  To learn more, you can read the full Metaphor API\n  <a href=\"https://platform.metaphor.systems/blog/building-search-for-the-post-chatgpt-world\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   blog post\n  </a>\n  .\n </p>\n <h1>\n  Integration Details\n </h1>\n <p>\n  The\n  <a href=\"https://llamahub.ai/l/tools-metaphor\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Metaphor Tool Spec in LlamaHub\n  </a>\n  is an API interface that consists of 5 tools that an agent can use.\n </p>\n <ul>\n  <li>\n   <strong>\n    Search:\n   </strong>\n   The entrypoint to Metaphor \u2014 allows an agent to pass a natural language query that will then be passed to the Metaphor search engine. This endpoint also contains some additional parameters, such as the number of results, domains to include/exclude, and a date filter.\n  </li>\n  <li>\n   <strong>\n    Retrieve Documents:\n   </strong>\n   This will retrieve the content of a set of documents given IDs. These ids are returned as part of the results from the search endpoint above.\n  </li>\n  <li>\n   <strong>\n    Search and Retrieve Documents:\n   </strong>\n   This is a convenience endpoint that combines the functionality of `search` and `retrieve_documents`.\n  </li>\n  <li>\n   <strong>\n    Find Similar:\n   </strong>\n   This directly calls an endpoint offered by Metaphor, which will return a list of documents similar to a given URL.\n  </li>\n  <li>\n   <strong>\n    Current Date:\n   </strong>\n   This is a convenience function that returns the current date. On its own it is unrelated to Metaphor\u2019s API, but may be called beforehand to figure out the right date filters to pass to some of Metaphor\u2019s endpoints.\n  </li>\n </ul>\n <p>\n  In the next section, let\u2019s walk through how a data agent can make use of these endpoints through various use cases.\n </p>\n <h1>\n  Example Walkthrough\n </h1>\n <p>\n  Let\u2019s walk through our example notebook showing how LlamaIndex data agents can be used with Metaphor.\n </p>\n <p>\n  <strong>\n   Testing the Metaphor Tools\n  </strong>\n </p>\n <p>\n  The first step is to import the Metaphor tool spec.\n </p>\n <pre><span class=\"qj os gt qg b bf qk ql l qm qn\" id=\"cd91\"><span class=\"hljs-comment\"># Set up Metaphor tool</span>\n<span class=\"hljs-keyword\">from</span> llama_hub.tools.metaphor.base <span class=\"hljs-keyword\">import</span> MetaphorToolSpec\nmetaphor_tool = MetaphorToolSpec(\napi_key=<span class=\"hljs-string\">'your-key'</span>,\n)\n<span class=\"hljs-comment\"># convert tool spec to a list of tools</span>\nmetaphor_tool_list = metaphor_tool.to_tool_list()\n<span class=\"hljs-keyword\">for</span> tool <span class=\"hljs-keyword\">in</span> metaphor_tool_list:\n<span class=\"hljs-built_in\">print</span>(tool.metadata.name)</span></pre>\n <p>\n  In this walkthrough, we make use of all of the tools. But you\u2019re free to pick and choose to use specific tools if you want to define a more custom workflow and restrict the agent action space.\n </p>\n <p>\n  We can play around with the set of tools before defining our agent. All of our Metaphor tools make use of the `AutoPrompt` option where Metaphor will pass a query through an LLM to refine and improve the query.\n </p>\n <p>\n  Example input:\n </p>\n <pre><span class=\"qj os gt qg b bf qk ql l qm qn\" id=\"2c38\">metaphor_tool.search('machine learning transformers', num_results=3)</span></pre>\n <p>\n  Example output:\n </p>\n <pre><span class=\"qj os gt qg b bf qk ql l qm qn\" id=\"6cae\">[{'title': 'On the potential of Transformers in Reinforcement Learning',\n'url': 'https://lorenzopieri.com/rl_transformers/',\n'id': 'ysJlYSgeGW3l4zyOBoSGcg'},\n{'title': 'Transformers: Attention in Disguise',\n'url': 'https://www.mihaileric.com/posts/transformers-attention-in-disguise/',\n'id': 'iEYMai5rS9k0hN5_BH0VZg'},\n{'title': 'Transformers in Computer Vision: Farewell Convolutions!',\n'url': 'https://towardsdatascience.com/transformers-in-computer-vision-farewell-convolutions-f083da6ef8ab?gi=a1d0a9a2896c',\n'id': 'kX1Z89DdjSvBrH1S1XLvwg'}]</span></pre>\n <p>\n  The notebook also contains examples of us playing around with the other endpoints:\n  <code class=\"cw qo qp qq qg b\">\n   retrieve_documents\n  </code>\n  ,\n  <code class=\"cw qo qp qq qg b\">\n   find_similar\n  </code>\n  ,\n  <code class=\"cw qo qp qq qg b\">\n   search_and_retrieve_documents\n  </code>\n  .\n </p>\n <p>\n  <strong>\n   Setting up an OpenAI Function Calling Agent with Metaphor\n  </strong>\n </p>\n <p>\n  We can create an agent with access to all of the above tools and start testing it out:\n </p>\n <pre><span class=\"qj os gt qg b bf qk ql l qm qn\" id=\"05ee\"><span class=\"hljs-keyword\">from</span> llama_index.agent <span class=\"hljs-keyword\">import</span> OpenAIAgent\n<span class=\"hljs-comment\"># We don't give the Agent our unwrapped retrieve document tools, instead passing the wrapped tools</span>\nagent = OpenAIAgent.from_tools(\n  metaphor_tool_list,\n  verbose=<span class=\"hljs-literal\">True</span>,\n)</span></pre>\n <p>\n  That\u2019s it in terms of setup! Let\u2019s try giving an example query:\n </p>\n <pre><span class=\"qj os gt qg b bf qk ql l qm qn\" id=\"29c0\"><span class=\"hljs-built_in\">print</span>(agent.chat(<span class=\"hljs-string\">'What are the best restaurants in toronto?\"))</span></span></pre>\n <p>\n  We walk through the execution trace of this agent to see how it is interacting with the Metaphor tool.\n </p>\n <pre><span class=\"qj os gt qg b bf qk ql l qm qn\" id=\"a4ef\">=== <span class=\"hljs-title class_\">Calling</span> <span class=\"hljs-title class_\">Function</span> ===\n<span class=\"hljs-title class_\">Calling</span> <span class=\"hljs-attr\">function</span>: search <span class=\"hljs-keyword\">with</span> <span class=\"hljs-attr\">args</span>: {\n  <span class=\"hljs-string\">\"query\"</span>: <span class=\"hljs-string\">\"best restaurants in Toronto\"</span>\n}\n[<span class=\"hljs-title class_\">Metaphor</span> <span class=\"hljs-title class_\">Tool</span>] <span class=\"hljs-title class_\">Autoprompt</span> <span class=\"hljs-attr\">string</span>: <span class=\"hljs-title class_\">Here</span><span class=\"hljs-string\">'s a link to the best restaurant in Toronto:\nGot output: [{'</span>title<span class=\"hljs-string\">': '</span><span class=\"hljs-title class_\">Via</span> <span class=\"hljs-title class_\">Allegro</span> <span class=\"hljs-title class_\">Ristorante</span> - <span class=\"hljs-title class_\">Toronto</span> <span class=\"hljs-title class_\">Fine</span> <span class=\"hljs-title class_\">Dining</span> <span class=\"hljs-title class_\">Restaurant</span><span class=\"hljs-string\">', '</span>url<span class=\"hljs-string\">': '</span><span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//viaallegroristorante.com/', 'id': 'EVlexzJh-lzkVr4tb2y_qw'}, {'title': 'The Senator \u2013 Home', 'url': 'https://thesenator.com/', 'id': 'dA3HVr5P8E0Bs7nH2gH7ZQ'}, {'title': 'Home - The Rushton', 'url': 'https://therushton.com/', 'id': '6Je-igG-i-ApqISC5XXmGQ'}, {'title': 'Location', 'url': 'https://osteriagiulia.ca/', 'id': 'HjP5c54vqb3n3UNa3HevSA'}, {'title': 'StockYards | Stockyards Toronto', 'url': 'https://www.thestockyards.ca/', 'id': 'Pffz-DQlOepqVgKQDmW5Ig'}, {'title': 'Select A Restaurant', 'url': 'https://www.torontopho.com/', 'id': 'DiQ1hU1gmrIzpKnOaVvZmw'}, {'title': 'Home | Kit Kat Italian Bar &amp;amp; Grill', 'url': 'http://www.kitkattoronto.com/', 'id': 'kdAcLioBgnwzuHyd0rWS1w'}, {'title': 'La Fenice', 'url': 'https://www.lafenice.ca/', 'id': 'M-LHQZP6V40V81fqLFAQxQ'}, {'title': 'Le Ph\u00e9nix', 'url': 'https://www.lephenixto.com/', 'id': 'spCTcFr0GHlFUTzyngfRVw'}, {'title': 'ITALIAN, INSPIRED.', 'url': 'https://figotoronto.com/', 'id': 'OvBcTqEo1tCSywr4ATptCg'}]</span>\n========================\n<span class=\"hljs-title class_\">Here</span> are some <span class=\"hljs-keyword\">of</span> the best restaurants <span class=\"hljs-keyword\">in</span> <span class=\"hljs-title class_\">Toronto</span>:\n\n<span class=\"hljs-number\">1.</span> [<span class=\"hljs-title class_\">Via</span> <span class=\"hljs-title class_\">Allegro</span> <span class=\"hljs-title class_\">Ristorante</span>](<span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//viaallegroristorante.com/)</span>\n<span class=\"hljs-number\">2.</span> [<span class=\"hljs-title class_\">The</span> <span class=\"hljs-title class_\">Senator</span>](<span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//thesenator.com/)</span>\n<span class=\"hljs-number\">3.</span> [<span class=\"hljs-title class_\">The</span> <span class=\"hljs-title class_\">Rushton</span>](<span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//therushton.com/)</span>\n<span class=\"hljs-number\">4.</span> [<span class=\"hljs-title class_\">Osteria</span> <span class=\"hljs-title class_\">Giulia</span>](<span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//osteriagiulia.ca/)</span>\n<span class=\"hljs-number\">5.</span> [<span class=\"hljs-title class_\">Stockyards</span>](<span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//www.thestockyards.ca/)</span>\n<span class=\"hljs-number\">6.</span> [<span class=\"hljs-title class_\">Toronto</span> <span class=\"hljs-title class_\">Pho</span>](<span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//www.torontopho.com/)</span>\n<span class=\"hljs-number\">7.</span> [<span class=\"hljs-title class_\">Kit</span> <span class=\"hljs-title class_\">Kat</span> <span class=\"hljs-title class_\">Italian</span> <span class=\"hljs-title class_\">Bar</span> &amp;amp; <span class=\"hljs-title class_\">Grill</span>](<span class=\"hljs-attr\">http</span>:<span class=\"hljs-comment\">//www.kitkattoronto.com/)</span>\n<span class=\"hljs-number\">8.</span> [<span class=\"hljs-title class_\">La</span> <span class=\"hljs-title class_\">Fenice</span>](<span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//www.lafenice.ca/)</span>\n<span class=\"hljs-number\">9.</span> [<span class=\"hljs-title class_\">Le</span> <span class=\"hljs-title class_\">Ph</span>\u00e9nix](<span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//www.lephenixto.com/)</span>\n<span class=\"hljs-number\">10.</span> [<span class=\"hljs-title class_\">Figo</span>](<span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//figotoronto.com/)</span>\n\n<span class=\"hljs-title class_\">You</span> can visit their websites <span class=\"hljs-keyword\">for</span> more information. <span class=\"hljs-title class_\">Enjoy</span> your dining experience <span class=\"hljs-keyword\">in</span> <span class=\"hljs-title class_\">Toronto</span>!</span></pre>\n <p>\n  The execution trace shows that the agent is simply calling the `search` endpoint with \u201cbest restaurants in Toronto\u201d, and returning that as a list of dictionaries representing the search results.\n </p>\n <p>\n  Note that we can ask a followup question as well:\n </p>\n <pre><span class=\"qj os gt qg b bf qk ql l qm qn\" id=\"21a6\"><span class=\"hljs-built_in\">print</span>(agent.chat(<span class=\"hljs-string\">'tell me more about Osteria Giulia'</span>))</span></pre>\n <p>\n  And we get the following result (note: we truncate some of the intermediate output):\n </p>\n <pre><span class=\"qj os gt qg b bf qk ql l qm qn\" id=\"375f\">=== <span class=\"hljs-title class_\">Calling</span> <span class=\"hljs-title class_\">Function</span> ===\n<span class=\"hljs-title class_\">Calling</span> <span class=\"hljs-attr\">function</span>: retrieve_documents <span class=\"hljs-keyword\">with</span> <span class=\"hljs-attr\">args</span>: {\n<span class=\"hljs-string\">\"ids\"</span>: [<span class=\"hljs-string\">\"HjP5c54vqb3n3UNa3HevSA\"</span>]\n}\n<span class=\"hljs-title class_\">Got</span> <span class=\"hljs-attr\">output</span>: [\u2026]\n========================\n<span class=\"hljs-title class_\">Osteria</span> <span class=\"hljs-title class_\">Giulia</span> is a restaurant located at <span class=\"hljs-number\">134</span> <span class=\"hljs-title class_\">Avenue</span> <span class=\"hljs-title class_\">Road</span> <span class=\"hljs-keyword\">in</span> <span class=\"hljs-title class_\">Toronto</span>, <span class=\"hljs-title class_\">Ontario</span>. <span class=\"hljs-title class_\">You</span> can contact them at <span class=\"hljs-number\">416.964</span><span class=\"hljs-number\">.8686</span> or via email at info@osteriagiulia.<span class=\"hljs-property\">ca</span> (<span class=\"hljs-keyword\">for</span> general inquiries only, no reservation requests via email).\n<span class=\"hljs-title class_\">The</span> restaurant<span class=\"hljs-string\">'s operating hours are from Monday to Saturday, from 5:00pm to 11:00pm. On Sundays, the restaurant is available for private bookings.\nParking is available on Avenue Road and Davenport Road.\nYou can follow Osteria Giulia on Instagram [@osteriagiulia](https://www.instagram.com/osteriagiulia). They also have a sister restaurant called Giulietta, which you can visit at [giu.ca](https://giu.ca) or on Instagram [@giulietta972](https://www.instagram.com/giulietta972).\nPlease note that the information provided is based on the available document and may be subject to change. It is recommended to visit their official website or contact them directly for the most up-to-date information.</span></span></pre>\n <p>\n  Since \u201cOsteria Giulia\u201d is in the agent conversation history, the agent now knows to call the `retrieve` endpoint to return more information about the relevant search result.\n </p>\n <p>\n  <strong>\n   Advanced: Avoiding Context Window Issues\n  </strong>\n </p>\n <p>\n  One issue with using\n  <code class=\"cw qo qp qq qg b\">\n   retrieve\n  </code>\n  is that the content can be quite long. If the content is naively appended to the conversation history and dumped into the LLM context window, then we may run into context window limitations.\n </p>\n <p>\n  LlamaIndex offers tool abstractions to help deal with this. Our\n  <code class=\"cw qo qp qq qg b\">\n   LoadAndSearchToolSpec\n  </code>\n  wraps any given tool that may return a large amount of data, and it splits it into two tools: a load tool that will dynamically store the data in an index, and a search tool that allows for search over that index.\n </p>\n <p>\n  On the Metaphor side, this is also where we define a\n  <code class=\"cw qo qp qq qg b\">\n   search_and_retrieve_documents\n  </code>\n  endpoint that combines\n  <code class=\"cw qo qp qq qg b\">\n   search\n  </code>\n  and\n  <code class=\"cw qo qp qq qg b\">\n   retrieve\n  </code>\n  . This allows the agent to make a single query to retrieve a large number of documents, which when combined with the\n  <code class=\"cw qo qp qq qg b\">\n   LoadAndSearchToolSpec\n  </code>\n  will get directly stored within an index. If the agent were to call\n  <code class=\"cw qo qp qq qg b\">\n   search\n  </code>\n  and\n  <code class=\"cw qo qp qq qg b\">\n   retrieve\n  </code>\n  separately, then it would both take longer and consume more tokens to write the search results to conversation history, and then passing that into the prompt again to call\n  <code class=\"cw qo qp qq qg b\">\n   retrieve\n  </code>\n  over all document IDs.\n </p>\n <p>\n  Creating the\n  <code class=\"cw qo qp qq qg b\">\n   LoadAndSearchToolSpec\n  </code>\n  :\n </p>\n <pre><span class=\"qj os gt qg b bf qk ql l qm qn\" id=\"e255\"><span class=\"hljs-keyword\">from</span> llama_index.tools.tool_spec.load_and_search.base <span class=\"hljs-keyword\">import</span> LoadAndSearchToolSpec\n<span class=\"hljs-comment\"># The search_and_retrieve_documents tool is the third in the tool list, as seen above</span>\nwrapped_retrieve = LoadAndSearchToolSpec.from_defaults(\n  metaphor_tool_list[<span class=\"hljs-number\">2</span>],\n)</span></pre>\n <p>\n  Now let\u2019s walk through a full execution example:\n </p>\n <pre><span class=\"qj os gt qg b bf qk ql l qm qn\" id=\"6050\"><span class=\"hljs-comment\"># Just pass the wrapped tools and the get_date utility</span>\nagent = OpenAIAgent.from_tools(\n  [*wrapped_retrieve.to_tool_list(), metaphor_tool_list[<span class=\"hljs-number\">4</span>]],\n  verbose=<span class=\"hljs-literal\">True</span>,\n)\n<span class=\"hljs-built_in\">print</span>(agent.chat(<span class=\"hljs-string\">'Can you summarize everything published in the last month regarding news on superconductors'</span>))</span></pre>\n <p>\n  The output here shows that the agent calls multiple tools in succession to get the right answer.\n </p>\n <pre><span class=\"qj os gt qg b bf qk ql l qm qn\" id=\"bfb3\">=== <span class=\"hljs-title class_\">Calling</span> <span class=\"hljs-title class_\">Function</span> ===\n<span class=\"hljs-title class_\">Calling</span> <span class=\"hljs-attr\">function</span>: current_date <span class=\"hljs-keyword\">with</span> <span class=\"hljs-attr\">args</span>: {}\n<span class=\"hljs-title class_\">Got</span> <span class=\"hljs-attr\">output</span>: <span class=\"hljs-number\">2023</span>-<span class=\"hljs-number\">08</span>-<span class=\"hljs-number\">20</span>\n========================\n=== <span class=\"hljs-title class_\">Calling</span> <span class=\"hljs-title class_\">Function</span> ===\n<span class=\"hljs-title class_\">Calling</span> <span class=\"hljs-attr\">function</span>: search_and_retrieve_documents <span class=\"hljs-keyword\">with</span> <span class=\"hljs-attr\">args</span>: {\n  <span class=\"hljs-string\">\"query\"</span>: <span class=\"hljs-string\">\"superconductors\"</span>,\n  <span class=\"hljs-string\">\"start_published_date\"</span>: <span class=\"hljs-string\">\"2023-07-20\"</span>,\n  <span class=\"hljs-string\">\"end_published_date\"</span>: <span class=\"hljs-string\">\"2023-08-20\"</span>\n}\n[<span class=\"hljs-title class_\">Metaphor</span> <span class=\"hljs-title class_\">Tool</span>] <span class=\"hljs-title class_\">Autoprompt</span>: <span class=\"hljs-string\">\"Here is an interesting article about superconductors:\nGot output: Content loaded! You can now search the information using read_search_and_retrieve_documents\n========================\n=== Calling Function ===\nCalling function: read_search_and_retrieve_documents with args: {\n  \"</span>query<span class=\"hljs-string\">\": \"</span>superconductors<span class=\"hljs-string\">\"\n}\nGot output: \nSuperconductors are materials that can perfectly conduct electricity. They are used in a variety of applications, such as particle accelerators, nuclear fusion devices, MRI machines, and maglev trains. However, so far, no superconductor has been proven to work at ambient pressures and temperatures. On July 22, scientists in South Korea published research claiming to have solved this problem with a material called LK-99, which has an electrical resistivity that drops to near zero at 30 degrees Celsius (86 degrees Fahrenheit).\n========================\nIn the last month, there have been developments in the field of superconductors. Scientists in South Korea have published research on a material called LK-99, which has the ability to conduct electricity with near-zero resistance at a temperature of 30 degrees Celsius (86 degrees Fahrenheit). This breakthrough could potentially lead to the development of superconductors that work at ambient pressures and temperatures, opening up new possibilities for various applications such as particle accelerators, nuclear fusion devices, MRI machines, and maglev trains.</span></span></pre>\n <p>\n  The agent used the\n  <code class=\"cw qo qp qq qg b\">\n   get_date\n  </code>\n  tool to determine the current month, and then applied the filters in Metaphor based on publication date when calling\n  <code class=\"cw qo qp qq qg b\">\n   search\n  </code>\n  . It then loaded the documents using\n  <code class=\"cw qo qp qq qg b\">\n   retrieve_documents\n  </code>\n  and read them using\n  <code class=\"cw qo qp qq qg b\">\n   read_retrieve_documents\n  </code>\n  .\n </p>\n <h1>\n  Conclusion\n </h1>\n <p>\n  As shown above, the integration between LlamaIndex data agents + Metaphor search has the potential to bypass existing limitations with LLMs and even RAG systems. We\u2019re excited to continue exploring this further in future blog posts.\n </p>\n <p>\n  We encourage you to play around with the notebook \u2014 make sure to check it out!\n </p>\n <p>\n  <strong>\n   Resources:\n  </strong>\n </p>\n <ul>\n  <li>\n   Notebook:\n   <a href=\"https://github.com/emptycrown/llama-hub/blob/main/llama_hub/tools/notebooks/metaphor.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    https://github.com/emptycrown/llama-hub/blob/main/llama_hub/tools/notebooks/metaphor.ipynb\n   </a>\n  </li>\n  <li>\n   LlamaHub:\n   <a href=\"https://llamahub.ai/l/tools-metaphor\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    https://llamahub.ai/l/tools-metaphor\n   </a>\n  </li>\n  <li>\n   Metaphor:\n   <a href=\"https://platform.metaphor.systems/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    https://platform.metaphor.systems/\n   </a>\n  </li>\n  <li>\n   Metaphor API Docs:\n   <a href=\"https://docs.metaphor.systems/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    https://docs.metaphor.systems/\n   </a>\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 26063, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd9bd377-d076-4e2f-ad94-4c70fb203431": {"__data__": {"id_": "fd9bd377-d076-4e2f-ad94-4c70fb203431", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-news-special-edition-openai-developer-day-e955f16db4e2.html", "file_name": "llamaindex-news-special-edition-openai-developer-day-e955f16db4e2.html", "file_type": "text/html", "file_size": 3202, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-news-special-edition-openai-developer-day-e955f16db4e2.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-news-special-edition-openai-developer-day-e955f16db4e2.html", "file_name": "llamaindex-news-special-edition-openai-developer-day-e955f16db4e2.html", "file_type": "text/html", "file_size": 3202, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "21f445f1a6402547177b62aef9eb99ade9be75ff9ccda88e9a4a7bc098a5fafe", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Hello Llama fans!\n </p>\n <p>\n  Yesterday was a big day in the world of LLMs; OpenAI held their\n  <a href=\"https://devday.openai.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   developer day conference\n  </a>\n  and there were a slew of new features. The team were all hands on deck to bring support for these features to the library as fast as possible \u2014 which is to say,\n  <a href=\"https://twitter.com/llama_index/status/1721737484961480836\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   the same day\n  </a>\n  !\n </p>\n <p>\n  In case you missed our tweet about it, if you install the\n  <a href=\"https://github.com/run-llama/llama_index/releases\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   latest build\n  </a>\n  of LlamaIndex you\u2019ll get everything below:\n </p>\n <h1>\n  Support for two new models released today\n </h1>\n <ul>\n  <li>\n   <code class=\"cw pe pf pg ph b\">\n    gpt-4-1106-preview\n   </code>\n   , aka\n   <strong>\n    GPT-4 Turbo\n   </strong>\n   , the latest GPT-4 model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and a 128,000 token context window\n  </li>\n  <li>\n   <code class=\"cw pe pf pg ph b\">\n    gpt-4-vision-preview\n   </code>\n   , aka\n   <strong>\n    GPT 4 Turbo with vision\n   </strong>\n   with long-awaited multimodal support, has the ability to understand images in addition to all the other GPT-4 Turbo capabilities.\n  </li>\n </ul>\n <p>\n  You can use these models just as you would any other OpenAI model:\n </p>\n <pre><span class=\"pv nz gt ph b bf pw px l py pz\" id=\"8c37\"><span class=\"hljs-keyword\">from</span> llama_index.llms <span class=\"hljs-keyword\">import</span> OpenAI\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n\nllm = OpenAI(model=<span class=\"hljs-string\">\"gpt-4-1106-preview\"</span>)\nservice_context = ServiceContext.from_defaults(llm=llm)\ndocuments = SimpleDirectoryReader(<span class=\"hljs-string\">\"data\"</span>).load_data()\nindex = VectorStoreIndex.from_documents(\n    documents, service_context=service_context\n)</span></pre>\n <h1>\n  Azure OpenAI endpoints\n </h1>\n <p>\n  Check out the\n  <a href=\"https://docs.llamaindex.ai/en/stable/examples/llm/azure_openai.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   OpenAI Azure notebook\n  </a>\n  for examples.\n </p>\n <h1>\n  New embeddings abstractions\n </h1>\n <p>\n  Including Azure embeddings.\n </p>\n <h1>\n  Function calling\n </h1>\n <p>\n  Check out\n  <a href=\"https://docs.llamaindex.ai/en/stable/examples/llm/openai.html#function-calling\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   our notebook\n  </a>\n  for examples.\n </p>\n <h1>\n  SEC insights\n </h1>\n <p>\n  Our demo of the power of retrieval-augmented generation for financial filings,\n  <a href=\"https://www.secinsights.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   SEC Insights\n  </a>\n  , has been updated to use the latest version of GPT-4! Watch as you instantly get deeper insights and more relevant responses.\n </p>\n <p>\n  Look out for more OpenAI updates soon! Our regular newsletter will also be posted tomorrow.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3197, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "286dc06c-44d3-4ddb-81fe-c5e32f246a8e": {"__data__": {"id_": "286dc06c-44d3-4ddb-81fe-c5e32f246a8e", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-02-13-26fa79601ba5.html", "file_name": "llamaindex-newsletter-2023-02-13-26fa79601ba5.html", "file_type": "text/html", "file_size": 10106, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-02-13-26fa79601ba5.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-02-13-26fa79601ba5.html", "file_name": "llamaindex-newsletter-2023-02-13-26fa79601ba5.html", "file_type": "text/html", "file_size": 10106, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "147971629299913862b72199d4e22ad685074865e166b980be6aaaf861aa683e", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Greetings, LlamaIndex Adventurers \ud83e\udd99,\n </p>\n <p>\n  Welcome to an exhilarating week of discoveries at LlamaIndex! Our community\u2019s lively input and the abundance of learning tools await to supercharge your journey through LlamaIndex.\n </p>\n <p>\n  Before we dive into the updates, we have two major announcements:\n </p>\n <ul>\n  <li>\n   <strong>\n    LlamaIndex v0.10\n   </strong>\n   : Our latest open-source release marks a monumental step towards production readiness. With a new core package and hundreds of integrations and LlamaPacks now available as separate PyPi packages, we\u2019ve massively improved organization and version tracking. Major updates include the refactoring of LlamaHub into a central hub for all integrations and the deprecation of ServiceContext for an enhanced development experience.\n   <a href=\"/llamaindex-v0-10-838e735948f8\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog\n   </a>\n   ,\n   <a href=\"https://www.notion.so/6ede431dcb8841b09ea171e7f133bd77?pvs=21\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Migration Guide\n   </a>\n   .\n  </li>\n  <li>\n   Introducing Short Courses on Advanced RAG Development: Master complex RAG systems with our series, covering everything from unstructured data to agent integration. Learn through LlamaIndex Query Pipelines, from basic text-to-SQL to advanced query techniques, and build scalable RAG applications with hands-on guidance.\n   <a href=\"https://www.youtube.com/watch?v=CeDS1yvw9E4\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Video1\n   </a>\n   ,\n   <a href=\"https://www.youtube.com/watch?v=L1o1VPVfbb0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Video2\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  Your creativity fuels our inspiration! We\u2019re excited to see any projects, articles, or videos you\u2019re passionate about. Share your incredible creations with us at\n  <a href=\"mailto:news@llamaindex.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   news@llamaindex.ai\n  </a>\n  . Haven\u2019t joined our newsletter yet? Make sure to subscribe on our\n  <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   website\n  </a>\n  for the latest LlamaIndex updates delivered directly to your inbox.\n </p>\n <p>\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </p>\n <ol>\n  <li>\n   <strong>\n    Self-RAG\n   </strong>\n   : Introducing Self-RAG, now part of LlamaIndex as a LlamaPack. Boosts LLM training and RAG workflows with dynamic capabilities.\n   <a href=\"https://github.com/run-llama/llama-hub/blob/main/llama_hub/llama_packs/self_rag/self_rag.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1754909796594221187?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    LlamaIndex + FlowiseAI Integration\n   </strong>\n   : Seamlessly merge LlamaIndex with FlowiseAI for effortless, no-code RAG app development.\n   <a href=\"https://docs.flowiseai.com/integrations/llamaindex\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1755641567174684953?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    RAG Guide with MistralAI\n   </strong>\n   : MistralAI\u2019s new doc includes a RAG guide with LlamaIndex. Utilize Mistral-medium for enhanced RAG functions.\n   <a href=\"https://docs.mistral.ai/guides/basic-RAG/#rag-with-llamaindex\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   .\n  </li>\n </ol>\n <p>\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </p>\n <ul>\n  <li>\n   We have introduced a seamless integration between LlamaIndex and FlowiseAI, enabling easy, no-code development of advanced RAG applications with a drag-and-drop interface for quick chatbot or agent integration.\n   <a href=\"https://docs.flowiseai.com/integrations/llamaindex\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1755641567174684953?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have introduced Self-RAG, a dynamic retrieval tool by\n   <a href=\"https://twitter.com/AkariAsai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Akari Asai\n   </a>\n   \u2019s team, now available as a LlamaPack for easy integration, enhancing LLM training and RAG workflows with dynamic, iterative capabilities.\n   <a href=\"https://github.com/run-llama/llama-hub/blob/main/llama_hub/llama_packs/self_rag/self_rag.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1754909796594221187?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have introduced the RAG CLI tool that allows you to search any file on your filesystem using on-device language model embeddings, featuring the power of Mistral-7B and bge-m3 for an advanced, customizable experience.\n   <a href=\"https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1754678983881621595?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have launched full-stack agent servers with a single CLI command using\n   <code class=\"cw oz pa pb pc b\">\n    <strong>\n     create-llama\n    </strong>\n   </code>\n   from LlamaIndex, offering instant access to 50+ tools for any agent project.\n   <a href=\"https://x.com/jerryjliu0/status/1755289964517167184?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have introduced agents in LlamaIndex.TS, enabling advanced AI software development in TypeScript with features like function calling and multi-document handling.\n   <a href=\"/how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog\n   </a>\n   ,\n   <a href=\"https://ts.llamaindex.ai/modules/agent/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1755688725106114818?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   DeepEval is integrated with LlamaIndex, significantly enhancing RAG evaluation capabilities and introducing unit testing for LlamaIndex apps in CI/CD environments.\n   <a href=\"https://docs.confident-ai.com/docs/integrations-llamaindex\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://docs.mistral.ai/guides/basic-RAG/#rag-with-llamaindex\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   to RAG with LlamaIndex in MistralAI\u2019s new documentation with Mistral-medium and Mistral embedding models.\n  </li>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/agent/custom_agent.html#step-wise-queries\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   to Building Agentic RAG to incorporate user feedback in real-time enhancing complex searches with a human-in-the-loop approach.\n  </li>\n  <li>\n   <a href=\"https://huggingface.co/blog/tgi-messages-api\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   to Integrating Huggingface\u2019s New Messages API with OpenAI compatibility, simplifying the integration process for Inference Endpoints and Text Generation Inference.\n  </li>\n </ul>\n <p>\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://nayakpplaban.medium.com/\" rel=\"noopener\">\n    Plaban Nayak\n   </a>\n   <a href=\"https://www.notion.so/LlamaIndex-Newsletter-2024-02-06-86c1d1db060249f2ab8032357f3df323?pvs=21\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Setting up Query Pipeline For Advanced RAG Workflow using LlamaIndex.\n  </li>\n  <li>\n   <a href=\"https://www.youtube.com/@krishnaik06\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Krish Naik\n   </a>\n   <a href=\"https://www.youtube.com/watch?v=f-AXdiCyiT8\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Step-by-Step Guide to Building a RAG LLM App with Llama2 and LlamaIndex.\n  </li>\n  <li>\n   HelixML\n   <a href=\"https://helixml.substack.com/p/how-we-got-fine-tuning-mistral-7b\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   to Knowledge Memorization by fine-tuning Mistral-7B for enhanced knowledge memorization, offering a new way to reason across contexts without RAG\u2019s limitations.\n  </li>\n  <li>\n   <a href=\"https://medium.com/@wenqiglantz\" rel=\"noopener\">\n    Wenqi Glantz\n   </a>\n   <a href=\"https://towardsdatascience.com/nemo-guardrails-the-ultimate-open-source-llm-security-toolkit-0a34648713ef\" rel=\"noopener\" target=\"_blank\">\n    tutorial\n   </a>\n   on NeMo Guardrails, the Ultimate Open-Source LLM Security Toolkit.\n  </li>\n </ul>\n <p>\n  \ud83c\udfa5\n  <strong>\n   Webinar:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=96mRmQD4RnE\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Webinar\n   </a>\n   of Laurie with Ankit Khare(Rockset) delves into the essentials of RAG \u2014 its purpose, methodology, how LlamaIndex facilitates it, and exciting developments for 2024.\n  </li>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=Ya1DhVW9gTo\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Webinar\n   </a>\n   with Zilong Wang, and Tianyang Liu on Advanced Tabular Data Understanding with LLMs.\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83c\udfe2 Calling all enterprises:\n  </strong>\n </p>\n <p>\n  Are you building with LlamaIndex? We are working hard to make LlamaIndex, even more, Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested?\n  <a href=\"https://docs.google.com/forms/d/e/1FAIpQLScBNdM2a_fn8UZOKmFQt6lBsrd1o6FflvsdPH-Pn3JkdlN_Rg/viewform\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Get in touch.\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 10060, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b13ab53-9b38-4ac2-b634-9361051cd562": {"__data__": {"id_": "1b13ab53-9b38-4ac2-b634-9361051cd562", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-17-33514cbc04a2.html", "file_name": "llamaindex-newsletter-2023-10-17-33514cbc04a2.html", "file_type": "text/html", "file_size": 15358, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-17-33514cbc04a2.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-17-33514cbc04a2.html", "file_name": "llamaindex-newsletter-2023-10-17-33514cbc04a2.html", "file_type": "text/html", "file_size": 15358, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "d06853ebd28c7cb07a0bb6b2891dc38e3256d66a84e11231a14e56fee3b7ee98", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Hello Llama Enthusiasts \ud83e\udd99!\n </p>\n <p>\n  Another week has flown by, and we\u2019re back with a jam-packed newsletter filled with updates on hackathons, guides, integrations, features, webinars, tutorials, blogs, and demos. If you have a project, blog post, or video that deserves a spotlight, we\u2019d love to feature it! Just reach out to us at\n  <a href=\"mailto:news@llamaindex.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   news@llamaindex.ai\n  </a>\n  .\n </p>\n <p>\n  Bonus: You can now get all these updates straight to your inbox! Simply visit our\n  <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   homepage\n  </a>\n  and sign up for our email updates.\n </p>\n <p>\n  \ud83e\udd29\n  <strong>\n   First, the highlights:\n  </strong>\n </p>\n <ol>\n  <li>\n   <strong>\n    AI.Engineer Summit\n   </strong>\n   : At the AI.Engineer Summit, Jerry Liu discussed RAG applications, while Simon led a workshop on RAG app optimization (Jerry\u2019s\n   <a href=\"https://docs.google.com/presentation/d/1v7T6ejrSo87ndGeGC7tt6zeq-cftu03WWw7WL8Jskug/edit#slide=id.p\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    slides\n   </a>\n   , Simon\u2019s\n   <a href=\"https://github.com/run-llama/ai-engineer-workshop/blob/main/presentation.pdf\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    slides\n   </a>\n   )\n  </li>\n  <li>\n   <strong>\n    Text to pgVector\n   </strong>\n   : we launched PGVectorSQLQueryEngine for combined SQL and vector queries on PostgreSQL. (\n   <a href=\"https://t.co/4h3nTzzJ5E\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/jerryjliu0/status/1712496323742851188?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   )\n  </li>\n  <li>\n   <strong>\n    Hugging Face Integration\n   </strong>\n   : Integrated with HuggingFace\u2019s text-embeddings-inference server for high-speed, large-scale BERT model serving. (\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/embeddings/text_embedding_inference.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/jerryjliu0/status/1712943016590381554?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   )\n  </li>\n  <li>\n   <strong>\n    Multi-Document Agents\n   </strong>\n   : New V1 agents support advanced multi-document retrieval and async query planning. (\n   <a href=\"https://t.co/bWYv0R7J2B\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1712129914386993295?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   )\n  </li>\n  <li>\n   <strong>\n    Unstructured Parsing\n   </strong>\n   : Unveiled UnstructuredElementNodeParser, a hierarchical parser for embedded tables/text using UnstructuredIO. (\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/query_engine/sec_tables/tesla_10q_table.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1711768906866864403?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   )\n  </li>\n  <li>\n   <strong>\n    LLM Compatibility\n   </strong>\n   : We have charted LLM performances on various tasks and found that the Zephyr-7b-alpha model stands out as the top-performing 7B model in advanced RAG tasks. (\n   <a href=\"https://docs.llamaindex.ai/en/latest/core_modules/model_modules/llms/root.html#llm-compatibility-tracking\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   )\n  </li>\n </ol>\n <h1>\n  \ud83c\udfc6 Congratulations to our AGI House Hackathon Winners!\n </h1>\n <p>\n  We love seeing people build amazing things with LlamaIndex!\n </p>\n <p>\n  <strong>\n   Build:\n  </strong>\n </p>\n <ol>\n  <li>\n   <a href=\"https://drive.google.com/file/d/18Ru1FCchVpMi8jzjr2dHdDuZtCG83zOJ/view\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Demostify\n   </a>\n  </li>\n  <li>\n   <a href=\"https://docs.google.com/presentation/d/1pOa8AppiKpuF-aQvsD5vKebeL6Gf9lmP505FavrFOm4/edit#slide=id.g2899cea0752_0_15\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Stick with Fit\n   </a>\n   ,\n   <a href=\"https://github.com/chisler/safequery\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    SafeQuery\n   </a>\n   , Cherry\n  </li>\n </ol>\n <p>\n  <strong>\n   Break:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://github.com/jeremy-brouillet/agi-hackathon\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Fuzzy Access\n   </a>\n  </li>\n </ul>\n <p>\n  <strong>\n   Test:\n  </strong>\n </p>\n <ul>\n  <li>\n   X-Ray Insight\n  </li>\n </ul>\n <p>\n  <strong>\n   Honorable Mentions:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://glasp.co/know-thyself/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    KindleGPT\n   </a>\n  </li>\n  <li>\n   PenTest\n  </li>\n </ul>\n <h1>\n  \ud83c\udfa4 LlamaIndex at\n  <a href=\"https://twitter.com/aiDotEngineer\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   AI.Engineer Summit\n  </a>\n  :\n </h1>\n <ol>\n  <li>\n   <a href=\"https://twitter.com/jerryjliu0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Jerry Liu\n   </a>\n   gave a talk on Building production-ready RAG applications.\n   <a href=\"https://docs.google.com/presentation/d/1v7T6ejrSo87ndGeGC7tt6zeq-cftu03WWw7WL8Jskug/edit#slide=id.p\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Slides\n   </a>\n   .\n  </li>\n  <li>\n   <a href=\"https://twitter.com/disiok\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Simon\n   </a>\n   conducted a workshop on Building, Evaluating, and Optimizing your RAG App for Production with LlamaIndex.\n   <a href=\"https://github.com/run-llama/ai-engineer-workshop/blob/main/presentation.pdf\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Slides\n   </a>\n   ,\n   <a href=\"https://github.com/run-llama/ai-engineer-workshop/tree/main\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Code\n   </a>\n   .\n  </li>\n </ol>\n <h1>\n  \ud83d\uddfa\ufe0f Guides:\n </h1>\n <ol>\n  <li>\n   <strong>\n    LLM Compatibility Tracking:\n   </strong>\n   We\u2019ve charted LLM performances on various tasks, revealing zephyr-7b-alpha as the only current 7B model excelling in advanced RAG/ Agentic tasks.\n   <a href=\"https://docs.llamaindex.ai/en/latest/core_modules/model_modules/llms/root.html#llm-compatibility-tracking\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Evaluations:\n   </strong>\n   Adjusting chunk size is essential for RAG apps. Having more chunks isn\u2019t necessarily better, and re-ranking might be counterproductive. To fine-tune, experiment with different chunk sizes and top-k values. The Arize AI team has provided a guide to help you evaluate using Arize AI Phoenix and Llama Index.\n   <a href=\"https://docs.google.com/presentation/d/18Z7H3WSncPzLOTHKZAj36w0E7HSGY78VkDooSzvvySE/edit\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Slides\n   </a>\n   ,\n   <a href=\"https://colab.research.google.com/drive/1Siufl13rLI-kII1liaNfvf-NniBdwUpS?usp=sharing\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   .\n  </li>\n </ol>\n <h1>\n  \u270d\ufe0f Tutorials:\n </h1>\n <ol>\n  <li>\n   <a href=\"https://twitter.com/Shahules786\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Shahul\u2019s\n   </a>\n   <a href=\"https://t.co/oTA2O8sE21\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   demonstrates how to choose the best embeddings for your data, emphasizing that retriever performance and embedding quality are crucial for a RAG system\u2019s efficacy using the LlamaIndex and RAGAS libraries.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/wenqi_glantz\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Wenqi Glantz\n   </a>\n   \u2019s\n   <a href=\"https://levelup.gitconnected.com/evaluation-driven-development-the-swiss-army-knife-for-rag-pipelines-dba24218d47e\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Evaluation Driven Development for RAG Pipelines.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/wenqi_glantz\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Wenqi Glantz\n   </a>\n   \u2019s\n   <a href=\"https://betterprogramming.pub/masking-pii-data-in-rag-pipeline-326d2d330336\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Masking PII Data in the RAG Pipeline.\n  </li>\n  <li>\n   Ofer Mendelevitch\u2019s from\n   <a href=\"https://twitter.com/vectara\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Vectara\n   </a>\n   has a\n   <a href=\"https://vectara.com/retrieval-augmented-generation-rag-done-right-retrieval/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Retrieval Augmented Generation with LlamaIndex on comparing Vectara\u2019s new Boomerang model to OpenAI and Cohere.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/patloeber\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Patrick Loeber\n   </a>\n   from AssemblyAI has a\n   <a href=\"https://www.youtube.com/watch?v=alT-0mNRF-c\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Build LlamaIndex Audio Apps.\n  </li>\n  <li>\n   <a href=\"https://www.linkedin.com/in/pradipnichite/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Pradip Nichite\n   </a>\n   made a\n   <a href=\"https://www.youtube.com/watch?v=ZRSI8LHpqBA\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on NL2SQL with LlamaIndex: Querying Databases Using Natural Language.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/mayowaoshin\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Mayo Oshin\n   </a>\n   has a\n   <a href=\"https://www.youtube.com/watch?v=UmvqMscxwoc\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on How to Compare Multiple Large PDF Files.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/mesudarshan\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Sudarshan Koirala\n   </a>\n   made a\n   <a href=\"https://www.youtube.com/watch?v=4kwAhzzaW4A\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Chat With Documents with LlamaIndex and Pinecone.\n  </li>\n </ol>\n <h1>\n  \ud83d\udca1 Demos:\n </h1>\n <ul>\n  <li>\n   <a href=\"https://twitter.com/siva_1gc\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Siva Surendira\n   </a>\n   built\n   <a href=\"https://www.theycbot.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    YC Bot\n   </a>\n   to get instant startup advice from your favorite YC mentors.\n  </li>\n </ul>\n <h1>\n  \u2728 Feature Releases and Enhancements:\n </h1>\n <ol>\n  <li>\n   <strong>\n    Text to pgVector:\n   </strong>\n   We introduced the PGVectorSQLQueryEngine, which allows you to query a PostgreSQL database using both full SQL and vector search simultaneously.\n   <a href=\"https://t.co/4h3nTzzJ5E\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/jerryjliu0/status/1712496323742851188?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Multi-Document Agents:\n   </strong>\n   We introduce Multi-Document Agents (V1) that can now retrieve across multiple docs and plan queries asynchronously, offering a superior analysis compared to standard RAG.\n   <a href=\"https://t.co/bWYv0R7J2B\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1712129914386993295?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    UnstructuredIO:\n   </strong>\n   We\u2019ve partnered with UnstructuredIO to enhance LLM/RAG applications. By extracting tables from PDFs, we\u2019ve improved query methods beyond basic vector indexing, enabling hybrid queries and cross-document comparisons, especially for tabular questions.\n   <a href=\"https://t.co/Ezts2C9Rpw\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/jerryjliu0/status/1710685292913668595?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    UnstructuredElementNodeParser:\n   </strong>\n   Going beyond basic text-splitting, we introduce the UnstructuredElementNodeParser. It models embedded tables/text hierarchically in a data graph using UnstructuredIO.\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/query_engine/sec_tables/tesla_10q_table.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1711768906866864403?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Cross-Encoder Fine-Tuning:\n   </strong>\n   Cross-encoders enhance RAG by refining post-embedding search results. With LlamaIndex, you can now fine-tune cross-encoders on any document, boosting performance.\n   <a href=\"https://t.co/vAyv94dFk2\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/jerryjliu0/status/1712856457413370110?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <h1>\n  \u2699\ufe0f Integrations &amp; Collaborations:\n </h1>\n <ol>\n  <li>\n   <strong>\n    Assembly AI:\n   </strong>\n   We introduced a new data reader for audio data integration with AssemblyAI. This integration allows effortless audio loading and facilitates building vector store indices and query engines for inquiries.\n   <a href=\"https://llamahub.ai/l/assemblyai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1711156989106299249?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Nougat \u2014 MetaAI:\n   </strong>\n   We integrated Nougat, an exceptional OCR tool from Meta, that excels in interpreting scientific papers, notably mathematical notations, and LaTeX as a loader in LlamaHub, allowing streamlined processing of ArXiv papers within the RAG pipeline.\n   <a href=\"https://llamahub.ai/l/nougat_ocr\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1711896904928292976?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Hugging Face-Text Embeddings Inference:\n   </strong>\n   We integrated with the new text-embeddings-inference server from HuggingFace offering production-scale serving with distributed tracing for all BERT models at impressive speeds.\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/embeddings/text_embedding_inference.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/jerryjliu0/status/1712943016590381554?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <h1>\n  \ud83c\udfa5 Webinars And Podcast:\n </h1>\n <ol>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=EYMZVfKcRzM\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Webinar\n   </a>\n   with Timescale on Time-based retrieval for RAG.\n  </li>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=POBcYr0sbcg\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Webinar\n   </a>\n   with Omar Khattab and Thomas Joshi on DSPy \u2014 a framework for LLMs that emphasizes programming over prompting.\n  </li>\n  <li>\n   Jerry Liu\u2019s\n   <a href=\"https://www.latent.space/p/llamaindex#details\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    podcast\n   </a>\n   with Latent Space on LlamaIndex\u2019s origin story, fine-tuning, and more.\n  </li>\n </ol>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 15286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de3e81b8-63b3-4ea1-8dbd-0e9608cd55c7": {"__data__": {"id_": "de3e81b8-63b3-4ea1-8dbd-0e9608cd55c7", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-24-4a76204eeaa3.html", "file_name": "llamaindex-newsletter-2023-10-24-4a76204eeaa3.html", "file_type": "text/html", "file_size": 10205, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-24-4a76204eeaa3.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-24-4a76204eeaa3.html", "file_name": "llamaindex-newsletter-2023-10-24-4a76204eeaa3.html", "file_type": "text/html", "file_size": 10205, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "e69baaa3690bb762986f0c7c0513e39c95f482a14e63ef008e7e6fc4c6c3ad0d", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Hello Llama Fans \ud83e\udd99!\n </p>\n <p>\n  Welcome back to our newsletter covering new features, guides, integrations, webinars, tutorials, and more. Got a project, blog, or video you\u2019re proud of? Let\u2019s spotlight it! Contact us at\n  <a href=\"mailto:news@llamaindex.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   news@llamaindex.ai\n  </a>\n  .\n </p>\n <p>\n  Plus, for direct updates in your email, just head to\n  <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   our homepage\n  </a>\n  and subscribe to our newsletter.\n </p>\n <p>\n  \ud83e\udd29\n  <strong>\n   First, the highlights:\n  </strong>\n </p>\n <ol>\n  <li>\n   <code class=\"cw op oq or os b\">\n    <strong>\n     QueryFusionRetriever\n    </strong>\n   </code>\n   <strong>\n    Launch:\n   </strong>\n   Inspired by\n   <a href=\"https://twitter.com/Raudaschl\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Adrian Raudaschl\u2019s\n   </a>\n   RAG-Fusion, enhancing multiple query generation with LLMs.\n   <a href=\"https://twitter.com/jerryjliu0/status/1713573483228356733?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   ,\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/retrievers/simple_fusion.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Router Fine-Tuning:\n   </strong>\n   Our innovative router fine-tuning approach has achieved an outstanding 99% match rate, outpacing both the gpt-3.5\u2019s 65% and the base model\u2019s 12%.\n   <a href=\"https://twitter.com/jerryjliu0/status/1714668623510618346?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   ,\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/finetuning/router/router_finetune.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Fusion Retriever Guide:\n   </strong>\n   Guide on building an advanced Fusion Retriever from scratch.\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/low_level/fusion_retriever.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n  </li>\n  <li>\n   <strong>\n    Amazon Bedrock LLMs and AI21 Labs LLMs:\n   </strong>\n   We have expanded our LLM compatibility, now seamlessly integrating with both Amazon Bedrock and AI21 Labs models.\n  </li>\n </ol>\n <p>\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </p>\n <ul>\n  <li>\n   <code class=\"cw op oq or os b\">\n    <strong>\n     QueryFusionRetriever\n    </strong>\n   </code>\n   : We introduced the\n   <code class=\"cw op oq or os b\">\n    QueryFusionRetriever\n   </code>\n   , inspired by\n   <a href=\"https://twitter.com/Raudaschl\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Adrian Raudaschl\u2019s\n   </a>\n   work on RAG-Fusion. This retriever allows users to generate multiple queries with LLMs, run various retrieval methods, and apply reciprocal rank fusion for improved results.\n   <a href=\"https://twitter.com/jerryjliu0/status/1713573483228356733?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   ,\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/retrievers/simple_fusion.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Router Fine-Tuning:\n   </strong>\n   We introduced router fine-tuning (V0) for improved LLM automated decision-making. Our approach achieved a 99% match rate, outperforming gpt-3.5\u2019s 65% and the base model\u2019s 12%.\n   <a href=\"https://twitter.com/jerryjliu0/status/1714668623510618346?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   ,\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/finetuning/router/router_finetune.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    SQLRetriever:\n   </strong>\n   We introduce SQLRetriever, merging Text-to-SQL and RAG, enabling a RAG pipeline setup over SQL databases for structured table node retrieval and response synthesis.\n   <a href=\"https://twitter.com/llama_index/status/1715518806012092497?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   ,\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/index_structs/struct_indices/SQLIndexDemo.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/low_level/fusion_retriever.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tutorial\n   </a>\n   guide on\n   <strong>\n    Building an Advanced Fusion Retriever from Scratch.\n   </strong>\n  </li>\n </ul>\n <p>\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://www.linkedin.com/in/sauravjoshi23/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Saurav Joshi\n   </a>\n   \u2019s\n   <a href=\"https://medium.com/@sauravjoshi23/complex-query-resolution-through-llamaindex-utilizing-recursive-retrieval-document-agents-and-sub-d4861ecd54e6\" rel=\"noopener\">\n    tutorial\n   </a>\n   on Complex Query Resolution through LlamaIndex Utilizing Recursive Retrieval, Document Agents, and Sub Question Query Decomposition.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/GregOnLock\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Greg Loughnane\n   </a>\n   and\n   <a href=\"https://twitter.com/llm_wizard\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Chris Alexiuk\n   </a>\n   <a href=\"https://www.youtube.com/watch?v=0QaUqoICNBo\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on tackling domain-specific fine tuning using LlamaIndex.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/VishwasAiTech\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Vishwas Gowda\n   </a>\n   \u2019s\n   <a href=\"/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    blog post\n   </a>\n   on Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/manelferreira_\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Emanuel Ferreira\n   </a>\n   \u2019s blog post on the RA-DIT paper and its implementation in LlamaIndex.\n  </li>\n  <li>\n   Yujian Tang\u2019s\n   <a href=\"https://zilliz.com/blog/chat-with-towards-data-science-using-llamaindex?utm_source=twitter&amp;utm_medium=social&amp;utm_term=zilliz\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    blog post\n   </a>\n   on Chat with Towards Data Science using LlamaIndex.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/mesudarshan\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Sudarshan Koirala\n   </a>\n   <a href=\"https://www.youtube.com/watch?v=4kwAhzzaW4A\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Chat with documents with Pinecone and LlamaIndex.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/mesudarshan\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Sudarshan Koirala\n   </a>\n   <a href=\"https://www.youtube.com/watch?v=BngaodT1q_4\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Combined Text-TO-SQL + Semantic Search with LlamaIndex.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/engineerrprompt\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    PromptEngineer\n   </a>\n   <a href=\"https://www.youtube.com/watch?v=JeruKKuMxCg\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on building LLM-powered financial analyst with LlamaIndex.\n  </li>\n </ul>\n <p>\n  <strong>\n   \u2699\ufe0f Integrations &amp; Collaborations:\n  </strong>\n </p>\n <ul>\n  <li>\n   <strong>\n    Gradient AI:\n   </strong>\n   We introduce a collaboration with Gradient AI to easily integrate fine-tuned LLMs into your LlamaIndex RAG pipeline.\n   <a href=\"https://twitter.com/llama_index/status/1713970425422856477?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   ,\n   <a href=\"https://gradient.ai/blog/introducing-the-llamindex-integration\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blogpost\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    PrivateGPT:\n   </strong>\n   <a href=\"https://twitter.com/PrivateGPT_AI\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    PrivateGPT\n   </a>\n   partners with LlamaIndex allowing private document interactions using default or custom integrations.\n   <a href=\"https://twitter.com/PrivateGPT_AI/status/1715331924644815274\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    VectorFlow &amp; LlamaHub Collaboration:\n   </strong>\n   VectorFlow\u2019s open-source vector-embedding pipeline now leverages LlamaHub for data connectors to streamline code and reduce maintenance.\n   <a href=\"https://twitter.com/llama_index/status/1714446321078137015\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Amazon Bedrock &amp; AI21 Labs LLMs:\n   </strong>\n   We\u2019ve broadened our LLM compatibility range by integrating with Amazon Bedrock LLMs and AI21 Labs LLMs.\n  </li>\n  <li>\n   <strong>\n    DashVector\n   </strong>\n   : We have introduced an integration with DashVector, a robust, fully-managed vectorDB service.\n  </li>\n  <li>\n   <strong>\n    Tencent Cloud:\n   </strong>\n   We\u2019ve integrated with Tencent Cloud VectorDB.\n  </li>\n  <li>\n   PGVectorStore within LlamaIndex has been enhanced to support custom Postgres schemas. This facilitates better index management and promotes easy schema-based versioning.\n  </li>\n  <li>\n   We now accommodate custom models that align with the OpenAI-compatible API.\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83c\udfa5 Webinars:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://www.linkedin.com/in/wenqi-glantz-b5448a5a/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Wenqi Glantz\n   </a>\n   workshop webinar on Evaluation-Driven Development (EDD).\n  </li>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=mzb6WNSaLXQ\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Webinar\n   </a>\n   showcasing the winning projects from the recent AGI House hackathon: \u201cBuild, Test, and Launch LLM Apps\u201d. This event was co-sponsored by LlamaIndex, TruEra, and Pinecone.\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 10144, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ffec9c1b-f29d-4596-89ee-323ae052143e": {"__data__": {"id_": "ffec9c1b-f29d-4596-89ee-323ae052143e", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-31-36244e2b3f0c.html", "file_name": "llamaindex-newsletter-2023-10-31-36244e2b3f0c.html", "file_type": "text/html", "file_size": 11836, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-31-36244e2b3f0c.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-31-36244e2b3f0c.html", "file_name": "llamaindex-newsletter-2023-10-31-36244e2b3f0c.html", "file_type": "text/html", "file_size": 11836, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "d2c7e891f6146ee71575a768a943e3656e52520ad2a0fc174b5e2c0338fff544", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Greetings Llama Enthusiasts \ud83e\udd99!\n </p>\n <p>\n  Another week has zoomed past, and here we are with our latest roundup of updates, features, tutorials, and so much more. Have a noteworthy project, article, or video to share? We\u2019d love to feature it! Reach out to us at\n  <a href=\"mailto:news@llamaindex.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   news@llamaindex.ai\n  </a>\n  .\n </p>\n <p>\n  Want these updates straight to your inbox? Simply subscribe to our newsletter on our\n  <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   homepage\n  </a>\n  .\n </p>\n <p>\n  \ud83e\udd29\n  <strong>\n   First, the highlights:\n  </strong>\n </p>\n <ol>\n  <li>\n   <strong>\n    Revamped Documentation:\n   </strong>\n   Overhauled\n   <a href=\"https://docs.llamaindex.ai/en/stable/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    docs\n   </a>\n   for smoother LLM/RAG app development.\n  </li>\n  <li>\n   <strong>\n    Contribution Board:\n   </strong>\n   Our new\n   <a href=\"https://github.com/orgs/run-llama/projects/2\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    board\n   </a>\n   welcomes community-driven LlamaIndex enhancements.\n  </li>\n  <li>\n   <strong>\n    Zephyr-7b-beta Insights:\n   </strong>\n   <a href=\"https://colab.research.google.com/drive/1UoPcoiA5EOBghxWKWduQhChliMHxla7U?usp=sharing\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tested and verified\n   </a>\n   for unmatched ReAct agent task efficiency on LlamaIndex.\n  </li>\n  <li>\n   <strong>\n    Image Captioning Boost For RAG:\n   </strong>\n   LLaVa\u2019s outputs are now supercharged with knowledge-based augmentation.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/llava_multi_modal_tesla_10q.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/jerryjliu0/status/1717205234269983030?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n </ol>\n <p>\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </p>\n <ul>\n  <li>\n   We introduced Retrieval-Augmented Image Captioning, enhancing LLaVa multi-modal model outputs with knowledge base insights.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/llava_multi_modal_tesla_10q.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/jerryjliu0/status/1717205234269983030?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We introduced the ability to view and set prompts for LlamaIndex modules in just two lines of code.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/prompts/prompt_mixin.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1716847554401628516?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We introduced the integration of our\n   <code class=\"cw ov ow ox oy b\">\n    OpenAILike\n   </code>\n   class, allowing users to tap into various open-source LLM projects with OpenAI-compatible APIs, irrespective of the model provider.\n   <a href=\"https://x.com/llama_index/status/1716950167474356715?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We introduced Prompt Compression for RAG: with LongLLMLingua, which helps to cut token usage and latency by up to 20x.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/node_postprocessor/LongLLMLingua.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1717601235828973681?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We introduced a method to refine open-source LLMs like llama2 for structured data outputs. Using LlamaIndex, transform llama2\u20137b to produce Pydantic objects without PyTorch. Our guide covers synthetic dataset creation, fine-tuning, and RAG pipeline integration.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/finetuning/gradient/gradient_structured.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1718299602884137182?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83c\udfa5\n  </strong>\n  Demos:\n </p>\n <ul>\n  <li>\n   Harshad Suryawanshi did a\n   <a href=\"https://ai-eqty-rsrch-anlyst.streamlit.app/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    demo\n   </a>\n   on equity research report generator using LlamaIndex and Streamlit.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/ParamBharat\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Bharat Ramanathan\n   </a>\n   built\n   <a href=\"https://x.com/llama_index/status/1717940770270011898?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Wandbot\n   </a>\n   , a live RAG app enabling chat over Weights &amp; Biases documentation, integrated with Discord and SlackHQ. Key features include periodic data ingestion, custom document and code parsing, model fallback, and logging with Weights and biases.\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </p>\n <ul>\n  <li>\n   We introduced a revamped documentation structure tailored to guide users from prototyping to production of LLM/RAG apps using LlamaIndex. Dive into our 200+ guides to enhance your app.\n   <a href=\"https://docs.llamaindex.ai/en/stable/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1717627450690269284?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We unveil our new Request For Contribution Github board\n   <a href=\"https://github.com/orgs/run-llama/projects/2\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    here\n   </a>\n   . It\u2019s your guide to contribute to LlamaIndex, streamlining community suggestions.\n  </li>\n  <li>\n   We released the\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/embeddings/jina_embeddings.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    guide\n   </a>\n   on using the Jina 8k open-source text embedding model with LlamaIndex.\n  </li>\n  <li>\n   We introduce our comprehensive survey of llama2-chat models across varying capacities in LlamaIndex. The major insight: While reasoning is enhanced with more parameters, structured outputs remain a challenge.\n   <a href=\"https://twitter.com/llama_index/status/1717337923853664573?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We share a\n   <a href=\"https://colab.research.google.com/drive/1UoPcoiA5EOBghxWKWduQhChliMHxla7U?usp=sharing\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    guide\n   </a>\n   to test the newly released HuggingFace Zephyr-7b-beta model on LlamaIndex RAG/agent tasks, it stood out as the only 7B LLM capable of handling ReAct agent tasks over data.\n  </li>\n  <li>\n   We share a new\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/prompts/prompts_rag.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    guide\n   </a>\n   on Advanced Prompt Engineering for RAG. Learn about understanding, customizing, and extending RAG prompts, from QA templates to few-shot examples and context/query transformations.\n   <a href=\"https://x.com/llama_index/status/1718657660697149509?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://www.linkedin.com/in/kirannpanicker/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Kiran\n   </a>\n   made a\n   <a href=\"/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    blog post\n   </a>\n   on Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser.\n  </li>\n  <li>\n   <a href=\"https://medium.com/@wenqiglantz\" rel=\"noopener\">\n    Wenqi Glantz\n   </a>\n   gave us an excellent\n   <a href=\"https://levelup.gitconnected.com/optimizing-text-embeddings-with-huggingfaces-text-embeddings-inference-server-and-llamaindex-ef7df35882a4\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    blog post\n   </a>\n   on Optimizing Text Embeddings with HuggingFace\u2019s text-embeddings-inference Server and LlamaIndex.\n  </li>\n  <li>\n   <a href=\"https://www.linkedin.com/in/ravidesetty/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Ravi Theja\u2019s\n   </a>\n   <a href=\"/nvidia-research-rag-with-long-context-llms-7d94d40090c4\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    blog post\n   </a>\n   delves into NVIDIA Research on RAG vs Long Context LLMs, questioning the necessity of RAG in the presence of long-context LLMs.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/mesudarshan\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Sudarshan Koirala\n   </a>\n   has a tutorial on Extracting Tables + Texts from .htm pages for RAG Using LlamaIndex.\n  </li>\n  <li>\n   <a href=\"https://medium.com/@wenqiglantz\" rel=\"noopener\">\n    Wenqi Glantz\n   </a>\n   also made a second\n   <a href=\"https://levelup.gitconnected.com/multimodal-retrieval-with-text-embedding-and-clip-image-embedding-for-backyard-birds-599f19057a70\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    blog post\n   </a>\n   on Multimodal Retrieval with Text Embedding and CLIP Image Embedding for Backyard Birds.\n  </li>\n </ul>\n <p>\n  <strong>\n   \u2699\ufe0f Integrations &amp; Collaborations:\n  </strong>\n </p>\n <ul>\n  <li>\n   We introduced our new cookbooks in partnership with Gradient AI, enabling effortless fine-tuning of open-source LLMs like Llama 2 and integration into your LlamaIndex RAG pipeline.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/finetuning/gradient/gradient_text2sql.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/jerryjliu0/status/1716476285046952049?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We introduced integration with HuggingFace Inference API which gives access to over 150,000 models. Now you can plugin any\n   <code class=\"cw ov ow ox oy b\">\n    conversational\n   </code>\n   ,\n   <code class=\"cw ov ow ox oy b\">\n    text_generation\n   </code>\n   ,\n   <code class=\"cw ov ow ox oy b\">\n    feature_extraction\n   </code>\n   endpoints into your LlamaIndex app.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/huggingface.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1716847554401628516?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83c\udfa5 Webinars:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://twitter.com/mayowaoshin\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Mayo Oshin\n   </a>\n   and\n   <a href=\"https://twitter.com/jerryjliu0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Jerry Liu\n   </a>\n   gave a\n   <a href=\"https://www.crowdcast.io/c/n0roka37yfw0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    webinar\n   </a>\n   on Unlocking ChatGPT for Business.\n  </li>\n </ul>\n <p>\n  \ud83d\udcdaWorkshops:\n </p>\n <ul>\n  <li>\n   Jerry Liu and Simon conducted a Multipart LlamaIndex workshop in collaboration with Anyscale.\n  </li>\n  <li>\n   Ravi Theja conducted a day-long\n   <a href=\"https://hasgeek.com/fifthelephant/llamaindex-workshop-10-28/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    workshop\n   </a>\n   on Retrieval Augmented Generation with LlamaIndex.\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 11793, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb45192e-23b8-4f4a-adb4-ebc82f1f3440": {"__data__": {"id_": "bb45192e-23b8-4f4a-adb4-ebc82f1f3440", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-07-cf20b9a833aa.html", "file_name": "llamaindex-newsletter-2023-11-07-cf20b9a833aa.html", "file_type": "text/html", "file_size": 11074, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-07-cf20b9a833aa.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-07-cf20b9a833aa.html", "file_name": "llamaindex-newsletter-2023-11-07-cf20b9a833aa.html", "file_type": "text/html", "file_size": 11074, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "b0343638464cad3d64462cdd2898dc10dafda7e796b30334467ddc590e3d1123", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Hi again Llama Fans! \ud83e\udd99\n </p>\n <p>\n  We hope you enjoyed our\n  <a href=\"/llamaindex-news-special-edition-openai-developer-day-e955f16db4e2\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   OpenAI Dev Day special edition\n  </a>\n  yesterday! Here\u2019s our wrap-up of everything else that happened last week. As always, if you\u2019ve got a project, article, or video that\u2019s turning heads? We\u2019re all ears! Drop us a line at\n  <a href=\"mailto:news@llamaindex.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   news@llamaindex.ai\n  </a>\n  .\n </p>\n <p>\n  And for all this goodness delivered directly to you, don\u2019t forget to subscribe to our newsletter via our\n  <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   website\n  </a>\n  .\n </p>\n <p>\n  \ud83e\udd29\n  <strong>\n   First, the highlights:\n  </strong>\n </p>\n <ol>\n  <li>\n   <strong>\n    LlamaIndex Chat:\n   </strong>\n   We unveiled a customizable LLM chatbot template with system prompts and avatars, all within an open-source MIT-licensed framework using LlamaIndex for TypeScript. Explore the\n   <a href=\"https://chat.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Demo\n   </a>\n   or check the\n   <a href=\"https://x.com/llama_index/status/1719021921462067654?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Evaluator Fine-Tuning:\n   </strong>\n   We launched a method to enhance LLM output assessment by distilling GPT-4 into GPT-3.5, optimizing both cost and speed. See our\n   <a href=\"https://x.com/llama_index/status/1719868813318271242?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    ParamTuner:\n   </strong>\n   We introduced a new hyperparameter tuning abstraction to refine RAG pipeline performance, featuring objective functions, grid search, and Ray Tune integration. Check out the\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/param_optimizer/param_optimizer.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   and\n   <a href=\"https://twitter.com/llama_index/status/1721209688703062234?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    CohereAI Embed v3 &amp; Voyage AI Integration:\n   </strong>\n   We strengthened the LlamaIndex RAG pipeline with two powerful embedding model additions: the latest Embed v3 from CohereAI and the high-performing embedding model from Voyage AI.\n   <a href=\"https://twitter.com/llama_index/status/1720216603584069875?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   and\n   <a href=\"https://x.com/llama_index/status/1720578050180686129?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tweet\n   </a>\n   .\n  </li>\n </ol>\n <p>\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </p>\n <ul>\n  <li>\n   We introduced LlamaIndex Chat, a new feature allowing users to create and share custom LLM chatbots tailored to their data, complete with personalized system prompts and avatars. Additionally, we\u2019re proud to share that it\u2019s a fully open-source template under the MIT license, crafted using LlamaIndexTS for a seamless start to LLM application development.\n   <a href=\"https://chat.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Demo\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1719021921462067654?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We introduced a method for fine-tuning an Evaluator to distill GPT-4 into GPT-3.5, enhancing LLM output assessment while reducing costs and improving speed.\n   <a href=\"https://x.com/llama_index/status/1719868813318271242?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We introduced\n   <code class=\"cw ow ox oy oz b\">\n    ParamTuner\n   </code>\n   , a hyperparameter tuning abstraction for LlamaIndex RAG, streamlining the process with objective functions and support for grid search, including integration with Ray Tune for enhanced optimization.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/param_optimizer/param_optimizer.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1721209688703062234?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83c\udfa5\n  </strong>\n  Demos:\n </p>\n <ul>\n  <li>\n   GPTDiscord is a versatile LLM-powered Discord bot with over 20 features, including multi-modal image understanding and advanced data analysis. It boasts an infinite conversational memory and the ability to interact with various file types and internet services.\n   <a href=\"https://twitter.com/llama_index/status/1720151524280881335?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </p>\n <ul>\n  <li>\n   We shared a\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/retrievers/deep_memory.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    guide\n   </a>\n   for integrating Activeloop\u2019s Deep Memory with LlamaIndex, a module that enhances your embeddings at ingestion and can improve RAG metrics by 15%, all while seamlessly fitting into LlamaIndex\u2019s automated dataset and vector store features.\n  </li>\n  <li>\n   We shared a\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/prompts/prompt_optimization.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    guide\n   </a>\n   inspired by\n   <a href=\"https://twitter.com/chengrun_yang\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     Chengrun Yang\n    </strong>\n   </a>\n   and GoogleDeepMind\u2019s\n   <code class=\"cw ow ox oy oz b\">\n    Optimization by Prompting\n   </code>\n   paper, demonstrating how to automate prompt tuning in LlamaIndex RAG pipelines using meta-prompting, boosting evaluation performance while acknowledging the experimental nature of this technique.\n  </li>\n  <li>\n   We shared a\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/prompts/emotion_prompt.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    guide\n   </a>\n   on how to implement Emotion Prompting in LlamaIndex, allowing you to enhance your RAG pipeline with various emotional stimuli and evaluate their impact on task performance.\n  </li>\n  <li>\n   We showcased MongoDB\n   <a href=\"https://github.com/run-llama/mongodb-demo\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    starter\n   </a>\n   kit, a comprehensive LlamaIndex RAG setup with Flask backend, Next frontend, and easy deployment to Render.\n  </li>\n </ul>\n <p>\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://medium.com/@wenqiglantz\" rel=\"noopener\">\n    Wenqi Glantz\n   </a>\n   made a\n   <a href=\"https://levelup.gitconnected.com/optimizing-text-embeddings-with-huggingfaces-text-embeddings-inference-server-and-llamaindex-ef7df35882a4\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    blog post\n   </a>\n   on deploying the HuggingFace\n   <code class=\"cw ow ox oy oz b\">\n    <strong>\n     text-embeddings-inference\n    </strong>\n   </code>\n   server on an AWS EC2 GPU instance, enhancing LlamaIndex RAG pipeline's performance and results.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/sophiamyang\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Sophia Yang\u2019s\n   </a>\n   <a href=\"https://www.youtube.com/watch?v=QqDZVg9S_Vk\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Zephyr-7b-beta showcases its leading capabilities in LLM technology, including how it\u2019s benchmarked with LlamaIndex for diverse AI tasks.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/mesudarshan\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Sudarshan Koirala\n   </a>\n   gave a\n   <a href=\"https://www.youtube.com/watch?v=vJz9WVgsu9g\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on how to build a multi-modal retrieval system with LlamaIndex, Qdrant, and bge/CLIP embeddings.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/sophiamyang\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Sophia Yang\u2019s\n   </a>\n   gave another\n   <a href=\"https://www.youtube.com/watch?v=ihSiRrOUwmg\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   , this time on Small-to-Big Retrieval with LlamaIndex in building advanced RAG systems.\n  </li>\n  <li>\n   <a href=\"https://www.linkedin.com/in/ravidesetty/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Ravi Theja\u2019s\n   </a>\n   <a href=\"https://www.youtube.com/watch?v=X8BHWGXXdW0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on the Router Query Engine that helps you to set up multiple indices/ query engines for your dataset, allowing the LLM to choose the most suitable one for each specific question.\n  </li>\n </ul>\n <p>\n  <strong>\n   \u2699\ufe0f Integrations &amp; Collaborations:\n  </strong>\n </p>\n <ul>\n  <li>\n   We integrated the\n   <a href=\"https://tavily.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     Tavily AI\n    </strong>\n   </a>\n   research API into the LlamaIndex RAG pipeline, offering a robust tool for web research to enhance LLM agent automation.\n   <a href=\"https://github.com/run-llama/llama-hub/blob/main/llama_hub/tools/notebooks/tavily.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1719745197729599681?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We integrated\n   <a href=\"https://twitter.com/noamgat\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     Noam Gat\n    </strong>\n   </a>\n   \u2019s LLM Enforcer into the LlamaIndex RAG pipeline to ensure structured outputs for various models.\n   <a href=\"https://docs.llamaindex.ai/en/latest/community/integrations/lmformatenforcer.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1720103157412647265?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We integrated the latest Embed v3 model from CohereAI, enhancing document retrieval quality within the LlamaIndex RAG pipeline.\n   <a href=\"https://t.co/NOQxN9RJi3\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1720216603584069875?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We integrated the new Voyage AI embedding model, a top-performing option for RAG pipelines.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/embeddings/voyageai.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1720578050180686129?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 11019, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dae6e131-5d00-4a87-8cee-c2e54de14693": {"__data__": {"id_": "dae6e131-5d00-4a87-8cee-c2e54de14693", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-14-dad06ae4284a.html", "file_name": "llamaindex-newsletter-2023-11-14-dad06ae4284a.html", "file_type": "text/html", "file_size": 11683, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-14-dad06ae4284a.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-14-dad06ae4284a.html", "file_name": "llamaindex-newsletter-2023-11-14-dad06ae4284a.html", "file_type": "text/html", "file_size": 11683, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "7f0f6b000bd1591ef2cadf0a8fb34a2cdfc70ee55e5c2e7e1515377e35f370df", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Hello Llama Friends \ud83e\udd99\n </p>\n <p>\n  LlamaIndex is 1 year old this week! \ud83c\udf89 To celebrate, we\u2019re taking a stroll down memory lane on our\n  <a href=\"/llamaindex-turns-1-f69dcdd45fe3\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   blog\n  </a>\n  with twelve milestones from our first year. Be sure to check it out.\n </p>\n <p>\n  Last week we had a blast with all the new things from OpenAI Dev day to learn and explore at LlamaIndex. There was a\n  <a href=\"/llamaindex-news-special-edition-openai-developer-day-e955f16db4e2\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   special edition newsletter\n  </a>\n  with the things we released the same day as the conference, but this week\u2019s newsletter is full of follow-up releases and explorations \u2014 don\u2019t miss our slide deck summing up all the new features!\n </p>\n <p>\n  As always, if you\u2019ve got a cool project or a video to share we\u2019d love to see it! Just drop us a line at\n  <a href=\"mailto:news@llamaindex.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   news@llamaindex.ai\n  </a>\n  .\n </p>\n <p>\n  \ud83e\udd29\n  <strong>\n   First, the highlights:\n  </strong>\n </p>\n <ol>\n  <li>\n   <strong>\n    Multi-Modal RAG Stack:\n   </strong>\n   we unveiled Multi-Modal RAG ****for complex Q&amp;A on documents and images, with new text/image queries and retrieval solutions.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/gpt4v_multi_modal_retrieval.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/jerryjliu0/status/1723076174698672417?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   ,\n   <a href=\"/multi-modal-rag-621de7525fea\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog post\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    OpenAIAssistantAgent Abstractions:\n   </strong>\n   we released new abstractions to connect OpenAI Assistant API with any vector database.\n   <a href=\"https://t.co/W78d2WCpnn\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1722276583883657388?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Parallel Function Calling:\n   </strong>\n   we enhanced our data extraction and tool execution using OpenAI\u2019s parallel function calling.\n   <a href=\"https://x.com/llama_index/status/1722686015276753073?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    MechGPT Project:\n   </strong>\n   Prof.\n   <a href=\"https://twitter.com/ProfBuehlerMIT\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     Markus J. Buehler\n    </strong>\n   </a>\n   \u2019s work merges LLM fine-tuning with knowledge graphs for scientific discovery.\n   <a href=\"https://x.com/llama_index/status/1723379654550245719?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   ,\n   <a href=\"https://t.co/l8J55BqUfn\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Paper\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Feature Slide Deck:\n   </strong>\n   Released a\n   <a href=\"https://docs.google.com/presentation/d/1i1bUDWXeCYPd6O8pio57ST6AQIuSTWXM3rvvkvrBpBM/edit#slide=id.p\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    slide deck\n   </a>\n   with 10+ new features and guides post-OpenAI updates.\n  </li>\n </ol>\n <p>\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </p>\n <ul>\n  <li>\n   We introduced a multi-modal RAG stack for complex document and image QA, featuring text/image queries, joint text/ image embeddings, and versatile storage and retrieval options.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/gpt4v_multi_modal_retrieval.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/jerryjliu0/status/1723076174698672417?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   ,\n   <a href=\"/multi-modal-rag-621de7525fea\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog post\n   </a>\n   .\n  </li>\n  <li>\n   We now offer experimental GPT-4-vision support in\n   <a href=\"http://chat.llamaindex.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    chat.llamaindex.ai\n   </a>\n   . Users can now upload images for enhanced chatbot interactions.\n   <a href=\"https://x.com/llama_index/status/1723120887988384177?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We integrated OpenAI\u2019s parallel function calling for efficient extraction of structured data from unstructured text and improving tool execution with agents.\n   <a href=\"https://x.com/llama_index/status/1722686015276753073?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We introduced\n   <code class=\"cw ov ow ox oy b\">\n    <strong>\n     OpenAIAssistantAgent\n    </strong>\n   </code>\n   abstractions for seamless connection of OpenAI Assistants API with your chosen vector database.\n   <a href=\"https://t.co/W78d2WCpnn\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1722276583883657388?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We introduced a new agent leveraging OpenAI Assistants API with features like in-house code interpretation, file retrieval, and function calling for external tools integration.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/agent/openai_assistant_agent.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1721949693754917035?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83c\udfa5\n  </strong>\n  Demos:\n </p>\n <ul>\n  <li>\n   MechGPT by Professor\n   <a href=\"https://twitter.com/ProfBuehlerMIT\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     Markus J. Buehler\n    </strong>\n   </a>\n   showcases the integration of LLM fine-tuning and knowledge graph creation with LlamaIndex, leading to interesting insights in cross-disciplinary scientific research and hypothesis generation.\n   <a href=\"https://x.com/llama_index/status/1723379654550245719?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   ,\n   <a href=\"https://t.co/l8J55BqUfn\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Paper\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </p>\n <ul>\n  <li>\n   We released a concise\n   <a href=\"https://docs.google.com/presentation/d/1i1bUDWXeCYPd6O8pio57ST6AQIuSTWXM3rvvkvrBpBM/edit#slide=id.p\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    slide deck\n   </a>\n   that aggregates over 10+ newly shipped features, guides, and analyses, complete with links to accompanying notebooks for developer use based on OpenAI\u2019s recent updates.\n  </li>\n  <li>\n   We also released a full\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/agent/openai_assistant_query_cookbook.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    cookbook\n   </a>\n   showing how you can build advanced RAG with the Assistants API \u2014 beyond just using the in-house Retrieval tool.\n  </li>\n  <li>\n   We produced a\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/agent/openai_retrieval_benchmark.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    guide\n   </a>\n   on evaluating the OpenAI Assistant API vs RAG with LlamaIndex.\n  </li>\n  <li>\n   Here\u2019s a\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/response_synthesizers/long_context_test.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    guide\n   </a>\n   on evaluating How well long-context LLMs (gpt-4-turbo, claude-2) recall specifics in BIG documents? (&gt;= 250k tokens).\n  </li>\n  <li>\n   Here\u2019s another\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/openai_json_vs_function_calling.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    guide\n   </a>\n   that highlights how function calling simplifies structured data extraction, while JSON mode ensures format correctness without schema enforcement.\n  </li>\n  <li>\n   Finally, we released a guide to craft a GPT Builder, enabling an agent to programmatically construct another task-specific agent. This builder streamlines the creation of systems for specific functions.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/agent/agent_builder.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/jerryjliu0/status/1721639447207583882?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://twitter.com/_bhaveshbhatt\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     Bhavesh Bhat\n    </strong>\n   </a>\n   gave us a\n   <a href=\"https://twitter.com/_bhaveshbhatt/status/1721551513103839392\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on How to Chat with YouTube Videos Using LlamaIndex.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/DGarnitz\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    David Garnitz\n   </a>\n   \u2019s tutorial blog explores the use of VectorFlow alongside ArizePhoenix, Weaviate, and LlamaIndex to manage large data sets.\n  </li>\n  <li>\n   <a href=\"https://harshadsuryawanshi.medium.com/\" rel=\"noopener\">\n    Harshad Suryawanshi\n   </a>\n   \u2019s\n   <a href=\"/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   covers Building My Own ChatGPT Vision with PaLM, KOSMOS-2 and LlamaIndex.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/mesudarshan\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Sudarshan Koirala\n   </a>\n   \u2019s made a\n   <a href=\"https://www.youtube.com/watch?v=LRP-0iSVQaA\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Creating OpenAI Assistant Agent with LlamaIndex.\n  </li>\n  <li>\n   Our own\n   <a href=\"https://twitter.com/ravithejads\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Ravi Theja\n   </a>\n   released his\n   <a href=\"/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Boosting RAG with Embeddings &amp; Rerankers.\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83c\udfa5\n  </strong>\n  Webinars:\n </p>\n <ul>\n  <li>\n   Check out our\n   <a href=\"https://www.youtube.com/watch?v=upPK6pRbZYQ\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    webinar\n   </a>\n   with Dan Shipper, CEO of\n   <a href=\"http://every.to/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    every\n   </a>\n   to talk about the implications of OpenAI\u2019s release updates.\n  </li>\n  <li>\n   A second\n   <a href=\"https://www.youtube.com/watch?v=rBpZvMAim5E\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    webinar\n   </a>\n   with Victoria Lin, author of the RA-DIT paper on Fine-tuning + RAG.\n  </li>\n  <li>\n   Last but not least,\n   <a href=\"https://twitter.com/mayowaoshin\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Mayo Oshin\n   </a>\n   \u2019s\n   <a href=\"https://www.youtube.com/watch?v=xT6JpDELKPg&amp;t=61s\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    webinar\n   </a>\n   with\n   <a href=\"https://twitter.com/jerryjliu0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Jerry Liu\n   </a>\n   on How to Analyze Tables In Large Financial Reports Using GPT-4.\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 11620, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c6b3573-c71f-4c67-a579-7be2e27c9846": {"__data__": {"id_": "1c6b3573-c71f-4c67-a579-7be2e27c9846", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-21-aa3a71e339f8.html", "file_name": "llamaindex-newsletter-2023-11-21-aa3a71e339f8.html", "file_type": "text/html", "file_size": 10593, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-21-aa3a71e339f8.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-21-aa3a71e339f8.html", "file_name": "llamaindex-newsletter-2023-11-21-aa3a71e339f8.html", "file_type": "text/html", "file_size": 10593, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "12ca24bd50039036505baad0e0de41060f428952e7a37197f8567db5ec923765", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Hello Llama Fam \ud83e\udd99\n </p>\n <p>\n  What an amazing week we\u2019ve had! We\u2019re excited to share that, according to the\n  <a href=\"https://retool.com/reports/state-of-ai-2023\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Retool State of AI 2023 survey\n  </a>\n  , 1 in 12 respondents are now using LlamaIndex. We\u2019re grateful for all your support.\n </p>\n <p>\n  If you have a fascinating project or video you\u2019d like to share, we\u2019d love to see it! Feel free to send it to us at\n  <a href=\"mailto:news@llamaindex.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   news@llamaindex.ai\n  </a>\n  . And remember to subscribe to our newsletter on our\n  <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   website\n  </a>\n  to stay in the loop. We can\u2019t wait to connect with you there!\n </p>\n <p>\n  \ud83e\udd29\n  <strong>\n   First, the highlights:\n  </strong>\n </p>\n <ol>\n  <li>\n   <strong>\n    LlamaIndex 0.9 Release:\n   </strong>\n   we introduced LlamaIndex version 0.9 featuring streamlined data handling with a new IngestionPipeline, automated caching, improved text processing interfaces, tokenizer updates, PyPi packaging enhancements, consistent import paths, and a beta version of MultiModal RAG Modules.\n   <a href=\"/announcing-llamaindex-0-9-719f03282945\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog post\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1724836383259582548?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Multi-Modal Evaluation Tools:\n   </strong>\n   we launched multi-modal evaluation with the introduction of MultiModalRelevancyEvaluator and MultiModalFaithfulnessEvaluator, plus a guide for their application in multi-modal settings.\n   <a href=\"https://t.co/gw4txOw0gY\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog post\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1725249971551879601?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <code class=\"cw ou ov ow ox b\">\n    <strong>\n     create-llama\n    </strong>\n   </code>\n   <strong>\n    CLI Tool:\n   </strong>\n   we unveiled\n   <code class=\"cw ou ov ow ox b\">\n    <strong>\n     create-llama\n    </strong>\n   </code>\n   , a versatile CLI tool for building full-stack LLM apps with options like FastAPI, ExpressJS, and Next.js for backends and a Next.js frontend with Vercel AI SDK components.\n   <a href=\"https://t.co/JpH5Trq4Yb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog post\n   </a>\n   ,\n   <a href=\"https://x.com/jerryjliu0/status/1724481554528014660?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Cohere Reranker Fine-Tuning:\n   </strong>\n   we enhanced RAG pipeline retrieval performance with the fine-tuning of the Cohere reranker.\n   <a href=\"/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog post\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1725189652003610888?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <p>\n  Coming up this week: we have a YouTube live event in partnership with\n  <a href=\"https://www.youtube.com/channel/UCbDZFHUjTCCUKyXgcp3g50Q\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   AI Makerspace\n  </a>\n  exploring the potential of LlamaIndex to handle complex PDFs with tables, charts and more.\n  <a href=\"https://lu.ma/RAG4PDF\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Register for free!\n  </a>\n </p>\n <p>\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </p>\n <ul>\n  <li>\n   We introduced the LlamaIndex 0.9 version with updates on streamlined data handling with new IngestionPipeline, automated caching, improved interfaces for text processing, tokenizer updates, enhanced PyPi packaging, consistent import paths, and a beta of MultiModal RAG Modules for text and image integration.\n   <a href=\"/announcing-llamaindex-0-9-719f03282945\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog post\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1724836383259582548?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We introduced multi-modal evaluation which includes MultiModalRelevancyEvaluator and MultiModalFaithfulnessEvaluator, and a guide on using them in multi-modal applications.\n   <a href=\"https://t.co/gw4txOw0gY\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog post\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1725249971551879601?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We introduced\n   <code class=\"cw ou ov ow ox b\">\n    <strong>\n     create-llama\n    </strong>\n   </code>\n   , a CLI tool for easily building full-stack LLM apps, offering choices like FastAPI, ExpressJS, and Next.js backends with Llama Index, and a Next.js frontend with Vercel AI SDK components, enabling extensive customization for AI engineers.\n   <a href=\"https://t.co/JpH5Trq4Yb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog post\n   </a>\n   ,\n   <a href=\"https://x.com/jerryjliu0/status/1724481554528014660?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We introduced fine-tuning of the cohere reranker to improve retrieval performance in the RAG pipeline.\n   <a href=\"/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog post\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1725189652003610888?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  Integrations:\n </p>\n <ul>\n  <li>\n   We integrated with Chroma\u2019s multi-modal collections which allows for indexing both text and images in a single collection, enhancing RAG pipelines by combining text and image information for use with multi-modal models like GPT-4V, LLaVa, and Fuyu.\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/multi_modal/ChromaMultiModalDemo.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1724228502168518857?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/multi_modal_retrieval.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   on Multi-Modal Retrieval using GPT text embedding and CLIP image embedding for Wikipedia Articles.\n  </li>\n  <li>\n   <a href=\"https://nanonets.com/blog/llamaindex/#using-index-to-chat-with-data\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   on LlamaIndex by Nanonets covering over 12 key areas such as data management, indexing/storage, querying with top-k RAG, structured outputs, chat functionalities with memory, and agent development incorporating tool use.\n  </li>\n  <li>\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/transforms/TransformsEval.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   on using Ingestion pipeline focusing on showcasing experiments on chunk overlaps and the use of metadata extractors, including title, summary, and other elements.\n  </li>\n  <li>\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/perplexity.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   on using Perplexity API with LlamaIndex by\n   <a href=\"https://twitter.com/vishhvak\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Vishhvak\n   </a>\n   .\n  </li>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/stable/community/integrations/fleet_libraries_context.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   on using\n   <a href=\"https://twitter.com/fleet_ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Fleet Context\n   </a>\n   to download the embeddings for LlamaIndex\u2019s documentation and build a hybrid dense/sparse vector retrieval engine on top of it.\n  </li>\n  <li>\n   <a href=\"https://github.com/jerryjliu/create_llama_projects\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   on building a full-stack financial analysis bot using\n   <code class=\"cw ou ov ow ox b\">\n    <strong>\n     create-llama\n    </strong>\n   </code>\n   and Llama Index's RAG, capable of querying text and tables across SEC filings.\n  </li>\n </ul>\n <p>\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://medium.com/@wenqiglantz\" rel=\"noopener\">\n    Wenqi Glantz\n   </a>\n   made a\n   <a href=\"https://levelup.gitconnected.com/llava-vs-gpt-4v-amidst-snow-geese-migration-c2561b16113d\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on LLaVA vs. GPT-4V Amidst Snow Geese Migration.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/glenn__parham\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Glenn Parham\u2019s\n   </a>\n   <a href=\"https://github.com/deptofdefense/LLMs-at-DoD/blob/main/tutorials/Chatting%20with%20your%20Docs.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    cookbook\n   </a>\n   on LlamaIndex, hosted in the Department of Defense\u2019s official repository, showcases methods for applying RAG on unclassified DoD policy documents.\n  </li>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=PHEZ6AHR57w\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Sudarshan Koirala\n   </a>\n   made a tutorial on Using Perplexity API with LlamaIndex.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/ravithejads\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Ravi Theja\n   </a>\n   <a href=\"/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    analysis\n   </a>\n   on GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques\n  </li>\n </ul>\n <p>\n  \ud83c\udfa5\n  <strong>\n   Webinars:\n  </strong>\n </p>\n <ul>\n  <li>\n   Check out our CEO \u2014\n   <a href=\"https://twitter.com/jerryjliu0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Jerry Liu\u2019s\n   </a>\n   talk on Building Production-Ready RAG Applications at\n   <a href=\"https://t.co/46VrFt8GCV\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    AI.engineer\n   </a>\n   Summit.\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 10548, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b11322e-cfc5-490a-9095-8a596889d6dc": {"__data__": {"id_": "8b11322e-cfc5-490a-9095-8a596889d6dc", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-28-a31be430a786.html", "file_name": "llamaindex-newsletter-2023-11-28-a31be430a786.html", "file_type": "text/html", "file_size": 9884, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-28-a31be430a786.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-28-a31be430a786.html", "file_name": "llamaindex-newsletter-2023-11-28-a31be430a786.html", "file_type": "text/html", "file_size": 9884, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "211a0f961eb7e859d723b12ec6ca8e6df30e11240bc897db03532223ad79104f", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Hello to Our Llama Community! \ud83e\udd99\n </p>\n <p>\n  Hope your Thanksgiving was delightful! We\u2019re thrilled to announce a major milestone: LlamaIndex has hit\n  <strong>\n   1 million monthly downloads\n  </strong>\n  on our Python package! A big thank you to everyone for your support, feedback, and contributions that have fueled our journey. Stay tuned for more exciting new products and features coming your way.\n </p>\n <p>\n  If you have a fascinating project or video you\u2019d like to share, we\u2019d love to see it! Feel free to send it to us at\n  <a href=\"mailto:news@llamaindex.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   news@llamaindex.ai\n  </a>\n  . And remember to subscribe to our newsletter on our\n  <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   website\n  </a>\n  to stay in the loop. We can\u2019t wait to connect with you there!\n </p>\n <p>\n  \ud83e\udd29\n  <strong>\n   First, the highlights:\n  </strong>\n </p>\n <ul>\n  <li>\n   <strong>\n    Launched Llama Packs:\n   </strong>\n   Prepackaged modules and templates streamlining LLM app development.\n   <a href=\"/introducing-llama-packs-e14f453b913a\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1727365908119917016?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    RAGs Project:\n   </strong>\n   Build your own retrieval augmented generation app just by talking.\n   <a href=\"https://github.com/run-llama/rags\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Project\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1727502719706132516?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Introduced FuzzyCitationEnginePack:\n   </strong>\n   Precisely aligns LLM responses to source sentences via fuzzy matching, available as an easy-to-implement LlamaPack.\n   <a href=\"https://t.co/xiGJCjNCfc\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1729182899470311541?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  Coming up this week: on Thursday 30th our very own Yi Ding will be giving a workshop on Building an Open Source RAG Application Using LlamaIndex.\n  <a href=\"https://www.datastax.com/workshops/building-an-open-source-rag-application-using-llamaindex?utm_medium=social_organic&amp;utm_source=linkedin&amp;utm_campaign=workshop&amp;utm_content=llamaindex-channels\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Sign up for free here\n  </a>\n </p>\n <p>\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </p>\n <ul>\n  <li>\n   We introduced Llama Packs \ud83e\udd99\ud83d\udce6, a series of prepackaged modules and templates designed to jumpstart your LLM app development. These packs eliminate the need for assembling and tuning custom components for each use case.\n   <a href=\"/introducing-llama-packs-e14f453b913a\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1727365908119917016?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have introduced the RAGs project for programming AI agents using natural language, inspired by the interest in OpenAI\u2019s GPTs. Our approach involves a \u2018Builder Agent\u2019 that crafts a \u2018Custom Agent\u2019 tailored to specific tasks, incorporating tools for system prompt setting, data loading, model configuration, and RAG parameter adjustments.\n   <a href=\"https://github.com/run-llama/rags\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Project\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1727502719706132516?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We introduced a LlamaPack that enables the setup of a fully local RAG pipeline with just one line of code. This pack includes Zephyr-7b as the LLM and bge-base as the embedding model.\n   <a href=\"https://t.co/IGIyPl5iE2\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1728931304211951944?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We introduced\n   <code class=\"cw ou ov ow ox b\">\n    <strong>\n     FuzzyCitationEnginePack\n    </strong>\n   </code>\n   that maps parts of an LLM-generated response from a RAG pipeline to the exact sentences in the source context using fuzzy matching. This innovation elevates citation accuracy and is now available as a LlamaPack for easy implementation with just one line of code.\n   <a href=\"https://t.co/xiGJCjNCfc\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1729182899470311541?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83d\udc40 Demo:\n  </strong>\n </p>\n <ul>\n  <li>\n   AI-Einblick Prompt is a JupyterLab extension that uses OpenAI\u2019s GPT 3.5 and 4, powered by LlamaIndex, to assist in data science workflows by generating, modifying, and fixing code, creating charts, and building models, seamlessly integrated within the JupyterLab environment.\n   <a href=\"https://pypi.org/project/ai-einblick-prompt/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Project\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1727492316242583571?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <a href=\"https://twitter.com/khemiri_ranya\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     Ranya Khemiri\n    </strong>\n   </a>\n   uploaded a research paper to RAGs to help with a school assignment and observed results better than file retrieval with ChatGPT.\n   <a href=\"https://raniaprojects.wixsite.com/raniakhemiri/post/i-set-up-a-rag-pipeline-to-help-with-a-school-assignment\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1728581037863961019?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83e\udd1d Integrations:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://twitter.com/CogniSwitch?lang=en\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    CogniSwitch\n   </a>\n   introduced a fusion RAG approach combining vectors, knowledge graphs, and rules for streamlined ingestion and retrieval. This allows for flexible usage, either as an independent query engine or as an integrated tool within an agent with LlamaIndex.\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/query_engine/cogniswitch_query_engine.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1727127794289959396?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   on shipping your RAG application to production with create-llama.\n  </li>\n  <li>\n   <a href=\"https://t.co/ija26e25PR\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   on multi-modal models: Our comparison tables detail differences in image reasoning, embeddings, and synthesis capabilities. We also provide insights into multi-modal support for vector stores, focusing on image support with future audio/video integration.\n  </li>\n  <li>\n   <a href=\"https://gradient.ai/blog/rag-101-for-enterprise\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   on getting started with AI in your enterprise from Gradient AI. This introductory guide explains retrieval-augmented generation (RAG), its relevance for businesses, and how to balance fine-tuning, prompt engineering, and RAG for optimal results, along with strategies for RAG optimization.\n  </li>\n </ul>\n <p>\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://medium.com/@andysingal\" rel=\"noopener\">\n    Ankush k Singal\n   </a>\n   made a\n   <a href=\"https://t.co/qabwKVuPSJ\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Document Extraction with Zephyr 7b LLM using LlamaIndex.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/wenqi_glantz\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Wenqi Glantz\n   </a>\n   made a\n   <a href=\"https://t.co/93XM9BnaCF\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Automating Hyperparameter Tuning with LlamaIndex.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/tonicfakedata\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tonic AI\n   </a>\n   <a href=\"https://www.tonic.ai/blog/rag-evaluation-series-validating-rag-performance-openai-vs-llamaindex\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    analysis\n   </a>\n   on OpenAI Assistant API vs LlamaIndex RAG.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/pradip_nichite\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     Pradip Nichite\n    </strong>\n   </a>\n   made ****a ****video\n   <a href=\"https://t.co/v4TsPF9xmt\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on using RAGs which provides easy-to-follow instructions on how to build or customize a chatbot capable of advanced summarization over your data, making it accessible even for non-developers.\n  </li>\n </ul>\n <p>\n  \ud83c\udfa5\n  <strong>\n   Webinars:\n  </strong>\n </p>\n <ul>\n  <li>\n   Jerry Liu presented a\n   <a href=\"https://arize.com/resource/advanced-llm-evals/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    webinar\n   </a>\n   with Arize AI on LLM Retrieval Evaluations.\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 9831, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b917ee22-46b2-4ba6-b9f4-499ee3e46976": {"__data__": {"id_": "b917ee22-46b2-4ba6-b9f4-499ee3e46976", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-05-faf5ab930264.html", "file_name": "llamaindex-newsletter-2023-12-05-faf5ab930264.html", "file_type": "text/html", "file_size": 13772, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-05-faf5ab930264.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-05-faf5ab930264.html", "file_name": "llamaindex-newsletter-2023-12-05-faf5ab930264.html", "file_type": "text/html", "file_size": 13772, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "d45508549dfde710ff822c44460bf4116422a5a294eb858d8b18ecef352bb73b", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Hello Llama Community \ud83e\udd99,\n </p>\n <p>\n  We are excited to collaborate with DeepLearningAI and TruEraAI to launch an extensive course on advanced Retrieval-Augmented Generation (RAG) and its evaluations. The course includes Sentence Window Retrieval, Auto-merging Retrieval, and Evaluations with TruLensML, providing practical tools for enhanced learning and application. To make the most of this learning opportunity, we invite you to\n  <a href=\"https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/?utm_campaign=truerallamaindex-launch&amp;utm_medium=video&amp;utm_source=youtube&amp;utm_content=teaser\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   take the course\n  </a>\n  .\n </p>\n <p>\n  We appreciate your support and are always excited to see your projects and videos. Feel free to share them at\n  <a href=\"mailto:news@llamaindex.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   news@llamaindex.ai\n  </a>\n  . Also, remember to subscribe to our newsletter on our\n  <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   website\n  </a>\n  for the latest updates and to connect with our vibrant community.\n </p>\n <p>\n  \ud83e\udd29\n  <strong>\n   First, the highlights:\n  </strong>\n </p>\n <ol>\n  <li>\n   <strong>\n    Launch of Seven Advanced Retrieval LlamaPacks\n   </strong>\n   : Simplifies building advanced RAG systems to nearly a single line of code, offering techniques like Hybrid Fusion and Auto-merging Retriever.\n   <a href=\"https://x.com/llama_index/status/1729303619760259463?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Introduction of the OpenAI Cookbook\n   </strong>\n   : A comprehensive guide for evaluating RAG systems with LlamaIndex, covering system understanding, building, and performance evaluation.\n   <a href=\"/openai-cookbook-evaluating-rag-systems-fe393c61fb93\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog\n   </a>\n   ,\n   <a href=\"https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n  </li>\n  <li>\n   <strong>\n    Speed Enhancement in Structured Metadata Extraction\n   </strong>\n   : Achieved 2x to 10x faster processing in extracting structured metadata from text, boosting RAG performance.\n   <a href=\"https://t.co/sBVWeO8jKo\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1730400634757939691?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We launched versions 3 of\n   <a href=\"https://github.com/run-llama/rags\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    RAGs\n   </a>\n   , our project that lets you use natural language to generate a RAG bot customized to your needs. This version incorporates web search, so your bot can incorporate answers fresh from the web.\n   <a href=\"https://x.com/llama_index/status/1730320635279331524?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Core\n   </strong>\n   <a href=\"https://docs.llamaindex.ai/en/latest/community/full_stack_projects.html#\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     guide\n    </strong>\n   </a>\n   <strong>\n    for Full-Stack LLM App Development\n   </strong>\n   : Simplifies complex app development with tools like \u2018create-llama\u2019 for full-stack apps, \u2018SEC Insights\u2019 for multi-document processing, and \u2018LlamaIndex Chat\u2019 for chatbot customization.\n  </li>\n </ol>\n <p>\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </p>\n <ul>\n  <li>\n   We\u2019ve launched seven advanced retrieval LlamaPacks, serving as templates to easily build advanced RAG systems. These packs simplify the process to almost a single line of code, moving away from the traditional notebook approach. The techniques include Hybrid Fusion, Query Rewriting + Fusion, Retrieval with Embedded Tables, Auto-merging Retriever, Sentence Window Retriever, Node Reference Retriever, and Multi-Document Agents for handling complex queries.\n   <a href=\"https://x.com/llama_index/status/1729303619760259463?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We introduce new abstractions for structured output extraction in multi-modal settings, enabling the transformation of images into structured Pydantic objects. This enhancement is particularly useful for applications like product reviews, restaurant listings, and OCR.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/multi_modal_pydantic.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1729535952912290050?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We introduce the\n   <a href=\"https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    OpenAI Cookbook\n   </a>\n   , a guide focused on evaluating RAG systems using LlamaIndex. It encompasses understanding RAG systems, building them with LlamaIndex, and evaluating their performance in retrieval and response generation.\n   <a href=\"/openai-cookbook-evaluating-rag-systems-fe393c61fb93\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog\n   </a>\n   ,\n   <a href=\"https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1729587400240967761?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We launched RAGs v3 \u2014 a bot that transcends traditional limits by incorporating web search capabilities. This bot, designed to operate in natural language rather than code, offers an enhanced experience compared to the combination of ChatGPT and Bing. Leveraging our integration with Metaphor Systems \u2014 a search engine tailored for Large Language Models (LLMs) \u2014 the bot can retrieve relevant text from the internet to provide answers beyond its internal corpus. Additionally, users can now view the tools the agent uses, with the web search feature exclusively accessible to our OpenAI agent.\n   <a href=\"https://t.co/838BDVOEbA\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Repo\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1730320635279331524?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have significantly improved the speed of extracting structured metadata (like titles and summaries) from text to enhance RAG performance. Our new implementation offers 2x to 10x faster processing, overcoming the limitations of previous slower methods.\n   <a href=\"https://t.co/sBVWeO8jKo\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1730400634757939691?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have made it incredibly easy to set up a RAG + Streamlit app, now possible with just a single line of code using our\n   <code class=\"cw ov ow ox oy b\">\n    <strong>\n     StreamlitChatPack\n    </strong>\n   </code>\n   . This pack provides a ready-to-use RAG pipeline and a Streamlit chat interface, customizable in terms of data sources and retrieval algorithms.\n   <a href=\"https://llamahub.ai/l/llama_packs-streamlit_chatbot\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1731121252142878982?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83d\udc40 Demo:\n  </strong>\n </p>\n <p>\n  AInimal Go \u2014 an innovative multi-modal app inspired by Pokemon-Go. This interactive application, developed by\n  <a href=\"https://harshadsuryawanshi.medium.com/\" rel=\"noopener\">\n   Harshad Suryawanshi\n  </a>\n  , lets users capture or upload images of animals, classify them using the ResNet-18 model, and engage in conversations with the animals, augmented by a knowledge base of over 200 Wikipedia articles. Notably, the app employs a targeted ResNet model for classification, offering enhanced speed and cost efficiency, instead of using GPT-4V.\n </p>\n <p>\n  <a href=\"/multimodal-rag-building-ainimal-go-fecf8404ed97\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Blog\n  </a>\n  ,\n  <a href=\"https://github.com/AI-ANK/AInimalGo-Chat-with-Animals\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Repo\n  </a>\n  ,\n  <a href=\"https://huggingface.co/spaces/AI-ANK/AInimal_Go\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   HuggingFace Space\n  </a>\n  ,\n  <a href=\"https://x.com/llama_index/status/1729246724911477165?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Tweet\n  </a>\n  .\n </p>\n <p>\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </p>\n <ul>\n  <li>\n   We introduce a core\n   <a href=\"https://docs.llamaindex.ai/en/latest/community/full_stack_projects.html#\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    guide\n   </a>\n   within the LlamaIndex ecosystem, designed to simplify \u201cfull-stack\u201d app development, which is notably more complex than notebook development. This includes \u2018create-llama\u2019 for building full-stack apps with advanced templates, \u2018SEC Insights\u2019 for multi-document handling of over 10,000 filings, and \u2018LlamaIndex Chat\u2019 for a customizable chatbot experience. All tools are open-source with full guides and tutorials available.\n  </li>\n  <li>\n   <a href=\"https://t.co/2Ygxs6bPoX\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   on using the Table Transformer model with GPT-4V for advanced RAG applications in parsing tables from PDFs: Our method involves CLIP for page retrieval, Table Transforms for table image extraction, and GPT-4V for answer synthesis. This approach is compared with three other multi-modal table understanding techniques, including using CLIP for whole page retrieval, text extraction and indexing with GPT-4V, and OCR on table images for context.\n  </li>\n  <li>\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/multi_modal_pydantic.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   on analyzing various multi-modal models for their ability to extract structured data from complex product images on an Amazon page. The models compared include GPT-4V, Fuyu-8B, MiniGPT4, CogVLM-4, and LLaVa-13B. Key findings reveal that all models incorrectly identified the number of reviews (correct answer: 5685), only GPT-4V and Fuyu accurately determined the price, each model\u2019s product description varied from the original, and Mini-GPT4 incorrectly assessed the product rating.\n  </li>\n </ul>\n <p>\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://www.linkedin.com/in/jo-bergum/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Jo Kristian Bergum\n   </a>\n   <a href=\"https://blog.vespa.ai/scaling-personal-ai-assistants-with-streaming-mode/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    blog post\n   </a>\n   on Hands-On RAG guide for personal data with Vespa and LLamaIndex.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/wenqi_glantz\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Wenqi Glantz\n   </a>\n   made a\n   <a href=\"https://levelup.gitconnected.com/llama-packs-the-low-code-solution-to-building-your-llm-apps-269eec05557b\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Llama Packs: The Low-Code Solution to Building Your LLM Apps.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/Lazer\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Liza Shulyayeva\n   </a>\n   \u2019s in-depth\n   <a href=\"https://www.daily.co/blog/search-your-video-content-library-with-llamaindex-and-chroma/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on building and deploying a retrieval-augmented generation (RAG) app to conversationally query the contents of your video library\n  </li>\n </ul>\n <p>\n  \ud83c\udfa5\n  <strong>\n   Webinars:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=0zGHrcE-Zy4\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Webinar\n   </a>\n   on PrivateGPT \u2014 Production RAG with Local Models.\n  </li>\n </ul>\n <p>\n  \ud83c\udfc6\n  <strong>\n   Hackathons:\n  </strong>\n </p>\n <ul>\n  <li>\n   Your reminder that there\u2019s still time to join\n   <a href=\"https://lablab.ai/event/truera-challenge-build-llm-applications?utm_medium=post&amp;utm_source=twitter&amp;utm_campaign=truera_challenge_hackathon&amp;utm_term=hackathon_page&amp;utm_content=event_promo\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    the TruEra Challenge\n   </a>\n   , an online hackathon from Dec 1st to 8th, and explore AI observability with technology from TruEra AI and Google Vertex AI. Use the LlamaIndex framework to enhance your LLM-based app. Participants receive $30 in Google Cloud credits, plus an additional $100 upon solution submission. Winners share a $9,000 cash prize pool and $14,000 in Google Cloud credits.\n  </li>\n  <li>\n   We partnered with Zilliz Universe to participate in their\n   <a href=\"https://t.co/hGzBA1acf6\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Advent of Code\n   </a>\n   event. This December, explore 25 open-source projects, with daily challenges to build something in 30 minutes or less. It\u2019s a great opportunity to learn new skills and have winter fun. For tips, tutorials, and resources, visit the Advent of Code channel in Discord each day.\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 13697, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2896da82-3211-4bde-8115-d8d376dfeeb2": {"__data__": {"id_": "2896da82-3211-4bde-8115-d8d376dfeeb2", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-12-4a5d542fbb1e.html", "file_name": "llamaindex-newsletter-2023-12-12-4a5d542fbb1e.html", "file_type": "text/html", "file_size": 14752, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-12-4a5d542fbb1e.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-12-4a5d542fbb1e.html", "file_name": "llamaindex-newsletter-2023-12-12-4a5d542fbb1e.html", "file_type": "text/html", "file_size": 14752, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "36618229597305a14af3fee930f32d9a2c1a65316c1e39d8887ea3f8318f36c6", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Howdy, Llama Enthusiasts \ud83e\udd99,\n </p>\n <p>\n  We are thrilled to announce another exciting week filled with full of the latest updates, features, insightful tutorials, guides, webinars, and so much more. Have a groundbreaking project, compelling article, or captivating video? We\u2019re all ears! Reach out to us at\n  <a href=\"mailto:news@llamaindex.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   news@llamaindex.ai\n  </a>\n  .\n </p>\n <p>\n  Don\u2019t forget to subscribe to our newsletter via our\n  <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   website\n  </a>\n  to have all these exciting developments delivered directly to your inbox.\n </p>\n <p>\n  \ud83e\udd29\n  <strong>\n   First, the highlights:\n  </strong>\n </p>\n <ul>\n  <li>\n   <strong>\n    Llama Datasets:\n   </strong>\n   A diverse collection of community-contributed datasets for benchmarking RAG pipelines.\n   <a href=\"/introducing-llama-datasets-aadb9994ad9e\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1731718080223707148?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    RAGs v5:\n   </strong>\n   Enables multi-modal data handling with natural language for both text and image sources.\n   <a href=\"https://x.com/llama_index/status/1731843485115064531?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Production RAG Pipeline:\n   </strong>\n   New features and a guide for efficient RAG while handling updates to your data, including incremental re-indexing for Google Docs and enhanced transformation and caching processes.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/ingestion/ingestion_gdrive.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1732121799033487361?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Revamped LlamaHub:\n   </strong>\n   A community-driven hub with universal data loaders, a new user interface, and a range of tools, templates, and datasets.\n   <a href=\"https://x.com/llama_index/status/1732814499235962907?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    AutoTranslateDoc:\n   </strong>\n   An open-source project for translating GitHub repository documentation into over 15 languages.\n   <a href=\"/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog\n   </a>\n   ,\n   <a href=\"https://github.com/run-llama/automatic-doc-translate\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Repo\n   </a>\n   ,\n   <a href=\"https://x.com/jerryjliu0/status/1732926141118472448?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n </ul>\n <p>\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </p>\n <ul>\n  <li>\n   We launched\n   <strong>\n    Llama Datasets\n   </strong>\n   \ud83e\udd99\ud83d\udcdd, a collection of community-contributed datasets tailored for benchmarking RAG pipelines in various use cases. These datasets offer flexibility in selecting the most appropriate one for specific LLM applications. The initial release includes a diverse range, such as Code Help Desk, FinanceBench, Mini TruthfulQA, Mini Squad V2, Blockchain Solana, Uber 10K, Llama 2 Paper, Paul Graham Essay, Origin of COVID-19, CovidQADataset, MiniCovidQADataset and LLM Survey Paper. Each dataset, designed as a QA set, integrates smoothly with Llama Index abstractions, providing a platform for comprehensive benchmarking across multiple metrics. All datasets are available on LlamaHub for easy download and evaluation.\n   <a href=\"/introducing-llama-datasets-aadb9994ad9e\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1731718080223707148?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We launched\n   <strong>\n    RAGs v5\n   </strong>\n   , enabling multi-modal data handling with natural language for both text and image sources. Key features include enhanced multi-modal indexing, the capability to view sources in any RAG agent, and support for loading entire directories, not just single files.\n   <a href=\"https://x.com/llama_index/status/1731843485115064531?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have launched new features and a guide for building a\n   <strong>\n    production RAG pipeline\n   </strong>\n   , enabling efficient question-answering with LLMs on production data even while it is continuously updated. This includes incremental re-indexing for Google Docs changes and enhanced transformation and caching processes in our updated\n   <code class=\"cw ou ov ow ox b\">\n    <strong>\n     IngestionPipeline\n    </strong>\n   </code>\n   .\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/ingestion/ingestion_gdrive.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1732121799033487361?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We launched a one-click, full-stack LlamaIndex template now available on\n   <strong>\n    Replit\n   </strong>\n   ! This template features a full-stack Next.js app in TypeScript, capable of reading any files you provide, and includes a chat interface for querying those documents. It\u2019s completely customizable and based on our popular create-llama generator.\n   <a href=\"https://replit.com/@LlamaIndex/createllama\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Replit Template\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1732150579928150247?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have introduced\n   <code class=\"cw ou ov ow ox b\">\n    <strong>\n     RAGEvaluatorPack\n    </strong>\n   </code>\n   to easily benchmark your RAG pipeline on any dataset with a single line of code, offering metrics like correctness, relevancy, and context similarity.\n   <a href=\"https://llamahub.ai/l/llama_packs-rag_evaluator\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1732210229574824357?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We released community templates for create-llama, offering a selection of community-contributed starter templates during setup. Current examples include\n   <code class=\"cw ou ov ow ox b\">\n    embedded-tables\n   </code>\n   for analyzing complex tables in large PDFs, and\n   <code class=\"cw ou ov ow ox b\">\n    multi-document-agent\n   </code>\n   for comparing multiple documents.\n   <a href=\"https://x.com/llama_index/status/1732480745804022240?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We launched multi-modal support in create-llama, our user-friendly command-line tool for generating full-stack LlamaIndex apps. Now, easily integrate GPT-4-vision in your app, allowing you to upload images to the web interface and receive answers about them in just seconds.\n   <a href=\"https://x.com/llama_index/status/1732480613763215783?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We launched the Ollama LlamaPack, a new offering that integrates local LLMs and embeddings into a fully local RAG pipeline, enhancing language model accessibility and capabilities.\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/llama_hub/llama_pack_ollama.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1732565478546223322?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We launched the revamped\n   <a href=\"https://llamahub.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaHub\n   </a>\n   , a hub for community-driven modules to enhance LLM app development, featuring universal data loaders, a new user interface, and a range of tools, templates, and datasets.\n   <a href=\"https://x.com/llama_index/status/1732814499235962907?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We introduced AutoTranslateDoc, an open-source project for translating GitHub repository documentation into over 15 languages, including Chinese, Spanish, and French. This tool, successfully implemented in our own LlamaIndex.TS docs, simplifies the internationalization process for open-source projects.\n   <a href=\"/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog\n   </a>\n   ,\n   <a href=\"https://github.com/run-llama/automatic-doc-translate\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Repo\n   </a>\n   ,\n   <a href=\"https://x.com/jerryjliu0/status/1732926141118472448?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   We released support for exact match and range queries in 4 vector databases including Weaviate, Chroma, Qdrant and Pinecone, allowing auto-retrieval via metadata filters, elevating the functionality of structured and unstructured data querying.\n   <a href=\"https://x.com/llama_index/status/1733289204380311703?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://docs.google.com/presentation/d/1o4OeOvyaXAGNF1Dlbw4s5KYOMAE-2fUzqxo2lproprw/edit#slide=id.g2a2d0d2fc2a_0_0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   on building LLM apps for financial data which is presented at MindsDB event. Learn to query diverse financial data using advanced RAG with techniques for multi-document comparisons, embedded tables, and converting text queries into domain-specific languages.\n  </li>\n  <li>\n   <a href=\"https://docs.google.com/presentation/d/1IJ1bpoLmHfFzKM3Ef6OoWGwvrwDwLV7EcoOHxLZzizE/edit#slide=id.g23d546514bd_0_290\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   on advanced RAG Cheat Sheet, a concise guide offering solutions for different RAG-related pain points and techniques. It\u2019s part of our Snowflake BUILD talk and PyData Global talk.\n  </li>\n </ul>\n <p>\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog\n   </a>\n   by\n   <a href=\"http://waii.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Waii.ai\n   </a>\n   on creating an agent that queries both enterprise databases and PDF data, combining advanced text-to-SQL techniques and a Llama Index RAG pipeline, for effective analysis of structured and unstructured data like retail sales trends.\n  </li>\n  <li>\n   Wenqi Glantz\u2019s\n   <a href=\"https://levelup.gitconnected.com/a-simpler-way-to-query-neo4j-knowledge-graphs-99c0a8bbf1d7\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on using LLMs for querying knowledge graphs introduces seven strategies, now easily accessible through our LlamaPacks and featured in our Neo4j query engine.\n  </li>\n  <li>\n   An hour comprehensive workshop\n   <a href=\"https://www.youtube.com/watch?v=oa82yoJ6zYc\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   by\n   <a href=\"https://twitter.com/AIMakerspace\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    AIMakerspace\n   </a>\n   on RAG strategies over complex documents through recursive retrieval.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/seldo\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Laurie\u2019s\n   </a>\n   <a href=\"https://www.notion.so/Content-roadmap-6f27c002d67e428497326d972afa7eb6?pvs=21\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    video\n   </a>\n   on using LlamaIndex for multi-modal retrieval-augmented generation apps teaches you to build indexes and retrieve data from text and images, for enhanced query responses.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/ravithejads\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Ravi Theja\u2019s\n   </a>\n   <a href=\"https://www.youtube.com/watch?v=_vU-biwMoGk\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    video\n   </a>\n   on Understanding LlamaIndex 0.9v abstractions and features.\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83e\udd1d Integrations:\n  </strong>\n </p>\n <ul>\n  <li>\n   We integrated AssemblyAI with Llama Index TS, enhancing the capabilities and offering new, innovative solutions.\n   <a href=\"https://www.assemblyai.com/blog/announcing-the-assemblyai-integration-for-llamaindex-ts/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog\n   </a>\n   .\n  </li>\n  <li>\n   We integrated Panel, a powerful framework for building interactive data apps as a LlamaPack. This provides you with a robust chat interface for talking to your data with full streaming support in a single line of code.\n   <a href=\"https://t.co/QSVdFxvfAi\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1733894204076720551?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We integrated FlagEmbeddingReranker to further boost your RAG pipeline.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/node_postprocessor/FlagEmbeddingReranker.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1734019264166953421?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  \ud83c\udfa5\n  <strong>\n   Webinars:\n  </strong>\n </p>\n <p>\n  Webinar featuring Haotian Liu, the author of LLaVa which includes a deep dive into the open-source multi-modal models of LLaVa, which are competitive with GPT-4V, and a presentation on multi-modal use cases with LLaVa + LlamaIndex by Haotian Zhang from the LlamaIndex team.\n </p>\n <p>\n  <strong>\n   \ud83c\udfe2 Calling all enterprises:\n  </strong>\n </p>\n <p>\n  Are you building with LlamaIndex? We are working hard to make LlamaIndex even more Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested?\n  <a href=\"https://docs.google.com/forms/d/e/1FAIpQLScBNdM2a_fn8UZOKmFQt6lBsrd1o6FflvsdPH-Pn3JkdlN_Rg/viewform\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Get in touch.\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 14705, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5fcab154-b832-4485-b490-a1fc906a44ce": {"__data__": {"id_": "5fcab154-b832-4485-b490-a1fc906a44ce", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-19-2965a2d03726.html", "file_name": "llamaindex-newsletter-2023-12-19-2965a2d03726.html", "file_type": "text/html", "file_size": 15040, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-19-2965a2d03726.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-19-2965a2d03726.html", "file_name": "llamaindex-newsletter-2023-12-19-2965a2d03726.html", "file_type": "text/html", "file_size": 15040, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "58ea14915c47a8d01b4e84fb387c28379f4b91dd8b4bdadfe14fb9d0ba7a256f", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  What\u2019s up, Llama Followers \ud83e\udd99,\n </p>\n <p>\n  We\u2019re excited to bring you another week packed with the latest updates, features, exciting community demos, insightful tutorials, guides, and webinars. This week, don\u2019t miss our special holiday\n  <a href=\"https://lu.ma/0eru1il4\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   workshop\n  </a>\n  on 12/21, where we\u2019ll dive into innovative LLM + RAG use cases with Google Gemini team.\n </p>\n <p>\n  Got a groundbreaking project, compelling article, or captivating video? We\u2019re all ears! Reach out to us at\n  <a href=\"mailto:news@llamaindex.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   news@llamaindex.ai\n  </a>\n  . Remember to subscribe to our newsletter via our\n  <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   website\n  </a>\n  to get all these exciting developments straight to your inbox.\n </p>\n <p>\n  \ud83e\udd29\n  <strong>\n   First, the highlights:\n  </strong>\n </p>\n <ol>\n  <li>\n   <strong>\n    Google Gemini Partnership:\n   </strong>\n   Now offering day 1 support for Gemini API on LlamaIndex, complete with comprehensive cookbooks for advanced RAG capabilities.\n   <a href=\"https://x.com/llama_index/status/1734965271004340610?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    MistralAI Integrations:\n   </strong>\n   Introduced day-0 integrations with MistralAI LLMs and Embedding model for building RAG solutions on LlamaIndex.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/mistralai.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1734387934722499022?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Docugami Multi-Doc Llama Dataset:\n   </strong>\n   Launched the Multi-Doc SEC 10Q Dataset by Taqi Jaffri, offering a range of question complexities for advanced RAG research.\n   <a href=\"https://llamahub.ai/l/llama_datasets-docugami_kg_rag-sec_10_q\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1735370350316405058?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Proposition-Based Retrieval:\n   </strong>\n   Implemented a new retrieval unit based on propositions, enhancing QA performance with LLMs.\n   <a href=\"https://llamahub.ai/l/llama_packs-dense_x_retrieval?from=llama_packs\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1735102459000013283?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    RAG Pipeline Enhancement Guide:\n   </strong>\n   Introduced a guide featuring modules like Routing, Query-Rewriting, and Agent Reasoning for more complex QA over documents.\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/query_transformations/query_transform_cookbook.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   .\n  </li>\n </ol>\n <p>\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </p>\n <ul>\n  <li>\n   We launched a partnership with Google Gemini, offering day 1 support for the Gemini API on LlamaIndex, including full-feature support for Gemini (text and multi-modal) and Semantic Retriever API, complemented by three comprehensive cookbooks:\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/gemini.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Gemini LLM\n   </a>\n   ,\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/gemini.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Gemini Multi-modal\n   </a>\n   , and\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/managed/GoogleDemo.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Semantic Retriever API\n   </a>\n   , promising advanced RAG capabilities and multi-modal integrations.\n   <a href=\"https://x.com/llama_index/status/1734965271004340610?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We introduced day-0 integrations with the MistralAI LLMs (mistral-tiny, mistral-small, mistral-medium) and the MistralAI Embedding model for building RAG solutions with LlamaIndex both on Python and Typescript versions.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/mistralai.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1734387934722499022?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We launched the COVID-QA dataset on LlamaHub, a human-annotated, substantial set of 300+ QA pairs about COVID from various web articles, complete with source URLs for easy integration into RAG pipelines, offering ample scope for improvement.\n   <a href=\"https://llamahub.ai/l/llama_datasets-covidqa?from=llama_datasets\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1734383167711441000?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We launched a new multi-modal template in Create-llama, enabling image input and output generation using the latest GPT-4-vision model from OpenAI, expanding possibilities for diverse use cases.\n   <a href=\"https://github.com/run-llama/create_llama_projects/tree/main/nextjs-multi-modal\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1735017333180223585?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have introduced Proposition-Based Retrieval in LlamaIndex: Implementing a new retrieval unit based on propositions, as introduced in the \u2018Dense X Retrieval\u2019 paper, enhancing QA performance with LLMs by indexing propositions and linking to the underlying text.\n   <a href=\"https://llamahub.ai/l/llama_packs-dense_x_retrieval?from=llama_packs\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1735102459000013283?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We partnered with Docugami to launch a new Multi-Doc SEC 10Q Dataset by\n   <a href=\"https://twitter.com/tjaffri\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Taqi Jaffri\n   </a>\n   , aimed at advancing QA datasets for RAG evaluation. This dataset offers a range of question complexities: Single-Doc, Single-Chunk RAG; Single-Doc, Multi-Chunk RAG; and Multi-Doc RAG, addressing the need for more intricate datasets in RAG research.\n   <a href=\"https://llamahub.ai/l/llama_datasets-docugami_kg_rag-sec_10_q\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1735370350316405058?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We launched a SharePoint data loader, enabling direct integration of SharePoint files into LLM/RAG pipelines.\n   <a href=\"https://llamahub.ai/l/microsoft_sharepoint?from=all\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1735829020187767092?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83d\udc40 Community Demos\n  </strong>\n  :\n </p>\n <ul>\n  <li>\n   MemoryCache: Mozilla\u2019s new experimental project that curates your online experience into a private, on-device RAG application using PrivateGPT_AI and LlamaIndex, enhancing personal knowledge management while maintaining privacy.\n   <a href=\"https://memorycache.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Website\n   </a>\n   ,\n   <a href=\"https://github.com/Mozilla-Ocho/Memory-Cache\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Repo\n   </a>\n   .\n  </li>\n  <li>\n   OpenBB Finance showcases its enhanced chat widget feature in Terminal Pro, utilizing LlamaIndex\u2019s data chunking combined with Cursor AI for improved large context management and accuracy.\n   <a href=\"https://x.com/josedonato__/status/1734992691090325616?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   AI Chatbot Starter (from the DataStax team), a web server powered by AstraDB and LlamaIndex, allows easy setup for chatting over web documentation. It can be used as a standalone service or integrated into full-stack applications, with simple credential setup and document ingestion.\n   <a href=\"https://github.com/datastax/ai-chatbot-starter\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Repo\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1735350801609179371?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   Na2SQL (by\n   <a href=\"https://twitter.com/HarshadSurya1c\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     Harshad\n    </strong>\n   </a>\n   <strong>\n    )\n   </strong>\n   to ****Build an End-to-End SQL Analyst App on Streamlit featuring interactive database viewing, SQL query displays, and integration with Llama Index.\n   <a href=\"/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog\n   </a>\n   ,\n   <a href=\"https://github.com/AI-ANK/Na2SQL/tree/main\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Repo\n   </a>\n   .\n  </li>\n  <li>\n   LionAGI (by\n   <a href=\"https://twitter.com/quantoceanli\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     Ocean Li\n    </strong>\n   </a>\n   ) is an agent framework for efficient data operations and support for concurrent calls and JSON mode with OpenAI. Check it to integrate it with a Llama Index RAG pipeline for automated AI assistants like an ArXiv research assistant.\n   <a href=\"https://lionagi.readthedocs.io/en/latest/index.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://github.com/lion-agi/lionagi\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Repo\n   </a>\n   .\n  </li>\n  <li>\n   Local RAG for Windows (from Marklysze): A comprehensive resource for integrating advanced LLMs into RAG workflows using Windows Subsystem for Linux, featuring five detailed cookbooks.\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/query_transformations/query_transform_cookbook.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   for enhancing RAG pipelines with a Query Understanding Layer, featuring modules like Routing, Query-Rewriting, Sub-Question creation, and Agent Reasoning, all designed to enable more complex and \u2018agentic\u2019 QA over documents.\n  </li>\n  <li>\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/gemini.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   to Building a Restaurant Recommendation QA System with Gemini to extract structured image data and utilize multi-modal Retrieval-Augmented Generation for enhanced query responses.\n  </li>\n  <li>\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/managed/GoogleDemo.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   to building Advanced RAG with Safety Guardrails to create constrained RAG systems with Gemini API\u2019s semantic search, safety features, and Google Semantic Retriever integrations.\n  </li>\n  <li>\n   Guide on\n   <a href=\"https://qdrant.tech/documentation/tutorials/llama-index-multitenancy/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Qdrant\u2019s Multitenancy with LlamaIndex\n   </a>\n   on setting up payload-based partitioning for user data isolation in vector services.\n  </li>\n  <li>\n   <a href=\"/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   on using Prometheus \u2014 an open-source 13B LLM for RAG Evaluations, comparing it with GPT-4 evaluation with insights on its performance in terms of cost-effectiveness, accuracy, and scoring biases.\n  </li>\n </ul>\n <p>\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://twitter.com/seldo\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Laurie\u2019s\n   </a>\n   <a href=\"https://www.youtube.com/watch?v=Y0FL7BcSigI\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Advanced Querying &amp; Retrieval Techniques comprehensive code-level tutorial\n   </a>\n   on 7 advanced querying and retrieval techniques including SubQuestionQuery Engine, Small-to-big retrieval, Metadata filtering, Hybrid search, Recursive Retrieval, Text to SQL, and Multi-document agents.\n  </li>\n  <li>\n   <a href=\"https://www.youtube.com/@hubel-labs\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Hubel Labs\n   </a>\n   \u2019 Advanced RAG\n   <a href=\"https://www.youtube.com/watch?v=oDzWsynpOyI\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    video tutorial\n   </a>\n   with Llamaindex &amp; OpenAI GPT: Sentence Window Retrieval vs Basic Chunking\n  </li>\n  <li>\n   <a href=\"https://twitter.com/Dev__Digest\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Developers Digest\n   </a>\n   <a href=\"https://www.youtube.com/watch?v=i1qTOKpTUWo\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    video tutorial\n   </a>\n   on getting started with llamaindex.ts .\n  </li>\n  <li>\n   <a href=\"https://twitter.com/matchaman11\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Anil\u2019s\n   </a>\n   <a href=\"/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on How to train a custom GPT on your data with EmbedAI + LlamaIndex.\n  </li>\n </ul>\n <p>\n  \ud83c\udfa5\n  <strong>\n   Webinars:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://twitter.com/tonykipkemboi\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tony Kipkemboi\n   </a>\n   (Streamlit) and\n   <a href=\"https://twitter.com/yi_ding\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Yi Ding\n   </a>\n   (LlamaIndex)\n   <a href=\"https://www.youtube.com/watch?v=PLKkudXYCNI\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    webinar\n   </a>\n   on Demystifying RAG apps with LlamaIndex!\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83c\udfe2 Calling all enterprises:\n  </strong>\n </p>\n <p>\n  Are you building with LlamaIndex? We are working hard to make LlamaIndex even more Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested?\n  <a href=\"https://docs.google.com/forms/d/e/1FAIpQLScBNdM2a_fn8UZOKmFQt6lBsrd1o6FflvsdPH-Pn3JkdlN_Rg/viewform\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Get in touch.\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 14979, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "982f24c2-20a2-49d4-9f3a-cb216cb40c75": {"__data__": {"id_": "982f24c2-20a2-49d4-9f3a-cb216cb40c75", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-02-f349db8c1842.html", "file_name": "llamaindex-newsletter-2024-01-02-f349db8c1842.html", "file_type": "text/html", "file_size": 17293, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-02-f349db8c1842.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-02-f349db8c1842.html", "file_name": "llamaindex-newsletter-2024-01-02-f349db8c1842.html", "file_type": "text/html", "file_size": 17293, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "780758123860007109dc0e52b7a8cf11848bfa8b872dddbad7156e0253dd1ea0", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Hello, Llama Lovers \ud83e\udd99,\n </p>\n <p>\n  Happy New Year! As we step into 2024, we\u2019re thrilled to bring you a special edition of our newsletter, packed with updates from the last two weeks of 2023. This edition is brimming with the latest features, community demos, courses, insightful tutorials, guides, and webinars that we\u2019ve curated for you.\n </p>\n <p>\n  Have you been working on an interesting project, written an engaging article, or created a video? We can\u2019t wait to hear about it! Please share your work with us at\n  <a href=\"mailto:news@llamaindex.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   news@llamaindex.ai\n  </a>\n  . Don\u2019t forget to subscribe to our newsletter via our\n  <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   website\n  </a>\n  to receive all these exciting updates directly in your inbox.\n </p>\n <p>\n  \ud83e\udd29\n  <strong>\n   First, the highlights:\n  </strong>\n </p>\n <ol>\n  <li>\n   <strong>\n    LLMCompiler Implementation:\n   </strong>\n   A SOTA agent implementation for faster, efficient handling of complex queries.\n   <a href=\"https://github.com/run-llama/llama-hub/blob/main/llama_hub/llama_packs/agents/llm_compiler/llm_compiler.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1740778394856648843?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    MultiDocAutoRetrieverPack:\n   </strong>\n   A RAG template for structured retrieval and dynamic responses to large documents and metadata.\n   <a href=\"https://x.com/llama_index/status/1739307699773518201?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   ,\n   <a href=\"https://llamahub.ai/l/llama_packs-multidoc_autoretrieval?from=llama_packs\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaPack\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Structured Hierarchical RAG:\n   </strong>\n   New RAG technique for optimized retrieval over multiple documents, ensuring precise, relevant responses.\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/query_engine/multi_doc_auto_retrieval/multi_doc_auto_retrieval.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1737515390664872040?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Custom Agents:\n   </strong>\n   A simple abstraction for custom agent reasoning loops, enabling easy integration with RAG, SQL, and other systems, and enhancing response refinement for complex queries.\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/agent/custom_agent.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1741141394558001414?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    New lower-level agent API:\n   </strong>\n   For enhanced transparency, debuggability, and control, supporting step-wise execution and task modification.\n   <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/agent_runner.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1736809248947155076?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <p>\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </p>\n <ul>\n  <li>\n   We have introduced a simple abstraction for building custom agent reasoning loops, surpassing prepackaged frameworks like ReAct. This tool allows for easy integration with RAG, SQL, or other systems, and we demonstrated how to build an agent with retry logic for routers, enhancing its ability to manage complex, multi-part questions and refine query responses.\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/agent/custom_agent.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1741141394558001414?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have implemented the LLMCompiler project, a SOTA agent framework enabling DAG-based planning and parallel function execution. This surpasses traditional sequential methods in speed, allowing for quicker and more efficient handling of complex queries in any LLM and data pipeline.\n   <a href=\"https://github.com/run-llama/llama-hub/blob/main/llama_hub/llama_packs/agents/llm_compiler/llm_compiler.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1740778394856648843?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have introduced MultiDocAutoRetrieverPack, a RAG template for efficiently handling large documents and metadata, offering structured retrieval and dynamic responses tailored to specific queries.\n   <a href=\"https://x.com/llama_index/status/1739307699773518201?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   ,\n   <a href=\"https://llamahub.ai/l/llama_packs-multidoc_autoretrieval?from=llama_packs\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaPack\n   </a>\n   .\n  </li>\n  <li>\n   We have introduced a Structured Hierarchical RAG technique, optimizing RAG over multiple documents. It involves modeling documents as structured metadata for auto-retrieval, indexed in a vector database. This method dynamically selects documents based on inferred properties and performs recursive retrieval within each document for precise, relevant responses in your RAG pipeline.\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/query_engine/multi_doc_auto_retrieval/multi_doc_auto_retrieval.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1737515390664872040?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have launched a new feature for advanced RAG that allows step-wise feedback for complex query executions, improving interpretability and control. This is particularly beneficial for weaker models that struggle with multi-part tasks. We also introduced a step-by-step chat interface for enhanced user interaction and control.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/agent/agent_runner/agent_runner_rag_controllable.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1737161312944468412?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have integrated with OpenRouterAI, offering a unified API for easy LLM access, cost efficiency, and reliable fallback options. OpenRouterAI allows users to compare costs, latency, and throughput for various models, like mixtral-8x7b, directly on their platform.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/openrouter.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1737176999712731349?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have introduced a new lower-level agent API that enhances transparency, debuggability, and control. This API allows for granular control over agents, decouples task creation from execution, and supports step-wise execution. It also enables viewing each step, upcoming steps, and soon, modifying intermediate steps with human feedback.\n   <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/agent_runner.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1736809248947155076?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83d\udc40 Community Demos\n  </strong>\n  :\n </p>\n <ul>\n  <li>\n   <strong>\n    Automated LeetCode Crash Course:\n   </strong>\n   The Project integrates advanced ML with traditional algorithms to streamline LeetCode study for technical interviews. It involves extracting and summarizing LeetCode problems using an LLM, organizing these summaries in a vector store, and employing scikit-learn for clustering.\n   <a href=\"https://medium.com/@kevinchwong/from-machine-learning-to-learning-machine-483eaa4b2855\" rel=\"noopener\">\n    Blog\n   </a>\n   ,\n   <a href=\"https://github.com/kevinchwong/leetcode-intensive/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Code\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    RAG Assisted Auto Developer\n   </strong>\n   : A project by\n   <a href=\"https://twitter.com/quantoceanli\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     Ocean Li\n    </strong>\n   </a>\n   for building a devbot that understands and writes code. It integrates various tools: LlamaIndex for indexing codebases, Autogen / OpenAI Code Interpreter for code writing and testing, and\n   <a href=\"http://lionagi.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    lionagi.ai\n   </a>\n   for orchestration.\n   <a href=\"https://github.com/lion-agi/lionagi/blob/main/notebooks/AutoDev_with_llama_autogen_assistant.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83d\udcda Courses:\n  </strong>\n </p>\n <ul>\n  <li>\n   We\u2019ve partnered with\n   <a href=\"https://twitter.com/activeloopai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    ActiveLoop AI\n   </a>\n   to provide a\n   <a href=\"https://learn.activeloop.ai/courses/rag\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    free course\n   </a>\n   on retrieval-augmented generation for production, featuring 33 lessons, 7 hands-on assignments, and a certification upon completion.\n  </li>\n  <li>\n   Beginner-friendly\n   <a href=\"https://cognitiveclass.ai/courses/course-v1:IBMSkillsNetwork+GPXX0OLGEN+v1#about-course\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    course\n   </a>\n   from\n   <a href=\"https://twitter.com/skillsnetworkhq\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    IBM Skills Network\n   </a>\n   on using LlamaIndex with IBM Watsonx to create effective product recommendations.\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/structured_image_retrieval.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   to Semi-Structured Image QA with Gemini: Learn to extract data from unlabeled images and query it, using multi-modal models and advanced retrieval techniques, as demonstrated with the SROIE v2 dataset which contains images of receipts/invoices.\n  </li>\n  <li>\n   <a href=\"https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   to Advanced RAG Concepts: A comprehensive survey by\n   <a href=\"https://twitter.com/ivanilin9\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Ivan Ilin\n   </a>\n   , covering twelve core concepts including chunking, hierarchical indexing, query rewriting, and more. Each section provides resources and guides from our system for deeper understanding and practical application.\n  </li>\n  <li>\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/vector_stores/qdrant_hybrid.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   to Building Hybrid Search: Learn to create a hybrid search for RAG from scratch. The process involves generating sparse vectors, fusing sparse and dense queries, and implementing this in a Qdrant engine database for effective RAG integration.\n  </li>\n  <li>\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/vector_stores/pinecone_auto_retriever.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   to Building Structured Retrieval with LLMs: Set up auto-retrieval in Pinecone vector database, monitor prompts with Arize AI Phoenix, and tailor prompts for specific queries to enhance your document handling and structured data analysis.\n  </li>\n  <li>\n   <a href=\"/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   on Evaluating LLM Evaluators: our new evaluation method and dataset bundle, are designed to benchmark LLMs as evaluators against human annotations. This involves comparing LLM judge predictions (1\u20135 score) with ground-truth judgments, using metrics like Correlation, Hamming Distance, and Agreement Rate.\n  </li>\n </ul>\n <p>\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://medium.com/@ryanntk\" rel=\"noopener\">\n    Ryan Nguyen\n   </a>\n   <a href=\"https://levelup.gitconnected.com/a-guide-to-processing-tables-in-rag-pipelines-with-llamaindex-and-unstructuredio-3500c8f917a7\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Processing Tables in RAG Pipelines with LlamaIndex and UnstructuredIO.\n  </li>\n  <li>\n   <a href=\"https://www.linkedin.com/in/wenqi-glantz-b5448a5a/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Wenqi Glantz\n   </a>\n   <a href=\"https://towardsdatascience.com/safeguarding-your-rag-pipelines-a-step-by-step-guide-to-implementing-llama-guard-with-llamaindex-6f80a2e07756\" rel=\"noopener\" target=\"_blank\">\n    tutorial\n   </a>\n   on Safeguarding RAG Pipelines: A Step-by-Step Guide to Implementing Llama Guard with LlamaIndex.\n  </li>\n  <li>\n   <a href=\"https://www.linkedin.com/in/wenqi-glantz-b5448a5a/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Wenqi Glantz\n   </a>\n   <a href=\"https://towardsdatascience.com/safeguarding-your-rag-pipelines-a-step-by-step-guide-to-implementing-llama-guard-with-llamaindex-6f80a2e07756\" rel=\"noopener\" target=\"_blank\">\n    tutorial\n   </a>\n   on 10+ Ways to Run Open-Source Models with LlamaIndex.\n  </li>\n  <li>\n   Jina AI\n   <a href=\"https://jina.ai/news/full-stack-rag-with-jina-embeddings-v2-and-llamaindex/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on enhancing RAG applications by integrating Jina v2 embeddings with LlamaIndex and Mixtral LLM via Hugging Face.\n  </li>\n  <li>\n   <a href=\"https://medium.com/@andysingal\" rel=\"noopener\">\n    Ankush Singal\n   </a>\n   <a href=\"https://ai.gopubby.com/benchmarking-rag-pipelines-with-a-evaluation-pack-in-forward-looking-active-retrieval-augmented-a8bd057c856b\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Benchmarking RAG Pipelines With A Evaluation Pack in Forward-Looking Active Retrieval Augmented Generation (FLARE).\n  </li>\n  <li>\n   <a href=\"https://twitter.com/seldo\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Laurie\u2019s\n   </a>\n   <a href=\"/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Effortlessly Running Mistral AI\u2019s Mixtral 8x7b: Learn to use OLLAMA with LlamaIndex for a one-line setup of a local, open-source retrieval-augmented generation app with API, featuring Qdrant engine integration for vector storage.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/tb_tomaz\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tomaz Bratanic\n   </a>\n   <a href=\"/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Multimodal RAG pipeline with LlamaIndex and Neo4j.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/mesudarshan\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Sudarshan Koirala\n   </a>\n   video\n   <a href=\"https://www.youtube.com/watch?v=N8is20i2tqA\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on using Mistral API with LlamaIndex.\n  </li>\n  <li>\n   <a href=\"https://chiajy.medium.com/\" rel=\"noopener\">\n    Chia Jeng Yang\n   </a>\n   <a href=\"https://medium.com/enterprise-rag/a-first-intro-to-complex-rag-retrieval-augmented-generation-a8624d70090f\" rel=\"noopener\">\n    tutorial\n   </a>\n   on Technical Considerations for Complex RAG.\n  </li>\n </ul>\n <p>\n  \ud83c\udfa5\n  <strong>\n   Webinars:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=fdpaHJlN0PQ\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Webinar\n   </a>\n   with Google Developers on advanced RAG applications and multi-modal settings with Google Gemini.\n  </li>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=kZxl4gpe3OM\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Webinar\n   </a>\n   of Jerry Liu with Louis-Fran\u00e7ois on the Future of AI: LlamaIndex, LLMs, RAG, Prompting, and more.\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83c\udfe2 Calling all enterprises:\n  </strong>\n </p>\n <p>\n  Are you building with LlamaIndex? We are working hard to make LlamaIndex even more Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested?\n  <a href=\"https://docs.google.com/forms/d/e/1FAIpQLScBNdM2a_fn8UZOKmFQt6lBsrd1o6FflvsdPH-Pn3JkdlN_Rg/viewform\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Get in touch.\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 17246, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "547215de-f1fe-402d-a7cb-bce82b5fb107": {"__data__": {"id_": "547215de-f1fe-402d-a7cb-bce82b5fb107", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-09-6209000da2e6.html", "file_name": "llamaindex-newsletter-2024-01-09-6209000da2e6.html", "file_type": "text/html", "file_size": 13781, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-09-6209000da2e6.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-09-6209000da2e6.html", "file_name": "llamaindex-newsletter-2024-01-09-6209000da2e6.html", "file_type": "text/html", "file_size": 13781, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "8d9907fb6454b0f40a571c2a77724db199c31eb5bca264477501d8b4516866e1", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Hola, LlamaIndex Lovers \ud83e\udd99,\n </p>\n <p>\n  Welcome to another thrilling week at LlamaIndex, filled with vibrant community contributions and enriching educational content. Immerse yourself in our engaging tutorials, guides, community demos, and webinars, all crafted to amplify your LlamaIndex experience. Before we jump into our latest updates, we\u2019re thrilled to share two major announcements:\n </p>\n <p>\n  \ud83e\uddd1\u200d\ud83c\udfeb\n  <strong>\n   Join Our LlamaIndex Community Office Hours\n  </strong>\n  : Struggling with complex LLM/RAG queries or have feedback that our documentation doesn\u2019t cover?\n  <a href=\"https://t.co/A16nitoIgV\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Register\n  </a>\n  for our community office hours for a chance to have an enlightening conversation and get your questions answered!\n </p>\n <p>\n  \ud83d\uddfa\ufe0f\n  <a href=\"https://github.com/run-llama/llama_index/discussions/9888\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    Explore Our Open-Source Roadmap for 2024\n   </strong>\n   :\n  </a>\n  We\u2019re excited to unveil our ambitious roadmap for the LlamaIndex ecosystem. Over the next 3\u20136 months, we aim to enhance LlamaIndex\u2019s production readiness, accessibility, and its advanced features, including RAG, agents, and more. This\n  <a href=\"https://github.com/run-llama/llama_index/discussions/9888\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   living document\n  </a>\n  is available on our GitHub discussions page \u2014 a must-visit to be part of our exciting journey!\n </p>\n <p>\n  Additionally, if you\u2019ve been working on an interesting project, written an insightful article, or created a captivating video, we\u2019d love to hear about it! Please share your work with us at\n  <a href=\"mailto:news@llamaindex.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   news@llamaindex.ai\n  </a>\n  . And remember to subscribe to our newsletter through our\n  <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   website\n  </a>\n  to get all these exciting updates straight to your inbox\n </p>\n <p>\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </p>\n <ol>\n  <li>\n   <strong>\n    Query Pipelines\n   </strong>\n   : Introducing a new declarative API for effortless orchestration of simple to complex RAG query workflows.\n   <a href=\"https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/root.html#\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"/introducing-query-pipelines-025dc2bb0537\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blogpost\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1744406288724017278?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    ETL Pipeline Launch\n   </strong>\n   : New repository for setting up production ETL pipelines in RAG/LLM apps, boasting a 4x speed boost and integrating Hugging Face, RabbitMQ, and AWS EKS.\n   <a href=\"https://github.com/run-llama/llamaindex_aws_ingestion\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Github Repo\n   </a>\n   ,\n   <a href=\"/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blogpost\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1742226327900721168?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Multimodal ReAct Agent\n   </strong>\n   : Launch of an agent capable of processing text and images, enhancing RAG pipeline and web search functionalities using GPT-4V.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/mm_agent.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1742588989884989477?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    RAGatouille LlamaPack\n   </strong>\n   : Introduction of an easy-to-use pack for ColBERT retrieval, enabling one-line code integration in LlamaIndex RAG pipelines.\n   <a href=\"https://llamahub.ai/l/llama_packs-ragatouille_retriever?from=llama_packs\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1743076579302105338?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <a href=\"/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     Advanced RAG Cheat Sheet\n    </strong>\n   </a>\n   : A comprehensive cheat sheet with techniques for RAG enhancement, perfect for both new and experienced LLM users.\n  </li>\n </ol>\n <p>\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </p>\n <ul>\n  <li>\n   We have introduced Query Pipelines, a declarative API designed to simplify the creation and customization of advanced RAG workflows. This tool enables the orchestration of query workflows, ranging from basic sequential chains to complex DAGs, tailored to specific use cases.\n   <a href=\"https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/root.html#\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"/introducing-query-pipelines-025dc2bb0537\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blogpost\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1744406288724017278?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have launched a repository for easily setting up a production ETL pipeline for RAG/LLM apps, offering a 4x speed increase over laptop-based operations. This solution integrates Hugging Face, RabbitMQ, Llama Index, and AWS EKS, providing fast document indexing and efficient data handling, complete with an AWS Lambda API endpoint. Ideal for RAG apps transitioning to production, especially on AWS.\n   <a href=\"https://github.com/run-llama/llamaindex_aws_ingestion\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Github Repo\n   </a>\n   ,\n   <a href=\"/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blogpost\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1742226327900721168?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have launched the Multimodal ReAct Agent, combining GPT-4V with the ability to process both text and images. This agent can perform tasks like querying a RAG pipeline or conducting web searches based on visual and textual inputs.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/mm_agent.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1742588989884989477?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   RAGatouille LlamaPack: RAGatouille simplifies the use of ColBERT, a more advanced retrieval model compared to dense embedding-based retrieval techniques. This pack allows you to build an end-to-end LlamaIndex RAG pipeline with just one line of code by ingesting documents using any of our 150+ data loaders, combined with your preferred LLM for response synthesis.\n   <a href=\"https://llamahub.ai/l/llama_packs-ragatouille_retriever?from=llama_packs\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1743076579302105338?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have integrated with Pathway\u2019s open data processing framework which enables us to handle dynamic data sources in production, automatically updating indexes based on real-time changes, ensuring up-to-date and accurate query responses.\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/data_connectors/PathwayReaderDemo.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1741862685103579476?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <a href=\"https://twitter.com/ianmst\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     Ian McCrystal\n    </strong>\n   </a>\n   has added the StripeDocsLoader to LlamaHub, enabling a quick setup of RAG over Stripe\u2019s documentation using Llama Index.\n   <a href=\"https://llamahub.ai/l/stripe_docs\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   .\n  </li>\n  <li>\n   <a href=\"https://twitter.com/mightyjeremy\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Jeremy Dyer\n   </a>\n   has integrated NVIDIA\u2019s Triton Inference Server which allows you to run optimized inference on any AI framework. It supports the TensorRT-LLM backend, enhancing LLM performance on Nvidia GPUs.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/nvidia_triton.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1744044446029901943?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83d\udc40 Community Demos\n  </strong>\n  :\n </p>\n <ul>\n  <li>\n   Context-Augmented Agent for Food Delivery: A full-stack application guide by lucastonon for creating an RAG agent. This tool performs in-browser tasks like opening restaurant pages and adding food to carts, purely via voice commands, integrating with Llama Index, Pinecone, OpenAI\u2019s Whisper, LLMs, Function Calling, vue.js, and FastAPI.\n   <a href=\"https://github.com/lucastononro/llm-food-delivery\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Github Repo\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1741987664272986534?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <a href=\"https://medium.com/@dheymann314/ai-infused-optimization-in-the-wild-developing-a-companion-planting-app-357e5da29d10\" rel=\"noopener\">\n    GRDN.AI\n   </a>\n   : A fascinating side project from Danielle Heymann, using a genetic algorithm and LLM to optimize plant placement based on compatibility. This project harnesses local models from HuggingFace, accessed through LlamaIndex for the LLM part, combining traditional mathematical strategies with LLMs.\n   <a href=\"https://medium.com/@dheymann314/ai-infused-optimization-in-the-wild-developing-a-companion-planting-app-357e5da29d10\" rel=\"noopener\">\n    Blogpost\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1742703399081271555?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <a href=\"https://www.activeloop.ai/resources/use-llama-index-to-build-an-ai-shopping-assistant-with-rag-and-agents/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Build an AI Shopping Assistant with RAG and Agents\n   </a>\n   : This assistant can analyze a picture of an item and suggest weather-appropriate accessories. The work by D. Kiedanski and Lucas Micol from Tryolabs explains how to transform APIs into problem-solving tools for a LlamaIndex agent.\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   to Advanced RAG: Our comprehensive cheat sheet offers insights into improving RAG with techniques like optimized retrieval, effective document use in generation, and interleaving generation with retrieval. Ideal for both new and seasoned LLM users, it\u2019s a must-have resource, complete with LlamaIndex links.\n  </li>\n  <li>\n   <a href=\"https://github.com/NVIDIA/GenerativeAIExamples/blob/main/notebooks/04_llamaindex_hier_node_parser.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   to building advanced RAG CHATBOT with NVIDIA\u2019S TensorRT-LLM: This chatbot is designed to maintain contiguous document or code blocks, avoiding awkward chunking. It features a stack combining Llama Index\u2019s auto-merging retriever with NVIDIA\u2019s TensorRT-LLM and a custom postprocessor, optimized for RAG using open-source models.\n  </li>\n </ul>\n <p>\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </p>\n <ul>\n  <li>\n   BentoML\n   <a href=\"https://www.bentoml.com/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Building An Intelligent Query-Response System with LlamaIndex and OpenLLM.\n  </li>\n  <li>\n   <a href=\"https://www.linkedin.com/in/akashmathur22/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Akash Mathur\n   </a>\n   <a href=\"https://akash-mathur.medium.com/advanced-rag-optimizing-retrieval-with-additional-context-metadata-using-llamaindex-aeaa32d7aa2f\" rel=\"noopener\">\n    tutorial\n   </a>\n   on Advanced RAG: Optimizing Retrieval with Additional Context &amp; MetaData using LlamaIndex.\n  </li>\n </ul>\n <p>\n  \ud83c\udfa5\n  <strong>\n   Webinars:\n  </strong>\n </p>\n <ul>\n  <li>\n   Weights &amp; Biases\n   <a href=\"https://www.youtube.com/watch?v=xejCaLsYzV4\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    podcast\n   </a>\n   with Jerry Liu on Revolutionizing AI Data Management.\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83c\udfe2 Calling all enterprises:\n  </strong>\n </p>\n <p>\n  Are you building with LlamaIndex? We are working hard to make LlamaIndex, even more, Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested?\n  <a href=\"https://docs.google.com/forms/d/e/1FAIpQLScBNdM2a_fn8UZOKmFQt6lBsrd1o6FflvsdPH-Pn3JkdlN_Rg/viewform\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Get in touch.\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 13709, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b293f6a2-825e-4e15-9fa8-bacd8a1a6255": {"__data__": {"id_": "b293f6a2-825e-4e15-9fa8-bacd8a1a6255", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-16-752195bed96d.html", "file_name": "llamaindex-newsletter-2024-01-16-752195bed96d.html", "file_type": "text/html", "file_size": 9794, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-16-752195bed96d.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-16-752195bed96d.html", "file_name": "llamaindex-newsletter-2024-01-16-752195bed96d.html", "file_type": "text/html", "file_size": 9794, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "23bcd4934b03bd750402e272e142b76dc01dd7fed20edd795547a6e8f4ef35fc", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Hello LlamaIndex Enthusiasts \ud83e\udd99,\n </p>\n <p>\n  Get ready for an exciting week at LlamaIndex, teeming with dynamic community contributions and insightful learning resources. Dive into our range of new features, tutorials, guides, and events, all designed to enhance your LlamaIndex journey.\n </p>\n <p>\n  We\u2019re excited to announce our\n  <a href=\"https://rag-a-thon.devpost.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   very first in-person hackathon\n  </a>\n  , scheduled for February 2nd-4th. Join us to connect with fellow RAG enthusiasts and compete for prizes totaling over $4,000!\n </p>\n <p>\n  If you\u2019ve been working on a fascinating project, penned an insightful article, or produced an engaging video, we\u2019re eager to see it! Share your contributions with us at\n  <a href=\"mailto:news@llamaindex.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   news@llamaindex.ai\n  </a>\n  . Don\u2019t forget to subscribe to our newsletter on our\n  <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   website\n  </a>\n  to receive all the latest updates directly in your inbox.\n </p>\n <p>\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </p>\n <ol>\n  <li>\n   <strong>\n    Chain-of-Table:\n   </strong>\n   Step-by-step table reasoning and operations for enhanced LLM tabular data understanding.\n   <a href=\"https://llamahub.ai/l/llama_packs-tables-chain_of_table?from=llama_packs\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaPack\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1746217167706894467?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    LLM Self-Consistency:\n   </strong>\n   Merges textual and symbolic reasoning with majority voting for precise answers.\n   <a href=\"https://t.co/pGcRG4ieD4\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaPack\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1746937012798800272?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Semantic Text Splitting in RAG:\n   </strong>\n   Greg Kamradt\u2019s embedding similarity method for efficient document splitting.\n   <a href=\"https://llamahub.ai/l/llama_packs-node_parser-semantic_chunking?from=all\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaPack\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1745482959237615847?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Parallel RAG Ingestion:\n   </strong>\n   Up to 15x faster document processing in LlamaIndex.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/ingestion/parallel_execution_ingestion_pipeline.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1745849571614539984?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    TogetherAI\u2019s Embeddings Support:\n   </strong>\n   Guide to build retrieval-augmented apps with MistralAI\u2019s 8x7b model and TogetherAI Embeddings.\n   <a href=\"https://www.together.ai/blog/rag-tutorial-llamaindex\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blogpost\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1745551739368222815?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <p>\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </p>\n <ul>\n  <li>\n   We launched Chain-of-Table Framework in LlamaPack for LLM Tabular Data Understanding. This approach enables step-by-step table reasoning and operations like adding columns, row selection, grouping, and sorting, mimicking a data scientist\u2019s method for concise data representation.\n   <a href=\"https://llamahub.ai/l/llama_packs-tables-chain_of_table?from=llama_packs\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaPack\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1746217167706894467?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We launched LLM Self-Consistency Mechanism for Tabular Data in LlamaPack. This method combines textual and symbolic reasoning, utilizing a novel mix self-consistency approach with majority voting to select the best answer.\n   <a href=\"https://t.co/pGcRG4ieD4\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaPack\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1746937012798800272?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have Introduced Semantic Text Splitting in RAG with LlamaPack. Check Greg Kamradt\u2019s method of splitting documents based on embedding similarity between sentences. This auto-tuned threshold approach enhances RAG pipelines, soon to be available in LlamaPack using LlamaIndex abstractions.\n   <a href=\"https://llamahub.ai/l/llama_packs-node_parser-semantic_chunking?from=all\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaPack\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1745482959237615847?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We launched Parallel RAG Ingestion in LlamaIndex for up to 15x Faster Document Processing.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/ingestion/parallel_execution_ingestion_pipeline.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1745849571614539984?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have launched Support for TogetherAI\u2019s Embeddings Endpoint. Check the blog for a step-by-step guide on creating a retrieval-augmented generation app with MistralAI\u2019s 8x7b model and TogetherAI Embeddings.\n   <a href=\"https://www.together.ai/blog/rag-tutorial-llamaindex\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blogpost\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1745551739368222815?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We integrated AgentSearch-v1 as a data loader and Retriever in LlamaHub, offering a robust alternative for internet content search/retrieval without relying on Bing/Google APIs.\n   <a href=\"https://llamahub.ai/l/llama_packs-agent_search_retriever?from=llama_packs\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaPack\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1745903362128617842?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   Raduaschl introduced Ensembling and Fusion in Advanced RAG with LlamaPack. Learn to build an ensembling + fusion pipeline in about 30 lines of code using QueryPipeline syntax, featuring full async support.\n   <a href=\"https://t.co/iD1v5FuIdy\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaPack\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1745228497646449021?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=4U8viyAQkJ8\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   to Building Full-Stack RAG Applications with LlamaIndex and Azure Cosmos DB.\n  </li>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/retrievers/vectara_auto_retriever.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   showing to combine auto-retrieval for semi-structured retrieval with metadata with MMR to enforce diversity in results.\n  </li>\n  <li>\n   <a href=\"https://github.com/mickymultani/RAG-with-Cross-Encoder-Reranker\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   by\n   <a href=\"https://twitter.com/MountainMicky\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    MountainMicky\n   </a>\n   to understanding the Importance of Reranking in Advanced RAG Pipelines.\n  </li>\n </ul>\n <p>\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://twitter.com/andrejusb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Andrej Baranovskij\n   </a>\n   <a href=\"https://www.youtube.com/watch?v=VKeYaIEk82s\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Transforming Invoice Data into JSON with LlamaIndex and Pydantic.\n  </li>\n  <li>\n   NVIDIA\n   <a href=\"https://developer.nvidia.com/blog/supercharging-llm-applications-on-windows-pcs-with-nvidia-rtx-systems/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Building AI apps with local LLMs running on Windows with LlamaIndex and NVIDIA\n  </li>\n  <li>\n   <a href=\"https://harshadsuryawanshi.medium.com/\" rel=\"noopener\">\n    Harshad Suryawanshi\n   </a>\n   <a href=\"/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5.\n  </li>\n </ul>\n <p>\n  \ud83c\udfa5 Events:\n </p>\n <ul>\n  <li>\n   Ravi Theja gave talk on Building Multi-Tenancy RAG System with LlamaIndex and Qdrant at FOSS United, Bangalore, India.\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83c\udfe2 Calling all enterprises:\n  </strong>\n </p>\n <p>\n  Are you building with LlamaIndex? We are working hard to make LlamaIndex, even more, Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested?\n  <a href=\"https://docs.google.com/forms/d/e/1FAIpQLScBNdM2a_fn8UZOKmFQt6lBsrd1o6FflvsdPH-Pn3JkdlN_Rg/viewform\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Get in touch.\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 9748, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0215726-4154-4f22-abca-96c226bdf066": {"__data__": {"id_": "d0215726-4154-4f22-abca-96c226bdf066", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-23-11ee2c211bab.html", "file_name": "llamaindex-newsletter-2024-01-23-11ee2c211bab.html", "file_type": "text/html", "file_size": 10910, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-23-11ee2c211bab.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-23-11ee2c211bab.html", "file_name": "llamaindex-newsletter-2024-01-23-11ee2c211bab.html", "file_type": "text/html", "file_size": 10910, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "0d2190aebff2fb35c9ede6e93ef7f06a74535500b90b05b6db3080a353f92336", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Hello LlamaIndex Explorers \ud83e\udd99,\n </p>\n <p>\n  Another exciting week at LlamaIndex, filled with vibrant community contributions and educational resources. Explore our array of new features, tutorials, guides, and demos, all tailored to enrich your experience with LlamaIndex.\n </p>\n <p>\n  Before delving into the updates, we have two significant announcements:\n </p>\n <ul>\n  <li>\n   We\u2019re thrilled to host our\n   <a href=\"https://rag-a-thon.devpost.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    first in-person hackathon\n   </a>\n   , set for February 2nd-4th. This is a fantastic opportunity to meet fellow RAG enthusiasts, collaborate, and compete for prizes totaling over $8000!\n  </li>\n  <li>\n   Don\u2019t miss our\n   <a href=\"https://lu.ma/lf9iroox\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    webinar\n   </a>\n   featuring Sehoon Kim and Amir Gholami, scheduled for Thursday at 9 am PT. They will introduce LLMCompiler, an agent compiler for parallel multi-function planning and execution.\n  </li>\n </ul>\n <p>\n  We\u2019re always excited to see your projects, articles, or videos. If you\u2019ve created something you\u2019re proud of, share it with us at\n  <a href=\"mailto:news@llamaindex.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   news@llamaindex.ai\n  </a>\n  . Also, remember to subscribe to our newsletter on our\n  <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   website\n  </a>\n  to get all the latest news straight to your inbox.\n </p>\n <p>\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </p>\n <ol>\n  <li>\n   <strong>\n    RankGPT:\n   </strong>\n   Introducing RankGPT leveraging GPT-3.5 and GPT-4 for top-tier document ranking and a novel sliding window technique for extensive context management.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/node_postprocessor/rankGPT.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1747681530347216995?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Composable Retrievers:\n   </strong>\n   An interface centralizing advanced retrieval and RAG techniques, enhancing RAG setups with IndexNodes for linking diverse retrievers and pipelines.\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/retrievers/composable_retrievers.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1748019272679649386?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Advanced QA over Tabular Data Tutorial:\n   </strong>\n   A detailed guide to crafting query pipelines over tabular data, featuring Pandas, SQL, and Query Pipelines for an integrated few-shot, LLM, and custom function setup.\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/pipeline/query_pipeline_sql.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Text-to-SQL\n   </a>\n   ,\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/pipeline/query_pipeline_pandas.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Text-to-Pandas\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Long-Context Embedding Models:\n   </strong>\n   Explore models like M2-BERT-80M-32k-retrieval tackling the embedding chunking problem in RAG, with a focus on hybrid retrieval methods and hierarchical retrieval approaches.\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/retrievers/multi_doc_together_hybrid.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   .\n  </li>\n </ol>\n <p>\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </p>\n <ul>\n  <li>\n   We have introduced RankGPT in our advanced module that utilizes GPT-3.5 and GPT-4 for efficient document ranking, featuring a unique sliding window strategy for handling large contexts.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/node_postprocessor/rankGPT.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1747681530347216995?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have launched Composable Retrievers which centralizes various advanced retrieval and RAG techniques into a versatile interface. It simplifies creating complex RAG setups by allowing you to define IndexNodes to link different retrievers or RAG pipelines.\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/retrievers/composable_retrievers.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1748019272679649386?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   Anoop Sharma has introduced LlamaPack for Multi-Stock Ticker Analysis for analyzing various stock tickers with a single code line, enabling easy specification of tickers, time frames, and structured queries.\n   <a href=\"https://llamahub.ai/l/llama_packs-stock_market_data_query_engine?from=llama_packs\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaPack\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1748422554841448600?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex.TS (LITS) supports streaming on all endpoints.\n   <a href=\"https://x.com/llama_index/status/1747746779058290800?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We announced a new integration with Tonic Validate to allow simple access to LLM-powered evaluations.\n   <a href=\"https://www.tonic.ai/blog/tonic-ai-and-llamaindex-join-forces-to-help-developers-build-more-performant-rag-systems\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog post\n   </a>\n  </li>\n </ul>\n <p>\n  \ud83c\udfa5\n  <strong>\n   Demo:\n  </strong>\n </p>\n <ul>\n  <li>\n   <strong>\n    RAG-Maestro for ArXiv Research:\n   </strong>\n   Developed by Aymen Kallala, this web app utilizes RAG to efficiently search scientific concepts in ArXiv papers. It extracts keywords using RAKE, queries ArXiv for relevant papers, and offers on-the-fly indexing with in-line citations \u2014 a valuable tool for ML researchers navigating through ArXiv\u2019s extensive library.\n   <a href=\"https://rag-maestro-o2wbip4gla-uc.a.run.app/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Demo\n   </a>\n   ,\n   <a href=\"https://github.com/AymenKallala/RAG_Maestro\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    GitHub Repo\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </p>\n <ul>\n  <li>\n   Guide to Advanced QA over Tabular Data which provides a comprehensive tutorial on creating sophisticated query pipelines over tabular data using Pandas or SQL, constructing a query DAG using our Query Pipelines, integrating few-shot examples, linked prompts, LLMs, custom functions, retrievers, and more.\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/pipeline/query_pipeline_sql.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Text-to-SQL\n   </a>\n   ,\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/pipeline/query_pipeline_pandas.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Text-to-Pandas\n   </a>\n   .\n  </li>\n  <li>\n   <a href=\"https://medium.com/@marco.bertelli/revolutionizing-chatbot-performance-unleashing-three-potent-strategies-for-rag-enhancement-c1188e395d9d\" rel=\"noopener\">\n    Guide\n   </a>\n   to a Five-Part Series on Building a Full-Stack RAG Chatbot by\n   <a href=\"https://medium.com/@marco.bertelli\" rel=\"noopener\">\n    Marco Bertelli\n   </a>\n   , extensive tutorials covering every aspect of creating an RAG chatbot \u2014 from model selection and Flask backend setup to constructing the ChatEngine and optimizing the RAG pipeline.\n  </li>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/retrievers/multi_doc_together_hybrid.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   to Long-Context Embedding Models: The models, like M2-BERT-80M-32k-retrieval, offer a solution to the embedding chunking issue in RAG by grounding retrieval in broader semantic contexts. Learn about hybrid retrieval, combining chunk and document-level similarity, and other approaches like hierarchical retrieval.\n  </li>\n </ul>\n <p>\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://twitter.com/wenqi_glantz\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Wenqi\n   </a>\n   <a href=\"https://towardsdatascience.com/democratizing-llms-4-bit-quantization-for-optimal-llm-inference-be30cf4e0e34\" rel=\"noopener\" target=\"_blank\">\n    tutorial\n   </a>\n   on Democratizing LLMs: 4-bit Quantization for Optimal LLM Inference with LlamaIndex.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/andrejusb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Andrej\n   </a>\n   <a href=\"https://www.youtube.com/watch?v=vntNI33wrcI\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on FastAPI and LlamaIndex RAG: Creating Efficient APIs.\n  </li>\n  <li>\n   <a href=\"https://medium.com/@brezeanu.iulia\" rel=\"noopener\">\n    Lulia Brezeanu\n   </a>\n   <a href=\"https://towardsdatascience.com/advanced-query-transformations-to-improve-rag-11adca9b19d1\" rel=\"noopener\" target=\"_blank\">\n    tutorial\n   </a>\n   on Advanced Query Transformations to Improve RAG.\n  </li>\n  <li>\n   <a href=\"https://akash-mathur.medium.com/\" rel=\"noopener\">\n    Akash Mathur\n   </a>\n   in-depth\n   <a href=\"https://akash-mathur.medium.com/advanced-rag-query-augmentation-for-next-level-search-using-llamaindex-d362fed7ecc3\" rel=\"noopener\">\n    tutorial\n   </a>\n   on Advanced RAG: Query Augmentation for Next-Level Search using LlamaIndex.\n  </li>\n  <li>\n   <a href=\"https://medium.com/@ryanntk\" rel=\"noopener\">\n    Ryan Nguyen\n   </a>\n   <a href=\"https://levelup.gitconnected.com/live-indexing-for-rag-a-guide-for-real-time-indexing-using-llamaindex-and-aws-51353083ace4\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Live Indexing for RAG: A Guide For Real-Time Indexing Using LlamaIndex and AWS.\n  </li>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=TOeAe8KB68E\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Nipuna\n   </a>\n   (Paragon AI) tutorial on Building a Full-Stack Complex PDF AI chatbot with LlamaIndex.\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83c\udfe2 Calling all enterprises:\n  </strong>\n </p>\n <p>\n  Are you building with LlamaIndex? We are working hard to make LlamaIndex, even more, Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested?\n  <a href=\"https://docs.google.com/forms/d/e/1FAIpQLScBNdM2a_fn8UZOKmFQt6lBsrd1o6FflvsdPH-Pn3JkdlN_Rg/viewform\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Get in touch.\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 10870, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "befbcdfe-273d-447f-a8a8-443643f2518e": {"__data__": {"id_": "befbcdfe-273d-447f-a8a8-443643f2518e", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-30-0d01eb0d8cef.html", "file_name": "llamaindex-newsletter-2024-01-30-0d01eb0d8cef.html", "file_type": "text/html", "file_size": 11180, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-30-0d01eb0d8cef.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-30-0d01eb0d8cef.html", "file_name": "llamaindex-newsletter-2024-01-30-0d01eb0d8cef.html", "file_type": "text/html", "file_size": 11180, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "d0767ad763deec7cd75f5c1bda9022e9146e4b4f8137c84723d8dd01dc9766a6", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Hello LlamaIndex Adventurers \ud83e\udd99,\n </p>\n <p>\n  Welcome to another thrilling week at LlamaIndex! It\u2019s brimming with community contributions and a wealth of educational content that will take your LlamaIndex experience to new heights. Dive into our latest features, comprehensive tutorials, insightful guides, and interactive demos, all designed to supercharge your journey with LlamaIndex.\n </p>\n <p>\n  But first, let\u2019s ignite your excitement with a reminder about our upcoming\n  <a href=\"https://rag-a-thon.devpost.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   first-ever in-person hackathon\n  </a>\n  , happening February 2nd-4th. Don\u2019t miss this incredible chance to mingle with fellow RAG aficionados, collaborate on exciting projects, and vie for a share of over $16,000 in prizes!\n </p>\n <p>\n  Your creations inspire us! Whether it\u2019s a project, article, or video that you\u2019re proud of, we\u2019d love to see it. Share your brilliance with us at\n  <a href=\"mailto:news@llamaindex.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   news@llamaindex.ai\n  </a>\n  . And for those who haven\u2019t yet, make sure to subscribe to our newsletter on our\n  <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   website\n  </a>\n  \u2014 it\u2019s your gateway to all the latest and greatest from LlamaIndex, delivered directly to your inbox.\n </p>\n <p>\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </p>\n <ol>\n  <li>\n   <strong>\n    RAG CLI\n   </strong>\n   : Easy-to-use tool for local file indexing and search, with advanced integration and customization features.\n   <a href=\"https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1750950516925079777?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    JSONalyze\n   </strong>\n   : Efficiently summarizes large JSON datasets, transforming them into SQLite for detailed SQL queries.\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/query_engine/JSONalyze_query_engine.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1749541492191039873?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    OpenAI Embeddings\n   </strong>\n   : We now support the latest OpenAI\n   <code class=\"cw oy oz pa pb b\">\n    <strong>\n     text-embedding-3-small\n    </strong>\n   </code>\n   <strong>\n    and\n   </strong>\n   <code class=\"cw oy oz pa pb b\">\n    <strong>\n     text-embedding-3-large\n    </strong>\n   </code>\n   embeddings for improved accuracy and cost-effectiveness in data retrieval.\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/embeddings/OpenAI.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1750640685894783068?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    ReAct Agent\n   </strong>\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/agent/agent_runner/query_pipeline_agent.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     Guide\n    </strong>\n   </a>\n   : From scratch guide for building ReAct agents, covering all key aspects from setup to memory management.\n  </li>\n  <li>\n   <strong>\n    Slack Bot\n   </strong>\n   : Step-by-step\n   <a href=\"/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    guide\n   </a>\n   for developing a learning Slack bot, integrated with advanced data engines and deployment tools.\n  </li>\n </ol>\n <p>\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </p>\n <ul>\n  <li>\n   We have launched RAG CLI: A straightforward command-line tool for indexing and searching any local file, featuring integration with IngestionPipeline, QueryPipeline, and ChromaDB, with support for local models and customizable logic.\n   <a href=\"https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1750950516925079777?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have introduced JSONalyze, a query engine that swiftly summarizes large JSON datasets. It transforms JSON data into an SQLite table, enabling precise SQL queries for efficient data analysis, combining LlamaIndex\u2019s capabilities with text-to-SQL technology.\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/query_engine/JSONalyze_query_engine.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1749541492191039873?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have launched day 0 support for OpenAI\u2019s latest embedding models featuring cost-effective\n   <code class=\"cw oy oz pa pb b\">\n    <strong>\n     text-embedding-3-small\n    </strong>\n   </code>\n   and high-performance\n   <code class=\"cw oy oz pa pb b\">\n    <strong>\n     text-embedding-3-large\n    </strong>\n   </code>\n   , both with customizable dimensions for enhanced retrieval accuracy in Python and TypeScript versions of LlamaIndex.\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/embeddings/OpenAI.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1750640685894783068?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have launched Infer-Retrieve-Rerank as a LlamaPack, a technique developed by Karel Doostrlnck, as a simple yet effective LLM-based approach for tackling complex classification challenges with numerous categories, applicable in areas like medical diagnosis and job skill assessment.\n   <a href=\"https://llamahub.ai/l/llama_packs-research-infer_retrieve_rerank?from=all\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaPack\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1752008109835559123?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have launched LlamaPack with Vanna AI: An advanced text-to-SQL tool using RAG for storing, indexing, and generating SQL queries.\n   <a href=\"https://llamahub.ai/l/llama_packs-vanna?from=all\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaPack\n   </a>\n   .\n  </li>\n  <li>\n   We have integrated with Zilliz Cloud Pipeline in partnership with Zilliz Universe. This fully managed, scalable retrieval service supports multi-tenancy.\n   <a href=\"/building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1750621271250096558?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have partnered with Exa which created an advanced RAG-powered web search, designed for LLMs and now integrated with Llama Index agents, enhancing workflow automation and data source combination.\n   <a href=\"https://github.com/run-llama/llama-hub/blob/main/llama_hub/tools/notebooks/exa.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1751011851952152710?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   We have integrated with Neutrino, offering GPT-4 level performance at significantly reduced costs by smartly allocating queries to the most suitable model from a diverse range.\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/llm/neutrino.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1749504764172493161?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Twitter\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/agent/agent_runner/query_pipeline_agent.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   to Building a ReAct Agent from Scratch and cookbook detailing the essential components for creating your agents, including reasoning prompts, output parsing, tool selection, and memory management.\n  </li>\n  <li>\n   <a href=\"/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   to Building Slack Bot: Create and deploy an intelligent Slack bot that learns from conversations and accurately answers organizational queries, featuring integration with Qdrant Engine and Render.\n  </li>\n </ul>\n <p>\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://medium.com/@marco.bertelli\" rel=\"noopener\">\n    Marco Bertelli\n   </a>\n   <a href=\"https://medium.com/@marco.bertelli/empowering-your-chatbot-unveiling-dynamic-knowledge-sources-with-advanced-integration-e8353e85099c\" rel=\"noopener\">\n    tutorial\n   </a>\n   on Empowering Your Chatbot: Unveiling Dynamic Knowledge Sources with Advanced Integration.\n  </li>\n  <li>\n   <a href=\"/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tonic Validate\n   </a>\n   tutorial on Implementing integration tests for LlamaIndex.\n  </li>\n  <li>\n   <a href=\"https://chiajy.medium.com/\" rel=\"noopener\">\n    Chia Jeng Yang\n   </a>\n   <a href=\"https://medium.com/enterprise-rag/injecting-knowledge-graphs-in-different-rag-stages-a3cd1221f57b\" rel=\"noopener\">\n    tutorial\n   </a>\n   on Injecting Knowledge Graphs in different RAG stages.\n  </li>\n  <li>\n   <a href=\"https://medium.com/@wenqiglantz\" rel=\"noopener\">\n    Wenqi Glantz\n   </a>\n   <a href=\"https://towardsdatascience.com/jump-start-your-rag-pipelines-with-advanced-retrieval-llamapacks-and-benchmark-with-lighthouz-ai-80a09b7c7d9d\" rel=\"noopener\" target=\"_blank\">\n    tutorial\n   </a>\n   on Jump-start Your RAG Pipelines with Advanced Retrieval LlamaPacks and Benchmark with Lighthouz AI.\n  </li>\n </ul>\n <p>\n  \ud83c\udfa5\n  <strong>\n   Webinar\n  </strong>\n </p>\n <ul>\n  <li>\n   LlamaIndex\n   <a href=\"https://www.youtube.com/watch?v=aoLtTIYAafY\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Webinar\n   </a>\n   on Efficient Parallel Function Calling Agents with LLMCompiler with Sehoon Kim and Amir Gholami.\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83c\udfe2 Calling all enterprises:\n  </strong>\n </p>\n <p>\n  Are you building with LlamaIndex? We are working hard to make LlamaIndex, even more, Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested?\n  <a href=\"https://docs.google.com/forms/d/e/1FAIpQLScBNdM2a_fn8UZOKmFQt6lBsrd1o6FflvsdPH-Pn3JkdlN_Rg/viewform\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Get in touch.\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 11134, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "651f6397-c19d-4a2f-9fdb-2a657699aa69": {"__data__": {"id_": "651f6397-c19d-4a2f-9fdb-2a657699aa69", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-06-9a303130ad9f.html", "file_name": "llamaindex-newsletter-2024-02-06-9a303130ad9f.html", "file_type": "text/html", "file_size": 9110, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-06-9a303130ad9f.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-06-9a303130ad9f.html", "file_name": "llamaindex-newsletter-2024-02-06-9a303130ad9f.html", "file_type": "text/html", "file_size": 9110, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "03742682a85e544071a8ff4e273cf3fe6d3123df0a7509388f58d5bc830ad21d", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Hello, LlamaIndex Explorers \ud83e\udd99,\n </p>\n <p>\n  Step into a week full of exciting updates at LlamaIndex! Our community\u2019s vibrant contributions and extensive educational resources are here to amplify your LlamaIndex exploration.\n </p>\n <p>\n  Before diving into the updates, we have an exciting announcement: We\u2019ve launched a\n  <a href=\"https://replit.com/bounties?search=llamaindex\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   $2,000 bounty program with Replit\n  </a>\n  . This initiative invites open-source contributors to create projects and templates focused on advanced RAG with LlamaIndex, from building RAG across thousands of documents to implementing cutting-edge RAG research and crafting advanced templates.\n </p>\n <p>\n  We\u2019re inspired by your creativity! If you have a project, article, or video you\u2019re excited about, we\u2019re eager to see it. Send your amazing work to\n  <a href=\"https://www.notion.so/LlamaIndex-Newsletter-2024-02-06-86c1d1db060249f2ab8032357f3df323?pvs=21\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   news@llamaindex.ai\n  </a>\n  . If you haven\u2019t subscribed to our newsletter yet, don\u2019t miss out. Visit our\n  <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   website\n  </a>\n  and subscribe today to get all the newest updates from LlamaIndex straight to your inbox.\n </p>\n <p>\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </p>\n <ol>\n  <li>\n   <strong>\n    Ollama Multimodal Integration Launch:\n   </strong>\n   Introduced day-1 integration with Ollama Multi-Modal for developing local multimodal applications, including image extraction, multimodal RAG, and captioning.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/ollama_multi_modal.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook,\n   </a>\n   <a href=\"https://x.com/llama_index/status/1753875735776018786?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   <code class=\"cw ou ov ow ox b\">\n    create-llama\n   </code>\n   <strong>\n    Enhanced RAG\n   </strong>\n   : Updated create-llama for improved website content crawling and the creation of comprehensive RAG applications.\n   <a href=\"https://x.com/MarcusSchiesser/status/1753373534708175263?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Nomic Embedding\n   </strong>\n   :\n   <a href=\"/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   to Building a Fully Open Source Retriever with Nomic Embed and LlamaIndex.\n  </li>\n </ol>\n <p>\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </p>\n <ul>\n  <li>\n   We introduced day-1 integration with the Ollama Multi-Modal release enabling the creation of local multimodal applications on MacBook, including structured image extraction, multimodal RAG, and image captioning.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/ollama_multi_modal.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook,\n   </a>\n   <a href=\"https://x.com/llama_index/status/1753875735776018786?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   We have updated create-llama on crawling a website\u2019s content, and create a full-stack RAG application based on the data.\n   <a href=\"https://x.com/MarcusSchiesser/status/1753373534708175263?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   to Building a Fully Open Source Retriever with Nomic Embed and LlamaIndex.\n  </li>\n </ul>\n <p>\n  \ud83c\udfa5 Demo:\n </p>\n <ul>\n  <li>\n   LlamaBot:\n   <a href=\"https://twitter.com/clusteredbytes\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Rohan\n   </a>\n   developed an open-source Discord bot that listens to, remembers, and answers questions across servers, was inspired by a similar bot for Slack and developed using LlamaIndex, Gemini Pro, and Qdrant Engine.\n   <a href=\"https://github.com/rsrohan99/llamabot\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    GitHub Repository\n   </a>\n   ,\n   <a href=\"https://x.com/clusteredbytes/status/1754220009885163957?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://medium.com/@wenqiglantz\" rel=\"noopener\">\n    Wenqi Glantz\n   </a>\n   <a href=\"https://towardsdatascience.com/jump-start-your-rag-pipelines-with-advanced-retrieval-llamapacks-and-benchmark-with-lighthouz-ai-80a09b7c7d9d\" rel=\"noopener\" target=\"_blank\">\n    tutorial\n   </a>\n   on Jump-start Your RAG Pipelines with Advanced Retrieval LlamaPacks and Benchmark with Lighthouz AI.\n  </li>\n  <li>\n   <a href=\"https://www.linkedin.com/in/ravidesetty/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Ravi Theja\n   </a>\n   <a href=\"/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG.\n  </li>\n  <li>\n   <a href=\"https://medium.com/@wenqiglantz\" rel=\"noopener\">\n    Wenqi Glantz\n   </a>\n   <a href=\"https://towardsdatascience.com/12-rag-pain-points-and-proposed-solutions-43709939a28c\" rel=\"noopener\" target=\"_blank\">\n    tutorial\n   </a>\n   on 12 RAG Pain Points and Proposed Solutions.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/CobusGreylingZA\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Cobus Reyling\n   </a>\n   <a href=\"/agentic-rag-with-llamaindex-2721b8a49ff6\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Agentic RAG With LlamaIndex.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/ChrisSamiullah\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    ChristopherGS\n   </a>\n   <a href=\"https://christophergs.com/blog/ai-engineering-retrieval-augmented-generation-rag-llama-index\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Retrieval Augmented Generation (RAG) with Llama Index and Open-Source Models.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/_nerdai_\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Andrei\n   </a>\n   workshop tutorial on Evaluation of Multimodal RAG Systems using the LlamaIndex.\n  </li>\n  <li>\n   <a href=\"https://docs.pinecone.io/docs/llamaindex\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tutorial\n   </a>\n   on Building RAG application with Pinecone and LlamaIndex.\n  </li>\n  <li>\n   <a href=\"https://www.linkedin.com/in/sudalairajkumar/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Sudalai Rajkumar\n   </a>\n   <a href=\"https://srk.ai/blog/004-ai-llm-retrieval-eval-llamaindex\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on RAG \u2014 Encoder and Reranker evaluation.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/HarshadSurya1c\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Harshad Suryawanshi\n   </a>\n   <a href=\"/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on RAGArch: Building a No-Code RAG Pipeline Configuration &amp; One-Click RAG Code Generation Tool Powered by LlamaIndex.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/Otmane404\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Otmane Boughaba\u2019s\n   </a>\n   <a href=\"https://otmaneboughaba.com/posts/local-rag-api/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Building a Local RAG API with LlamaIndex, Qdrant, Ollama, and FastAPI.\n  </li>\n  <li>\n   <a href=\"https://medium.com/@brezeanu.iulia\" rel=\"noopener\">\n    Iulia Brezeanu\n   </a>\n   <a href=\"https://towardsdatascience.com/how-to-find-the-best-multilingual-embedding-model-for-your-rag-40325c308ebb\" rel=\"noopener\" target=\"_blank\">\n    tutorial\n   </a>\n   on How to Find the Best Multilingual Embedding Model for Your RAG.\n  </li>\n </ul>\n <p>\n  \ud83c\udfa5 Events\n </p>\n <ul>\n  <li>\n   <a href=\"https://twitter.com/jerryjliu0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Jerry Liu\n   </a>\n   <a href=\"https://t.co/hrUMF8bq8Q\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    keynote\n   </a>\n   on Beyond Naive Rag: Adding Agentic Layers.\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83c\udfe2 Calling all enterprises:\n  </strong>\n </p>\n <p>\n  Are you building with LlamaIndex? We are working hard to make LlamaIndex, even more, Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested?\n  <a href=\"https://docs.google.com/forms/d/e/1FAIpQLScBNdM2a_fn8UZOKmFQt6lBsrd1o6FflvsdPH-Pn3JkdlN_Rg/viewform\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Get in touch.\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 9063, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c292cf04-4202-46ef-9615-ebd8b4f691e1": {"__data__": {"id_": "c292cf04-4202-46ef-9615-ebd8b4f691e1", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4.html", "file_name": "llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4.html", "file_type": "text/html", "file_size": 10623, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4.html", "file_name": "llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4.html", "file_type": "text/html", "file_size": 10623, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "4daa1a51a94f739d9c8945017236f7481f76688a3ba3a0af21e2ed4bea15a2bd", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Hi there, LlamaIndex Enthusiasts \ud83e\udd99,\n </p>\n <p>\n  Today marks a milestone for the LlamaIndex ecosystem with the\n  <a href=\"/introducing-llamacloud-and-llamaparse-af8cedf9006b\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   introduction of LlamaCloud\n  </a>\n  , a next-generation suite of managed parsing, ingestion, and retrieval services tailored for\n  <strong>\n   production-grade\n  </strong>\n  <strong>\n   context augmentation\n  </strong>\n  in your LLM and RAG applications.\n </p>\n <p>\n  As an enterprise AI engineer using LlamaCloud, you can concentrate on crafting the business logic, leaving the heavy lifting of data management to us. Process vast amounts of production data effortlessly, enhancing response quality instantly. LlamaCloud debuts with:\n </p>\n <ul>\n  <li>\n   <strong>\n    LlamaParse:\n   </strong>\n   A specialized parsing service for complex documents, including tables and figures, seamlessly integrated with LlamaIndex for handling semi-structured documents. This enables answering intricate queries previously out of reach.\n  </li>\n  <li>\n   <strong>\n    Managed Ingestion and Retrieval API:\n   </strong>\n   Simplify data loading, processing, and storage for your RAG applications, supported by over 150 data sources via\n   <a href=\"https://llamahub.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaHub\n   </a>\n   , including LlamaParse, and more than 40 data storage solutions.\n  </li>\n </ul>\n <p>\n  LlamaParse is now in public preview, with a current focus on PDFs and a usage cap for public users;\n  <a href=\"https://llamaindex.ai/contact\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   contact us\n  </a>\n  for commercial terms. The managed API is in private preview, and available to a select group of enterprise partners. Interested?\n  <a href=\"https://llamaindex.ai/contact\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Reach out\n  </a>\n  for more details.\n </p>\n <p>\n  Your inventive spirit is our driving force! We look forward to discovering the projects, articles, or videos that excite you. Send your outstanding contributions to\n  <a href=\"https://www.notion.so/15c61b205d114f0ebbe0be547a645628?pvs=21\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   news@llamaindex.ai\n  </a>\n  . If you haven\u2019t yet, join our newsletter via our\n  <a href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   website\n  </a>\n  to get all the newest LlamaIndex news directly in your inbox.\n </p>\n <p>\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </p>\n <ol>\n  <li>\n   <strong>\n    Corrective RAG LlamaPack:\n   </strong>\n   We have launched LlamaPack with CRAG insights, refining information retrieval for enhanced accuracy and relevance.\n   <a href=\"https://x.com/llama_index/status/1758530939276378255?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   ,\n   <a href=\"https://llamahub.ai/l/llama-packs/llama-index-packs-corrective-rag?from=llama-packs\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaPack\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    SELF-DISCOVER LlamaPack\n   </strong>\n   : We have launched SELF-DISCOVER paper implementation as LlamaPack, leveraging meta-reasoning in LLMs for adaptive, complex problem-solving.\n   <a href=\"https://x.com/llama_index/status/1759620529982755324?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   ,\n   <a href=\"https://llamahub.ai/l/llama-packs/llama-index-packs-self-discover?from=\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaPack\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    RAG Production\n   </strong>\n   <a href=\"https://www.youtube.com/watch?v=ZP1F9z-S7T0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     Guide\n    </strong>\n   </a>\n   <strong>\n    :\n   </strong>\n   A comprehensive guide to production-ready RAG, featuring insights and strategies from Sisil Mehta at JasperAI.\n  </li>\n </ol>\n <p>\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </p>\n <ul>\n  <li>\n   We have integrated insights from the Corrective Retrieval Augmented Generation (CRAG) paper as LlamaPack, enhancing our system\u2019s ability to evaluate and refine retrieved information for more accurate and relevant responses.\n   <a href=\"https://x.com/llama_index/status/1758530939276378255?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   ,\n   <a href=\"https://llamahub.ai/l/llama-packs/llama-index-packs-corrective-rag?from=llama-packs\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaPack\n   </a>\n   .\n  </li>\n  <li>\n   We have integrated SELF-DISCOVER a novel approach as LlamaPack to empower LLMs with meta-reasoning, allowing them to self-discover and adapt the most suitable reasoning modules from a selection, enabling more versatile and complex problem-solving capabilities.\n   <a href=\"https://x.com/llama_index/status/1759620529982755324?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   ,\n   <a href=\"https://llamahub.ai/l/llama-packs/llama-index-packs-self-discover?from=\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaPack\n   </a>\n   .\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83c\udfa5 Demos:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://x.com/ycombinator/status/1757434457382916571?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    DanswerAI\n   </a>\n   , an out-of-the-box ChatGPT integration for enterprise knowledge, enhances efficiency across sales, IT, engineering, and customer support teams by connecting to common workplace tools like GDrive, Slack, and Jira, powered by Llama Index.\n  </li>\n  <li>\n   <a href=\"https://x.com/llama_index/status/1758207209140601315?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    GenAI for ADU Planning:\n   </a>\n   A comprehensive\n   <a href=\"https://x.com/llama_index/status/1758207209140601315?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    app\n   </a>\n   by\n   <a href=\"https://twitter.com/rujun_gao\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     Rujun Gao\n    </strong>\n   </a>\n   that navigates local ADU regulations, analyzes buildable space via satellite imagery, offers floor plan suggestions, and connects users to local contractors, showcasing the power of multi-modal AI automation in enhancing productivity\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=ZP1F9z-S7T0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   to building production-ready RAG covering practical tips and tricks inspired by\n   <a href=\"https://twitter.com/sisilmehta\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Sisil Mehta\n   </a>\n   from JasperAI.\n  </li>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/embeddings/nomic.html#\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   to Nomic-embed-text-v1.5 with LlamaIndex: Get embeddings of any dimension from 64 to 768, inspired by Matryoshka Representation Learning.\n  </li>\n  <li>\n   <a href=\"https://github.com/ohdearquant/lionagi/blob/main/notebooks/RAG/AutoResearch2_ReAct.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   to creating a RAG-powered research agent, notebook on building a workflow for scientific investigation, leveraging ArXiv, Wikipedia, textbooks, and more, with capabilities for fetching abstracts, generating ideas, and comprehensive information lookup, powered by Llama Index and LionAGI.\n  </li>\n </ul>\n <p>\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://twitter.com/jerryjliu0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Jerry Liu\n   </a>\n   <a href=\"https://www.youtube.com/watch?v=swJsw9Jvgoc\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Introduction to LlamaIndex v0.10.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/jerryjliu0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Jerry Li\n   </a>\n   <a href=\"https://www.youtube.com/watch?v=T0bgevj0vto\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Building Agents from scratch using Query Pipelines.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/ravithejads\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Ravi Theja\n   </a>\n   <a href=\"https://www.youtube.com/watch?v=tBA6yRmMF74\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   video on Building Multi-Modal applications with Ollama and LlamaIndex.\n  </li>\n  <li>\n   Brett Young\n   <a href=\"https://wandb.ai/byyoung3/ml-news/reports/Building-a-RAG-Based-Digital-Restaurant-Menu-with-LlamaIndex-and-W-B-Weave--Vmlldzo2NjE5Njkw\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Building a RAG-Based Digital Restaurant Menu with LlamaIndex and W&amp;B Weave.\n  </li>\n  <li>\n   <a href=\"https://www.linkedin.com/in/raghav-dixit/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Raghav\n   </a>\n   <a href=\"/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on MultiModal RAG for Advanced Video Processing with LlamaIndex &amp; LanceDB.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/TechWithTimm\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tech With Tim\n   </a>\n   <a href=\"https://www.youtube.com/watch?v=ul0QsodYct4\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on querying unstructured data, analyzing tabular data with Pandas, and actioning results in a concise, step-by-step approach.\n  </li>\n  <li>\n   <a href=\"https://medium.com/@florian_algo\" rel=\"noopener\">\n    Florian June\n   </a>\n   \u2019s\n   <a href=\"https://ai.plainenglish.io/advanced-rag-03-using-ragas-llamaindex-for-rag-evaluation-84756b82dca7\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on using RAGAs and LlamaIndex for RAG evaluation.\n  </li>\n  <li>\n   <a href=\"https://dgallitelli95.medium.com/\" rel=\"noopener\">\n    Davide Gallitelli\n   </a>\n   \u2019s\n   <a href=\"https://dgallitelli95.medium.com/deploying-an-huggingface-embedding-model-to-amazon-sagemaker-and-consuming-it-with-llama-index-4d4f6dcd2fbc\" rel=\"noopener\">\n    tutorial\n   </a>\n   on Deploying a HuggingFace embedding model to Amazon SageMaker and consuming it with Llama-Index.\n  </li>\n </ul>\n <p>\n  \ud83c\udfa5\n  <strong>\n   Webinar:\n  </strong>\n </p>\n <ul>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=k5Txq5C_AWA\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Webinar\n   </a>\n   with Henry Heng, co-founder of Flowise on building advanced no-code RAG apps over your data.\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 10591, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b59e0f9-8a67-446d-a31a-47f64aabb86c": {"__data__": {"id_": "8b59e0f9-8a67-446d-a31a-47f64aabb86c", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-27-4b9102a0f824.html", "file_name": "llamaindex-newsletter-2024-02-27-4b9102a0f824.html", "file_type": "text/html", "file_size": 12685, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-27-4b9102a0f824.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-27-4b9102a0f824.html", "file_name": "llamaindex-newsletter-2024-02-27-4b9102a0f824.html", "file_type": "text/html", "file_size": 12685, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "cbc12f05386cd46d1f8e7ecf98d9db9e8ec13b010a289e68700d4abcca085a1e", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Yo, LlamaIndex Fans \ud83e\udd99,\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Dive into a week brimming with thrilling developments at LlamaIndex! The dynamic input from our community and our rich selection of learning materials are all set to enhance your journey with LlamaIndex.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Last week, the LlamaIndex ecosystem took a significant leap forward with the launch of LlamaCloud, a suite of advanced services designed for\n  <strong>\n   production-level\n  </strong>\n  <strong>\n   context enhancement\n  </strong>\n  in LLM and RAG applications:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    LlamaParse:\n   </strong>\n   Offers sophisticated parsing for complex documents, making it possible to answer detailed queries.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Managed Ingestion and Retrieval API:\n   </strong>\n   Facilitates easier data management, connecting with over 150 sources and 40+ storage solutions.\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  LlamaParse is now available for a public preview, primarily focusing on PDFs with a user cap, while the API is in a private preview for select enterprise partners. If you haven\u2019t explored these new features yet, we invite you to\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://blog.llamaindex.ai/introducing-llamacloud-and-llamaparse-af8cedf9006b\" rel=\"noreferrer noopener\">\n   check them out\n  </a>\n  for more details or to discuss commercial terms.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Your innovation inspires us! We\u2019re eager to see the projects, articles, or videos that inspire you. Share your remarkable works with us at\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"mailto:news@llamaindex.ai\" rel=\"noreferrer noopener\">\n   news@llamaindex.ai\n  </a>\n  . And if you haven\u2019t already, subscribe to our newsletter on our website to receive the latest LlamaIndex updates straight to your inbox.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </p>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Enhanced RAG Retrieval with Sub-Document Summaries:\n   </strong>\n   Introducing a novel chunking method that improves RAG performance by incorporating hierarchical metadata into chunks, ensuring precise and context-aware information retrieval.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-subdoc-summary/examples/subdoc-summary.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1761793821422264757?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    MistralAI Cookbook:\n   </strong>\n   A comprehensive guide to leveraging the Mistral-Large model from MistralAI, featuring near-GPT-4 reasoning, function calling, and JSON output for cutting-edge applications.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/cookbooks/mistralai.html\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1762231085243719748?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Gemma Cookbook:\n   </strong>\n   A comprehensive guide to using Gemma, GoogleDeepMind\u2019s latest LLM offering, with options for 2B and 7B parameters, facilitating the development of local RAG systems on your laptop.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/ollama_gemma.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/jerryjliu0/status/1760471196402069771?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    ColBERT Integration:\n   </strong>\n   Document reranking with ColBERT via LlamaIndex, delivering a solution that is about 100x faster than BERT-based models for more efficient data processing.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/node_postprocessor/ColbertRerank.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1760830777179471933?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Counselor Copilot \u2014 Social Impact Through RAG:\n   </strong>\n   Spotlight on Counselor Copilot, an innovative RAG project supporting the Trevor Project\u2019s crisis counselors, providing real-time assistance with context, suggestions, and actions to aid LGBTQ+ youth effectively.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://blog.llamaindex.ai/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3\" rel=\"noreferrer noopener\">\n    BlogPost\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1761433854458614075?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have launched a new chunking strategy to enhance RAG retrieval: Sub-Document Summaries. This approach overcomes the limitations of naive chunking by injecting hierarchical metadata, offering a nuanced balance of global context awareness and precision through subdocument summaries for improved performance.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-subdoc-summary/examples/subdoc-summary.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1761793821422264757?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have launched a cookbook for the latest\n   <code class=\"SanityPortableText_inlineCode__cI85z\">\n    mistral-large\n   </code>\n   model from MistralAI offering advanced features like near GPT-4 level reasoning, Function calling, JSON Output, and more.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/cookbooks/mistralai.html\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1762231085243719748?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have launched a cookbook on Gemma, a new family of state-of-the-art LLMs by GoogleDeepMind, with 2B and 7B parameter options using Ollama to build local RAG on your laptop.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/ollama_gemma.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/jerryjliu0/status/1760471196402069771?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have introduced ColBERT through LlamaIndex, offering a one-line integration for a reranking model that\u2019s ~100x faster than traditional BERT-based models, ensuring efficient document handling with superior performance.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/node_postprocessor/ColbertRerank.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1760830777179471933?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have introduced a way to integrate advanced RAG into full-stack web apps with create-llama, using LlamaPacks, in just two lines of code.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/LlamaIndexTS/tree/main/packages/create-llama\" rel=\"noreferrer noopener\">\n    create-llama\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1761159412629336404?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   \ud83c\udfa5 Demos:\n  </strong>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/zrizvi93/trevorhack\" rel=\"noreferrer noopener\">\n   Counselor Copilot\n  </a>\n  : An interesting RAG project by\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/FintechRiya\" rel=\"noreferrer noopener\">\n   Riya Jagetia\n  </a>\n  and team, designed to assist crisis counselors at the Trevor Project in supporting LGBTQ+ youth. This tool acts as a real-time copilot, offering context, suggested replies, and various actions to enhance counselor effectiveness, showcasing a unique and socially impactful application of advanced RAG techniques.\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://blog.llamaindex.ai/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3\" rel=\"noreferrer noopener\">\n   BlogPost\n  </a>\n  ,\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1761433854458614075?s=20\" rel=\"noreferrer noopener\">\n   Tweet\n  </a>\n  .\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.google.com/presentation/d/1NAAI6DoLEIw7RDvx4vXvFG2jz3WUKVpL4j39-E9MouQ/edit#slide=id.g2b99c281f78_0_0\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to simplifying advanced RAG development: Our latest insights pinpoint solutions for key challenges, including our innovative LlamaParse for complex PDF QA, shared in our AI in Production presentation.\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/@marco.bertelli\" rel=\"noreferrer noopener\">\n    Marco Bertelli\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/@marco.bertelli/unveiling-the-power-of-rag-building-an-interactive-chatbot-with-react-a-comprehensive-guide-99c409a5f69a\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on Building an Interactive Chatbot with React.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/wenqi_glantz\" rel=\"noreferrer noopener\">\n    Wenqi Glantz\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://towardsdatascience.com/the-journey-of-rag-development-from-notebook-to-microservices-cc065d0210ef\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on The Journey of RAG Development: From Notebook to Microservices.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/wenqi_glantz\" rel=\"noreferrer noopener\">\n    Wenqi Glantz\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=EBpT_cscTis\" rel=\"noreferrer noopener\">\n    tutorial video\n   </a>\n   on 12 RAG Pain Points and Solutions in the RAG pipeline.\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  \ud83c\udfa5\n  <strong>\n   Webinar:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=ZP1F9z-S7T0\" rel=\"noreferrer noopener\">\n    Webinar\n   </a>\n   with Sisil from JasperAI on Practical Tips and Tricks for Productionizing RAG.\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 12647, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "309f00b5-e781-4d35-998a-88bcadea4915": {"__data__": {"id_": "309f00b5-e781-4d35-998a-88bcadea4915", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-05.html", "file_name": "llamaindex-newsletter-2024-03-05.html", "file_type": "text/html", "file_size": 13864, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-05.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-05.html", "file_name": "llamaindex-newsletter-2024-03-05.html", "file_type": "text/html", "file_size": 13864, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "951430a5bc04cad70d11849f0441123fc4662962810e3819e701961c3e51108a", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Greetings, LlamaIndex devotees! \ud83e\udd99\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  It was another fun week to be at the center of the LLM universe, and we have tons to share!\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We shared our thoughts on the future of\n   <strong>\n    long-context RAG.\n   </strong>\n   As LLMs with context windows over 1M tokens begin to appear, what changes about RAG, and how will LlamaIndex evolve?\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1763620476847632744\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/towards-long-context-rag\" rel=\"noreferrer noopener\">\n    Blog post\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    llama-index-networks\n   </strong>\n   lets you build a super-RAG application by combining answers from independent RAG apps over the network.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1762552542981230769\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f\" rel=\"noreferrer noopener\">\n    Blog post\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/tree/main/llama-index-networks\" rel=\"noreferrer noopener\">\n    repo\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   People loved our release of LlamaParse, a world-beating PDF parsing service, so we made it even better!\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1763008808216060186\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b\" rel=\"noreferrer noopener\">\n    blog post\n   </a>\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We released a new llama-index-networks feature that lets you combine multiple independent RAG applications over the network, allowing you to run a single query across all the applications and get a single, combined answer.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1762552542981230769\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f\" rel=\"noreferrer noopener\">\n    Blog post\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/tree/main/llama-index-networks\" rel=\"noreferrer noopener\">\n    repo\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Inference engine\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://groq.com/\" rel=\"noreferrer noopener\">\n    Groq\n   </a>\n   wowed us and the world with their incredibly fast query times and we were delighted to introduce first-class support for their LLM APIs.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1762869201222639927\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/examples/llm/groq.html\" rel=\"noreferrer noopener\">\n    notebook\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Users love LlamaParse, the world-beating PDF parsing service we released last week. We pushed improved parsing and OCR support for 81+ languages! We also increased the usage cap from 1k to 10k pages per day.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1763008808216060186\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b\" rel=\"noreferrer noopener\">\n    blog post\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We migrated our\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog\" rel=\"noreferrer noopener\">\n    blog off of Medium\n   </a>\n   , we hope you like the new look and the absence of nag screens!\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   RAPTOR is a new tree-structured technique for advanced RAG; we turned the paper into a LlamaPack, allowing you to use the new technique in one line of code.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1763972097628684607\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://llamahub.ai/l/llama-packs/llama-index-packs-raptor?from=\" rel=\"noreferrer noopener\">\n    package\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-raptor/examples/raptor.ipynb\" rel=\"noreferrer noopener\">\n    notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://arxiv.org/pdf/2401.18059.pdf\" rel=\"noreferrer noopener\">\n    original paper\n   </a>\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   \ud83c\udfa5\u00a0Demos:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   The Koda Retriever is a new retrieval concept: hybrid search where the alpha parameter controlling the importance of vector search vs. keyword search is tuned on a per-query basis by the LLM itself, based on a few-shot examples.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1763252392639042024\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-koda-retriever/examples/alpha_tuning.ipynb\" rel=\"noreferrer noopener\">\n    notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://llamahub.ai/l/llama-packs/llama-index-packs-koda-retriever?from=\" rel=\"noreferrer noopener\">\n    package\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00\" rel=\"noreferrer noopener\">\n    blog post\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"http://Mixedbread.ai\" rel=\"noreferrer noopener\">\n    Mixedbread.ai\n   </a>\n   released some state-of-the-art rerankers that perform better than anything seen before; we whipped up a quick cookbook to show you how to use them directly in LlamaIndex.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1763654691886674134\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/cookbooks/mixedbread_reranker.html\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.mixedbread.ai/blog/mxbai-rerank-v1\" rel=\"noreferrer noopener\">\n    blog post\n   </a>\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Function-calling cookbook with open source models\n   </strong>\n   shows you how to use Fireworks AI\u2019s OpenAI-compatible API to use all native LlamaIndex support for function calling.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/examples/llm/fireworks_cookbook.html\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1762532341795487815\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We released a best practices cookbook showing how to use LlamaParse, our amazing PDF parser.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1763729200106840548\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_parse/blob/main/examples/demo_table_comparisons.ipynb\" rel=\"noreferrer noopener\">\n    notebook\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   A\n   <strong>\n    comprehensive guide to semantic chunking for RAG\n   </strong>\n   by Florian June covers embedding-based chunking, BERT-based chunking techniques, and LLM-based chunking for everything you need to know about this highly effective technique to improve retrieval quality.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1764335221141631471\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://pub.towardsai.net/advanced-rag-05-exploring-semantic-chunking-97c12af20a4d\" rel=\"noreferrer noopener\">\n    Blog post\n   </a>\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Our own\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/_nerdai_\" rel=\"noreferrer noopener\">\n    Andrei\n   </a>\n   presented a notebook on building Basic RAG with LlamaIndex at\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://vectorinstitute.ai/\" rel=\"noreferrer noopener\">\n    Vector Institute\n   </a>\n   \u2019s RAG bootcamp.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/_nerdai_/status/1762924582183350782\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/docs/presentations/materials/2024-02-28-rag-bootcamp-vector-institute.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   ClickHouse presented an in-depth tutorial using LlamaIndex to query both structured and unstructured data, and built a bot that queries Hacker News to find what people are saying about the most popular technologies.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1763282902358585445\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://clickhouse.com/blog/building-hackernews-stackoverflow-chatbot-with-llamaindex-and-clickhouse\" rel=\"noreferrer noopener\">\n    blog post\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   POLM (Python, OpenAI, LlamaIndex, MongoDB) is a new reference architecture for building RAG applications and MongoDB has a beautiful, step-by-step tutorial for building it out.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1764078471276642469\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.mongodb.com/developer/products/atlas/rag-with-polm-stack-llamaindex-openai-mongodb/\" rel=\"noreferrer noopener\">\n    blog post\n   </a>\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  \ud83c\udfa5\n  <strong>\n   Webinar:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Our CEO Jerry Liu will do a joint webinar with Adam Kamor of\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"http://Tonic.ai\" rel=\"noreferrer noopener\">\n    Tonic.ai\n   </a>\n   about building fully-local RAG applications with Ollama and Tonic. People love local models!\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1763304038442192978\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.tonic.ai/webinars/preserve-privacy-using-local-rag-with-tonic-ai-llamaindex\" rel=\"noreferrer noopener\">\n    Registration page\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Jerry also did a webinar with\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.traceloop.com/\" rel=\"noreferrer noopener\">\n    Traceloop\n   </a>\n   on leveling up your LLM application with observability.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1763364010676900080\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=if-2elFrfgM\" rel=\"noreferrer noopener\">\n    YouTube\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Our hackathon at the beginning of February was a huge success! Check out this webinar in which we invited the winners to come and talk about their projects.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1764020728427667605\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=THkeXnwwSw4\" rel=\"noreferrer noopener\">\n    YouTube\n   </a>\n   .\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 13835, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5507c3b5-7743-46ce-85e1-540285feec65": {"__data__": {"id_": "5507c3b5-7743-46ce-85e1-540285feec65", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-12.html", "file_name": "llamaindex-newsletter-2024-03-12.html", "file_type": "text/html", "file_size": 11140, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-12.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-12.html", "file_name": "llamaindex-newsletter-2024-03-12.html", "file_type": "text/html", "file_size": 11140, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "1ce98b90b3592745ee4965c6e724f3dfbfd14237bc1337cfa5f9fbb60e04e0c9", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Salutations, LlamaIndex fans! \ud83e\udd99\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  It's been another thrilling week in LlamaLand! With the release of Anthropic\u2019s new models Claude-3 Opus, Sonnet, and Haiku, we have numerous tutorials, cookbooks, and updates to share with you.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  A quick reminder: we are running our\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.surveymonkey.com/r/9W8VX2H\" rel=\"noreferrer noopener\">\n   first ever user survey\n  </a>\n  . It takes only 3 minutes and it helps us out a lot! Now let\u2019s dive in.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </p>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    LlamaParse JSON Mode\n   </strong>\n   : A new feature that transforms PDF content into structured data, simplifying RAG pipeline development for complex documents containing images, text and tables.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_parse/blob/main/examples/demo_json.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1765439865351766135?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Hierarchical Code Splitting\n   </strong>\n   : Enhance code understanding with a novel technique that organizes large codebases into a hierarchical structure for improved navigation and task-solving.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://llamahub.ai/l/llama-packs/llama-index-packs-code-hierarchy?from=llama-packs\" rel=\"noreferrer noopener\">\n    LlamaPack\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1766152269874266170?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Anthropic Cookbook Series\n   </strong>\n   : Learn to build various LLM applications with Claude 3, ranging from simple to complex, through detailed guides and tutorials.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/anthropics/anthropic-cookbook/tree/main/third_party/LlamaIndex\" rel=\"noreferrer noopener\">\n    Cookbooks\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1767218890856358115?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We launched LlamaParse JSON Mode, a new feature that structures text and images from PDFs into a dict format. With the integration of multimodal models like claude-3 opus, it's now simpler to develop RAG pipelines for complex PDFs containing text, images, and tables.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_parse/blob/main/examples/demo_json.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1765439865351766135?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We launched a novel hierarchical code splitting technique to enhance RAG/agents for code comprehension, featuring\n   <code class=\"SanityPortableText_inlineCode__cI85z\">\n    CodeHierarchyNodeParser\n   </code>\n   by ryanpeach. This method breaks down large code files into a hierarchical structure, enabling a knowledge graph-like approach for efficient code navigation and task-solving.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://llamahub.ai/l/llama-packs/llama-index-packs-code-hierarchy?from=llama-packs\" rel=\"noreferrer noopener\">\n    LlamaPack\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1766152269874266170?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We integrated with Videodb to run RAG over video streams using LlamaIndex. This tool allows you to upload, search, and stream videos based on spoken words or visual scenes, now available as a built-in retriever in LlamaIndex.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1765481657765912599?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   \ud83c\udfa5\u00a0Demos:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/mithril-security/LaVague?tab=readme-ov-file\" rel=\"noreferrer noopener\">\n    <strong>\n     Build an AI Browser Copilot\n    </strong>\n   </a>\n   : a project by\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/dhuynh95\" rel=\"noreferrer noopener\">\n    <strong>\n     Daniel Huynh\n    </strong>\n   </a>\n   that demonstrates how to create a browser agent using RAG, local embeddings, and Mixtral to execute browser tasks from a Colab notebook, showcased with a video on navigating HuggingFace datasets.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://lightning.ai/lightning-ai/studios/chat-with-your-code-using-rag\" rel=\"noreferrer noopener\">\n    RAG over your code\n   </a>\n   : a project by\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/akshay_pachaar\" rel=\"noreferrer noopener\">\n    <strong>\n     Akshay\n    </strong>\n   </a>\n   on creating a local code assistant using LlamaIndex, MistralAI, and Streamlit to index and query GitHub repositories, offering a foundational guide for advanced code QA.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://nething.xyz/:\" rel=\"noreferrer noopener\">\n    https://nething.xyz/:\n   </a>\n   a project by Raymond Weitekamp on generating production-ready 3D CAD models from text prompts. It uses LLM code generation to create commands to generate printable 3D objects from text prompts using LlamaIndex.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/unlocking-the-3rd-dimension-for-generative-ai-part-1\" rel=\"noreferrer noopener\">\n    Blog\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1764771352077320517?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1767218890856358115?s=20\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to the Anthropic Cookbook Series: Create context-augmented LLM apps using Claude 3, from basic RAG to advanced agents, through six notebooks and four videos.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Video\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=VRDNyFj-xeE\" rel=\"noreferrer noopener\">\n    guide\n   </a>\n   exploring diverse applications of Claude-3 with LlamaIndex. tooling, covering Vanilla RAG, routing, sub-question query planning, structured data extraction, text-to-SQL, and agents\u2014a perfect starter kit for Claude enthusiasts.\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://ai.gopubby.com/leveraging-llamaindex-step-wise-react-agent-for-efficient-document-handling-3a0f92b9ca22\" rel=\"noreferrer noopener\">\n    Tutorial\n   </a>\n   by\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/@andysingal?source=post_page-----3a0f92b9ca22--------------------------------\" rel=\"noreferrer noopener\">\n    Ankush k Singal\n   </a>\n   on building local LLM agents with Llama.cpp for step-wise execution and incorporating human feedback during execution.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=7qsxz2rURG4\" rel=\"noreferrer noopener\">\n    \u201cRAG over Complex PDFs V2\"\n   </a>\n   : a comprehensive tutorial by\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/AIMakerspace\" rel=\"noreferrer noopener\">\n    <strong>\n     AI Makerspace\n    </strong>\n   </a>\n   on crafting advanced RAG pipelines for handling messy PDFs with LlamaParse and LlamaIndex, where naive RAG falls short.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://blog.streamlit.io/build-a-real-time-rag-chatbot-google-drive-sharepoint/\" rel=\"noreferrer noopener\">\n    Tutorial\n   </a>\n   on building a real-time RAG chatbot using Google Drive and Sharepoint by\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://blog.streamlit.io/author/anup/\" rel=\"noreferrer noopener\">\n    Anup Surendran\n   </a>\n   and\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://blog.streamlit.io/author/berke/\" rel=\"noreferrer noopener\">\n    Berke Can Rizai\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Step-by-step\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://jina.ai/news/precise-rag-with-jina-reranker-and-llamaindex/\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   to improve the quality of your RAG application using JinaAI reranker, LlamaIndex, and MistralAI.\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  \ud83c\udfa5\n  <strong>\n   Webinars:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=Bhnq8grQm5Y\" rel=\"noreferrer noopener\">\n    Webinar\n   </a>\n   with\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/parthsarthi03\" rel=\"noreferrer noopener\">\n    Parth Sarthi\n   </a>\n   , lead author of RAPTOR - Tree-Structured Indexing and Retrieval.\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  \ud83d\udcc5\u00a0Events:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We are hosting a RAG\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.meetup.com/paris-retrieval-augmented-generation-group/events/299374545/\" rel=\"noreferrer noopener\">\n    meetup\n   </a>\n   in Paris on March 27th featuring talks on advanced RAG strategies, building a RAG CLI, and the significance of open-source RAG in business.\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 11103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fae755c6-64eb-4e29-b14e-ec1f5fe151b0": {"__data__": {"id_": "fae755c6-64eb-4e29-b14e-ec1f5fe151b0", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-19.html", "file_name": "llamaindex-newsletter-2024-03-19.html", "file_type": "text/html", "file_size": 11259, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-19.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-19.html", "file_name": "llamaindex-newsletter-2024-03-19.html", "file_type": "text/html", "file_size": 11259, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "ef372aa291ffc53119df859ee021c76213dbfd049bf725e5168222987f46702f", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Greetings, LlamaIndex enthusiasts! \ud83e\udd99\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Welcome to another exciting weekly update from the world of LlamaVerse!\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We have an amazing news for you from LlamaIndex. We've officially launched LlamaParse, a GenAI-native document parsing solution. With state-of-the-art table and chart extraction, natural language steerable instructions, and compatibility with over a dozen document types, LlamaParse excels in creating accurate RAG applications from complex documents. After a successful private preview with 2k users and 1M pages parsed, it's now ready to transform your document handling. Check out our\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/launching-the-first-genai-native-document-parsing-platform\" rel=\"noreferrer noopener\">\n   launch post\n  </a>\n  for all the details!\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </p>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    New observability with Instrumentation:\n   </strong>\n   Enhanced developer workflow with a new Instrumentation module for improved observability.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/module_guides/observability/instrumentation.html\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1768730443921396220?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    LlamaParse accepts natural language parsing instructions\n   </strong>\n   : Easily extract math snippets from PDFs into LaTeX with LlamaParse.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/launching-the-first-genai-native-document-parsing-platform\" rel=\"noreferrer noopener\">\n    Blogpost\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1768443551267049492?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Financial Data Parsing:\n   </strong>\n   Transform PowerPoint parsing, utilizing LlamaParse to extract and interpret complex financial data from .pptx files, enabling detailed and accurate financial analysis.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_parse/blob/main/examples/other_files/demo_ppt_financial.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1768303288381030408?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We introduced LlamaIndex v0.10.20, featuring our new Instrumentation module, a leap in observability that simplifies developer workflows by providing a module-level dispatcher, reducing the need for individual callback managers and facilitating comprehensive handler sets across your application.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/module_guides/observability/instrumentation.html\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1768730443921396220?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have launched parsing by prompting feature in LlamaParse to properly extract out any math snippets from PDFs into LaTex which helps you to plug easily into your RAG pipeline.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/launching-the-first-genai-native-document-parsing-platform\" rel=\"noreferrer noopener\">\n    Blogpost\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1768443551267049492?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have launched an advanced RAG pipeline for Financial PowerPoints, using LlamaParse to tackle the challenge of parsing .pptx files. Our solution accurately extracts slides, including text, tables, and charts, enabling precise question-answering over complex financial data.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_parse/blob/main/examples/other_files/demo_ppt_financial.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1768303288381030408?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We collaborated with langfuse to launch open-source observability for your RAG pipeline, enhancing your application with integrated tracing, prompt management, and evaluation in just two lines of code.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/one-click-open-source-rag-observability-with-langfuse\" rel=\"noreferrer noopener\">\n    Blogpost\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/examples/callbacks/LangfuseCallbackHandler.html\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1769790083564208218?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Search-in-the-Chain: a method by Shicheng Xu et al., is now integrated into LlamaIndex, enhancing question-answering with an advanced system that interleaves retrieval and planning. This approach verifies each reasoning step in a chain, allowing for dynamic replanning and application in various agent reasoning contexts.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://llamahub.ai/l/llama-packs/llama-index-packs-searchain?from=\" rel=\"noreferrer noopener\">\n    LlamaPack\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1769035278063399208?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   \ud83c\udfa5\u00a0Demos:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Home AI, a tool created with create-llama, to help home searches by using LLMs to automate the parsing of complex property disclosures, enabling users to filter searches with unprecedented detail and efficiency.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://devpost.com/software/home-ai\" rel=\"noreferrer noopener\">\n    Blogpost\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/2sunflower33/homeai\" rel=\"noreferrer noopener\">\n    Code\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1767289805719978288?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/tensorsense/Retrieval-Framework/blob/main/hierarchical_retrieval.ipynb\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to using LlamaIndex and Mathpix to parse, index, and query complex mathematics within scientific papers, detailing steps from parsing tables and extracting images to indexing in a RAG app and answering questions with precise LaTeX outputs, to showcase hierarchical retrieval technique.\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/taupirho\" rel=\"noreferrer noopener\">\n    Thomas Reid\n   </a>\n   \u2019s\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://ai.gopubby.com/llamaparse-rag-beats-all-comers-60948c6cc0e4\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on using LlamaParse can help properly extract text from a Tesla quarterly filings.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/mesudarshan\" rel=\"noreferrer noopener\">\n    Sudarshan Koirala\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=w7Ap6gZFXl0\" rel=\"noreferrer noopener\">\n    video tutorial\n   </a>\n   on RAG with LlamaParse, Qdrant, and Groq.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Kyosuke Morita\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://pub.towardsai.net/rag-based-job-search-assistant-98dd72c98fbd\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   showing how to match a candidate to jobs based on their CV with LlamaParse + LlamaIndex.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/CobusGreylingZA\" rel=\"noreferrer noopener\">\n    Cobus Greyling\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://cobusgreyling.medium.com/agentic-rag-context-augmented-openai-agents-578e96212bc0\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on Agentic RAG: Context-Augmented OpenAI Agents.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/RoeyBC\" rel=\"noreferrer noopener\">\n    Roey Ben Chaim\n   </a>\n   \u2019s\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/pii-detector-hacking-privacy-in-rag\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on PII Detector: hacking privacy in RAG.\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  \ud83c\udfa5\n  <strong>\n   Webinars:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=Bhnq8grQm5Y\" rel=\"noreferrer noopener\">\n    Webinar\n   </a>\n   with Charles Packer, lead author of MemGPT on Long-Term, Self-Editing Memory with MemGPT\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  \ud83d\udcc5\u00a0Events:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We are hosting a RAG\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.meetup.com/paris-retrieval-augmented-generation-group/events/299374545/\" rel=\"noreferrer noopener\">\n    meetup\n   </a>\n   in Paris on March 27th featuring talks on advanced RAG strategies, building a RAG CLI, and the significance of open-source RAG in business.\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 11226, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee8f4315-d714-4901-be24-598390cde4ae": {"__data__": {"id_": "ee8f4315-d714-4901-be24-598390cde4ae", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-26.html", "file_name": "llamaindex-newsletter-2024-03-26.html", "file_type": "text/html", "file_size": 13007, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-26.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-26.html", "file_name": "llamaindex-newsletter-2024-03-26.html", "file_type": "text/html", "file_size": 13007, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "aeefd8a7ef5da38141f211b9bfd907b90bf156ad9b03b229bd3e502103d1aff0", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Hi there, LlamaIndex followers! \ud83e\udd99\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Welcome to another thrilling weekly update from the LlamaUniverse. We're excited to bring you a fantastic array of updates, including Privacy-Preserving In-Context Learning with LlamaPacks and RAG Networks. Dive into our guides on MistralAI, explore Gemma LLMs, and enjoy a plethora of engaging tutorials using LlamaIndex, alongside upcoming webinars and events.\n </p>\n <h4 class=\"Text_text__zPO0D Text_text-size-32__koGps\">\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </h4>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Privacy-Preserving In-Context Learning:\n   </strong>\n   Leveraging\n   <strong>\n    <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/XinyuTang7\" rel=\"noreferrer noopener\">\n     Xinyu Tang\n    </a>\n    \u2019s\n   </strong>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://arxiv.org/abs/2309.11765\" rel=\"noreferrer noopener\">\n    paper\n   </a>\n   , we've introduced LlamaPack for LLM/RAG apps, enabling the creation of few-shot demonstrations that maintain privacy and data integrity.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://llamahub.ai/l/llama-packs/llama-index-packs-diff-private-simple-dataset?from=llama-packs\" rel=\"noreferrer noopener\">\n    LlamaPack\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1770837291855991085?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Privacy-Preserving RAG Network:\n   </strong>\n   We present Privacy-Preserving RAG Network which facilitates the use of confidential datasets in healthcare and online platforms while safeguarding user privacy.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network\" rel=\"noreferrer noopener\">\n    Blogpost\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1770966231769854076?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Advanced RAG and Agents with MistralAI:\n   </strong>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.google.com/presentation/d/1dbfoxzNcoI-D45RKZfO1UfBJIr4v0YtHhj1cwuCj020/edit#slide=id.p\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   on using MistralAI with LlamaIndex and LlamaParse, advancing RAG capabilities and agent development through custom pipelines and sophisticated parsing.\n  </li>\n </ol>\n <h4 class=\"Text_text__zPO0D Text_text-size-32__koGps\">\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </h4>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We launched a LlamaPack based on\n   <strong>\n    <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/XinyuTang7\" rel=\"noreferrer noopener\">\n     Xinyu Tang\n    </a>\n    \u2019s\n   </strong>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://arxiv.org/abs/2309.11765\" rel=\"noreferrer noopener\">\n    paper\n   </a>\n   ****for secure in-context learning in LLM/RAG apps, focusing on generating few-shot demonstrations from private datasets with differential privacy, ensuring the synthetic examples reflect the data distribution without exposing sensitive details.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://llamahub.ai/l/llama-packs/llama-index-packs-diff-private-simple-dataset?from=llama-packs\" rel=\"noreferrer noopener\">\n    LlamaPack\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1770837291855991085?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We introduced a privacy-preserving RAG network by\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.linkedin.com/in/nerdai/\" rel=\"noreferrer noopener\">\n    Andrei\n   </a>\n   in LlamaIndex, enabling the use of sensitive datasets like healthcare and online user data without compromising individual privacy. This approach allows data providers to synthetically generate and share data for RAG queries securely.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network\" rel=\"noreferrer noopener\">\n    Blogpost\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1770966231769854076?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We introduce a template by\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/hackgoofer\" rel=\"noreferrer noopener\">\n    <strong>\n     Sasha\n    </strong>\n   </a>\n   for agent-human interaction in RAG implementations, focusing on minimal human input. It triggers human intervention only for vague or malformed queries, enhancing clarity and precision in the response process.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://llamahub.ai/l/llama-packs/llama-index-packs-query-understanding-agent?from=llama-packs\" rel=\"noreferrer noopener\">\n    LlamaPack\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1771207903439159404?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/BAMelevate\" rel=\"noreferrer noopener\">\n    BAM Elevate\n   </a>\n   integrated Databricks Vector Search into LlamaIndex, enabling vector search capabilities within the Databricks ecosystem.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.bamelevate.com/news/llamaindex-and-databricks-integration-announcement\" rel=\"noreferrer noopener\">\n    Blogpost\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1770585400840699974?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We launched LlamaParse integration with LlamaIndex TypeScript, an industry-leading parser for PDFs and various document types accessible directly from JS/TS. Utilize the create-llama command-line tool or integrate LlamaParse directly into your app for enhanced document processing.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/LlamaIndexTS/blob/main/examples/readers/src/llamaparse.ts\" rel=\"noreferrer noopener\">\n    Example\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1770496142020895159?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <h4 class=\"Text_text__zPO0D Text_text-size-32__koGps\">\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </h4>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.google.com/presentation/d/1dbfoxzNcoI-D45RKZfO1UfBJIr4v0YtHhj1cwuCj020/edit#slide=id.p\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to Advanced RAG and Agents with MistralAI using LlamaIndex and LlamaParse to construct sophisticated RAG and agents, including custom query pipelines, document parsing, and reference applications.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.kaggle.com/code/iamleonie/advanced-rag-with-gemma-weaviate-and-llamaindex\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to Integrating Custom Models with LlamaIndex:\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/helloiamleonie\" rel=\"noreferrer noopener\">\n    Leonie Monigatti\n   </a>\n   demonstrates the process of incorporating your custom model, like Gemma, into LlamaIndex\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to combat prompt injection attacks, like the \"white text\" attack, by rigorously screening data during ingestion and retrieval, ensuring the integrity of LLM-powered systems against deceptive manipulations by\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/alex_yaremchuk\" rel=\"noreferrer noopener\">\n    Oleksandr Yaremchuk\n   </a>\n   from\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/ProtectAICorp\" rel=\"noreferrer noopener\">\n    Protect AI\n   </a>\n   .\n  </li>\n </ul>\n <h4 class=\"Text_text__zPO0D Text_text-size-32__koGps\">\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </h4>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/AkritiUpadhya13\" rel=\"noreferrer noopener\">\n    Akriti Upadhyay\n   </a>\n   \u2019s\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/@akriti.upadhyay/integrating-llamaindex-and-qdrant-similarity-search-for-patient-record-retrieval-7090e77b971e\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   to prototype on patient data safely, featuring synthetic dataset generation, storage in Qdrant Vector DB, and querying with llama.cpp LLM using LlamaIndex.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/BaeleFrank\" rel=\"noreferrer noopener\">\n    Frank Baele\n   </a>\n   \u2019s\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://franz.be/blogpost/bringing-a-naive-rag-to-production\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on developing a production-grade RAG pipeline with LlamaParse, detailing document parsing, advanced ingestion techniques, Vector DB selection, and insights on evaluation, deployment, and budget management.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=xwfR8fC_Azs\" rel=\"noreferrer noopener\">\n    Video tutorial\n   </a>\n   by Ashish on creating an advanced PDF RAG agent, utilizing LlamaParse for text and tables extraction, defining retrievers and routers, and adding a sub-question layer, all integrated with LlamaIndex and MistralAI.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/UpTrainAI\" rel=\"noreferrer noopener\">\n    UpTrain\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/ravithejads\" rel=\"noreferrer noopener\">\n    Ravi Theja\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://ravidesetty.medium.com/introducing-navarasa-2-0-indic-gemma-7b-2b-instruction-tuned-model-on-15-indian-languages-31f6565b2750\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on showcasing RAG with LlamaIndex on 15 Indian languages using Navarasa-2.0 - a Gemma finetuned model on 15 Indian languages.\n  </li>\n </ul>\n <h4 class=\"Text_text__zPO0D Text_text-size-32__koGps\">\n  \ud83c\udfa5\n  <strong>\n   Webinars:\n  </strong>\n </h4>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://lu.ma/z2vhi06e\" rel=\"noreferrer noopener\">\n   Register for a webinar\n  </a>\n  with\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/dhuynh95\" rel=\"noreferrer noopener\">\n   <strong>\n    Daniel Huynh\n   </strong>\n  </a>\n  featuring LaVague, an agent that can navigate the web in your Jupyter/Colab notebook.\n </p>\n <h4 class=\"Text_text__zPO0D Text_text-size-32__koGps\">\n  \ud83d\udcc5\u00a0Events:\n </h4>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/lablabai/status/1770847744313241637?s=20\" rel=\"noreferrer noopener\">\n    Join us\n   </a>\n   for a Panel discussion on 'Why RAG Will Never Die - The Context Window Myth\u2019 with panelists from LlamaIndex, Vectara, Nvidia, and TogetherAI.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We are hosting a RAG\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.meetup.com/paris-retrieval-augmented-generation-group/events/299374545/\" rel=\"noreferrer noopener\">\n    meetup\n   </a>\n   in Paris on March 27th featuring talks on advanced RAG strategies, building a RAG CLI, and the significance of open-source RAG in business.\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 12972, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2645d982-aadc-4e88-8d16-a0cda3215a7c": {"__data__": {"id_": "2645d982-aadc-4e88-8d16-a0cda3215a7c", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-02.html", "file_name": "llamaindex-newsletter-2024-04-02.html", "file_type": "text/html", "file_size": 13087, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-02.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-02.html", "file_name": "llamaindex-newsletter-2024-04-02.html", "file_type": "text/html", "file_size": 13087, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "b6038056331642b5861c05255a44dfc639fc8441719af85b4f4aa5103aeaee12", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Greetings, LlamaIndex community! \ud83e\udd99\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Welcome to another exciting weekly update from LlamaGalaxy! We're thrilled to share a range of fantastic updates with you, including the introduction of RAFT LlamaPack, enhanced memory and cost efficiency in RAG with Cohere's embeddings, and much more.\n </p>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </h3>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    DeepLearningAI Course:\n   </strong>\n   JavaScript RAG Web Apps with\u00a0LlamaIndex collaborative course with DeepLearningAI.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.deeplearning.ai/short-courses/javascript-rag-web-apps-with-llamaindex/\" rel=\"noreferrer noopener\">\n    Course\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/AndrewYNg/status/1773006786058219889?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    RAFTDatasetPack LlamaPack\n   </strong>\n   : Introduced RAFTDatasetPack for dataset generation using RAFT - Retrieval Augmented Fine Tuning for training models to differentiate between relevant 'oracle' documents and 'distractor' documents.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/tree/main/llama-index-packs/llama-index-packs-raft-dataset\" rel=\"noreferrer noopener\">\n    LlamaPack\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1772662480210198809?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Memory Efficiency with Cohere Embeddings:\n   </strong>\n   Utilize Cohere's Int8 and binary embeddings for cost-effective and low-memory RAG operations.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/cohere_retriever_eval.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1773402379016138955?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Python Docs Makeover:\n   </strong>\n   Revamped Python documentation with accessible example notebooks, advanced search, and comprehensive API details.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/api_reference/\" rel=\"noreferrer noopener\">\n    API Ref\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1772355240299520083?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://t.co/BS7oDqZ7qW\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n  </li>\n </ol>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </h3>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We introduced RAFT - Retrieval Augmented Fine Tuning, a method from\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.linkedin.com/in/tianjun-zhang-333bb2126/overlay/about-this-profile/?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base%3B1dQCZFffT4aXk6ePSYdUYg%3D%3D\" rel=\"noreferrer noopener\">\n    Tianjun Zhang\n   </a>\n   and\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.linkedin.com/in/shishir-patil/overlay/about-this-profile/?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base%3BNG6wPCQHTaWKxcdltRvvjw%3D%3D\" rel=\"noreferrer noopener\">\n    Shishir Patil\n   </a>\n   to enhance domain-specific RAG performance in LLMs. By training models to differentiate between relevant 'oracle' documents and 'distractor' documents, RAFT improves context understanding. Try it out with our new RAFTDatasetPack LlamaPack for dataset generation.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/tree/main/llama-index-packs/llama-index-packs-raft-dataset\" rel=\"noreferrer noopener\">\n    LlamaPack\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1772662480210198809?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We collaborated with DeepLearningAI for a course that goes beyond teaching RAG techniques; it guides you on integrating RAG into a full-stack application. Learn to construct a backend API, develop an interactive React component, and tackle the unique challenges of deploying RAG on a server rather than just in a notebook.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.deeplearning.ai/short-courses/javascript-rag-web-apps-with-llamaindex/\" rel=\"noreferrer noopener\">\n    Course\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/AndrewYNg/status/1773006786058219889?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We integrated with Cohere's Int8 and Binary Embeddings for a memory-efficient solution for your RAG pipeline. This addresses the high memory usage and costs associated with large dataset operations in RAG.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/cohere_retriever_eval.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1773402379016138955?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We launched revamped Python docs with top-level example notebooks, improved search with previews, and overhauled API documentation.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/api_reference/\" rel=\"noreferrer noopener\">\n    API Ref\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1772355240299520083?s=20\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://t.co/BS7oDqZ7qW\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n  </li>\n </ol>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  <strong>\n   \ud83c\udfa5\u00a0Demos:\n  </strong>\n </h3>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1774159755675898010?s=20\" rel=\"noreferrer noopener\">\n    RestAI\n   </a>\n   , a project by\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/pedromdias\" rel=\"noreferrer noopener\">\n    Pedro Dias\n   </a>\n   is a nifty platform that offers RAG, advanced text-to-SQL, and multimodal inference as a service with a nifty UI.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/bennyschmidt/ragdoll\" rel=\"noreferrer noopener\">\n    Ragdoll\n   </a>\n   and\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/bennyschmidt/ragdoll-studio\" rel=\"noreferrer noopener\">\n    Ragdoll Studio\n   </a>\n   by bennyschmidt: Create AI Personas for characters, web assistants, or game NPCs using LlamaIndex TS, local LLMs, and image generation with Ollama and StabilityAI.\n  </li>\n </ul>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </h3>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://towardsdatascience.com/designing-rags-dbb9a7c1d729\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to Designing RAG Systems by\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://michaloleszak.medium.com/\" rel=\"noreferrer noopener\">\n    Micha\u0142 Oleszak\n   </a>\n   for an in-depth look at crucial design decisions in building efficient RAG systems, spanning five key areas: Indexing, Storing, Retrieval, Synthesis, and Evaluation.\n  </li>\n </ul>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </h3>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/palsujit\" rel=\"noreferrer noopener\">\n    Sujit Patil\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://sujitpal.blogspot.com/2024/03/hierarchical-and-other-indexes-using.html\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on combining semantic chunking with hierarchical clustering and indexing for RAG content enrichment.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Florian June's\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://ai.gopubby.com/advanced-rag-08-self-rag-c0c5b5952e0e\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on crafting a dynamic RAG system with integrated reflection, a guide to building Self-RAG from scratch.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Laurie's\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1773783011785585141?s=20\" rel=\"noreferrer noopener\">\n    video tutorial\n   </a>\n   on using LlamaParse's LLM-powered parsing turns complex insurance policies into clear yes-or-no statements, improving LLM responses on coverage queries.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/AkritiUpadhya13\" rel=\"noreferrer noopener\">\n    Akriti\u2019s\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/@akriti.upadhyay/building-real-time-financial-news-rag-chatbot-with-gemini-and-qdrant-64c0a3fbe45b\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on Building Real-Time Financial News RAG Chatbot with Gemini, and Qdrant.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Marco Bertelli's\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://python.plainenglish.io/deploying-a-production-ready-rag-server-a-comprehensive-guide-with-llamaindex-dbe57cc960df\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on deploying a RAG server for real-time use, and covering efficient embedding serving, concurrent request handling, and failure resilience.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/mesudarshan\" rel=\"noreferrer noopener\">\n    Sudarshan Koirala\u2019s\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=wCFXae8hiYA\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on building advanced PDF RAG with LlamaParse and purely local models for embedding, LLMs, and reranking.\n  </li>\n </ul>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  \ud83c\udfa5\n  <strong>\n   Webinars:\n  </strong>\n </h3>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://lu.ma/v1bdat63\" rel=\"noreferrer noopener\">\n    Register for a webinar\n   </a>\n   with\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.linkedin.com/in/tianjun-zhang-333bb2126/overlay/about-this-profile/?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base%3B1dQCZFffT4aXk6ePSYdUYg%3D%3D\" rel=\"noreferrer noopener\">\n    Tianjun Zhang\n   </a>\n   and\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.linkedin.com/in/shishir-patil/overlay/about-this-profile/?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base%3BNG6wPCQHTaWKxcdltRvvjw%3D%3D\" rel=\"noreferrer noopener\">\n    Shishir Patil\n   </a>\n   on how to do retrieval-augmented fine-tuning (RAFT).\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=TeEX7CoHT9k\" rel=\"noreferrer noopener\">\n    Webinar\n   </a>\n   with\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/dani_avila7\" rel=\"noreferrer noopener\">\n    Daniel\n   </a>\n   on\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://codegpt.co/\" rel=\"noreferrer noopener\">\n    CodeGPT\n   </a>\n   - a platform for AI Copilots that help your coding workflows, with components built on top of LlamaIndex components.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/vectara?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor\" rel=\"noreferrer noopener\">\n    Vectara\u2019s\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=R5pddHfUThQ&amp;t=351s\" rel=\"noreferrer noopener\">\n    Panel Discussion\n   </a>\n   on 'Why RAG will Never Die?\u2019.\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 13052, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "309648a9-822c-44ba-aa22-09311c2de138": {"__data__": {"id_": "309648a9-822c-44ba-aa22-09311c2de138", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-09.html", "file_name": "llamaindex-newsletter-2024-04-09.html", "file_type": "text/html", "file_size": 11147, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-09.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-09.html", "file_name": "llamaindex-newsletter-2024-04-09.html", "file_type": "text/html", "file_size": 11147, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "32f2f3de01380a514a19752b6a218d4f1c6ebabfef861f965c21c910b2fdc952", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Hello, LlamaIndex members! \ud83e\udd99\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Welcome to another thrilling weekly update from LlamaUniverse! We're excited to present a variety of outstanding updates, including Anthropic's Function Calling, Cookbooks, RankLLM, Guides, Tutorials, and much more.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Anthropic's Claude Function Calling Agent:\n   </strong>\n   Enhance QA/RAG and workflow automation with advanced tool calling in an agent framework.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/agent/anthropic_agent.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1776051869850476840\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    RankLLM Integration:\n   </strong>\n   RankLLM is an open-source LLM collection for reranking, surpassing GPT-4 based alternatives is now integrated with LlamaIndex.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/examples/node_postprocessor/rankLLM/\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1775166279911186930\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    LlamaIndex + MistralAI Cookbook Series:\n   </strong>\n   Launched a cookbook series with MistralAI for building diverse RAG applications, from basic to advanced, with distinctive methods and abstractions.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/mistralai/cookbook/tree/main/third_party/LlamaIndex\" rel=\"noreferrer noopener\">\n    Cookbooks\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1775977013054259210\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </h2>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have introduced the Anthropic\u2019s Claude Function Calling Agent, leveraging advanced tool calling capabilities within an agent framework for enhanced QA/RAG and workflow automation.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/agent/anthropic_agent.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1776051869850476840\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   RankLLM (by\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/rpradeep42\" rel=\"noreferrer noopener\">\n    Ronak Pradeep\n   </a>\n   ) integration with LlamaIndex - an open-source LLM collection fine-tuned for reranking, offering top-notch results and outperforming GPT-4 based rerankers.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/examples/node_postprocessor/rankLLM/\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1775166279911186930\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have launched the LlamaIndex + MistralAI Cookbook Series for creating a range of RAG applications, from simple setups to advanced agents, featuring unique abstractions and techniques.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/mistralai/cookbook/tree/main/third_party/LlamaIndex\" rel=\"noreferrer noopener\">\n    Cookbooks\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1775977013054259210\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We launched create-llama for building full-stack RAG/agent applications with a single CLI command, akin to create-react-app, for a comprehensive chatbot setup including tool use.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1776615916102463775\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83c\udfa5\u00a0Demos:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/Marker-Inc-Korea/AutoRAG\" rel=\"noreferrer noopener\">\n    AutoRAG\n   </a>\n   by Marker-Inc-Korea: Streamline RAG pipeline optimization with an automated three-step process, from data preparation to evaluation and optimal pipeline adoption, enhancing the efficiency of the RAG pipeline using LlamaIndex.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/examples/vector_stores/KDBAI_Advanced_RAG_Demo/\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to Building Advanced RAG with Temporal Filters: Learn how to enhance your RAG pipeline with time-based metadata for more effective financial report analysis using LlamaIndex and\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"http://KDB.AI\" rel=\"noreferrer noopener\">\n    KDB.AI\n   </a>\n   vector store.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/mistralai/cookbook/blob/main/third_party/LlamaIndex/Adaptive_RAG.ipynb\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to Adaptive RAG for dynamically selecting RAG strategies based on query complexity, enhancing efficiency across varying question types.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/lambdaEranga\" rel=\"noreferrer noopener\">\n     (\u03bbx.x)eranga\n    </a>\n    \u2019s\n   </strong>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/rahasak/build-rag-application-using-a-llm-running-on-local-computer-with-ollama-and-llamaindex-97703153db20\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on the step-by-step process for building RAG with local models (LlamaIndex, Ollama, HuggingFace Embeddings, ChromaDB) and wrapping it all in a Flask server.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/ivanilin9\" rel=\"noreferrer noopener\">\n    Ivan Ilin\n   </a>\n   \u2019s\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=9QqNVxlySGE\" rel=\"noreferrer noopener\">\n    video tutorial\n   </a>\n   on\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://t.co/fHXUEcF7sZ\" rel=\"noreferrer noopener\">\n    iki.ai\n   </a>\n   - an LLM-powered digital library, for organizing, and sharing information within teams or organizations.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.koyeb.com/tutorials/use-llamaindex-to-build-a-retrieval-augmented-generation-rag-application#introduction\" rel=\"noreferrer noopener\">\n    Tutorial\n   </a>\n   on scaling LLM Applications with Koyeb on deploying a full-stack RAG application globally without infrastructure setup, using Koyeb, LlamaIndex.TS, and TogetherAI.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/andysingal\" rel=\"noreferrer noopener\">\n    Ankush Singal\n   </a>\n   's\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://ai.gopubby.com/unlocking-the-power-of-multi-document-agents-with-llamaindex-d09e4d7dfe0e\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on Building Multi-Document Agents with LlamaIndex covers advanced multi-document agent concepts, where documents serve as sub-agents enabling complex QA, semantic search, and summarization.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/clusteredbytes\" rel=\"noreferrer noopener\">\n    Rohan\n   </a>\n   \u2019s\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/rsrohan99/rag-stream-intermediate-events-tutorial\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on building a Full-Stack RAG application that streams intermediate results to visual UI components with event queues and server-side events.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.linkedin.com/in/hanane-d-algo-trader/\" rel=\"noreferrer noopener\">\n    Hanane Dupouy\n   </a>\n   's\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.linkedin.com/posts/hanane-d-algo-trader_financial-agent-with-llamaindex-activity-7181531880146513920-mf9e/?utm_source=share&amp;utm_medium=member_desktop\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on building a Finance Agent using an LLM with Yahoo Finance and LlamaIndex abstractions to analyze financial data for publicly traded companies, covering everything from balance sheets to stock recommendations.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  \ud83c\udfa5\n  <strong>\n   Webinars:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=YY0VaSjPV1Y\" rel=\"noreferrer noopener\">\n    Webinar\n   </a>\n   with\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.linkedin.com/in/dhuynh95/\" rel=\"noreferrer noopener\">\n    Daniel Huynh\n   </a>\n   ****featuring LaVague - an agent that can navigate the web in your Jupyter/Colab notebook.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=t_xhaa-iTNw\" rel=\"noreferrer noopener\">\n    Webinar\n   </a>\n   with\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.linkedin.com/in/logankkelly/\" rel=\"noreferrer noopener\">\n    Logan Kelly\n   </a>\n   featuring\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.callsine.com/\" rel=\"noreferrer noopener\">\n    CallSine\n   </a>\n   that utilizes LlamaIndex abstractions and LLMs for personalized sales outreach.\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 11113, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "231ef033-a676-4456-9ec2-d998a21fddba": {"__data__": {"id_": "231ef033-a676-4456-9ec2-d998a21fddba", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-16.html", "file_name": "llamaindex-newsletter-2024-04-16.html", "file_type": "text/html", "file_size": 13388, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-16.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-16.html", "file_name": "llamaindex-newsletter-2024-04-16.html", "file_type": "text/html", "file_size": 13388, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "7ada839f5b9520d58fea4f72422a6ec7c5a3b03068958a3df0b689b7ffe508e3", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Hello, LlamaIndex Family! \ud83e\udd99\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Welcome to another thrilling weekly update from LlamaGalaxy! We're excited to bring you a variety of outstanding updates, including the Chain of Abstraction LlamaPack, create-tsi, demos, guides, tutorials, and much more.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Before we delve into these updates, we have an exciting tutorial series on Agents and Tools for you to check out. Perfect for beginners, this series covers everything from advanced QA/RAG implementations to step-wise execution. By the end, you\u2019ll have gained a deeper understanding of how to use agent reasoning with tool use to build simple applications. Check them out:\n </p>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=-AuHlVMyEA0\" rel=\"noreferrer noopener\">\n    Overview\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://youtu.be/pRUc6JPw6CY\" rel=\"noreferrer noopener\">\n    ReAct agents\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://youtu.be/6INvyrC4WrA\" rel=\"noreferrer noopener\">\n    Function Calling agents\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://youtu.be/K7h17Jjtbzg\" rel=\"noreferrer noopener\">\n    Retrieval-Augmented agent\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://youtu.be/gFRbkRtLGZQ\" rel=\"noreferrer noopener\">\n    Controlling tool outputs\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://youtu.be/JGkSxdPFgyQ\" rel=\"noreferrer noopener\">\n    Agents with step-by-step execution\n   </a>\n  </li>\n </ol>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Chain of Abstraction LlamaPack:\n   </strong>\n   Chain of Abstraction technique as llamapack a method enabling multi-step reasoning for enhanced tool use introduced by Silin Gao's team.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/agent/coa_agent.ipynb\" rel=\"noreferrer noopener\">\n    LlamaPack\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1778845258119524640\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Create-tsi Toolkit:\n   </strong>\n   Launched a toolkit for building full-stack RAG applications with customizable features like web crawling, local file indexing, and multilingual support, all hosted in EU data centers.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/telekom/create-tsi\" rel=\"noreferrer noopener\">\n    Code\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1778812761893650551\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Improved Agent Control\n   </strong>\n   :\n   <strong>\n    <code class=\"SanityPortableText_inlineCode__cI85z\">\n     return_direct\n    </code>\n   </strong>\n   feature in tools allows direct output returns, reducing costs and enhancing response efficiency.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/examples/agent/return_direct_agent/\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1778072285003550932\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </h2>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have introduced the Chain of Abstraction Technique Developed by Silin Gao, and team as LlamaPack, this new method enables LLMs to generate multi-step reasoning chains for efficient sequence planning, enhancing tool use beyond single-shot functions.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/agent/coa_agent.ipynb\" rel=\"noreferrer noopener\">\n    LlamaPack\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1778845258119524640\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have launched create-tsi: A toolkit in collaboration with T-Systems and Marcus Schiesser to generate GDPR-compliant, full-stack AI applications via a CLI interface. Build enterprise-grade RAG bots with customizable features like web crawling, local file indexing, and multilingual support, all hosted in EU data centers.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/telekom/create-tsi\" rel=\"noreferrer noopener\">\n    Code\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1778812761893650551\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have introduced\n   <strong>\n    <code class=\"SanityPortableText_inlineCode__cI85z\">\n     return_direct\n    </code>\n   </strong>\n   feature in tools that enhances agent controllability by allowing direct output returns as final responses. This optimizes for reduced latency and costs, and effectively halts the agent after crucial actions like booking confirmations or answering queries.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/examples/agent/return_direct_agent/\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1778072285003550932\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83c\udfa5\u00a0Demos:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1777851305308102845\" rel=\"noreferrer noopener\">\n    RAG-enhanced MetaGPT\n   </a>\n   : A robust multi-agent framework that features structured team dynamics for problem-solving, now supercharged with domain-specific knowledge from LlamaIndex modules. This framework supports diverse data inputs, sophisticated retrieval options, and efficient data management for enhanced agent performance.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://towardsdatascience.com/mastering-rag-systems-from-fundamentals-to-advanced-with-strategic-component-evaluation-3551be31858f\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to Building and Evaluating Advanced RAG by Hamza Gharbi for setting up a basic RAG pipeline, defining custom evaluation functions, and optimizing retrieval techniques.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://arxiv.org/abs/2403.11996\" rel=\"noreferrer noopener\">\n    Paper\n   </a>\n   by Prof.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/ProfBuehlerMIT\" rel=\"noreferrer noopener\">\n    Markus J. Buehler\n   </a>\n   : Using LLM-Generated Knowledge Graphs to Accelerate Biomaterials Discovery - This study showcases how a comprehensive knowledge graph from over 1000 scientific papers reveals novel insights and connections, driving innovation in biomaterials through art as inspiration. The KG construction was done with the help of LlamaIndex modules.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://aws.plainenglish.io/rag-implementation-using-aws-bedrock-and-llamaindex-62b346fd0156\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to Full-Stack RAG Application with AWS Bedrock: Set up Bedrock embeddings, use LlamaIndex for PDF retrieval, and build an interactive Streamlit interface, an ideal resource for enterprises starting with AWS services.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/examples/pipeline/query_pipeline_memory/\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to Building a Lightweight ColBERT Retrieval Agent: Learn how to create an agent capable of advanced document retrieval and maintaining conversation memory, without the complexity of heavyweight agent frameworks.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://arxiv.org/pdf/2404.01037.pdf\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to the Best RAG Techniques: 'ARAGOG'\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://arxiv.org/pdf/2404.01037.pdf\" rel=\"noreferrer noopener\">\n    paper\n   </a>\n   by Matous Eibich is a comprehensive evaluation survey exploring various RAG methods from classic vector databases to LlamaIndex's advanced techniques. Key findings highlight the effectiveness of HyDE, LLM reranking, and sentence window retrieval for improving precision and answer similarity.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://akash-mathur.medium.com/\" rel=\"noreferrer noopener\">\n    Akash Mathur\n   </a>\n   \u2019s\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://akash-mathur.medium.com/data-management-in-llamaindex-smart-tracking-and-debugging-of-document-changes-7b81c304382b\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on Data Management in LlamaIndex: Featuring LlamaCloud and its open-source counterpart, this tutorial showcases efficient live data handling to minimize costs and latency in LLM applications.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/helloiamleonie\" rel=\"noreferrer noopener\">\n    Leonie\n   </a>\n   \u2019s interactive\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://lightning.ai/weaviate/studios/chat-with-your-code-rag-with-weaviate-and-llamaindex\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   to create an app that lets you converse with code from a GitHub repository.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/kingzzm\" rel=\"noreferrer noopener\">\n    kingzzm\u2019s\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://generativeai.pub/advanced-rag-retrieval-strategies-auto-merging-retrieval-dc3f869654c4\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on enhancing RAG Performance to overcome the issue of 'broken' context in RAG construction by dynamically creating contiguous chunks with auto-merging retrieval.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/activeloop\" rel=\"noreferrer noopener\">\n    Activeloop\n   </a>\n   \u2019s\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.activeloop.ai/resources/ai-pill-identifier/\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on Multimodal RAG for Pill Search teaches how to identify pills using images and text. This helps in identifying unknown pills, checking drug interactions and side effects, and confirming proper dosage amounts.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Fanghua Yu's\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/@yu-joshua/using-llamaparse-for-knowledge-graph-creation-from-documents-3bd1e1849754\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on using LlamaParse for Knowledge Graph Creation from Documents.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  \ud83c\udfa5\n  <strong>\n   Webinars:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=pira_p6aRVA\" rel=\"noreferrer noopener\">\n    Webinar\n   </a>\n   with\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/tianjun_zhang\" rel=\"noreferrer noopener\">\n    Tianjun Zhang\n   </a>\n   and\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/shishirpatil_\" rel=\"noreferrer noopener\">\n    Shishir Patil\n   </a>\n   , the two lead co-authors of RAFT, where they presented RAFT and discussed fine-tuning and RAG.\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 13353, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fa6018c4-71e9-4a2b-a80d-629135ec97a9": {"__data__": {"id_": "fa6018c4-71e9-4a2b-a80d-629135ec97a9", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-23.html", "file_name": "llamaindex-newsletter-2024-04-23.html", "file_type": "text/html", "file_size": 10277, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-23.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-23.html", "file_name": "llamaindex-newsletter-2024-04-23.html", "file_type": "text/html", "file_size": 10277, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "154bb75d7342a83305bed8d463206a8a7b6a8b3b86a7a928f150566d312ff86e", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Hello LlamaIndex Community! \ud83e\udd99\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Welcome to another thrilling weekly update from LlamaWorld! We're excited to bring you a variety of outstanding updates, including Cookbooks, demos, guides, and tutorials.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    MistralAI's 8x22b Model Cookbook:\n   </strong>\n   Released cookbook for MistralAI's 8x22b model with detailed guidance on RAG, query routing, and tool applications.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/examples/cookbooks/mistralai/\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1780646484712788085\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Llama 3 Model Cookbook:\n   </strong>\n   A comprehensive cookbook for Meta's Llama 3 model from simple prompt runs to complex RAG pipeline, agents and tools, accessible directly from Hugging Face.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/examples/cookbooks/llama3_cookbook/\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1781039161325293981\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    create-llama Llama 3 template\n   </strong>\n   : create-llama template for Meta's Llama 3 to quickly start building full-stack LLM applications using the\n   <strong>\n    <code class=\"SanityPortableText_inlineCode__cI85z\">\n     nextjs-llama3\n    </code>\n   </strong>\n   template with a single CLI command.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/jerryjliu0/status/1781843300938666050\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </h2>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have released a cookbook for the latest MistralAI model, the powerful 8x22b, which sets a new standard for open models. The cookbook covers RAG, query routing, and tool use cases.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/examples/cookbooks/mistralai/\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1780646484712788085\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have released a cookbook for latest Meta's new Llama 3 model, available directly from Hugging Face. This guide covers everything from running basic prompts to setting up a full RAG pipeline, agents and tools.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/examples/cookbooks/llama3_cookbook/\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1781039161325293981\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have introduced a template for integrating Meta's Llama 3 in create-llama. Simply run\n   <strong>\n    <code class=\"SanityPortableText_inlineCode__cI85z\">\n     npx create-llama\n    </code>\n   </strong>\n   and select the\n   <strong>\n    <code class=\"SanityPortableText_inlineCode__cI85z\">\n     nextjs-llama3\n    </code>\n   </strong>\n   template to build full-stack LLM application with Llama 3 in one CLI command.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/jerryjliu0/status/1781843300938666050\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83c\udfa5\u00a0Demos:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/rsrohan99/ai-diagram-generator\" rel=\"noreferrer noopener\">\n    Open Source AI Diagram Generator\n   </a>\n   by\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/clusteredbytes\" rel=\"noreferrer noopener\">\n    Rohan\n   </a>\n   using LlamaIndex's Pydantic program with partial JSON parsing and Vercel AI SDK to generate and stream diagrams dynamically for an enhanced user experience.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/aishwaryaprabhat/goku/tree/main/goku/dream\" rel=\"noreferrer noopener\">\n    DREAM\n   </a>\n   : A Distributed RAG Experimentation Framework by Aishwarya Prabhat, featuring a full-stack blueprint for optimizing RAG setups in a distributed environment. This setup includes Ray for computing, LlamaIndex for advanced techniques, Ragas for synthetic data, MinIO, MLflow, Project Jupyter, and ArgoCD.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://firecrawl.dev/?ref=github\" rel=\"noreferrer noopener\">\n    Firecrawl\n   </a>\n   from\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.mendable.ai/\" rel=\"noreferrer noopener\">\n    Mendable\n   </a>\n   is an API service that crawls a given URL and converts its content, including all accessible subpages, into clean markdown format. It utilizes LlamaParse from LlamaIndex for PDF parsing.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://qdrant.tech/documentation/examples/hybrid-search-llamaindex-jinaai/\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to integrating Qdrant Hybrid Cloud with LlamaIndex, featuring JinaAI embeddings, MistralAI's Mixtral 8x7b, and our LlamaParse document parser.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.elastic.co/search-labs/blog/rag-with-llamaIndex-and-elasticsearch\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to building RAG using completely open and free components from Elastic, featuring Ollama and MistralAI, demonstrates how to assemble a RAG application with LlamaIndex using entirely free software.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=JLmI0GJuGlY\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to Building a Code-Writing Agent:\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/TechWithTimm\" rel=\"noreferrer noopener\">\n    TechWithTimm\n   </a>\n   demonstrated how to create an agent that writes code by reading your documentation. Learn how to set up local LLMs with Ollama, parse documentation using LlamaParse, build an agent, and teach it to write code.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/@diagnosta/lora-fine-tuning-of-embedding-models-using-llamaindex-a60b823a2c94\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to Fine-tuning Embedding Models for RAG with LoRA by Mariboo demonstrates how to enhance Hugging Face models using LlamaIndex's finetuning techniques, including steps from quantization to fine-tuning with QLoRA.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Khye Wei's\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/advanced-rag-with-azure-ai-search-and-llamaindex/ba-p/4115007\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   from Microsoft demonstrates how to use LlamaIndex with Azure's AI Search to create powerful RAG applications, including Hybrid Search, Query Rewriting, and SubQuestionQuery Engine.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.linkedin.com/in/hanane-d-algo-trader/\" rel=\"noreferrer noopener\">\n    Hanane Dupouy\n   </a>\n   's\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.linkedin.com/posts/hanane-d-algo-trader_react-financial-agent-llamaindex-activity-7186333474256035840-jyQV/?utm_source=share&amp;utm_medium=member_desktop\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on Building a Finance Agent with LlamaIndex to query public companies with tools for looking up stock prices, summarizing financial news, and plotting stock data, all streamlined through LlamaIndex's ReAct agent and API abstractions.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/andysingal\" rel=\"noreferrer noopener\">\n    Andy Singal\n   </a>\n   's\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://ai.gopubby.com/enhancing-document-retrieval-with-memory-a-tutorial-for-llamaindex-with-colbert-based-agent-1c3c47461122\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on Building a ColBERT-powered Retrieval Agent with Memory demonstrates how to enhance a RAG pipeline with \"state\" storage for a more personalized, conversational assistant using LlamaIndex's custom agent and query pipeline abstractions.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Mariboo\u2019s\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/@diagnosta/lora-fine-tuning-of-embedding-models-using-llamaindex-a60b823a2c94\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on Fine-tuning Embedding Models for RAG with LoRA using LlamaIndex's finetuning abstractions.\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 10253, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "017f7ad5-1379-4285-8155-0a6ccf3c4800": {"__data__": {"id_": "017f7ad5-1379-4285-8155-0a6ccf3c4800", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-30.html", "file_name": "llamaindex-newsletter-2024-04-30.html", "file_type": "text/html", "file_size": 8240, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-30.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-30.html", "file_name": "llamaindex-newsletter-2024-04-30.html", "file_type": "text/html", "file_size": 8240, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "90facaf75d350d9eace318d69e723aaf16df303a67660133a0d4f99af73cac65", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Greetings, LlamaIndex fans! \ud83e\udd99\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  It\u2019s delightful springtime weather out here in San Francisco and we hope you\u2019re having a good day! Check out this week\u2019s summary of news, guides and tutorials.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Day 0 support for Microsoft\u2019s Phi-3 Mini!\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1782893301214986593\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   create-llama now supports Llama 3 and Phi-3 and has lots of new features!\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/MarcusSchiesser/status/1783049713601933487\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Simon was on a security podcast!\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1783963718256411126\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </p>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Jina AI released powerful new open-source rerankers and we have day 0 support as usual!\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1782531355970240955\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Phi-3 mini was released by Microsoft, a powerful new small model, and we\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1782870458121282003\" rel=\"noreferrer noopener\">\n    put it through its paces\n   </a>\n   (spoiler: it\u2019s good!) and released day-0 support via Ollama!\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1782893301214986593\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Our create-llama application generator was updated with many features including being able to show the sources it retrieved from, as well as Llama3 and Phi-3 support. Build an app from scratch in 30 seconds!\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/MarcusSchiesser/status/1783049713601933487\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Language Agent Tree Search (LATS) is a powerful new technique that iteratively plans out an array of potential futures, interleaving tool use and reflection to solve problems. We released a Llama Pack implementation.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1783147291882443112\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n </ol>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   \ud83c\udfa5\u00a0Demos:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Memary\n   </strong>\n   is a reference implementation of using long-term memory in knowledge graph form for building agents.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1784604356224164186\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Our hackathon winners wrote a blog post about their winning project, a knowledge-retrieval bot trained on documentation, including how they built it.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1785060253995851992\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Co-founder Jerry shared his latest deck, a guide to building a context-augmented research assistant that enables multi-hop Q&amp;A, reflection and more.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.google.com/presentation/d/1IWjo8bhoatWccCfGLYw_QhUI4zfF-MujN3ORIDCBIbc/edit#slide=id.g27749d82178_0_92\" rel=\"noreferrer noopener\">\n    Slides\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/jerryjliu0/status/1782428884287578213\" rel=\"noreferrer noopener\">\n    tweet\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Corrective RAG or CRAG adds a retrieval evaluation module that determines whether the retrieved context is \u201ccorrect\u201d and improves retrieval. Check out this guide on how to build it step-by-step!\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1782799757376963006\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Jerry also went in-depth on the ingredients necessary for building a complex agent.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/jerryjliu0/status/1784279431739265439\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Michael of KX Systems demonstrated making retrieval a multi-hop process for better results.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1784363604340576615\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   A reference architecture for advanced RAG with LlamaIndex and AWS Bedrock.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1784962053641478454\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Build a best-in-class RAG application using Qdrant as a vector store, Jina AI embeddings, and Mixtral 8x7b as the LLM.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1783601807903863184\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Learn 3+ patterns for building LLM apps on AWS with LlamaIndex.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1783877951278432733\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   A 9-part series on taking RAG from prototype to production.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1784257178758697272\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  \ud83c\udfa5\n  <strong>\n   Webinars:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   KX Systems are hosting a webinar on May 1 about getting the most out of LlamaParse!\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1783622871614664990\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Co-founder Simon appeared on the MLSecOps podcast talking about security in LLM applications.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1783963718256411126\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  \ud83d\udc6f\u200d\u2640\ufe0f\n  <strong>\n   Community:\n  </strong>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We launched a\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1783286375640613300\" rel=\"noreferrer noopener\">\n   LlamaIndex user group in Korea\n  </a>\n  !\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 8192, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60ecb273-470b-431a-a3ec-512ae5c68015": {"__data__": {"id_": "60ecb273-470b-431a-a3ec-512ae5c68015", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-07.html", "file_name": "llamaindex-newsletter-2024-05-07.html", "file_type": "text/html", "file_size": 11641, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-07.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-07.html", "file_name": "llamaindex-newsletter-2024-05-07.html", "file_type": "text/html", "file_size": 11641, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "820354584311ce412620c5c072434ea5648c378e36584e5f5a18242a10eb02ff", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Hello LlamaIndex fam! \ud83e\udd99\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  If you\u2019re in SF, join us for the first-ever\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/cerebral_valley/status/1785366241030607209\" rel=\"noreferrer noopener\">\n   Llama 3 Hackathon\n  </a>\n  (\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://partiful.com/e/p5bNF0WkDd1n7JYs3m0A\" rel=\"noreferrer noopener\">\n   invitation here\n  </a>\n  )! Shack15 is an amazing venue and it\u2019s sure to be a fun time. If you can\u2019t make it, stay tuned for the rundown on the cool projects that come out of the event. Now, on to the highlights:\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1785722400480637291\" rel=\"noreferrer noopener\">\n    LlamaIndex.TS hits v0.3!\n   </a>\n   Loads of new features inside!\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1786426320731488692\" rel=\"noreferrer noopener\">\n    LlamaIndex Python hits v0.10.34\n   </a>\n   ! A bumper release!\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   That\u2019s a lot!\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Two big releases this week!\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   LlamaIndex.TS hit version 0.3!\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1785722400480637291\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://ts.llamaindex.ai/blog/welcome-llamaindexts-v0.3\" rel=\"noreferrer noopener\">\n    Blog post\n   </a>\n   <ul>\n    <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n     Features:\n     <ul>\n      <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n       Agent support including ReAct, Anthropic and OpenAI agents, as well as a generic AgentRunner class\n      </li>\n      <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n       Standardized Web Streams compatible with React 19, Deno, and Node 22\n      </li>\n      <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n       More comprehensive type system\n      </li>\n      <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n       Enhanced support for deployment on Next.js, Deno, Cloudflare Workers and Waku\n      </li>\n     </ul>\n    </li>\n   </ul>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   LlamaIndex Python hit version 0.10.34!\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1786426320731488692\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   <ul>\n    <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n     Features:\n     <ul>\n      <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n       <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/examples/agent/introspective_agent_toxicity_reduction/\" rel=\"noreferrer noopener\">\n        Introspective agents that work through reflection\n       </a>\n      </li>\n      <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n       <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/examples/llm/huggingface/\" rel=\"noreferrer noopener\">\n        Support for huggingface text-generation-inference API\n       </a>\n      </li>\n      <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n       <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/examples/agent/structured_planner/\" rel=\"noreferrer noopener\">\n        Structured planning agent\n       </a>\n      </li>\n      <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n       <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/examples/memory/ChatSummaryMemoryBuffer/\" rel=\"noreferrer noopener\">\n        A chat summary memory buffer\n       </a>\n      </li>\n      <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n       <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://llamahub.ai/l/llms/llama-index-llms-mistral-rs\" rel=\"noreferrer noopener\">\n        Rust-based LLM support with Mistral.rs\n       </a>\n      </li>\n      <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n       <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/examples/vector_stores/MilvusHybridIndexDemo/\" rel=\"noreferrer noopener\">\n        Milvus sparse hybrid search\n       </a>\n      </li>\n      <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n       <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/examples/vector_stores/FirestoreVectorStore/\" rel=\"noreferrer noopener\">\n        Google Firestore vector store support\n       </a>\n      </li>\n     </ul>\n    </li>\n   </ul>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   A new LlamaPack for the Reflection Agentic Pattern.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/_nerdai_/status/1786486394996617341\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   \ud83c\udfa5\u00a0Demos:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Filter AirBnB listings using natural language with this open-source demo! It uses Mistral AI\u2019s Mixtral 8x7b and Qdrant engine, plus Streamlit to build UI.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1786138653271671257\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://harshadsuryawanshi.medium.com/exploring-airbnb-listings-with-semantic-search-a-qdrant-and-llm-powered-approach-a65cd170f710\" rel=\"noreferrer noopener\">\n    Blog post\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Fully local RAG with Llama 3, Ollama and LlamaIndex! A short, sweet guide.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1787164022565183849\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://blog.gopenai.com/improved-rag-with-llama3-and-ollama-c17dc01f66f6\" rel=\"noreferrer noopener\">\n    Blog post\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Fine-tune your embedding model using labels from a reranker.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1787296562504425903\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/rahasak/optimizing-rag-supervised-embeddings-reranking-with-your-data-with-llamaindex-88344ff89da7\" rel=\"noreferrer noopener\">\n    Blog post\n   </a>\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Hanane Dupouy walks us through building an agent that can perform complex financial calculations.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1785325832317415641\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.linkedin.com/posts/hanane-d-algo-trader_anthropic-agent-rag-with-complex-financial-activity-7190389324343881728-TOXX/?utm_source=share&amp;utm_medium=member_desktop\" rel=\"noreferrer noopener\">\n    Slides\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Plaban Nayak sets up a local, open-source RAG pipeline that uses Llama 3 and Qdrant to demonstrate how to improve the accuracy of your RAG with reranking.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1786093311658451337\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://nayakpplaban.medium.com/build-an-advanced-reranking-rag-system-using-llama-index-llama-3-and-qdrant-a8b8654174bc\" rel=\"noreferrer noopener\">\n    Blog post\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Jason Zhou talks about the components needed for agentic RAG.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1786830550441099524\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Divyanshu Dixit walks us through agents dedicated to workflow automation.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/divyanshu_van/status/1786786672648110415\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://div.beehiiv.com/p/need-talk-agents\" rel=\"noreferrer noopener\">\n    Blog post\n   </a>\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Tyler Hutcherson of Redis and our own Laurie Voss walk you through building agentic RAG with semantic caching and other production-ready techniques.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=mTNiGfYfdWY&amp;t=5s\" rel=\"noreferrer noopener\">\n    Video\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/redis-developer/agentic-rag/blob/main/Agentic_RAG_Redis_LlamaIndex.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Cleanlab has a tutorial on getting trustworthiness scores from your RAG pipeline to allow you to avoid hallucinations and course-correct.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1786914595342589978\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://help.cleanlab.ai/tutorials/tlm_rag/\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  \ud83c\udfa5\n  <strong>\n   Webinars:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   On May 8 we\u2019ll be co-hosting a webinar with Pulumi on deploying AI applications to AWS.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1785456924852715907\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Our own Andrei and friends walk you from basic RAG through handling long-context RAG all the way to evaluating your RAG pipeline.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/llama_index/status/1785802821604127205\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=bNqSRNMgwhQ\" rel=\"noreferrer noopener\">\n    Video\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/nerdai/talks/blob/main/2024/mlops/mlops-rag-bootcamp.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 11604, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64c022dc-191e-4e0c-b783-6676845d219e": {"__data__": {"id_": "64c022dc-191e-4e0c-b783-6676845d219e", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-14.html", "file_name": "llamaindex-newsletter-2024-05-14.html", "file_type": "text/html", "file_size": 8288, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-14.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-14.html", "file_name": "llamaindex-newsletter-2024-05-14.html", "file_type": "text/html", "file_size": 8288, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "7da1d0a5d4a1f3317ea48d089cc1443328368e2b84dd3bb68383fd988d09e936", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Hello LlamaIndex Family! \ud83e\udd99\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Welcome to another thrilling weekly update from LlamaIndex! We're excited to share a variety of outstanding updates, guides, and tutorials with you. But first, we have an exciting announcement to make.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We are thrilled to announce a new\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.deeplearning.ai/short-courses/building-agentic-rag-with-llamaindex/?utm_campaign=llamaindexC2-launch&amp;utm_content=292515650&amp;utm_medium=social&amp;utm_source=twitter&amp;hss_channel=tw-992153930095251456\" rel=\"noreferrer noopener\">\n   course\n  </a>\n  in collaboration with DeepLearningAI\u2014Building Agentic RAG. In this course, you\u2019ll learn how to build a research assistant that can reason over multiple documents and answer complex questions. You\u2019ll also learn how to step through the execution of the agent and steer it with human feedback.\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.deeplearning.ai/short-courses/building-agentic-rag-with-llamaindex/?utm_campaign=llamaindexC2-launch&amp;utm_content=292515650&amp;utm_medium=social&amp;utm_source=twitter&amp;hss_channel=tw-992153930095251456\" rel=\"noreferrer noopener\">\n   Check it out\n  </a>\n  and take your RAG skills to the next level!\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </h2>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Day 0 Support for GPT-4o\n   <strong>\n    -\n   </strong>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1790081409039872070\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Llama3 Cookbook\n   <strong>\n    -\n   </strong>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1790047097024348444\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   TypeScript Agent\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/ts-agents\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   <strong>\n    .\n   </strong>\n  </li>\n </ol>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </h2>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have introduced day 0 support for GPT-4o in both Python and TypeScript. Additionally, we've created demo notebooks (\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://colab.research.google.com/drive/1CTIs9C6HLqKLqU_PpG1NXaZaH0Ja5pzg#scrollTo=FB55THvgVWXs\" rel=\"noreferrer noopener\">\n    demo1\n   </a>\n   and\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://colab.research.google.com/drive/1CTIs9C6HLqKLqU_PpG1NXaZaH0Ja5pzg#scrollTo=0ZTqYzIl2DU1\" rel=\"noreferrer noopener\">\n    demo2\n   </a>\n   ) to help you easily experiment with GPT-4o using LlamaIndex.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1790081409039872070\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have launched Llama3 cookbooks showcasing interesting use cases for Llama 3, from basic chat functionalities to advanced agent development. Ideal for anyone building with local models, either on your laptop or through an API.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/llamaindex_cookbook.ipynb\" rel=\"noreferrer noopener\">\n    Notebook,\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1790047097024348444\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/ts-agents\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to building agents in TypeScript: Dive into our comprehensive, open-source guide developed by Laurie that walks you through every step of agent development, from setting up with basic functions to integrating advanced features like local and remote LLMs, and data querying with vectorDB.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1789694066382537079\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to using RAG for content moderation:\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/cloudraftio\" rel=\"noreferrer noopener\">\n    CloudRaft\n   </a>\n   shows how to set up a RAG pipeline to moderate user-generated images effectively, ensuring compliance with predefined rules through techniques like semantic search and efficient inferencing with small LLMs.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/kxsystems\" rel=\"noreferrer noopener\">\n    Kxsystems\n   </a>\n   advanced workshop on \"Building Advanced RAG over Complex PDFs with LlamaParse\" to demonstrate how LlamaParse can tackle the challenge of extracting diverse elements like text, tables, images, and graphs from complex research papers.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=EL9lCOgLR58\" rel=\"noreferrer noopener\">\n    Video Tutorial\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/kx-systems/rag-llamaparse-advanced-pdf-parsing-for-retrieval-c393ab29891b\" rel=\"noreferrer noopener\">\n    BlogPost\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://colab.research.google.com/github/KxSystems/kdbai-samples/blob/main/LlamaParse_pdf_RAG/llamaParse_demo.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/naivebaesian\" rel=\"noreferrer noopener\">\n    Arslan Shahid\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/firebird-technologies/generate-powerpoints-using-llama-3-a-first-step-in-automating-slide-decks-536f5fcb6e0e\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on Generating PowerPoints with Llama 3, using LlamaIndex to create a Llama3 RAG pipeline. The approach not only answers questions but also generates PowerPoint slide decks by utilizing the python-pptx library to write code programmatically for slide creation.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.linkedin.com/in/hanane-d-algo-trader/\" rel=\"noreferrer noopener\">\n    Hanane Dupouy\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.linkedin.com/posts/hanane-d-algo-trader_introspective-agent-llamaindex-financial-activity-7193317247342243841-kqor/?utm_source=share&amp;utm_medium=member_desktop\" rel=\"noreferrer noopener\">\n    demonstrates\n   </a>\n   Building a Financial Agent that can Perform Reflection. The approach helps in analyzing stock prices by implementing two types of reflection: CRITIC (tool use) and self-reflection (no tools).\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/kingzzm\" rel=\"noreferrer noopener\">\n    zhaozhiming\u2019s\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://generativeai.pub/llamaindex-and-rag-evaluation-tools-59bae2944bb3\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on evaluating RAG systems, utilizing evaluation libraries like TruLens, Ragas, UpTrain, and DeepEval to assess RAG systems using metrics such as faithfulness, relevance, and answer correctness.\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 8262, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e40d8b2d-e098-40e0-88d3-a53b9ae8308b": {"__data__": {"id_": "e40d8b2d-e098-40e0-88d3-a53b9ae8308b", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-21.html", "file_name": "llamaindex-newsletter-2024-05-21.html", "file_type": "text/html", "file_size": 9430, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-21.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-21.html", "file_name": "llamaindex-newsletter-2024-05-21.html", "file_type": "text/html", "file_size": 9430, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "d348b4b726a8cac38d5ecd5960ed8973c14f2815718aee48538914f3e6f68130", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Hello LlamaIndex Community! \ud83e\udd99\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Welcome to another exciting weekly update from LlamaIndex! Last week was a standout in the AI world with significant updates on GPT-4o and Gemini models. We're thrilled to bring you a host of exceptional integration updates, guides, tutorials, webinars, and events.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    LlamaIndex on Vertex AI (Google Cloud):\n   </strong>\n   Introducing the new RAG API powered by advanced LlamaIndex modules on Vertex AI (Google Cloud).\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/llamaindex-on-vertexai\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1790768330099580940\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Enhanced Document Parsing with GPT-4o:\n   </strong>\n   Integrated GPT-4o in LlamaParse for superior document parsing.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_parse/blob/main/examples/test_tesla_impact_report/test_gpt4o.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1791145604955152767\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Cookbook on Structured Image Extraction with GPT-4o:\n   </strong>\n   Check out our new cookbook on using GPT-4o for Structured Image Extraction.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/multi_modal/gpt4o_mm_structured_outputs.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1791258285993230786\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </h2>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have launched LlamaIndex on Vertex AI (Google Cloud) to introduce the new RAG API, enhanced by LlamaIndex's advanced modules. This integration simplifies setup and enhances user access for the developers with the flexibility to connect a variety of data sources and file types. It fully supports the latest LLMs, including Gemini 1.5 Flash, Gemini 1.5 Pro, and Gemini 1.0 models.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/llamaindex-on-vertexai\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1790768330099580940\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have introduced GPT-4o with LlamaParse, offering enhanced document parsing into markdown for complex files, ensuring higher data quality for your RAG pipeline. Note the increased cost of $0.60 USD per page. Note the increased cost of $0.60 USD per page compared to the standard $0.003 per page.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_parse/blob/main/examples/test_tesla_impact_report/test_gpt4o.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1791145604955152767\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have released a cookbook on using GPT-4o for Structured Image Extraction, showing how to convert images into structured JSONs with a 0% failure rate and higher quality than GPT-4V.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/multi_modal/gpt4o_mm_structured_outputs.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1791258285993230786\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   LlamaParse integration with Quivr to enhance document parsing capabilities. Now, you can easily process complex documents like PDFs, PPTX, and Markdown files, ensuring clean data storage and accurate retrieval in your personalized AI assistants.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.quivr.app/configuring/llamaparse\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1790880249049485313\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_parse/blob/main/examples/caltrain/caltrain_text_mode.ipynb\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to Enhanced QA with LlamaParse on complex tables like train schedules. This approach uses spatial text layout and GPT-4o to preserve essential information, ensuring accurate and error-free data interpretation.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://jina.ai/news/binary-embeddings-all-the-ai-3125-of-the-fat/\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to Speeding Up Vector Search with Minimal Accuracy Loss using Jina Embeddings to achieve 32x faster vector search performance at just a 4% cost in accuracy. It involves encoding your data as binary digits, significantly reducing storage and compute requirements.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Kate Silverstein\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on building local research assistant**,** set up a local, private research assistant on your laptop effortlessly with Mozilla's llamafile. Tutorial covers everything from downloading and activating the model, to connecting via LlamaIndex and managing your data.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Plaban Nayak\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/the-ai-forum/multi-document-agentic-rag-using-llama-index-and-mistral-b334fa45d3ee\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on Multi-document Agentic RAG using Llama-Index and Mistral.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Diptiman Raichaudhuri\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://diptimanrc.medium.com/text2sql-opensource-duckdb-nsql-7b-with-ollama-and-llamaindex-on-local-setup-6f266f78bc4f\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on fully local Text-to-SQL using DuckDB as the database, Ollama + Mixtral-8x7B as the model, and LlamaIndex for text-to-SQL orchestration.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Mandar Karhade\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://pub.towardsai.net/how-to-optimize-chunk-sizes-for-rag-in-production-fae9019796b6\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on showing an end-to-end experimentation pipeline for iterating on chunk sizes, generating a synthetic dataset, and measuring how it affects evaluation metrics.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\udcf9\u00a0Webinar:\n  </strong>\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Join us for a\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://lu.ma/nzh3o83f\" rel=\"noreferrer noopener\">\n   webinar\n  </a>\n  on \"Open-Source Longterm Memory for Autonomous Agents\" this Thursday at 9am PT, where we'll explore the memary architecture with Julian Saks, Kevin Li, Seyeong Han and rest of memary team, diving deep into the challenges and future of long-term memory for autonomous systems.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\udcc5\u00a0Events:\n  </strong>\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We are having our first-ever\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://partiful.com/e/IwbFDzAX0xPCPsC897SX\" rel=\"noreferrer noopener\">\n   meetup\n  </a>\n  at our new office in San Francisco!\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://partiful.com/e/IwbFDzAX0xPCPsC897SX\" rel=\"noreferrer noopener\">\n   Join us\n  </a>\n  to connect with our team and friends from Activeloop and Tryolabs, as we discuss the latest developments in generative AI.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 9404, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d064c70-90b1-4465-97e2-2eabafc0feed": {"__data__": {"id_": "4d064c70-90b1-4465-97e2-2eabafc0feed", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-28.html", "file_name": "llamaindex-newsletter-2024-05-28.html", "file_type": "text/html", "file_size": 9205, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-28.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-28.html", "file_name": "llamaindex-newsletter-2024-05-28.html", "file_type": "text/html", "file_size": 9205, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "a756c3a770c3f2cbde542e907b61ff2e90d824b614db8c3991f16ac5c9f694b8", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Greetings, LlamaIndex Family! \ud83e\udd99\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Welcome to your latest weekly update from LlamaIndex! We're excited to present a variety of outstanding integration updates, detailed guides, demos, educational tutorials, and informative webinars this week.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Secure Code Execution with AzureCodeInterpreterTool:\n   </strong>\n   Securely run LLM-generated code with Azure Container Apps, integrated with LlamaIndex for safe code execution.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Build Automated Email Agents:\n   </strong>\n   Create email agents with MultiOn and LlamaIndex that autonomously read, index, and respond to emails.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    LlamaFS for Organized Files:\n   </strong>\n   Alex Reibman's team developed LlamaFS to automatically structure messy file directories, enhanced by Llama 3 and Groq Inc.'s API.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    RAGApp's No-Code Chatbots:\n   </strong>\n   Deploy RAG chatbots easily with RAGApp's no-code interface, fully open-source and cloud-compatible.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </h2>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have launched Azure Container Apps dynamic sessions to securely run LLM-generated code in a sandbox. Integrated into LlamaIndex, this feature ensures safe execution of complex code tasks by your agents. Set up a session pool on Azure, add the AzureCodeInterpreterTool to your agent, and you\u2019re ready to go.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions\" rel=\"noreferrer noopener\">\n    Blogpost\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1792958928357335115\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have integrated with the open source Nomic embed, now fully operable locally. This integration allows for completely local embeddings and introduces a dynamic inference mode that optimizes embedding latency. The system automatically selects between local and remote embeddings based on speed, ensuring optimal performance.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/examples/embeddings/nomic/\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1793677965978673598\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have integrated the Vespa vector store, supporting hybrid search with BM25.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/examples/vector_stores/VespaIndexDemo/\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1794106979213869413\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have integrated with MyMagic AI to facilitate batch data processing for GenAI applications. This setup allows you to pre-process large datasets with an LLM, enabling advanced analysis and querying capabilities.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1793385512386150856\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/automate-online-tasks-with-multion-and-llamaindex\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to building an automated Email Agent with MultiOn and LlamaIndex that can autonomously read and index emails for easy retrieval and draft responses using advanced browsing capabilities.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.koyeb.com/tutorials/using-llamaindex-and-mongodb-to-build-a-job-search-assistant#create-a-job-indexing-api-endpoint\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to building Full-Stack Job Search Assistant by Rishi Raj Jain using Gokoyeb, MongoDB, and LlamaIndex. This guide takes you through setting up MongoDB Atlas, crafting a Next.js application, developing UI components, and deploying your app on Koyeb, complete with real-time response streaming and continuous job updates.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\udda5\ufe0f\u00a0Demos:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   LlamaFS, a project developed by\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/AlexReibman\" rel=\"noreferrer noopener\">\n    Alex Reibman\n   </a>\n   and his team, automatically organizes messy file directories into neatly structured folders with interpretable names. Enhanced by Llama 3 and supported by Groq Inc.'s API, Ollama's fully local mode and LlamaIndex, this tool significantly improves file management efficiency.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/iyaja/llama-fs\" rel=\"noreferrer noopener\">\n    Code\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1794762651769430381\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   RAGApp, a project developed by\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/MarcusSchiesser\" rel=\"noreferrer noopener\">\n    Marcus Schiesser\n   </a>\n   , offers a no-code interface for configuring RAG chatbots as simply as GPTs by OpenAI. This fully open-source docker container can be deployed on any cloud platform, allowing users to set up the LLM, define system prompts, upload knowledge bases, and launch chatbots via UI or API.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/ragapp/ragapp\" rel=\"noreferrer noopener\">\n    Code\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1794030544415818062\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/ronoh4\" rel=\"noreferrer noopener\">\n    Phil Chirchir\u2019s\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/@leighphil4/dspy-rag-with-llamaindex-programming-llms-over-prompting-1b12d12cbc43\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on DSPy RAG with LlamaIndex. It demonstrates how to integrate DSPy bootstrapping models with a LlamaIndex RAG pipeline powered by LlamaParse.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/pavan_mantha1\" rel=\"noreferrer noopener\">\n    Pavan Kumar\u2019s\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://towardsdev.com/harnessing-gpt-4os-vision-for-advanced-search-building-image-embeddings-with-qdrant-5dd887cf40b5\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on advanced image indexing for RAG demonstrates how to combine image embeddings with structured annotations using multimodal models. It details how to enhance image search with LlamaIndex and Qdrant Engine\u2019s capabilities.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Jayita Bhattacharyya\u2019s\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://itsjb13.medium.com/building-a-rag-chatbot-using-llamaindex-groq-with-llama3-chainlit-b1709f770f55\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on Building a RAG Chatbot using Llamaindex, Groq with Llama3 &amp; Chainlit.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\udcf9\u00a0Webinar:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=_1JZfv7r4mY\" rel=\"noreferrer noopener\">\n    Webinar\n   </a>\n   with OpenDevin team to learn how to build an Open-Source Coding Assistant using OpenDevin.\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 9167, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8180bf62-60e1-450e-9589-4b0667a566bf": {"__data__": {"id_": "8180bf62-60e1-450e-9589-4b0667a566bf", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-04.html", "file_name": "llamaindex-newsletter-2024-06-04.html", "file_type": "text/html", "file_size": 10632, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-04.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-04.html", "file_name": "llamaindex-newsletter-2024-06-04.html", "file_type": "text/html", "file_size": 10632, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "30537a0f6aa9ae3aa698c1deb53e00fc1420fde3ad53c6d378e3920169020a03", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Hello, LlamaIndex Family! \ud83e\udd99\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We're thrilled to connect with you again and bring you the latest and greatest from the world of LlamaIndex. This week, we're excited to present an array of updates and a diverse lineup of content designed to enhance your LlamaIndex experience, particularly when working with Knowledge Graphs. From integrations and guides to demos and tutorials, we've got you covered with all the tools and insights you need.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Elevating Knowledge Graphs:\n   </strong>\n   The Property Graph Index, introduced in LlamaIndex, transforms how knowledge graphs (KGs) are built and queried. This powerful toolkit enhances graph searches with vector capabilities.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/module_guides/indexing/lpg_index_guide/\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1795869279457546447\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Spreadsheet Insights with LlamaParse:\n   </strong>\n   LlamaParse now supports spreadsheet parsing, turning complex Excel files into LLM-friendly tables for improved performance and data handling.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_parse/blob/main/examples/demo_excel.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1796237002364613040\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Code Generation with Codestral:\n   </strong>\n   Codestral, a cutting-edge model from MistralAI, is now integrated into LlamaIndex. This code-generating tool supports over 80 programming languages.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/examples/cookbooks/codestral/\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1795900182439276731\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </h2>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have introduced the Property Graph Index, a major feature that establishes LlamaIndex as the premier framework for building knowledge graphs (KGs) with LLMs. This sophisticated toolkit enables the construction and querying of KGs, allowing for joint vector and graph searches even in graph stores that lack native vector support.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/module_guides/indexing/lpg_index_guide/\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1795869279457546447\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have launched support for parsing spreadsheets in LlamaParse, allowing you to convert complex Excel files and other spreadsheet formats into clean, LLM-friendly tables for improved RAG pipeline performance.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_parse/blob/main/examples/demo_excel.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1796237002364613040\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have integrated Codestral from MistralAI into LlamaIndex, providing day 0 support for this cutting-edge code-generating model trained on over 80 programming languages.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/examples/cookbooks/codestral/\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1795900182439276731\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have integrated PostgresML into LlamaIndex, perfect for those who love Postgres and want to build AI applications. It serves open-source models locally, handles embeddings, and allows you to train or fine-tune models directly in Python and JavaScript.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/simplify-your-rag-application-architecture-with-llamaindex-postgresml\" rel=\"noreferrer noopener\">\n    Blogpost\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1795561227319734360\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have integrated with Milvus Lite to provide an easy start to vector search, offering day-1 support with LlamaIndex.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/examples/vector_stores/MilvusIndexDemo/\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1796305277073174654\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/examples/property_graph/property_graph_custom_retriever/\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to Building a Custom Graph Retriever to create a custom graph retriever for your specific needs by combining vector search and graph search with reranking for improved results.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/examples/llm/nvidia_nim/\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to Building GenAI Applications in minutes with NVIDIA's NIM inference microservices, offering an easy and fast way to deploy GenAI applications. This step-by-step guide teaches you how to run models, generate embeddings, and re-rank data for optimal results.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/property_graph/property_graph_advanced.ipynb\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to Constructing Knowledge Graphs with LLMs**,** build knowledge graphs using local models and Neo4j, starting with defining entities and relationships, using SchemaLLMPathExtractor to create structured graphs, and querying to uncover insights.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\udda5\ufe0f\u00a0Demos:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/ammirsm/llamaindex-omakase-rag\" rel=\"noreferrer noopener\">\n    Omakase RAG Orchestrator\n   </a>\n   , a project developed by\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/thatisamir\" rel=\"noreferrer noopener\">\n    Amir Mehr\n   </a>\n   , is a web app template designed to help you build scalable RAG applications using Django, LlamaIndex, and Google Drive. It features a full-featured RAG API, data source management, user access control, and an admin panel.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/gmail-extractor\" rel=\"noreferrer noopener\">\n    gmail-extractor\n   </a>\n   , a project by Laurie project that trains a Python script with an LLM to extract structured data from Gmail. By iteratively improving the script based on email data, the LLM can effectively modify and enhance it to extract information with precision.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Sherlock Xu\u2019s\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.bentoml.com/blog/serving-a-llamaindex-rag-app-as-rest-apis\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   from BentoML on Serving A LlamaIndex RAG App as REST APIs.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\udcd1\u00a0Papers:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   FinTextQA, a new benchmark dataset for long-form financial question answering, has been introduced by Jian Chen and their team. This benchmark was evaluated using LlamaIndex's Auto-Merging and Sentence Window Retrievers, along with various embeddings, rerankers, and LLMs, offering a comprehensive question-answering system for financial text.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\udcf9\u00a0Webinar:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=o0DPxvgML5c\" rel=\"noreferrer noopener\">\n    Webinar\n   </a>\n   with authors of memary - Julian Saks, Kevin Li, Seyeong Han. Memary is a fully open-source reference implementation for long-term memory in autonomous agents\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  \ud83d\udcc5\n  <strong>\n   Events:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.meetup.com/nlp_london/events/301171675/\" rel=\"noreferrer noopener\">\n    Join\n   </a>\n   Pierre from LlamaIndex along with speakers from Weaviate, and Weights &amp; Biases on June 12th at the London NLP meetup, focusing on the challenges and solutions for using LLMs with financial services data in production settings.\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 10595, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed9e395e-b9b9-4821-a4f9-06cc2b7e5329": {"__data__": {"id_": "ed9e395e-b9b9-4821-a4f9-06cc2b7e5329", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-11.html", "file_name": "llamaindex-newsletter-2024-06-11.html", "file_type": "text/html", "file_size": 11257, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-11.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-11.html", "file_name": "llamaindex-newsletter-2024-06-11.html", "file_type": "text/html", "file_size": 11257, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "2f4e44edbe7f18082cd706c2c761f2cefac5accb0ff099c25284c44892729d14", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Hello Llama Fans\ud83e\udd99\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Step into this week's edition of the LlamaIndex newsletter, where we bring you a slew of exciting updates, in-depth guides, demos, enriching educational tutorials, and webinars designed to enhance your experience and understanding of our platforms and tools.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Enhanced Memory Modules:\n   </strong>\n   New memory modules in LlamaIndex boost agentic RAG capabilities with Vector Memory for message storage and retrieval, and Simple Composable Memory for integrating multiple memory sources.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/examples/agent/memory/vector_memory/\" rel=\"noreferrer noopener\">\n    Notebook1\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/examples/agent/memory/composable_memory/\" rel=\"noreferrer noopener\">\n    Notebook2\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1799114410985988399\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Create-llama and E2B Integration:\n   </strong>\n   Launched integration turns agents into advanced data analysts, enabling Python coding for data analysis and generating detailed files like graph images.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1799176083381866757\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    LlamaParse and Knowledge Graphs:\n   </strong>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_parse/blob/main/examples/knowledge_graphs/kg_agent.ipynb\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   on integrating LlamaParse with Knowledge Graphs to develop RAG pipelines and agents for complex query handling.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Prometheus-2 RAG Evaluation:\n   </strong>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/examples/cookbooks/prometheus2_cookbook/\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   on using Prometheus-2, an affordable, transparent LLM based on Mistral models for effective RAG application evaluation with customized criteria.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Agentic RAG :\n   </strong>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=MXPYbjjyHXc\" rel=\"noreferrer noopener\">\n    Video tutorial\n   </a>\n   on Agentic RAG covering memory, planning, and reasoning, enhancing knowledge retrieval and agent capabilities.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </h2>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have introduced new memory modules in LlamaIndex for enhancing agentic RAG capabilities. The Vector Memory module enables storage and retrieval of user messages using vector search, while the Simple Composable Memory module allows for integration of multiple memory sources.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/examples/agent/memory/vector_memory/\" rel=\"noreferrer noopener\">\n    Notebook1\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/examples/agent/memory/composable_memory/\" rel=\"noreferrer noopener\">\n    Notebook2\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1799114410985988399\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have launched an integration between Create-llama and E2B\u2019s sandbox, transforming agents into powerful data analysts. This new feature allows agents to write Python code for data analysis and return comprehensive files, like graph images, enhancing the scope of what agents can accomplish.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1799176083381866757\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have launched an integration with Nomic-Embed-Vision that transforms Nomic-Embed-Text into a multimodal embedding that excels in handling image, text, and combined tasks, outperforming OpenAI CLIP with open access for all.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/zanussbaum/llama_index/blob/aeaaec27a21064836e4dbf956333e4d66bb41e1d/docs/docs/examples/multi_modal/multi_modal_rag_nomic.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   .\n  </li>\n </ol>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_parse/blob/main/examples/knowledge_graphs/kg_agent.ipynb\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to Integrating LlamaParse with Knowledge Graphs to develop a RAG pipeline for sophisticated query retrieval, and create an agent capable of answering complex queries effectively.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/examples/cookbooks/prometheus2_cookbook/\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to Using Prometheus-2 for RAG Evaluation for assessing RAG applications, built on Mistral base models, it offers an affordable and transparent solution for evaluation, capable of direct assessments, pairwise rankings, and tailored criteria, ensuring alignment with human judgments.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://generativeai.pub/advanced-rag-retrieval-strategy-query-rewriting-a1dd61815ff0\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to Three Forms of Query Rewriting for RAG to enhance RAG pipelines with techniques like sub-question decomposition, HyDE for aligning questions with embedding semantics, and step-back prompting for tackling complex queries more effectively.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\udda5\ufe0f\u00a0Demos:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/seldo\" rel=\"noreferrer noopener\">\n    Laurie Voss\n   </a>\n   \u2019s\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/file-organizer\" rel=\"noreferrer noopener\">\n    LLM-powered file organizer project\n   </a>\n   that categorizes files into folders based on LLM-decided categories without renaming them, ensuring important filenames remain intact. It organizes your files in multiple passes to balance folder sizes, resulting in descriptive yet practical folder names to help you find files easily.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/seldo\" rel=\"noreferrer noopener\">\n    Laurie Voss\n   </a>\n   \u2019s\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=MXPYbjjyHXc\" rel=\"noreferrer noopener\">\n    video tutorial\n   </a>\n   on transitioning from basic RAG to fully agentic knowledge retrieval, featuring real-world code examples that cover routing, memory, planning, tool use, and advanced agentic reasoning methods like Chain of Thought and Tree of Thought, along with insights into observability, controllability, and customizability.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/Prince_krampah\" rel=\"noreferrer noopener\">\n    Prince krampah\n   </a>\n   's\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1799463683179098203\" rel=\"noreferrer noopener\">\n    tutorials\n   </a>\n   on Agentic RAG Systems, offering comprehensive insights into advanced system building with detailed explanations on router query engines, function calling, and multi-step reasoning across complex documents.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/kingzzm\" rel=\"noreferrer noopener\">\n    kingzzm\u2019s\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://generativeai.pub/advanced-rag-retrieval-strategy-query-rewriting-a1dd61815ff0\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on Three Forms of Query Rewriting for RAG to enhance RAG pipelines with techniques like sub-question decomposition, HyDE for aligning questions with embedding semantics, and step-back prompting for tackling complex queries more effectively.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/rborgohain4\" rel=\"noreferrer noopener\">\n    Rajdeep Borgohain\n   </a>\n   's\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.inferless.com/cookbook/serverless-customer-service-bot\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   to build a customer-support voicebot with advanced features like speech-to-text and text-to-speech, integrated into a RAG pipeline for efficient handling of customer support exchanges using Inferless, LlamaIndex, faster-whisper, Piper, and Pinecone.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/pavan_mantha1\" rel=\"noreferrer noopener\">\n    Pavan Mantha\n   </a>\n   's\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://towardsdev.com/production-ready-secure-and-powerful-ai-implementations-with-azure-services-671b68631212\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on securing RAG apps using Azure for application security, including identity management, secure key storage, and managed Qdrant.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\udcf9\u00a0Webinar:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://lu.ma/kqxmbuou\" rel=\"noreferrer noopener\">\n    Join us\n   </a>\n   for our webinar with\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/tb_tomaz\" rel=\"noreferrer noopener\">\n    Tomaz Bratanic\n   </a>\n   from Neo4j on LlamaIndex property graph for insights into high-level and low-level graph construction, retrieval, and knowledge graph agents.\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 11221, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d42b824-bce9-4a63-9053-cb9e91c8801f": {"__data__": {"id_": "3d42b824-bce9-4a63-9053-cb9e91c8801f", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-18.html", "file_name": "llamaindex-newsletter-2024-06-18.html", "file_type": "text/html", "file_size": 12216, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-18.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-18.html", "file_name": "llamaindex-newsletter-2024-06-18.html", "file_type": "text/html", "file_size": 12216, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "9b856e0b34ab165ab5598430dab3f4b71b6f02b3f030facbf4a15e1e305a8e09", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Hey Llama Followers\ud83e\udd99\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Welcome to this week\u2019s edition of the LlamaIndex newsletter! We\u2019re bringing you an exciting set of updates and valuable resources from Mixture-of-Agents (MoA) paper as LlamaPack to how AtomicWork\u2019s Atom AI assistant leverages LlamaIndex to boost productivity and manage data effectively. Be sure to check out our in-depth guides, educational tutorials, and webinars for deeper insights into our tools.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Mixture-of-Agents (MoA) LlamaPack:\n   </strong>\n   We have integrated the Mixture-of-Agents (MoA) demonstrating that open-source LLMs can boost task capabilities. MoA outperforms GPT-4 Omni in the AlpacaEval 2.0 benchmarks.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-mixture-of-agents/README.md\" rel=\"noreferrer noopener\">\n    LlamaPack\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1801305617878937959\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    TiDB Integration with LlamaIndex:\n   </strong>\n   PingCap has now integrated their TiDB database with our LlamaIndex\u2019s knowledge graph functionality, making it available as an open-source project.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/examples/vector_stores/TiDBVector/\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1800987302837297387\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    RAG and Agents Cookbook:\n   </strong>\n   We have released a detailed cookbook on building RAG and Agents. This guide features enhanced observability through our LlamaIndex instrumentation module and ArizeAI.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/docs/docs/presentations/materials/2024-06-13-vector-ess-oss-tools.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1801726691813036214\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    AtomicWork\u2019s Enterprise AI Assistant:\n   </strong>\n   AtomicWork\u2019s enterprise AI assistant, Atom, leverages LlamaIndex to handle diverse data formats, boosting productivity and improving the employee experience. Check out the details in their detailed\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.atomicwork.com/blog/llamaindex-loaders-powering-atom\" rel=\"noreferrer noopener\">\n    blog\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_parse/blob/main/examples/excel/dcf_rag.ipynb\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to RAG Over Excel Files: Guide to use LlamaParse to accurately represent Excel files in a spatial grid format, enhancing data interpretation and reducing errors in question-answering.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </h2>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have integrated Mixture-of-Agents (MoA)\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://arxiv.org/abs/2406.04692\" rel=\"noreferrer noopener\">\n    paper\n   </a>\n   from\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/togethercompute\" rel=\"noreferrer noopener\">\n    TogetherAI\n   </a>\n   as LlamaPack from demonstrating that open-source large language models (LLMs) can enhance task capabilities. The paper shows that MoA outperforms GPT-4 Omni in the AlpacaEval 2.0 benchmarks.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-mixture-of-agents/README.md\" rel=\"noreferrer noopener\">\n    LlamaPack\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1801305617878937959\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/pingcap\" rel=\"noreferrer noopener\">\n    PingCap\n   </a>\n   has integrated their TiDB database with our LlamaIndex\u2019s knowledge graph functionality, now accessible as an open source project.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/examples/vector_stores/TiDBVector/\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1800987302837297387\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have released a detailed cookbook on building RAG and Agents, featuring supercharged observability throughout the call stack, enabled by our LlamaIndex instrumentation module and ArizeAI.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/docs/docs/presentations/materials/2024-06-13-vector-ess-oss-tools.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1801726691813036214\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have released the workshop slides and notebooks from our presentation on \u201cBuilding an Advanced Research Agent on Databricks\u201d at the Data AI Summit. This workshop focused on enhancing research assistants beyond the standard RAG setups.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.google.com/presentation/d/1yiuHEQEAhWEvVskbD9jwmfjopznVeZGwwWUzBIZ_P9U/\" rel=\"noreferrer noopener\">\n    Slide deck\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://colab.research.google.com/drive/18RUkf8IpHVSJF-rDh8cOj0QJ6UwQonfh?usp=sharing\" rel=\"noreferrer noopener\">\n    Notebook1\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://colab.research.google.com/drive/18RUkf8IpHVSJF-rDh8cOj0QJ6UwQonfh?usp=sharing\" rel=\"noreferrer noopener\">\n    Notebook2\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1802734801201623117\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\udca1\u00a0Real-World Use cases:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   AtomicWork\u2019s enterprise AI assistant, Atom, utilizes LlamaIndex to handle various data formats, ensuring accurate and secure data retrieval. Atom enhances decision-making and manages unstructured data effectively, boosting productivity and improving the employee experience. Check out the details in their detailed\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.atomicwork.com/blog/llamaindex-loaders-powering-atom\" rel=\"noreferrer noopener\">\n    blog\n   </a>\n   .\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_parse/blob/main/examples/excel/dcf_rag.ipynb\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to RAG Over Excel Files using LlamaParse to accurately represent Excel files in a spatial grid format, enhancing data interpretation and reducing errors in question-answering.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.singlestore.com/blog/claude-3-multimodal-with-llamaindex-and-singlestore/?utm_medium=referral&amp;utm_source=pavan&amp;utm_term=lnkdn&amp;utm_content=multimod\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to Building a Multimodal RAG Pipeline by\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/Pavan_Belagatti/status/1802955250795417790\" rel=\"noreferrer noopener\">\n    Pavan Belagatti\n   </a>\n   using Claude-3 and SingleStoreDB.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/mistralai/cookbook/blob/main/third_party/LlamaIndex/ollama_mistral_llamaindex.ipynb\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to building fully local RAG application using MistralAI, Ollama and LlamaIndex.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/tb_tomaz\" rel=\"noreferrer noopener\">\n    Tomaz Bratanic\n   </a>\n   \u2019s\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamaindex\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on constructing a knowledge graph, perform entity deduplication, design a custom graph retriever, and implement a question-answering flow.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/MervinPraison\" rel=\"noreferrer noopener\">\n    Mervin Praison\n   </a>\n   \u2019s\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=jnWaUtS2Fr8\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on creating the core components of an agent defining tools, integrating them into an agent reasoning loop, and wrapping everything with a user interface. using local models and\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/Chainlit/chainlit\" rel=\"noreferrer noopener\">\n    chainlit\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/AkritiUpadhya13\" rel=\"noreferrer noopener\">\n    Arkiti\n   </a>\n   \u2019s\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/@akriti.upadhyay/text-to-sql-using-singlestore-helios-groq-and-llama-3-0ebe1150cbe2\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on building a dynamic text-to-SQL solution using Llama 3 and GroqInc, highlighting the scalable and fast capabilities of SingleStoreDB Helios for multi-cloud deployments.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/kingzzm\" rel=\"noreferrer noopener\">\n    Kingzzm\n   </a>\n   \u2019s\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://ai.gopubby.com/advanced-rag-retrieval-strategy-embedded-tables-fdb3e44003a5\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on Advanced RAG Patterns detailing effective strategies for handling documents with embedded tables, utilizing tools like LlamaParse and Nougat for enhanced QA performance.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\udcf9\u00a0Webinar:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=9AqdqJdCNCw\" rel=\"noreferrer noopener\">\n    Webinar\n   </a>\n   on The Future of Web Agents with MultiOn.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/DivGarg9\" rel=\"noreferrer noopener\">\n    Div Garg\n   </a>\n   provided a full demo walkthrough and discuss the agentification of the internet.\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 12162, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51420421-c965-4215-85a4-5d3e334431ab": {"__data__": {"id_": "51420421-c965-4215-85a4-5d3e334431ab", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-25.html", "file_name": "llamaindex-newsletter-2024-06-25.html", "file_type": "text/html", "file_size": 7557, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-25.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-25.html", "file_name": "llamaindex-newsletter-2024-06-25.html", "file_type": "text/html", "file_size": 7557, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "b9b47fb1033a08711c788e6cbfa967ae6d1a41f573ddfbd8cdee80c0e09fe973", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Hello to All Llama Lovers!\ud83e\udd99\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Welcome to this week\u2019s issue of the LlamaIndex newsletter! This edition is packed with thrilling updates, comprehensive guides, and detailed tutorials to help you gain a deeper understanding of our tools.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    CrewAI Multi-Agent Integration:\n   </strong>\n   Integrated with CrewAI to enhance task-solving with specialized agent crews and LlamaIndex integrations.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/crewai_llamaindex.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1803818106063925749\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    MistralAI Fine-Tuning API Integration:\n   </strong>\n   Enhance model training and performance monitoring with our new integration of MistralAI\u2019s Fine-Tuning API.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/finetuning/mistralai_fine_tuning.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1803470522455380044\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </h2>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have launched a Multi-Agent integration with CrewAI to build a\n   <code class=\"SanityPortableText_inlineCode__cI85z\">\n    crew\n   </code>\n   of specialized agents that collaboratively solve tasks. Enhance these agents with external knowledge and third-party tools through easy integrations with LlamaIndex, including advanced RAG query engines and tools from LlamaHub.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/crewai_llamaindex.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1803818106063925749\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have integrated MistralAI\u2019s Fine-Tuning API to create and synthesize training and evaluation datasets, assess model after fine-tuning, and monitor performance metrics with RAGAS and Weights &amp; Biases.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/finetuning/mistralai_fine_tuning.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1803470522455380044\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\udca1\u00a0Demos:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/ragapp/ragapp\" rel=\"noreferrer noopener\">\n    RAGapp\n   </a>\n   by\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/MarcusSchiesser\" rel=\"noreferrer noopener\">\n    **Marcus Schiesser\n   </a>\n   ** simplifies Agentic RAG in enterprise settings with functionalities akin to using GPTs by OpenAI. The latest version includes a code interpreter and a tool to call any OpenAPI, all built using LlamaIndex.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://lightning.ai/blochsphere/studios/multi-document-agentic-rag-for-quantum-computing\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to Multi-Document Agentic RAG Using LightningAI: Jay Shah\u2019s template that enables you to set up a multi-document agent for search and summarization across research notebooks. This out-of-the-box solution, integrated with Streamlit, allows for full visualization and is part of LightningAI\u2019s suite of tools for developing and sharing ML and genAI native apps.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/Azure-Samples/llama-index-python/tree/main\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to Making a Serverless RAG Chatbot: Azure\u2019s quick start repository for creating a serverless RAG chatbot using LlamaIndex and AzureOpenAI.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/understanding/agent/basic_agent/\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to Building an Agent in LlamaIndex: Our comprehensive guide which covers building a basic agent, using local models, adding RAG, enhancing retrieval with LlamaParse, and developing custom tools.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/jino_rohit\" rel=\"noreferrer noopener\">\n    JinoRohit\u2019s\n   </a>\n   tutorial on using a LlamaIndex pipeline with MLflow for systematic tracking and tuning of RAG parameters, enhancing answer accuracy through precise evaluation metrics and datasets.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Hanane Dupouy\u2019s\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.linkedin.com/posts/hanane-d-algo-trader_corrective-rag-crag-for-financial-analysis-activity-7203277247321886721-G9yj?utm_source=share&amp;utm_medium=member_desktop\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   demonstrates how to apply CRAG (Corrective RAG) for financial analysis using LlamaIndex\u2019s CRAG LlamaPack. This technique assesses retrieval quality and supplements the knowledge base with web searches to ensure contextual accuracy and relevance.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/GanatraSoham\" rel=\"noreferrer noopener\">\n    Soham\u2019s\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=5AnK0DXHuy8\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   to create an agent that automates GitHub commits using Composio and LlamaIndex Tools.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/deltaaruna\" rel=\"noreferrer noopener\">\n    Aruna Withanage\u2019s\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/rahasak/llamaindex-query-pipelines-tutorial-text-to-sql-example-d859ed90b87c\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on creating custom text-to-SQL pipelines using LlamaIndex\u2019s DAG capabilities.\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 7511, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ab4246f-4d29-4686-abe8-2f0c710de25e": {"__data__": {"id_": "9ab4246f-4d29-4686-abe8-2f0c710de25e", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-02.html", "file_name": "llamaindex-newsletter-2024-07-02.html", "file_type": "text/html", "file_size": 9058, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-02.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-02.html", "file_name": "llamaindex-newsletter-2024-07-02.html", "file_type": "text/html", "file_size": 9058, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "78a345e94e14bd99668a0ba4ff5df1c398159e73ebc650d3180d4b0b0315cb01", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Hello, Llama enthusiasts! \ud83e\udd99\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Welcome to this week\u2019s edition of the LlamaIndex newsletter! In this issue, we\u2019re excited to bring you exciting updates about\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   llama-agents\n  </code>\n  , live demos, extensive guides, and in-depth tutorials to enhance your understanding of our tools.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Before moving into our newsletter, we have an exciting update on our enterprise offerings. We are thrilled to announce the waitlist release of LlamaCloud, our fully-managed ingestion service.\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"http://bit.ly/llamacloud\" rel=\"noreferrer noopener\">\n   Sign up\n  </a>\n  now if you\u2019re eager to collaborate and build LLM applications with LlamaCloud.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Launched Llama-Agents Framework:\n   </strong>\n   Our new alpha-release, llama-agents, enables multi-agent AI systems for production with a distributed architecture, HTTP API communication, and agentic orchestration. It\u2019s designed for easy deployment, scalability, and observability.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\" rel=\"noreferrer noopener\">\n    Blogpost\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1806116419995844947\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    <code class=\"SanityPortableText_inlineCode__cI85z\">\n     create-llama\n    </code>\n    Integrated with LlamaCloud:\n   </strong>\n   Streamline your LLM application data pipelines with create-llama, now integrated with LlamaCloud for faster setup and efficient system maintenance.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/MarcusSchiesser/status/1806960577299767767\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </h2>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have launched llama-agents - new alpha-release framework that enables multi-agent AI systems to go into production. It features a distributed, service-oriented architecture, communication through standard HTTP APIs, agentic orchestration of flows, and is designed for easy deployment, scalability, and observability.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\" rel=\"noreferrer noopener\">\n    Blogpost\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1806116419995844947\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   create-llama is now integrated with LlamaCloud to streamline the setup and management of data pipelines for LLM applications, providing a fast and efficient way to deploy and maintain these systems.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/MarcusSchiesser/status/1806960577299767767\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have integrated with DSPy for Optimized RAG by utilizing DSPy\u2019s optimization capabilities with LlamaIndex\u2019s data tools to enhance your query pipelines, optimize prompts, or repurpose DSPy predictors.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/stanfordnlp/dspy/blob/main/examples/llamaindex/dspy_llamaindex_rag.ipynb\" rel=\"noreferrer noopener\">\n    Cookbook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1805622494130586078\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\udca1\u00a0Demos:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Automating Code Reviews,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/GanatraSoham/status/1807787558157320376\" rel=\"noreferrer noopener\">\n    project\n   </a>\n   by\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/composiohq\" rel=\"noreferrer noopener\">\n    Composio\n   </a>\n   with LlamaIndex automates code reviews using an AI agent in under 100 lines of code that monitors GitHub PRs, reviews them immediately upon creation, and posts feedback directly to your Slack channel.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/ComposioHQ/composio/tree/master/python/examples/pr_agent/pr_agent_llama_index\" rel=\"noreferrer noopener\">\n    Codebase\n   </a>\n   .\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama-agents/blob/main/examples/agentic_rag_toolservice.ipynb\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to Building an Agentic RAG Service with our comprehensive notebook that walks you through creating vector indexes, transforming them into query engines, turning each engine into a tool, providing these tools to agents, and launching the agents as services.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Guide to AI Agents with LlamaIndex: Andrei\u2019s comprehensive workshop from Gen AI Philippines, showcasing LLM applications through LlamaIndex. This beginner-friendly session covers topics from RAG to multi-hop agents.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://drive.google.com/file/d/1kInT-szYWH71DvKvhhE5XAUVhTl8ZXJA/view\" rel=\"noreferrer noopener\">\n    Video\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/docs/docs/presentations/materials/2024-06-22-genai-philippines.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   .\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/kingzzm\" rel=\"noreferrer noopener\">\n    Kingzzm\u2019s\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://generativeai.pub/advanced-rag-retrieval-strategies-hybrid-retrieval-997d39659720\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on crafting a custom hybrid retriever using LlamaIndex\u2019s flexible abstractions. This tutorial teaches you how to integrate full text and dense search capabilities from Elastic, and how to write your own reciprocal rank fusion function for optimal retrieval strategy.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/gswithai\" rel=\"noreferrer noopener\">\n    Jeff\u2019s\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=i8ldunneSW8\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on which outlines the essential tools needed to construct a report generator using a ReAct agent. Learn how to integrate a RAG tool over guideline documents, a web search tool, and a report generation tool that converts markdown text into PDFs.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/1littlecoder\" rel=\"noreferrer noopener\">\n    1littlecoder\u2019s\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=_aTEI3ISkQA\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on llama-agents provides a detailed introduction to transforming multi-agent systems into microservices for production, including setup examples and a walkthrough of the architecture involving the control plane, message queue, and agent services using LlamaIndex abstractions.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/MervinPraison\" rel=\"noreferrer noopener\">\n    Mervin Praison\u2019s\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=nEQCpSd5mx8\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on the llama-agents framework provides a concise guide to setting up agent services, from notebook synchronization to server-client interactions, complete with over 10 practical examples.\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 9012, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "141f71f0-efff-4a47-8861-6b1a809a0a15": {"__data__": {"id_": "141f71f0-efff-4a47-8861-6b1a809a0a15", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-09.html", "file_name": "llamaindex-newsletter-2024-07-09.html", "file_type": "text/html", "file_size": 12456, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-09.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-09.html", "file_name": "llamaindex-newsletter-2024-07-09.html", "file_type": "text/html", "file_size": 12456, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "cc14c0258d97286e97e15eb8beabddeb1e692e7015b52bf02d990446259ac09b", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Hello, Llama Lovers! \ud83e\udd99\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Welcome to this week\u2019s edition of the LlamaIndex newsletter! We\u2019re thrilled to share some exciting updates about\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   llama-agents\n  </code>\n  , along with demos, extensive guides, and in-depth tutorials to enhance your understanding of our tools.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Before we dive into our newsletter, we\u2019re excited to announce the return of Community Office Hours. If you have use-cases, in-depth questions, or feedback for the team at LlamaIndex, join us during our community office hours! We\u2019ll set up a 15-30 minute Zoom call to discuss it.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSefrnmxQWD-1OhSP51kUKtdbw9EGDjrMLefkZFACKD19TKsuQ/viewform?usp=sf_link\" rel=\"noreferrer noopener\">\n   <strong>\n    Sign up here\n   </strong>\n  </a>\n  to participate.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Multi-Agent Kubernetes Kit Launched:\n   </strong>\n   Deploy multi-agent systems easily with our new Kubernetes Starter Kit featuring ready-to-use tools and configurations.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama-agents/tree/main/examples/docker-kubernetes\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1807801281324765469\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Enhanced Communication with RabbitMQ:\n   </strong>\n   Boost multi-agent system reliability and scalability in production with our new RabbitMQ integration.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama-agents/tree/main/examples/rabbitmq\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1810342085171855753\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Reflection as a Service Guide:\n   </strong>\n   Improve agent reliability with our guide on building Reflection as a Service, perfect for output validation and correction.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama-agents/blob/main/examples/reflection/toxicity_reflection_service.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1808898730638389262\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Corrective RAG as a Service Guide:\n   </strong>\n   Create a self-correcting RAG that ensures context relevance and integrates search fallbacks before generation.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama-agents/blob/main/examples/corrective_rag.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1809282069606068486\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Tutorial series on Property Graphs:\n   </strong>\n   6-part video series on Property Graphs in LlamaIndex using MistralAI, Neo4j, and Ollama.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/playlist?list=PLTZkGHtR085ZYstpcTFWqP27D-SPZe6EZ\" rel=\"noreferrer noopener\">\n    Videos\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1810410943215710510\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </h2>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have launched a Multi-Agent on Kubernetes Starter Kit to build and deploy a multi-agent system using Docker Compose and Kubernetes using llama-agents. This kit includes prebuilt agent loops and tools, as well as Dockerfiles and Kubernetes manifests for easy production deployment.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama-agents/tree/main/examples/docker-kubernetes\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1807801281324765469\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have integrated RabbitMQ with llama-agents to enhance multi-agent communication, offering scalability and reliability for handling large request volumes in production.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama-agents/tree/main/examples/rabbitmq\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1810342085171855753\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   [\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"http://Yi-01.AI\" rel=\"noreferrer noopener\">\n    Yi-01.AI\n   </a>\n   ](\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"http://Yi-01.AIhttps\" rel=\"noreferrer noopener\">\n    http://Yi-01.AIhttps\n   </a>\n   ://x.com/01AI_Yi) is integrated with LlamaIndex for enhanced retrieval and indexing, streamlining the development of smarter, faster RAG applications.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/examples/llm/yi/\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have launched a\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/playlist?list=PLTZkGHtR085ZYstpcTFWqP27D-SPZe6EZ\" rel=\"noreferrer noopener\">\n    6-part video series\n   </a>\n   on Property Graphs in LlamaIndex using MistralAI, Neo4j and Ollama.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1810410943215710510\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\udca1\u00a0Demos:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/JSv4/OpenContracts\" rel=\"noreferrer noopener\">\n    OpenContracts\n   </a>\n   by\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/johnscrudato\" rel=\"noreferrer noopener\">\n    John Scrudato\n   </a>\n   : A fully open-source, AI-powered Document Analytics Tool, integrates genAI capabilities and LlamaIndex for robust query handling and data extraction across documents. This tool is particularly valuable for legal analysis, enabling users to manage, process, and query vast arrays of contracts and legal documents.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://jsv4.github.io/OpenContracts/\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   .\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Guide to build Reflection as a Service to enhance agent reliability with our new standalone service, ideal for validating and correcting outputs across multiple agents.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama-agents/blob/main/examples/reflection/toxicity_reflection_service.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1808898730638389262\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Guide to build Corrective RAG as a Service, a self-correcting RAG that dynamically validates context relevance, seamlessly integrating web search fallbacks before generation.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama-agents/blob/main/examples/corrective_rag.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1809282069606068486\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/pavan_mantha1\" rel=\"noreferrer noopener\">\n    Pavan Kumar\u2019s\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://blog.gopenai.com/harnessing-ai-at-the-edge-building-a-rag-system-with-ollama-qdrant-and-raspberry-pi-45ac3212cf75\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   to build a RAG pipeline that lives on a Raspberry Pi device with docker, Ollama, Qdrant, and using LlamaIndex as the orchestration layer.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/AdiDror6\" rel=\"noreferrer noopener\">\n    Trade Mamba\u2019s\n   </a>\n   video\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=uOLhleiOM84\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   to build an AI-enabled trading assistant using LlamaIndex\u2019s agent/tool/RAG abstractions for tasks like tracking portfolio values, managing stock orders, and conducting vector searches for semantic information.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/giskard_ai\" rel=\"noreferrer noopener\">\n    Giskard\u2019s\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.giskard.ai/en/stable/reference/notebooks/RAGET.html\" rel=\"noreferrer noopener\">\n    toolkit\n   </a>\n   enables diverse question generation featuring question types like simple, complex, distracting, situational, double, and conversational for RAG evaluation, as demonstrated in the tutorial on using a LlamaIndex pipeline with an IPCC Climate Report.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/pavan_mantha1\" rel=\"noreferrer noopener\">\n    Pavan Kumar\u2019s\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://blog.stackademic.com/building-a-multi-document-react-agent-for-financial-analysis-using-llamaindex-and-qdrant-72a535730ac3\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   demonstrates building a Multi-Document Financial Analyst Agent using LlamaIndex RAG and ReAct tools, analyzing categorized SEC documents with SnowflakeDB embeddings and MistralAI via Ollama.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Ross A.\u2019s\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/@rossashman/the-art-of-rag-part-4-retrieval-evaluation-427bb5db0475\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on retrieval evaluations for RAG delves into essential metrics like precision@K and NDCG, and demonstrates how to convert datasets to BEIR format for assessing LlamaIndex retrievers.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83c\udfa5\u00a0Webinar:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Join us for a\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://lu.ma/dywrdye5\" rel=\"noreferrer noopener\">\n    webinar\n   </a>\n   on July 10th, featuring Jerry Liu (LlamaIndex) and Ayush Thakur (Weights &amp; Biases) on\n   <strong>\n    A Principled Approach to RAG Experimentation + Evaluation\n   </strong>\n   to learn how to build, evaluate, and refine RAG pipelines.\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 12410, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "257dc101-bd52-471d-9104-3ab57355740d": {"__data__": {"id_": "257dc101-bd52-471d-9104-3ab57355740d", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-16.html", "file_name": "llamaindex-newsletter-2024-07-16.html", "file_type": "text/html", "file_size": 12112, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-16.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-16.html", "file_name": "llamaindex-newsletter-2024-07-16.html", "file_type": "text/html", "file_size": 12112, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "b3de3b7dce6e073b8a813e74f026e7cee04e9e0929f2c8b42822633199dd5ded", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Hello, Llama Family! \ud83e\udd99\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Welcome to this week\u2019s edition of the LlamaIndex newsletter! We\u2019re thrilled to share some exciting updates about our products, the implementation of GraphRAG, demos that have achieved over $1M in ARR, extensive guides, in-depth tutorials, and hackathons.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Before we get into the details of our newsletter, we\u2019re thrilled to share the beta launch of LlamaCloud. This new data processing layer boosts RAG workflows with sophisticated parsing, indexing, and retrieval functions. Alongside this, we\u2019re also introducing LlamaTrace in partnership with Arize AI, which provides unmatched tracing, observability, and evaluation capabilities for LLM application workflows.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Signup here:\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://t.co/yQGTiRSNvj\" rel=\"noreferrer noopener\">\n   cloud.llamaindex.ai\n  </a>\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  \ud83e\udd29\n  <strong>\n   The highlights:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    LlamaCloud Launch:\n   </strong>\n   We\u2019ve launched the beta release of LlamaCloud, a data processing layer designed to enhance RAG workflows with state-of-the-art parsing, indexing, and retrieval capabilities.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/llamacloud-built-for-enterprise-llm-app-builders\" rel=\"noreferrer noopener\">\n    Blogpost\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1810716602247348242\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    LlamaTrace Launch:\n   </strong>\n   In collaboration with Arize AI, we\u2019ve introduced LlamaTrace, offering unmatched tracing, observability, and evaluation capabilities for LLM application workflows. It features detailed call stack tracing, one-click setup through LlamaIndex, and seamless integration with LlamaCloud.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications\" rel=\"noreferrer noopener\">\n    Blogpost\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1811462543535464796\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    GraphRAG Implementation:\n   </strong>\n   Implementation of GraphRAG with LlamaIndex, focusing on graph generation, community building, summaries, and community-based retrieval to improve answer aggregation.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/GraphRAG_v1.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1812517033445396754\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Redis Queue Integration with Llama-Agents:\n   </strong>\n   We have integrated Redis Queue with llama-agents to boost coordination and communication in multi-agent workflows, ensuring robust performance.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama-agents/tree/main/examples/redis/simple-redis-app\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1812202419025293784\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \u2728 Feature Releases and Enhancements:\n  </strong>\n </h2>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have launched the beta release of LlamaCloud, a data processing layer that enhances RAG workflows with advanced parsing, indexing, and retrieval capabilities.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/llamacloud-built-for-enterprise-llm-app-builders\" rel=\"noreferrer noopener\">\n    Blogpost\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1810716602247348242\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have launched an implementation[beta] of GraphRAG concepts with LlamaIndex focussing on graph generation, building communities and community summaries, and community-based retrieval to aggregate answers from summaries.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/GraphRAG_v1.ipynb\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1812517033445396754\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have integrated Redis Queue with llama-agents to enhance coordination in multi-agent workflows, allowing for robust communication.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama-agents/tree/main/examples/redis/simple-redis-app\" rel=\"noreferrer noopener\">\n    Notebook\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1812202419025293784\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have introduced LlamaTrace in collaboration with Arize AI, offering unparalleled tracing, observability, and evaluation capabilities for LLM application workflows. LlamaTrace stands out for its detailed tracing, which logs the entire call stack, one-click setup through LlamaIndex, and seamless integration with LlamaCloud for easy access and authentication.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications\" rel=\"noreferrer noopener\">\n    Blogpost\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1811462543535464796\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We have integrated NebulaGraph with LlamaIndex, enhancing PropertyGraph capabilities with sophisticated extractors, customizable properties on nodes and edges, and advanced retrieval options.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/examples/property_graph/property_graph_nebula/\" rel=\"noreferrer noopener\">\n    Docs\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1811190191597773282\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\udca1\u00a0Demos:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex\" rel=\"noreferrer noopener\">\n    Lyzrai\n   </a>\n   has achieved over $1M ARR using LlamaIndex! This full-stack autonomous AI agent framework enhances AI sales and marketing functions with LlamaIndex\u2019s data connectors and RAG capabilities, boasting rapid revenue growth, high accuracy, and customer satisfaction.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83d\uddfa\ufe0f Guides:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_parse/blob/main/examples/multimodal/multimodal_rag_slide_deck.ipynb\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to Multi-Modal RAG for Document Processing that introduces a multi-modal RAG architecture using LlamaParse, LlamaIndex, and GPT-4o, designed to handle complex slide decks.\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1812963306032013586\" rel=\"noreferrer noopener\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/llama_index/status/1812157431788835094\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to using LlamaParse and GPT-4o for Financial Report RAG to to effectively parse and synthesize complex financial documents, enhancing clarity and accuracy in data analysis.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/meta-llama/llama-recipes/tree/main/recipes/3p_integrations/llamaindex/dlai_agentic_rag\" rel=\"noreferrer noopener\">\n    Guide\n   </a>\n   to Building Agentic RAG with Llama3: Explore our comprehensive cookbooks, created in collaboration with AI at Meta, featuring advanced techniques from routing and tool use to constructing complex agent reasoning loops and multi-document agents using purely local models like Llama3.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \u270d\ufe0f Tutorials:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/1littlecoder\" rel=\"noreferrer noopener\">\n    1LittleCoder\u2019s\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=aiySmi5JocQ\" rel=\"noreferrer noopener\">\n    video tutorial\n   </a>\n   demonstrates how to deploy self-hosted llama-agents using Arcee AI, MistralAI, and Ollama, including setup, local model integration, and tool development.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/kingzzm\" rel=\"noreferrer noopener\">\n    kingzzm\u2019s\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://ai.gopubby.com/advanced-rag-retrieval-strategies-flow-and-modular-672493acb4a7\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on using LlamaIndex to build advanced RAG flows, detailing how to compose and visualize each step from basic retrieval and prompting to advanced techniques and evaluation with RAGAS.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/MervinPraison\" rel=\"noreferrer noopener\">\n    Mervin Praison\u2019s\n   </a>\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.youtube.com/watch?v=nEQCpSd5mx8\" rel=\"noreferrer noopener\">\n    tutorial\n   </a>\n   on using llama-agents, detailing the framework\u2019s purpose, a step-by-step setup guide for multi-agent services, and how it stands out from other frameworks.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   \ud83c\udfa4\u00a0Events:\n  </strong>\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://lablab.ai/event/llama-3-ai-hackathon\" rel=\"noreferrer noopener\">\n    Join our online hackathon\n   </a>\n   this Friday, 19th, to build AI apps with Llama 3 from Meta and win cash, credits, and prizes from us and our co-hosts\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/togethercompute\" rel=\"noreferrer noopener\">\n    TogetherAI\n   </a>\n   ,\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/milvusio\" rel=\"noreferrer noopener\">\n    Milvus\n   </a>\n   , and\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/lablabai\" rel=\"noreferrer noopener\">\n    LablabAI\n   </a>\n   .\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 12064, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94092881-2ff9-4817-a884-f349c4240baf": {"__data__": {"id_": "94092881-2ff9-4817-a884-f349c4240baf", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.html", "file_name": "llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.html", "file_type": "text/html", "file_size": 11234, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.html", "file_name": "llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.html", "file_type": "text/html", "file_size": 11234, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "a8084dae7a6f4719d715cd009c849eb7265088843291132dd7250e626ef0c32e", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h1>\n  Overview\n </h1>\n <p>\n  I had the pleasure of speaking with Sam Charrington on the\n  <a href=\"https://twimlai.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   TWIML AI podcast\n  </a>\n  about LlamaIndex, and the episode was just released this past Monday (5/8/23).\n </p>\n <p>\n  I thought it would be a fun experiment to distill some highlights from the podcast! And what better way to do this than using LlamaIndex itself (plus OpenAI Whisper for transcription)? \ud83d\ude09\n </p>\n <p>\n  I did the following:\n </p>\n <ol>\n  <li>\n   Ran the podcast through\n   <a href=\"https://github.com/ggerganov/whisper.cpp\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    whisper.cpp\n   </a>\n  </li>\n  <li>\n   Did some light manual cleaning of the text, and uploaded it as a\n   <a href=\"https://www.dropbox.com/s/gn2rpfvkjkygemb/twiml.txt?dl=0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Dropbox file\n   </a>\n   .\n  </li>\n  <li>\n   Get a high-level summary + ask some more targeted questions over the podcast transcript. Uses LlamaIndex \ud83e\udd99.\n   <a href=\"https://colab.research.google.com/drive/1sAHWbyQRjtp_w-r-HOMpL0gkkhigkdGR?usp=sharing\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Colab notebook here\n   </a>\n   !\n  </li>\n </ol>\n <p>\n  We used the our list index to get an overall summary of the podcast, and also our vector index to ask more specific questions. We ask some basic questions ourselves, but also directly ask questions that Sam asked during the podcast in order to extract more concise versions of the responses that I gave (including removing a bunch of filler \u201cyou know\u201d words).\n </p>\n <p>\n  All the distilled highlights using LlamaIndex are showcased below. Let us know what you think!\n </p>\n <h1>\n  Distilled Highlights\n </h1>\n <p>\n  <strong>\n   What are three key points described in this podcast? Give a followup of each key point with an explanation.\n  </strong>\n </p>\n <ol>\n  <li>\n   <em class=\"pt\">\n    LlamaIndex is a toolkit for connecting language models to data.\n   </em>\n   It was initially designed as a fun tool to play around with and solve a specific problem, but has since evolved into a set of useful tools and instructions that people can use to build applications on top of. The technique people were playing around with at the time was the idea of prompting and context learning, which is providing the right conditioning variable for the input and using that to send some sort of input prompt to the language model and get back a response. Additionally, LlamaIndex offers more advanced primitives such as decision-making at the top level to route queries to the right sub-data structure, and the ability to synthesize an answer from multiple data sources. It also provides an outer abstraction layer that can automatically reason which structure to use given an outer query request, and can be used as a drop-in module on top of existing data systems without having to worry about complexity. Examples of applications that can be built on top of LlamaIndex include ingesting video and structured data to parse into an audio transcript, running image captioning models, and creating augmented chatbot experiences on top of web scrapers.\n  </li>\n  <li>\n   <em class=\"pt\">\n    LlamaIndex is also exploring the idea of automation and unifying everything under a single query interface,\n   </em>\n   so that users don\u2019t have to specify a different parameter for every use case. This includes optimizing token usage, making queries faster, and reducing costs for the user. Additionally, LlamaIndex is looking into applying automation to the data system world, such as teaching Oracle databases how to spit out natural language prompt responses, and making the data stack more efficient. This includes simplifying the data stack over time, especially as language models take off, and leveraging capabilities of LLM\u2019s and various components of the data landscape to simplify the number of steps it takes from raw data to insight for the user. They are also exploring the idea of inferring the right schemas and writing structured data from unstructured data, as well as automatically building a natural language query interface with a view of the data within the data system.\n  </li>\n  <li>\n   <em class=\"pt\">\n    LlamaIndex is also exploring the idea of agents as a layer of automation for decision making over any sort of function that you want to run.\n   </em>\n   This includes taking in some input and doing reasoning under the hood to decide, make a decision over some input, as well as some access to some context, for instance, over your data or over the set of tools that is able to have access to. Additionally, LlamaIndex is looking into ways to reduce cost and latency, such as using more fine-tuned distilled models that are a bit smaller, and making sure that the more decisions that are chained together, the less errors propagate over time. They are also exploring the idea of observability and evidence across a chain of relatively independent decisions that individual agents are making, as well as the interfaces that these agents might use, such as traditional software and agent worlds.\n  </li>\n </ol>\n <p>\n  <strong>\n   What is the origin story of LlamaIndex?\n  </strong>\n </p>\n <p>\n  The origin story of LlamaIndex is that it was founded in November by Jerry, who was trying to build a sales bot. He was playing around with GPT-3 and wanted to use it on his internal company data. He wanted to use it to synthesize a to do list for him for the next customer meeting, as he had to spend 20\u201330 minutes reviewing notes from the previous call transcripts. This led to the idea of stuffing data from Notions, Slack, Salesforce, data lakes, vector databases, and structure databases into language models. This was the impetus for LlamaIndex, which is focused on connecting data to language models and tapping into the capabilities of language models to utilize them on top of private sources of data.\n </p>\n <p>\n  <strong>\n   What is LlamaIndex doing beyond top-k retrieval?\n  </strong>\n </p>\n <p>\n  LlamaIndex is offering more advanced primitives on top of basic top-k retrieval in order to provide responses to more complicated questions. These primitives include decision-making at the top level to route queries to the right sub-data structure, synthesizing information from multiple data systems, and providing trade-offs between different approaches.\n </p>\n <p>\n  Additionally, LlamaIndex is working on building tooling to help users create customizable indexes and views of their data to allow them to execute different types of queries. This includes connecting to existing data systems, defining metadata on top of each unit of data, providing the building blocks to create different types of indexes, and abstracting away complexity with an outer agent layer that can automatically reason which structure to use given a query request. This allows users to get the best results for a query, while also providing an alternative to something like a langchain or using it as part of building a broader solution.\n </p>\n <p>\n  <strong>\n   [Sam] It sounds like we\u2019re starting to identify a higher level of abstraction that different use cases will fall under. Is it more the case that there\u2019s some manageable number of these primitives, like 10, 20, or is it that every use case is going to be a little bit different, and there are hundreds of thousands of kind of fundamental ways that people want to work with their documents, and so you need to just give them a very open capability?\n  </strong>\n </p>\n <p>\n  Jerry\u2019s response is that there are probably a few different use cases that people tend to want to get answers from over their data, and it is possible there is a giant long tail of different tasks. He believes that the complexity of the task scales with the number of steps it requires to execute, and that users need to be given customizable building blocks in order to get the results they want. He also believes that the next natural step is to automate the process and unify everything under a single query interface, so that users don\u2019t have to specify different parameters for every use case.\n </p>\n <p>\n  He also believes that this paradigm is displacing more static paradigms like ETL, and that it is applicable to a wide range of applications. He sees this agent type environment becoming fundamental infrastructure that reimagines the entire existing enterprise data stack, and that it can be used to parse unstructured data into structured data, as well as to automatically reason how to best transform data from one place to another. He also believes that this will make the job of the data engineer and data scientist much more efficient, and that it will enable the creation of natural language query interfaces that have a view of the data within the data system.\n </p>\n <p>\n  <strong>\n   [Sam] When you think about the interface between LLM-based data processing system and the data sources of record, what does that interface evolve to look like? For example, does it evolve to look like the chat GPT plugin model, where we\u2019re going to teach our Oracle databases how to spit out natural language prompt responses, that kind of thing, or do you think that there\u2019s some more efficient way of doing that or is that more efficient? Like, what\u2019s your view of the way these things evolve?\n  </strong>\n </p>\n <p>\n  I think the way this interface will evolve is that it will become more automated and efficient. We will be able to use language models to understand raw text and extract the relevant information from it, without having to manually enter data into a structured format. We will also be able to use agents to automate decision making and provide a unified query interface, so that users don\u2019t have to specify different parameters for every use case.\n </p>\n <p>\n  Additionally, we can use LlamaIndex to structure data in a way that allows us to make use of the limited prompt size of GPT-3, while still being able to achieve the task. We can also use this data stack to infer the right schemas and further write structured data from unstructured data, as well as automatically build a natural language query interface that has a view of the data within the data system. This will enable us to make the job of the data engineer and data scientist much more efficient by having automated reasoning agents over deciding, making decisions at every stage of the data infrastructure stack.\n </p>\n <h1>\n  Want to ask your own questions over the podcast?\n </h1>\n <p>\n  If you want to build your own LLM-powered chatbot over our TWIML podcast, check out the resources below!\n </p>\n <p>\n  <a href=\"https://colab.research.google.com/drive/1sAHWbyQRjtp_w-r-HOMpL0gkkhigkdGR?usp=sharing\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Colab notebook\n  </a>\n </p>\n <p>\n  <a href=\"https://www.dropbox.com/s/gn2rpfvkjkygemb/twiml.txt?dl=0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Raw Transcript\n  </a>\n </p>\n <p>\n  <a href=\"https://open.spotify.com/episode/2vEO6dkzfEw5e7eZqngsGz?si=397cc8d7496a479c\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Podcast on Spotify\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 11201, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ddf56f3-b705-465d-984a-043fc1b59144": {"__data__": {"id_": "4ddf56f3-b705-465d-984a-043fc1b59144", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.html", "file_name": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.html", "file_type": "text/html", "file_size": 44532, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.html", "file_name": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.html", "file_type": "text/html", "file_size": 44532, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "1f9435be8813e6872f5ce015845019db9403e41a25099d32a266ea14216943ef", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h1>\n  Introduction\n </h1>\n <p>\n  Evaluation is a critical component in enhancing your Retrieval-Augmented Generation (RAG) pipeline, traditionally reliant on GPT-4. However, the open-source\n  <a href=\"https://huggingface.co/kaist-ai/prometheus-13b-v1.0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Prometheus model\n  </a>\n  has recently emerged as a notable alternative for such evaluation tasks.\n </p>\n <p>\n  In this blog post, we will demonstrate how to effectively use the Prometheus model for evaluation purposes, integrating it smoothly with the LlamaIndex framework by comparing it with GPT-4 evaluation. Our primary focus will be on assessing RAG using our standard metrics: Correctness, Faithfulness, and Context Relevancy. To provide a clearer understanding, here\u2019s what each metric entails:\n </p>\n <ol>\n  <li>\n   <strong>\n    Correctness\n   </strong>\n   : Assesses whether the generated answer aligns with the reference answer, given the query (this necessitates labeled data).\n  </li>\n  <li>\n   <strong>\n    Faithfulness\n   </strong>\n   : Measures if the answer remains true to the retrieved contexts, essentially checking for the absence of hallucinations.\n  </li>\n  <li>\n   <strong>\n    Context Relevancy\n   </strong>\n   : Evaluate the relevance of both the retrieved context and the answer to the query.\n  </li>\n </ol>\n <p>\n  For an in-depth exploration, our documentation is available\n  <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/evaluating/root.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   here\n  </a>\n  .\n </p>\n <p>\n  For those who are exploring the Prometheus model for the first time, the paper summary by\n  <a href=\"https://www.linkedin.com/in/nerdai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Andrei\n  </a>\n  is an excellent resource to gain a better understanding.\n </p>\n <p>\n  A crucial aspect to remember when using the Prometheus model is its dependence on rubric scores within the prompt for effective evaluation. An example of such Rubric scores in the context of\n  <code class=\"cw py pz qa qb b\">\n   Correctness Evaluation\n  </code>\n  is as follows:\n </p>\n <blockquote>\n  <p class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\" id=\"26fb\">\n   ###Score Rubrics: \nScore 1: If the generated answer is not relevant to the user query and reference answer. \nScore 2: If the generated answer is according to reference answer but not relevant to user query. \nScore 3: If the generated answer is relevant to the user query and reference answer but contains mistakes. \nScore 4: If the generated answer is relevant to the user query and has the exact same metrics as the reference answer, but it is not as concise. \nScore 5: If the generated answer is relevant to the user query and fully correct according to the reference answer.\n  </p>\n </blockquote>\n <p>\n  You\u2019ll find comprehensive details on this in the prompts section of this tutorial.\n </p>\n <p>\n  For a detailed walkthrough of the code, feel free to follow along with our\n  <a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/evaluation/prometheus_evaluation.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Google Colab Notebook\n  </a>\n  accompanying this blog post. In the notebook, we conducted evaluations on both the\n  <code class=\"cw py pz qa qb b\">\n   Paul Graham Essay Text\n  </code>\n  and the\n  <code class=\"cw py pz qa qb b\">\n   Llama2 Paper\n  </code>\n  . However, for this blog post, we\u2019ll focus exclusively on the Llama2 Paper, as it revealed some particularly interesting insights.\n </p>\n <h1>\n  Outline:\n </h1>\n <ol>\n  <li>\n   Setup Evaluation Pipeline.\n  </li>\n </ol>\n <ul>\n  <li>\n   Download Dataset.\n  </li>\n  <li>\n   Define LLMs (Prometheus, GPT-4) needed for evaluation.\n  </li>\n  <li>\n   Define Correctness, Faithfulness, and Relevancy prompt templates.\n  </li>\n  <li>\n   Define Prometheus, GPT-4 Evaluators, and Batch Eval Runner.\n  </li>\n  <li>\n   Run the Correctness, Faithfulness, and Relevancy Evaluation over the Llama2 dataset.\n  </li>\n </ul>\n <p>\n  2. Results\n </p>\n <ul>\n  <li>\n   Correctness Evaluation score distribution between Prometheus and GPT-4.\n  </li>\n  <li>\n   Feedback comparison between Prometheus and GPT-4 for correctness evaluation.\n  </li>\n  <li>\n   Faithfulness and Relevancy Evaluation scores with Prometheus and GPT-4.\n  </li>\n  <li>\n   Hamming Distance comparison between Prometheus and GPT-4.\n  </li>\n  <li>\n   Feedback comparison between Prometheus and GPT-4 for Faithfulness and Relevancy\n  </li>\n </ul>\n <p>\n  3. Summary with Cost Analysis.\n </p>\n <h1>\n  Setup Evaluation Pipeline\n </h1>\n <p>\n  Please be aware that certain functions mentioned here are not defined in detail within the blog post. We have showcased only the essential parts of the pipeline to provide an overview of its setup. For a comprehensive code walkthrough, we recommend visiting our\n  <a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/evaluation/prometheus_evaluation.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Google Colab Notebook\n  </a>\n  .\n </p>\n <h2>\n  Download Dataset\n </h2>\n <p>\n  We will use the Llama2 paper dataset from Llama Datasets which contains 100 questions and their reference answers.\n </p>\n <pre><span class=\"qz nb gt qb b bf ra rb l rc rd\" id=\"87c5\"><span class=\"hljs-keyword\">from</span> llama_index.llama_dataset <span class=\"hljs-keyword\">import</span> download_llama_dataset\n\nllama2_rag_dataset, llama2_documents = download_llama_dataset(\n    <span class=\"hljs-string\">\"Llama2PaperDataset\"</span>, <span class=\"hljs-string\">\"./data/llama2\"</span>\n)</span></pre>\n <h2>\n  Define Prometheus LLM hosted on HuggingFace And OpenAI for creating an Index (RAG) pipeline\n </h2>\n <p>\n  We need to host the model on HF Inference endpoint using Nvidia A100 GPU, 80 GB RAM.\n </p>\n <pre><span class=\"qz nb gt qb b bf ra rb l rc rd\" id=\"74bd\"><span class=\"hljs-keyword\">from</span> llama_index.llms <span class=\"hljs-keyword\">import</span> HuggingFaceInferenceAPI\n<span class=\"hljs-keyword\">import</span> os\n\nHF_TOKEN = <span class=\"hljs-string\">\"YOUR HF TOKEN\"</span>\nHF_ENDPOINT_URL = <span class=\"hljs-string\">\"HF END POINT URL\"</span>\n\nprometheus_llm = HuggingFaceInferenceAPI(\n    model_name=HF_ENDPOINT_URL,\n    token=HF_TOKEN,\n    temperature=<span class=\"hljs-number\">0.1</span>,\n    do_sample=<span class=\"hljs-literal\">True</span>,\n    top_p=<span class=\"hljs-number\">0.95</span>,\n    top_k=<span class=\"hljs-number\">40</span>,\n    repetition_penalty=<span class=\"hljs-number\">1.1</span>,\n)\n\nos.environ[<span class=\"hljs-string\">\"OPENAI_API_KEY\"</span>] = <span class=\"hljs-string\">\"YOUR OPENAI API KEY\"</span>\n\n<span class=\"hljs-keyword\">from</span> llama_index.llms <span class=\"hljs-keyword\">import</span> OpenAI\n\ngpt4_llm = OpenAI(<span class=\"hljs-string\">\"gpt-4\"</span>)</span></pre>\n <h2>\n  Prompt templates.\n </h2>\n <p>\n  We will use the same prompts for the Prometheus model and GPT-4 to make consistent performance comparisons.\n </p>\n <p>\n  <strong>\n   Correctness Evaluation Prompt:\n  </strong>\n </p>\n <pre><span class=\"qz nb gt qb b bf ra rb l rc rd\" id=\"0cb1\">prometheus_correctness_eval_prompt_template = <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"###Task Description: An instruction (might include an Input inside it), a query, a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given. \n   1. Write a detailed feedback that assesses the quality of the response strictly based on the given score rubric, not evaluating in general. \n   2. After writing a feedback, write a score that is either 1 or 2 or 3 or 4 or 5. You should refer to the score rubric. \n   3. The output format should look as follows: 'Feedback: (write a feedback for criteria) [RESULT] (1 or 2 or 3 or 4 or 5)'\n   4. Please do not generate any other opening, closing, and explanations. \n   5. Only evaluate on common things between generated answer and reference answer. Don't evaluate on things which are present in reference answer but not in generated answer.\n\n   ###The instruction to evaluate: Your task is to evaluate the generated answer and reference answer for the query: {query}\n   \n   ###Generate answer to evaluate: {generated_answer} \n\n   ###Reference Answer (Score 5): {reference_answer}\n            \n   ###Score Rubrics: \n   Score 1: If the generated answer is not relevant to the user query and reference answer.\n   Score 2: If the generated answer is according to reference answer but not relevant to user query.\n   Score 3: If the generated answer is relevant to the user query and reference answer but contains mistakes.\n   Score 4: If the generated answer is relevant to the user query and has the exact same metrics as the reference answer, but it is not as concise.\n   Score 5: If the generated answer is relevant to the user query and fully correct according to the reference answer.\n    \n   ###Feedback:\"</span><span class=\"hljs-string\">\"\"</span>\n</span></pre>\n <p>\n  <strong>\n   Faithfulness Evaluation Prompt:\n  </strong>\n </p>\n <pre><span class=\"qz nb gt qb b bf ra rb l rc rd\" id=\"650f\">prometheus_faithfulness_eval_prompt_template= <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"###Task Description: An instruction (might include an Input inside it), an information, a context, and a score rubric representing evaluation criteria are given.\n1. You are provided with evaluation task with the help of information, context information to give result based on score rubrics.\n2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general.\n3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric.\n4. The output format should look as follows: \"</span><span class=\"hljs-title class_\">Feedback</span>: (write a feedback <span class=\"hljs-keyword\">for</span> criteria) [<span class=\"hljs-variable constant_\">RESULT</span>] (<span class=\"hljs-variable constant_\">YES</span> or <span class=\"hljs-variable constant_\">NO</span>)\u201d\n<span class=\"hljs-number\">5.</span> <span class=\"hljs-title class_\">Please</span> <span class=\"hljs-keyword\">do</span> not generate any other opening, closing, and explanations.\n\n###<span class=\"hljs-title class_\">The</span> instruction to <span class=\"hljs-attr\">evaluate</span>: <span class=\"hljs-title class_\">Your</span> task is to evaluate <span class=\"hljs-keyword\">if</span> the given piece <span class=\"hljs-keyword\">of</span> information is supported by context.\n\n###<span class=\"hljs-title class_\">Information</span>: {query_str}\n\n###<span class=\"hljs-title class_\">Context</span>: {context_str}\n\n###<span class=\"hljs-title class_\">Score</span> <span class=\"hljs-title class_\">Rubrics</span>:\n<span class=\"hljs-title class_\">Score</span> <span class=\"hljs-attr\">YES</span>: <span class=\"hljs-title class_\">If</span> the given piece <span class=\"hljs-keyword\">of</span> information is supported by context.\n<span class=\"hljs-title class_\">Score</span> <span class=\"hljs-attr\">NO</span>: <span class=\"hljs-title class_\">If</span> the given piece <span class=\"hljs-keyword\">of</span> information is not supported by context\n\n###<span class=\"hljs-title class_\">Feedback</span>: <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n\nprometheus_faithfulness_refine_prompt_template= \"</span><span class=\"hljs-string\">\"\"</span>###<span class=\"hljs-title class_\">Task</span> <span class=\"hljs-title class_\">Description</span>: <span class=\"hljs-title class_\">An</span> instruction (might include an <span class=\"hljs-title class_\">Input</span> inside it), a information, a context information, an existing answer, and a score rubric representing a evaluation criteria are given.\n<span class=\"hljs-number\">1.</span> <span class=\"hljs-title class_\">You</span> are provided <span class=\"hljs-keyword\">with</span> evaluation task <span class=\"hljs-keyword\">with</span> the help <span class=\"hljs-keyword\">of</span> information, context information and an existing answer.\n<span class=\"hljs-number\">2.</span> <span class=\"hljs-title class_\">Write</span> a detailed feedback based on evaluation task and the given score rubric, not evaluating <span class=\"hljs-keyword\">in</span> general.\n<span class=\"hljs-number\">3.</span> <span class=\"hljs-title class_\">After</span> writing a feedback, write a score that is <span class=\"hljs-variable constant_\">YES</span> or <span class=\"hljs-variable constant_\">NO</span>. <span class=\"hljs-title class_\">You</span> should refer to the score rubric.\n<span class=\"hljs-number\">4.</span> <span class=\"hljs-title class_\">The</span> output format should look <span class=\"hljs-keyword\">as</span> <span class=\"hljs-attr\">follows</span>: <span class=\"hljs-string\">\"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)\"</span>\n<span class=\"hljs-number\">5.</span> <span class=\"hljs-title class_\">Please</span> <span class=\"hljs-keyword\">do</span> not generate any other opening, closing, and explanations.\n\n###<span class=\"hljs-title class_\">The</span> instruction to <span class=\"hljs-attr\">evaluate</span>: <span class=\"hljs-title class_\">If</span> the information is present <span class=\"hljs-keyword\">in</span> the context and also provided <span class=\"hljs-keyword\">with</span> an existing answer.\n\n###<span class=\"hljs-title class_\">Existing</span> <span class=\"hljs-attr\">answer</span>: {existing_answer}\n\n###<span class=\"hljs-title class_\">Information</span>: {query_str}\n\n###<span class=\"hljs-title class_\">Context</span>: {context_msg}\n\n###<span class=\"hljs-title class_\">Score</span> <span class=\"hljs-title class_\">Rubrics</span>:\n<span class=\"hljs-title class_\">Score</span> <span class=\"hljs-attr\">YES</span>: <span class=\"hljs-title class_\">If</span> the existing answer is already <span class=\"hljs-variable constant_\">YES</span> or <span class=\"hljs-title class_\">If</span> the <span class=\"hljs-title class_\">Information</span> is present <span class=\"hljs-keyword\">in</span> the context.\n<span class=\"hljs-title class_\">Score</span> <span class=\"hljs-attr\">NO</span>: <span class=\"hljs-title class_\">If</span> the existing answer is <span class=\"hljs-variable constant_\">NO</span> and <span class=\"hljs-title class_\">If</span> the <span class=\"hljs-title class_\">Information</span> is not present <span class=\"hljs-keyword\">in</span> the context.\n\n###<span class=\"hljs-title class_\">Feedback</span>: <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"</span></span></pre>\n <p>\n  <strong>\n   Relevancy Evaluation Prompt:\n  </strong>\n </p>\n <pre><span class=\"qz nb gt qb b bf ra rb l rc rd\" id=\"8df3\">prometheus_relevancy_eval_prompt_template = <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"###Task Description: An instruction (might include an Input inside it), a query with response, context, and a score rubric representing evaluation criteria are given. \n       1. You are provided with evaluation task with the help of a query with response and context.\n       2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general. \n       3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric. \n       4. The output format should look as follows: \"</span><span class=\"hljs-title class_\">Feedback</span>: (write a feedback <span class=\"hljs-keyword\">for</span> criteria) [<span class=\"hljs-variable constant_\">RESULT</span>] (<span class=\"hljs-variable constant_\">YES</span> or <span class=\"hljs-variable constant_\">NO</span>)\u201d \n       <span class=\"hljs-number\">5.</span> <span class=\"hljs-title class_\">Please</span> <span class=\"hljs-keyword\">do</span> not generate any other opening, closing, and explanations. \n\n        ###<span class=\"hljs-title class_\">The</span> instruction to <span class=\"hljs-attr\">evaluate</span>: <span class=\"hljs-title class_\">Your</span> task is to evaluate <span class=\"hljs-keyword\">if</span> the response <span class=\"hljs-keyword\">for</span> the query is <span class=\"hljs-keyword\">in</span> line <span class=\"hljs-keyword\">with</span> the context information provided.\n\n        ###<span class=\"hljs-title class_\">Query</span> and <span class=\"hljs-title class_\">Response</span>: {query_str} \n\n        ###<span class=\"hljs-title class_\">Context</span>: {context_str}\n            \n        ###<span class=\"hljs-title class_\">Score</span> <span class=\"hljs-title class_\">Rubrics</span>: \n        <span class=\"hljs-title class_\">Score</span> <span class=\"hljs-attr\">YES</span>: <span class=\"hljs-title class_\">If</span> the response <span class=\"hljs-keyword\">for</span> the query is <span class=\"hljs-keyword\">in</span> line <span class=\"hljs-keyword\">with</span> the context information provided.\n        <span class=\"hljs-title class_\">Score</span> <span class=\"hljs-attr\">NO</span>: <span class=\"hljs-title class_\">If</span> the response <span class=\"hljs-keyword\">for</span> the query is not <span class=\"hljs-keyword\">in</span> line <span class=\"hljs-keyword\">with</span> the context information provided.\n    \n        ###<span class=\"hljs-title class_\">Feedback</span>: <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n\nprometheus_relevancy_refine_prompt_template = \"</span><span class=\"hljs-string\">\"\"</span>###<span class=\"hljs-title class_\">Task</span> <span class=\"hljs-title class_\">Description</span>: <span class=\"hljs-title class_\">An</span> instruction (might include an <span class=\"hljs-title class_\">Input</span> inside it), a query <span class=\"hljs-keyword\">with</span> response, context, an existing answer, and a score rubric representing a evaluation criteria are given. \n   <span class=\"hljs-number\">1.</span> <span class=\"hljs-title class_\">You</span> are provided <span class=\"hljs-keyword\">with</span> evaluation task <span class=\"hljs-keyword\">with</span> the help <span class=\"hljs-keyword\">of</span> a query <span class=\"hljs-keyword\">with</span> response and context and an existing answer.\n   <span class=\"hljs-number\">2.</span> <span class=\"hljs-title class_\">Write</span> a detailed feedback based on evaluation task and the given score rubric, not evaluating <span class=\"hljs-keyword\">in</span> general. \n   <span class=\"hljs-number\">3.</span> <span class=\"hljs-title class_\">After</span> writing a feedback, write a score that is <span class=\"hljs-variable constant_\">YES</span> or <span class=\"hljs-variable constant_\">NO</span>. <span class=\"hljs-title class_\">You</span> should refer to the score rubric. \n   <span class=\"hljs-number\">4.</span> <span class=\"hljs-title class_\">The</span> output format should look <span class=\"hljs-keyword\">as</span> <span class=\"hljs-attr\">follows</span>: <span class=\"hljs-string\">\"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)\"</span> \n   <span class=\"hljs-number\">5.</span> <span class=\"hljs-title class_\">Please</span> <span class=\"hljs-keyword\">do</span> not generate any other opening, closing, and explanations. \n\n   ###<span class=\"hljs-title class_\">The</span> instruction to <span class=\"hljs-attr\">evaluate</span>: <span class=\"hljs-title class_\">Your</span> task is to evaluate <span class=\"hljs-keyword\">if</span> the response <span class=\"hljs-keyword\">for</span> the query is <span class=\"hljs-keyword\">in</span> line <span class=\"hljs-keyword\">with</span> the context information provided.\n\n   ###<span class=\"hljs-title class_\">Query</span> and <span class=\"hljs-title class_\">Response</span>: {query_str} \n\n   ###<span class=\"hljs-title class_\">Context</span>: {context_str}\n            \n   ###<span class=\"hljs-title class_\">Score</span> <span class=\"hljs-title class_\">Rubrics</span>: \n   <span class=\"hljs-title class_\">Score</span> <span class=\"hljs-attr\">YES</span>: <span class=\"hljs-title class_\">If</span> the existing answer is already <span class=\"hljs-variable constant_\">YES</span> or <span class=\"hljs-title class_\">If</span> the response <span class=\"hljs-keyword\">for</span> the query is <span class=\"hljs-keyword\">in</span> line <span class=\"hljs-keyword\">with</span> the context information provided.\n   <span class=\"hljs-title class_\">Score</span> <span class=\"hljs-attr\">NO</span>: <span class=\"hljs-title class_\">If</span> the existing answer is <span class=\"hljs-variable constant_\">NO</span> and <span class=\"hljs-title class_\">If</span> the response <span class=\"hljs-keyword\">for</span> the query is <span class=\"hljs-keyword\">in</span> line <span class=\"hljs-keyword\">with</span> the context information provided.\n    \n   ###<span class=\"hljs-title class_\">Feedback</span>: <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"</span></span></pre>\n <h2>\n  Define Correctness, FaithFulness, Relevancy Evaluators\n </h2>\n <pre><span class=\"qz nb gt qb b bf ra rb l rc rd\" id=\"9f97\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> ServiceContext\n<span class=\"hljs-keyword\">from</span> llama_index.evaluation <span class=\"hljs-keyword\">import</span> (\n    CorrectnessEvaluator,\n    FaithfulnessEvaluator,\n    RelevancyEvaluator,\n)\n<span class=\"hljs-keyword\">from</span> llama_index.callbacks <span class=\"hljs-keyword\">import</span> CallbackManager, TokenCountingHandler\n<span class=\"hljs-keyword\">import</span> tiktoken\n\n<span class=\"hljs-comment\"># Provide Prometheus model in service_context</span>\nprometheus_service_context = ServiceContext.from_defaults(llm=prometheus_llm)\n\n<span class=\"hljs-comment\"># CorrectnessEvaluator with Prometheus model</span>\nprometheus_correctness_evaluator = CorrectnessEvaluator(\n    service_context=prometheus_service_context,\n    parser_function=parser_function,\n    eval_template=prometheus_correctness_eval_prompt_template,\n)\n\n<span class=\"hljs-comment\"># FaithfulnessEvaluator with Prometheus model</span>\nprometheus_faithfulness_evaluator = FaithfulnessEvaluator(\n    service_context=prometheus_service_context,\n    eval_template=prometheus_faithfulness_eval_prompt_template,\n    refine_template=prometheus_faithfulness_refine_prompt_template,\n)\n\n<span class=\"hljs-comment\"># RelevancyEvaluator with Prometheus model</span>\nprometheus_relevancy_evaluator = RelevancyEvaluator(\n    service_context=prometheus_service_context,\n    eval_template=prometheus_relevancy_eval_prompt_template,\n    refine_template=prometheus_relevancy_refine_prompt_template,\n)\n\n<span class=\"hljs-comment\"># Set the encoding model to `gpt-4` for token counting.</span>\ntoken_counter = TokenCountingHandler(\n    tokenizer=tiktoken.encoding_for_model(<span class=\"hljs-string\">\"gpt-4\"</span>).encode\n)\n\ncallback_manager = CallbackManager([token_counter])\n\n<span class=\"hljs-comment\"># Provide GPT-4 model in service_context</span>\ngpt4_service_context = ServiceContext.from_defaults(\n    llm=gpt4_llm, callback_manager=callback_manager\n)\n\n<span class=\"hljs-comment\"># CorrectnessEvaluator with GPT-4 model</span>\ngpt4_correctness_evaluator = CorrectnessEvaluator(\n    service_context=gpt4_service_context,\n    <span class=\"hljs-comment\"># parser_function=parser_function,</span>\n)\n\n<span class=\"hljs-comment\"># FaithfulnessEvaluator with GPT-4 model</span>\ngpt4_faithfulness_evaluator = FaithfulnessEvaluator(\n    service_context=gpt4_service_context,\n    eval_template=prometheus_faithfulness_eval_prompt_template,\n    refine_template=prometheus_faithfulness_refine_prompt_template,\n)\n\n<span class=\"hljs-comment\"># RelevancyEvaluator with GPT-4 model</span>\ngpt4_relevancy_evaluator = RelevancyEvaluator(\n    service_context=gpt4_service_context,\n    eval_template=prometheus_relevancy_eval_prompt_template,\n    refine_template=prometheus_relevancy_refine_prompt_template,\n)\n\n<span class=\"hljs-comment\"># create a dictionary of evaluators</span>\nprometheus_evaluators = {\n    <span class=\"hljs-string\">\"correctness\"</span>: prometheus_correctness_evaluator,\n    <span class=\"hljs-string\">\"faithfulness\"</span>: prometheus_faithfulness_evaluator,\n    <span class=\"hljs-string\">\"relevancy\"</span>: prometheus_relevancy_evaluator,\n}\n\ngpt4_evaluators = {\n    <span class=\"hljs-string\">\"correctness\"</span>: gpt4_correctness_evaluator,\n    <span class=\"hljs-string\">\"faithfulness\"</span>: gpt4_faithfulness_evaluator,\n    <span class=\"hljs-string\">\"relevancy\"</span>: gpt4_relevancy_evaluator,\n}</span></pre>\n <h2>\n  Function to run batch evaluations on defined evaluators\n </h2>\n <pre><span class=\"qz nb gt qb b bf ra rb l rc rd\" id=\"b1fb\"><span class=\"hljs-keyword\">from</span> llama_index.evaluation <span class=\"hljs-keyword\">import</span> BatchEvalRunner\n\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">batch_eval_runner</span>(<span class=\"hljs-params\">\n    evaluators, query_engine, questions, reference=<span class=\"hljs-literal\">None</span>, num_workers=<span class=\"hljs-number\">8</span>\n</span>):\n    batch_runner = BatchEvalRunner(\n        evaluators, workers=num_workers, show_progress=<span class=\"hljs-literal\">True</span>\n    )\n\n    eval_results = <span class=\"hljs-keyword\">await</span> batch_runner.aevaluate_queries(\n        query_engine, queries=questions, reference=reference\n    )\n\n    <span class=\"hljs-keyword\">return</span> eval_results</span></pre>\n <h2>\n  Get Query Engine, Questions, and References.\n </h2>\n <pre><span class=\"qz nb gt qb b bf ra rb l rc rd\" id=\"d93d\">query_engine, rag_dataset = create_query_engine_rag_dataset(<span class=\"hljs-string\">\"./data/llama2\"</span>)\n\nquestions = [example.query <span class=\"hljs-keyword\">for</span> example <span class=\"hljs-keyword\">in</span> rag_dataset.examples]\n\nreference = [[example.reference_answer] <span class=\"hljs-keyword\">for</span> example <span class=\"hljs-keyword\">in</span> rag_dataset.examples]</span></pre>\n <h2>\n  Compute Correctness, Faithfulness, and Relevancy Evaluation.\n </h2>\n <pre><span class=\"qz nb gt qb b bf ra rb l rc rd\" id=\"72a2\">prometheus_eval_results = await batch_eval_runner(\n    prometheus_evaluators, query_engine, questions, reference\n)\n\ngpt4_eval_results = await batch_eval_runner(\n    gpt4_evaluators, query_engine, questions, reference\n)</span></pre>\n <h1>\n  Results\n </h1>\n <h2>\n  Correctness Evaluation score distribution.\n </h2>\n <h2>\n  With the Prometheus Model:\n </h2>\n <blockquote>\n  <p class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\" id=\"e31c\">\n   3.0: 56.0, \n1.0: 26.0, \n5.0: 9.0, \n4.0: 8.0, \n2.0: 1.0\n  </p>\n </blockquote>\n <h2>\n  With GPT-4 Model:\n </h2>\n <blockquote>\n  <p class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\" id=\"d4f1\">\n   4.5: 57.99, \n1.0: 6.0, \n4.0: 12.0, \n5.0: 10.0, \n2.0: 5.0, \n3.5: 5.0, \n2.5: 3.0, \n3.0: 1.0\n  </p>\n </blockquote>\n <h2>\n  <strong>\n   Observation:\n  </strong>\n </h2>\n <p>\n  Prometheus Evaluator gives more\n  <code class=\"cw py pz qa qb b\">\n   3.0\n  </code>\n  scores compared to GPT-4 which gives\n  <code class=\"cw py pz qa qb b\">\n   4.5\n  </code>\n  . Let\u2019s investigate it with the help of feedback.\n </p>\n <h2>\n  Feedback comparison between Prometheus and GPT-4 for correctness.\n </h2>\n <blockquote>\n  <p class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\" id=\"c272\">\n   <strong>\n    <em class=\"gt\">\n     Query:\n    </em>\n   </strong>\n   <em class=\"gt\">\n    Based on the abstract of \u201cLlama 2: Open Foundation and Fine-Tuned Chat Models,\u201d what are the two primary objectives achieved in this work, and what is the range of parameters for the large language models developed?\n   </em>\n  </p>\n  <p class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\" id=\"3a16\">\n   <strong>\n    <em class=\"gt\">\n     Generated Answer:\n    </em>\n   </strong>\n   <em class=\"gt\">\n    The two primary objectives achieved in this work are the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. The range of parameters for the large language models developed is from 7 billion to 70 billion.\n   </em>\n  </p>\n  <p class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\" id=\"2c5f\">\n   <strong>\n    <em class=\"gt\">\n     Reference Answer:\n    </em>\n   </strong>\n   <em class=\"gt\">\n    The two primary objectives achieved in the work described in the abstract of \u201cLlama 2: Open Foundation and Fine-Tuned Chat Models\u201d are: 1. The development and release of a collection of pretrained and fine-tuned large language models (LLMs) specifically optimized for dialogue use cases. 2. The demonstration that these fine-tuned LLMs, referred to as Llama 2-Chat, outperform open-source chat models on most benchmarks tested and may be a suitable substitute for closed-source models, particularly in terms of helpfulness and safety based on human evaluations. The range of parameters for the large language models developed in this work is from 7 billion to 70 billion parameters.\n   </em>\n  </p>\n  <p class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\" id=\"cd6c\">\n   <strong>\n    <em class=\"gt\">\n     Prometheus Feedback:\n    </em>\n   </strong>\n   <em class=\"gt\">\n    The generated response is relevant to the user query and correctly identifies the two primary objectives of the work described in the abstract of \u201cLlama 2: Open Foundation and Fine-Tuned Chat Models.\u201d However, it does not mention the demonstration of the fine-tuned LLMs outperforming open-source chat models on most benchmarks tested, which is a key point in the reference response. The range of parameters for the large language models developed is correctly identified, but the response does not mention the specific models referred to as Llama 2-Chat. So the overall score is 3.\n   </em>\n  </p>\n  <p class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\" id=\"6fc5\">\n   <strong>\n    <em class=\"gt\">\n     Prometheus Score:\n    </em>\n   </strong>\n   <em class=\"gt\">\n    3.0\n   </em>\n  </p>\n  <p class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\" id=\"a3f6\">\n   <strong>\n    <em class=\"gt\">\n     GPT-4 Feedback:\n    </em>\n   </strong>\n   <em class=\"gt\">\n    The generated answer is relevant and almost fully correct. It correctly identifies the two primary objectives and the range of parameters for the large language models. However, it misses the detail about Llama 2-Chat outperforming other models on most benchmarks and potentially being a suitable substitute for closed-source models.\n   </em>\n  </p>\n  <p class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\" id=\"8c97\">\n   <strong>\n    <em class=\"gt\">\n     GPT-4 Score:\n    </em>\n   </strong>\n   <em class=\"gt\">\n    4.5\n   </em>\n  </p>\n </blockquote>\n <p>\n  <strong>\n   Observation:\n  </strong>\n </p>\n <p>\n  The feedback from Prometheus is a little more precise compared to GPT-4 and it penalizes and gives a score of\n  <code class=\"cw py pz qa qb b\">\n   3.0\n  </code>\n  but GPT-4 gives a score of\n  <code class=\"cw py pz qa qb b\">\n   4.5\n  </code>\n  . Prometheus penalizes more even if some of the text in the reference answer is missed in the generated answer.\n </p>\n <h2>\n  Faithfulness and Relevancy Evaluation scores.\n </h2>\n <h2>\n  With the Prometheus Model:\n </h2>\n <blockquote>\n  <p class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\" id=\"c4ea\">\n   faithfulness Score: 0.39 \nrelevancy Score: 0.57\n  </p>\n </blockquote>\n <h2>\n  With GPT-4 Model:\n </h2>\n <blockquote>\n  <p class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\" id=\"c9d6\">\n   faithfulness Score: 0.93\nrelevancy Score: 0.98\n  </p>\n </blockquote>\n <p>\n  <strong>\n   Observation:\n  </strong>\n </p>\n <p>\n  We see a higher faithfulness and relevancy score with GPT-4 compared to the Prometheus model. Let\u2019s investigate it through feedback.\n </p>\n <h2>\n  Hamming Distance comparison between Prometheus and GPT-4:\n </h2>\n <blockquote>\n  <p class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\" id=\"48b6\">\n   Faithfulness Hamming Distance: 58 \nRelevancy Hamming Distance: 41\n  </p>\n </blockquote>\n <h2>\n  Observation:\n </h2>\n <p>\n  The comparison reveals that approximately\n  <code class=\"cw py pz qa qb b\">\n   42%\n  </code>\n  of the scores in case of\n  <code class=\"cw py pz qa qb b\">\n   Faithfulness\n  </code>\n  and\n  <code class=\"cw py pz qa qb b\">\n   59%\n  </code>\n  in case of\n  <code class=\"cw py pz qa qb b\">\n   Relevancy\n  </code>\n  are common between Prometheus and GPT-4 evaluations. This indicates a decent amount of correlation in terms of faithfulness and relevance scoring between the Prometheus and GPT-4 models.\n </p>\n <h2>\n  Feedback comparison between Prometheus and GPT-4 for Faithfulness and Relevancy\n </h2>\n <blockquote>\n  <p class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\" id=\"ac48\">\n   <strong>\n    Query:\n   </strong>\n   Based on the abstract of \u201cLlama 2: Open Foundation and Fine-Tuned Chat Models,\u201d what are the two primary objectives achieved in this work, and what is the range of parameters for the large language models developed? Generated Answer: The two primary objectives achieved in this work are the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. The range of parameters for the large language models developed is from 7 billion to 70 billion.\n  </p>\n  <p class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\" id=\"be60\">\n   <strong>\n    Context-1:\n   </strong>\n   Llama 2 : Open Foundation and Fine-Tuned Chat Models Hugo Touvron\u2217Louis Martin\u2020Kevin Stone\u2020 Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic Sergey Edunov Thomas Scialom\u2217 GenAI, Meta Abstract In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed- source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs. \u2217Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com \u2020Second author Contributions for all the authors can be found in Section A.1.arXiv:2307.09288v2 [cs.CL] 19 Jul 2023\n  </p>\n  <p class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\" id=\"7e4b\">\n   <strong>\n    Context-2:\n   </strong>\n   (2021)alsoilluminatesthedifficultiestiedtochatbot-oriented LLMs, with concerns ranging from privacy to misleading expertise claims. Deng et al. (2023) proposes a taxonomic framework to tackle these issues, and Bergman et al. (2022) delves into the balance between potential positive and negative impacts from releasing dialogue models. InvestigationsintoredteamingrevealspecificchallengesintunedLLMs,withstudiesbyGangulietal.(2022) and Zhuoet al. (2023) showcasing a variety ofsuccessful attack typesand their effects onthe generation of harmful content. National security agencies and various researchers, such as (Mialon et al., 2023), have also raisedredflagsaroundadvancedemergentmodelbehaviors,cyberthreats,andpotentialmisuseinareaslike biological warfare. Lastly, broader societal issues like job displacement due to accelerated AI research and an over-reliance on LLMs leading to training data degradation are also pertinent considerations (Acemoglu andRestrepo,2018;AutorandSalomons,2018;Webb,2019;Shumailovetal.,2023). Wearecommittedto continuing our work engaging with the broader policy, academic, and industry community on these issues. 7 Conclusion Inthisstudy,wehaveintroduced Llama 2,anewfamilyofpretrainedandfine-tunedmodelswithscales of7billionto70billionparameters. Thesemodelshavedemonstratedtheircompetitivenesswithexisting open-source chat models, as well as competency that is equivalent to some proprietary models on evaluation setsweexamined,althoughtheystilllagbehindothermodelslikeGPT-4. Wemeticulouslyelaboratedonthe methodsandtechniquesappliedinachievingourmodels,withaheavyemphasisontheiralignmentwiththe principlesofhelpfulnessandsafety. Tocontributemoresignificantlytosocietyandfosterthepaceofresearch, wehaveresponsiblyopenedaccessto Llama 2 andLlama 2-Chat . Aspartofourongoingcommitmentto transparency and safety, we plan to make further improvements to Llama 2-Chat in future work. 36\n  </p>\n  <p class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\" id=\"07ab\">\n   <strong>\n    Prometheus Faithfulness Feedback:\n   </strong>\n   The information provided in the context is not supported by the given information. The context is about the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. However, the information provided in the context does not align with the given information. The context does not mention the range of parameters for the large language models developed, which is the primary objective mentioned in the information. The context only talks about the development and release of Llama 2 and its optimization for dialogue use cases, but it does not provide any information about the range of parameters for the large language models developed. So the overall score is NO. [RESULT] NO\n  </p>\n  <p class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\" id=\"98bd\">\n   <strong>\n    Prometheus Faithfulness Score:\n   </strong>\n   0.0\n  </p>\n  <p class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\" id=\"9617\">\n   <strong>\n    Prometheus Relevancy Feedback:\n   </strong>\n   The response is not in line with the context information provided. The query asked for the two primary objectives achieved in the work and the range of parameters for the large language models developed. However, the response provided the abstract of the paper and mentioned the authors, which is not relevant to the query. The response also did not mention the two primary objectives achieved in the work or the range of parameters for the large language models developed. So the overall score is NO. [RESULT] NO\n  </p>\n  <p class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\" id=\"dfd9\">\n   <strong>\n    Prometheus Relevancy Score:\n   </strong>\n   0.0\n  </p>\n  <p class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\" id=\"871f\">\n   <strong>\n    GPT-4 Faithfulness Feedback:\n   </strong>\n   The given piece of information is well supported by the context. The context clearly states that Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), was developed and released. It also mentions that these models range in scale from 7 billion to 70 billion parameters. Furthermore, the context confirms that these models are optimized for dialogue use cases. Therefore, the information provided is accurate and is corroborated by the context. [RESULT] YES\n  </p>\n  <p class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\" id=\"2c3d\">\n   <strong>\n    GPT-4 Faithfulness Score:\n   </strong>\n   1.0\n  </p>\n  <p class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\" id=\"efdb\">\n   <strong>\n    GPT-4 Relevancy Feedback:\n   </strong>\n   The response accurately reflects the context provided. The response correctly identifies the two primary objectives of the work as the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. This is in line with the information provided in the abstract of the context. The response also correctly states the range of parameters for the large language models developed as being from 7 billion to 70 billion, which is also confirmed in the context. Therefore, the response is in line with the context information provided. [RESULT] YES\n  </p>\n  <p class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\" id=\"fc08\">\n   <strong>\n    GPT-4 Relevancy Score:\n   </strong>\n   1.0\n  </p>\n </blockquote>\n <h1>\n  Observation:\n </h1>\n <ol>\n  <li>\n   Prometheus: If you compare the feedback and contexts, there is mention of a range of parameters in the context and response but the feedback says the model could not find such information.\n  </li>\n  <li>\n   GPT-4: Evaluates it correctly, unlike the Prometheus model.\n  </li>\n </ol>\n <h1>\n  Summary\n </h1>\n <ol>\n  <li>\n   The cost for evaluation (approx.):\n   <code class=\"cw py pz qa qb b\">\n    $1.5\n   </code>\n   for Prometheus Model and\n   <code class=\"cw py pz qa qb b\">\n    $15\n   </code>\n   for GPT4.\n  </li>\n  <li>\n   The Prometheus model, though offering more detailed feedback than GPT-4, occasionally provides incorrect feedback, necessitating cautious application.\n  </li>\n  <li>\n   If a generated answer lacks certain facts present in the reference answer, the Prometheus model applies stricter penalties to scores than GPT-4.\n  </li>\n  <li>\n   The faithfulness and relevancy feedback of Prometheus shows more hallucinations/ wrong interpretations in the feedback compared to GPT-4.\n  </li>\n </ol>\n <h1>\n  <strong>\n   Note:\n  </strong>\n </h1>\n <ul>\n  <li>\n   You can check detailed analysis with code on\n   <a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/evaluation/prometheus_evaluation.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Google Colab Notebook\n   </a>\n   .\n  </li>\n  <li>\n   The endpoint on HF is served on AWS Nvidia A100G \u00b7 1x GPU \u00b7 80 GB which costs $6.5/h. (We extend our gratitude to the Hugging Face team for their assistance whenever we encounter issues.)\n  </li>\n  <li>\n   We used the\n   <a href=\"https://huggingface.co/kaist-ai/prometheus-13b-v1.0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Prometheus model\n   </a>\n   for the analysis here. We also made a similar analysis with the\n   <a href=\"https://huggingface.co/TheBloke/prometheus-13B-v1.0-GPTQ\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    GPTQ Quantized version\n   </a>\n   of the\n   <a href=\"https://huggingface.co/kaist-ai/prometheus-13b-v1.0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Prometheus model\n   </a>\n   and observed a bit more hallucinations in feedback compared to the original unquantized model. Thanks to the authors of the paper for open-sourcing the model and\n   <a href=\"https://twitter.com/TheBlokeAI\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tom Jobbins\n   </a>\n   for the quantized version of the model.\n  </li>\n </ul>\n <h1>\n  References:\n </h1>\n <ul>\n  <li>\n   <a href=\"https://arxiv.org/abs/2310.08491\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Prometheus paper\n   </a>\n   .\n  </li>\n  <li>\n   <a href=\"https://huggingface.co/kaist-ai/prometheus-13b-v1.0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Prometheus model on HuggingFace.\n   </a>\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 44487, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "525538fd-c063-4fa1-bf29-c8f33d33b1fc": {"__data__": {"id_": "525538fd-c063-4fa1-bf29-c8f33d33b1fc", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-turns-1-f69dcdd45fe3.html", "file_name": "llamaindex-turns-1-f69dcdd45fe3.html", "file_type": "text/html", "file_size": 8172, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-turns-1-f69dcdd45fe3.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-turns-1-f69dcdd45fe3.html", "file_name": "llamaindex-turns-1-f69dcdd45fe3.html", "file_type": "text/html", "file_size": 8172, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "93d65351a4dfdefd75edf5b136bd525b13d8860e94efaf8fb7aad55763325d18", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  It\u2019s our birthday! One year ago, Jerry pushed his\n  <a href=\"https://github.com/run-llama/llama_index/tree/2e62c6987808797611e9bb7c1ae8c86e72a88727\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   first commit\n  </a>\n  to GPT Index, the project that would become LlamaIndex. It worked with GPT-3, the state of the art model available at the time. That initial version was very simple, but the problem statement \u2014 and the solution \u2014 remain the same:\n </p>\n <blockquote>\n  <p class=\"no np oq nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\" id=\"3ac8\">\n   <em class=\"gt\">\n    one fundamental limitation of GPT-3 is the context size [\u2026] the ability to feed \u201cknowledge\u201d to GPT-3 is mostly limited to this limited prompt size [\u2026] But what if GPT-3 can have access to potentially a much larger database of knowledge[\u2026]?\n   </em>\n  </p>\n </blockquote>\n <p>\n  Twelve months have passed and there\u2019s been a tsunami of new developments in the world of generative AI and LLMs, but the reason LlamaIndex was invented remains: even the most sophisticated model isn\u2019t trained on\n  <strong>\n   your\n  </strong>\n  data, which can be locked behind an API or in a SQL database, and even the latest GPT-4-Turbo context size of 128,000 tokens isn\u2019t enough to hold even a relatively modest dataset. Retrieval-Augmented Generation (RAG) is here to stay.\n </p>\n <h1>\n  Big numbers\n </h1>\n <p>\n  At just 1 year old, LlamaIndex has gotten very big. How big? Here\u2019s some numbers:\n </p>\n <ul>\n  <li>\n   Over\n   <a href=\"https://github.com/run-llama/llama_index/graphs/contributors\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    450 contributors\n   </a>\n   to our open-source library!\n  </li>\n  <li>\n   Nearly 3,000 open-source projects\n   <a href=\"https://github.com/run-llama/llama_index/network/dependents\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    depend on LlamaIndex\n   </a>\n   !\n  </li>\n  <li>\n   Nearly 4,000 members in our Discord (\n   <a href=\"https://discord.com/invite/eN6D2HQ4aX\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    come join us!\n   </a>\n   )\n  </li>\n  <li>\n   47,000 lines of Python in the library! (Don\u2019t worry, it\u2019s still just\n   <a href=\"https://pypi.org/project/llama-index/#files\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    0.5MB to download\n   </a>\n   )\n  </li>\n  <li>\n   Nearly\n   <a href=\"https://pypistats.org/packages/llama-index\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    900,000 downloads every month\n   </a>\n   !\n  </li>\n  <li>\n   RAG deployed among\n   <a href=\"https://openbb.co/blog/breaking-barriers-with-openbb-and-llamaIndex\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    popular\n   </a>\n   <a href=\"https://github.com/imartinez/privateGPT\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    open-source\n   </a>\n   <a href=\"https://github.com/TransformerOptimus/SuperAGI\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    projects\n   </a>\n   , as well as in\n   <a href=\"https://www.gunder.com/news/gunderson-dettmer-launches-chatgd-a-homegrown-generative-ai-chat-app-to-its-lawyers/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    production\n   </a>\n   in\n   <a href=\"https://www.springworks.in/albus/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    enterprise\n   </a>\n   <a href=\"https://mqube.com/blog/4-lessons-from-launching-an-llm-chatbot\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    settings\n   </a>\n   .\n  </li>\n </ul>\n <h1>\n  Big thanks\n </h1>\n <p>\n  But big numbers aside, the thing we\u2019re proudest of is our community: we have users in (nearly) every country in the world, from single hobby developers to Fortune 500 companies and everyone in between. LlamaIndex\u2019s founder, Jerry Liu, says:\n </p>\n <blockquote>\n  <p class=\"no np oq nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\" id=\"309b\">\n   <em class=\"gt\">\n    Our community is everything at LlamaIndex. We love seeing the amazing things people are building every day! It\u2019s what gets us up in the morning and keeps us motivated to keep pushing the boundaries of what developers can do with GenAI. And we\u2019re especially grateful to the developers who give back by pushing PRs, issues and bug reports. They\u2019re what makes the open source world go round.\n   </em>\n  </p>\n </blockquote>\n <h1>\n  Big milestones\n </h1>\n <p>\n  What\u2019s happened in a year? Well, everything! But here\u2019s some highlights:\n </p>\n <ul>\n  <li>\n   November 2022: Launched\n   <a href=\"https://twitter.com/jerryjliu0/status/1590192512639332353?lang=en\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    GPT Tree Index\n   </a>\n   , a way of organizing information into a tree. Based on the initial interest/traction, we expanded this into a List Index and Keyword Index. Then ChatGPT launched in November\n  </li>\n  <li>\n   December 2022: Some big feature releases: support for\n   <a href=\"https://docs.llamaindex.ai/en/stable/understanding/indexing/indexing.html#what-is-an-embedding\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    indexing embeddings + vector stores\n   </a>\n   , and initial data loaders for Notion, Slack, and Google Drive\n  </li>\n  <li>\n   January 2023: LlamaIndex hits Github trending for the first time!\n  </li>\n  <li>\n   February 2023: We launched\n   <a href=\"https://x.com/jerryjliu0/status/1622981509849444354?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaHub\n   </a>\n   with Jesse Zhang, containing an initial repository of data loaders for users to access. We ran a sweepstakes with OctoML and got 50+ data loader submissions!\n  </li>\n  <li>\n   March 2023:\n   <a href=\"https://openai.com/blog/introducing-chatgpt-and-whisper-apis\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    ChatGPT API launched\n   </a>\n   and then\n   <a href=\"https://openai.com/blog/chatgpt-plugins\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Plugins\n   </a>\n   . We scrambled to support the new API +\n   <a href=\"https://gpt-index.readthedocs.io/en/v0.6.3/how_to/integrations/chatgpt_plugins.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Plugin\n   </a>\n   integrations.\n  </li>\n  <li>\n   April 2023: We incorporated!\n  </li>\n  <li>\n   May 2023: At the end of April, we launched\n   <a href=\"https://betterprogramming.pub/llamaindex-0-6-0-a-new-query-interface-over-your-data-331996d47e89\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    0.6.0\n   </a>\n   , where we completely rewrote the entire framework from the ground-up for greater modularity and composability for different levels of abstraction.\n  </li>\n  <li>\n   June 2023: We announced that we raised $8.5M in funding!\n  </li>\n  <li>\n   July 2023: We launched\n   <a href=\"/data-agents-eed797d7972f\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Data Agents\n   </a>\n   + Agent Tools on LlamaHub. We also launched a\n   <a href=\"https://x.com/jerryjliu0/status/1683560483071328256?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Typescript package\n   </a>\n  </li>\n  <li>\n   August 2023: We integrated with\n   <a href=\"https://x.com/jerryjliu0/status/1694370574808887496?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    OpenAI fine-tuning and launched a variety of LLM and embedding fine-tuning abstractions\n   </a>\n   .\n  </li>\n  <li>\n   September 2023: We\n   <a href=\"https://twitter.com/llama_index/status/1699116440056651976\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    launched\n   </a>\n   <a href=\"http://secinsights.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    secinsights.ai\n   </a>\n   \u2014 a production-ready full-stack application\n  </li>\n  <li>\n   October 2023: We launched\n   <a href=\"https://twitter.com/jerryjliu0/status/1719022164203196824\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaIndex Chat\n   </a>\n   \u2014 a full-stack Typescript template.\n  </li>\n  <li>\n   November 2023: Went\n   <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/models/multi_modal.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    fully multi-modal\n   </a>\n   with the release of GPT-4-vision!\n  </li>\n </ul>\n <h1>\n  Big plans\n </h1>\n <p>\n  With all that growth and all those features, what\u2019s next for us? Stay tuned!\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 8123, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a900c947-cee8-4e8f-9915-97ae111d8b8a": {"__data__": {"id_": "a900c947-cee8-4e8f-9915-97ae111d8b8a", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-update-07-10-2023-4ceebdab96cb.html", "file_name": "llamaindex-update-07-10-2023-4ceebdab96cb.html", "file_type": "text/html", "file_size": 14119, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-update-07-10-2023-4ceebdab96cb.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-update-07-10-2023-4ceebdab96cb.html", "file_name": "llamaindex-update-07-10-2023-4ceebdab96cb.html", "file_type": "text/html", "file_size": 14119, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "57c674bc45f424b1e1e456fff5231cea2751ded9c3a0c65bc9af9d2bbfceaca2", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Greetings once again, LlamaIndex community!\n </p>\n <p>\n  Welcome back to our second installment in the LlamaIndex Update series. In our ongoing commitment to keep you informed and engaged with our rapidly evolving open-source project, this blog post brings you more exciting updates on features, webinars, hackathons, and community events.\n </p>\n <p>\n  Building on the foundation of our inaugural post, we will continue to strive to keep both our long-standing contributors and fresh faces synced with our progress. We aim to not just inform but also inspire you to partake in our collective journey towards growth and innovation.\n </p>\n <p>\n  Without further delay, let\u2019s delve into the latest happenings in this edition of the LlamaIndex Update.\n </p>\n <h2>\n  Features And Integrations:\n </h2>\n <ol>\n  <li>\n   LlamaIndex\u2019s partnership with Anyscale uses the Ray platform to boost performance and deployment. It accelerates LlamaIndex\u2019s operations by a factor of ten and streamlines deployment to production servers. The core Ray Distributed Toolkit aids in efficient task parallelization, while Ray Serve ensures easy deployment of query engines to production.\n   <a href=\"https://www.anyscale.com/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-ray\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blogpost\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1673451316398653440?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   LlamaIndex enhanced metadata representation in documents. The\n   <code class=\"cw po pp pq pr b\">\n    extra_info\n   </code>\n   and\n   <code class=\"cw po pp pq pr b\">\n    node_info\n   </code>\n   fields are now replaced with a\n   <code class=\"cw po pp pq pr b\">\n    metadata\n   </code>\n   dictionary. This facilitates precise control over data and allows users to exclude metadata keys during embedding or LLM prediction. This boosts LLM and retrieval performance and offers customizable metadata injection, formatting, and template creation.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_documents.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1673721486757199872?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   LlamaIndex supports both Text Completion API, involving output parsing and input prompt modification, and Structured API, requiring input function signatures and output conversion. Despite Structured API being easier to use, its limited availability keeps Text Completion API relevant. Both are supported by LlamaIndex\u2019s\n   <code class=\"cw po pp pq pr b\">\n    PydanticProgram\n   </code>\n   .\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/how_to/structured_outputs/root.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1674075533548871681?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   LlamaIndex now collaborates with Chainlit.io, facilitating swift construction of advanced chat UIs for any LLM app. This integration, beyond providing a basic chat interface, also logs intermediate results and sources.\n   <a href=\"https://docs.chainlit.io/integrations/llama-index\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blogpost\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1674107773758611456?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   LlamaIndex now incorporates the DePlot model for interpreting charts and plots in QA/chatbot applications. Primarily effective for simple charts, such as bar charts and time series, DePlot converts these visuals into text format for easy embedding, indexing, and usage in downstream applications. This functionality is now accessible via the LlamaHub data loader, expanding LlamaIndex\u2019s capabilities for diverse applications.\n   <a href=\"https://llama-hub-ui.vercel.app/l/file-image_deplot\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1674442367087316992?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   LlamaIndex now incorporates the Github Issues reader, which allows for comprehensive loading and querying of issues from any GitHub repository. Additionally, the Sitemap Loader reader enables users to read all webpages from a specified sitemap.\n   <a href=\"https://twitter.com/llama_index/status/1674443061118791680?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   LlamaIndex introduces the\n   <code class=\"cw po pp pq pr b\">\n    ContextRetrieverOpenAIAgent\n   </code>\n   feature, which enhances tool picking by incorporating more context from user messages. It performs a retrieval step before the LLM call, ensuring increased reliability and better mapping of queries to the right tools, especially in the presence of domain-specific terms. Unlike a \u201cretrieval tool\u201d, this feature guarantees retrieval before any action is taken.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/agent/openai_agent_context_retrieval.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1674807074918928385?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   LlamaIndex now features code-based extraction for efficient data extraction from arbitrary text. This feature includes a \u201cFit\u201d step to generate functions based on training data, and an \u201cInference\u201d step to run these functions on new data. It offers two versions: DFEvaporateProgram for extracting one value per field from a text, and MultiValueEvaporateProgram for extracting multiple values per field. This feature can be used to extract structured data from raw HTML sources and also offers the ability to identify salient fields in a text given a topic.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/output_parsing/evaporate_program.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1675901084840390656?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   LlamaIndex has significantly improved its text-to-SQL capabilities, offering a \u201cDefault\u201d SQL query engine and an SQL query engine with an object index for handling large table schemas. These upgrades simplify the process, requiring only a SQL database for the default engine and enabling indexing of large table schemas with the ObjectIndex. Additionally, LlamaIndex now also integrates with\n   <a href=\"http://twitter.com/duckdb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    duckdb\n   </a>\n   , further enhancing the SQL querying process.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/guides/tutorials/sql_guide.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs_SQL\n   </a>\n   ,\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/index_structs/struct_indices/duckdb_sql_query.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs_duckdb\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1676002583381692421?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   LlamaIndex 0.7.0 enhances modularity for LLM app development. It includes native LLM abstractions for platforms like OpenAI and Hugging Face, a standalone Response Synthesis module, and improved Document Metadata Management. These abstractions can be used independently or integrated into indices/query engines. The Response Synthesis module abstracts away context window limitations, while the Document Metadata Management feature allows deep customization of metadata, potentially boosting retrieval performance.\n   <a href=\"https://medium.com/llamaindex-blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024\" rel=\"noopener\">\n    Blogpost\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1676255154662969345?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   LlamaIndex introduces Recursive Retrieval, a concept that utilizes the hierarchical nature of knowledge. A Node in LlamaIndex can contain references to other retrievers or query engines. This process starts with a retriever and recursively explores links to others. For instance, structured tables from a PDF can be extracted, each represented as a data frame. These tables can be referenced by\n   <code class=\"cw po pp pq pr b\">\n    IndexNode\n   </code>\n   objects embedded with other Nodes. During a query, if an IndexNode is among the top-k nodes, it triggers another retriever or query engine, allowing sophisticated querying overall data.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/query_engine/pdf_tables/recursive_retriever.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1676606169002004481?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   LlamaIndex introduces OpenAI agent streaming for efficient function calling and enhances user experience by providing progress bars during index creation for a real-time understanding of the process duration.\n   <a href=\"https://twitter.com/llama_index/status/1676742253669408768?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   LlamaIndex introduces personalized data interaction through system prompts, callback events for SubQuestionQueryEngine, and a streamlined process for Azure OpenAI integration.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/customization/llms/AzureOpenAI.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs_AOI\n   </a>\n   ,\n   <a href=\"https://github.com/jerryjliu/llama_index/blob/main/docs/examples/chat_engine/chat_engine_personality.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook_personality\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1676981157513265153?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   LlamaIndex leverages LLM\u2019s to automatically extract metadata, significantly enhancing the relevance and precision of information retrieval. This is achieved through five key MetadataExtractor modules (SummaryExtractor, QuestionsAnsweredExtractor, TitleExtractor, MetadataFeatureExtractor) that augment text with rich, context-specific details.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/metadata_extraction/MetadataExtractionSEC.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1677706208017518593?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n </ol>\n <h2>\n  <strong>\n   Tutorials:\n  </strong>\n </h2>\n <ol>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=Vd_8lS1iDBg\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Anyscale tutorial\n   </a>\n   on \u201cHow to Build an LLM Query Engine in 10 Minutes using LlamaIndex.\u201d\n  </li>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=Bu9skgCrJY8\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Erika Cardenas tutorial\n   </a>\n   on how to load data into Weaviate and how to connect LlamaIndex to a Weaviate instance using LlamaIndex.\n  </li>\n  <li>\n   <a href=\"https://betterprogramming.pub/refreshing-private-data-sources-with-llamaindex-document-management-1d1f1529f5eb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Wenqi Glantz tutorial\n   </a>\n   on Refreshing Private Data Sources with LlamaIndex Document Management.\n  </li>\n  <li>\n   <a href=\"https://medium.com/llamaindex-blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7\" rel=\"noopener\">\n    Michael Hunger tutorial\n   </a>\n   on\n   Load in data from\n   <a href=\"https://twitter.com/neo4j\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    neo4j\n   </a>\n   ,\n   <a href=\"https://twitter.com/NebulaGraph\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    NebulaGraph\n   </a>\n   , and index/query with LlamaIndex using GraphDB Cypher and GraphQL data loaders.\n  </li>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=XGBQ_f-Yy48\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Pradip Nichite video tutorial\n   </a>\n   and\n   <a href=\"https://blog.futuresmart.ai/mastering-llamaindex-create-save-load-indexes-customize-llms-prompts-embeddings\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    blogpost\n   </a>\n   on Mastering LlamaIndex: Create, Save &amp; Load Indexes, Customize LLMs, Prompts &amp; Embeddings.\n  </li>\n </ol>\n <h2>\n  Webinars And Podcasts:\n </h2>\n <ol>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=bPoNCkjDmco\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Webinar\n   </a>\n   on Graph Databases, Knowledge Graphs, and RAG with Wey (NebulaGraph).\n  </li>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=gbyfXRxU0Gw\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Webinar\n   </a>\n   with Albus \u2014 a comprehensive Slackbot for enterprise search,\n   <a href=\"http://Xpress.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    xpress.ai\n   </a>\n   \u2014 a low-code solution for building LLM workflows + agents and\n   <a href=\"https://t.co/QAJyGqZPcB\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    ImmigrantFirst.ai\n   </a>\n   \u2014 assistant to help immigrants complete their EB-1A/O1 apps more efficiently.\n  </li>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=NAoqOJrE8rQ&amp;list=PLnTmH22EvTFTtWJRPTNzosDIDblnSg0PD&amp;t=1s\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Data Exchange Podcast\n   </a>\n   with Ben Lorica on LlamaIndex\n  </li>\n </ol>\n <h2>\n  Events:\n </h2>\n <p>\n  Ravi Theja gave talks on \u201cLlamaIndex: Basics To Production\u201d at Accel Partners and Together VC Fund in India.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 14076, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9abdf40-a41f-4099-8d3f-e24b5f91a5c3": {"__data__": {"id_": "c9abdf40-a41f-4099-8d3f-e24b5f91a5c3", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-update-08-01-2023-185514d9b897.html", "file_name": "llamaindex-update-08-01-2023-185514d9b897.html", "file_type": "text/html", "file_size": 18864, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-update-08-01-2023-185514d9b897.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-update-08-01-2023-185514d9b897.html", "file_name": "llamaindex-update-08-01-2023-185514d9b897.html", "file_type": "text/html", "file_size": 18864, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "4dc5247533a0b51d5487e5c7f22616c2354310813e11b533ac72c4fed895a1b3", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Greetings once again, LlamaIndex community!\n </p>\n <p>\n  Welcome to the third installment of our LlamaIndex Update series. Your active participation continues to drive our open-source community forward. We appreciate every contribution whether you\u2019re an experienced LlamaIndex contributor or a newcomer!\n </p>\n <p>\n  In our latest edition, we\u2019ve prepared an assortment of updates for you. From advancements in Data Agents and LlamaIndex TS, benchmarking, and a host of inspiring events, webinars, blog posts, and demos, we\u2019ve got plenty in store.\n </p>\n <p>\n  Without more ado, let\u2019s dive into these updates.\n </p>\n <h1>\n  New Features:\n </h1>\n <ol>\n  <li>\n   We heard you! LlamaIndex has completely revamped our documentation. The update includes new clearer documents on high-level concepts, detailed module guides, comprehensive tutorials, and an all-inclusive API reference.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/index.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1678558820552040448?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   LlamaIndex launched Data Agents, an innovative feature that combines AI agents with data. This launch introduces components like an agent reasoning loop and tool abstractions. Accompanied by an extensive upgrade to LlamaHub, the new feature offers more than 15 tool specs for easy integration. Data Agents enhance query capabilities and are designed to handle varied data applications.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/agents/root.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1679185930287222784?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   ,\n   <a href=\"https://medium.com/llamaindex-blog/data-agents-eed797d7972f\" rel=\"noopener\">\n    Blog Post\n   </a>\n  </li>\n  <li>\n   LlamaIndex launched LlamaIndex.TS, a lean Typescript package for building robust Retrieval Augmented Generation (RAG) systems. It simplifies tasks like document parsing and tackling context window limitations. LlamaIndex.TS is ideal for quickly building apps like using frameworks like Next.JS to chat over your data.\n   <a href=\"https://ts.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1683556970945736704?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   ,\n   <a href=\"https://medium.com/llamaindex-blog/introducing-llamaindex-ts-89f41a1f24ab\" rel=\"noopener\">\n    Blogpost\n   </a>\n  </li>\n  <li>\n   LlamaIndex teams up with Zapier Natural Language API (NLA), reducing the cognitive load on the data agent when handling APIs with multiple parameters. Zapier NLA translates complex third-party APIs into simpler interfaces using a single natural language parameter: instruction. This helps the data agent concentrate on tool selection and action orchestration.\n   <a href=\"https://twitter.com/llama_index/status/1683880312173129728?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   ,\n   <a href=\"https://medium.com/llamaindex-blog/data-agents-zapier-nla-67146395ce1\" rel=\"noopener\">\n    Blogpost\n   </a>\n  </li>\n  <li>\n   LlamaIndex\u2019s\n   <code class=\"cw pt pu pv pw b\">\n    ContextChatEngine\n   </code>\n   addresses the issue of conversational agents hallucinating information by ensuring retrieval of context with every user interaction. This feature, compatible with all ReAct and OpenAI Function agent types, prepends retrieved-context as a system message.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/chat_engine/chat_engine_context.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1685690590527430656?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   This month marked the launch of two new exciting LLMs. First off was Anthropic Claude 2.0. We launched with day 1 support of the new model.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/llm/anthropic.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1678944607965708288?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   . The other one was Llama2, and LlamaIndex now offers best-in-class integration with the Llama2 model on Replicate.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/llm/llama_2.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1681438906296991749?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   LlamaIndex is day one compatible with Chroma v0.4.0, enhancing support for in-memory, persisted, and self-hosted databases. This upgrade simplifies the use of Chroma within LlamaIndex, making database handling easier and more efficient.\n   <a href=\"https://twitter.com/llama_index/status/1681167979176665088?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   LlamaIndex\u2019s newly launched Data Agents can automatically interact with any API defined via an OpenAPI spec. It handles indexing/loading of large data from API specs and facilitates easy integration of the OpenAPI Tool, enhancing the ability to call web services.\n   <a href=\"https://llamahub.ai/l/tools-openapi\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1679522417558040577?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   LlamaIndex now utilizes the\n   <code class=\"cw pt pu pv pw b\">\n    rebel-large\n   </code>\n   model for high-speed relation extraction. Combined with CUDA, you can generate knowledge graphs from your text data.\n   <a href=\"https://twitter.com/jerryjliu0/status/1685078740555169793?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   LlamaIndex introduced a code interpreter tool. This feature equips any LLM with the ability to analyze data and generate visualizations, expanding their capabilities similar to those of ChatGPT.\n   <a href=\"https://twitter.com/jerryjliu0/status/1681304143930212357?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   LlamaIndex now integrates with Eduardo Reis\u2019s Llama 2 functions API at\n   <a href=\"http://llama-api.com\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    llama-api.com\n   </a>\n   .\n   <a href=\"https://twitter.com/llama_index/status/1683231608038641664?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   LlamaIndex TS now supports integration with OpenAI Whisper.\n   <a href=\"https://www.npmjs.com/package/llamaindex-whisper\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/yi_ding/status/1683990169815502848?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   LlamaIndex now seamlessly integrates with\n   <a href=\"https://twitter.com/kuzudb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    K\u00f9zudb\n   </a>\n   , allowing users to directly store extracted knowledge graphs/triples for advanced processing, querying, and visualization.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/KuzuGraphDemo.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/kuzudb/status/1685010132277530624?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   LlamaIndex combines data agents with text-to-image models enhancing user prompts with relevant context from a knowledge base. This integration allows for more advanced multimodal reasoning by merging LLM RAG systems with text-to-image tools.\n   <a href=\"https://llamahub.ai/l/tools-text_to_image\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1686026926442315778?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n </ol>\n <h1>\n  Benchmarking:\n </h1>\n <ol>\n  <li>\n   LlamaIndex now supports BEIR, an Information Retrieval benchmark. Users can define custom retrievers within LlamaIndex, apply the vector index, or implement reranking steps, and then easily evaluate their methods using any dataset from BEIR.\n   <a href=\"https://twitter.com/llama_index/status/1680569394198372352?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   LlamaIndex\u2019s Llama2 agents have shown promising performance in our agent task benchmark. Especially notable is their capability to appropriately use tools within a ReACT loop. However, the tasks\u2019 difficulty varies, with both 13B and 70B models notably refraining from dialing a phone number, underlining the AI\u2019s limitations.\n   <a href=\"https://twitter.com/llama_index/status/1681724356764872704?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   LlamaIndex now has integration with the HotpotQA benchmark! This enables rigorous testing of LLM\u2019s multi-hop reasoning capabilities by providing the full context to the models, helping you evaluate their performance more accurately. Perfect for stress-testing LLMs like ChatGPT, Claude 2, PaLM, and more. Plus, explore how context reordering can simplify tasks for your LLMs.\n   <a href=\"https://twitter.com/jerryjliu0/status/1684589377614413825?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   LlamaIndex now supports over 20 vector databases, each with unique features and capabilities. To help understand their differences, we have compiled a comprehensive comparison table, guiding the choice of the optimal database for the use case.\n   <a href=\"https://twitter.com/llama_index/status/1685326422175535104?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n </ol>\n <h1>\n  Tutorials:\n </h1>\n <p>\n  We were excited to see so many people making tutorials for LlamaIndex this month!\n </p>\n <ol>\n  <li>\n   <a href=\"https://medium.com/@adam.hofmann\" rel=\"noopener\">\n    Adam Hofmann\n   </a>\n   \u2019s blog post on\n   <a href=\"https://medium.com/llamaindex-blog/building-better-tools-for-llm-agents-f8c5a6714f11\" rel=\"noopener\">\n    Building Better Tools for LLM Agents\n   </a>\n   .\n  </li>\n  <li>\n   <a href=\"https://weaviate.io/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Weav\n   </a>\n   iate\u2019s\n   <a href=\"https://github.com/weaviate/recipes/blob/main/integrations/llama2-demo/notebook.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on using the Llama2 model with LlamaIndex and Weaviet on external data.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/ecardenas300\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Erika\n   </a>\n   \u2019s\n   <a href=\"https://twitter.com/ecardenas300/status/1681669892741775361?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on VectorStore Index, List Index, and Tree Index.\n  </li>\n  <li>\n   <a href=\"https://www.linkedin.com/in/james-maslek/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    James Maslek\n   </a>\n   \u2019s\n   <a href=\"https://openbb.co/blog/breaking-barriers-with-openbb-and-llamaIndex\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Breaking Barriers with OpenBB and LlamaIndex: Simplifying data access to 100+ trusted sources.\n  </li>\n  <li>\n   <a href=\"https://wandb.ai/ayush-thakur\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Ayush Thakur\n   </a>\n   \u2019s tutorial on\n   <a href=\"https://wandb.ai/ayush-thakur/llama-index-report/reports/Building-Advanced-Query-Engine-and-Evaluation-with-LlamaIndex-and-W-B--Vmlldzo0OTIzMjMy\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Building Advanced Query Engine and Evaluation with LlamaIndex and W&amp;B\n   </a>\n   .\n  </li>\n  <li>\n   <a href=\"https://www.trulens.org/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Trulens\n   </a>\n   \u2019s\n   <a href=\"https://github.com/truera/trulens/blob/main/trulens_eval/examples/frameworks/llama_index/llamaindex-yelp-agent.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on using LlamaIndex Yelp agent to answer queries using Yelp data, and evaluate it for definitiveness and accuracy using custom feedback functions, compare its performance against a standalone LLM.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/AirbyteHQ\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Airbyte\n   </a>\n   \u2019s\n   <a href=\"https://airbyte.com/tutorials/airbyte-and-llamaindex-elt-and-chat-with-your-data-warehouse-without-writing-sql\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Chat with your data warehouse without writing SQL.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/matchaman11\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Anil Chandra Naidu\n   </a>\n   \u2019s tutorial on\n   <a href=\"https://github.com/SamurAIGPT/LlamaIndex-course/blob/main/retrievers/Retrievers.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Retrievers\n   </a>\n   and\n   <a href=\"https://github.com/SamurAIGPT/LlamaIndex-course/blob/main/query_engines/Query_Engines.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    QueryEngines\n   </a>\n   .\n  </li>\n  <li>\n   <a href=\"https://medium.com/@wenqiglantz\" rel=\"noopener\">\n    Wenqi Glantz\n   </a>\n   \u2019s tutorial on\n   <a href=\"https://betterprogramming.pub/exploring-snowflake-and-streamlit-with-llamaindex-text-to-sql-f66fec6e321b\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Exploring Snowflake and Streamlit With LlamaIndex Text-to-SQL\n   </a>\n   .\n  </li>\n </ol>\n <p>\n  And from the LlamaIndex team:\n </p>\n <ol>\n  <li>\n   <a href=\"https://twitter.com/LoganMarkewich\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Logan\n   </a>\n   \u2019s\n   <a href=\"https://www.youtube.com/watch?v=2c64G-iDJKQ\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on a comprehensive understanding of embedding models, their benchmarking, and their implementation in LlamaIndex, with a focus on OpenAI and Instructor embeddings, enabling semantic search through numerical text representations.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/LoganMarkewich\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Logan\n   </a>\n   \u2019s\n   <a href=\"https://www.youtube.com/watch?v=LQy8iHOJE2A\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on the evaluation of query engines using LlamaIndex, learn to handle uncontrolled outputs and runtime costs while measuring performance with GPT-4.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/ravithejads\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Ravi Theja\n   </a>\n   \u2019s\n   <a href=\"https://www.youtube.com/watch?v=A3iqOJHBQhM\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Key Components to build QA Systems.\n  </li>\n </ol>\n <h1>\n  Webinars:\n </h1>\n <ol>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=s8ZNLqi9hzc\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Webinar\n   </a>\n   with Didier Lopes, CEO/Co-Founder at OpenBB on LLMs for Investment Research.\n  </li>\n  <li>\n   <a href=\"https://llamaindex-and.wandb.events/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Webinar\n   </a>\n   on Building &amp; Evaluating an Advanced Query Engine Over Your Data with Weights and Biases.\n  </li>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=TdVbH7uJR_Y\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Webinar\n   </a>\n   with\n   <a href=\"https://twitter.com/jxnlco\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Jason\n   </a>\n   Liu on From Prompt to Schema Engineering with Pydantic.\n  </li>\n </ol>\n <h1>\n  Events:\n </h1>\n <ol>\n  <li>\n   LlamaIndex and Arize\n   <a href=\"https://arize.com/resource/llm-search-retrieval-systems-with-arize-and-llamaindex-powering-llms-on-your-proprietary-data/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    workshop\n   </a>\n   on LLM Search &amp; Retrieval Systems with Arize and LlamaIndex: Powering LLMs on Your Proprietary Data.\n  </li>\n  <li>\n   LlamaIndex and TruLens\n   <a href=\"https://go.truera.com/event-llm-app-workshop-with-llamaindex-and-trulens?utm_campaign=event-2023-07-27-san-francisco&amp;utm_source=twitter&amp;utm_medium=social\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    workshop\n   </a>\n   on building an LLM App.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/TheProductfolks\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    TPF\n   </a>\n   (The Product Folks)\n   <a href=\"https://www.youtube.com/watch?v=2ul5XQXp-YI\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    workshop session\n   </a>\n   on Building QA Systems With LlamaIndex by\n   <a href=\"https://twitter.com/ravithejads\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Ravi Theja\n   </a>\n   .\n  </li>\n  <li>\n   <a href=\"https://twitter.com/ravithejads\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Ravi Theja\n   </a>\n   <a href=\"https://twitter.com/ravithejads/status/1684768609111801856?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    talk\n   </a>\n   at the\n   <a href=\"https://twitter.com/specialeinvest?lang=en\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Speciale VC\n   </a>\n   GenAI meetup in Chennai on Beyond the Basics: Leveraging LlamaIndex from Concept to Production.\n  </li>\n  <li>\n   Data Agents session at TPF X Nexus VC\n   <a href=\"https://twitter.com/TheProductfolks/status/1685167361060737024?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Buildathon\n   </a>\n   by\n   <a href=\"https://twitter.com/ravithejads\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Ravi Theja\n   </a>\n   .\n  </li>\n </ol>\n <h1>\n  Demos:\n </h1>\n <ol>\n  <li>\n   <a href=\"https://twitter.com/TryTaliAI\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tali.AI\n   </a>\n   at the Augment hackathon dove into the future of support roles by developing an Autonomous Support Bot using LlamaIndex.\n   <a href=\"https://twitter.com/TryTaliAI/status/1683960220702371845?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   <a href=\"https://superagi.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    SuperAGI\n   </a>\n   integrated with LlamaIndex which enables AI agents to process a wide variety of data from both structured and unstructured sources including Docx, PDF, CSV files, videos, and images.\n   <a href=\"https://twitter.com/_superAGI/status/1679058876023603201?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n </ol>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 18816, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b725d51-5e7f-41e5-9688-ac6ad095544b": {"__data__": {"id_": "1b725d51-5e7f-41e5-9688-ac6ad095544b", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-update-09-03-2023-4a7c21c0f60b.html", "file_name": "llamaindex-update-09-03-2023-4a7c21c0f60b.html", "file_type": "text/html", "file_size": 34061, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-update-09-03-2023-4a7c21c0f60b.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-update-09-03-2023-4a7c21c0f60b.html", "file_name": "llamaindex-update-09-03-2023-4a7c21c0f60b.html", "file_type": "text/html", "file_size": 34061, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "5f71ae857503defb0e4732e4b1ce50744230cad636affee822d1f58b0874afb2", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Hello LlamaIndex Community!\n </p>\n <p>\n  We\u2019re thrilled to bring you the latest edition of our LlamaIndex Update series. Whether you\u2019ve been a part of our journey from the start or have just recently joined us, your engagement and input are invaluable to us.\n </p>\n <p>\n  In this update, we\u2019re excited to unveil some significant advancements. We\u2019ve got comprehensive updates on new features for both the Python and TypeScript versions of LlamaIndex. In addition, we\u2019re offering some expert insights on RAG tips that you won\u2019t want to miss. To keep you ahead of the curve, we\u2019ve also curated a selection of webinars, tutorials, events, and demos.\n </p>\n <p>\n  So without further ado, let\u2019s delve into the latest developments.\n </p>\n <h1>\n  <strong>\n   New Features:\n  </strong>\n </h1>\n <h2>\n  LlamaIndex\n </h2>\n <ol>\n  <li>\n   LlamaIndex introduces the Sweep AI code splitter for RAG apps, addressing the challenges of traditional code splitting. This tool features recursive splitting combined with CSTs across 100+ languages, enhancing the LlamaIndex experience.\n   <a href=\"https://docs.sweep.dev/blogs/chunking-2m-files\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    BlogPost\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1686413452988878849?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex now supports streaming data ETL, enhancing structured data extraction with the OpenAI Function API. By inputting a Pydantic object class in LlamaIndex, users can receive streamed data objects from OpenAI individually.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/output_parsing/openai_pydantic_program.html#extraction-into-album-streaming\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1686752090197008387?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex has teamed up with Neo4j to amplify knowledge graph capabilities with LLM\u2019s. This integration not only allows for storing any knowledge graph created in LlamaIndex directly in Neo4j but also introduces a specialized text-to-cypher prompt for Neo4j users.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/Neo4jKGIndexDemo.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1687224679340064768?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex, in collaboration with Mendable AI and Nomic AI, unveils a Nomic Atlas visual map detailing user questions from the Mendable AI bot. This innovative tool groups similar questions, providing insights for improved app deployment, prompt control, language support, and documentation. New users can find the helpful Mendable AI bot on LlamaIndex\u2019s documentation site.\n   <a href=\"https://twitter.com/llama_index/status/1687485785228906496?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex, in collaboration with Predibase, offers an optimal way to operationalize LLMs. Experience top-tier RAG by privately hosting open-source LLMs on managed infrastructure right within your VPC.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/llm/predibase.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/predibase/status/1687493843619598336?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   The LlamaIndex playground\n   <a href=\"https://llama-playground.vercel.app/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    app\n   </a>\n   enhances the RAG experience. Updates include new Temperature and Top P options, along with intuitive tooltips offering plain language explanations.\n  </li>\n  <li>\n   LlamaIndex Tip\ud83d\udca1: Boost your RAG systems by adding structured data to raw text. This allows for easier metadata filtering and optimal embedding biases. Dive into our guide on harnessing the HuggingFace span marker for targeted entity extraction.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/metadata_extraction/EntityExtractionClimate.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1688711638537682944?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex now has the Semantic Scholar Loader. With it, users can swiftly set up citation-based Q&amp;A systems.\n   <a href=\"https://llamahub.ai/l/semanticscholar\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/shauryr/status/1687858252481236993?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex highlights the significance of text chunk size in LLM QA systems. To determine the best chunk size without human intervention, we suggest ensembling different sizes and using a reranker for context relevance during queries. This method involves simultaneous queries across retrievers of various sizes and consolidating results for reranking. Though experimental, this approach aims to discern the optimal chunk size strategy.\n   <a href=\"https://gpt-index.readthedocs.io/en/stable/examples/retrievers/ensemble_retrieval.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1688948298781249536?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex\u2019s customer support bot seamlessly interfaces with Shopify\u2019s 50k-line GraphQL API Spec. Through smart tools and LlamaIndex features, it offers quick insights like\n   <code class=\"cw qm qn qo qp b\">\n    refunded orders\n   </code>\n   despite the vast spec size. Efficient indexing ensures precise user query responses.\n   <a href=\"https://llamahub.ai/l/tools-shopify\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1689295239822069760?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex\u2019s integration with Xinference enables users to effortlessly expand models like llama 2, chatglm, and vicuna to incorporate RAG and agents.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/llm/XinferenceLocalDeployment.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1689426281015005184?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex introduces\n   <code class=\"cw qm qn qo qp b\">\n    One-click Observability\n   </code>\n   . With just a single code line, integrate LlamaIndex with advanced observability tools from partners like Weights &amp; Biases, ArizeAI, and TruEra, simplifying LLM app debugging for production.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/end_to_end_tutorials/one_click_observability.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1689659395465191424?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex has updated the LLM default temperature value to 0.1.\n   <a href=\"https://twitter.com/yi_ding/status/1689692197871042561?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex integration with Zep, enhancing the memory layer of LLM apps. It\u2019s not just about storage but also enriching data with summaries, metadata, and more.\n   <a href=\"https://medium.com/llamaindex-blog/zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc\" rel=\"noopener\">\n    BlogPost\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1690018390059225088?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex has revamped its defaults! Now, gpt-3.5-turbo is the go-to LLM, with enhanced prompts and a superior text splitter. Additionally, if OpenAI\u2019s key isn\u2019t set, it has backup options with llama.cpp. New embedding features have also been added.\n   <a href=\"https://twitter.com/llama_index/status/1690081661453803520?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex now seamlessly integrates with FastChat by\n   <a href=\"https://twitter.com/lmsysorg\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    lmsysorg\n   </a>\n   . Elevate your LLM deployments like Vicuna and Llama 2, serving as an alternative to OpenAI.\n   <a href=\"https://twitter.com/jerryjliu0/status/1691114369705533440?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex provides a seamless integration with Azure AI Services. Dive into a richer ecosystem of AI tools from Computer Vision, Translation, and speech enhancing your multi-modal AI interactions.\n   <a href=\"https://llamahub.ai/l/tools-azure_translate\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs1\n   </a>\n   ,\n   <a href=\"https://llamahub.ai/l/tools-azure_speech\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs2\n   </a>\n   ,\n   <a href=\"https://llamahub.ai/l/tools-azure_cv\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs3\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1691605500079800674?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex unveils\n   <code class=\"cw qm qn qo qp b\">\n    Graph RAG\n   </code>\n   \u2014 an approach to enhance LLMs with context from graph databases. Extract valuable subgraphs from any knowledge graph for superior question-answering capabilities.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/query_engine/knowledge_graph_rag_query_engine.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1691835187519459338?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex has expanded native async support, enhancing the scalability of full-stack LLM apps. We now offer async agents, tool execution, and callback support, and have introduced async methods in vector stores.\n   <a href=\"https://twitter.com/llama_index/status/1691965149840908642?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex enhances debugging with data agent trace observability. Additionally, system prompts can now be added to any query engine and we have begun the transition of LLM and embedding modules to Pydantic.\n   <a href=\"https://twitter.com/llama_index/status/1692696993900974399?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/callbacks/LlamaDebugHandler.html#see-traces-events-for-agents\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex\u2019s\n   <code class=\"cw qm qn qo qp b\">\n    Recursive Document Agents\n   </code>\n   enhance RAG by retrieving based on summaries and adjusting chunk retrieval per need. This boosts querying across varied documents, offering both question-answering and summarization within a document.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/query_engine/recursive_retriever_agents.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1693421308674289822?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex integrates with Metaphor to supercharge data agents. This integration offers a specialized search engine tailored for LLMs, allowing dynamic data lookup beyond just RAG, and answering a broader range of questions.\n   <a href=\"https://medium.com/llamaindex-blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f\" rel=\"noopener\">\n    BlogPost\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1693649115983618278?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex now supports integration with OpenAI\u2019s fine-tuned models via their new endpoint. Seamlessly integrate these models into your RAG pipeline.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/model_modules/llms/usage_custom.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1694116968008401201?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex introduces the\n   <code class=\"cw qm qn qo qp b\">\n    OpenAIFineTuningHandler\n   </code>\n   to streamline data collection for fine-tuning gpt-3.5-turbo with GPT-4 outputs. Run RAG with GPT-4 and effortlessly generate a dataset to train a more cost-effective model.\n   <a href=\"https://github.com/jerryjliu/llama_index/blob/main/experimental/openai_fine_tuning/openai_fine_tuning.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1694395355725746397?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex presents the\n   <code class=\"cw qm qn qo qp b\">\n    Principled Development Practices\n   </code>\n   guide, detailing best practices for LLM app development Observability, Evaluation, and Monitoring.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/end_to_end_tutorials/principled_dev_practices.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1694736328276271248?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex introduces a refined Prompt system. With just three core classes:\n   <code class=\"cw qm qn qo qp b\">\n    PromptTemplate\n   </code>\n   ,\n   <code class=\"cw qm qn qo qp b\">\n    ChatPromptTemplate\n   </code>\n   , and\n   <code class=\"cw qm qn qo qp b\">\n    SelectorPromptTemplate\n   </code>\n   , users can effortlessly format as chat messages or text and tailor prompts based on model conditions.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/model_modules/prompts.html#\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1695093392378880324?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex delves into\n   <code class=\"cw qm qn qo qp b\">\n    chunk dreaming\n   </code>\n   a concept inspired by\n   <a href=\"https://twitter.com/tomchapin\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Thomas H. Chapin IV\n   </a>\n   . By auto-extracting metadata from a text chunk, it can identify potential questions and provide summaries over neighboring nodes. This enriched context boosts RAG\u2019s performance.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/metadata_extraction/MetadataExtraction_LLMSurvey.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1695233836983144764?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex is integrated with BagelDB, enabling developers to effortlessly tap into vector data stored on BagelDB.\n   <a href=\"https://twitter.com/BagelDB_ai/status/1695158701387059319?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex now lets the LLM choose between vector search for semantic queries or our BM25 retriever for keyword-specific ones.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/retrievers/bm25_retriever.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1695590257054630149?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex introduces the\n   <code class=\"cw qm qn qo qp b\">\n    AutoMergingRetriever\n   </code>\n   , crafted with insights from\n   <a href=\"https://twitter.com/jxnlco\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Jason\n   </a>\n   and ChatGPT. This technique fetches precise context chunks and seamlessly merges them, optimizing LLM responses. Using the HierarchicalNodeParser, we ensure interconnected chunks for enhanced context clarity.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/retrievers/auto_merging_retriever.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1695832757560356871?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex introduces embedding finetuning for optimized retrieval performance. Beyond enhancing RAG, we\u2019ve simplified retrieval evaluations with automatic QA dataset generation from text, streamlining both finetuning and evaluation processes.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1696583119539966070?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex now integrates directly with Airbyte sources including Gong, Hubspot, Salesforce, Shopify, Stripe, Typeform, and Zendesk Support. Easily enhance your LlamaIndex application with these platforms implemented as data loaders.\n   <a href=\"https://airbyte.com/blog/introducing-airbyte-sources-within-llamaindex\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    BlogPost\n   </a>\n   ,\n   <a href=\"https://twitter.com/AirbyteHQ/status/1696633858316243046?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex integrates with DeepEval, a comprehensive library to evaluate LLM and RAG apps. Assess on four key metrics: Relevance, Factual Consistency, Answer Similarity, and Bias/Toxicity.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/community/integrations/deepeval.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1696674470566764846?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex recommends evaluating LLM + RAG step-by-step, especially retrieval. Create synthetic retrieval datasets from text chunks using LLMs. This method not only evaluates retrieval but also fine-tunes embeddings.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding.html#generate-corpus\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1696675525442609166?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex unveils a managed index abstraction simplifying RAG\u2019s ingestion and storage processes with Vectara.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/community/integrations/managed_indices.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1696919525151899671?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex has significantly enhanced its callback handling support, encompassing features like tracebacks, LLM token counts, templates, and detailed agent tool information. These advancements pave the way for smoother integrations with evaluation and observability applications.\n   <a href=\"https://twitter.com/llama_index/status/1697407787154997754?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex has integrated with AskMarvinAI, enabling automated metadata extraction from text corpora. Just annotate a Pydantic model and effortlessly log metadata from all associated text chunks.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/metadata_extraction/MarvinMetadataExtractorDemo.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1697632035186372745?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LlamaIndex is integrated with RunGPT by JinaAI, an outstanding framework for one-click deployment of various open-source models such as Llama, Vicuna, Pythia, and more. Coupled with LlamaIndex\u2019s innate chat/streaming capabilities, users can now deploy and utilize powerhouse models like Llama-7B seamlessly.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/llm/rungpt.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1698001332563837165?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <h2>\n  LlamaIndex.TS\n </h2>\n <ol>\n  <li>\n   LITS has Full Azure OpenAI integration.\n   <a href=\"https://twitter.com/yi_ding/status/1688564790007087104\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LITS Enhanced Llama2 support, new default temperature (0.1), and GPT chat integration.\n   <a href=\"https://twitter.com/llama_index/status/1689086106036547584?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LITS helps to use\n   <code class=\"cw qm qn qo qp b\">\n    fromDocuments\n   </code>\n   without repeat checks; auto SHA256 comparison.\n   <a href=\"https://twitter.com/llama_index/status/1691502243286228993?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LITS now supports OpenAI v4, Anthropic 0.6, &amp; Replicate 0.16.1., CSV loader, Merged NodeWithEmbeddings &amp; BaseNode.\n   <a href=\"https://twitter.com/llama_index/status/1691984600506257462?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LITS now supports PapaCSVLoader for math.\n   <a href=\"https://twitter.com/yi_ding/status/1691991221974217104?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LITS is now integrated with LiteLLM.\n   <a href=\"https://twitter.com/yi_ding/status/1692408213340340328\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LITS now has additional session options for proxy server support, Default timeout reset to 60 seconds for OpenAI.\n   <a href=\"https://twitter.com/llama_index/status/1693072438404276460?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LITS now has Pinecone integration.\n   <a href=\"https://twitter.com/yi_ding/status/1693275444848840745?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LITS has Optimized ChatGPT prompts, fixed metadata rehydration issues, and OpenAI Node v4.1.0 with fine-tuned model support.\n   <a href=\"https://twitter.com/llama_index/status/1694382741218005153?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LITS has introduced enhanced text-splitting features, including a specialized tokenizer for Chinese, Japanese, and Korean, and refinements to the SentenceSplitter for handling decimal numbers.\n   <a href=\"https://twitter.com/llama_index/status/1694719208217588070?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LITS has a Markdown loader and metadata support in the response synthesizer.\n   <a href=\"https://twitter.com/llama_index/status/1695156772783395255?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LITS revamped usability:\n   <code class=\"cw qm qn qo qp b\">\n    ListIndex\n   </code>\n   is now\n   <code class=\"cw qm qn qo qp b\">\n    SummaryIndex\n   </code>\n   for clarity, and prompts have been made typed and customizable to enhance user control and experience.\n   <a href=\"https://twitter.com/llama_index/status/1696626780277481491?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   LITS has Notion Reader. Now, users can effortlessly import their documents directly into their RAG or Data Agent application in LITS.\n   <a href=\"https://twitter.com/llama_index/status/1698053712521146389?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <h1>\n  RAG Tips:\n </h1>\n <p>\n  LlamaIndex shares\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/end_to_end_tutorials/dev_practices/production_rag.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   four tactics\n  </a>\n  to boost your RAG pipeline:\n </p>\n <p>\n  1\ufe0f\u20e3 Use summaries for retrieval, and a broader context for synthesis.\n </p>\n <p>\n  2\ufe0f\u20e3 Use metadata for structured retrieval over large docs.\n </p>\n <p>\n  3\ufe0f\u20e3 Deploy LLMs for dynamic retrieval based on tasks.\n </p>\n <p>\n  4\ufe0f\u20e3 Fine-tune embeddings for better retrieval.\n </p>\n <h1>\n  Tutorials:\n </h1>\n <ol>\n  <li>\n   <a href=\"https://twitter.com/jasonzhou1993\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Jason's\n   </a>\n   <a href=\"https://www.youtube.com/watch?v=qKtM2AlDTs8&amp;t=475s\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on adding Image Responses to GPT knowledge retrieval apps.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/wenqi_glantz\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Wenqi Glantz\n   </a>\n   <a href=\"https://betterprogramming.pub/building-production-ready-llm-apps-with-llamaindex-document-metadata-for-higher-accuracy-retrieval-a8ceca641fb5\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Building Production-Ready LLM Apps with LlamaIndex: Document Metadata for Higher Accuracy Retrieval\n  </li>\n  <li>\n   Streamlit\n   <a href=\"https://blog.streamlit.io/build-a-chatbot-with-custom-data-sources-powered-by-llamaindex/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Building a chatbot with custom data sources, powered by LlamaIndex.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/wenqi_glantz\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Wenqi Glantz\n   </a>\n   <a href=\"https://betterprogramming.pub/building-production-ready-llm-apps-with-llamaindex-recursive-document-agents-for-dynamic-retrieval-1f4b25287918\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Building Production-Ready LLM Apps With LlamaIndex: Recursive Document Agents for Dynamic Retrieval.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/ecardenas300\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Erika Cardenas\n   </a>\n   covers the usage of\n   <a href=\"https://twitter.com/ecardenas300/status/1695816617207153016?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    LlamaIndex in building an RAG app\n   </a>\n   .\n  </li>\n  <li>\n   <a href=\"https://argilla.io/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Argilla\n   </a>\n   blog post on\n   <a href=\"https://docs.argilla.io/en/latest/guides/llms/examples/fine-tuning-openai-rag-feedback.html#Evaluating-base-vs-fine-tuned-with-human-preference-data\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Fine-tuning and evaluating GPT-3.5 with human feedback for RAG using LlamaIndex\n   </a>\n   .\n  </li>\n  <li>\n   <a href=\"https://www.kdnuggets.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    KDNuggests\n   </a>\n   blog post on\n   <a href=\"https://www.kdnuggets.com/build-your-own-pandasai-with-llamaindex\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Build Your Own PandasAI with LlamaIndex\n   </a>\n   .\n  </li>\n </ol>\n <p>\n  From the LlamaIndex team:\n </p>\n <ol>\n  <li>\n   <a href=\"https://twitter.com/jerryjliu0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Jerry Liu\n   </a>\n   \u2019s\n   <a href=\"https://medium.com/llamaindex-blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d\" rel=\"noopener\">\n    tutorial\n   </a>\n   on fine-tuning Llama 2 for Text-to-SQL Applications.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/jerryjliu0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Jerry Liu's\n   </a>\n   <a href=\"https://medium.com/llamaindex-blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971\" rel=\"noopener\">\n    tutorial\n   </a>\n   on Fine-Tuning Embeddings for RAG with Synthetic Data.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/ravithejads\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Ravi Theja\u2019s\n   </a>\n   <a href=\"https://medium.com/llamaindex-blog/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b\" rel=\"noopener\">\n    tutorial\n   </a>\n   on combining Text2SQL and RAG with LlamaIndex to analyze product reviews.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/ravithejads\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Ravi Theja\u2019s\n   </a>\n   tutorial on different\n   <a href=\"https://www.youtube.com/watch?v=gQXXeLHTxkI\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Indicies, Storage Context, and Service Context of LlamaIndex.\n   </a>\n  </li>\n  <li>\n   <a href=\"https://twitter.com/ravithejads\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Ravi Theja\u2019s\n   </a>\n   tutorial on\n   <a href=\"https://www.youtube.com/watch?v=hsEWohYtg0I\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Custom Retrievers and Hybrid Search in LlamaIndex.\n   </a>\n  </li>\n  <li>\n   <a href=\"https://twitter.com/ajhofmann18\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Adam's\n   </a>\n   tutorial on\n   <a href=\"https://www.youtube.com/watch?v=GkIEEdIErm8\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Introduction to Data Agents for Developers\n   </a>\n   .\n  </li>\n  <li>\n   <a href=\"https://twitter.com/ravithejads\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Ravi Theja\u2019s\n   </a>\n   tutorial on creating\n   <a href=\"https://medium.com/llamaindex-blog/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af\" rel=\"noopener\">\n    Automatic Knowledge Transfer (KT) Generation for Code Bases using LlamaIndex.\n   </a>\n  </li>\n </ol>\n <h1>\n  Webinars:\n </h1>\n <ol>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=njzB6fm0U8g\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Webinar\n   </a>\n   with members from Docugami on Document Metadata and Local Models for Better, Faster Retrieval.\n  </li>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=CwdAi1tts9c\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Webinar\n   </a>\n   with Shaun and Piaoyang on building Personalized AI Characters with RealChar.\n  </li>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=Zj5RCweUHIk\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Webinar\n   </a>\n   with Bob (Weaviet), Max (sid.ai), and Tuana (HayStack) on making RAG Production-Ready.\n  </li>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=hb8uT-VBEwQ\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Workshop\n   </a>\n   by Wey Gu on Building RAG with Knowledge Graphs.\n  </li>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=mndiDJ5k26A\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Webinar\n   </a>\n   with Jo Bergum and Shishir Patil on fine-tuning and RAG.\n  </li>\n </ol>\n <h1>\n  Events:\n </h1>\n <ol>\n  <li>\n   <a href=\"https://twitter.com/jerryjliu0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Jerry Liu\n   </a>\n   spoke about LlamaIndex at the\n   <a href=\"https://www.youtube.com/watch?v=QtYL4Cm-pjE\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    NYSE Floor Talk\n   </a>\n   .\n  </li>\n  <li>\n   <a href=\"https://twitter.com/ravithejads\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Ravi Theja\n   </a>\n   spoke about LlamaIndex at the\n   <a href=\"https://twitter.com/ravithejads/status/1689491855543599104?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Fifth Elephant conference\n   </a>\n   in Bengaluru, India.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/ravithejads\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Ravi Theja\n   </a>\n   conducted a\n   <a href=\"https://twitter.com/fifthel/status/1692785973283656052?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    workshop\n   </a>\n   on LlamaIndex in Bengaluru, India.\n  </li>\n </ol>\n <h1>\n  Demos And Papers:\n </h1>\n <ol>\n  <li>\n   The paper titled\n   <a href=\"https://www.nature.com/articles/s41598-023-41512-8\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Performance of ChatGPT, human radiologists, and context-aware ChatGPT in identifying AO codes from radiology reports\n   </a>\n   is an intriguing medical research. It leverages both LlamaIndex and ChatGPT to pinpoint AO codes within radiology reports, enhancing fracture classification. A fantastic fusion of tech and medicine!\n  </li>\n  <li>\n   <a href=\"https://www.secinsights.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    SEC Insights AI\n   </a>\n   does SEC document analysis using LlamaIndex is on Product Hunt as the 5th product of the day.\n  </li>\n  <li>\n   <a href=\"https://lablab.ai/event/autonomous-agents-hackathon/kbve/atlas\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    RentEarth\n   </a>\n   : an agent to build your own startup with an amazing 3D interface and LlamaIndex.\n  </li>\n </ol>\n <p>\n  In wrapping up this edition of our LlamaIndex Update series, we\u2019re reminded of the power of collaboration and innovation. From new features to integrations and tutorials, our mission to revolutionize the AI realm marches forward. To every member of our community, thank you for your unwavering support and enthusiasm. Let\u2019s continue to elevate the world of AI together!\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 33981, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3cae4701-e505-4058-9d4c-7ecef3082ea5": {"__data__": {"id_": "3cae4701-e505-4058-9d4c-7ecef3082ea5", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-update-20-09-2023-86ed66f78bac.html", "file_name": "llamaindex-update-20-09-2023-86ed66f78bac.html", "file_type": "text/html", "file_size": 15447, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-update-20-09-2023-86ed66f78bac.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-update-20-09-2023-86ed66f78bac.html", "file_name": "llamaindex-update-20-09-2023-86ed66f78bac.html", "file_type": "text/html", "file_size": 15447, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "ed5e7a8b928efd35be6c36fa5949eb2af59bebf2e5adecedd9566bca320323a1", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  <strong>\n   Hello LlamaIndex Enthusiasts!\n  </strong>\n </p>\n <p>\n  Welcome to the fifth edition of our LlamaIndex Update series.\n </p>\n <h2>\n  <strong>\n   Most Important Takeaways:\n  </strong>\n </h2>\n <ol>\n  <li>\n   We\u2019ve open-sourced\n   <a href=\"http://secinsights.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     SECInsights.ai\n    </strong>\n   </a>\n   \u2014 your gateway to the production RAG framework.\n  </li>\n  <li>\n   Replit templates \u2014 kickstart your projects with zero environment setup hassles.\n  </li>\n  <li>\n   Build RAG from scratch and get hands-on with our processes.\n  </li>\n </ol>\n <p>\n  But wait, there\u2019s more!\n </p>\n <ul>\n  <li>\n   Feature Releases and Enhancements\n  </li>\n  <li>\n   Fine-Tuning Guides\n  </li>\n  <li>\n   Retrieval Tips for RAG\n  </li>\n  <li>\n   Building RAG from Scratch Guides\n  </li>\n  <li>\n   Tutorials\n  </li>\n  <li>\n   Integration with External Platforms\n  </li>\n  <li>\n   Events\n  </li>\n  <li>\n   Webinars\n  </li>\n </ul>\n <p>\n  So, let\u2019s embark on this journey together. Dive in and explore the offerings of the fifth edition of the LlamaIndex Update series!\n </p>\n <h1>\n  <strong>\n   Feature Releases and Enhancements\n  </strong>\n </h1>\n <ol>\n  <li>\n   <strong>\n    Open-Sourced RAG Platform\n   </strong>\n   : LlamaIndex open-sourced\n   <a href=\"http://secinsights.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     http://secinsights.ai\n    </strong>\n   </a>\n   , accelerating RAG app development with chat-based Q&amp;A features.\n   <a href=\"https://twitter.com/llama_index/status/1699116440056651976?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   <strong>\n    Linear Adapter Fine-Tuning\n   </strong>\n   : LlamaIndex enables efficient fine-tuning of linear adapters on any embedding without re-embedding, enhancing retrieval/RAG across various models.\n   <a href=\"https://twitter.com/llama_index/status/1699566421506671043?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   ,\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding_adapter.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://medium.com/llamaindex-blog/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383\" rel=\"noopener\">\n    BlogPost\n   </a>\n  </li>\n  <li>\n   <strong>\n    Hierarchical Agents\n   </strong>\n   : By structuring LLM agents in a parent-child hierarchy, we enhance complex search and retrieval tasks across diverse data, offering more reliability than a standalone agent.\n   <a href=\"https://twitter.com/llama_index/status/1699929022027718729?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   <strong>\n    SummaryIndex\n   </strong>\n   : We\u2019ve renamed ListIndex to SummaryIndex to make it clearer what its main functionality is. Backward compatibility is maintained for existing code using ListIndex.\n   <a href=\"https://twitter.com/llama_index/status/1698728395948081247?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   <strong>\n    Evaluation:\n   </strong>\n   LlamaIndex\u2019s new RAG evaluation toolkit offers async capabilities, diverse assessment criteria, and a centralized BaseEvaluator for easier developer integrations.\n   <a href=\"https://x.com/llama_index/status/1703074307763818775?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   ,\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/supporting_modules/evaluation/modules.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Hybrid Search for Postgres/pgvector\n   </strong>\n   : LlamaIndex introduces a hybrid search for Postgres/pgvector.\n   <a href=\"https://twitter.com/llama_index/status/1700892425592696915?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   ,\n   <a href=\"https://gpt-index.readthedocs.io/en/stable/examples/vector_stores/postgres.html#hybrid-search\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Replit Templates:\n   </strong>\n   LlamaIndex partners with Replit for easy LLM app templates, including ready-to-use Streamlit apps and full Typescript templates.\n   <a href=\"https://x.com/llama_index/status/1702847763183235278?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   ,\n   <a href=\"https://replit.com/@LlamaIndex\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Replit Templates\n   </a>\n   .\n  </li>\n </ol>\n <h2>\n  <strong>\n   LlamaIndex.TS:\n  </strong>\n </h2>\n <ol>\n  <li>\n   Launches with MongoDBReader and type-safe metadata.\n   <a href=\"https://x.com/llama_index/status/1702382520631959721?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   Launches with chat history, enhanced keyword index, and Notion DB support.\n   <a href=\"https://twitter.com/llama_index/status/1701292211764338898?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <h1>\n  Fine-Tuning Guides:\n </h1>\n <ol>\n  <li>\n   <strong>\n    OpenAI Fine-Tuning:\n   </strong>\n   LlamaIndex unveils a fresh guide on harnessing OpenAI fine-tuning to embed knowledge from any text corpus. In short: generate QA pairs with GPT-4, format them into a training dataset, and proceed to fine-tuning.\n   <a href=\"https://twitter.com/llama_index/status/1701264116311322937?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   ,\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/finetuning/knowledge/finetune_knowledge.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Embedding Fine-Tuning:\n   </strong>\n   LlamaIndex has a more advanced embedding fine-tuning feature, enabling complex NN query transformations on any embedding, including custom ones, and offering the ability to save intermediate checkpoints for enhanced model control.\n   <a href=\"https://twitter.com/llama_index/status/1701983207946965285?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   ,\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding_adapter.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   .\n  </li>\n </ol>\n <h1>\n  Retrieval Tips For RAG:\n </h1>\n <ul>\n  <li>\n   Use references (smaller chunks or summaries) instead of embedding full text.\n  </li>\n  <li>\n   Results in 10\u201320 % improvement.\n  </li>\n  <li>\n   Embeddings decoupled from main text chunks.\n  </li>\n  <li>\n   Smaller references allow efficient LLM synthesis.\n  </li>\n  <li>\n   Deduplication applied for repetitive references.\n  </li>\n  <li>\n   Evaluated using synthetic dataset; 20\u201325% MRR boost.\n  </li>\n </ul>\n <p>\n  <a href=\"https://twitter.com/jerryjliu0/status/1698727872721285282?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Tweet\n  </a>\n </p>\n <h1>\n  Building RAG from Scratch Guides:\n </h1>\n <ol>\n  <li>\n   Build Data Ingestion from scratch.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/low_level/ingestion.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   .\n  </li>\n  <li>\n   Build Retrieval from scratch.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/low_level/retrieval.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   .\n  </li>\n  <li>\n   Build Vector Store from scratch.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/low_level/vector_store.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   .\n  </li>\n  <li>\n   Build Response Synthesis from scratch.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/low_level/response_synthesis.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   .\n  </li>\n  <li>\n   Build Router from scratch.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/low_level/router.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   .\n  </li>\n  <li>\n   Build Evaluation from scratch.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/low_level/evaluation.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   .\n  </li>\n </ol>\n <h1>\n  Tutorials:\n </h1>\n <ol>\n  <li>\n   <a href=\"https://twitter.com/wenqi_glantz\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Wenqi Glantz\n   </a>\n   <a href=\"https://betterprogramming.pub/fine-tuning-gpt-3-5-rag-pipeline-with-gpt-4-training-data-49ac0c099919\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Fine-Tuning GPT-3.5 RAG Pipeline with GPT-4 Training Data with LlamaIndex fine-tuning abstractions.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/wenqi_glantz\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Wenqi Glantz\n   </a>\n   <a href=\"https://betterprogramming.pub/fine-tuning-gpt-3-5-rag-pipeline-with-gpt-4-training-data-49ac0c099919\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Fine-Tuning Your Embedding Model to Maximize Relevance Retrieval in RAG Pipeline with LlamaIndex.\n  </li>\n </ol>\n <p>\n  Tutorials from the LlamaIndex Team.\n </p>\n <ul>\n  <li>\n   <a href=\"https://twitter.com/thesourabhd\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Sourabh\n   </a>\n   <a href=\"https://www.youtube.com/watch?v=2O52Tfj79T4\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on SEC Insights, End-to-End Guide on\n   <a href=\"https://t.co/VY9we1zhip\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    secinsights.ai\n   </a>\n  </li>\n  <li>\n   <a href=\"https://twitter.com/ajhofmann18\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Adam\u2019s\n   </a>\n   <a href=\"https://www.youtube.com/watch?v=lcuL6Gqw_-g\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Custom Tools for Data Agents.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/LoganMarkewich\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Logan\n   </a>\n   <a href=\"https://www.youtube.com/watch?v=mIyZ_9gqakE\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on retrieval/reranking, covering Node Parsing, AutoMergingRetriever, HierarchicalNodeParser, node post-processors, and the setup of a RouterQueryEngine.\n  </li>\n </ul>\n <h1>\n  <strong>\n   Integrations with External Platforms\n  </strong>\n </h1>\n <ol>\n  <li>\n   <strong>\n    Integration with PortkeyAI\n   </strong>\n   : LlamaIndex integrates with PortkeyAI, boosting LLM providers like OpenAI with features like auto fallbacks and load balancing.\n   <a href=\"https://x.com/llama_index/status/1699087716183638256?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet,\n   </a>\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/llm/portkey.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Documentation\n   </a>\n  </li>\n  <li>\n   <strong>\n    Collaboration with Anyscale\n   </strong>\n   : LlamaIndex collaborates with anyscalecompute, enabling easy tuning of open-source LLMs using Ray Serve/Train.\n   <a href=\"https://twitter.com/llama_index/status/1699444987627466986?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet,\n   </a>\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/llm/anyscale.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Documentation\n   </a>\n  </li>\n  <li>\n   <strong>\n    Integration with Elastic\n   </strong>\n   : LlamaIndex integrates with Elastic, enhancing capabilities such as vector search, text search, hybrid search models, enhanced metadata handling, and es_filters.\n   <a href=\"https://twitter.com/llama_index/status/1700195709041954929?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet,\n   </a>\n   <a href=\"https://gpt-index.readthedocs.io/en/stable/examples/vector_stores/ElasticsearchIndexDemo.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Documentation\n   </a>\n  </li>\n  <li>\n   <strong>\n    Integration with MultiOn\n   </strong>\n   : LlamaIndex integrates with MultiOn, enabling data agents to navigate the web and handle tasks via an LLM-designed browser.\n   <a href=\"https://twitter.com/llama_index/status/1700221470427754610?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet,\n   </a>\n   <a href=\"https://llamahub.ai/l/tools-multion\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Documentation\n   </a>\n  </li>\n  <li>\n   <strong>\n    Integration with Vectara\n   </strong>\n   : LlamaIndex collaborates with Vectara to streamline RAG processes from loaders to databases.\n   <a href=\"https://twitter.com/llama_index/status/1701673229675552876?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet,\n   </a>\n   <a href=\"https://medium.com/llamaindex-blog/llamaindex-vectara-7a3889cd34cb\" rel=\"noopener\">\n    Blog Post\n   </a>\n  </li>\n  <li>\n   <strong>\n    Integration with LiteLLM\n   </strong>\n   : LlamaIndex integrates with LiteLLM, offering access to over 100 LLM APIs and features like chat, streaming, and async operations.\n   <a href=\"https://x.com/llama_index/status/1703188185323561432?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet,\n   </a>\n   <a href=\"https://gpt-index.readthedocs.io/en/stable/examples/llm/litellm.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Documentation\n   </a>\n  </li>\n  <li>\n   <strong>\n    Integration with MonsterAPI\n   </strong>\n   : LlamaIndex integrates with MonsterAPI, allowing users to query data using LLMs like Llama 2 and Falcon.\n   <a href=\"https://x.com/monsterapis/status/1702252516061372595?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet,\n   </a>\n   <a href=\"https://blog.monsterapi.ai/llama-index-monsterapi-integration-llm-rag/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blog Post\n   </a>\n  </li>\n </ol>\n <h1>\n  <strong>\n   Events:\n  </strong>\n </h1>\n <ol>\n  <li>\n   <a href=\"https://twitter.com/jerryjliu0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Jerry Liu\n   </a>\n   spoke on\n   <a href=\"https://docs.google.com/presentation/d/1uzhz1aFWbyXSrWBzQ1FPQWtVjMgJqAYGoGoVzEnNmAg/edit#slide=id.p\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Production Ready LLM Applications\n   </a>\n   at the Arize AI event.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/ravithejads\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Ravi Theja\n   </a>\n   conducted a\n   <a href=\"https://x.com/ravithejads/status/1699644440002826350?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    workshop\n   </a>\n   at LlamaIndex + Replit Pune Generative AI meetup.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/jerryjliu0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Jerry Liu\n   </a>\n   <a href=\"https://www.youtube.com/watch?v=wlKe9U8hmi0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    session\n   </a>\n   on Building a Lending Criteria Chatbot in Production with Stelios from MQube.\n  </li>\n </ol>\n <h1>\n  <strong>\n   Webinars\n  </strong>\n  :\n </h1>\n <ol>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=l-SGgWRe58A\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Webinar\n   </a>\n   on How to Win an LLM Hackathon by Alex Reibman, Rahul Parundekar, Caroline Frasca, and Yi Ding.\n  </li>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=eGC7m8_SgDk\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Webinar\n   </a>\n   on LLM Challenges in Production with Mayo Oshin, AI Jason, and Dylan.\n  </li>\n </ol>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 15426, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b857c6d8-1c28-4c5e-ad15-aeb0017a4539": {"__data__": {"id_": "b857c6d8-1c28-4c5e-ad15-aeb0017a4539", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-update-2023-10-10-3718a3d19fb9.html", "file_name": "llamaindex-update-2023-10-10-3718a3d19fb9.html", "file_type": "text/html", "file_size": 19541, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-update-2023-10-10-3718a3d19fb9.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-update-2023-10-10-3718a3d19fb9.html", "file_name": "llamaindex-update-2023-10-10-3718a3d19fb9.html", "file_type": "text/html", "file_size": 19541, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "fbe8f20275396f075954f359d5ede9ba4a1af3995a814fad8baf4818b5b0507b", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Here\u2019s our weekly look at developments across the LLM space and RAG (Retrieval Augmented Generation) in particular, as well as the latest news and features from your favorite open source library. If you\u2019ve got a project (or a blog post, or a video) that you think people should hear about, we\u2019re happy to feature it in here! Drop us a line at\n  <a href=\"mailto:news@llamaindex.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   news@llamaindex.ai\n  </a>\n  .\n </p>\n <p>\n  This update is now available in handy email form! Just head to our\n  <a href=\"https://llamaindex.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   home page\n  </a>\n  and enter your email to sign up.\n </p>\n <p>\n  \ud83e\udd29\n  <strong>\n   First, the highlights:\n  </strong>\n </p>\n <ol>\n  <li>\n   <strong>\n    Full observability with Arize AI Phoenix\n   </strong>\n   : we launched a one-code-line integration with Arize AI for comprehensive tracing and observability in all RAG/agent pipelines. Enjoy local data storage, track LLM input/output prompts, monitor token usage, timing, retrieval visualizations, and agent loops. Additionally, export traces for evaluations and data analysis. All while ensuring your data stays local.\n   <a href=\"https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_tracing_tutorial.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1708866998149816540?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    RetrieverEvaluator\n   </strong>\n   : new in the library, \u201cRetrieverEvaluator\u201d allows enhanced retrieval evaluations, complementing LLM generation tests. The module supports benchmarking, standard ranking metrics, and synthetic dataset creation for comprehensive retrieval assessments.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/evaluation/retrieval/retriever_eval.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1704884477552660612?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    HuggingFace Embeddings\n   </strong>\n   : we added native support for three more Hugging Face embedding models, including the base embeddings wrapper, instructor embeddings, and optimum embeddings in ONNX format.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/embeddings/huggingface.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1706096762933653962?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Multi-Document Agents\n   </strong>\n   : we\u2019ve introduced v0 experimental support for multi-document agents for advanced QA, beyond typical top-k RAG. It supports diverse queries from single to multiple docs. This foundational version sets the stage for future enhancements like parallel query planning and reduced latency.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/agent/multi_document_agents.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1708523212366393403?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <p>\n  \ud83c\udfc6\n  <strong>\n   Congratulations to our Streamlit Hackathon Winners!\n  </strong>\n </p>\n <p>\n  We love seeing people build amazing things with LlamaIndex!\n </p>\n <ol>\n  <li>\n   NewsGPT by Kang-Chi Ho:\n   <a href=\"https://buff.ly/46jkutx\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    https://buff.ly/46jkutx\n   </a>\n  </li>\n  <li>\n   FinSight by Vishwas Gowda:\n   <a href=\"https://buff.ly/3PzOnyC\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    https://buff.ly/3PzOnyC\n   </a>\n  </li>\n </ol>\n <p>\n  \u2728\n  <strong>\n   Feature Releases\n  </strong>\n  <strong>\n   and Enhancements:\n  </strong>\n </p>\n <ol>\n  <li>\n   <strong>\n    Multi-Document Agents\n   </strong>\n   : we introduced multi-document agents (V0) for advanced QA, beyond typical top-k RAG. They support diverse queries from single to multiple docs. This foundational version sets the stage for future enhancements like parallel query planning and reduced latency.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/agent/multi_document_agents.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1708523212366393403?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Ensemble Retriever:\n   </strong>\n   we\u2019re addressing the RAG challenge of determining chunk size by experimenting with diverse document chunking and ensembling for retrieval.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/retrievers/ensemble_retrieval.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1707541270934212737?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    HuggingFace Embeddings\n   </strong>\n   : we added native support for three more Hugging Face embedding models, including the base embeddings wrapper, instructor embeddings, and optimum embeddings in ONNX format.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/embeddings/huggingface.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1706096762933653962?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    OpenAI Function Calling fine-tuning:\n   </strong>\n   we\u2019re using OpenAI\u2019s latest function calling fine-tuning which enhanced structured data extraction, optimizing gpt-3.5-turbo for improved extraction in RAG.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/finetuning/openai_fine_tuning_functions.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1709986951661818185?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Metadata Extraction\n   </strong>\n   : we\u2019re making metadata extraction efficient by extracting a complete Pydantic object from a document with just one LLM call.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/metadata_extraction/PydanticExtractor.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1705302359038202101?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Structured RAG Outputs\n   </strong>\n   : we now efficiently structure RAG pipeline outputs with native Pydantic outputs from all queries without the need for an additional LLM parsing call.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/query_engine/pydantic_query_engine.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1709229624709025972?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Streamlined\n   </strong>\n   <a href=\"http://secinsights.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     secinsights.ai\n    </strong>\n   </a>\n   <strong>\n    deployment\n   </strong>\n   : Our open-sourced\n   <a href=\"https://secinsights.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    secinsights.ai\n   </a>\n   offers a RAG app template, now enhanced with GitHub Codespaces and Docker for swift cloud deployment without setup hassles.\n   <a href=\"https://twitter.com/llama_index/status/1707773681605419296?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    LongContextReorder:\n   </strong>\n   We introduced LongContextReorder****,**** Zeneto\u2019s approach to reposition vital context in RAG systems, addressing the challenge of over-retrieving which can obscure essential details.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/query_modules/node_postprocessors/modules.html#longcontextreorder\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1705009024050270456?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    RA-DIT:\n   </strong>\n   We drew inspiration from the RA-DIT paper, which introduced LLM fine-tuning for retrieval-augmented input prompts to improve RAG systems. This method fosters enhanced utilization of context and more effective answer synthesis, even in the presence of suboptimal context.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/finetuning/knowledge/finetune_retrieval_aug.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1709646787076935818?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Blockchain:\n   </strong>\n   LlamaIndex data agents can be now used to analyze any blockchain subgraph using natural language queries.\n   <a href=\"https://twitter.com/llama_index/status/1707417880420294741?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <p>\n  \ud83d\udd0e\n  <strong>\n   RAG Evaluation Enhancements:\n  </strong>\n </p>\n <ol>\n  <li>\n   <strong>\n    RetrieverEvaluator\n   </strong>\n   : We introduced \u201cRetrieverEvaluator\u201d for enhanced retrieval evaluations, complementing LLM generation tests. The module supports benchmarking, standard ranking metrics, and synthetic dataset creation for comprehensive retrieval assessments.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/evaluation/retrieval/retriever_eval.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1704884477552660612?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    SemanticSimilarityEvaluator\n   </strong>\n   : We introduced a new semantic similarity evaluator \u2014 SemanticSimilarityEvaluator for LLM/RAG outputs, comparing embedding similarity between reference and generated answers.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/evaluation/semantic_similarity_eval.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1705614101601509630?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <p>\n  \ud83d\udcda\n  <strong>\n   Tutorials:\n  </strong>\n </p>\n <ol>\n  <li>\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/low_level/oss_ingestion_retrieval.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Guide\n   </a>\n   on building RAG from scratch with open-source modules.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/dstackai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Dstack\n   </a>\n   <a href=\"https://dstack.ai/examples/llama-index-weaviate/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on implementing RAG with OSS LLMs using LlamaIndex and Weaviate.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/wenqi_glantz\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Wenqi Glantz\n   </a>\n   <a href=\"https://betterprogramming.pub/exploring-react-agent-for-better-prompting-in-rag-pipeline-b231aae0ca7c\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    turorial\n   </a>\n   on Exploring ReAct Agent for Better Prompting in RAG Pipeline.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/Javtorr_\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Javier Torres\n   </a>\n   <a href=\"https://gpt-index.readthedocs.io/en/stable/end_to_end_tutorials/chatbots/building_a_chatbot.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on building a multi-document chatbot.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/ecardenas300\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Erika Cardenas\n   </a>\n   <a href=\"https://www.youtube.com/watch?v=Su-ROQMaiaw\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on RAG techniques in LlamaIndex covering SQL Router Query Engine, Sub Question Query Engine, Recursive Retriever Query Engine, Self-Correcting Query Engine.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/wenqi_glantz\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Wenqi Glantz\n   </a>\n   <a href=\"https://betterprogramming.pub/7-query-strategies-for-navigating-knowledge-graphs-with-llamaindex-ed551863d416\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on 7 Query Strategies for Navigating Knowledge Graphs With LlamaIndex.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/ravithejads\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Ravi Theja\n   </a>\n   <a href=\"/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    tutorial\n   </a>\n   on Evaluating the Ideal Chunk Size for RAG using LlamaIndex.\n  </li>\n </ol>\n <p>\n  \u2699\ufe0f\n  <strong>\n   Integrations &amp; Collaborations:\n  </strong>\n </p>\n <ol>\n  <li>\n   <strong>\n    Arize AI Phoenix\n   </strong>\n   : We launched a one-code-line integration with Arize AI for comprehensive tracing and observability in all RAG/agent pipelines. Enjoy local data storage, track LLM input/output prompts, monitor token usage, timing, retrieval visualizations, and agent loops. Additionally, export traces for evaluations and data analysis. All while ensuring your data stays local.\n   <a href=\"https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_tracing_tutorial.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1708866998149816540?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Neo4j\n   </strong>\n   : We introduced an API spec for LLM-agent interaction with Neo4j, offering beyond just \u201ctext-to-cypher\u201d with full agent reasoning.\n   <a href=\"https://llamahub.ai/l/tools-neo4j_db\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1706049423644774910?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    TimescaleDB\n   </strong>\n   : We integrated with TimescaleDB for enhanced time-based retrieval in RAG systems, offering time filters and cost-effective storage solutions.\n   <a href=\"/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Blogpost\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1707057039950991798?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    BraintrustData\n   </strong>\n   : We integrated with BraintrustData, enabling seamless RAG pipeline construction, evaluations, and easy public URL sharing for results.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/retrievers/recurisve_retriever_nodes_braintrust.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1707485040018632776?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    LocalAI\n   </strong>\n   : We integrated LocalAI_API LLM support for on-prem runs or as an alternative to OpenAI LLM.\n   <a href=\"https://x.com/llama_index/status/1708227476684734555?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    HoneyHiveAI\n   </strong>\n   : We integrated with HoneyHiveAI for enhanced multi-step RAG/agent pipeline monitoring. Log traces, gather user feedback, and utilize it for precise fine-tuning and evaluations.\n   <a href=\"https://docs.honeyhive.ai/quickstart/llamaindex\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://x.com/llama_index/status/1709311769947357425?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    UnstructuredIO\n   </strong>\n   : We integrated with UnstructuredIO to tackle the RAG challenge of querying embedded tables in 10-K filings. Now, seamlessly query any tabular data or text within a 10-K document.\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/query_engine/sec_tables/tesla_10q_table.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1709352476456132702?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n  <li>\n   <strong>\n    Clarifai\n   </strong>\n   : We integrated with Clarifai, offering access to 40+ LLMs and various embedding models.\n   <a href=\"https://twitter.com/llama_index/status/1710065802672476342?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n   .\n  </li>\n </ol>\n <p>\n  \ud83c\udfa5\n  <strong>\n   Webinars:\n  </strong>\n </p>\n <ol>\n  <li>\n   <a href=\"https://www.singlestore.com/resources/webinar-how-to-build-a-genai-app-with-llama-index/?utm_source=kunal-kushwaha&amp;utm_medium=influencer&amp;utm_campaign=How-to-Build-a-GenAI-App-with-LlamaIndex&amp;campaignid=7014X0000029YtwQAE\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Webinar\n   </a>\n   by SingleStoreDB on How to Build a GenAI App with LlamaIndex.\n  </li>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=C5NhoMBkaQU\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Webinar\n   </a>\n   on projects built during the SuperAGI Autonomous Agents Hackathon featuring evo.ninja, RicAI, Atlas and MunichAI.\n  </li>\n </ol>\n <p>\n  \ud83c\udf88\n  <strong>\n   Events:\n  </strong>\n </p>\n <ol>\n  <li>\n   Jerry Liu and Simon\n   <a href=\"https://github.com/anyscale/ray-summit-2023-training/tree/main/Ray-LlamaIndex/notebooks\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    conducted\n   </a>\n   a workshop on RAG + Evaluation at RaySummit.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/yi_ding\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Yi Ding\n   </a>\n   spoke on \u2018LLM Quirks Mode\u2019 at MLOps community event.\n  </li>\n  <li>\n   Jerry Liu spoke on Evals/ Benchmarking\n   <a href=\"https://docs.google.com/presentation/d/1v7T6ejrSo87ndGeGC7tt6zeq-cftu03WWw7WL8Jskug/edit?usp=sharing\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    and Advanced RAG techniques\n   </a>\n   at AIConf 2023.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/ravithejads\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Ravi Theja\n   </a>\n   conducted a workshop on Mastering RAG with LlamaIndex at PyCon India, 2023.\n  </li>\n  <li>\n   <a href=\"https://twitter.com/ravithejads\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Ravi Theja\n   </a>\n   presented a\n   <a href=\"https://x.com/ravithejads/status/1709431323989856348?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    poster\n   </a>\n   on Automatic Knowledge Transfer(KT) Video generation on code bases using LlamaIndex at PyCon India, 2023.\n  </li>\n </ol>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 19480, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c5f7ca6-b5b6-4fbe-8e7e-19da0ce2f118": {"__data__": {"id_": "6c5f7ca6-b5b6-4fbe-8e7e-19da0ce2f118", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-update-6-26-2023-ed30a9d45f84.html", "file_name": "llamaindex-update-6-26-2023-ed30a9d45f84.html", "file_type": "text/html", "file_size": 16810, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-update-6-26-2023-ed30a9d45f84.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-update-6-26-2023-ed30a9d45f84.html", "file_name": "llamaindex-update-6-26-2023-ed30a9d45f84.html", "file_type": "text/html", "file_size": 16810, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "50db8707eda6a0307e43672195cf77d151278becee8376eefd89448badaccd8d", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Greetings, LlamaIndex community!\n </p>\n <p>\n  We\u2019re excited to introduce our new blog series, the LlamaIndex Update. Recognizing the fast pace of our open-source project, this series will serve as your continual guide, tracking the latest advancements in features, webinars, hackathons, and community events.\n </p>\n <p>\n  Our goal is simple: to keep you updated, engaged, and inspired. Whether you\u2019re a long-time contributor or a new joiner, these updates will help you stay in sync with our progress.\n </p>\n <p>\n  So, let\u2019s explore the recent happenings in our premier edition of the LlamaIndex Update.\n </p>\n <h2>\n  <strong>\n   Features And Integrations:\n  </strong>\n </h2>\n <ol>\n  <li>\n   LLMs with Knowledge Graphs, supported by NebulaGraph. This new stack enables unique retrieval-augmented generation techniques. Our Knowledge Graph index introduces a GraphStore abstraction, complementing our existing data store types.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1667196231863656448\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   Better LLM app UX supports in-line citations of its sources, enhancing interpretability and traceability. Our new\n   <code class=\"cw po pp pq pr b\">\n    CitationQueryEngine\n   </code>\n   enables these citations and ensures they correspond with retrieved documents. This feature marks a leap towards improving transparency in LlamaIndex applications.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/query_engine/citation_query_engine.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1667563694472175616?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   LlamaIndex integrates with Microsoft Guidance to ensure structured outputs from LLMs. It allows direct prompting of JSON keys and facilitates the conversion of Pydantic objects into the Guidance format, enhancing structured interactions. It can be used independently or with the SubQuestionQueryEngine.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/how_to/integrations/guidance.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1668281830347530242\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   The GuidelineEvaluator module allows users to set text guidelines, thereby aiding in the evaluation of LLM-generated text responses. This paves the way toward automated error correction capabilities.\n   <a href=\"https://github.com/jerryjliu/llama_index/blob/main/docs/examples/evaluation/RetryQuery.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1667920234500751361?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   We now include a simple\n   <code class=\"cw po pp pq pr b\">\n    OpenAIAgent\n   </code>\n   , offering an agent interface capable of sequential tool use and async callbacks. This integration was made possible with the help of the OpenAI function API and the LangChain abstractions.\n   <a href=\"https://twitter.com/llama_index/status/1668995630356725762\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   <code class=\"cw po pp pq pr b\">\n    OpenAIPydanticProgram\n   </code>\n   in LlamaIndex enhances structured output extraction. This standalone module allows any LLM input to be converted into a Pydantic object, providing a streamlined approach to data structuring.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/output_parsing/openai_pydantic_program.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1668995632873234435\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   We now incorporate the FLARE technique for a knowledge-augmented long-form generation. FLARE uses iterative retrieval to construct extended content, deciding to perform retrieval with each sentence. Unlike conventional vector index methods, our FLARE implementation builds a template iteratively, filling gaps with retrieval for more pertinent responses. Please note, this is a beta feature and works best with GPT-4.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/query_engine/flare_query_engine.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1669719509987643392?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   We now employ the Maximal Marginal Relevance (MMR) algorithm to enhance diversity and minimize redundancy in retrieved results. This technique measures the similarity between a candidate document and the query while minimizing similarity with previous documents, depending on a user-specified threshold. Please note that careful calibration is necessary to ensure that increased diversity doesn\u2019t introduce irrelevant context. The threshold value is key to balancing diversity and relevance.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/SimpleIndexDemoMMR.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1669801174109925377?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   We now support recursive Pydantic objects for complex schema extraction. This enhancement, inspired by parsing directory trees, employs a mix of recursive (Node) and non-recursive (DirectoryTree) Pydantic models, facilitating more sophisticated agent-tool interactions.\n   <a href=\"https://twitter.com/jerryjliu0/status/1670823521801621505?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   We have developed agents that can perform advanced query planning over data using the Function API and Pydantic. These agents input a full Pydantic graph in the function signature of a query plan tool, which is then executed. This system can work with any tool and has the potential to construct complex query plans. However, it has limitations like difficulty in producing deep nesting and the possibility of outputting invalid responses.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/agent/openai_agent_query_plan.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1671183584072470529?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   `OpenAIAgent` is capable of advanced data retrieval and analysis, such as auto-vector database retrieval and joint text-to-SQL and semantic search. We have also built a query plan tool interface that allows the agent to generate structured/nested query plans, which can then be executed against any set of tools, enabling advanced reasoning and analysis.\nDocs:\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/agent/openai_agent_query_cookbook.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    OpenAI Agent + Query Engine\n   </a>\n   ,\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/agent/openai_agent_retrieval.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Retrieval Augmented OpenAI Agent\n   </a>\n   ,\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/agent/openai_agent_query_plan.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    OpenAI Agent Query Planning\n   </a>\n   .\n   <a href=\"https://twitter.com/llama_index/status/1671185213538578433?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   The new multi-router feature allows for QA over complex data collections, where answers may be spread across multiple sources. It uses a \u201cMultiSelector\u201d object to select relevant choices given a query. The router can pick up to a maximum number of choices. It can use either a raw LLM completion API or the OpenAI Function API. If the Function API is used, schema validity can be enforced. A simple usage example involves a RouterQueryEngine, where the PydanticMultiSelector selects the relevant vector and keyword index to synthesize an answer.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/query_engine/RouterQueryEngine.html#define-router-query-engine\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1671536412498477057?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   We have made a significant upgrade to our token tracking feature. Users can now easily track prompt, completion, and embedding tokens through the platform\u2019s callback handler. The upgrade aims to make token counting more efficient and user-friendly.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/how_to/callbacks/token_counting_migration.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/llama_index/status/1671893230412247042?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   We released a guide that demonstrates how to build a custom retriever that combines vector similarity search with knowledge graphs in LLM RAG systems. It involves constructing a vector index and a knowledge graph index and combining the results from both during query time. This method can improve results by providing additional context for entities. However, it may lead to a slight increase in latency.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1671895098270031872?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   In an LLM workflow, managing large amounts of data, including PDFs, agent Tools, SQL table schemas, etc., requires efficient indexing. To handle this, we introduce our Object Index, a wrapper over our existing index data structures. This allows any object to be converted into an indexable text format, providing a unified interface that enhances the functionality of our indices over various data types.\n   <a href=\"https://twitter.com/jerryjliu0/status/1672263302628646912?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   The OpenBB Finance Terminal is a great platform for investment research and is completely open-source. It now includes a feature called AskOBB, powered by Llama Index, which allows users to easily access any financial data through natural language.\n   <a href=\"https://twitter.com/jerryjliu0/status/1672637698136489989?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n  <li>\n   The TruLens team has introduced tracing for LlamaIndex-based LLM applications in its latest release. This new feature allows developers to evaluate and track their experiments more efficiently. It automatically evaluates various components of the application stack, including app inputs and outputs, LLM calls, retrieved-context chunks from an index, and latency. This is part of an ongoing collaboration between the LlamaIndex and TruLens teams to improve the development, evaluation, and iteration of LLM apps.\n   <a href=\"https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.3.0/trulens_eval/examples/vector-dbs/llama_index/quickstart.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://medium.com/llamaindex-blog/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c\" rel=\"noopener\">\n    Blogpost\n   </a>\n  </li>\n  <li>\n   Prem App has successfully integrated with Llama Index, enhancing privacy in AI development. This union allows developers to connect custom data sources to large language models easily, simplifying data ingestion, indexing, and querying. To use this integration, download the Prem App and connect your data sources through the Llama Index platform. This allows for efficient data management and boosts AI application development, providing developers with more control and flexibility.\n   <a href=\"https://github.com/premAI-io/prem-daemon/blob/main/resources/notebooks/llama_index.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Notebook\n   </a>\n   ,\n   <a href=\"https://medium.com/llamaindex-blog/llama-index-prem-ai-join-forces-51702fecedec\" rel=\"noopener\">\n    Blogpost\n   </a>\n  </li>\n  <li>\n   We now enable the extraction of tabular data frames from unstructured text. This feature, powered by the OpenAI Function API and Pydantic models, simplifies text-to-SQL or text-to-DF conversions within structured data workflows. Note that effective use may require significant prompt optimization.\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/output_parsing/df_output_parser.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Docs\n   </a>\n   ,\n   <a href=\"https://twitter.com/jerryjliu0/status/1673004155227750401?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Tweet\n   </a>\n  </li>\n </ol>\n <p>\n  <strong>\n   Tutorials:\n  </strong>\n </p>\n <ol>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=WKvAWub8VCU&amp;t=2s\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    James Brigg\u2019s tutorial\n   </a>\n   on using LlamaIndex with Pinecone.\n  </li>\n  <li>\n   <a href=\"https://weaviate.io/blog/llamaindex-and-weaviate\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Jerry Liu's tutorial\n   </a>\n   on using LlamaIndex with Weaviate.\n  </li>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=cNMYeW2mpBs\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Sophia Yang tutorial\n   </a>\n   on LlamaIndex overview, Use cases, and integration with LangChain.\n  </li>\n  <li>\n   Anil Chandra Naidu is building a\n   <a href=\"https://github.com/SamurAIGPT/LlamaIndex-course\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    course\n   </a>\n   on LlamaIndex. The course presently covers topics such as introduction, fundamentals, and data connectors.\n  </li>\n  <li>\n   <a href=\"https://github.com/openai/openai-cookbook/blob/main/examples/third_party_examples/financial_document_analysis_with_llamaindex.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    OpenAI cookbook by Simon\n   </a>\n   on how to perform financial analysis with LlamaIndex.\n  </li>\n </ol>\n <h2>\n  <strong>\n   Webinars And Podcasts:\n  </strong>\n </h2>\n <ol>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=6ot9io-brzI&amp;t=2s\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Webinar\n   </a>\n   on Demonstrate-Search-Predict (DSP) with Omar Khattab.\n  </li>\n  <li>\n   <a href=\"https://www.youtube.com/watch?v=7aIzxFyJP-A&amp;t=22s\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Webinar\n   </a>\n   on Practical challenges of building a Legal Chatbot over your PDFs with Sam Yu\n  </li>\n  <li>\n   <a href=\"https://podcasters.spotify.com/pod/show/maml-podcast/episodes/Jerry-Liu---Building-LlamaIndex--the-Data-Framework-for-LLMs-e25u3ga\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    MaML podcast\n   </a>\n   with Jerry Liu.\n  </li>\n </ol>\n <h2>\n  <strong>\n   Hackathons:\n  </strong>\n </h2>\n <p>\n  The LlamaIndex team has presented at the UC Berkeley Hackathon and the Stellaris VP Hackathon in India. The community has warmly welcomed LlamaIndex, and teams at these hackathons have developed intriguing use cases \u2014 Customer support during emergency cases, Understanding Legal documents.\n </p>\n <h2>\n  <strong>\n   Events:\n  </strong>\n </h2>\n <ol>\n  <li>\n   Jerry Liu spoke on Building and troubleshooting an AI Search &amp; Retrieval System at Arize \u2014 LlamaIndex event.\n  </li>\n  <li>\n   Ravi Theja presented about LlamaIndex and its applications at Together in India.\n  </li>\n </ol>\n <p>\n  That\u2019s all for this edition of the LlamaIndex Update. We hope you found this information useful and are as excited as we are about the progress we\u2019re making. We\u2019re grateful for the continued support and contributions from our community. Remember, your feedback and suggestions are invaluable to us, so don\u2019t hesitate to reach out.\n </p>\n <p>\n  Stay tuned for our next update, where we\u2019ll share more exciting developments from the LlamaIndex project. Until then, happy indexing!\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 16779, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6fb7feeb-82a2-4dab-bf53-1e4295f8cd5e": {"__data__": {"id_": "6fb7feeb-82a2-4dab-bf53-1e4295f8cd5e", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-v0-10-838e735948f8.html", "file_name": "llamaindex-v0-10-838e735948f8.html", "file_type": "text/html", "file_size": 26572, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-v0-10-838e735948f8.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-v0-10-838e735948f8.html", "file_name": "llamaindex-v0-10-838e735948f8.html", "file_type": "text/html", "file_size": 26572, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "455a94bcb19f046324e6c2572d19354bb586fc9bb80911ad745fb92017b1fe0b", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Today we\u2019re excited to launch LlamaIndex v0.10.0. It is by far the biggest update to our Python package to date (\n  <a href=\"https://github.com/run-llama/llama_index/pull/10537\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   see this gargantuan PR\n  </a>\n  ), and it takes a massive step towards making LlamaIndex a next-generation,\n  <strong>\n   production-ready data framework\n  </strong>\n  for your LLM applications.\n </p>\n <p>\n  LlamaIndex v0.10 contains some major updates:\n </p>\n <ol>\n  <li>\n   <strong>\n    We have created a\n   </strong>\n   <code class=\"cw oq or os ot b\">\n    <strong>\n     llama-index-core\n    </strong>\n   </code>\n   <strong>\n    package, and split all integrations and templates into separate packages:\n   </strong>\n   Hundreds of integrations (LLMs, embeddings, vector stores, data loaders, callbacks, agent tools, and more) are now versioned and packaged as a separate PyPI packages, while preserving namespace imports: for example, you can still use\n   <code class=\"cw oq or os ot b\">\n    from llama_index.llms.openai import OpenAI\n   </code>\n   for a LLM.\n  </li>\n  <li>\n   <a href=\"https://llamahub.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     LlamaHub\n    </strong>\n   </a>\n   <strong>\n    will be the central hub for all integrations:\n   </strong>\n   the former\n   <a href=\"https://github.com/run-llama/llama-hub\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    llama-hub\n   </a>\n   repo itself is consolidated into the main\n   <a href=\"https://github.com/run-llama/llama_index/tree/main/llama-index-integrations\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    llama_index\n   </a>\n   repo. Instead of integrations being split between the core library and LlamaHub, every integration will be listed on LlamaHub. We are actively working on updating the site, stay tuned!\n  </li>\n  <li>\n   <strong>\n    ServiceContext is deprecated:\n   </strong>\n   Every LlamaIndex user is familiar with ServiceContext, which over time has become a clunky, unneeded abstraction for managing LLMs, embeddings, chunk sizes, callbacks, and more. As a result we are completely deprecating it; you can now either directly specify arguments or set a default.\n  </li>\n </ol>\n <p>\n  Upgrading your codebase to LlamaIndex v0.10 may lead to some breakages, primarily around our integrations/packaging changes, but fortunately we\u2019ve included some scripts to make it as easy as possible to migrate your codebase to use LlamaIndex v0.10.\n </p>\n <p>\n  Check out the below sections for more details, and go to the very last section for resource links to everything.\n </p>\n <h1>\n  Splitting into `llama-index-core` and integration packages\n </h1>\n <p>\n  The first and biggest change we\u2019ve made is a massive packaging refactor.\n </p>\n <p>\n  LlamaIndex has evolved into a broad toolkit containing hundreds of integrations:\n </p>\n <ul>\n  <li>\n   150+ data loaders\n  </li>\n  <li>\n   35+ agent tools\n  </li>\n  <li>\n   50+ LlamaPack templates\n  </li>\n  <li>\n   50+ LLMs\n  </li>\n  <li>\n   25+ embeddings\n  </li>\n  <li>\n   40+ vector stores\n  </li>\n </ul>\n <p>\n  and more across the\n  <code class=\"cw oq or os ot b\">\n   llama_index\n  </code>\n  and\n  <code class=\"cw oq or os ot b\">\n   llama-hub\n  </code>\n  repos. The rapid growth of our ecosystem has been awesome to see, but it\u2019s also come with growing pains:\n </p>\n <ul>\n  <li>\n   Many of the integrations lack proper tests\n  </li>\n  <li>\n   Users are responsible for figuring out dependencies\n  </li>\n  <li>\n   If an integration updates, users will have to update their entire\n   <code class=\"cw oq or os ot b\">\n    llama-index\n   </code>\n   Python package.\n  </li>\n </ul>\n <p>\n  In response to this, we\u2019ve done the following.\n </p>\n <ul>\n  <li>\n   <strong>\n    Created\n   </strong>\n   <code class=\"cw oq or os ot b\">\n    <strong>\n     llama-index-core\n    </strong>\n   </code>\n   <strong>\n    :\n   </strong>\n   This is a slimmed-down package that contains the core LlamaIndex abstractions and components, without any integrations.\n  </li>\n  <li>\n   <strong>\n    Created separate packages for all integrations/templates:\n   </strong>\n   Every integration is now available as a separate package. This includes\n   <em class=\"qd\">\n    all\n   </em>\n   integrations, including those on LlamaHub! See\n   <a href=\"https://pretty-sodium-5e0.notion.site/ce81b247649a44e4b6b35dfb24af28a6?v=53b3c2ced7bb4c9996b81b83c9f01139\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    our Notion registry\n   </a>\n   page for a full list of all packages.\n  </li>\n </ul>\n <p>\n  The\n  <code class=\"cw oq or os ot b\">\n   llama-index\n  </code>\n  package still exists, and it imports\n  <code class=\"cw oq or os ot b\">\n   llama-index-core\n  </code>\n  and a minimal set of integrations. Since we use OpenAI by default, this includes OpenAI packages (\n  <code class=\"cw oq or os ot b\">\n   llama-index-llms-openai\n  </code>\n  ,\n  <code class=\"cw oq or os ot b\">\n   llama-index-embeddings-openai\n  </code>\n  , and OpenAI programs/question generation/multimodal), as well as our beloved SimpleDirectoryReader (which is in\n  <code class=\"cw oq or os ot b\">\n   llama-index-readers-file\n  </code>\n  ).\n </p>\n <p>\n  <strong>\n   NOTE:\n  </strong>\n  if you don\u2019t want to migrate to v0.10 yet and want to continue using the current LlamaIndex abstractions, we are maintaining\n  <code class=\"cw oq or os ot b\">\n   llama-index-legacy\n  </code>\n  (pinned to the latest release 0.9.48) for the foreseeable future.\n </p>\n <h2>\n  Revamped Folder Structure\n </h2>\n <p>\n  We\u2019ve completely revamped the folder structure in the\n  <code class=\"cw oq or os ot b\">\n   llama_index\n  </code>\n  repo. The most important folders you should care about are:\n </p>\n <ul>\n  <li>\n   <code class=\"cw oq or os ot b\">\n    <strong>\n     llama-index-core\n    </strong>\n   </code>\n   : This folder contains all core LlamaIndex abstractions.\n  </li>\n  <li>\n   <code class=\"cw oq or os ot b\">\n    <strong>\n     llama-index-integrations\n    </strong>\n   </code>\n   : This folder contains third-party integrations for 19 LlamaIndex abstractions. This includes data loaders, LLMs, embedding models, vector stores, and more. See below for more details.\n  </li>\n  <li>\n   <code class=\"cw oq or os ot b\">\n    <strong>\n     llama-index-packs\n    </strong>\n   </code>\n   : This folder contains our 50+ LlamaPacks, which are templates designed to kickstart a user\u2019s application.\n  </li>\n </ul>\n <p>\n  Other folders:\n </p>\n <ul>\n  <li>\n   <code class=\"cw oq or os ot b\">\n    llama-index-legacy\n   </code>\n   : contains the legacy LlamaIndex code.\n  </li>\n  <li>\n   <code class=\"cw oq or os ot b\">\n    llama-index-experimental\n   </code>\n   : contains experimental features. Largely unused right now (outside parameter tuning).\n  </li>\n  <li>\n   <code class=\"cw oq or os ot b\">\n    llama-index-finetuning\n   </code>\n   : contains LlamaIndex fine-tuning abstractions. These are still relatively experimental.\n  </li>\n </ul>\n <p>\n  The sub-directories in\n  <code class=\"cw oq or os ot b\">\n   integrations\n  </code>\n  and\n  <code class=\"cw oq or os ot b\">\n   packs\n  </code>\n  represent individual packages. The name of the folder corresponds to the package name. For instance,\n  <code class=\"cw oq or os ot b\">\n   llama-index-integrations/llms/llama-index-llms-gemini\n  </code>\n  corresponds to the\n  <code class=\"cw oq or os ot b\">\n   llama-index-llms-gemini\n  </code>\n  PyPI package.\n </p>\n <p>\n  Within each package folder, the source files are arranged in the same paths that you use to import them. For example, in the Gemini LLM package, you\u2019ll see a folder called\n  <code class=\"cw oq or os ot b\">\n   llama_index/llms/gemini\n  </code>\n  containing the source files. This folder structure is what allows you to\n  <strong>\n   preserve the top-level\n  </strong>\n  <code class=\"cw oq or os ot b\">\n   <strong>\n    llama_index\n   </strong>\n  </code>\n  <strong>\n   namespace during importing.\n  </strong>\n  In the case of Gemini LLM, you would pip install\n  <code class=\"cw oq or os ot b\">\n   llama-index-llms-gemini\n  </code>\n  and then import using\n  <code class=\"cw oq or os ot b\">\n   from llama_index.llms.gemini import Gemini\n  </code>\n  .\n </p>\n <p>\n  Every one of these subfolders also has the resources needed to packagify it:\n  <code class=\"cw oq or os ot b\">\n   pyproject.toml\n  </code>\n  ,\n  <code class=\"cw oq or os ot b\">\n   poetry.lock\n  </code>\n  , and a\n  <code class=\"cw oq or os ot b\">\n   Makefile\n  </code>\n  , along with a script to automatically create a package.\n </p>\n <p>\n  If you\u2019re looking to contribute an integration or pack, don\u2019t worry! We have a\n  <a href=\"https://docs.llamaindex.ai/en/stable/contributing/contributing.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   full contributing guide\n  </a>\n  designed to make this as seamless as possible, make sure to check it out.\n </p>\n <h1>\n  Integrations\n </h1>\n <p>\n  <em class=\"qd\">\n   All\n  </em>\n  third-party integrations are now under\n  <code class=\"cw oq or os ot b\">\n   llama-index-integrations\n  </code>\n  . There are 19 folders in here. The main integration categories are:\n </p>\n <ul>\n  <li>\n   <code class=\"cw oq or os ot b\">\n    llms\n   </code>\n  </li>\n  <li>\n   <code class=\"cw oq or os ot b\">\n    embeddings\n   </code>\n  </li>\n  <li>\n   <code class=\"cw oq or os ot b\">\n    multi_modal_llms\n   </code>\n  </li>\n  <li>\n   <code class=\"cw oq or os ot b\">\n    readers\n   </code>\n  </li>\n  <li>\n   <code class=\"cw oq or os ot b\">\n    tools\n   </code>\n  </li>\n  <li>\n   <code class=\"cw oq or os ot b\">\n    vector_stores\n   </code>\n  </li>\n </ul>\n <p>\n  For completeness here are all the other categories:\n  <code class=\"cw oq or os ot b\">\n   agent\n  </code>\n  ,\n  <code class=\"cw oq or os ot b\">\n   callbacks\n  </code>\n  ,\n  <code class=\"cw oq or os ot b\">\n   evaluation\n  </code>\n  ,\n  <code class=\"cw oq or os ot b\">\n   extractors\n  </code>\n  ,\n  <code class=\"cw oq or os ot b\">\n   graph_stores\n  </code>\n  ,\n  <code class=\"cw oq or os ot b\">\n   indices\n  </code>\n  ,\n  <code class=\"cw oq or os ot b\">\n   output_parsers\n  </code>\n  ,\n  <code class=\"cw oq or os ot b\">\n   postprocessor\n  </code>\n  ,\n  <code class=\"cw oq or os ot b\">\n   program\n  </code>\n  ,\n  <code class=\"cw oq or os ot b\">\n   question_gen\n  </code>\n  ,\n  <code class=\"cw oq or os ot b\">\n   response_synthesizers\n  </code>\n  ,\n  <code class=\"cw oq or os ot b\">\n   retrievers\n  </code>\n  ,\n  <code class=\"cw oq or os ot b\">\n   storage\n  </code>\n  ,\n  <code class=\"cw oq or os ot b\">\n   tools\n  </code>\n  .\n </p>\n <p>\n  The integrations in the most common categories can be found in our temporary\n  <a href=\"https://pretty-sodium-5e0.notion.site/ce81b247649a44e4b6b35dfb24af28a6?v=53b3c2ced7bb4c9996b81b83c9f01139\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    Notion package registry page\n   </strong>\n  </a>\n  . All integrations can be found in\n  <a href=\"https://github.com/run-llama/llama_index/tree/main/llama-index-integrations\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   our Github repo\n  </a>\n  . The folder name of each integration package corresponds to the name of the package \u2014 so if you find an integration you like, you now know how to pip install it!\n </p>\n <p>\n  We are actively working to make all integrations viewable on LlamaHub. Our vision for LlamaHub is to be\n  <em class=\"qd\">\n   the\n  </em>\n  hub for all third-party integrations.\n </p>\n <p>\n  If you\u2019re interested in contributing a package, see our\n  <code class=\"cw oq or os ot b\">\n   contributing\n  </code>\n  section below!\n </p>\n <h1>\n  Usage Example\n </h1>\n <p>\n  Here is a simple example of installing and using an Anthropic LLM.\n </p>\n <p>\n  <code class=\"cw oq or os ot b\">\n   pip install llama-index-llms-anthropic\n  </code>\n </p>\n <pre><span class=\"rb pa gt ot b bf rc rd l re rf\" id=\"8760\"><span class=\"hljs-keyword\">from</span> llama_index.llms.anthropic <span class=\"hljs-keyword\">import</span> Anthropic\nllm = Anthropic(api_key=<span class=\"hljs-string\">\"&amp;lt;api_key&amp;gt;\"</span>)</span></pre>\n <p>\n  Here is an example of using a data loader.\n </p>\n <p>\n  <code class=\"cw oq or os ot b\">\n   pip install llama-index-readers-notion\n  </code>\n </p>\n <pre><span class=\"rb pa gt ot b bf rc rd l re rf\" id=\"e19a\"><span class=\"hljs-keyword\">from</span> llama_index.readers.notion <span class=\"hljs-keyword\">import</span> NotionPageReader\nintegration_token = os.getenv(<span class=\"hljs-string\">\"NOTION_INTEGRATION_TOKEN\"</span>)\npage_ids = [<span class=\"hljs-string\">\"&amp;lt;page_id&amp;gt;\"</span>]\nreader = NotionPageReader(integration_token=integration_token)\ndocuments = reader.load_data(page_ids=page_ids)</span></pre>\n <p>\n  Here is an example of using a LlamaPack:\n </p>\n <p>\n  <code class=\"cw oq or os ot b\">\n   pip install llama-index-packs-sentence-window-retriever\n  </code>\n </p>\n <pre><span class=\"rb pa gt ot b bf rc rd l re rf\" id=\"ec7c\"><span class=\"hljs-keyword\">from</span> llama_index.packs.sentence_window_retriever <span class=\"hljs-keyword\">import</span> SentenceWindowRetrieverPack\n<span class=\"hljs-comment\"># create the pack</span>\n<span class=\"hljs-comment\"># get documents from any data loader</span>\nsentence_window_retriever_pack = SentenceWindowRetrieverPack(\n  documents\n)\nresponse = sentence_window_retriever_pack.run(<span class=\"hljs-string\">\"Tell me a bout a Music celebritiy.\"</span>)</span></pre>\n <h1>\n  Dealing with Breaking Changes\n </h1>\n <p>\n  This update comes with breaking changes, mostly around imports. For all integrations, you can no longer do any of these:\n </p>\n <pre><span class=\"rb pa gt ot b bf rc rd l re rf\" id=\"31a0\"><span class=\"hljs-comment\"># no more using `llama_index.llms` as a top-level package</span>\n<span class=\"hljs-keyword\">from</span> llama_index.llms <span class=\"hljs-keyword\">import</span> OpenAI\n<span class=\"hljs-comment\"># no more using `llama_index.vector_stores` as a top-level package</span>\n<span class=\"hljs-keyword\">from</span> llama_index.vector_stores <span class=\"hljs-keyword\">import</span> PineconeVectorStore\n<span class=\"hljs-comment\"># llama_hub imports are now no longer supported.</span>\n<span class=\"hljs-keyword\">from</span> llama_hub.slack.base <span class=\"hljs-keyword\">import</span> SlackReader</span></pre>\n <p>\n  Instead you can do these:\n </p>\n <pre><span class=\"rb pa gt ot b bf rc rd l re rf\" id=\"f96d\"><span class=\"hljs-keyword\">from</span> llama_index.llms.openai <span class=\"hljs-keyword\">import</span> OpenAI\n<span class=\"hljs-keyword\">from</span> llama_index.vector_stores.pinecone <span class=\"hljs-keyword\">import</span> PineconeVectorStore\n<span class=\"hljs-comment\"># <span class=\"hljs-doctag\">NOTE:</span> no longer import a separate llama_hub package</span>\n<span class=\"hljs-keyword\">from</span> llama_index.readers.slack <span class=\"hljs-keyword\">import</span> SlackReader</span></pre>\n <p>\n  See our\n  <a href=\"https://pretty-sodium-5e0.notion.site/v0-10-0-Migration-Guide-6ede431dcb8841b09ea171e7f133bd77\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   migration guide\n  </a>\n  (also described below) for more details.\n </p>\n <h1>\n  LlamaHub as a Central Hub for Integrations\n </h1>\n <p>\n  With these packaging updates, we\u2019re expanding the concept of LlamaHub to become a central hub of\n  <em class=\"qd\">\n   all\n  </em>\n  LlamaIndex integrations to fulfill its vision of becoming an integration site at the center of the LLM ecosystem. This expands beyond its existing domain of loaders, tools, packs, and datasets, to include LLMs, embeddings, vector stores, callbacks, and more.\n </p>\n <p>\n  <strong>\n   This effort is still a WIP.\n  </strong>\n  If you go to\n  <a href=\"http://llamahub.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   llamahub.ai\n  </a>\n  today, you\u2019ll see that the site has not been updated yet, and it still contains the current set of integrations (data loaders, tools, LlamaPacks, datasets). Rest assured we\u2019ll be updating the site in a few weeks; in the meantime check out our\n  <a href=\"https://pretty-sodium-5e0.notion.site/ce81b247649a44e4b6b35dfb24af28a6?v=53b3c2ced7bb4c9996b81b83c9f01139\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Notion package registry\n  </a>\n  /\n  <a href=\"https://github.com/run-llama/llama_index/tree/main/llama-index-integrations\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   repo\n  </a>\n  for a list of all integrations/packages.\n </p>\n <h2>\n  Sunsetting llama-hub repo\n </h2>\n <p>\n  Since all integrations have been moved to the llama_index repo, we are sunsetting the llama-hub repo (but\n  <a href=\"https://llamahub.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaHub\n  </a>\n  itself lives on!). We did the painstaking work of migrating and packaging all existing llama-hub integrations. For all future contributions please submit directly to the llama_index repo!\n </p>\n <h2>\n  `download` syntax\n </h2>\n <p>\n  A popular UX for fetching integrations through LlamaHub has been the\n  <code class=\"cw oq or os ot b\">\n   download\n  </code>\n  syntax:\n  <code class=\"cw oq or os ot b\">\n   download_loader\n  </code>\n  ,\n  <code class=\"cw oq or os ot b\">\n   download_llama_pack\n  </code>\n  , and more.\n </p>\n <p>\n  <strong>\n   This will still work, but have different behavior. Check out the details below:\n  </strong>\n </p>\n <ul>\n  <li>\n   <code class=\"cw oq or os ot b\">\n    download_llama_pack\n   </code>\n   : Will download a pack under\n   <code class=\"cw oq or os ot b\">\n    llama-index-packs\n   </code>\n   to a local file on your disk. This allows you to directly use and modify the source code from the template.\n  </li>\n  <li>\n   Every other download function\n   <code class=\"cw oq or os ot b\">\n    download_loader\n   </code>\n   ,\n   <code class=\"cw oq or os ot b\">\n    download_tool\n   </code>\n   : This will directly run pip install on the relevant integration package.\n  </li>\n </ul>\n <h1>\n  Deprecating ServiceContext\n </h1>\n <p>\n  Last but not least, we are deprecating our\n  <code class=\"cw oq or os ot b\">\n   ServiceContext\n  </code>\n  construct and as a result improving the developer experience of LlamaIndex.\n </p>\n <p>\n  Our\n  <code class=\"cw oq or os ot b\">\n   ServiceContext\n  </code>\n  object existed as a general configuration container containing an LLM, embedding model, callback, and more; it was created before we had proper LLM, embedding, prompt abstractions and was meant to be an intermediate user-facing layer to let users define these parameters.\n </p>\n <p>\n  Over time however, this object became increasingly difficult to use. Passing in an entire\n  <code class=\"cw oq or os ot b\">\n   service_context\n  </code>\n  container to any module (index, retriever, post-processor, etc.) made it hard to reason about which component was actually getting used. Since all modules use OpenAI by default, users were getting asked to unnecessarily specify their OpenAI key even in cases where they\u2019d want to use a local model (because the embedding model default was still OpenAI). It was also laborious to import and type out.\n </p>\n <p>\n  Another related pain point was that if you had a custom model or especially a custom callback, you had to manually pass in the\n  <code class=\"cw oq or os ot b\">\n   service_context\n  </code>\n  to\n  <em class=\"qd\">\n   all\n  </em>\n  modules. This was laborious and it was easy for users to forget, resulting in missed callbacks or inconsistent model usage.\n </p>\n <p>\n  Therefore we\u2019ve made the following changes:\n </p>\n <ol>\n  <li>\n   <strong>\n    ServiceContext is now deprecated:\n   </strong>\n   You should now directly pass in relevant parameters to modules, such as the embedding model for indexing and the LLM for querying/response synthesis.\n  </li>\n  <li>\n   <strong>\n    You can now define global settings:\n   </strong>\n   Define this once, and don\u2019t worry about specifying any custom parameters at all in your downstream code. This is especially useful for callbacks.\n  </li>\n </ol>\n <p>\n  All references to ServiceContext in our docs/notebooks have been removed and changed to use either direct modules or the global settings object. See our usage example below as well.\n </p>\n <h1>\n  Usage Example\n </h1>\n <p>\n  To build a\n  <code class=\"cw oq or os ot b\">\n   VectorStoreIndex\n  </code>\n  and then query it, you can now pass in the embedding model and LLM directly\n </p>\n <pre><span class=\"rb pa gt ot b bf rc rd l re rf\" id=\"9560\"><span class=\"hljs-keyword\">from</span> llama_index.embeddings.openai <span class=\"hljs-keyword\">import</span> OpenAIEmbedding\n<span class=\"hljs-keyword\">from</span> llama_index.llms.openai <span class=\"hljs-keyword\">import</span> OpenAI\n<span class=\"hljs-keyword\">from</span> llama_index.core.callbacks <span class=\"hljs-keyword\">import</span> CallbackManager\n\nembed_model = OpenAIEmbedding()\nllm = OpenAI()\ncallback_manager = CallbackManager()\nindex = VectorStoreIndex.from_documents(\n documents, embed_model=embed_model, callback_manager=callback_manager\n)\nquery_engine = index.as_query_engine(llm=llm)</span></pre>\n <p>\n  Or you can define a global settings object\n </p>\n <pre><span class=\"rb pa gt ot b bf rc rd l re rf\" id=\"a11f\"><span class=\"hljs-keyword\">from</span> llama_index.core.settings <span class=\"hljs-keyword\">import</span> Settings\nSettings.llm = llm\nSettings.embed_model = embed_model\nSettings.callback_manager = callback_manager\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()</span></pre>\n <h1>\n  Contributing to LlamaIndex v0.10\n </h1>\n <p>\n  v0.10 makes the\n  <code class=\"cw oq or os ot b\">\n   llama_index\n  </code>\n  repo the central place for all community contributions, whether you are interested in contributing core refactors, or integrations/packs!\n </p>\n <p>\n  If you\u2019re contributing an integration/pack, v0.10 makes it way easier for you to contribute something that can be independently versioned, tested, and packaged.\n </p>\n <p>\n  We have utility scripts to make the package creation process for an integration or pack effortless:\n </p>\n <pre><span class=\"rb pa gt ot b bf rc rd l re rf\" id=\"1b83\"><span class=\"hljs-meta\"># create a new pack</span>\ncd ./llama-index-packs\nllamaindex-cli <span class=\"hljs-keyword\">new</span>-package --kind <span class=\"hljs-string\">\"packs\"</span> --name <span class=\"hljs-string\">\"my new pack\"</span>\n\n<span class=\"hljs-meta\"># create a new integration</span>\ncd ./llama-index-integrations/readers\nllamaindex-cli <span class=\"hljs-keyword\">new</span>-pacakge --kind <span class=\"hljs-string\">\"readers\"</span> --name <span class=\"hljs-string\">\"new reader\"</span></span></pre>\n <p>\n  Take a look at our updated\n  <a href=\"https://docs.llamaindex.ai/en/latest/contributing/contributing.html#contribute-a-pack-reader-tool-or-dataset-formerly-from-llama-hub\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   contributing guide here\n  </a>\n  for more details.\n </p>\n <h1>\n  Migration to v0.10\n </h1>\n <p>\n  If you want to use LlamaIndex v0.10, you will need to do two main things:\n </p>\n <ol>\n  <li>\n   Adjust imports to fit the new package structure for core modules/integrations\n  </li>\n  <li>\n   Deprecate ServiceContext\n  </li>\n </ol>\n <p>\n  Luckily, we\u2019ve created a comprehensive migration guide that also contains a CLI tool to\n  <em class=\"qd\">\n   automatically\n  </em>\n  upgrade your existing code and notebooks to v0.10!\n </p>\n <p>\n  Just do\n </p>\n <pre><span class=\"rb pa gt ot b bf rc rd l re rf\" id=\"a5f8\">llamaindex-cli upgrade &lt;source-dir&gt;</span></pre>\n <p>\n  <a href=\"https://pretty-sodium-5e0.notion.site/v0-10-0-Migration-Guide-6ede431dcb8841b09ea171e7f133bd77\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Check out the full migration guide here.\n  </a>\n </p>\n <h1>\n  Next Steps\n </h1>\n <p>\n  We\u2019ve painstakingly revamped all of our README, documentation and notebooks to reflect these v0.10 changes. Check out the below section for a compiled list of all resources.\n </p>\n <h2>\n  Documentation\n </h2>\n <p>\n  <a href=\"https://docs.llamaindex.ai/en/stable/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   v0.10 Documentation\n  </a>\n </p>\n <p>\n  <a href=\"https://docs.llamaindex.ai/en/stable/getting_started/installation.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   v0.10 Installation Guide\n  </a>\n </p>\n <p>\n  <a href=\"https://docs.llamaindex.ai/en/stable/getting_started/starter_example.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   v0.10 Quickstart\n  </a>\n </p>\n <p>\n  <a href=\"https://docs.llamaindex.ai/en/stable/contributing/contributing.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Updated Contribution Guide\n  </a>\n </p>\n <p>\n  Temporary\n  <a href=\"https://pretty-sodium-5e0.notion.site/ce81b247649a44e4b6b35dfb24af28a6?v=53b3c2ced7bb4c9996b81b83c9f01139\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   v0.10 Package Registry\n  </a>\n </p>\n <p>\n  <a href=\"https://pretty-sodium-5e0.notion.site/v0-10-0-Migration-Guide-6ede431dcb8841b09ea171e7f133bd77\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   v0.10 Migration Guide\n  </a>\n </p>\n <h2>\n  Repo\n </h2>\n <p>\n  <a href=\"https://github.com/run-llama/llama_index/tree/main\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Repo README\n  </a>\n </p>\n <p>\n  <a href=\"https://github.com/run-llama/llama_index/tree/main/llama-index-integrations\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   llama-index-integrations\n  </a>\n </p>\n <p>\n  <a href=\"https://github.com/run-llama/llama_index/tree/main/llama-index-packs\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   llama-index-packs\n  </a>\n </p>\n <h2>\n  Example Notebooks\n </h2>\n <p>\n  These are mostly to show our updated import syntax.\n </p>\n <ul>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/query_engine/sub_question_query_engine.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Sub-Question Query Engine\n   </a>\n   (primarily uses core)\n  </li>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/vector_stores/WeaviateIndexDemo.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Weaviate Vector Store Demo\n   </a>\n  </li>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    OpenAI Agent over RAG Pipelines\n   </a>\n  </li>\n </ul>\n <h1>\n  Bug reports\n </h1>\n <p>\n  We\u2019ll be actively monitoring our\n  <a href=\"https://github.com/run-llama/llama_index/issues\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Github Issues\n  </a>\n  and\n  <a href=\"https://discord.gg/dGcwcsnxhU\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Discord\n  </a>\n  . If you run into any issues don\u2019t hesitate to hop into either of these channels!\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 26523, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca5cd227-31a4-4889-a697-58fde080ff56": {"__data__": {"id_": "ca5cd227-31a4-4889-a697-58fde080ff56", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-vectara-7a3889cd34cb.html", "file_name": "llamaindex-vectara-7a3889cd34cb.html", "file_type": "text/html", "file_size": 13681, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-vectara-7a3889cd34cb.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-vectara-7a3889cd34cb.html", "file_name": "llamaindex-vectara-7a3889cd34cb.html", "file_type": "text/html", "file_size": 13681, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "b5103e80f3ccfadd408ad7f989535272e9fb24f71b26d9fb2cf3fd3b7bc00b48", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  (co-authored by Ofer Mendelevitch, head of Developer Relations at Vectara, and Logan Markewich, founding engineer at LlamaIndex)\n </p>\n <h1>\n  Introduction\n </h1>\n <p>\n  <a href=\"https://vectara.com\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Vectara\n  </a>\n  is a trusted GenAI platform. Exposing a set of easy to use\n  <a href=\"https://docs.vectara.com/docs/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   APIs\n  </a>\n  , Vectara\u2019s platform reduces the complexity involved in developing\n  <a href=\"https://vectara.com/grounded-generation-making-generative-ai-safe-trustworthy-more-relevant/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Grounded Generation\n  </a>\n  (aka retrieval-augmented-generation) applications, and managing the LLM infrastructure that\u2019s required to deploy them at scale in production.\n </p>\n <p>\n  Today we\u2019re happy to announce Vectara\u2019s integration with\n  <a href=\"https://github.com/jerryjliu/llama_index\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaIndex\n  </a>\n  via a new type of Index: the\n  <em class=\"pp\">\n   Managed Index\n  </em>\n  . In this blog post, we\u2019ll dig deeper into how a\n  <a href=\"https://gpt-index.readthedocs.io/en/stable/community/integrations/managed_indices.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   ManagedIndex\n  </a>\n  works, and show examples of using Vectara as a Managed Index.\n </p>\n <h1>\n  What is Vectara?\n </h1>\n <p>\n  Vectara is an end-to-end platform that offers powerful generative AI capabilities for developers, including:\n </p>\n <p>\n  <strong>\n   Data processing.\n  </strong>\n  Vectara supports various file types for ingestion including markdown, PDF, PPT, DOC, HTML and many others. At ingestion time, the text is automatically extracted from the files, and chunked into sentences. Then a vector embedding is computed for each chunk, so you don\u2019t need to call any additional service for that.\n </p>\n <p>\n  <strong>\n   Vector and text storage.\n  </strong>\n  Vectara hosts and manages the vector store (where the document embeddings are stored) as well as the associated text. Developers don\u2019t need to go through a long and expensive process of evaluation and choice of vector databases. Nor do they have to worry about setting up that Vector database, managing it in their production environment, re-indexing, and many other DevOps considerations that become important when you scale your application beyond a simple prototype.\n </p>\n <p>\n  <strong>\n   Query flow.\n  </strong>\n  When issuing a query, calculating the embedding vector for that query and retrieving the resulting text segments (based on similarity match) is fully managed by Vectara. Vectara also provides a robust implementation of hybrid search and re-ranking out of the box, which together with a state of the art embedding model ensures the most relevant text segments are returned in the retrieval step.\n </p>\n <p>\n  <strong>\n   Security and Privacy.\n  </strong>\n  Vectara\u2019s API is fully encrypted in transit and at rest, and supports customer-managed-keys (CMK). We never train on your data, so you can be sure your data is safe from privacy leaks.\n </p>\n <p>\n  <strong>\n   Figure 1:\n  </strong>\n  Vectara\u2019s API platform for \u201cGrounded Generation\u201d\n </p>\n <p>\n  The nice thing is that all this complexity is fully managed by Vectara, taking a lot of the heavy lifting off of the developer\u2019s shoulders, so that they don\u2019t have to specialize in the constantly evolving skills of large language models, embedding models, vector stores and MLOps.\n </p>\n <h1>\n  From VectorStoreIndex to ManagedIndex\n </h1>\n <p>\n  LlamaIndex is a data framework for building LLM applications. It provides a set of composable modules for users to define a data pipeline for their application. This consists of data loaders, text splitters, metadata extractors, and vector store integrations.\n </p>\n <p>\n  A popular abstraction that users use is the VectorStoreIndex, providing integrations with different vector databases.\n </p>\n <p>\n  However, a challenge here is that users still need to carefully define how to load data, parse it, as well as choose an embedding model and a vector DB to use. Since Vectara abstracts away this complexity, the Vectara and LlamaIndex teams jointly came up with a new abstraction: The ManagedIndex.\n </p>\n <p>\n  As shown in figure 2, when ingesting data into a VectorStoreIndex, data is processed locally taking advantage of multiple components like Data Connectors and Node parsers.\n </p>\n <p>\n  <strong>\n   Figure 2:\n  </strong>\n  typical flow of document processing in LlamaIndex for a VectorStoreIndex\n </p>\n <p>\n  With Vectara (figure 3), this whole flow is replaced by a single \u201cindexing\u201d API call , and all this processing is instead performed in the backend by the Vectara platform.\n </p>\n <p>\n  <strong>\n   Figure 3:\n  </strong>\n  pre-processing with the VectaraIndex simplifies the complex ingest flow to a single step.\n </p>\n <h1>\n  How does the VectaraIndex work?\n </h1>\n <p>\n  Let\u2019s take a look at a simple question-answering example using VectaraIndex, in this case asking questions from one of Paul Graham\u2019s Essays.\n </p>\n <p>\n  <strong>\n   Step 1: Setup your Vectara account and Index\n  </strong>\n </p>\n <p>\n  To get started, follow our\n  <a href=\"https://docs.vectara.com/docs/quickstart\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   quickstart\n  </a>\n  guide:\n  <a href=\"https://console.vectara.com/signup\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   signup\n  </a>\n  for a free Vectara account, create a corpus (index), and generate your API key.\n </p>\n <p>\n  Then setup your Vectara customer_id, corpus_id and api_key as environment variables, so that the VectaraIndex can access those easily, for example:\n </p>\n <pre><span class=\"px om gt pu b bf py pz l qa qb\" id=\"5d53\">VECTARA_CUSTOMER_ID=&lt;YOUR_CUSTOMER_ID&gt;\nVECTARA_CORPUS_ID=&lt;YOUR_CORPUS_ID&gt;\nVECTARA_API_KEY=\"zwt_RbZfGT\u2026\"</span></pre>\n <p>\n  <strong>\n   Step 2: Create a VectaraIndex instance with LlamaIndex\n  </strong>\n </p>\n <p>\n  Building the Vectara Index is extremely simple:\n </p>\n <pre><span class=\"px om gt pu b bf py pz l qa qb\" id=\"c370\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> SimpleDirectoryReader\n<span class=\"hljs-keyword\">from</span> llama_index.indices <span class=\"hljs-keyword\">import</span> VectaraIndex\nFrom pprint Import pprint\n\ndocuments = SimpleDirectoryReader(<span class=\"hljs-string\">\"paul_graham\"</span>).load_data()\nindex = VectaraIndex.from_documents(documents)</span></pre>\n <p>\n  Here we load Paul Graham\u2019s Essay using LlamaIndex\u2019s SimpleDirectoryReader into a single document. The from_documents() constructor is then used to generate the VectaraIndex instance.\n </p>\n <p>\n  Unlike the common flow that uses LlamaIndex tools like data connectors, parsers and embedding models to process the input data, with VectaraIndex the documents are sent directly to Vectara via the\n  <a href=\"https://docs.vectara.com/docs/api-reference/indexing-apis/indexing\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Indexing API\n  </a>\n  . Vectara\u2019s platform then processes, chunks, encodes and stores the text and embeddings into a Vectara corpus, making it available instantly for querying.\n </p>\n <p>\n  <strong>\n   Step 3: Query\n  </strong>\n </p>\n <p>\n  After the data is fully ingested, you can take advantage of the rich set of query constructs built into LlamaIndex. For example let\u2019s use the index to retrieve the top-k most relevant nodes:\n </p>\n <pre><span class=\"px om gt pu b bf py pz l qa qb\" id=\"f186\">retriever = index.as_retriever(similarity_top_k=<span class=\"hljs-number\">7</span>)\n<span class=\"hljs-comment\"># docs should contain the 7 most relevant documents for the query</span>\ndocs = retriever.retrieve(\u201cWhat <span class=\"hljs-keyword\">is</span> the IBM <span class=\"hljs-number\">1401</span>?\u201d)\npprint(docs[<span class=\"hljs-number\">0</span>].node.text)</span></pre>\n <blockquote>\n  <p class=\"mz na pp nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gm bj\" id=\"78fd\">\n   (\u2018My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep. The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \u201cdata processing.\u201d This was in 9th grade, so I was 13 or 14. The school district\u2019s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it.\u2019)\n  </p>\n </blockquote>\n <p>\n  Here we printed out the top matching Node given the query \u201cwhat is the IBM 1401?\u201d This in turn results in a call to Vectara\u2019s\n  <a href=\"https://docs.vectara.com/docs/api-reference/search-apis/search\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Search API\n  </a>\n  that returns the top-k matching document segments.\n </p>\n <p>\n  Those are transformed into NodeWithScore objects and thus can be used as usual with the rest of the LlamaIndex querying tools. For example we can use LlamaIndex\u2019s query_engine() to convert the retrieved matching document segments (nodes) into a comprehensive response to our question:\n </p>\n <pre><span class=\"px om gt pu b bf py pz l qa qb\" id=\"48dc\"><span class=\"hljs-comment\"># Get an answer to the query based on the content of the essay</span>\nresponse = index.as_query_engine().query(<span class=\"hljs-string\">\"What can the 1401 do?\"</span>)\n<span class=\"hljs-built_in\">print</span>(response)</span></pre>\n <blockquote>\n  <p class=\"mz na pp nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gm bj\" id=\"ee96\">\n   The 1401 was used for \u201cdata processing\u201d and could load programs into memory and run them. It had a card reader, printer, CPU, disk drives, and used an early version of Fortran as the programming language. The only form of input to programs was data stored on punched cards.\n  </p>\n </blockquote>\n <h1>\n  Why Use VectaraIndex with LlamaIndex?\n </h1>\n <p>\n  By adding the concept of a \u201cManaged Index\u201d and the VectaraIndex to LlamaIndex, users can continue to take advantage of the tools and capabilities offered by the LlamaIndex library while integrating with a generative AI platform like Vectara.\n </p>\n <p>\n  Retrievers and Query Engines are just the tip of the iceberg. Using a managed index with Vectara, developers have full access to advanced utilities like routers, advanced query engines, data agents, chat engines, and more! Being able to retrieve context using Vectara empowers developers to build these complex applications using LlamaIndex components.\n </p>\n <p>\n  For example, in the following code we use the chat engine in LlamaIndex to quickly create a chat interaction using our VectaraIndex:\n </p>\n <pre><span class=\"px om gt pu b bf py pz l qa qb\" id=\"53b8\">chat = index.as_chat_engine(chat_mode='context')\nres = chat.chat(\"When did the author learn Lisp?\")\nprint(res.response)</span></pre>\n <blockquote>\n  <p class=\"mz na pp nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gm bj\" id=\"d442\">\n   \u201cThe author learned Lisp in college.\u201d\n  </p>\n </blockquote>\n <p>\n  A follow up question retains the chat history for context, as you might expect:\n </p>\n <pre><span class=\"px om gt pu b bf py pz l qa qb\" id=\"aaee\">chat.chat(\"and was it helpful for projects?\").response</span></pre>\n <blockquote>\n  <p class=\"mz na pp nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gm bj\" id=\"b180\">\n   \u201cYes, learning Lisp was helpful for the author\u2019s projects. They used Lisp in both Viaweb and Y Combinator, indicating its usefulness in their work.\u201d\n  </p>\n </blockquote>\n <pre><span class=\"px om gt pu b bf py pz l qa qb\" id=\"ef9d\">chat.chat(\"what was a distinctive characteristic of that programming language?\").response</span></pre>\n <blockquote>\n  <p class=\"mz na pp nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gm bj\" id=\"defa\">\n   \u201cA distinctive characteristic of Lisp is that its core is a language defined by writing an interpreter in itself. It was originally designed as a formal model of computation and an alternative to the Turing machine. This self-referential nature of Lisp sets it apart from other programming languages.\u201d\n  </p>\n </blockquote>\n <p>\n  For more information on how to use chat-engines, check out the\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/query_modules/chat_engines/usage_pattern.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   documentation\n  </a>\n  , and for more information on other query capabilities with LlamaIndex, check out the full documentation\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/index.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   here\n  </a>\n  .\n </p>\n <h1>\n  Summary\n </h1>\n <p>\n  LlamaIndex makes it super easy to populate VectaraIndex with content from any document or data source, while utilizing the Vectara service for managing the document processing, chunking, embedding and making all of this data available for advanced retrieval in query time using the LlamaIndex library.\n </p>\n <p>\n  VectaraIndex is based on the new LlamaIndex Managed Index abstraction, which better supports GenAI platforms like Vectara, and enables additional vendors who also provide end-to-end platforms to join in.\n </p>\n <p>\n  To get started with Vectara and LlamaIndex you can follow the Vectara quickstart\n  <a href=\"https://docs.vectara.com/docs/quickstart\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   guide\n  </a>\n  to setup your account, and the examples above with your own data.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 13592, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16f2c1f8-4c52-4e46-b267-afb162402d47": {"__data__": {"id_": "16f2c1f8-4c52-4e46-b267-afb162402d47", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.html", "file_name": "llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.html", "file_type": "text/html", "file_size": 15546, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.html", "file_name": "llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.html", "file_type": "text/html", "file_size": 15546, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "126ef641399332f32bdc250e3d248cf25e186b6f4373ce1c42bc0ef42acbc5f2", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h1>\n  Introduction\n </h1>\n <p>\n  In many enterprises, data primarily resides in databases and generally, it\u2019s very difficult to combine database data with other forms of data, such as PDFs, when trying to generate actionable insights.\n </p>\n <p>\n  We envision the development of an agent that empowers anyone to leverage data from all of these data sources for informed decision-making. Imagine an agent proficient in creating documents by merging data from diverse sources, including JIRA and databases, further enriched with the latest internet-sourced information.\n </p>\n <p>\n  At\n  <a href=\"http://waii.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   waii.ai\n  </a>\n  , we are committed to delivering an enterprise text-to-SQL API with the most complete and accurate translation of plain English to SQL available. Waii allows companies to build text-to-SQL right into their products as well as enable no-code analytics for their internal data/business teams. Waii works out of the box and can be self-hosted/on-prem.\n </p>\n <p>\n  LlamaIndex introduces a remarkable RAG framework, facilitating the connection of various customer data sources, such as PDFs, Notion, and internal knowledge bases, to large language models (LLMs). This advancement simplifies the creation of data-augmented chatbots and analysis agents.\n </p>\n <p>\n  This opens up a prime opportunity to develop an enterprise agent that can access data from multiple sources, including your preferred database. We will explore this further in the rest of the blog.\n </p>\n <h1>\n  Why a New Text-to-SQL LlamaIndex Plugin?\n </h1>\n <p>\n  To enable the Llama Index agent to utilize text-to-SQL APIs, a plugin is essential. LlamaIndex already has a built-in text-to-SQL plugin, but why did we decide to create a new LlamaHub plugin?\n </p>\n <p>\n  The existing text-to-SQL plugin in LlamaIndex has been suitable for handling simple databases (less than 10 tables, 100 columns) with straightforward SQL queries. However, managing medium to large databases, which can include 100s of tables and 1000s of columns, presents a complex challenge. Limitations arise due to the restricted context windows of LLMs, and even those with large context windows, like GPT-4-turbo with its 128K tokens, can suffer from inaccuracies and regression in task retrieval when overloaded with content. This issue is discussed in a\n  <a href=\"https://x.com/jerryjliu0/status/1722657583331491860?s=20\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaIndex study\n  </a>\n  .\n </p>\n <p>\n  In contrast, Waii focuses on making query generation more efficient. We have developed a built-in compiler to deal with compilation errors from LLMs to support multiple dialects. Our internal knowledge graph, created from database metadata, constraints, and query history, aids in table/schema selection. Users can also apply semantic rules to schema/table/column, or integrate with their data catalog services, ensuring the semantic correctness of generated queries, in addition to syntactic correctness.\n </p>\n <p>\n  To utilize our service, users simply need to connect their database to Waii and copy a Waii API key to create a LlamaIndex agent.\n </p>\n <h1>\n  LlamaIndex + Waii Agent\n </h1>\n <p>\n  We are thrilled to showcase the integration of Waii with LlamaIndex to create an agent capable of executing various text-to-SQL tasks and validating the data based on a PDF.\n </p>\n <p>\n  We\u2019ll be analyzing customers\u2019 top-purchased categories during Christmas time, and compare it with\n  <a href=\"https://www2.deloitte.com/content/dam/insights/articles/us176694_cic_holiday-retail-survey/DI_2023-Deloitte-holiday-retail-survey.pdf\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Deloitte\u2019s holiday retail survey\n  </a>\n  report.\n </p>\n <h1>\n  Architecture of LlamaIndex + Waii\n </h1>\n <p>\n  Before diving into the code example, let\u2019s look at the architecture first:\n </p>\n <p>\n  The LlamaIndex agent operates on the client side, accompanied by a number of\n  tools: Each tool provides function specifications and allows functions to be\n  selected based on context and the user\u2019s input to\n  <code>\n   <strong>\n    chat(\"\u2026\")\n   </strong>\n  </code>\n  . For example, if the question indicates information needs to be retrieved\n  from the \u201cinternet\u201d, the Google search tool will be chosen. Internally it uses\n  LLM which returns selected functions with parameters for a given context.\n </p>\n <p>\n  When the Waii tool is chosen, whether for describing a dataset, generating a\n  query, or running a query, it sends the API request to the Waii Service.\n </p>\n <p>\n  The Waii Service can be deployed as a hosted SaaS or as Docker containers\n  running in your on-premises environment. The components of the Waii Service\n  include:\n </p>\n <ul>\n  <li>\n   <strong>\n    The Query Generator:\n   </strong>\n   coordinates the entire workflow of\n    query generation and communicates with the LLM for this purpose.\n  </li>\n  <li>\n   <strong>\n    Knowledge Graph / Metadata Management\n   </strong>\n   : connects to\n    databases, extracting metadata and query history as a knowledge graph to\n    assist the Query Generator in choosing the right tables and schemas.\n  </li>\n  <li>\n   <strong>\n    Semantic Rules\n   </strong>\n   : These aid the Query Generator in producing\n    semantically correct queries.\n  </li>\n  <li>\n   <strong>\n    Waii Compiler\n   </strong>\n   : After a query is generated by the LLM, the\n    Waii Compiler patches identified issues in the query. If a compilation issue\n    is not fixable, it regenerates the query with an articulated error message.\n  </li>\n </ul>\n <h1>\n  Create LlamaIndex agent with Waii + PDF Loader\n </h1>\n <p>\n  Let\u2019s first create two LlamaHub tools \u2014 Waii and PDF Loader. LlamaHub tools\n  include specs to identify available functions along with their parameters, the\n  agent will select and execute which function to use based on available\n  functions and context.\n </p>\n <p>\n  Let\u2019s start with creating an agent which includes the Waii tool:\n </p>\n <pre><span>from llama_hub.tools.google_search import GoogleSearchToolSpecfrom llama_hub.tools.waii import WaiiToolSpecfrom llama_index.agent import OpenAIAgentfrom llama_index.llms import OpenAIwaii_tool = WaiiToolSpec(    api_key='waii_api_key',    # Connection key of WAII connected database, see     # https://github.com/waii-ai/waii-sdk-py#get-connections    database_key='database_to_use',    verbose=True)</span></pre>\n <p>\n  And then create a PDF tool:\n </p>\n <pre><span>from pathlib import Pathfrom llama_index import download_loaderfrom llama_index import VectorStoreIndexPDFReader = download_loader(\"PDFReader\")loader = PDFReader()documents = loader.load_data(file=Path('DI_2023-Deloitte-holiday-retail-survey.pdf'))index = VectorStoreIndex.from_documents(documents)engine = index.as_query_engine(similarity_top_k=5)deloitte_retail_survey_tool = QueryEngineTool(        query_engine=engine,        metadata=ToolMetadata(            name=\"deloitte_retail_survey\",            description=(                \"Provides retail survey report for holiday sales based on Deloitte's data\"                \"Use a detailed plain text question as input to the tool, and output using plain text based on pdf data\"            ),        ),    )</span></pre>\n <p>\n  And at last, create an agent which combines Waii and PDF tools:\n </p>\n <pre><span>agent = OpenAIAgent.from_tools(  [deloitte_retail_survey_tool] + waii_tool.to_tool_list(),   llm=OpenAI(model='gpt-4-1106-preview', temperature=0),   verbose=True)</span></pre>\n <p>\n  <code>\n   [deloitte_retail_survey_tool] + waii_tool.to_tool_list()\n  </code>\n  indicate using all functions (such as getting answers from the database,\n  generating a query, executing a query, describing datasets, etc.) provided by\n  Waii and PDF Search.\n </p>\n <h1>\n  Understand your dataset\n </h1>\n <p>\n  The first step in doing data analysis is to get a better understanding of your\n  dataset.\n </p>\n <p>\n  You can start asking questions to your agent:\n </p>\n <pre><span>agent.chat(\"Describe my database\")</span></pre>\n <p>\n  The output of which is:\n </p>\n <pre><span>STARTING TURN 1---------------=== Calling Function ===Calling function: describe_dataset with args:   {\"ask\":\"Can you describe the whole database?\"}...========================STARTING TURN 2---------------The database I have access to consists of several schemas, each with its own set of tables and domains:1. **CRUNCHBASE_2016**: This schema includes tables related to acquisitions,    companies, investments, and funding rounds in the Crunchbase database for   the year 2016. It's useful for analyzing acquisition trends, company    information, investment activity, and funding trends.2. **RETAIL_DATA**: This schema contains tables related to retail operations,    such as call centers, customers, addresses, demographics, inventory,    items, promotions, stores, returns, sales, and warehouses. It can be used    to analyze call center performance, customer demographics, inventory    management, sales performance, and other retail operations.3. ...</span></pre>\n <p>\n  As you can see, the agent understood the request, called\n  <code>\n   describe_dataset\n  </code>\n  function provided by Waii, and generated a\n  summary of the dataset.\n </p>\n <p>\n  I\u2019m interested in the\n  <code>\n   RETAIL_DATA\n  </code>\n  schema, so let me ask more of the schema -\n </p>\n <pre><span>agent.chat(\"What can i do with the retail_data schema\")</span></pre>\n <p>\n  And I get this:\n </p>\n <pre><span>The RETAIL_DATA schema in the TWEAKIT_PLAYGROUND database is designed to support a wide range of analyses related to retail operations. Here are some of the capabilities and types of analyses you can perform with this schema:1. **Call Center Analysis**: Evaluate the performance of call centers,    understand call volumes, and assess customer service efficiency.2. **Customer Demographics**: Analyze customer profiles, including    demographics, purchasing behaviors, and preferences. This can help    in targeted marketing and customer segmentation.Specific questions that can be addressed using the RETAIL_DATA schema include:- What is the total number of call centers?- How many customers have a preferred customer flag?- What is the average price of items?</span></pre>\n <p>\n  Let me do some more data analysis.\n </p>\n <h1>\n  Generate an SQL query and run it\n </h1>\n <p>\n  Let\u2019s generate a SQL query (asking top 10 item categories sold during\n  Christmas time):\n </p>\n <pre><span>agent.chat(\"Top 10 item category sold during christmas time across all years\")</span></pre>\n <p>\n  Now it calls\n  <code>\n   get_answer\n  </code>\n  function from Waii tool:\n </p>\n <pre><span>=== Calling Function ===Calling function: get_answer with args:   {\"ask\":\"What are the top 10 item categories sold during           Christmas time across all years?\"}</span></pre>\n <p>\n  And got a SQL like:\n </p>\n <pre><span>WITH christmas_sales AS (    SELECT        ss_item_sk,        SUM(ss_quantity) AS total_quantity    FROM tweakit_playground.retail_data.store_sales AS ss    INNER JOIN tweakit_playground.retail_data.date_dim AS dd        ON ss_sold_date_sk = d_date_sk    WHERE        d_holiday = 'Y' AND d_date ILIKE '%-12-25'    GROUP BY        ss_item_sk),ranked_categories AS (    SELECT        i_category,        SUM(total_quantity) AS category_quantity    FROM christmas_sales AS cs    INNER JOIN tweakit_playground.retail_data.item AS i        ON ss_item_sk = i_item_sk    GROUP BY        i_category    ORDER BY        category_quantity DESC    LIMIT 10)SELECT    i_category AS category,    category_quantityFROM ranked_categories</span></pre>\n <p>\n  Let me ask it to run the query:\n </p>\n <pre><span>agent.chat(\"Run it\")</span></pre>\n <p>\n  I got a Dataframe along with its summary:\n </p>\n <pre><span>The top 10 item categories sold during Christmas time across all years, based on the total quantity of items, are:1. Women: 1,487,8912. Sports: 1,486,6443. Children: 1,483,5314. Electronics: 1,478,4635. Music: 1,476,0456. Books: 1,472,9117. Home: 1,471,3488. Jewelry: 1,459,0259. Shoes: 1,456,29610. Men: 1,451,285</span></pre>\n <h1>\n  Use with a PDF report\n </h1>\n <p>\n  Let\u2019s try to cross-check the result with the\n  <a href=\"https://www2.deloitte.com/content/dam/insights/articles/us176694_cic_holiday-retail-survey/DI_2023-Deloitte-holiday-retail-survey.pdf\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   holiday retailer report (in PDF form) from Deloitte\n  </a>\n  \u2014 Are Deloitte\u2019s top-purchased gift categories the same as what we saw from\n  the database?\n </p>\n <figure>\n  <figcaption>\n   Top gift categories during the holiday\n  </figcaption>\n </figure>\n <p>\n  There\u2019s one page from the pdf we are interested in \u2014 let\u2019s see if we can\n  retrieve this page\n </p>\n <pre><span>agent.chat(\"\"\"Give me top holiday gift categories based on Deloitte's holiday retail survey, and how it is different from the top 10 item category sold during christmas time across all years from my database\"\"\")</span></pre>\n <p>\n  The output looks like the following, which indicates it uses the\n  <code>\n   deloitte_retail_survey\n  </code>\n  tool to obtain insights from the PDF.\n </p>\n <pre><span>STARTING TURN 1---------------=== Calling Function ===Calling function: deloitte_retail_survey with args: {\"input\": \"What are the top holiday gift categories based on Deloitte's holiday retail survey?\"}...</span></pre>\n <p>\n  It gives the following summary:\n </p>\n <pre><span>Based on Deloitte's holiday retail survey, the top holiday gift categories are:1. Clothing &amp; Accessories2. Gift Cards &amp; Other3. Food &amp; Beverage...From your database, the top 10 item categories sold during Christmas time across all years are:1. Women2. Sports3. Children...Comparing the two lists, we can see some differences and similarities:- \"Clothing &amp; Accessories\" from Deloitte's survey could correspond to   \"Women,\" \"Men,\" and possibly \"Children\" from your database.- \"Electronics &amp; Accessories\" is a common category in both lists.- \"Gift Cards &amp; Other\" and \"Food &amp; Beverage\" from Deloitte's survey do   not have a direct match in the top categories from your database....</span></pre>\n <p>\n  Bingo! Now we can compare the results from our database with PDFs. And I love\n  seeing how the agent can correlate the two lists and tell me that my store\n  doesn\u2019t have the \u201cGift Cards &amp; Other\u201d and \u201cFood &amp; Beverage\u201d\n  categories!\n </p>\n <p>\n  You can find the code from the Colab notebook\n  <a href=\"https://colab.research.google.com/drive/1hL_Ztb1DRcdyeO2JZwsc9osW0Y3chP9c#scrollTo=fvye9sqAcn5j\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   link\n  </a>\n </p>\n <h1>\n  Wrapping up\n </h1>\n <p>\n  The integration of Waii\u2019s text-to-SQL API with LlamaIndex\u2019s RAG framework\n  marks a significant advancement in enterprise data analytics. This powerful\n  combination enables companies to effortlessly merge and analyze data from\n  various sources, including databases, PDFs, and the Internet. We demonstrated\n  the agent\u2019s capability to generate SQL queries, understand complex datasets,\n  and correlate findings with external reports. This innovation not only\n  simplifies data analysis but also opens new avenues for informed\n  decision-making in the digital era.\n </p>\n <p>\n  To learn more about Waii, please contact us here:\n  <a href=\"https://www.waii.ai/#request-demo\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://www.waii.ai/#request-demo\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 15489, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "53468c8b-eb55-4386-8249-8e2dc11ff5a0": {"__data__": {"id_": "53468c8b-eb55-4386-8249-8e2dc11ff5a0", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.html", "file_name": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.html", "file_type": "text/html", "file_size": 14272, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.html", "file_name": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.html", "file_type": "text/html", "file_size": 14272, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "e0676ba93dd5e5fabef3b60e1100ee5778499da1eadba4bc6a08b3646ff38e86", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  <span class=\"l nz oa ob bn oc od oe of og fi\">\n   In\n  </span>\n  <strong>\n   <em class=\"oh\">\n    the RAG, after the retrieval phase, it\u2019s necessary to perform Re-ranking + Fine-Grained Prompt Compression + Subsequence Recovery to enhance LLM\u2019s perception of key information, which is LongLLMLingua.\n   </em>\n  </strong>\n </p>\n <blockquote>\n  <p class=\"na nb oh nc b nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx gm bj\" id=\"c32a\">\n   TL;DR: While Retrieval-Augmented Generation (RAG) is highly effective in various scenarios, it still has drawbacks such as 1) Performance drop, like the \u201cLost in the middle\u201d issue, 2) High costs, both financially and in terms of latency, and 3) Context windows limitation. LongLLMLingua offers a solution to these problems in RAG or Long Context scenarios via prompt compression. It can boost accuracy by as much as 21.4% while only using \u00bc of the tokens. In long context situations, it can save $28 for every 1000 examples.\n  </p>\n </blockquote>\n <p>\n  See real-world cases on the\n  <a href=\"https://llmlingua.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    project page\n   </strong>\n  </a>\n  .\n </p>\n <p>\n  We previously wrote a\n  <a href=\"https://wyydsb.xin/NLP/LLMLingua_en.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   blog post\n  </a>\n  introducing the design of\n  <strong>\n   LLMLingua\n  </strong>\n  , which started from the perspective of\n  <strong>\n   designing a special language for LLMs\n  </strong>\n  . This time, our focus will be on the scenarios involving RAG.\n </p>\n <p>\n  Retrieval-Augmented Generation is currently the most reliable and proven technique for creating AI-agents that are grounded on any specific collection of text. Frameworks like\n  <strong>\n   LlamaIndex\n  </strong>\n  provide comprehensive RAG solutions to help users utilize specialized data in LLMs more conveniently.\n </p>\n <p>\n  A common misunderstanding is that retrieving as many relevant documents as possible during the RAG process and stitching them together to form a long retrieved prompt is beneficial, especially as more and more LLMs support longer context windows. However, this method can introduce more\n  <strong>\n   noise\n  </strong>\n  into the prompt and\n  <strong>\n   weaken\n  </strong>\n  <strong>\n   the LLM\u2019s perception of key information\n  </strong>\n  , leading to issues such as \u2018\n  <strong>\n   lost in the middle\n  </strong>\n  \u2019[1].\n </p>\n <p>\n  These issues become more apparent in real-world scenarios involving RAG. Better retrieval mechanisms can introduce higher quality noise documents, which can more easily lead to a drop in performance.\n </p>\n <h1>\n  Re-ranking is an intuitive concept.\n </h1>\n <p>\n  One intuitive idea is to reposition the most relevant information to the sides of the prompt through re-ranking. This concept of re-ranking has already been implemented in frameworks such as\n  <a href=\"https://github.com/run-llama/llama_index/blob/main/llama_index/indices/postprocessor/sbert_rerank.py\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    LlamaIndex\n   </strong>\n  </a>\n  and\n  <a href=\"https://python.langchain.com/docs/modules/data_connection/document_transformers/post_retrieval/long_context_reorder\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    LangChain\n   </strong>\n  </a>\n  .\n </p>\n <p>\n  However, according to our experiments, it\u2019s difficult for an embedding model to serve as a \u2018good\u2019 re-ranker. The underlying reason is the lack of an interaction process between the query and the document. The dual-tower structure of embeddings is not suitable for re-ranking in general scenarios, although it may be effective after fine-tuning.\n </p>\n <p>\n  Using LLMs directly as a re-ranker may also lead to misjudgments due to\n  <strong>\n   hallucinations\n  </strong>\n  . Recently, some re-ranking models have been extended from embedding models, such as\n  <a href=\"https://huggingface.co/BAAI/bge-reranker-large\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    bge-rerank\n   </strong>\n  </a>\n  . However, such re-ranking models generally have context window limitations.\n </p>\n <p>\n  To address the above issues, we propose a Question-aware Coarse-Grained prompt compression method. This method evaluates the relevance between the context and the question based on the perplexity corresponding to the question.\n </p>\n <p>\n  To mitigate the hallucination problem in smaller LLMs, we append a restrictive statement, specifically \u201c\n  <em class=\"oh\">\n   We can get the answer to this question in the given documents\n  </em>\n  \u201d, after the question to limit the latent space caused by related hallucinations.\n </p>\n <figure>\n  <figcaption class=\"qf fe qg oi oj qh qi be b bf z dt\">\n   <strong>\n    Figure 1.\n   </strong>\n   The accuracy of different methods for ranking documents from Multi-documemnt QA dataset, which increases from top to bottom in terms of Recall@1. Different colors represent different types of methods. Among them, yellow represents traditional relevance methods, green signifies embedding-based methods, and red denotes rerank-based methods. You can find the script in\n   <a href=\"https://github.com/microsoft/LLMLingua/blob/main/examples/Retrieval.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    this link\n   </a>\n   .\n  </figcaption>\n </figure>\n <p>\n  Results show that this approach significantly outperforms both embedding models and re-ranking models. We\u2019ve added some recently released embedding and reranking models. As you can see, the performance of\n  <strong>\n   bge-rerank-large\n  </strong>\n  is very close to that of\n  <strong>\n   LongLLMLingua\n  </strong>\n  . Reranking models generally perform better than embedding models. Currently,\n  <strong>\n   Jina\n  </strong>\n  is the best performing method among the embedding models.\n </p>\n <h1>\n  Compress unrelated and unimportant information\n </h1>\n <p>\n  Besides recalling as many relevant documents as possible, another approach is to\n  <strong>\n   compress\n  </strong>\n  irrelevant or unimportant contexts as much as possible.\n </p>\n <p>\n  Previous work on long context has focused on how to extend LLMs to support longer context windows. However, almost no work has explored whether this can actually improve the performance of downstream tasks. Some previous studies have shown that the presence of more noise in the prompt, as well as the position of key information in the prompt, can affect the performance of LLMs.\n </p>\n <p>\n  From the perspective of prompt compression, Selective Context[2] and LLMLingua[3] estimate the importance of elements by using a small language model to calculate the mutual information or perplexity of the prompt. However, in scenarios like RAG or long context scenarios, this method can easily lose key information because it cannot perceive the question information.\n </p>\n <p>\n  In recent submissions to ICLR\u201924, there have been some similar practices. For example, Recomp[4] reduces the use of tokens in RAG scenarios by jointly training compressors of two different granularities. RAG in Long Context[5] decomposes the long context into a series of chunks and uses retrieval methods for compression, which is actually the retrieval-based method implemented in the LongLLMLingua paper. In addition, Walking Down the Memory Maze[6] also designed a hierarchical summarization tree to enhance the LLM\u2019s perception of key information.\n </p>\n <h1>\n  Question-aware Fine-grained Prompt Compression\n </h1>\n <p>\n  In order to make token-level prompt compression also perceive the information of the question, we propose a\n  <strong>\n   contrastive perplexity\n  </strong>\n  , which compares the difference between the perplexity distribution corresponding to the document and the perplexity distribution corresponding to the document with the question.\n </p>\n <p>\n  An intuitive feeling is that when the question serves as context, the perplexity corresponding to the relevant tokens in the document will decrease. This decrease in magnitude represents the importance of the tokens in the document relative to the question.\n </p>\n <figure>\n  <figcaption class=\"qf fe qg oi oj qh qi be b bf z dt\">\n   <strong>\n    Figure 3.\n   </strong>\n   Comparison between perplexities and contrastive perplexities of tokens in the prompt from Multi-documemnt QA dataset. The document with the ground truth is located on the left side of the dashed line.\n  </figcaption>\n </figure>\n <p>\n  Figure 3 shows the distribution difference in extracting key tokens between perplexity and contrastive perplexity.\n </p>\n <h1>\n  How to reduce the loss in the middle\n </h1>\n <p>\n  Since Coarse-grained Prompt compression far exceeds other retrieval methods in terms of accuracy, it is a very natural idea to use this ranking information to rearrange the documents that are more related to the question to the beginning and end of the prompt. However, through our testing, we found that rearranging to the beginning of the prompt is more effective than evenly distributing at both ends. So, we choose to reorder the most related document to the beginning of the prompt.\n </p>\n <h1>\n  How to achieve adaptive granular control during compression?\n </h1>\n <p>\n  In order to better use the information from the two grained compressions, in the fine-grained prompt compression, we dynamically allocate different compression ratios to different documents based on the rank information obtained from the coarse-grained compression, thereby preserving more important information from important documents.\n </p>\n <h1>\n  How to improve the integrity of key information?\n </h1>\n <p>\n  Since LongLLMLingua is a token-level prompt compression, it will inevitably delete some tokens of the word, which may result in some retrieval-related tasks not getting complete results. But this can actually be recovered through a simple subsequence matching method. Specifically, there is a subsequence relationship between the original prompt, compressed prompt and response. By establishing the mapping relationship between the response subsequence that appears in the compressed prompt and the subsequence of the original prompt, the original prompt content can be effectively recovered.\n </p>\n <h1>\n  Experiments\n </h1>\n <p>\n  To evaluate the effectiveness of LongLLMLingua, we conducted detailed tests in Multi-document QA (RAG) and two long Context benchmarks. Particularly, the dataset chosen for Multi-document QA is very close to the actual RAG scenario (e.g. Bing Chat), where\n  <strong>\n   Contriever\n  </strong>\n  (one of the state-of-the-art retrieval systems) is used to recall 20 relevant documents including one ground-truth. The original documents have a high semantic relevance with the question.\n </p>\n <p>\n  As can be seen, compared to Retrieval-based methods and compression-based methods, LongLLMLingua improves performance more in the RAG scenario, and can increase up to 21.4 points at a 4x compression rate, avoiding the original \u201clost in the middle\u201d situation.\n </p>\n <p>\n  The results of the two benchmarks, LongBench and ZeroScrolls, also reached similar conclusions. LongLLMLingua is better at retaining key information related to the question in long context scenarios.\n </p>\n <p>\n  Besides, LongLLMLingua is very efficient and can speed up the end-to-end inference process.\n </p>\n <h1>\n  Used in LlamaIndex\n </h1>\n <p>\n  Thank\n  <a href=\"https://medium.com/u/e76da1c45ef7?source=post_page-----54b559b9ddf7--------------------------------\" rel=\"noopener\" target=\"_blank\">\n   Jerry Liu\n  </a>\n  for your help with the LongLLMLingua project. Now you can use LongLLMLingua as a\n  <strong>\n   NodePostprocessor\n  </strong>\n  in this widely used RAG framework. For specific usage, you can refer to the\n  <a href=\"https://github.com/microsoft/LLMLingua/blob/main/examples/RAGLlamaIndex.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   example 1\n  </a>\n  ,\n  <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/node_postprocessor/LongLLMLingua.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   example 2\n  </a>\n  and the following code.\n </p>\n <pre><span class=\"qx pb gt qu b bf qy qz l ra rb\" id=\"aaaf\"><span class=\"hljs-keyword\">from</span> llama_index.query_engine <span class=\"hljs-keyword\">import</span> RetrieverQueryEngine\n<span class=\"hljs-keyword\">from</span> llama_index.response_synthesizers <span class=\"hljs-keyword\">import</span> CompactAndRefine\n<span class=\"hljs-keyword\">from</span> llama_index.indices.postprocessor <span class=\"hljs-keyword\">import</span> LongLLMLinguaPostprocessor\n\nnode_postprocessor = LongLLMLinguaPostprocessor(\n    instruction_str=<span class=\"hljs-string\">\"Given the context, please answer the final question\"</span>,\n    target_token=<span class=\"hljs-number\">300</span>,\n    rank_method=<span class=\"hljs-string\">\"longllmlingua\"</span>,\n    additional_compress_kwargs={\n        <span class=\"hljs-string\">\"condition_compare\"</span>: <span class=\"hljs-literal\">True</span>,\n        <span class=\"hljs-string\">\"condition_in_question\"</span>: <span class=\"hljs-string\">\"after\"</span>,\n        <span class=\"hljs-string\">\"context_budget\"</span>: <span class=\"hljs-string\">\"+100\"</span>,\n        <span class=\"hljs-string\">\"reorder_context\"</span>: <span class=\"hljs-string\">\"sort\"</span>,  <span class=\"hljs-comment\"># enable document reorder</span>\n        <span class=\"hljs-string\">\"dynamic_context_compression_ratio\"</span>: <span class=\"hljs-number\">0.4</span>, <span class=\"hljs-comment\"># enable dynamic compression ratio</span>\n    },\n)</span></pre>\n <h1>\n  References\n </h1>\n <p>\n  [1] Lost in the Middle: How Language Models Use Long Contexts. Nelson F. Liu etc.\n[2] Compressing Context to Enhance Inference Efficiency of Large Language Models. Yucheng Li etc.\n[3] LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models. Huiqiang Jiang, Qianhui Wu etc.\n[4] RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation. Fangyuan Xu etc.\n[5] Retrieval meets Long Context Large Language Models. Peng Xu etc.\n[6] Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading. Howard Chen etc.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 14236, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "46618cc7-3c24-4ae8-a484-5105fe1c8c62": {"__data__": {"id_": "46618cc7-3c24-4ae8-a484-5105fe1c8c62", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.html", "file_name": "mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.html", "file_type": "text/html", "file_size": 9736, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.html", "file_name": "mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.html", "file_type": "text/html", "file_size": 9736, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "3f0b841da5d461b2881fdca3c40804a9c1ab31e8f06c12f5e73e4edc40c7b12a", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Despite recent motivation to utilize NLP for wider range of real world\n  applications, most NLP papers, tasks and pipelines assume raw, clean texts.\n  However, many texts we encounter in the wild, including a vast majority of\n  legal documents (e.g., contracts and legal codes), are not so clean, with many\n  of them being visually structured documents (VSDs) such as PDFs. PDFs are\n  versatile, preserving the visual integrity of documents, but they often pose a\n  significant challenge when it comes to extracting and manipulating their\n  contents.\n </p>\n <p>\n  In this discussion, our focus will primarily be on text-only layered PDFs, a\n  category often regarded by many as a resolved issue.\n </p>\n <h1>\n  Complexity of Parsing PDFs\n </h1>\n <ol>\n  <li>\n   <strong>\n    Layout Complexity\n   </strong>\n   : PDFs can contain complex layouts, such\n    as multi-column text, tables, images, and intricate formatting. This layout\n    diversity complicates the extraction of structured data.\n  </li>\n  <li>\n   <strong>\n    Font encoding issue\n   </strong>\n   s: PDFs use a variety of font encoding\n    systems, and some of these systems do not map directly to Unicode. This can\n    make it difficult to extract the text accurately.\n  </li>\n  <li>\n   <strong>\n    Non-linear text storage:\n   </strong>\n   PDFs do not store text in the\n    order it appears on the page. Instead, they store text in objects that can\n    be placed anywhere on the page. This means that the order of the text in the\n    underlying code may not match the order of the text as it appears visually.\n  </li>\n  <li>\n   <strong>\n    Inconsistent use of spaces\n   </strong>\n   : In some PDFs, spaces are not\n    used consistently or are not used at all between words. This can make it\n    difficult to even identify word boundaries.\n  </li>\n </ol>\n <h1>\n  Do we need an efficient parser?\n </h1>\n <blockquote>\n  <p>\n   In the Age of LLMs, is an Efficient Parser Still Essential When LLMs Can\n    Process Entire PDFs?\n  </p>\n </blockquote>\n <p>\n  This question gains relevance if the answer to this next question is \u201cYes\u201d.\n </p>\n <blockquote>\n  <p>\n   Do we need Retrieval-Augmented Generation (RAG)?\n  </p>\n </blockquote>\n <p>\n  While LLMs are powerful, they have certain limitations in terms of the amount\n  of text they can process at once and the scope of information they can\n  reference. Further recent research have suggested LLM performance is often\n  highest when relevant information occurs at the beginning or end of the input\n  context, and significantly degrades when models must access relevant\n  information in the middle of long contexts. Techniques like RAG help overcome\n  these limitations, enabling more effective and efficient processing of large\n  documents and broader information retrieval.\n </p>\n <blockquote>\n  <p>\n   Still Skeptical? Let\u2019s ask an LLM for confirmation.\n  </p>\n </blockquote>\n <p>\n  Now that we\u2019ve established the importance of an efficient parser, it becomes\n  instrumental in constructing an effective Retrieval-Augmented Generation (RAG)\n  pipeline to address the limitations of an LLM. Let\u2019s explore how we are\n  achieving this today. It\u2019s crucial to remember that the quality of the context\n  fed to an LLM is the cornerstone of an effective RAG, as the saying goes,\n  \u2018\n  <strong>\n   <em>\n    Garbage In \u2014 Garbage Out\n   </em>\n  </strong>\n  .\u2019\n </p>\n <p>\n  In the context of building LLM-related applications,\n  <strong>\n   chunking\n  </strong>\n  is the process of breaking down large pieces of text\n  into smaller segments. It\u2019s an essential technique that helps optimize the\n  relevance of the content we get back from a database once we use the LLM to\n  embed content. Some of the strategies involved are\n </p>\n <ol>\n  <li>\n   <strong>\n    Fixed-size chunking\n   </strong>\n   . This is the most common and\n    straightforward approach to chunking: we simply decide the number of tokens\n    in our chunk and, optionally, whether there should be any overlap between\n    them. Easy to implement &amp; most commonly used, but never makes it to a\n    production setting because the output is satisfactory in a Proof of Concept\n    (POC) setup, but its accuracy degrades as we conduct further testing.\n  </li>\n  <li>\n   <strong>\n    \u201cContent-aware\u201d chunking\n   </strong>\n   . Set of methods for taking\n    advantage of the nature of the content we\u2019re chunking and applying more\n    sophisticated chunking to it. Challenging to implement due to the reasons\n    mentioned above, but if tackled correctly, it could be the most ideal\n    building block for a production-grade Information Retrieval (IR) engine.\n  </li>\n </ol>\n <h1>\n  Where\u2019s This Article Headed, Anyway?\n </h1>\n <p>\n  Certainly, let\u2019s put an end to the historical and background details, shall\n  we?\n </p>\n <p>\n  Introducing\n  <a href=\"https://github.com/nlmatics/llmsherpa#layoutpdfreader\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    LayoutPDFReader\n   </strong>\n  </a>\n  for \u201c\n  <em>\n   Context-aware\n  </em>\n  \u201d chunking.\n  <a href=\"https://github.com/nlmatics/llmsherpa\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LayoutPDFReader\n  </a>\n  can act as the most important tool in your RAG arsenal by parsing PDFs along\n  with hierarchical layout information such as:\n </p>\n <ol>\n  <li>\n   Identifying sections and subsections, along with their respective hierarchy\n    levels.\n  </li>\n  <li>\n   Merging lines into coherent paragraphs.\n  </li>\n  <li>\n   Establishing connections between sections and paragraphs.\n  </li>\n  <li>\n   Recognizing tables and associating them with their corresponding sections.\n  </li>\n  <li>\n   Handling lists and nested list structures with precision.\n  </li>\n </ol>\n <p>\n  The first step in using the\n  <a href=\"https://github.com/nlmatics/llmsherpa\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LayoutPDFReader\n  </a>\n  is to provide a URL or file path to it (assuming it\u2019s already been installed)\n  and get back a document object.\n </p>\n <pre><span>from llmsherpa.readers import LayoutPDFReaderllmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"pdf_url = \"https://arxiv.org/pdf/1910.13461.pdf\" # also allowed is a file path e.g. /home/downloads/xyz.pdfpdf_reader = LayoutPDFReader(llmsherpa_api_url)doc = pdf_reader.read_pdf(pdf_url)</span></pre>\n <h2>\n  Vector search and RAG with Smart Chunking\n </h2>\n <p>\n  LayoutPDFReader employs intelligent chunking to maintain the cohesion of\n  related text:\n </p>\n <ul>\n  <li>\n   It groups all list items together, along with the preceding paragraph.\n  </li>\n  <li>\n   Items within a table are chunked together.\n  </li>\n  <li>\n   It incorporates contextual information from section headers and nested\n    section headers.\n  </li>\n </ul>\n <p>\n  As a quick example, the following code snippet generates a\n  <a href=\"https://github.com/run-llama/llama_index\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaIndex\n  </a>\n  query engine from the document chunks produced by LayoutPDFReader.\n </p>\n <pre><span>from llama_index.readers.schema.base import Documentfrom llama_index import VectorStoreIndexindex = VectorStoreIndex([])for chunk in doc.chunks():    index.insert(Document(text=chunk.to_context_text(), extra_info={}))query_engine = index.as_query_engine()# Let's run one queryresponse = query_engine.query(\"list all the tasks that work with bart\")print(response)</span></pre>\n <p>\n  We get the following response:\n </p>\n <pre><span>BART works well for text generation, comprehension tasks, abstractive dialogue, question answering, and summarization tasks.</span></pre>\n <p>\n  Key Considerations:\n </p>\n <ol>\n  <li>\n   LLMSherpa leverages a cost-free and open API server. Your PDFs are not\n    retained beyond temporary storage during the parsing process.\n  </li>\n  <li>\n   LayoutPDFReader has undergone extensive testing with a diverse range of\n    PDFs. However, achieving flawless parsing for every PDF remains a\n    challenging task.\n  </li>\n  <li>\n   Please note that OCR (Optical Character Recognition) functionality is\n    presently unavailable. The tool exclusively supports PDFs equipped with a\n    text layer.\n  </li>\n  <li>\n   For inquiries regarding private hosting options, OCR support, or tailored\n    assistance with particular PDF-related concerns, feel free to reach out to\n   <a href=\"mailto:contact@nlmatics.com\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    contact@nlmatics.com\n   </a>\n   or to\n   <a href=\"mailto: kiran@nlmatics.com\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    me\n   </a>\n   directly.\n  </li>\n </ol>\n <p>\n  If you have any questions, please leave them in the comments section, and I\n  will try to respond ASAP.\n </p>\n <p>\n  <strong>\n   <em>\n    Connect?\n   </em>\n  </strong>\n </p>\n <p>\n  If you want to get in touch, feel free to shoot me a message on\n  <a href=\"https://www.linkedin.com/in/kirannpanicker/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LinkedIn\n  </a>\n  or via\n  <a href=\"mailto: kirankurup@gmail.com\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   email\n  </a>\n  .\n </p>\n <h1>\n  References\n </h1>\n <p>\n  <a href=\"https://github.com/nlmatics/llmsherpa#layoutpdfreader\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://github.com/nlmatics/llmsherpa\n  </a>\n </p>\n <p>\n  <a href=\"https://arxiv.org/abs/2105.00150\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Capturing Logical Structure of Visually Structured Documents with Multimodal Transition Parser\n  </a>\n </p>\n <p>\n  <a href=\"https://arxiv.org/abs/2307.03172\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Lost in the Middle: How Language Models Use Long Contexts\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 9699, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2934dde5-632a-4630-bd67-e76f5036cedd": {"__data__": {"id_": "2934dde5-632a-4630-bd67-e76f5036cedd", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/multi-modal-rag-621de7525fea.html", "file_name": "multi-modal-rag-621de7525fea.html", "file_type": "text/html", "file_size": 11704, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/multi-modal-rag-621de7525fea.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/multi-modal-rag-621de7525fea.html", "file_name": "multi-modal-rag-621de7525fea.html", "file_type": "text/html", "file_size": 11704, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "849b79b8e790c8c6c12214528629ca0335c51156fe39b87fcc5a15bf89ad2511", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  (co-authored by Haotian Zhang, Laurie Voss, and Jerry Liu @ LlamaIndex)\n </p>\n <h1>\n  Overview\n </h1>\n <p>\n  In this blog we\u2019re excited to present a fundamentally new paradigm: multi-modal Retrieval-Augmented Generation (RAG). We present new abstractions in LlamaIndex that now enable the following:\n </p>\n <ul>\n  <li>\n   Multi-modal LLMs and Embeddings\n  </li>\n  <li>\n   Multi-modal Indexing and Retrieval (integrates with vector dbs)\n  </li>\n </ul>\n <h1>\n  Multi-Modal RAG\n </h1>\n <p>\n  One of the most exciting announcements at OpenAI Dev Day was the release of the\n  <a href=\"https://platform.openai.com/docs/guides/vision\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   GPT-4V API\n  </a>\n  . GPT-4V is a\n  <em class=\"py\">\n   multi-modal\n  </em>\n  model that takes in both text/images, and can output text responses. It\u2019s the latest model in a recent series of advances around multi-modal models:\n  <a href=\"https://github.com/haotian-liu/LLaVA\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LLaVa\n  </a>\n  , and\n  <a href=\"https://www.adept.ai/blog/fuyu-8b\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Fuyu-8B\n  </a>\n  .\n </p>\n <p>\n  This extends the capabilities of LLMs in exciting new directions. In the past year, entire application stacks have emerged around the text-in/text-out paradigm. One of the most notable examples is Retrieval Augmented Generation (RAG) \u2014 combine an LLM with an external text corpus to reason over data that the model isn\u2019t trained on. One of the most significant impacts of RAG for end-users was how much it accelerated\n  <strong>\n   time-to-insight\n  </strong>\n  on unstructured text data. By processing an arbitrary document (PDF, web page), loading it into storage, and feeding it into the context window of an LLM, you could extract out any insights you wanted from it.\n </p>\n <p>\n  The introduction of GPT-4V API allows us to extend RAG concepts into the hybrid image/text domain, and unlock value from an even greater corpus of data (including images).\n </p>\n <p>\n  Think about all the steps in a standard RAG pipeline and how it can be extended to a multi-modal setting.\n </p>\n <ul>\n  <li>\n   <strong>\n    Input:\n   </strong>\n   The input can be text or images.\n  </li>\n  <li>\n   <strong>\n    Retrieval:\n   </strong>\n   The retrieved context can be text or images.\n  </li>\n  <li>\n   <strong>\n    Synthesis:\n   </strong>\n   The answer can be synthesized over both text and images.\n  </li>\n  <li>\n   <strong>\n    Response:\n   </strong>\n   The returned result can be text and/or images.\n  </li>\n </ul>\n <p>\n  This is just a small part of the overall space too. You can have chained/sequential calls that interleave between image and text reasoning, such as\n  <a href=\"https://twitter.com/jerryjliu0/status/1717205234269983030\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Retrieval Augmented Image Captioning\n  </a>\n  or multi-modal agent loops.\n </p>\n <h1>\n  Abstractions in LlamaIndex\n </h1>\n <p>\n  We\u2019re excited to present new abstractions in LlamaIndex that help make multi-modal RAG possible. For each abstraction, we explicitly note what we\u2019ve done so far and what\u2019s still to come.\n </p>\n <h2>\n  Multi-modal LLM\n </h2>\n <p>\n  We have direct support for GPT-4V via our\n  <code class=\"cw qo qp qq qr b\">\n   OpenAIMultiModal\n  </code>\n  class and support for open-source multi-modal models via our\n  <code class=\"cw qo qp qq qr b\">\n   ReplicateMultiModal\n  </code>\n  class (currently in beta, so that name might change). Our\n  <code class=\"cw qo qp qq qr b\">\n   SimpleDirectoryReader\n  </code>\n  has long been able to ingest audio, images and video, but now you can pass them directly to GPT-4V and ask questions about them, like this:\n </p>\n <pre><span class=\"qv on gt qr b bf qw qx l qy qz\" id=\"59b8\"><span class=\"hljs-keyword\">from</span> llama_index.multi_modal_llms <span class=\"hljs-keyword\">import</span> OpenAIMultiModal\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> SimpleDirectoryReader\n\nimage_documents = SimpleDirectoryReader(local_directory).load_data()\n\nopenai_mm_llm = OpenAIMultiModal(\n    model=<span class=\"hljs-string\">\"gpt-4-vision-preview\"</span>, api_key=OPENAI_API_TOKEN, max_new_tokens=<span class=\"hljs-number\">300</span>\n)\nresponse = openai_mm_llm.complete(\n    prompt=<span class=\"hljs-string\">\"what is in the image?\"</span>, image_documents=image_documents\n) </span></pre>\n <p>\n  This is a new base model abstraction. Unlike our default\n  <code class=\"cw qo qp qq qr b\">\n   LLM\n  </code>\n  class, which has standard completion/chat endpoints, the multi-modal model (\n  <code class=\"cw qo qp qq qr b\">\n   MultiModalLLM\n  </code>\n  ) can take in both image and text as input.\n </p>\n <p>\n  This also unifies the interface between both GPT-4V and open-source models.\n </p>\n <p>\n  <strong>\n   Resources\n  </strong>\n </p>\n <p>\n  We have initial implementations for both GPT-4V and vision models hosted on Replicate. We also have a docs page for multi-modal models:\n </p>\n <ul>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/latest/module_guides/models/multi_modal.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Multi-modal docs page\n   </a>\n  </li>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/multi_modal/openai_multi_modal.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    GPT-4V\n   </a>\n  </li>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/multi_modal/replicate_multi_modal.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Replicate\n   </a>\n  </li>\n </ul>\n <figure>\n  <figcaption class=\"rb fe rc ny nz rd re be b bf z dt\">\n   Displayed image and example output from GPT-4V given text query \u201cDescribe image as alternative text\u201d\n  </figcaption>\n </figure>\n <p>\n  <strong>\n   What\u2019s still to come:\n  </strong>\n </p>\n <ul>\n  <li>\n   More multi-modal LLM integrations\n  </li>\n  <li>\n   Chat endpoints\n  </li>\n  <li>\n   Streaming\n  </li>\n </ul>\n <h2>\n  Multi-Modal Embeddings\n </h2>\n <p>\n  We introduce a new\n  <code class=\"cw qo qp qq qr b\">\n   MultiModalEmbedding\n  </code>\n  base class that can embed both text and images. It contains all the methods as our existing embedding models (subclasses\n  <code class=\"cw qo qp qq qr b\">\n   BaseEmbedding\n  </code>\n  ) but also exposes\n  <code class=\"cw qo qp qq qr b\">\n   get_image_embedding\n  </code>\n  .\n </p>\n <p>\n  Our primary implementation here is\n  <code class=\"cw qo qp qq qr b\">\n   ClipEmbedding\n  </code>\n  with the CLIP model. See below for a guide on using this in action.\n </p>\n <p>\n  <strong>\n   What\u2019s still to come\n  </strong>\n </p>\n <ul>\n  <li>\n   More multi-modal embedding integrations\n  </li>\n </ul>\n <h2>\n  Multi-Modal Indexing and Retrieval\n </h2>\n <p>\n  We create a new index, a\n  <code class=\"cw qo qp qq qr b\">\n   MultiModalVectorIndex\n  </code>\n  that can index both text and images into underlying storage systems \u2014 specifically a vector database and docstore.\n </p>\n <p>\n  Unlike our existing (most popular) index, the\n  <code class=\"cw qo qp qq qr b\">\n   VectorStoreIndex\n  </code>\n  , this new index can store both text and image documents. Indexing text is unchanged \u2014 it is embedded using a text embedding model and stored in a vector database. Indexing images involves a separate process:\n </p>\n <ol>\n  <li>\n   Embed the image using CLIP\n  </li>\n  <li>\n   Represent the image node as a base64 encoding or path, and store it along with its embedding in a vector db (separate collection from text).\n  </li>\n </ol>\n <p>\n  We store images and text separately since we may want to use a text-only embedding model for text as opposed to CLIP embeddings (e.g. ada or sbert).\n </p>\n <p>\n  During retrieval-time, we do the following:\n </p>\n <ol>\n  <li>\n   Retrieve text via vector search on the text embeddings\n  </li>\n  <li>\n   Retrieve images via vector search on the image embeddings\n  </li>\n </ol>\n <p>\n  Both text and images are returned as Nodes in the result list. We can then synthesize over these results.\n </p>\n <p>\n  <strong>\n   What\u2019s still to Come\n  </strong>\n </p>\n <ul>\n  <li>\n   More native ways to store images in a vector store (beyond base64 encoding)\n  </li>\n  <li>\n   More flexible multi-modal retrieval abstractions (e.g. combining image retrieval with any text retrieval method)\n  </li>\n  <li>\n   Multi-modal response synthesis abstractions. Currently the way to deal with long text context is to do \u201ccreate-and-refine\u201d or \u201ctree-summarize\u201d over it. It\u2019s unclear what generic response synthesis over multiple images and text looks like.\n  </li>\n </ul>\n <h1>\n  Notebook Walkthrough\n </h1>\n <p>\n  Let\u2019s walk through a notebook example. Here we go over a use case of querying Tesla given screenshots of its website/vehicles, SEC fillings, and Wikipedia pages.\n </p>\n <p>\n  We load the documents as a mix of text docs and images:\n </p>\n <pre><span class=\"qv on gt qr b bf qw qx l qy qz\" id=\"73d9\">documents = SimpleDirectoryReader(\"./mixed_wiki/\").load_data()</span></pre>\n <p>\n  We then define two separate vector database collections in Qdrant: a collection for text docs, and a collection for images. We then define a\n  <code class=\"cw qo qp qq qr b\">\n   MultiModalVectorStoreIndex\n  </code>\n  .\n </p>\n <pre><span class=\"qv on gt qr b bf qw qx l qy qz\" id=\"b279\"># Create a local Qdrant vector store\nclient = qdrant_client.QdrantClient(path=\"qdrant_mm_db\")\n\ntext_store = QdrantVectorStore(\n    client=client, collection_name=\"text_collection\"\n)\nimage_store = QdrantVectorStore(\n    client=client, collection_name=\"image_collection\"\n)\nstorage_context = StorageContext.from_defaults(vector_store=text_store)\n\n# Create the MultiModal index\nindex = MultiModalVectorStoreIndex.from_documents(\n    documents, storage_context=storage_context, image_vector_store=image_store\n)</span></pre>\n <p>\n  We can then ask questions over our multi-modal corpus.\n </p>\n <h2>\n  Example 1: Retrieval Augmented Captioning\n </h2>\n <p>\n  Here we copy/paste an initial image caption as the input to get a retrieval-augmented output:\n </p>\n <pre><span class=\"qv on gt qr b bf qw qx l qy qz\" id=\"9261\">retriever_engine = index.as_retriever(\n    similarity_top_k=<span class=\"hljs-number\">3</span>, image_similarity_top_k=<span class=\"hljs-number\">3</span>\n)\n<span class=\"hljs-comment\"># retrieve more information from the GPT4V response</span>\nretrieval_results = retriever_engine.retrieve(query_str)</span></pre>\n <p>\n  The retrieved results contain both images and text:\n </p>\n <figure>\n  <figcaption class=\"rb fe rc ny nz rd re be b bf z dt\">\n   Retrieved Text/Image Results\n  </figcaption>\n </figure>\n <p>\n  We can feed this to GPT-4V to ask a followup question or synthesize a coherent response:\n </p>\n <figure>\n  <figcaption class=\"rb fe rc ny nz rd re be b bf z dt\">\n   Synthesized Result\n  </figcaption>\n </figure>\n <h2>\n  Example 2: Multi-Modal RAG Querying\n </h2>\n <p>\n  Here we ask a question and get a response from the entire multi-modal RAG pipeline. The\n  <code class=\"cw qo qp qq qr b\">\n   SimpleMultiModalQueryEngine\n  </code>\n  first retrieves the set of relevant images/text, and feeds the input to a vision model in order to synthesize a response.\n </p>\n <pre><span class=\"qv on gt qr b bf qw qx l qy qz\" id=\"ef13\"><span class=\"hljs-keyword\">from</span> llama_index.query_engine <span class=\"hljs-keyword\">import</span> SimpleMultiModalQueryEngine\n\nquery_engine = index.as_query_engine(\n    multi_modal_llm=openai_mm_llm,\n    text_qa_template=qa_tmpl\n)\n\nquery_str = <span class=\"hljs-string\">\"Tell me more about the Porsche\"</span>\nresponse = query_engine.query(query_str)</span></pre>\n <p>\n  The generated result + sources are shown below:\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 11663, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b6ae502-fa08-4814-a455-5bf4588d5787": {"__data__": {"id_": "5b6ae502-fa08-4814-a455-5bf4588d5787", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/multimodal-rag-building-ainimal-go-fecf8404ed97.html", "file_name": "multimodal-rag-building-ainimal-go-fecf8404ed97.html", "file_type": "text/html", "file_size": 16030, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/multimodal-rag-building-ainimal-go-fecf8404ed97.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/multimodal-rag-building-ainimal-go-fecf8404ed97.html", "file_name": "multimodal-rag-building-ainimal-go-fecf8404ed97.html", "file_type": "text/html", "file_size": 16030, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "45412031e5764515857c28ecd043f1b418f38f7542b71985f82e392e58450e82", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  In the current landscape where GPT-4 Vision (GPT-4V) use cases are everywhere, I wanted to explore an alternative approach: pairing deep learning vision models with large language models (LLMs). My latest project, \u2018AInimal Go!\u2019, is an attempt to showcase how a specialized vision model like ResNet18 can seamlessly integrate with an LLM, using LlamaIndex as the orchestration layer and Wikipedia articles as the knowledge base.\n </p>\n <h1>\n  Project Overview\n </h1>\n <p>\n  \u2018AInimal Go!\u2019 is an interactive app that allows users to either capture or upload images of animals. Upon uploading an image, the ResNet18 model swiftly classifies the animal. Following this, the Cohere LLM API, adeptly orchestrated by LlamaIndex, takes over. It roleplays as the identified animal, enabling users to engage in unique conversations about and with the animal. The dialogue is informed and enriched by a knowledge base of nearly 200 Wikipedia articles, providing accurate and relevant responses to user queries.\n </p>\n <h1>\n  Why Not GPT-4V?\n </h1>\n <p>\n  Amidst the surge in GPT-4 Vision use cases, I wanted to explore an efficient yet powerful alternative. It is important to choose the right tool for the job \u2014 using GPT-4V for every multimodal task can be overkill, like using a sledgehammer to crack a nut. My approach was to harness the agility and precision of ResNet18 for animal identification. This method not only\n  <strong>\n   curtails costs\n  </strong>\n  but also underscores the adaptability of specialized models in multi-modal realms.\n </p>\n <h1>\n  Tools and Tech\n </h1>\n <ul>\n  <li>\n   <strong>\n    ResNet for Animal Detection:\n   </strong>\n   A blazing-fast implementation to identify animals in images, utilizing the ImageNet classification scheme.\n  </li>\n  <li>\n   <strong>\n    Cohere LLM:\n   </strong>\n   For generating engaging, informative conversations based on the identified animal.\n  </li>\n  <li>\n   <strong>\n    LlamaIndex:\n   </strong>\n   Seamlessly orchestrates the workflow, managing the retrieval of information from pre-indexed Wikipedia articles about animals.\n  </li>\n  <li>\n   <strong>\n    Streamlit for UI\n   </strong>\n  </li>\n </ul>\n <figure>\n  <figcaption class=\"px fe py pa pb pz qa be b bf z dt\">\n   Gif showing the demo in action\n  </figcaption>\n </figure>\n <h1>\n  Deep Dive into app.py\n </h1>\n <p>\n  The heart of \u2018AInimal Go!\u2019 lies in the\n  <code class=\"cw qb qc qd qe b\">\n   app.py\n  </code>\n  script, where ResNet, Cohere LLM, and LlamaIndex seamlessly come together. Now, let\u2019s delve into the key aspects of the code:\n </p>\n <h2>\n  1. Image Capture/Upload\n </h2>\n <p>\n  In \u2018AInimal Go!\u2019, the flow begins with the user uploading an image or capturing one using their device\u2019s camera. This is a crucial step as it sets the stage for the subsequent interaction with the identified animal.\n </p>\n <p>\n  The code snippet below illustrates how Streamlit is used to create a UI for image upload and capture. It offers two options: a file uploader for selecting an image file and a camera input for real-time capture. Once an image is provided through either method, it\u2019s converted into a byte stream (\n  <code class=\"cw qb qc qd qe b\">\n   BytesIO\n  </code>\n  ) for processing. This streamlining ensures a seamless user experience, whether the image is uploaded from a gallery or captured on the spot.\n </p>\n <pre><span class=\"qx ny gt qe b bf qy qz l ra rb\" id=\"b281\"><span class=\"hljs-comment\"># Image upload section.</span>\n    image_file = st.file_uploader(<span class=\"hljs-string\">\"Upload an image\"</span>, <span class=\"hljs-built_in\">type</span>=[<span class=\"hljs-string\">\"jpg\"</span>, <span class=\"hljs-string\">\"jpeg\"</span>, <span class=\"hljs-string\">\"png\"</span>], key=<span class=\"hljs-string\">\"uploaded_image\"</span>, on_change=on_image_upload)\n    \n    col1, col2, col3 = st.columns([<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">1</span>])\n    <span class=\"hljs-keyword\">with</span> col2:  <span class=\"hljs-comment\"># Camera input will be in the middle column</span>\n        camera_image = st.camera_input(<span class=\"hljs-string\">\"Take a picture\"</span>, on_change=on_image_upload)\n        \n    \n    <span class=\"hljs-comment\"># Determine the source of the image (upload or camera)</span>\n    <span class=\"hljs-keyword\">if</span> image_file <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n        image_data = BytesIO(image_file.getvalue())\n    <span class=\"hljs-keyword\">elif</span> camera_image <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n        image_data = BytesIO(camera_image.getvalue())\n    <span class=\"hljs-keyword\">else</span>:\n        image_data = <span class=\"hljs-literal\">None</span>\n    \n    <span class=\"hljs-keyword\">if</span> image_data:\n        <span class=\"hljs-comment\"># Display the uploaded image at a standard width.</span>\n        st.session_state[<span class=\"hljs-string\">'assistant_avatar'</span>] = image_data\n        st.image(image_data, caption=<span class=\"hljs-string\">'Uploaded Image.'</span>, width=<span class=\"hljs-number\">200</span>)</span></pre>\n <h2>\n  2. Initializing ResNet for Image Classification\n </h2>\n <p>\n  Once the user uploads or captures an image, the next critical step is identifying the animal within it. This is where ResNet18, a robust deep learning model for image classification, comes into play.\n </p>\n <p>\n  The function\n  <code class=\"cw qb qc qd qe b\">\n   load_model_and_labels\n  </code>\n  performs two key tasks:\n </p>\n <ul>\n  <li>\n   <strong>\n    Loading Animal Labels:\n   </strong>\n   It starts by loading a subset of ImageNet labels specific to animals. These labels are stored in a dictionary, mapping class IDs to their corresponding animal names. This mapping is essential for interpreting the output of the ResNet model.\n  </li>\n  <li>\n   <strong>\n    Initializing ResNet18:\n   </strong>\n   The function then initializes the feature extractor and the ResNet18 model. The feature extractor preprocesses the images to the format required by ResNet18, while the model itself is responsible for the actual classification task.\n  </li>\n </ul>\n <pre><span class=\"qx ny gt qe b bf qy qz l ra rb\" id=\"c5d4\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">load_model_and_labels</span>():\n    <span class=\"hljs-comment\"># Load animal labels as a dictionary</span>\n    animal_labels_dict = {}\n    <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">'imagenet_animal_labels_subset.txt'</span>, <span class=\"hljs-string\">'r'</span>) <span class=\"hljs-keyword\">as</span> file:\n        <span class=\"hljs-keyword\">for</span> line <span class=\"hljs-keyword\">in</span> file:\n            parts = line.strip().split(<span class=\"hljs-string\">':'</span>)\n            class_id = <span class=\"hljs-built_in\">int</span>(parts[<span class=\"hljs-number\">0</span>].strip())\n            label_name = parts[<span class=\"hljs-number\">1</span>].strip().strip(<span class=\"hljs-string\">\"'\"</span>)\n            animal_labels_dict[class_id] = label_name\n\n    <span class=\"hljs-comment\"># Initialize feature extractor and model</span>\n    feature_extractor = AutoFeatureExtractor.from_pretrained(<span class=\"hljs-string\">\"microsoft/resnet-18\"</span>)\n    model = ResNetForImageClassification.from_pretrained(<span class=\"hljs-string\">\"microsoft/resnet-18\"</span>)\n\n    <span class=\"hljs-keyword\">return</span> feature_extractor, model, animal_labels_dict\n\nfeature_extractor, model, animal_labels_dict = load_model_and_labels()</span></pre>\n <p>\n  By integrating ResNet18 in this manner, \u2018AInimal Go!\u2019 leverages its speed and accuracy for the crucial task of identifying the animal in the user\u2019s image. This sets the foundation for the engaging and informative conversations that follow.\n </p>\n <h2>\n  3. Animal Detection with ResNet18\n </h2>\n <p>\n  After initializing ResNet18, the next step is to use it for detecting the animal in the uploaded image. The function\n  <code class=\"cw qb qc qd qe b\">\n   get_image_caption\n  </code>\n  handles this task.\n </p>\n <ul>\n  <li>\n   <strong>\n    Image Preprocessing:\n   </strong>\n   The uploaded image is first opened and then preprocessed using the feature extractor initialized earlier. This preprocessing adapts the image to the format required by ResNet18.\n  </li>\n  <li>\n   <strong>\n    Animal Detection:\n   </strong>\n   The preprocessed image is then fed into ResNet18, which predicts the class of the image. The logits (the model\u2019s raw output) are processed to find the class with the highest probability, which corresponds to the predicted animal.\n  </li>\n  <li>\n   <strong>\n    Retrieving the Animal Name:\n   </strong>\n   The predicted class ID is mapped to the corresponding animal name using the label dictionary created earlier. This name is then displayed to the user.\n  </li>\n </ul>\n <pre><span class=\"qx ny gt qe b bf qy qz l ra rb\" id=\"9de2\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">get_image_caption</span>(<span class=\"hljs-params\">image_data</span>):\n    image = Image.<span class=\"hljs-built_in\">open</span>(image_data)\n    inputs = feature_extractor(images=image, return_tensors=<span class=\"hljs-string\">\"pt\"</span>)\n\n    <span class=\"hljs-keyword\">with</span> torch.no_grad():\n        logits = model(**inputs).logits\n\n    predicted_label_id = logits.argmax(-<span class=\"hljs-number\">1</span>).item()\n    predicted_label_name = model.config.id2label[predicted_label_id]\n    st.write(predicted_label_name)\n    <span class=\"hljs-comment\"># Return the predicted animal name</span>\n    <span class=\"hljs-keyword\">return</span> predicted_label_name, predicted_label_id</span></pre>\n <h2>\n  4. Validating Animal Presence in Images\n </h2>\n <p>\n  To ensure that the conversation in \u2018AInimal Go!\u2019 is relevant and engaging, it\u2019s crucial to verify that the uploaded image indeed depicts an animal. This verification is handled by the\n  <code class=\"cw qb qc qd qe b\">\n   is_animal\n  </code>\n  function.\n </p>\n <pre><span class=\"qx ny gt qe b bf qy qz l ra rb\" id=\"861e\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">is_animal</span>(<span class=\"hljs-params\">predicted_label_id</span>):\n    <span class=\"hljs-comment\"># Check if the predicted label ID is within the animal classes range</span>\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-number\">0</span> &amp;lt;= predicted_label_id &amp;lt;= <span class=\"hljs-number\">398</span></span></pre>\n <p>\n  The function checks if the predicted label ID from ResNet18 falls within the range of animal classes (0 to 398 in ImageNet\u2019s classification). This simple yet effective check is essential for maintaining the app\u2019s focus on animal interactions.\n </p>\n <p>\n  Further in the script, this function is utilized to validate the detected object:\n </p>\n <pre><span class=\"qx ny gt qe b bf qy qz l ra rb\" id=\"17d4\"><span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> (is_animal(label_id)):\n    st.error(<span class=\"hljs-string\">\"Please upload image of an animal!\"</span>)\n    st.stop()</span></pre>\n <p>\n  If the uploaded image does not depict an animal, the app prompts the user to upload an appropriate image, ensuring that the conversation remains on track.\n </p>\n <h2>\n  5. Initializing LLM\n </h2>\n <p>\n  The\n  <code class=\"cw qb qc qd qe b\">\n   init_llm\n  </code>\n  function initializes the Cohere LLM along with the necessary contexts for storage and service (specify llm and embed_model). It also loads the pre-indexed Wikipedia articles for about 200 animals. The function sets up the environment in which the LLM operates, preparing it for generating responses.\n </p>\n <pre><span class=\"qx ny gt qe b bf qy qz l ra rb\" id=\"0dc3\">def init_llm(api_key):\n    llm = Cohere(model=\"command\", api_key=st.secrets['COHERE_API_TOKEN'])\n\n    service_context = ServiceContext.from_defaults(llm=llm, embed_model=\"local\")\n    storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n    index = load_index_from_storage(storage_context, index_id=\"index\", service_context=service_context)\n    \n    return llm, service_context, storage_context, index</span></pre>\n <p>\n  This function is critical for setting up the LLM, ensuring that all necessary components are in place for the chat functionality.\n </p>\n <h2>\n  6. Creating the Chat Engine\n </h2>\n <p>\n  The\n  <code class=\"cw qb qc qd qe b\">\n   create_chat_engine\n  </code>\n  function takes the animal description and utilizes it to create a query engine. This engine is responsible for handling user queries and generating responses based on the identified animal.\n </p>\n <pre><span class=\"qx ny gt qe b bf qy qz l ra rb\" id=\"21d5\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">create_chat_engine</span>(<span class=\"hljs-params\">img_desc, api_key</span>):\n    doc = Document(text=img_desc)\n    \n    query_engine = CitationQueryEngine.from_args(\n        index,\n        similarity_top_k=<span class=\"hljs-number\">3</span>,\n        citation_chunk_size=<span class=\"hljs-number\">512</span>,\n        verbose=<span class=\"hljs-literal\">True</span>\n    )\n    \n    <span class=\"hljs-keyword\">return</span> query_engine</span></pre>\n <pre><span class=\"qx ny gt qe b bf qy qz l ra rb\" id=\"6fcd\">system_prompt=f\"\"\"\n              You are a chatbot, able to have normal interactions. Do not make up information.\n              You always answer in great detail and are polite. Your job is to roleplay as an {img_desc}. \n              Remember to make {img_desc} sounds while talking but dont overdo it.\n              \"\"\"\n                    \nresponse = chat_engine.query(f\"{system_prompt}. {user_input}\")</span></pre>\n <p>\n  By creating a query engine specific to the identified animal, this function ensures that the conversations in the app are relevant, informative, and engaging. I have used the CitationQueryEngine to provide the future possibility of showing the sources as well, making the conversations not only engaging but also informative with credible references.\n </p>\n <h2>\n  7. Bringing It All Together\n </h2>\n <p>\n  With all the technical components in place, \u2018AInimal Go!\u2019 combines everything into a user-friendly chat interface. Here, users can interact directly with the AI, asking questions and receiving responses about the identified animal. This final interaction loop, skillfully managed by Streamlit, perfectly showcases the seamless integration of vision and language models in the app.\n </p>\n <h1>\n  Wrapping Up\n </h1>\n <p>\n  \u2018AInimal Go!\u2019 represents an exciting fusion of vision models, language models, and Wikipedia, with LlamaIndex serving as the orchestrator that seamlessly integrates ResNet for animal identification and Cohere\u2019s LLM for engaging conversations. This app is a stepping stone to even more innovative visual-language applications. The possibilities are boundless, and your insights can shape its future. I encourage you to explore the demo, experiment with the code, and join me in pushing the boundaries of what AI can achieve in the realm of multimodal interactions.\n </p>\n <p>\n  <a href=\"https://github.com/AI-ANK/AInimalGo-Chat-with-Animals\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   GitHub Repo\n  </a>\n </p>\n <p>\n  <a href=\"https://www.linkedin.com/in/harshadsuryawanshi/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Connect with Me on LinkedIn\n  </a>\n </p>\n <p>\n  <a href=\"https://www.linkedin.com/posts/harshadsuryawanshi_llamaindex-ai-deeplearning-activity-7134632983495327744-M7yy?utm_source=share&amp;utm_medium=member_desktop\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LinkedIn Post\n  </a>\n </p>\n <p>\n  <a href=\"https://huggingface.co/spaces/AI-ANK/AInimal_Go\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Live Demo\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 15977, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4802a12a-b25c-4d24-8cdf-6a91a0f50268": {"__data__": {"id_": "4802a12a-b25c-4d24-8cdf-6a91a0f50268", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.html", "file_name": "multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.html", "file_type": "text/html", "file_size": 22025, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.html", "file_name": "multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.html", "file_type": "text/html", "file_size": 22025, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "4fda312600e23e933925fe752fc9fbf7104ea94155e8f013bdf3eaaa2d332b10", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  The widespread consumption of videos on platforms like YouTube, Instagram, and others highlights the importance of efficiently processing and analyzing video content. This capability unlocks vast opportunities across various sectors, including media and entertainment, security, and education. However, the main challenge is effectively extracting meaningful information from videos, which are inherently complex and multimodal data streams.\n </p>\n <p>\n  This blog post introduces a solution that leverages the LlamaIndex\n  <a href=\"https://docs.llamaindex.ai/en/latest/index.html#\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Python API\n  </a>\n  for using the advanced capabilities of OpenAI\u2019s\n  <a href=\"https://help.openai.com/en/articles/8555496-gpt-4v-api\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   GPT4V\n  </a>\n  , combined with the efficient data management by\n  <a href=\"https://lancedb.github.io/lancedb/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LanceDB\n  </a>\n  across all data formats, to process videos.\n </p>\n <p>\n  \u2026But what does \u2018RAG\u2019 even mean?\n </p>\n <p>\n  Retrieval-augmented generation (RAG) is a technique that merges information retrieval with generative AI to produce systems capable of generating precise and contextually relevant responses by tapping into large data repositories.\n </p>\n <h1>\n  Core Concept of RAG\n </h1>\n <p>\n  RAG operates in two stages:\n </p>\n <ol>\n  <li>\n   <strong>\n    Retrieval\n   </strong>\n   : Utilizes semantic search to find documents related to a query, leveraging the context and meaning beyond mere keywords.\n  </li>\n  <li>\n   <strong>\n    Generation\n   </strong>\n   : Integrates retrieved information to produce coherent responses, allowing the AI to \u201clearn\u201d from a wide range of content dynamically.\n  </li>\n </ol>\n <h1>\n  RAG Architecture\n </h1>\n <p>\n  The architecture typically involves a dense vector search engine for retrieval and a transformer model for generation. The process:\n </p>\n <ul>\n  <li>\n   Performs a semantic search to fetch relevant documents.\n  </li>\n  <li>\n   Processes these documents with the query to create a comprehensive context.\n  </li>\n  <li>\n   The generative model then crafts a detailed response based on this enriched context.\n  </li>\n </ul>\n <h1>\n  Extending to Multimodality\n </h1>\n <p>\n  Multimodal RAG integrates various data types (text, images, audio, video) in both retrieval and generation phases, enabling richer information sourcing. For example, responding to queries about \u201cclimate change impacts on polar bears\u201d might involve retrieving scientific texts, images, and videos to produce an enriched, multi-format response.\n </p>\n <p>\n  Let\u2019s return to our use case and dive into how it\u2019s all done. Moving forward, you can access the full code on\n  <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/multi_modal_video_RAG.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Google Colab\n  </a>\n  .\n </p>\n <p>\n  The solution is divided into the following sections. Click on the topic to skip to a specific part:\n </p>\n <ol>\n  <li>\n   <a href=\"#e9e9\" rel=\"noopener ugc nofollow\">\n    Video Downloading\n   </a>\n  </li>\n  <li>\n   <a href=\"#fceb\" rel=\"noopener ugc nofollow\">\n    Video Processing\n   </a>\n  </li>\n  <li>\n   <a href=\"#a0b8\" rel=\"noopener ugc nofollow\">\n    Building the Multi-Modal Index and Vector Store\n   </a>\n  </li>\n  <li>\n   <a href=\"#21e4\" rel=\"noopener ugc nofollow\">\n    Retrieving Relevant Images and Context\n   </a>\n  </li>\n  <li>\n   <a href=\"#6d77\" rel=\"noopener ugc nofollow\">\n    Reasoning and Response Generation\n   </a>\n  </li>\n </ol>\n <h1>\n  1. Video Downloading\n </h1>\n <p>\n  To begin, we need to locally download multimodal content from a publicly available source, I used pytube to download a YouTube video by 3Blue1Brown on the Gaussian function.\n </p>\n <pre><span class=\"qi oo gt qf b bf qj qk l ql qm\" id=\"0992\"># SET CONFIG\nvideo_url = \"https://www.youtube.com/watch?v=d_qvLDhkg00\"\noutput_video_path = \"./video_data/\"\noutput_folder = \"./mixed_data/\"\noutput_audio_path = \"./mixed_data/output_audio.wav\"\n\nfilepath = output_video_path + \"input_vid.mp4\"\nPath(output_folder).mkdir(parents=True, exist_ok=True)</span></pre>\n <pre><span class=\"qi oo gt qf b bf qj qk l ql qm\" id=\"7909\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">download_video</span>(<span class=\"hljs-params\">url, output_path</span>):\n    <span class=\"hljs-string\">\"\"\"\n    Download a video from a given url and save it to the output path.\n\n    Parameters:\n    url (str): The url of the video to download.\n    output_path (str): The path to save the video to.\n\n    Returns:\n    dict: A dictionary containing the metadata of the video.\n    \"\"\"</span>\n  <span class=\"hljs-keyword\">from</span> pytube <span class=\"hljs-keyword\">import</span> YouTube\n\n    yt = YouTube(url)\n    metadata = {<span class=\"hljs-string\">\"Author\"</span>: yt.author, <span class=\"hljs-string\">\"Title\"</span>: yt.title, <span class=\"hljs-string\">\"Views\"</span>: yt.views}\n    yt.streams.get_highest_resolution().download(\n        output_path=output_path, filename=<span class=\"hljs-string\">\"input_vid.mp4\"</span>\n    )\n    <span class=\"hljs-keyword\">return</span> metadata\n</span></pre>\n <p>\n  Run\n  <code class=\"cw qo qp qq qf b\">\n   <strong>\n    metadata_vid = download_video(video_url, output_video_path)\n   </strong>\n  </code>\n  to invoke the function and store the video locally.\n </p>\n <h1>\n  <em class=\"qr\">\n   2. Video Processing\n  </em>\n </h1>\n <p>\n  We need to now extract multimodal content \u2014 Images, Text(via Audio). I extracted 1 frame every 5 seconds of the video (~160 frames) using\n  <code class=\"cw qo qp qq qf b\">\n   moviepy\n  </code>\n  .\n </p>\n <pre><span class=\"qi oo gt qf b bf qj qk l ql qm\" id=\"6811\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">video_to_images</span>(<span class=\"hljs-params\">video_path, output_folder</span>):\n    <span class=\"hljs-string\">\"\"\"\n    Convert a video to a sequence of images and save them to the output folder.\n\n    Parameters:\n    video_path (str): The path to the video file.\n    output_folder (str): The path to the folder to save the images to.\n\n    \"\"\"</span>\n    clip = VideoFileClip(video_path)\n    clip.write_images_sequence(\n        os.path.join(output_folder, <span class=\"hljs-string\">\"frame%04d.png\"</span>), fps=<span class=\"hljs-number\">0.2</span> <span class=\"hljs-comment\">#configure this for controlling frame rate.</span>\n    )</span></pre>\n <p>\n  Following this, we extract the audio component:\n </p>\n <pre><span class=\"qi oo gt qf b bf qj qk l ql qm\" id=\"1960\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">video_to_audio</span>(<span class=\"hljs-params\">video_path, output_audio_path</span>):\n    <span class=\"hljs-string\">\"\"\"\n    Convert a video to audio and save it to the output path.\n\n    Parameters:\n    video_path (str): The path to the video file.\n    output_audio_path (str): The path to save the audio to.\n\n    \"\"\"</span>\n    clip = VideoFileClip(video_path)\n    audio = clip.audio\n    audio.write_audiofile(output_audio_path)</span></pre>\n <p>\n  Next, let\u2019s extract text from the audio using the SpeechRecognition library:\n </p>\n <pre><span class=\"qi oo gt qf b bf qj qk l ql qm\" id=\"fdfd\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">audio_to_text</span>(<span class=\"hljs-params\">audio_path</span>):\n    <span class=\"hljs-string\">\"\"\"\n    Convert an audio file to text.\n\n    Parameters:\n    audio_path (str): The path to the audio file.\n\n    Returns:\n    test (str): The text recognized from the audio.\n\n    \"\"\"</span>\n    recognizer = sr.Recognizer()\n    audio = sr.AudioFile(audio_path)\n\n    <span class=\"hljs-keyword\">with</span> audio <span class=\"hljs-keyword\">as</span> source:\n        <span class=\"hljs-comment\"># Record the audio data</span>\n        audio_data = recognizer.record(source)\n\n        <span class=\"hljs-keyword\">try</span>:\n            <span class=\"hljs-comment\"># Recognize the speech</span>\n            text = recognizer.recognize_whisper(audio_data)\n        <span class=\"hljs-keyword\">except</span> sr.UnknownValueError:\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Speech recognition could not understand the audio.\"</span>)\n        <span class=\"hljs-keyword\">except</span> sr.RequestError <span class=\"hljs-keyword\">as</span> e:\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Could not request results from service; <span class=\"hljs-subst\">{e}</span>\"</span>)\n\n    <span class=\"hljs-keyword\">return</span> text</span></pre>\n <p>\n  Run the below chunk to complete the extraction and storage process:\n </p>\n <pre><span class=\"qi oo gt qf b bf qj qk l ql qm\" id=\"c99a\">video_to_images(filepath, output_folder)\nvideo_to_audio(filepath, output_audio_path)\ntext_data = audio_to_text(output_audio_path)\n\n<span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(output_folder + <span class=\"hljs-string\">\"output_text.txt\"</span>, <span class=\"hljs-string\">\"w\"</span>) <span class=\"hljs-keyword\">as</span> file:\n    file.write(text_data)\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Text data saved to file\"</span>)\nfile.close()\nos.remove(output_audio_path)\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Audio file removed\"</span>)</span></pre>\n <h1>\n  3. Building the Multi-Modal Index and Vector Store\n </h1>\n <p>\n  After processing the video, we proceed to construct a multi-modal index and vector store. This entails generating embeddings for both textual and visual data using OpenAI\u2019s CLIP model, subsequently storing and managing these embeddings in LanceDB VectorStore via the\n  <code class=\"cw qo qp qq qf b\">\n   <strong>\n    LanceDBVectorStore\n   </strong>\n  </code>\n  class.\n </p>\n <pre><span class=\"qi oo gt qf b bf qj qk l ql qm\" id=\"78c3\"><span class=\"hljs-keyword\">from</span> llama_index.indices.multi_modal.base <span class=\"hljs-keyword\">import</span> MultiModalVectorStoreIndex\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> SimpleDirectoryReader, StorageContext\n\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> SimpleDirectoryReader, StorageContext\n<span class=\"hljs-keyword\">from</span> llama_index.vector_stores <span class=\"hljs-keyword\">import</span> LanceDBVectorStore\n\n\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> (\n    SimpleDirectoryReader,\n)\n\ntext_store = LanceDBVectorStore(uri=<span class=\"hljs-string\">\"lancedb\"</span>, table_name=<span class=\"hljs-string\">\"text_collection\"</span>)\nimage_store = LanceDBVectorStore(uri=<span class=\"hljs-string\">\"lancedb\"</span>, table_name=<span class=\"hljs-string\">\"image_collection\"</span>)\nstorage_context = StorageContext.from_defaults(\n    vector_store=text_store, image_store=image_store\n)\n\n<span class=\"hljs-comment\"># Create the MultiModal index</span>\ndocuments = SimpleDirectoryReader(output_folder).load_data()\n\nindex = MultiModalVectorStoreIndex.from_documents(\n    documents,\n    storage_context=storage_context,\n)</span></pre>\n <h1>\n  4. Retrieving Relevant Images and Context\n </h1>\n <p>\n  With the index in place, the system can then retrieve pertinent images and contextual information based on input queries. This enhances the prompt with precise and relevant multimodal data, anchoring the analysis in the video\u2019s content.\n </p>\n <p>\n  Lets set up the engine for retrieving, I am fetching top 5 most relevant\n  <code class=\"cw qo qp qq qf b\">\n   Nodes\n  </code>\n  from the vectordb based on the similarity score:\n </p>\n <pre><span class=\"qi oo gt qf b bf qj qk l ql qm\" id=\"72d0\">retriever_engine = index.as_retriever(\n    similarity_top_k=<span class=\"hljs-number\">5</span>, image_similarity_top_k=<span class=\"hljs-number\">5</span>\n)</span></pre>\n <blockquote>\n  <p class=\"no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\" id=\"090b\">\n   <em class=\"gt\">\n    A\n   </em>\n   <code class=\"cw qo qp qq qf b\">\n    <em class=\"gt\">\n     Node\n    </em>\n   </code>\n   <em class=\"gt\">\n    object is a \u201cchunk\u201d of any source Document, whether it\u2019s text, an image, or other. It contains embeddings as well as meta information of the chunk of data.\n   </em>\n  </p>\n  <p class=\"no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\" id=\"d183\">\n   By default, LanceDB uses\n   <code class=\"cw qo qp qq qf b\">\n    l2\n   </code>\n   as metric type for evaluating similarity. You can specify the metric type as\n   <code class=\"cw qo qp qq qf b\">\n    cosine\n   </code>\n   or\n   <code class=\"cw qo qp qq qf b\">\n    dot\n   </code>\n   if required.\n  </p>\n </blockquote>\n <p>\n  Next, we create a helper function for executing the retrieval logic:\n </p>\n <pre><span class=\"qi oo gt qf b bf qj qk l ql qm\" id=\"d602\"><span class=\"hljs-keyword\">from</span> llama_index.response.notebook_utils <span class=\"hljs-keyword\">import</span> display_source_node\n<span class=\"hljs-keyword\">from</span> llama_index.schema <span class=\"hljs-keyword\">import</span> ImageNode\n\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">retrieve</span>(<span class=\"hljs-params\">retriever_engine, query_str</span>):\n    retrieval_results = retriever_engine.retrieve(query_str)\n\n    retrieved_image = []\n    retrieved_text = []\n    <span class=\"hljs-keyword\">for</span> res_node <span class=\"hljs-keyword\">in</span> retrieval_results:\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">isinstance</span>(res_node.node, ImageNode):\n            retrieved_image.append(res_node.node.metadata[<span class=\"hljs-string\">\"file_path\"</span>])\n        <span class=\"hljs-keyword\">else</span>:\n            display_source_node(res_node, source_length=<span class=\"hljs-number\">200</span>)\n            retrieved_text.append(res_node.text)\n\n    <span class=\"hljs-keyword\">return</span> retrieved_image, retrieved_textdef retrieve(retriever_engine, query_str):\n    retrieval_results = retriever_engine.retrieve(query_str)</span></pre>\n <p>\n  Lets input the query now and then move on to complete the process by retrieving and visualizing the data :\n </p>\n <pre><span class=\"qi oo gt qf b bf qj qk l ql qm\" id=\"eead\">query_str = \"\"\"\nUsing examples from the video, explain all things covered regarding\nthe Gaussian function\n\"\"\"</span></pre>\n <pre><span class=\"qi oo gt qf b bf qj qk l ql qm\" id=\"b939\">\nimg, txt = retrieve(retriever_engine=retriever_engine, query_str=query_str)\nimage_documents = SimpleDirectoryReader(\n    input_dir=output_folder, input_files=img\n).load_data()\ncontext_str = <span class=\"hljs-string\">\"\"</span>.join(txt)\nplot_images(img)</span></pre>\n <p>\n  You should see something similar to the example below (note that the output will vary depending on your query):\n </p>\n <figure>\n  <figcaption class=\"qx fe qy na nb qz ra be b bf z dt\">\n   Displaying the similar Text objects (nodes)\n  </figcaption>\n </figure>\n <figure>\n  <figcaption class=\"qx fe qy na nb qz ra be b bf z dt\">\n   Retrieved Images\n  </figcaption>\n </figure>\n <p>\n  Observe that the\n  <code class=\"cw qo qp qq qf b\">\n   node\n  </code>\n  object displayed shows the\n  <code class=\"cw qo qp qq qf b\">\n   Id\n  </code>\n  of the data chunk , its similarity score and the source text of the chunk that was matched (for images we get the filepath instead of text).\n </p>\n <h1>\n  5. Reasoning and Response Generation\n </h1>\n <p>\n  The final step leverages GPT4V to reason about the correlations between the input query and the augmented data. Below is the prompt template :\n </p>\n <pre><span class=\"qi oo gt qf b bf qj qk l ql qm\" id=\"392a\">qa_tmpl_str = (\n    <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n Given the provided information, including relevant images and retrieved context from the video, \\\n accurately and precisely answer the query without any additional prior knowledge.\\n\"</span>\n    <span class=\"hljs-string\">\"Please ensure honesty and responsibility, refraining from any racist or sexist remarks.\\n\"</span>\n    <span class=\"hljs-string\">\"---------------------\\n\"</span>\n    <span class=\"hljs-string\">\"Context: {context_str}\\n\"</span>\n    <span class=\"hljs-string\">\"Metadata for video: {metadata_str} \\n\"</span>\n    <span class=\"hljs-string\">\"---------------------\\n\"</span>\n    <span class=\"hljs-string\">\"Query: {query_str}\\n\"</span>\n    <span class=\"hljs-string\">\"Answer: \"</span>\n<span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n)</span></span></pre>\n <p>\n  The\n  <code class=\"cw qo qp qq qf b\">\n   OpenAIMultiModal\n  </code>\n  class from LlamaIndex enables us to incorporate image data directly into our prompt object. Thus, in the final step, we enhance the query and contextual elements within the template to produce the response as follows:\n </p>\n <pre><span class=\"qi oo gt qf b bf qj qk l ql qm\" id=\"902c\"><span class=\"hljs-keyword\">from</span> llama_index.multi_modal_llms.openai <span class=\"hljs-keyword\">import</span> OpenAIMultiModal\n\nopenai_mm_llm = OpenAIMultiModal(\n    model=<span class=\"hljs-string\">\"gpt-4-vision-preview\"</span>, api_key=OPENAI_API_TOKEN, max_new_tokens=<span class=\"hljs-number\">1500</span>\n)\n\n\nresponse_1 = openai_mm_llm.complete(\n    prompt=qa_tmpl_str.<span class=\"hljs-built_in\">format</span>(\n        context_str=context_str, query_str=query_str, metadata_str=metadata_str\n    ),\n    image_documents=image_documents,\n)\n\npprint(response_1.text)</span></pre>\n <blockquote>\n  <p class=\"no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\" id=\"e779\">\n   <em class=\"gt\">\n    The generated response captures the context pretty well and structures the answer correctly :\n   </em>\n  </p>\n  <p class=\"no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\" id=\"f18e\">\n   The video \u201cA pretty reason why Gaussian + Gaussian = Gaussian\u201d by 3Blue1Brown delves into the Gaussian function or normal distribution, highlighting several critical aspects:\n  </p>\n  <p class=\"no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\" id=\"e128\">\n   <strong>\n    Central Limit Theorem:\n   </strong>\n   It starts with the central limit theorem, illustrating how the sum of multiple random variable copies tends toward a normal distribution, improving with more variables.\n  </p>\n  <p class=\"no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\" id=\"2c09\">\n   <strong>\n    Convolution of Random Variables:\n   </strong>\n   Explains the addition of two random variables as their distributions\u2019 convolution, focusing on visualizing this through diagonal slices.\n  </p>\n  <p class=\"no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\" id=\"eed4\">\n   <strong>\n    Gaussian Function:\n   </strong>\n   Details the Gaussian function, emphasizing the normalization factor for a valid probability distribution, and describes the distribution\u2019s spread and center with standard deviation (\u03c3) and mean (\u03bc).\n  </p>\n  <p class=\"no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\" id=\"da5d\">\n   <strong>\n    Convolution of Two Gaussians:\n   </strong>\n   Discusses adding two normally distributed variables, equivalent to convolving two Gaussian functions, and visualizes this using the graph\u2019s rotational symmetry.\n  </p>\n  <p class=\"no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\" id=\"df0b\">\n   <strong>\n    Rotational Symmetry and Slices:\n   </strong>\n   Shows the rotational symmetry of e^(-x\u00b2) * e^(-y\u00b2) around the origin, a unique Gaussian function property. It explains computing the area under diagonal slices, equating to the functions\u2019 convolution.\n  </p>\n  <p class=\"no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\" id=\"36bd\">\n   <strong>\n    Resulting Distribution:\n   </strong>\n   Demonstrates the convolution of two Gaussian functions yielding another Gaussian, a notable exception in convolutions usually resulting in a different function type.\n  </p>\n  <p class=\"no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\" id=\"3053\">\n   <strong>\n    Standard Deviation of the Result:\n   </strong>\n   Concludes that convolving two normal distributions with mean 0 and standard deviation (\u03c3) produces a normal distribution with a standard deviation of sqrt(2) * \u03c3.\n  </p>\n  <p class=\"no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\" id=\"1def\">\n   <strong>\n    Implications for the Central Limit Theorem:\n   </strong>\n   Highlights the convolution of two Gaussians\u2019 role in the central limit theorem, positioning the Gaussian distribution as a distribution space fixed point.\n  </p>\n  <p class=\"no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\" id=\"f19c\">\n   The author uses visual examples and explanations throughout to clarify the mathematical concepts related to the Gaussian function and its significance in probability and statistics.\n  </p>\n </blockquote>\n <h1>\n  Conclusion\n </h1>\n <p>\n  The Multimodal RAG architecture offers a powerful and efficient solution for processing and analyzing video content. By leveraging the capabilities of OpenAI\u2019s GPT4V and LanceDB, this approach not only simplifies the video analysis process but also enhances its accuracy and relevance. Whether for content creation, security surveillance, or educational purposes, the potential applications of this technology are vast and varied. As we continue to explore and refine these tools, the future of video analysis looks promising, with AI-driven solutions leading the way towards more insightful and actionable interpretations of video data.\n </p>\n <p>\n  Stay tuned for upcoming projects !\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 21968, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96a79d46-9c47-43d1-a537-51395ef4596b": {"__data__": {"id_": "96a79d46-9c47-43d1-a537-51395ef4596b", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.html", "file_name": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.html", "file_type": "text/html", "file_size": 14311, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.html", "file_name": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.html", "file_type": "text/html", "file_size": 14311, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "72aa6c7dacc12c624a8a1ac91894ba875892c43a00c4531bc0d9d455abbcba0b", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  The field of AI and large language models is evolving rapidly. One year ago, nobody ever used an LLM to enhance their productivity. Today, most of us can\u2019t imagine working without or not offloading at least some minor tasks to LLMs. Due to much research and interest, LLMs are getting better and wiser every day. Not only that, but their comprehension is starting to span across multiple modalities. With the introduction of GPT-4-Vision and other LLMs that followed it, it seems that LLMs today can tackle and comprehend images very well. Here\u2019s one example of ChatGPT describing what\u2019s in the image.\n </p>\n <figure>\n  <figcaption class=\"oa fe ob nm nn oc od be b bf z dt\">\n   Using ChatGPT to describe images.\n  </figcaption>\n </figure>\n <p>\n  As you can observe, ChatGPT is quite good at comprehending and describing images. We can use its ability to understand images in an RAG application, where instead of relying only on text to generate an accurate and up-to-date answer, we can now combine information from text and pictures to generate more accurate answers than ever before. Using LlamaIndex, implementing multimodal RAG pipelines is as easy as it gets. Inspired by their\n  <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/gpt4v_multi_modal_retrieval.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   multimodal cookbook example\n  </a>\n  , I decided to test if I could implement a multimodal RAG application with Neo4j as the database.\n </p>\n <p>\n  To implement a multimodal RAG pipeline with LlamaIndex, you simply instantiate two vector stores, one for images and one for text, and then query both of them in order to retrieve relevant information to generate the final answer.\n </p>\n <figure>\n  <figcaption class=\"oa fe ob nm nn oc od be b bf z dt\">\n   Workflow diagram for the blog post. Image by author.\n  </figcaption>\n </figure>\n <p>\n  Articles are first split into images and text. These elements are then converted into vector representations and indexed separately. For text we will use\n  <em class=\"pd\">\n   ada-002\n  </em>\n  text embedding model, while for images we will be using\n  <a href=\"https://github.com/openai/CLIP\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   dual encoder model CLIP\n  </a>\n  , which can embed both text and images in the same embedding space. When a question is posed by an end user, two vector similarity search are performed; one to find relevant images and the other for documents. The results are fed into a multimodal LLM, which generates an answer for the user, demonstrating an integrated approach to processing and utilizing mixed media for information retrieval and response generation.\n </p>\n <p>\n  <em class=\"pd\">\n   The\n  </em>\n  <a href=\"https://github.com/tomasonjo/blogs/blob/master/llm/neo4j_llama_multimodal.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <em class=\"pd\">\n    code is available on GitHub\n   </em>\n  </a>\n  <em class=\"pd\">\n   .\n  </em>\n </p>\n <h2>\n  Data preprocessing\n </h2>\n <p>\n  We will use my Medium articles from 2022 and 2023 as the\n  <a href=\"https://github.com/tomasonjo/blog-datasets/blob/main/articles.zip\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   grounding dataset\n  </a>\n  for an RAG application. The articles contain vast information about Neo4j Graph Data Science library and combining Neo4j with LLM frameworks. When you download your own articles from Medium, you get them in an HTML format. Therefore, we need to employ a bit of coding to extract text and images separately.\n </p>\n <pre><span class=\"qg pf gt qd b bf qh qi l qj qk\" id=\"7aed\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">process_html_file</span>(<span class=\"hljs-params\">file_path</span>):\n    <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(file_path, <span class=\"hljs-string\">\"r\"</span>, encoding=<span class=\"hljs-string\">\"utf-8\"</span>) <span class=\"hljs-keyword\">as</span> file:\n        soup = BeautifulSoup(file, <span class=\"hljs-string\">\"html.parser\"</span>)\n\n    <span class=\"hljs-comment\"># Find the required section</span>\n    content_section = soup.find(<span class=\"hljs-string\">\"section\"</span>, {<span class=\"hljs-string\">\"data-field\"</span>: <span class=\"hljs-string\">\"body\"</span>, <span class=\"hljs-string\">\"class\"</span>: <span class=\"hljs-string\">\"e-content\"</span>})\n\n    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> content_section:\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"Section not found.\"</span>\n\n    sections = []\n    current_section = {<span class=\"hljs-string\">\"header\"</span>: <span class=\"hljs-string\">\"\"</span>, <span class=\"hljs-string\">\"content\"</span>: <span class=\"hljs-string\">\"\"</span>, <span class=\"hljs-string\">\"source\"</span>: file_path.split(<span class=\"hljs-string\">\"/\"</span>)[-<span class=\"hljs-number\">1</span>]}\n    images = []\n    header_found = <span class=\"hljs-literal\">False</span>\n\n    <span class=\"hljs-keyword\">for</span> element <span class=\"hljs-keyword\">in</span> content_section.find_all(recursive=<span class=\"hljs-literal\">True</span>):\n        <span class=\"hljs-keyword\">if</span> element.name <span class=\"hljs-keyword\">in</span> [<span class=\"hljs-string\">\"h1\"</span>, <span class=\"hljs-string\">\"h2\"</span>, <span class=\"hljs-string\">\"h3\"</span>, <span class=\"hljs-string\">\"h4\"</span>]:\n            <span class=\"hljs-keyword\">if</span> header_found <span class=\"hljs-keyword\">and</span> (current_section[<span class=\"hljs-string\">\"content\"</span>].strip()):\n                sections.append(current_section)\n            current_section = {\n                <span class=\"hljs-string\">\"header\"</span>: element.get_text(),\n                <span class=\"hljs-string\">\"content\"</span>: <span class=\"hljs-string\">\"\"</span>,\n                <span class=\"hljs-string\">\"source\"</span>: file_path.split(<span class=\"hljs-string\">\"/\"</span>)[-<span class=\"hljs-number\">1</span>],\n            }\n            header_found = <span class=\"hljs-literal\">True</span>\n        <span class=\"hljs-keyword\">elif</span> header_found:\n            <span class=\"hljs-keyword\">if</span> element.name == <span class=\"hljs-string\">\"pre\"</span>:\n                current_section[<span class=\"hljs-string\">\"content\"</span>] += <span class=\"hljs-string\">f\"```<span class=\"hljs-subst\">{element.get_text().strip()}</span>```\\n\"</span>\n            <span class=\"hljs-keyword\">elif</span> element.name == <span class=\"hljs-string\">\"img\"</span>:\n                img_src = element.get(<span class=\"hljs-string\">\"src\"</span>)\n                img_caption = element.find_next(<span class=\"hljs-string\">\"figcaption\"</span>)\n                caption_text = img_caption.get_text().strip() <span class=\"hljs-keyword\">if</span> img_caption <span class=\"hljs-keyword\">else</span> <span class=\"hljs-string\">\"\"</span>\n                images.append(ImageDocument(image_url=img_src))\n            <span class=\"hljs-keyword\">elif</span> element.name <span class=\"hljs-keyword\">in</span> [<span class=\"hljs-string\">\"p\"</span>, <span class=\"hljs-string\">\"span\"</span>, <span class=\"hljs-string\">\"a\"</span>]:\n                current_section[<span class=\"hljs-string\">\"content\"</span>] += element.get_text().strip() + <span class=\"hljs-string\">\"\\n\"</span>\n\n    <span class=\"hljs-keyword\">if</span> current_section[<span class=\"hljs-string\">\"content\"</span>].strip():\n        sections.append(current_section)\n\n    <span class=\"hljs-keyword\">return</span> images, sections</span></pre>\n <p>\n  I won\u2019t go into details for the parsing code, but we split the text based on headers\n  <strong>\n   h1\u2013h4\n  </strong>\n  and extract image links. Then, we simply run all the articles through this function to extract all relevant information.\n </p>\n <pre><span class=\"qg pf gt qd b bf qh qi l qj qk\" id=\"25a1\">all_documents = []\nall_images = []\n\n<span class=\"hljs-comment\"># Directory to search in (current working directory)</span>\ndirectory = os.getcwd()\n\n<span class=\"hljs-comment\"># Walking through the directory</span>\n<span class=\"hljs-keyword\">for</span> root, dirs, files <span class=\"hljs-keyword\">in</span> os.walk(directory):\n    <span class=\"hljs-keyword\">for</span> file <span class=\"hljs-keyword\">in</span> files:\n        <span class=\"hljs-keyword\">if</span> file.endswith(<span class=\"hljs-string\">\".html\"</span>):\n            <span class=\"hljs-comment\"># Update the file path to be relative to the current directory</span>\n            images, documents = process_html_file(os.path.join(root, file))\n            all_documents.extend(documents)\n            all_images.extend(images)\n\ntext_docs = [Document(text=el.pop(<span class=\"hljs-string\">\"content\"</span>), metadata=el) <span class=\"hljs-keyword\">for</span> el <span class=\"hljs-keyword\">in</span> all_documents]\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Text document count: <span class=\"hljs-subst\">{<span class=\"hljs-built_in\">len</span>(text_docs)}</span>\"</span>) <span class=\"hljs-comment\"># Text document count: 252</span>\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Image document count: <span class=\"hljs-subst\">{<span class=\"hljs-built_in\">len</span>(all_images)}</span>\"</span>) <span class=\"hljs-comment\"># Image document count: 328</span></span></pre>\n <p>\n  We get a total of 252 text chunks and 328 images. It\u2019s a bit surprising that I created so many photos, but I know that some are only images of table results. We could use a vision model to filter out irrelevant photos, but I skipped this step here.\n </p>\n <h2>\n  Indexing data vectors\n </h2>\n <p>\n  As mentioned, we have to instantiate two vector stores, one for images and the other for text. The CLIP embedding model has a dimension of 512, while the ada-002 has 1536 dimension.\n </p>\n <pre><span class=\"qg pf gt qd b bf qh qi l qj qk\" id=\"176b\">text_store = Neo4jVectorStore(\n    url=NEO4J_URI,\n    username=NEO4J_USERNAME,\n    password=NEO4J_PASSWORD,\n    index_name=\"text_collection\",\n    node_label=\"Chunk\",\n    embedding_dimension=1536\n)\nimage_store = Neo4jVectorStore(\n    url=NEO4J_URI,\n    username=NEO4J_USERNAME,\n    password=NEO4J_PASSWORD,\n    index_name=\"image_collection\",\n    node_label=\"Image\",\n    embedding_dimension=512\n\n)\nstorage_context = StorageContext.from_defaults(vector_store=text_store)</span></pre>\n <p>\n  Now that the vector stores have been initiated, we use the\n  <strong>\n   MultiModalVectorStoreIndex\n  </strong>\n  to index both modalities of information we have.\n </p>\n <pre><span class=\"qg pf gt qd b bf qh qi l qj qk\" id=\"0416\"><span class=\"hljs-comment\"># Takes 10 min without GPU / 1 min with GPU on Google collab</span>\nindex = MultiModalVectorStoreIndex.from_documents(\n    text_docs + all_images, storage_context=storage_context, image_vector_store=image_store\n)</span></pre>\n <p>\n  Under the hood,\n  <strong>\n   MultiModalVectorStoreIndex\n  </strong>\n  uses text and image embedding models to calculate the embeddings and store and index the results in Neo4j. Only the URLs are stored for images, not actual base64 or other representations of images.\n </p>\n <h2>\n  Multimodal RAG pipeline\n </h2>\n <p>\n  This piece of code is copied directly from the LlamaIndex multimodal cookbook. We begin by defining a multimodal LLM and the prompt template and then combine everything as a query engine.\n </p>\n <pre><span class=\"qg pf gt qd b bf qh qi l qj qk\" id=\"9c83\">openai_mm_llm = <span class=\"hljs-title class_\">OpenAIMultiModal</span>(\n    model=<span class=\"hljs-string\">\"gpt-4-vision-preview\"</span>, max_new_tokens=<span class=\"hljs-number\">1500</span>\n)\n\nqa_tmpl_str = (\n    <span class=\"hljs-string\">\"Context information is below.\\n\"</span>\n    <span class=\"hljs-string\">\"---------------------\\n\"</span>\n    <span class=\"hljs-string\">\"{context_str}\\n\"</span>\n    <span class=\"hljs-string\">\"---------------------\\n\"</span>\n    <span class=\"hljs-string\">\"Given the context information and not prior knowledge, \"</span>\n    <span class=\"hljs-string\">\"answer the query.\\n\"</span>\n    <span class=\"hljs-string\">\"Query: {query_str}\\n\"</span>\n    <span class=\"hljs-string\">\"Answer: \"</span>\n)\nqa_tmpl = <span class=\"hljs-title class_\">PromptTemplate</span>(qa_tmpl_str)\n\nquery_engine = index.<span class=\"hljs-title function_\">as_query_engine</span>(\n    multi_modal_llm=openai_mm_llm, text_qa_template=qa_tmpl\n)</span></pre>\n <p>\n  Now we can go ahead and test how well it performs.\n </p>\n <pre><span class=\"qg pf gt qd b bf qh qi l qj qk\" id=\"bf66\">query_str = <span class=\"hljs-string\">\"How do vector RAG application work?\"</span>\nresponse = query_engine.query(query_str)\n<span class=\"hljs-built_in\">print</span>(response)</span></pre>\n <p>\n  <em class=\"pd\">\n   Response\n  </em>\n </p>\n <figure>\n  <figcaption class=\"oa fe ob nm nn oc od be b bf z dt\">\n   Generated response by an LLM.\n  </figcaption>\n </figure>\n <p>\n  We can also visualize which images the retrieval fetched and were used to help inform the final answer.\n </p>\n <figure>\n  <figcaption class=\"oa fe ob nm nn oc od be b bf z dt\">\n   Image input to LLM.\n  </figcaption>\n </figure>\n <p>\n  The LLM got two identical images as input, which just shows that I reuse some of my diagrams. However, I am pleasantly surprised by CLIP embeddings as they were able to retrieve he most relevant image out of the collection. In a more production setting, you might want to clean and deduplicate images, but that is beyond the scope of this article.\n </p>\n <h2>\n  Conclusion\n </h2>\n <p>\n  LLMs are evolving faster than what we are historically used to and are spanning across multiple modalities. I firmly believe that by the end of the next year, LLMs will be soon able to comprehend videos, and be therefore able to pick up non-verbal cues while talking to you. On the other hand, we can use images as input to RAG pipeline and enhance the variety of information passed to an LLM, making responses better and more accurate. The multimodal RAG pipelines implementation with LlamaIndex and Neo4j is as easy as it gets.\n </p>\n <p>\n  The code is available on\n  <a href=\"https://github.com/tomasonjo/blogs/blob/master/llm/neo4j_llama_multimodal.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   GitHub\n  </a>\n  .\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 14298, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14357e15-3916-4aa0-aaa6-b3214b2a64d2": {"__data__": {"id_": "14357e15-3916-4aa0-aaa6-b3214b2a64d2", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.html", "file_name": "newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.html", "file_type": "text/html", "file_size": 15857, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.html", "file_name": "newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.html", "file_type": "text/html", "file_size": 15857, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "1d4d8a989442ebc975661ef276378a52d372f7e7115a51fbc52c276aac65e81a", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  We\u2019re excited to share with you the thought process and solution design of\n  <a href=\"https://newsgpt-clickbait-buster.streamlit.app/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    NewsGPT\n   </strong>\n  </a>\n  <strong>\n   (\n  </strong>\n  <a href=\"https://www.neotice.app\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    Neotice\n   </strong>\n  </a>\n  <strong>\n   ) \u2014 Clickbait Buster\n  </strong>\n  , a Streamlit LLM hackathon-winning app powered by LlamaIndex, Streamlit, and Qdrant. In this article, we\u2019ll define the problem we\u2019re trying to solve and discuss how we approached it. Lastly, we offer a workaround to enable LlamaIndex streaming on Streamlit Chat Bot and the all the code can be found\n  <a href=\"https://github.com/timho102003/NewsGPT.git\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   here\n  </a>\n  . We hope you\u2019ll find our insights helpful and informative.\n </p>\n <h1>\n  Introduction\n </h1>\n <h2>\n  <strong>\n   Problem Statement\n  </strong>\n </h2>\n <p>\n  It\u2019s evident that people\u2019s habits of consuming information have changed over time. Previously, we read lengthy articles content and watched long videos, such as newspapers and YouTube videos. However, we currently prefer reading titles and consuming short-form content, such as TikTok and YouTube shorts. Although this shift has made it easier to get more information in less time, it has also led to clickbait headlines that often contain incorrect information.\n </p>\n <p>\n  When we started developing NewsGPT, our primary focus was to solve the above-stated pain points and provide a solution that 1)\n  <strong>\n   provides accurate information\n  </strong>\n  and 2)\n  <strong>\n   saves time for users\n  </strong>\n  .\n </p>\n <h2>\n  Neotice\n </h2>\n <p>\n  We are excited to announce that the beta version of the\n  <a href=\"https://www.neotice.app\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    Neotice\n   </strong>\n  </a>\n  app, which is the production version of NewsGPT, is now available for users to try out! We are grateful to the Streamlit Hackathon for showcasing our prototype and philosophy. With the help of this platform, we are confident that our app will revolutionize the way people consume news.\n </p>\n <h2>\n  <strong>\n   Why NewsGPT Stands Out\n  </strong>\n </h2>\n <p>\n  NewsGPT has four main components: Reliable News Sources, Tailored News Recommendations, Efficient Information Retrieval, and Time Saver.\n </p>\n <p>\n  \u2705\n  <strong>\n   Reliable News Sources:\n  </strong>\n </p>\n <ul>\n  <li>\n   We\u2019ve established a dynamic data pipeline designed to ingest daily news, ensuring our information is up-to-date and relevant.\n  </li>\n  <li>\n   Sophisticated\n   <strong>\n    Named-Entity Recognition, Text Embedding with OpenAI API,\n   </strong>\n   and\n   <strong>\n    asynchronous article embedding processes\n   </strong>\n   are incorporated. This data is systematically stored in the\n   <a href=\"https://www.linkedin.com/company/qdrant/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Qdrant\n   </a>\n   Vector Database, promoting accuracy and efficiency.\n  </li>\n </ul>\n <p>\n  \u2705\n  <strong>\n   Tailored News Recommendations:\n  </strong>\n </p>\n <ul>\n  <li>\n   Our system does more than just present news; it learns from you. By analyzing your reading habits, we leverage article embeddings to curate a personal news feed tailored to your interests.\n  </li>\n  <li>\n   A versatile\n   <strong>\n    search bar\n   </strong>\n   is always at your disposal, letting users explore any news topics that capture their interest.\n  </li>\n </ul>\n <p>\n  \u2705\n  <strong>\n   Efficient Information Retrieval:\n  </strong>\n </p>\n <ul>\n  <li>\n   With just a single click on an article of interest, NewsGPT gets to work. It collates\n   <strong>\n    similar news from multiple sources (3\u20135)\n   </strong>\n   and activates a Streamlit chatbot.\n  </li>\n  <li>\n   Your engagement begins immediately: the first query is autonomously forwarded to our chatbot to fetch a concise news summary.\n  </li>\n  <li>\n   For ease of user experience, we display\n   <strong>\n    predefined prompts\n   </strong>\n   as clickable buttons. This means users can receive information without the need for manual input.\n  </li>\n  <li>\n   Curiosity welcomed: any questions users may have about the news article will be addressed as long as the answers are detailed within the source articles.\n  </li>\n </ul>\n <p>\n  \u2705\n  <strong>\n   Time-Saving Reminder &amp; Category Distribution Chart:\n  </strong>\n </p>\n <ul>\n  <li>\n   To keep you informed, our sidebar displays the time saved using NewsGPT and visually represents news category distribution.\n  </li>\n </ul>\n <h1>\n  Delving Deep into Architecture\n </h1>\n <h2>\n  Data Pipeline\n </h2>\n <p>\n  We start with a reliable and sustainable data pipeline to support the users to get fresh news with two powerful libraries,\n  <strong>\n   pygooglenews,\n  </strong>\n  and\n  <strong>\n   newspaper3k.\n  </strong>\n </p>\n <pre><span class=\"qu oo gt qr b bf qv qw l qx qy\" id=\"1305\">pip install pygooglenews --upgrade\npip install newspaper3k</span></pre>\n <p>\n  By utilizing\n  <strong>\n   Spark batch processing\n  </strong>\n  , we efficiently process data with\n  <strong>\n   NER(Named-Entity Recognition)\n  </strong>\n  and create embeddings via\n  <strong>\n   OpenAI API (Ada model)\n  </strong>\n  . After the preprocessing of the data, we collect the metadata, including keywords, NER results, summary, body, title, author, and so on, in the payload and push the payload with embedding to the Qdrant Vector Database.\n </p>\n <p>\n  We will skip the part on how to create embeddings with the OpenAI Ada model, as there are many existing tutorials available. To perform Named Entity Recognition, we utilize the\n  <strong>\n   dslim/bert-base-NER\n  </strong>\n  model from HuggingFace.\n </p>\n <pre><span class=\"qu oo gt qr b bf qv qw l qx qy\" id=\"b68d\"><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span> pipeline\n<span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer, AutoModelForTokenClassification\ntokenizer = AutoTokenizer.from_pretrained(<span class=\"hljs-string\">\"dslim/bert-base-NER\"</span>)\nmodel = AutoModelForTokenClassification.from_pretrained(<span class=\"hljs-string\">\"dslim/bert-base-NER\"</span>)\nnlp = pipeline(<span class=\"hljs-string\">\"ner\"</span>, model=model, tokenizer=tokenizer, batch_size=batch_size)\nner_results = nlp(texts)</span></pre>\n <h2>\n  Personalization\n </h2>\n <p>\n  When accessing NewsGPT, we have two options: we can either create a new account or use a guest login to test the service. It should be noted, however, that using the guest login will not provide personalization. If we choose to sign up, we will be asked about our preferred news categories, which will help the service make initial recommendations during the cold start.\n </p>\n <figure>\n  <figcaption class=\"ra fe rb nz oa rc rd be b bf z dt\">\n   Recommendation Pipeline for Personalization\n  </figcaption>\n </figure>\n <p>\n  After users sign up and log in, we query the preferences and activities from Google Firebase, if any; otherwise, use the favorite categories for recommendation cold start. If the activities and preferences are available, we will call the recommendation API hosted on AWS Lambda and generate personalized articles for the users. The activities related to the user reading history. Users can indicate their preference for articles by clicking thumbs-up or down buttons on the article page. Some of the recommendation logic can be found in the\n  <a href=\"https://github.com/timho102003/NewsGPT/blob/main/utils.py#L174\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   code\n  </a>\n  .\n </p>\n <p>\n  Besides the personalized feeds, users can also choose different categories or search for specific topics using the search bar.\n </p>\n <h2>\n  Chat with Article Powered by LlamaIndex\n </h2>\n <figure>\n  <figcaption class=\"ra fe rb nz oa rc rd be b bf z dt\">\n   LlamaIndex Pipeline\n  </figcaption>\n </figure>\n <p>\n  Unlike other news aggregator apps, NewsGPT offers a unique service by providing users with a summary and discrepancy key takeaway of a topic from various news sources. This feature not only saves users time by eliminating the need to read multiple articles but also helps identify which information can be trusted by comparing discrepancies. What is under the hood is when the user clicks on the \u201cchat with article\u201d button, the system first uses a search API hosted on AWS Lambda to find related articles from various sources. Then, the system utilizes the\n  <a href=\"https://www.llamaindex.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    LlamaIndex\n   </strong>\n  </a>\n  library to create a vector store and a query engine, which can be integrated with the\n  <strong>\n   Streamlit\n  </strong>\n  chat component to create an\n  <strong>\n   RAG\n  </strong>\n  application for information retrieval.\n </p>\n <h2>\n  Streaming Output with LlamaIndex and Streamlit\n </h2>\n <p>\n  Thanks to the powerful library, S\n  <a href=\"https://extras.streamlit.app\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   treamlit-Extras\n  </a>\n  , which provides additional functionality not officially supported. To enhance the user experience and make it more like chatting with ChatGPT, we use the\n  <a href=\"https://arnaudmiribel.github.io/streamlit-extras/extras/streaming_write/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    streaming_write\n   </strong>\n  </a>\n  functions from the Streamlit-Extras library. Additionally, we set\n  <strong>\n   streaming=True\n  </strong>\n  for the\n  <strong>\n   query_engine\n  </strong>\n  to ensure a smoother experience. Let\u2019s take a look at the code.\n </p>\n <pre><span class=\"qu oo gt qr b bf qv qw l qx qy\" id=\"3bf5\">pip install streamlit-extras</span></pre>\n <p>\n  To begin with, we set up the\n  <strong>\n   service_context\n  </strong>\n  .\n </p>\n <pre><span class=\"qu oo gt qr b bf qv qw l qx qy\" id=\"435b\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> ServiceContext\n\nst.session_state[<span class=\"hljs-string\">\"service_context\"</span>] = ServiceContext.from_defaults(\n            llm=OpenAI(\n                model=<span class=\"hljs-string\">\"gpt-3.5-turbo\"</span>,\n                temperature=<span class=\"hljs-number\">0.2</span>,\n                chunk_size=<span class=\"hljs-number\">1024</span>,\n                chunk_overlap=<span class=\"hljs-number\">100</span>,\n                system_prompt=<span class=\"hljs-string\">\"As an expert current affairs commentator and analyst,\\\n                               your task is to summarize the articles and answer the questions from the user related to the news articles\"</span>,\n            ),\n            chunk_size=<span class=\"hljs-number\">256</span>, \n            chunk_overlap=<span class=\"hljs-number\">20</span>\n        )</span></pre>\n <p>\n  Next, we create a text splitter using\n  <strong>\n   TokenTextSplitter\n  </strong>\n  and a node parser using\n  <strong>\n   SimpleNodeParser\n  </strong>\n  to parse multiple articles.\n </p>\n <pre><span class=\"qu oo gt qr b bf qv qw l qx qy\" id=\"865c\"><span class=\"hljs-keyword\">from</span> llama_index.text_splitter <span class=\"hljs-keyword\">import</span> TokenTextSplitter\n<span class=\"hljs-keyword\">from</span> llama_index.node_parser <span class=\"hljs-keyword\">import</span> SimpleNodeParser\n\ntext_splitter = TokenTextSplitter(separator=<span class=\"hljs-string\">\" \"</span>, chunk_size=<span class=\"hljs-number\">256</span>, chunk_overlap=<span class=\"hljs-number\">20</span>)\n<span class=\"hljs-comment\">#create node parser to parse nodes from document</span>\nnode_parser = SimpleNodeParser(text_splitter=text_splitter)\nnodes = node_parser.get_nodes_from_documents(documents)</span></pre>\n <p>\n  In the third step, we create an index using\n  <strong>\n   VectorStoreIndex\n  </strong>\n  . To enable streaming capability, ensure to set\n  <strong>\n   streaming=True\n  </strong>\n  while setting up the\n  <strong>\n   query_engine\n  </strong>\n  .\n </p>\n <pre><span class=\"qu oo gt qr b bf qv qw l qx qy\" id=\"b962\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> VectorStoreIndex\n\nindex = VectorStoreIndex(\n        nodes=nodes,\n        service_context=st.session_state[<span class=\"hljs-string\">\"service_context\"</span>]\n    )\nst.session_state[<span class=\"hljs-string\">\"chat_engine\"</span>] = index.as_query_engine(streaming=<span class=\"hljs-literal\">True</span>)</span></pre>\n <p>\n  To add streaming capability to Streamlit chat components, we took inspiration from this\n  <a href=\"https://arnaudmiribel.github.io/streamlit-extras/extras/streaming_write/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   code\n  </a>\n  . Instead of using\n  <strong>\n   st.write()\n  </strong>\n  from the regular chat implementation, we replaced it with the\n  <strong>\n   write\n  </strong>\n  function from\n  <strong>\n   streaming_write\n  </strong>\n  .\n </p>\n <pre><span class=\"qu oo gt qr b bf qv qw l qx qy\" id=\"6c47\">response = st.session_state[<span class=\"hljs-string\">\"chat_engine\"</span>].query(prompt)\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">stream_example</span>():\n    <span class=\"hljs-keyword\">for</span> word <span class=\"hljs-keyword\">in</span> response.response_gen:\n        <span class=\"hljs-keyword\">yield</span> word\nwrite(stream_example)</span></pre>\n <figure>\n  <figcaption class=\"ra fe rb nz oa rc rd be b bf z dt\">\n   Streaming Demo\n  </figcaption>\n </figure>\n <h2>\n  Predefined Prompts\n </h2>\n <p>\n  Three different prompts are predefined in the article chat page, allowing users to select from a drop-down menu to ask questions without typing. The prompts are 5W1H, Similar Viewpoints, and Discrepancy Viewpoints.\n </p>\n <pre><span class=\"qu oo gt qr b bf qv qw l qx qy\" id=\"7e2f\">prompt_content = {\n    <span class=\"hljs-string\">\"5W1H\"</span>: <span class=\"hljs-string\">'Summarize the content details in the \"5W1H\" approach (Who, What, When, Where, Why, and How) in bullet points'</span>,\n    <span class=\"hljs-string\">\"Similar Viewpoints\"</span>: <span class=\"hljs-string\">\"Compare between the articles and provide the similar viewpoints in bullet points\"</span>,\n    <span class=\"hljs-string\">\"Discrepency Viewpoints\"</span>: <span class=\"hljs-string\">\"Compare between the articles and provide the discrepency viewpoints in bullet points\"</span>\n}</span></pre>\n <h1>\n  What\u2019s Next\n </h1>\n <p>\n  Huge thanks to LlamaIndex and Streamlit for generously providing a massive platform that allows more people to gain awareness of the organic news digest and save valuable time through\n  <a href=\"https://newsgpt-clickbait-buster.streamlit.app\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    NewsGPT\n   </strong>\n  </a>\n  . If you enjoyed reading the article and agree with our concept, please do not hesitate to leave a clap for the article and join\n  <a href=\"https://www.neotice.app\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    Neotice\n   </strong>\n  </a>\n  , the production app of NewsGPT, to support us. We are confident in our mission and look forward to having you on board with us. Thank you!\n </p>\n <p>\n  You can also connect us on LinkedIn:\n  <a href=\"http://www.linkedin.com/in/kangchi-ho\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    Kang-Chi Ho\n   </strong>\n  </a>\n  <strong>\n   ,\n  </strong>\n  <a href=\"https://www.linkedin.com/in/chien-an-wang-6054b9110/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    Jian-An Wang\n   </strong>\n  </a>\n </p>\n <p>\n  \ud83c\udf89 Click Here to Join Neotice \ud83d\udc49\n  <a href=\"https://www.neotice.app\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    Neotice\n   </strong>\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 15816, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0212bfaa-89d6-456c-a78f-aa8ce03094df": {"__data__": {"id_": "0212bfaa-89d6-456c-a78f-aa8ce03094df", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/nvidia-research-rag-with-long-context-llms-7d94d40090c4.html", "file_name": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.html", "file_type": "text/html", "file_size": 10854, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/nvidia-research-rag-with-long-context-llms-7d94d40090c4.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/nvidia-research-rag-with-long-context-llms-7d94d40090c4.html", "file_name": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.html", "file_type": "text/html", "file_size": 10854, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "1a641d109358ca55607a3d66646e46605318903454398a5988dc60b8d69656d5", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h1>\n  Introduction\n </h1>\n <h2>\n  Why Long Context Matters and How Retrieval Augmentation Steps In:\n </h2>\n <p>\n  In the dynamic landscape of LLMs, two methods have gained traction and seem to be taking center stage: expanding the context window of Large Language Models (LLMs) and enhancing these models with retrieval capabilities. The continued evolution of GPU technology, coupled with breakthroughs in attention mechanisms, has given rise to long-context LLMs. Simultaneously, the concept of retrieval \u2014 where LLMs pick up only the most relevant context from a standalone retriever \u2014 promises a revolution in efficiency and speed.\n </p>\n <p>\n  In the midst of these evolving narratives, some interesting questions emerge:\n </p>\n <ol>\n  <li>\n   Retrieval-augmentation versus long context window, which one is better for downstream tasks?\n  </li>\n  <li>\n   Can both methods be combined to get the best of both worlds?\n  </li>\n </ol>\n <p>\n  To dissect these questions, in this blog post we turn to\n  <a href=\"https://arxiv.org/pdf/2310.03025v1.pdf\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   NVIDIA\u2019s recent study\n  </a>\n  , which harnesses the power of two powerful LLMs: the proprietary GPT \u2014 43B and LLaMA2\u201370B, the research strives to provide actionable insights for AI practitioners.\n </p>\n <h2>\n  Prior Research and the NVIDIA Divergence:\n </h2>\n <p>\n  Interestingly, while NVIDIA\u2019s findings are interesting in many respects, Another recent work by\n  <a href=\"https://arxiv.org/abs/2308.14508\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Bai et al. (2023)\n  </a>\n  also ventured into similar territory, although with differing outcomes.\n </p>\n <p>\n  Their work explored the impact of retrieval on long context LLMs, evaluating models like GPT-3.5-Turbo-16k and Llama2\u20137B-chat-4k. However, their findings diverge from NVIDIA\u2019s in crucial ways.\n  <a href=\"https://arxiv.org/abs/2308.14508\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Bai et al.\n  </a>\n  discerned that retrieval was beneficial only for the Llama2\u20137B-chat-4k with a 4K context window, but not for extended context models like GPT-3.5-Turbo-16k. One hypothesis for this difference centers on the challenges tied to experiments using black-box APIs and the smaller white-box LLMs they employed, which potentially had limited capability to integrate context through retrieval.\n </p>\n <p>\n  NVIDIA\u2019s work distinguishes itself by tapping into much larger LLMs, yielding results that not only match top-tier models like ChatGPT-3.5 but even indicate further enhancements when incorporating retrieval methods.\n </p>\n <h1>\n  Models, Datasets, and Evaluation Metrics\n </h1>\n <h2>\n  Large Language Models (LLMs) Explored:\n </h2>\n <p>\n  The researchers delved deep into the potential of large language models for tasks like generative QA and summarization. Specifically, two models were the primary focus:\n </p>\n <ul>\n  <li>\n   <strong>\n    Nemo GPT-43B:\n   </strong>\n   A proprietary 43 billion parameter model trained on 1.1T tokens, 70% of which were in English. This model was fed a rich diet of web archives, Wikipedia, Reddit, books, and more. It contains 48 layers and is trained using RoPE embeddings.\n  </li>\n  <li>\n   <strong>\n    LLaMA2\u201370B:\n   </strong>\n   A publicly available 70B parameter model trained on 2T tokens, primarily in English. It\u2019s structured with 80 layers and also utilizes RoPE embeddings.\n  </li>\n </ul>\n <h2>\n  Context Window Extension:\n </h2>\n <p>\n  To enhance the models\u2019 capability to process longer contexts, their initial 4K context window length was augmented. The GPT-43B was modified to handle 16K, while the LLaMA2\u201370B was expanded to both 16K and 32K, employing the position interpolation method.\n </p>\n <h2>\n  Instruction Tuning:\n </h2>\n <p>\n  To optimize the LLMs for the tasks at hand, instruction tuning was implemented. A diverse dataset blend, comprising sources like Soda, ELI5, FLAN, and others, was created. A consistent format template was adopted for multi-turn dialogue training, and the models were meticulously fine-tuned to accentuate the answer segment.\n </p>\n <h2>\n  Retrieval Models Tested:\n </h2>\n <p>\n  Three retrieval systems were put to the test:\n </p>\n <ul>\n  <li>\n   <strong>\n    Dragon:\n   </strong>\n   A state-of-the-art dual encoder model for both supervised and zero-shot information retrieval.\n  </li>\n  <li>\n   <strong>\n    Contriever:\n   </strong>\n   Utilizes a basic contrastive learning framework and operates unsupervised.\n  </li>\n  <li>\n   <strong>\n    OpenAI embedding:\n   </strong>\n   The latest version was used, accepting a maximum input of 8,191 tokens.\n  </li>\n </ul>\n <p>\n  The retrieval approach entailed segmenting each document into 300-word sections, encoding both questions and these chunks, and then merging the most pertinent chunks for response generation.\n </p>\n <h2>\n  Datasets Used for Evaluation:\n </h2>\n <p>\n  The study employed seven diverse datasets, sourced from the Scroll benchmark and LongBench.\n </p>\n <p>\n  A snapshot of these datasets includes:\n </p>\n <ul>\n  <li>\n   <strong>\n    QMSum:\n   </strong>\n   A query-based summarization dataset, QMSum consists of transcripts from diverse meetings and their corresponding summaries, built upon contextual queries.\n  </li>\n  <li>\n   <strong>\n    Qasper:\n   </strong>\n   A question-answering dataset centered on NLP papers, Qasper offers a mix of abstractive, extractive, yes/no, and unanswerable questions from the Semantic Scholar Open Research Corpus.\n  </li>\n  <li>\n   <strong>\n    NarrativeQA:\n   </strong>\n   Aimed at question-answering over entire books and movie scripts, NarrativeQA provides question-answer pairs created from summaries of these extensive sources.\n  </li>\n  <li>\n   <strong>\n    QuALITY:\n   </strong>\n   A multiple-choice question answering set based on stories and articles, QuALITY emphasizes thorough reading, with half the questions designed to be challenging and require careful consideration.\n  </li>\n  <li>\n   <strong>\n    MuSiQue:\n   </strong>\n   Designed for multi-hop reasoning in question answering, MuSiQue creates multi-hop questions from single-hop ones, emphasizing connected reasoning and minimizing shortcuts.\n  </li>\n  <li>\n   <strong>\n    HotpotQA:\n   </strong>\n   Based on Wikipedia, HotpotQA requires reading multiple supporting documents for reasoning. It features diverse questions and provides sentence-level support for answers.\n  </li>\n  <li>\n   <strong>\n    MultiFieldQA-en:\n   </strong>\n   Curated to test long-context understanding across fields, MFQA uses sources like legal documents and academic papers, with annotations done by Ph.D. students.\n  </li>\n </ul>\n <h2>\n  Evaluation Metrics:\n </h2>\n <p>\n  The research team used a wide range of metrics suited to each dataset. The geometric mean of ROUGE scores for QM, the exact matching (EM) score for QLTY, and F1 scores for others were the primary metrics.\n </p>\n <h1>\n  Results\n </h1>\n <ul>\n  <li>\n   Baseline models without retrieval, having a 4K sequence length, performed poorly since valuable texts get truncated.\n  </li>\n  <li>\n   With retrieval, performance for 4K models like LLaMA2\u201370B-4K and GPT-43B-4K significantly improved.\n  </li>\n  <li>\n   HotpotQA, a multi-hop dataset, particularly benefits from longer sequence models.\n  </li>\n  <li>\n   Models with longer contexts (16K, 32K) outperform their 4K counterparts even when fed the same evidence chunks.\n  </li>\n  <li>\n   There exists a unique \u201cU-shaped\u201d performance curve for LLMs due to the\n   <code class=\"cw qo qp qq qr b\">\n    lost in the middle\n   </code>\n   phenomenon, making them better at utilizing information at the beginning or end of the input.\n  </li>\n  <li>\n   The study presents a contrasting perspective to LongBench\u2019s findings, emphasizing that retrieval is beneficial for models regardless of their context window size.\n  </li>\n </ul>\n <h2>\n  Comparing to OpenAI Models:\n </h2>\n <ul>\n  <li>\n   The LLaMA2\u201370B-32k model with retrieval surpasses the performance of GPT-3.5-turbo variants and is competitive with Davinci-003, underscoring its robustness in handling long context tasks.\n  </li>\n </ul>\n <h2>\n  Comparison of Different Retrievers:\n </h2>\n <ul>\n  <li>\n   Retrieval consistently enhances the performance across different retrievers.\n  </li>\n  <li>\n   Public retrievers outperformed proprietary ones like OpenAI embeddings.\n  </li>\n </ul>\n <h2>\n  Comparing with the number of retrieved chunks:\n </h2>\n <ul>\n  <li>\n   The best performance is achieved by retrieving the top 5 or 10 chunks. Retrieving more, up to 20 chunks, doesn\u2019t offer additional benefits and can even degrade performance.\n  </li>\n  <li>\n   The deterioration in performance when adding more chunks could be due to the\n   <code class=\"cw qo qp qq qr b\">\n    lost-in-the-middle\n   </code>\n   phenomenon or the model being sidetracked by non-relevant information.\n  </li>\n </ul>\n <h1>\n  Conclusion\n </h1>\n <p>\n  As we delved deep into understanding how retrieval augmentation and long-context extension interact when applied to leading language models fine-tuned for long-context question-answering and summarization tasks. Here are some things to be noted:\n </p>\n <ol>\n  <li>\n   <strong>\n    Boost in Performance with Retrieval\n   </strong>\n   : Implementing retrieval techniques significantly enhances the performance of both shorter 4K context language models and their longer 16K/32K context counterparts.\n  </li>\n  <li>\n   <strong>\n    Efficiency of 4K Models with Retrieval\n   </strong>\n   : 4K context language models, when combined with retrieval augmentation, can achieve performance levels similar to 16K long context models. Plus, they have the added advantage of being faster during the inference process.\n  </li>\n  <li>\n   <strong>\n    Best Model Performance\n   </strong>\n   : After enhancing with both context window extension and retrieval augmentation, the standout model, LLaMA2\u201370B-32k-ret (LLaMA2\u201370B-32k with retrieval), surpasses well-known models like GPT-3.5-turbo-16k and davinci-003.\n  </li>\n </ol>\n <h1>\n  References:\n </h1>\n <ol>\n  <li>\n   <a href=\"https://arxiv.org/pdf/2310.03025v1.pdf\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Retrieval meets long context, large language models.\n   </a>\n  </li>\n  <li>\n   <a href=\"https://arxiv.org/abs/2308.14508\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Longbench: A bilingual, multitask benchmark for long context understanding.\n   </a>\n  </li>\n </ol>\n <p>\n  We trust that this blog post on the review of the paper on retrieval augmentation with long-context LLMs has furnished you with meaningful insights. We\u2019re keen to hear if your experiments align with our findings or present new perspectives \u2014 divergent results always make for interesting discussions and further exploration.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 10805, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3939f831-8ef2-4f7a-b240-ed12c62e4306": {"__data__": {"id_": "3939f831-8ef2-4f7a-b240-ed12c62e4306", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/one-click-open-source-rag-observability-with-langfuse.html", "file_name": "one-click-open-source-rag-observability-with-langfuse.html", "file_type": "text/html", "file_size": 9729, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/one-click-open-source-rag-observability-with-langfuse.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/one-click-open-source-rag-observability-with-langfuse.html", "file_name": "one-click-open-source-rag-observability-with-langfuse.html", "file_type": "text/html", "file_size": 9729, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "7d7815f4836c827eda7d9ef2644a819c11e89fbb25dfb1540b4ce7ae68c5e10d", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <em>\n   This is a guest post from the team at Langfuse\n  </em>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  There are so many different ways to make RAG work for a use case. What vector store to use? What retrieval strategy to use? LlamaIndex makes it easy to try many of them without having to deal with the complexity of integrations, prompts and memory all at once.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Initially, we at Langfuse worked on complex RAG/agent applications and quickly realized that there is a new need for observability and experimentation to tweak and iterate on the details. In the end, these details matter to get from something cool to an actually reliable RAG application that is safe for users and customers. Think of this: if there is a user session of interest in your\n  <em>\n   production\n  </em>\n  RAG application, how can you quickly see whether the retrieved context for that session was actually relevant or the LLM response was on point?\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Thus, we started working on\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"http://langfuse.com\" rel=\"noreferrer noopener\">\n   Langfuse.com\n  </a>\n  (\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/langfuse/langfuse\" rel=\"noreferrer noopener\">\n   GitHub\n  </a>\n  ) to establish an open source LLM engineering platform with tightly integrated features for tracing, prompt management, and evaluation. In the beginning we just solved our own and our friends\u2019 problems. Today we are at over 1000 projects which rely on Langfuse, and 2.3k stars on GitHub. You can either\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://langfuse.com/docs/deployment/self-host\" rel=\"noreferrer noopener\">\n   self-host\n  </a>\n  Langfuse or use the\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://cloud.langfuse.com\" rel=\"noreferrer noopener\">\n   cloud instance\n  </a>\n  maintained by us.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We are thrilled to announce our new integration with LlamaIndex today. This feature was\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/orgs/langfuse/discussions/828\" rel=\"noreferrer noopener\">\n   highly requested\n  </a>\n  by our community and aligns with our project's focus on native integration with major application frameworks. Thank you to everyone who contributed and tested it during the beta phase!\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  The challenge\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We love LlamaIndex, since the clean and standardized interface abstracts a lot of complexity away. Let\u2019s take this simple example of a VectorStoreIndex and a ChatEngine.\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index.core <span class=\"hljs-keyword\">import</span> SimpleDirectoryReader\n<span class=\"hljs-keyword\">from</span> llama_index.core <span class=\"hljs-keyword\">import</span> VectorStoreIndex\n\ndocuments = SimpleDirectoryReader(<span class=\"hljs-string\">\"./data\"</span>).load_data()\n\nindex = VectorStoreIndex.from_documents(documents)\n\nchat_engine = index.as_chat_engine()\n\n<span class=\"hljs-built_in\">print</span>(chat_engine.chat(<span class=\"hljs-string\">\"What problems can I solve with RAG?\"</span>))\n<span class=\"hljs-built_in\">print</span>(chat_engine.chat(<span class=\"hljs-string\">\"How do I optimize my RAG application?\"</span>))</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In just 3 lines we loaded our local documents, added them to an index and initialized a ChatEngine with memory. Subsequently we had a stateful conversation with the chat_engine.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  This is awesome to get started, but we quickly run into questions like:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <em>\n    \u201cWhat context is actually retrieved from the index to answer the questions?\u201d\n   </em>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <em>\n    \u201cHow is chat memory managed?\u201d\n   </em>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <em>\n    \u201cWhich steps add the most latency to the overall execution? How to optimize it?\u201d\n   </em>\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  One-click OSS observability to the rescue\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We integrated Langfuse to be a one-click integration with LlamaIndex using the global callback manager.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Preparation\n </p>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Install the community package (pip install llama-index-callbacks-langfuse)\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Copy/paste the environment variables from the Langfuse project settings to your Python project: 'LANGFUSE_SECRET_KEY', 'LANGFUSE_PUBLIC_KEY' and 'LANGFUSE_HOST'\n  </li>\n </ol>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Now, you only need to set the global langfuse handler:\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index.core <span class=\"hljs-keyword\">import</span> set_global_handler\n\nset_global_handler(<span class=\"hljs-string\">\"langfuse\"</span>)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  And voil\u00e1, with just two lines of code you get detailed traces for all aspects of your RAG application in Langfuse. They automatically include latency and usage/cost breakdowns.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Group multiple chat threads into a session\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Working with lots of teams building GenAI/LLM/RAG applications, we\u2019ve continuously added more features that are useful to debug and improve these applications. One example is\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://langfuse.com/docs/tracing/sessions\" rel=\"noreferrer noopener\">\n   session tracking\n  </a>\n  for conversational applications to see the traces in context of a full message thread.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  To activate it, just add an id that identifies the session as a trace param before calling the chat_engine.\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index.core <span class=\"hljs-keyword\">import</span> global_handler\n\nglobal_handler.set_trace_params(\n  session_id=<span class=\"hljs-string\">\"your-session-id\"</span>\n)\n\nchat_engine.chat(<span class=\"hljs-string\">\"What did he do growing up?\"</span>)\nchat_engine.chat(<span class=\"hljs-string\">\"What did he do at USC?\"</span>)\nchat_engine.chat(<span class=\"hljs-string\">\"How old is he?\"</span>)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Thereby you can see all these chat invocations grouped into a session view in Langfuse Tracing:\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Next to sessions, you can also track individual users or add tags and metadata to your Langfuse traces.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Trace more complex applications and use other Langfuse features for prompt management and evaluation\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  This integration makes it easy to get started with Tracing. If your application ends up growing into using custom logic or other frameworks/packages, all Langfuse integrations are fully interoperable.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We have also built additional features to version control and collaborate on prompts (langfuse\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://langfuse.com/docs/prompts/get-started\" rel=\"noreferrer noopener\">\n   prompt management\n  </a>\n  ), track\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://langfuse.com/docs/experimentation\" rel=\"noreferrer noopener\">\n   experiments\n  </a>\n  , and\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://langfuse.com/docs/scores/overview\" rel=\"noreferrer noopener\">\n   evaluate\n  </a>\n  production traces. For RAG specifically, we collaborated with the RAGAS team and it\u2019s easy to run their popular eval suite on traces captured with Langfuse (see\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://langfuse.com/docs/scores/model-based-evals/ragas\" rel=\"noreferrer noopener\">\n   cookbook\n  </a>\n  ).\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Get started\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The easiest way to get started is to follow the\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/examples/callbacks/LangfuseCallbackHandler.html\" rel=\"noreferrer noopener\">\n   cookbook\n  </a>\n  and check out the\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://langfuse.com/docs/integrations/llama-index/get-started\" rel=\"noreferrer noopener\">\n   docs\n  </a>\n  .\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Feedback? Ping us\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We\u2019d love to hear any feedback. Come join us on our\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://langfuse.com/discord\" rel=\"noreferrer noopener\">\n   community discord\n  </a>\n  or add your thoughts to this\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/orgs/langfuse/discussions/828\" rel=\"noreferrer noopener\">\n   GitHub thread\n  </a>\n  .\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 9705, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80cb2875-d534-42ff-88b0-2b247c4a4365": {"__data__": {"id_": "80cb2875-d534-42ff-88b0-2b247c4a4365", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/openai-cookbook-evaluating-rag-systems-fe393c61fb93.html", "file_name": "openai-cookbook-evaluating-rag-systems-fe393c61fb93.html", "file_type": "text/html", "file_size": 2220, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/openai-cookbook-evaluating-rag-systems-fe393c61fb93.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/openai-cookbook-evaluating-rag-systems-fe393c61fb93.html", "file_name": "openai-cookbook-evaluating-rag-systems-fe393c61fb93.html", "file_type": "text/html", "file_size": 2220, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "bd5e8fba007541791b2e48f4056f678b1c6f4321c0dcfdab4c6547fb0e76b075", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  We\u2019re excited to unveil our\n  <a href=\"https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   OpenAI Cookbook\n  </a>\n  , a guide to evaluating Retrieval-Augmented Generation (RAG) systems using LlamaIndex. We hope you\u2019ll find it useful in enhancing the effectiveness of your RAG systems, and we\u2019re thrilled to share it with you.\n </p>\n <p>\n  The OpenAI Cookbook has three sections:\n </p>\n <ol>\n  <li>\n   <strong>\n    Understanding Retrieval-Augmented Generation (RAG):\n   </strong>\n   provides a detailed overview of RAG systems, including the various stages involved in building the RAG system.\n  </li>\n  <li>\n   <strong>\n    Building RAG with LlamaIndex:\n   </strong>\n   Here, we dive into the practical aspects, demonstrating how to construct a RAG system using LlamaIndex, specifically applied to Paul Graham\u2019s essay, utilizing the\n   <code class=\"cw ou ov ow ox b\">\n    VectorStoreIndex\n   </code>\n   .\n  </li>\n  <li>\n   <strong>\n    Evaluating RAG with LlamaIndex:\n   </strong>\n   The final section focuses on assessing the RAG system\u2019s performance in two critical areas:\n   <em class=\"oy\">\n    the Retrieval System\n   </em>\n   and\n   <em class=\"oy\">\n    Response Generation.\n   </em>\n  </li>\n </ol>\n <p>\n  We use our unique synthetic dataset generation method,\n  <code class=\"cw ou ov ow ox b\">\n   generate_question_context_pairs\n  </code>\n  to conduct thorough evaluations in these areas.\n </p>\n <p>\n  Our goal with this\n  <a href=\"https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   cookbook\n  </a>\n  is to provide the community with an essential resource for effectively evaluating and enhancing RAG systems developed using LlamaIndex.\n </p>\n <p>\n  Join us in exploring the depths of RAG system evaluation and discover how to leverage the full potential of your RAG implementations with LlamaIndex.\n </p>\n <blockquote>\n  <p class=\"pa pb gt be pc pd pe pf pg ph pi ok dt\" id=\"30dd\">\n   Keep building with LlamaIndex!\ud83e\udd99\n  </p>\n </blockquote>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d177694f-0d97-438b-a3ce-7696eaf89084": {"__data__": {"id_": "d177694f-0d97-438b-a3ce-7696eaf89084", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/pii-detector-hacking-privacy-in-rag.html", "file_name": "pii-detector-hacking-privacy-in-rag.html", "file_type": "text/html", "file_size": 17508, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/pii-detector-hacking-privacy-in-rag.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/pii-detector-hacking-privacy-in-rag.html", "file_name": "pii-detector-hacking-privacy-in-rag.html", "file_type": "text/html", "file_size": 17508, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "efbcdef3054ff2c567677396669244bd813ff98227476146555f15f465637c49", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  A couple of days ago at the DataStax HQ, I had the chance to participate at the LlamaIndex RAG-A-THON. Over the span of the weekend, we had to implement a solution that leverages Retrieval Augmented Generation (RAG) technique.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Because of my background in cybersecurity, I was leaning towards the security pitfalls and obstacles of the RAG technique. One of the first things that came to mind was the fact that a lot of the unstructured data used is unsanitized and can contain sensitive data.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  PII: What? Why?\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  PII stands for Personally Identifiable Information. It refers to any information that can be used to identify a specific individual.\n  This can be names, addresses, phone numbers, email addresses, social security numbers, and financial information.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  There are a couple of reasons why handling PIIs is important:\n </p>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Privacy:\n   </strong>\n   PII often includes sensitive and private details (like addresses), so protecting it preserves customers\u2019 privacy.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Identity Theft:\n   </strong>\n   PII can also lead to identity theft (e.g. one\u2019s social security number gets compromised).\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Legal Compliance:\n   </strong>\n   Protecting PII is also the law. Many countries and regions have enacted laws and regulations that require organizations to protect PII. GDPR (General Data Protection Regulation) in the EU or HIPAA (Health Insurance Portability and Accountability Act) in the United States govern the way we handle PII.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Trust and Reputation:\n   </strong>\n   A data breach or mishandling of PII will severely damage one\u2019s reputation and trust.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Financial Security:\n   </strong>\n   PII may include financial information, such as credit card numbers and banking details. Compromised PII can lead to fraudulent transactions.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    National Security Concerns:\n   </strong>\n   All of the above are crucial in sovereign environments.\n  </li>\n </ol>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  PII in RAG\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Everything listed is applicable to almost all applications leveraging RAG. Remember that the RAG technique contains two components \u2014 the model and the vector database. For this reason, each of these components need to address PII.\n </p>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  Model\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Language models, are trained on large datasets that may contain real-world data, potentially including PII and customer data. When the models generate text, there is a risk that they\u2019ll produce content that includes PII. This is even more crucial if you\u2019re creating a multi-tenant application, and you want to prevent data leak. This risk can be mitigated by either filtering or anonymizing the response. Training the models on anonymized data that is stripped of any sensitive information is the better approach to prevent leaks of PII.\n </p>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  Vector Database\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Vector databases, just like regular databases should not persist sensitive information plainly. This kind of information should only be persisted using encryption, hashing, salt and access controls. Having said that, one should also make sure that the similarity search returned by the Database won\u2019t retrieve personal data.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  On top of that, various regulations such as GDPR and HIPAA still apply here. So, if the original data contain PII, you might need to add another instance in Europe or any additional region in accordance with regulations. Persisted data should be encrypted or hashed (and additionally salted).\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Introducing: Presidio\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://microsoft.github.io/presidio/\" rel=\"noreferrer noopener\">\n   Presidio\n  </a>\n  is an open-source library maintained by Microsoft (see our\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/microsoft/presidio\" rel=\"noreferrer noopener\">\n   GitHub\n  </a>\n  repo). It\u2019s derived from the Latin word praesidium which means \u201cprotection\u201d or garrison.\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   It enables organizations to preserve privacy using a unified SDK.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   It provides fast\n   <strong>\n    <em>\n     identification\n    </em>\n   </strong>\n   and\n   <strong>\n    <em>\n     anonymization\n    </em>\n   </strong>\n   modules for private entities in text and images such as credit card numbers, names, locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more.\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <em>\n   Disclaimer: Nothing is bulletproof. It\u2019s your responsibility to make sure sensitive data is anonymized.\n  </em>\n </p>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  How Does Presidio Work?\n </h3>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Predefined\n   </strong>\n   or\n   <strong>\n    custom PII recognizers\n   </strong>\n   leverage\n   <em>\n    Named Entity Recognition (NER)\n   </em>\n   ,\n   <em>\n    regular expressions\n   </em>\n   ,\n   <em>\n    rule-based logic\n   </em>\n   and\n   <em>\n    checksum\n   </em>\n   (e.g. bitcoin address validation).\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   It\u2019s extensible, so you can add your own entities and your own detection mechanisms.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   It\u2019s customizable, so you can create your own anonymizers, and exclude/include certain entities (e.g. exclude anonymization of geographical locations).\n  </li>\n </ol>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  LlamaIndex Post Processors\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  There was already some PII integration using NER models and LLMs! These were implemented as post processors that run in the end of the pipeline:\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index.postprocessor <span class=\"hljs-keyword\">import</span> NERPIINodePostprocessor\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> ServiceContext\n<span class=\"hljs-keyword\">from</span> llama_index.schema <span class=\"hljs-keyword\">import</span> TextNode\n\ntext = <span class=\"hljs-string\">\"\"\"\nMy name is Roey Ben Chaim and my credit card number is 4095-2609-9393-4932. \nMy email is robo@presidio.site and I live in Amsterdam.\nHave you been to a P\u00e1lmi Einarsson concert before?\nWhat is the limit for card 4158112277712? My IBAN is GB90YNTU67299444055881. \nWhat's your last name? Bob, it's Bob.\nMy great great grandfather was called Yulan Peres, \nand my great great grandmother was called Jennifer Holst\nI can't browse to your site, keep getting address 179.177.214.91 blocked error\nJust posted a photo https://www.FilmFranchise.dk/\n\"\"\"</span>\n\nnode = TextNode(text=text)\n\nservice_context = ServiceContext.from_defaults()\nprocessor = NERPIINodePostprocessor(service_context=service_context)\n\n<span class=\"hljs-keyword\">from</span> llama_index.schema <span class=\"hljs-keyword\">import</span> NodeWithScore\n\nnew_nodes = processor.postprocess_nodes([NodeWithScore(node=node)])\n<span class=\"hljs-built_in\">print</span>(new_nodes[<span class=\"hljs-number\">0</span>].node.get_text())</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Running the above code resulted in the following:\n </p>\n <pre><code>My name is [PER_12] and my credit card number is 4095-2609-9393-4932. \nMy email is robo@presidio.site and I live in [LOC_123].\nHave you been to a [PER_153] concert before?\nWhat is the limit for card 4158112277712? My IBAN is GB90YNTU67299444055881. \nWhat's your last name? [PER_286], it's [PER_286].\nMy great great grandfather was called [PER_339], \nand my great great grandmother was called [PER_395]\nI can't browse to your site, keep getting address 179.177.214.91 blocked error\nJust posted a photo https://www.[ORG_521].dk/</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  As can be seen in this example, while NER models do a decent job in detecting PII, they might miss some entities such as IBAN code, credit card numbers, emails, medical license and more.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Presidio detects more out of the box entities than traditional models. This is possible because Presidio leverages a couple of methods in detecting PII \u2014 from NER models to regular expressions and rule-based logic.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Integrating Presidio with LlamaIndex\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  I ended up integrating\n  <strong>\n   <em>\n    PresidioPIINodePostprocessor\n   </em>\n  </strong>\n  that got the text as an input and masked it. Doing this was possible using Presidio\u2019s analyzer and anonymizer:\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> presidio_analyzer <span class=\"hljs-keyword\">import</span> AnalyzerEngine\n<span class=\"hljs-keyword\">from</span> presidio_anonymizer <span class=\"hljs-keyword\">import</span> AnonymizerEngine\n\nanalyzer = AnalyzerEngine(supported_languages=[<span class=\"hljs-string\">\"en\"</span>])\nresults = analyzer.analyze(text=text, language=<span class=\"hljs-string\">'en'</span>)\nengine = AnonymizerEngine()\nnew_text = engine.anonymize(text=text, analyzer_results=results)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  This was pretty fun and simple. However, given the input text \u201cAlice and Bob are friends\u201d, the output would be: \u201c&lt;PERSON&gt; and &lt;PERSON&gt; are friends\u201d. I could not have that.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  So, I added a counter and mapped the original values with the masked values, making sure that whenever an entity was seen again, the previously asked value was used:\n </p>\n <pre><code><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">anonymize_function</span>(<span class=\"hljs-params\">origin, entity_type</span>):\n    <span class=\"hljs-keyword\">nonlocal</span> pii_counter\n    <span class=\"hljs-keyword\">nonlocal</span> inverted_mapping\n    <span class=\"hljs-keyword\">nonlocal</span> mapping\n    <span class=\"hljs-keyword\">if</span> entity_type <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span> inverted_mapping:\n        inverted_mapping[entity_type] = {}\n    typed_mapping = inverted_mapping[entity_type]\n    <span class=\"hljs-keyword\">if</span> origin <span class=\"hljs-keyword\">in</span> typed_mapping:\n        <span class=\"hljs-keyword\">return</span> typed_mapping[origin]\n    new_value = <span class=\"hljs-string\">f\"&lt;<span class=\"hljs-subst\">{entity_type}</span>_<span class=\"hljs-subst\">{pii_counter}</span>&gt;\"</span>\n    typed_mapping[origin] = new_value\n    mapping[new_value]=origin\n    pii_counter+=<span class=\"hljs-number\">1</span>\n    <span class=\"hljs-keyword\">return</span> typed_mapping[origin]\n\n<span class=\"hljs-keyword\">from</span> presidio_analyzer <span class=\"hljs-keyword\">import</span> AnalyzerEngine\n<span class=\"hljs-keyword\">from</span> presidio_anonymizer <span class=\"hljs-keyword\">import</span> AnonymizerEngine\n<span class=\"hljs-keyword\">from</span> presidio_anonymizer.entities <span class=\"hljs-keyword\">import</span> OperatorConfig\n\nanalyzer = AnalyzerEngine(supported_languages=[<span class=\"hljs-string\">\"en\"</span>])\nresults = analyzer.analyze(text=text, language=<span class=\"hljs-string\">'en'</span>)\nengine = AnonymizerEngine()\nnew_text = engine.anonymize(text=text, analyzer_results=results, \n                            operators={<span class=\"hljs-string\">\"DEFAULT\"</span>: OperatorConfig(<span class=\"hljs-string\">\"custom\"</span>, \n                            params={<span class=\"hljs-string\">\"lambda\"</span>: anonymize_function})})</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <em>\n   Note: Currently presidio doesn\u2019t contain the entity type as an input parameter in the lambda function, so I had to add this functionality.\n  </em>\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Test and Benchmark\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Once this was all up and running, I was able to call the newly added presidio post processor with the text from the previous run:\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index.postprocessor <span class=\"hljs-keyword\">import</span> PresidioPIINodePostprocessor\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> ServiceContext\n<span class=\"hljs-keyword\">from</span> llama_index.schema <span class=\"hljs-keyword\">import</span> TextNode\n\ntext = <span class=\"hljs-string\">\"\"\"\nMy name is Roey Ben Chaim and my credit card number is 4095-2609-9393-4932. \nMy email is robo@presidio.site and I live in Amsterdam.\nHave you been to a P\u00e1lmi Einarsson concert before?\nWhat is the limit for card 4158112277712? My IBAN is GB90YNTU67299444055881. \nWhat's your last name? Bob, it's Bob.\nMy great great grandfather was called Yulan Peres, \nand my great great grandmother was called Jennifer Holst\nI can't browse to your site, keep getting address 179.177.214.91 blocked error\nJust posted a photo https://www.FilmFranchise.dk/\n\"\"\"</span>\n\nnode = TextNode(text=text)\n\nservice_context = ServiceContext.from_defaults()\nprocessor = PresidioPIINodePostprocessor(service_context=service_context)\n\n<span class=\"hljs-keyword\">from</span> llama_index.schema <span class=\"hljs-keyword\">import</span> NodeWithScore\n\nnew_nodes = processor.postprocess_nodes([NodeWithScore(node=node)])\n<span class=\"hljs-built_in\">print</span>(new_nodes[<span class=\"hljs-number\">0</span>].node.get_text())</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Running the above code resulted in the following:\n </p>\n <pre><code>My name is &lt;PERSON_12&gt; and my credit card number is &lt;CREDIT_CARD_11&gt;. \nMy email is &lt;EMAIL_ADDRESS_10&gt; and I live in &lt;LOCATION_9&gt;.\nHave you been to a &lt;PERSON_8&gt; concert before?\nWhat is the limit for card &lt;CREDIT_CARD_7&gt;? My IBAN is &lt;IBAN_CODE_6&gt;. \nWhat's your last name? &lt;PERSON_5&gt;, it's &lt;PERSON_5&gt;.\nMy great great grandfather was called &lt;PERSON_4&gt;, \nand my great great grandmother was called &lt;PERSON_3&gt;\nI can't browse to your site, keep getting address &lt;IP_ADDRESS_2&gt; blocked error\nJust posted a photo &lt;URL_1&gt;</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Overall Presidio detected 12 entities while the other NER solution detected 8. Notice that credit card numbers, email address, IBAN, IP address and the URL (at least some of it) weren\u2019t detected.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  I was curious to see how the parsing of these strings would work on the LLM, so I populated the index and queried the following:\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> VectorStoreIndex\n\nindex = VectorStoreIndex([n.node <span class=\"hljs-keyword\">for</span> n <span class=\"hljs-keyword\">in</span> new_nodes])\nresponse = index.as_query_engine().query(\n    <span class=\"hljs-string\">\"What is my name?\"</span>\n)\n<span class=\"hljs-built_in\">print</span>(response)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Which resulted in:\n </p>\n <pre><code>Your name is &lt;PERSON_12&gt;.</code></pre>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  How It Ended\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Anyway, this project won the 3rd place (in the continuous track) in the RAG-A-THON.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Note: this picture doesn\u2019t contain PII\n </p>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  Update\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Presidio is now fully integrated into LlamaIndex as a post processor, follow this\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/ff51e2cbf9815a44ebade9a1382216435d7f4bc8/docs/examples/node_postprocessor/PII.ipynb#L289\" rel=\"noreferrer noopener\">\n   notebook\n  </a>\n  to learn how to use Presidio for PII masking. The next steps would be to add more customization and anonymization options.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 17461, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7886207-4b88-4a1a-8e30-820391e9deaf": {"__data__": {"id_": "c7886207-4b88-4a1a-8e30-820391e9deaf", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.html", "file_name": "pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.html", "file_type": "text/html", "file_size": 14744, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.html", "file_name": "pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.html", "file_type": "text/html", "file_size": 14744, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "9fe860045c98b7ea94c8ca8172790b3b6dada29515a8c66e90fb0447cb25adc3", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <figure>\n  <figcaption class=\"nn fe no mz na np nq be b bf z dt\">\n   ADU Planner in action\n  </figcaption>\n </figure>\n <p>\n  In the midst of a pressing housing shortage, the American dream of homeownership is being reimagined through the innovative concept of Accessory Dwelling Units (ADUs). These compact, efficient homes, nestled in the backyards of existing properties, are more than just a trend \u2014 they\u2019re a revolution in modular and prefabricated living solutions. In 2022, the ADU market, combined with modular and prefabricated houses, soared to an impressive $150 billion globally, with projections indicating a leap to $300 billion by 2032 (\n  <a href=\"https://www.factmr.com/report/4227/prefabricated-homes-market\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   link\n  </a>\n  ).\n </p>\n <p>\n  Yet, the journey to erecting an ADU has been far from simple. Traditionally, it involves extensive land surveys and numerous consultations with vendors to sift through an array of floor plans. Recognizing the need for a more efficient pathway, our team has crafted an innovative solution: a GenAI-powered ADU planning system. This cutting-edge tool deftly navigates through the maze of city building codes, effortlessly connecting users with local vendors and presenting viable floor plan options \u2014 all within our user-friendly app.\n </p>\n <p>\n  Our mission is to streamline your ADU creation process, making it less daunting and more accessible. Imagine the potential when the complexities of planning are reduced to a few clicks \u2014 this is the power of GenAI at your fingertips.\n </p>\n <h1>\n  <strong>\n   Award-Winning Innovation: ADU Planner Takes the Lead\n  </strong>\n </h1>\n <p>\n  We\u2019re thrilled to announce a milestone achievement: Our \u201cADU Planner\u201d project clinched the\n  <strong>\n   top honor\n  </strong>\n  at the\n  <a href=\"https://rag-a-thon.devpost.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    LlamaIndex RAG Hackathon\n   </strong>\n  </a>\n  , held in the first weekend of February 2024. This victory underscores our commitment to transforming the ADU space with our pioneering technology.\n </p>\n <p>\n  Discover the features that set our project apart and explore the full scope of our innovative planner through the following resources:\n </p>\n <p>\n  \u00b7\n  <strong>\n   Dive into the Details\n  </strong>\n  : Visit our\n  <a href=\"https://devpost.com/software/adu-planner?ref_content=my-projects-tab&amp;ref_feature=my_projects\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Devpost project page\n  </a>\n  for an in-depth look.\n </p>\n <p>\n  \u00b7\n  <strong>\n   Explore Our Code\n  </strong>\n  : For the tech enthusiasts, our codebase is available on\n  <a href=\"https://github.com/RJ-Heisenberg/ADU-Planner_rag-a-thon2024\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   GitHub\n  </a>\n  .\n </p>\n <p>\n  We\u2019re just getting started, and this recognition only fuels our drive to innovate and deliver solutions that matter.\n </p>\n <h1>\n  Watch Our Demo\n </h1>\n <h1>\n  <strong>\n   Technologies\n  </strong>\n </h1>\n <p>\n  At the heart of our \u201cADU Planner\u201d lies a synergy of cutting-edge technology and user-friendly design. The seamless interface is crafted with React, providing an intuitive frontend experience, while our robust Flask backend is the powerhouse of functionality, energized by the latest AI from GPT-3.5/4V and augmented by the precision of the LlamaIndex PDF parser and the versatility of Google Maps.\n </p>\n <p>\n  Here\u2019s how our workflow unfolds:\n </p>\n <p>\n  1.\n  <strong>\n   Start with Simplicity:\n  </strong>\n  Users begin by entering their address into our frontend. This action triggers our Google geocoding-powered backend to spring into action, capturing a detailed satellite image of the property in question.\n </p>\n <p>\n  2.\n  <strong>\n   Deciphering the Details:\n  </strong>\n  Next, the journey bifurcates into a dual analysis mode. One path involves parsing through local building codes, a task adeptly handled by GPT-3.5, to unearth specific ADU regulations such as minimum and maximum floor area constraints. Concurrently, GPT-4V meticulously scans the satellite imagery to pinpoint any potential obstructions and delineate the areas ripe for ADU development.\n </p>\n <p>\n  3.\n  <strong>\n   Bringing Plans to Life:\n  </strong>\n  With the viable regions marked, our system embarks on a virtual quest, scouring local builders\u2019 websites for ADU floor plans that not only fit the legal criteria but also the physical realities of your property. These plans are then vividly rendered within the identified buildable zones.\n </p>\n <p>\n  4.\n  <strong>\n   A Click to the Future:\n  </strong>\n  Engaging with a chosen floor plan is as simple as a click, transporting users to the builder\u2019s domain where immersive 3D renderings, pricing details, and more await. To cap it off, our tool thoughtfully prepares data exports to facilitate those all-important initial discussions with ADU builders.\n </p>\n <p>\n  This is more than a tool \u2014 it\u2019s a gateway to turning your backyard into a space of possibility.\n </p>\n <h1>\n  <strong>\n   Flowchart\n  </strong>\n </h1>\n <figure>\n  <figcaption class=\"nn fe no mz na np nq be b bf z dt\">\n   Flowchart\n  </figcaption>\n </figure>\n <h1>\n  <strong>\n   Code Samples\n  </strong>\n </h1>\n <p>\n  <strong>\n   Querying PDFs\n  </strong>\n </p>\n <p>\n  In the following example, we\u2019ll query local ADU building codes for Saratoga, CA. Once you\u2019ve setup your LlamaIndex and OpenAI API keys, you\u2019ll be able to parse the PDFs as follows:\n </p>\n <pre><span class=\"qg or gt qd b bf qh qi l qj qk\" id=\"ff05\"><span class=\"hljs-keyword\">from</span> llama_parse <span class=\"hljs-keyword\">import</span> LlamaParse\nparser = LlamaParse(result_type=<span class=\"hljs-string\">'markdown'</span>)\ndocs = <span class=\"hljs-built_in\">sum</span>([\n  parser.load_data(file) <span class=\"hljs-keyword\">for</span> file <span class=\"hljs-keyword\">in</span> [\n    <span class=\"hljs-string\">'Documents/ADU_FAQ.pdf'</span>,\n    <span class=\"hljs-string\">'Documents/ADU_Handbook.pdf'</span>,\n    <span class=\"hljs-string\">'Documents/SCC-ADU-Guidebook-FINAL-9.8.23.pdf'</span>]], [])</span></pre>\n <p>\n  Then, we can index the documents into an ephemeral, or in-memory,\n  <a href=\"https://www.trychroma.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Chroma DB\n  </a>\n  embeddings store.\n </p>\n <pre><span class=\"qg or gt qd b bf qh qi l qj qk\" id=\"5b82\">chroma_client = chromadb.EphemeralClient()\nchroma_collection = chroma_client.get_or_create_collection('embeddings')\nvector_store = ChromaVectorStore(chroma_collection=chroma_collection)\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex.from_documents(docs, storage_context=storage_context)</span></pre>\n <p>\n  Next, we can integrate the documents into our agentic flow by creating a tool for our agent to query them.\n </p>\n <pre><span class=\"qg or gt qd b bf qh qi l qj qk\" id=\"1ade\">tools = [\n  QueryEngineTool(\n       query_engine=index.as_query_engine(),\n       metadata=ToolMetadata(\n           name='saratoga_adu_codes',\n           description=('Provides information about ADU building codes for the city of Saratoga, CA.'),\n       ),\n   )\n]</span></pre>\n <p>\n  All together, our agent will be able to answer questions about the documents as follows:\n </p>\n <pre><span class=\"qg or gt qd b bf qh qi l qj qk\" id=\"781e\"><span class=\"hljs-keyword\">from</span> llama_index.agent <span class=\"hljs-keyword\">import</span> OpenAIAgent\n\nagent = OpenAIAgent.from_tools(tools, verbose=<span class=\"hljs-literal\">True</span>)\nagent.chat(<span class=\"hljs-string\">\"\"\"\\\nAnswer the following questions regarding Accessory Dwelling Unit (ADU) construction planning in Saratoga, California (CA):\n- What are the typical side and rear setbacks for a detached ADU?\n\"\"\"</span>)\n\n\n&amp;gt;&amp;gt;&amp;gt; <span class=\"hljs-string\">'For detached ADUs in Saratoga, California, the typical side and rear setbacks are a minimum of four feet from the lot lines.'</span></span></pre>\n <p>\n  <strong>\n   Image analysis\n  </strong>\n </p>\n <p>\n  Firstly, we\u2019ll prepare an image and plot it for visual recognization.\n </p>\n <pre><span class=\"qg or gt qd b bf qh qi l qj qk\" id=\"b63c\"><span class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\">from</span> pathlib <span class=\"hljs-keyword\">import</span> Path\n<span class=\"hljs-keyword\">from</span> PIL <span class=\"hljs-keyword\">import</span> Image\n<span class=\"hljs-keyword\">import</span> matplotlib.pyplot <span class=\"hljs-keyword\">as</span> plt\n\ninput_image_path = Path(<span class=\"hljs-string\">\"input_images\"</span>)\n<span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> input_image_path.exists():\n    Path.mkdir(input_image_path)\n\nimage_extensions = [<span class=\"hljs-string\">'.jpg'</span>, <span class=\"hljs-string\">'.jpeg'</span>, <span class=\"hljs-string\">'.png'</span>, <span class=\"hljs-string\">'.gif'</span>, <span class=\"hljs-string\">'.bmp'</span>]\n\n<span class=\"hljs-comment\"># Filter out non-image files</span>\nimage_paths = [<span class=\"hljs-built_in\">str</span>(input_image_path / img_path) <span class=\"hljs-keyword\">for</span> img_path <span class=\"hljs-keyword\">in</span> os.listdir(input_image_path) \n               <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">any</span>(img_path.lower().endswith(ext) <span class=\"hljs-keyword\">for</span> ext <span class=\"hljs-keyword\">in</span> image_extensions)]\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">plot_images</span>(<span class=\"hljs-params\">image_paths</span>):\n    images_shown = <span class=\"hljs-number\">0</span>\n    plt.figure(figsize=(<span class=\"hljs-number\">16</span>, <span class=\"hljs-number\">9</span>))\n    <span class=\"hljs-keyword\">for</span> img_path <span class=\"hljs-keyword\">in</span> image_paths:\n        <span class=\"hljs-keyword\">if</span> os.path.isfile(img_path):\n            image = Image.<span class=\"hljs-built_in\">open</span>(img_path)\n\n            plt.subplot(<span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">3</span>, images_shown + <span class=\"hljs-number\">1</span>)\n            plt.imshow(image)\n            plt.xticks([])\n            plt.yticks([])\n\n            images_shown += <span class=\"hljs-number\">1</span>\n            <span class=\"hljs-keyword\">if</span> images_shown &amp;gt;= <span class=\"hljs-number\">6</span>:  <span class=\"hljs-comment\"># Adjusted to match the subplot dimensions (2,3)</span>\n                <span class=\"hljs-keyword\">break</span>\n\nplot_images(image_paths)</span></pre>\n <p>\n  Secondly, we can do a image analysis and recognize the property layout based on GPT-4V, using the following prompt sample as an example.\n </p>\n <pre><span class=\"qg or gt qd b bf qh qi l qj qk\" id=\"6241\">instruction = '''\nYou are an intelligent Accessory Dwelling Units (ADU) architect designer and contractor.\nPlease analyze the image, describle the layout, and then describe the pool, property, and driveways in the format of:\n1. property: xxx\n2. pool: xxx\n...\n'''</span></pre>\n <pre><span class=\"qg or gt qd b bf qh qi l qj qk\" id=\"eacd\"><span class=\"hljs-keyword\">from</span> llama_index.multi_modal_llms.openai <span class=\"hljs-keyword\">import</span> OpenAIMultiModal\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> SimpleDirectoryReader\n\n<span class=\"hljs-comment\"># put your local directore here</span>\nimage_documents = SimpleDirectoryReader(<span class=\"hljs-string\">\"./input_images\"</span>).load_data()\n\nopenai_mm_llm = OpenAIMultiModal(\n    model=<span class=\"hljs-string\">\"gpt-4-vision-preview\"</span>, api_key=OPENAI_API_TOKEN, max_new_tokens=<span class=\"hljs-number\">1500</span>, temperature = <span class=\"hljs-number\">0.0</span>\n)\n\nresponse = openai_mm_llm.complete(\n    prompt = instruction,\n    image_documents=image_documents,\n)\n\n<span class=\"hljs-built_in\">print</span>(response)</span></pre>\n <pre><span class=\"qg or gt qd b bf qh qi l qj qk\" id=\"4828\">Based on the aerial image provided, here is an analysis of the layout:\n\n<span class=\"hljs-number\">1.</span> Property: The property appears to be a residential lot with a single-family home. The house has a hipped roof with multiple sections, indicating a complex floor plan with possibly several rooms <span class=\"hljs-keyword\">or</span> wings. There is a landscaped area surrounding the house with various trees <span class=\"hljs-keyword\">and</span> shrubs, <span class=\"hljs-keyword\">and</span> the terrain seems to be sloped, as indicated by the terracing on the land.\n\n<span class=\"hljs-number\">2.</span> Pool: There is a kidney-shaped pool located to the northwest of the main house. It is surrounded by a paved area, likely <span class=\"hljs-keyword\">for</span> lounging <span class=\"hljs-keyword\">and</span> poolside activities, <span class=\"hljs-keyword\">and</span> is accessible via a curved pathway that leads from the house to the pool area.\n\n<span class=\"hljs-number\">3.</span> Driveways: It is <span class=\"hljs-keyword\">not</span> entirely clear from the image where the driveway is located, as the specific access point to the property is <span class=\"hljs-keyword\">not</span> visible. However, there seems to be a paved path leading from the bottom right of the image towards the house, which could be the driveway <span class=\"hljs-keyword\">or</span> a walkway. If it is the driveway, it would be located on the southeast side of the property, leading up to the house.\n\nPlease note that without a broader view <span class=\"hljs-keyword\">or</span> additional context, some details about the property, such as the exact location of the driveway <span class=\"hljs-keyword\">or</span> additional structures, may <span class=\"hljs-keyword\">not</span> be accurately determined.</span></pre>\n <h1>\n  <strong>\n   Conclusions\n  </strong>\n </h1>\n <p>\n  Our application markedly simplifies the ADU construction process for homeowners. It adeptly navigates complex tasks such as land surveying and identifying viable floor plans. While ADU vendors need to validate the chosen floor plan and cities must grant construction approval, our tool propels this progression by facilitating comprehensive at-home analysis. Looking ahead, we are poised to officially launch this application and forge partnerships with reputable ADU vendors, enhancing oversight in modular and prefabricated home construction. We are committed to personalizing our service further by adapting our recommendations to align with individual budget constraints and specific layout preferences, ensuring that our users\u2019 visions for their homes are realized with precision and care.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 14701, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2862724-1e15-4d32-bd5b-11992d50b4f7": {"__data__": {"id_": "b2862724-1e15-4d32-bd5b-11992d50b4f7", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.html", "file_name": "querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.html", "file_type": "text/html", "file_size": 13717, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.html", "file_name": "querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.html", "file_type": "text/html", "file_size": 13717, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "0cb5d6286e5bb51298885ca672ae003cb5705bba007b9da4948419d7de62badb", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The main premise behind RAG is the injection of context (or knowledge) to the LLM in order to yield more accurate responses from it. As such, a crucial component to a RAG system is the data source from which it gets its knowledge. It\u2019s intuitive then to reason that the more knowledge that the RAG system can tap into, the better it would ultimately become (in terms of answering queries of potentially both depth and breadth). The spirit of this concept is not so different from that found in basically every other data-driven discipline \u2014 access to more (good) data, that is subsequently and effectively used, usually leads to better outcomes.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  It is with that backdrop, that we\u2019re thrilled to announce the release of our newest latest\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   llama-index\n  </code>\n  library extension, called\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   llama-index-networks\n  </code>\n  . This library extension makes it possible to build a network of RAGs built on top of external data sources and supplied by external actors. This new network paradigm allows for a new way for data suppliers to provide their data to those who want it in order to build more knowledgeable systems!\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In this blog post, we\u2019ll introduce the main classes of the new extension library and show you how in just a couple lines of code, you can make your QueryEngine ready to contribute as part of a network of RAGs. We\u2019ll also share ideas of what this can mean for how data suppliers actually supply their data to consumers within this new era of LLMs.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  A note on terminology: in this post, we use\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   llama-index-networks\n  </code>\n  to refer to the actual extension, whereas\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   llama-index[networks]\n  </code>\n  refers to an installation of\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   llama-index\n  </code>\n  that comes with the\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   llama-index-networks\n  </code>\n  extension.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  The story of Alex, Beth, and Bob\n </h2>\n <figure>\n  <figcaption>\n   An illustrative example of actors in a network and their problem statements.\n  </figcaption>\n </figure>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  To illustrate how the llama-index-networks package can be used, we consider three made-up characters, Alex, Bob, and Beth, and the following scenario:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Both Bob and Beth each have their own set of documents and both have already built quite the outstanding RAG systems over them (using llama-index of course!)\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Alex has heard about these insightful documents that both Bob and Beth have and would like to be able to query the individual RAGs built over the both of them.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Bob and Beth, being as kind as they are (or, perhaps they were paid some undisclosed dollar amount), agree to give access to Alex to their RAG systems.\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  To facilitate this new fashion of knowledge exchange, they agree to setup a network of RAGs that Alex can query.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Bob and Beth build a web service over their RAGs\n </h2>\n <figure>\n  <figcaption>\n   ContributorService is built around a QueryEngine.\n  </figcaption>\n </figure>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  With the\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   llama-index-networks\n  </code>\n  package, Bob and Beth can make their respective\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   QueryEngine\n  </code>\n  \u2019s ready to participate in the network with only a few lines of code.\n </p>\n <pre><code><span class=\"hljs-string\">\"\"\"Beth's contributor service file.\n\nBeth builds her QueryEngine and exposes it behind the standard\nLlamaIndex Network Contributor Service. \n\nNOTE: Bob would probably make use of Docker and cloud \ncompute services to make this production grade.\n\"\"\"</span>\n\n<span class=\"hljs-keyword\">from</span> llama_index.networks.contributor <span class=\"hljs-keyword\">import</span> ContributorService\n<span class=\"hljs-keyword\">import</span> uvicorn\n\nbeth_query_engine = ...\nbeth_contributor_service = ContributorService.from_config_file(\n    <span class=\"hljs-string\">\".env.contributor\"</span>,  <span class=\"hljs-comment\"># settings/secrets for the service</span>\n    beth_query_engine\n)\n\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__:\n    uvicorn.run(beth_contributor_service.app, port=8000)</span></code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Bob would use similar lines of code to make his\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   QueryEngine\n  </code>\n  ready to contribute to any LlamaIndex network. Note, that the dotenv file\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   .env.contributor\n  </code>\n  contains settings for the service as well as any necessary api keys (e.g.,\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   OPENAI_API_KEY\n  </code>\n  or\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   ANTHROPIC_API_KEY\n  </code>\n  ), which under the hood is implemented as REST service using FastAPI.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Alex builds a NetworkQueryEngine\n </h2>\n <figure>\n  <figcaption>\n   Alex builds a NetworkQueryEngine that connects to Beth and Bob\u2019s individual ContributorService\u2019s.\n  </figcaption>\n </figure>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  For Alex\u2019s part, he uses the\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   NetworkQueryEngine\n  </code>\n  class of the\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   llama-index-networks\n  </code>\n  extension to be able to connect to both Beth and Bob\u2019s\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   ContributorService\n  </code>\n  \u2019s.\n </p>\n <pre><code><span class=\"hljs-string\">\"\"\"Alex's network query engine.\n\nAlex builds a NetworkQueryEngine to connect to a \nlist of ContributorService\u2019s.\n\"\"\"</span>\n\n<span class=\"hljs-keyword\">from</span> llama_index.networks.contributor <span class=\"hljs-keyword\">import</span> ContributorClient\n<span class=\"hljs-keyword\">from</span> llama_index.networks.query_engine <span class=\"hljs-keyword\">import</span> NetworkQueryEngine\n<span class=\"hljs-keyword\">from</span> llama_index.llms.groq <span class=\"hljs-keyword\">import</span> Groq\n\n<span class=\"hljs-comment\"># Use ContributorClient to connect to a ContributorService</span>\nbeth_client = ContributorClient.from_config_file(\n    env_file=<span class=\"hljs-string\">\".env.beth_contributor.client\"</span>\n)\nbob_client = ContributorClient.from_config_file(\n    env_file=<span class=\"hljs-string\">\".env.bob_contributor.client\"</span>\n)\ncontributors = [beth_client, bob_client]\n\n<span class=\"hljs-comment\"># NetworkQueryEngine</span>\nllm = Groq()\nnetwork_query_engine = NetworkQueryEngine.from_args(\n    contributors=contributors,\n    llm=llm\n)\n\n<span class=\"hljs-comment\"># Can query it like any other query engine</span>\nnetwork_query_engine.query(<span class=\"hljs-string\">\"Why is the sky blue?\"</span>)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Here the dotenv files store the service parameters such as an\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   api_url\n  </code>\n  required to connect to the\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   ContributorService\n  </code>\n  .\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Before moving on to the next section of this blog post, we\u2019ll take the next few lines to explain a bit of what\u2019s involved under the hood when Alex query\u2019s his\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   NetworkQueryEngine\n  </code>\n  . When the\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   query()\n  </code>\n  method is invoked (Async is also supported!), the query is sent to all contributors. Their responses are stored as new\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   Nodes\n  </code>\n  and a response is synthesized (with the associated\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   ResponseSynthesizer\n  </code>\n  ). After reading that, some may notice that this is the usual pattern when working with our\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   QueryEngine\n  </code>\n  abstractions; and, that indeed was the point. Using a\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   NetworkQueryEngine\n  </code>\n  should be very similar to how you would use any other\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   QueryEngine\n  </code>\n  in our library!\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  This marks the end of our little story about Alex, Bob and Beth. Before wrapping up this blog post, we first provide a few potential opportunities that may arise through the usage of\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   llama-index-networks\n  </code>\n  .\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  A new world of data supply and consumption\n </h2>\n <figure>\n  <figcaption>\n   RAG marketplaces is a use case that can be made possible with llama-index[networks].\n  </figcaption>\n </figure>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  One possible world that could easily be powered by\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   llama-index[networks]\n  </code>\n  are marketplaces for RAG. A place where data suppliers package their data in the form of RAGs to data consumers look to expand their own query system\u2019s knowledge. Potential RAG (data) suppliers could be your local newspaper, book publishing companies, etc.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In this new data supply and consumption model, there is more onus on the data supplier to prepare the data in a fashion that makes it easier to consume \u2014 specifically, the suppliers own the building of the query engine. This should benefit the data consumer greatly since, with a standard interface that is provided by the likes of a\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   ContributorService\n  </code>\n  (that encapsulates a\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   QueryEngine\n  </code>\n  ), they can get access to the knowledge they seek from the data easier than ever before (i.e., in relation to traditional data marketplaces that exchange raw data).\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  It is with this kind of vision that we\u2019ve built\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   llama-index[networks]\n  </code>\n  to make it: (i) easier for data suppliers to supply the knowledge contained in their data in new and arguably more effective ways, and (ii) easier for data consumers to connect to these new forms of external knowledge.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Internal networks: another potential use case\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In addition to powering RAG marketplaces, we foresee the need of connecting RAGs that an overarching company may own, but not necessarily manage. More concretely, a franchise may have the rights to the data across all of its operations. And, while they could build a \u201ccentral\u201d, monolithic RAG over the entire database, it may still be more efficient and effective to build RAGs over the individual operators and query these instead.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The idea of exchanging information to build better, more knowledgeable systems is not new. However, the idea of using RAGs to facilitate that exchange may be (to our knowledge, it is), and we believe that both existing and new use cases requiring such collaboration can benefit from concept.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  A quick note on privacy\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  An important consideration in the collaboration of data is privacy and security. It bears mentioning that the examples above assume that the data that is being shared across the network is in compliance with data privacy and security laws and regulations. It is our belief that as this technology grows, that the necessary features and capabilities will be developed and incorporated to facilitate in-compliance RAG networks.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Check out the demo to learn more!\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  To see an actual demo of a network connecting to a set of contributors, check out the Github repository\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/tree/main/llama-index-networks\" rel=\"noreferrer noopener\">\n   code\n  </a>\n  for\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   llama-index-networks\n  </code>\n  and navigate to the\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   examples/demo\n  </code>\n  subfolder.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 13676, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d336599-7330-4600-95eb-3b84a5276fe7": {"__data__": {"id_": "2d336599-7330-4600-95eb-3b84a5276fe7", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.html", "file_name": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.html", "file_type": "text/html", "file_size": 33310, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.html", "file_name": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.html", "file_type": "text/html", "file_size": 33310, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "0e7a691e8160b7e8388ad76f8306d998104a86f4410a92e65222a786f2bc6c8b", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Unlocking the power of AI should be as intuitive as using your favorite apps. That\u2019s the philosophy behind RAGArch, my latest creation designed to demystify and streamline the process of setting up Retrieval-Augmented Generation (RAG) pipelines. This tool is born from a simple vision: to provide a straightforward, no-code platform that empowers both seasoned developers and curious explorers in the world of AI to craft, test, and implement RAG pipelines with confidence and ease.\n </p>\n <h1>\n  Features\n </h1>\n <p>\n  RAGArch leverages LlamaIndex\u2019s powerful LLM orchestration capabilities, to provide a seamless experience and granular control over your RAG pipeline.\n </p>\n <ul>\n  <li>\n   <strong>\n    Intuitive Interface:\n   </strong>\n   RAGArch\u2019s user-friendly interface, built with Streamlit, allows you to test different RAG pipeline components interactively.\n  </li>\n  <li>\n   <strong>\n    Custom Configuration\n   </strong>\n   : The app provides a wide range of options to configure Language Models, Embedding Models, Node Parsers, Response Synthesis Methods, and Vector Stores to suit your project\u2019s needs.\n  </li>\n  <li>\n   <strong>\n    Live Testing:\n   </strong>\n   Instantly test your RAG pipeline with your own data and see how different configurations affect the outcome.\n  </li>\n  <li>\n   <strong>\n    One-Click Code Generation\n   </strong>\n   : Once you\u2019re satisfied with the configuration, the app can generate the Python code for your custom RAG pipeline, ready to be integrated into your application.\n  </li>\n </ul>\n <h1>\n  Tools and Technologies\n </h1>\n <p>\n  The creation of RAGArch was made possible by integrating a variety of powerful tools and technologies:\n </p>\n <ul>\n  <li>\n   <strong>\n    UI:\n   </strong>\n   Streamlit\n  </li>\n  <li>\n   <strong>\n    Hosting:\n   </strong>\n   Hugging Face Spaces\n  </li>\n  <li>\n   <strong>\n    LLMs:\n   </strong>\n   OpenAI GPT 3.5 and 4, Cohere API, Gemini Pro\n  </li>\n  <li>\n   <strong>\n    LLM Orchestration:\n   </strong>\n   Llamaindex\n  </li>\n  <li>\n   <strong>\n    Embedding Models:\n   </strong>\n   \u201cBAAI/bge-small-en-v1.5\u201d, \u201cWhereIsAI/UAE-Large-V1\u201d, \u201cBAAI/bge-large-en-v1.5\u201d, \u201ckhoa-klaytn/bge-small-en-v1.5-angle\u201d, \u201cBAAI/bge-base-en-v1.5\u201d, \u201cllmrails/ember-v1\u201d, \u201cjamesgpt1/sf_model_e5\u201d, \u201cthenlper/gte-large\u201d, \u201cinfgrad/stella-base-en-v2\u201d and \u201cthenlper/gte-base\u201d\n  </li>\n  <li>\n   <strong>\n    Vector Stores:\n   </strong>\n   Simple (Llamaindex default), Pinecone and Qdrant\n  </li>\n </ul>\n <h1>\n  Deep Dive into the Code\n </h1>\n <p>\n  The\n  <code class=\"cw qa qb qc qd b\">\n   app.py\n  </code>\n  script is the backbone of RAGArch, integrating various components to provide a cohesive experience. The following are the key functions of app.py\n </p>\n <h2>\n  <code class=\"cw qa qb qc qd b\">\n   <strong>\n    upload_file\n   </strong>\n  </code>\n </h2>\n <p>\n  This function manages file uploads and uses Llamaindex's\n  <code class=\"cw qa qb qc qd b\">\n   SimpleDirectoryReader\n  </code>\n  to load documents into the system. It supports a wide array of document types, including PDFs, text files, HTML, JSON files, and more, making it versatile for processing diverse data sources.\n </p>\n <pre><span class=\"qx on gt qd b bf qy qz l ra rb\" id=\"891b\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">upload_file</span>():\n    file = st.file_uploader(<span class=\"hljs-string\">\"Upload a file\"</span>, on_change=reset_pipeline_generated)\n    <span class=\"hljs-keyword\">if</span> file <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n        file_path = save_uploaded_file(file)\n        \n        <span class=\"hljs-keyword\">if</span> file_path:\n            loaded_file = SimpleDirectoryReader(input_files=[file_path]).load_data()\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Total documents: <span class=\"hljs-subst\">{<span class=\"hljs-built_in\">len</span>(loaded_file)}</span>\"</span>)\n\n            st.success(<span class=\"hljs-string\">f\"File uploaded successfully. Total documents loaded: <span class=\"hljs-subst\">{<span class=\"hljs-built_in\">len</span>(loaded_file)}</span>\"</span>)\n        <span class=\"hljs-keyword\">return</span> loaded_file\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">None</span></span></pre>\n <h2>\n  save_uploaded_file\n </h2>\n <p>\n  This utility function saves the uploaded file to a temporary location on the server, making it accessible for further processing. It\u2019s a crucial part of the file handling process, ensuring data integrity and availability.\n </p>\n <pre><span class=\"qx on gt qd b bf qy qz l ra rb\" id=\"d86e\">def <span class=\"hljs-title function_\">save_uploaded_file</span>(uploaded_file):\n    <span class=\"hljs-attr\">try</span>:\n        <span class=\"hljs-keyword\">with</span> tempfile.<span class=\"hljs-title class_\">NamedTemporaryFile</span>(<span class=\"hljs-keyword\">delete</span>=<span class=\"hljs-title class_\">False</span>, suffix=os.<span class=\"hljs-property\">path</span>.<span class=\"hljs-title function_\">splitext</span>(uploaded_file.<span class=\"hljs-property\">name</span>)[<span class=\"hljs-number\">1</span>]) <span class=\"hljs-keyword\">as</span> <span class=\"hljs-attr\">tmp_file</span>:\n            tmp_file.<span class=\"hljs-title function_\">write</span>(uploaded_file.<span class=\"hljs-title function_\">getvalue</span>())\n            <span class=\"hljs-keyword\">return</span> tmp_file.<span class=\"hljs-property\">name</span>\n    except <span class=\"hljs-title class_\">Exception</span> <span class=\"hljs-keyword\">as</span> <span class=\"hljs-attr\">e</span>:\n        st.<span class=\"hljs-title function_\">error</span>(f<span class=\"hljs-string\">\"Error saving file: {e}\"</span>)\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-title class_\">None</span></span></pre>\n <h2>\n  <code class=\"cw qa qb qc qd b\">\n   <strong>\n    select_llm\n   </strong>\n  </code>\n </h2>\n <p>\n  Allows users to select a Large Language Model and initializes it for use. You can choose from Google\u2019s Gemini Pro, Cohere, OpenAI\u2019s GPT 3.5 and GPT 4.\n </p>\n <pre><span class=\"qx on gt qd b bf qy qz l ra rb\" id=\"c7d6\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">select_llm</span>():\n    st.header(<span class=\"hljs-string\">\"Choose LLM\"</span>)\n    llm_choice = st.selectbox(<span class=\"hljs-string\">\"Select LLM\"</span>, [<span class=\"hljs-string\">\"Gemini\"</span>, <span class=\"hljs-string\">\"Cohere\"</span>, <span class=\"hljs-string\">\"GPT-3.5\"</span>, <span class=\"hljs-string\">\"GPT-4\"</span>], on_change=reset_pipeline_generated)\n    \n    <span class=\"hljs-keyword\">if</span> llm_choice == <span class=\"hljs-string\">\"GPT-3.5\"</span>:\n        llm = OpenAI(temperature=<span class=\"hljs-number\">0.1</span>, model=<span class=\"hljs-string\">\"gpt-3.5-turbo-1106\"</span>)\n        st.write(<span class=\"hljs-string\">f\"<span class=\"hljs-subst\">{llm_choice}</span> selected\"</span>)\n    <span class=\"hljs-keyword\">elif</span> llm_choice == <span class=\"hljs-string\">\"GPT-4\"</span>:\n        llm = OpenAI(temperature=<span class=\"hljs-number\">0.1</span>, model=<span class=\"hljs-string\">\"gpt-4-1106-preview\"</span>)\n        st.write(<span class=\"hljs-string\">f\"<span class=\"hljs-subst\">{llm_choice}</span> selected\"</span>)\n    <span class=\"hljs-keyword\">elif</span> llm_choice == <span class=\"hljs-string\">\"Gemini\"</span>:\n        llm = Gemini(model=<span class=\"hljs-string\">\"models/gemini-pro\"</span>)\n        st.write(<span class=\"hljs-string\">f\"<span class=\"hljs-subst\">{llm_choice}</span> selected\"</span>)\n    <span class=\"hljs-keyword\">elif</span> llm_choice == <span class=\"hljs-string\">\"Cohere\"</span>:\n        llm = Cohere(model=<span class=\"hljs-string\">\"command\"</span>, api_key=os.environ[<span class=\"hljs-string\">'COHERE_API_TOKEN'</span>])\n        st.write(<span class=\"hljs-string\">f\"<span class=\"hljs-subst\">{llm_choice}</span> selected\"</span>)\n    <span class=\"hljs-keyword\">return</span> llm, llm_choice</span></pre>\n <h2>\n  <code class=\"cw qa qb qc qd b\">\n   select_embedding_model\n  </code>\n </h2>\n <p>\n  Offers a dropdown for users to select the embedding model of their choice from a predefined list. I have included some of the top embedding models from Hugging Face\u2019s MTEB leaderboard. Near the dropdown I have also included a handy link to the leaderboard where users can get more information about the embedding models.\n </p>\n <pre><span class=\"qx on gt qd b bf qy qz l ra rb\" id=\"6c63\">def <span class=\"hljs-title function_\">select_embedding_model</span>():\n    st.<span class=\"hljs-title function_\">header</span>(<span class=\"hljs-string\">\"Choose Embedding Model\"</span>)\n    col1, col2 = st.<span class=\"hljs-title function_\">columns</span>([<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">1</span>])\n    <span class=\"hljs-keyword\">with</span> <span class=\"hljs-attr\">col2</span>:\n        st.<span class=\"hljs-title function_\">markdown</span>(<span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n                    [Embedding Models Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n                    \"</span><span class=\"hljs-string\">\"\"</span>)\n    model_names = [\n        <span class=\"hljs-string\">\"BAAI/bge-small-en-v1.5\"</span>,\n        <span class=\"hljs-string\">\"WhereIsAI/UAE-Large-V1\"</span>,\n        <span class=\"hljs-string\">\"BAAI/bge-large-en-v1.5\"</span>,\n        <span class=\"hljs-string\">\"khoa-klaytn/bge-small-en-v1.5-angle\"</span>,\n        <span class=\"hljs-string\">\"BAAI/bge-base-en-v1.5\"</span>,\n        <span class=\"hljs-string\">\"llmrails/ember-v1\"</span>,\n        <span class=\"hljs-string\">\"jamesgpt1/sf_model_e5\"</span>,\n        <span class=\"hljs-string\">\"thenlper/gte-large\"</span>,\n        <span class=\"hljs-string\">\"infgrad/stella-base-en-v2\"</span>,\n        <span class=\"hljs-string\">\"thenlper/gte-base\"</span>\n    ]\n    selected_model = st.<span class=\"hljs-title function_\">selectbox</span>(<span class=\"hljs-string\">\"Select Embedding Model\"</span>, model_names,  on_change=reset_pipeline_generated)\n    <span class=\"hljs-keyword\">with</span> st.<span class=\"hljs-title function_\">spinner</span>(<span class=\"hljs-string\">\"Please wait\"</span>) <span class=\"hljs-keyword\">as</span> <span class=\"hljs-attr\">status</span>:\n        embed_model = <span class=\"hljs-title class_\">HuggingFaceEmbedding</span>(model_name=selected_model)\n        st.<span class=\"hljs-property\">session_state</span>[<span class=\"hljs-string\">'embed_model'</span>] = embed_model\n        st.<span class=\"hljs-title function_\">markdown</span>(F<span class=\"hljs-string\">\"Embedding Model: {embed_model.model_name}\"</span>)\n        st.<span class=\"hljs-title function_\">markdown</span>(F<span class=\"hljs-string\">\"Embed Batch Size: {embed_model.embed_batch_size}\"</span>)\n        st.<span class=\"hljs-title function_\">markdown</span>(F<span class=\"hljs-string\">\"Embed Batch Size: {embed_model.max_length}\"</span>)\n\n\n    <span class=\"hljs-keyword\">return</span> embed_model, selected_model</span></pre>\n <h2>\n  select_node_parser Function\n </h2>\n <p>\n  This function allows users to choose a node parser, which is instrumental in breaking down documents into manageable chunks or nodes, facilitating better handling and processing. I have included some of the most commonly used node parsers supported by Llamaindex, which include SentenceSplitter, CodeSplitter, SemanticSplitterNodeParser, TokenTextSplitter, HTMLNodeParser, JSONNodeParser and MarkdownNodeParser.\n </p>\n <pre><span class=\"qx on gt qd b bf qy qz l ra rb\" id=\"756c\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">select_node_parser</span>():\n    st.header(<span class=\"hljs-string\">\"Choose Node Parser\"</span>)\n    col1, col2 = st.columns([<span class=\"hljs-number\">4</span>,<span class=\"hljs-number\">1</span>])\n    <span class=\"hljs-keyword\">with</span> col2:\n        st.markdown(<span class=\"hljs-string\">\"\"\"\n                    [More Information](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/root.html)\n                    \"\"\"</span>)\n    parser_types = [<span class=\"hljs-string\">\"SentenceSplitter\"</span>, <span class=\"hljs-string\">\"CodeSplitter\"</span>, <span class=\"hljs-string\">\"SemanticSplitterNodeParser\"</span>,\n                    <span class=\"hljs-string\">\"TokenTextSplitter\"</span>, <span class=\"hljs-string\">\"HTMLNodeParser\"</span>, <span class=\"hljs-string\">\"JSONNodeParser\"</span>, <span class=\"hljs-string\">\"MarkdownNodeParser\"</span>]\n    parser_type = st.selectbox(<span class=\"hljs-string\">\"Select Node Parser\"</span>, parser_types, on_change=reset_pipeline_generated)\n    \n    parser_params = {}\n    <span class=\"hljs-keyword\">if</span> parser_type == <span class=\"hljs-string\">\"HTMLNodeParser\"</span>:\n        tags = st.text_input(<span class=\"hljs-string\">\"Enter tags separated by commas\"</span>, <span class=\"hljs-string\">\"p, h1\"</span>)\n        tag_list = tags.split(<span class=\"hljs-string\">','</span>)\n        parser = HTMLNodeParser(tags=tag_list)\n        parser_params = {<span class=\"hljs-string\">'tags'</span>: tag_list}\n        \n    <span class=\"hljs-keyword\">elif</span> parser_type == <span class=\"hljs-string\">\"JSONNodeParser\"</span>:\n        parser = JSONNodeParser()\n        \n    <span class=\"hljs-keyword\">elif</span> parser_type == <span class=\"hljs-string\">\"MarkdownNodeParser\"</span>:\n        parser = MarkdownNodeParser()\n        \n    <span class=\"hljs-keyword\">elif</span> parser_type == <span class=\"hljs-string\">\"CodeSplitter\"</span>:\n        language = st.text_input(<span class=\"hljs-string\">\"Language\"</span>, <span class=\"hljs-string\">\"python\"</span>)\n        chunk_lines = st.number_input(<span class=\"hljs-string\">\"Chunk Lines\"</span>, min_value=<span class=\"hljs-number\">1</span>, value=<span class=\"hljs-number\">40</span>)\n        chunk_lines_overlap = st.number_input(<span class=\"hljs-string\">\"Chunk Lines Overlap\"</span>, min_value=<span class=\"hljs-number\">0</span>, value=<span class=\"hljs-number\">15</span>)\n        max_chars = st.number_input(<span class=\"hljs-string\">\"Max Chars\"</span>, min_value=<span class=\"hljs-number\">1</span>, value=<span class=\"hljs-number\">1500</span>)\n        parser = CodeSplitter(language=language, chunk_lines=chunk_lines, chunk_lines_overlap=chunk_lines_overlap, max_chars=max_chars)\n        parser_params = {<span class=\"hljs-string\">'language'</span>: language, <span class=\"hljs-string\">'chunk_lines'</span>: chunk_lines, <span class=\"hljs-string\">'chunk_lines_overlap'</span>: chunk_lines_overlap, <span class=\"hljs-string\">'max_chars'</span>: max_chars}\n        \n    <span class=\"hljs-keyword\">elif</span> parser_type == <span class=\"hljs-string\">\"SentenceSplitter\"</span>:\n        chunk_size = st.number_input(<span class=\"hljs-string\">\"Chunk Size\"</span>, min_value=<span class=\"hljs-number\">1</span>, value=<span class=\"hljs-number\">1024</span>)\n        chunk_overlap = st.number_input(<span class=\"hljs-string\">\"Chunk Overlap\"</span>, min_value=<span class=\"hljs-number\">0</span>, value=<span class=\"hljs-number\">20</span>)\n        parser = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n        parser_params = {<span class=\"hljs-string\">'chunk_size'</span>: chunk_size, <span class=\"hljs-string\">'chunk_overlap'</span>: chunk_overlap}\n        \n    <span class=\"hljs-keyword\">elif</span> parser_type == <span class=\"hljs-string\">\"SemanticSplitterNodeParser\"</span>:\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-string\">'embed_model'</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span> st.session_state:\n            st.warning(<span class=\"hljs-string\">\"Please select an embedding model first.\"</span>)\n            <span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">None</span>, <span class=\"hljs-literal\">None</span>\n        \n        embed_model = st.session_state[<span class=\"hljs-string\">'embed_model'</span>]\n        buffer_size = st.number_input(<span class=\"hljs-string\">\"Buffer Size\"</span>, min_value=<span class=\"hljs-number\">1</span>, value=<span class=\"hljs-number\">1</span>)\n        breakpoint_percentile_threshold = st.number_input(<span class=\"hljs-string\">\"Breakpoint Percentile Threshold\"</span>, min_value=<span class=\"hljs-number\">0</span>, max_value=<span class=\"hljs-number\">100</span>, value=<span class=\"hljs-number\">95</span>)\n        parser = SemanticSplitterNodeParser(buffer_size=buffer_size, breakpoint_percentile_threshold=breakpoint_percentile_threshold, embed_model=embed_model)\n        parser_params = {<span class=\"hljs-string\">'buffer_size'</span>: buffer_size, <span class=\"hljs-string\">'breakpoint_percentile_threshold'</span>: breakpoint_percentile_threshold}\n        \n    <span class=\"hljs-keyword\">elif</span> parser_type == <span class=\"hljs-string\">\"TokenTextSplitter\"</span>:\n        chunk_size = st.number_input(<span class=\"hljs-string\">\"Chunk Size\"</span>, min_value=<span class=\"hljs-number\">1</span>, value=<span class=\"hljs-number\">1024</span>)\n        chunk_overlap = st.number_input(<span class=\"hljs-string\">\"Chunk Overlap\"</span>, min_value=<span class=\"hljs-number\">0</span>, value=<span class=\"hljs-number\">20</span>)\n        parser = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n        parser_params = {<span class=\"hljs-string\">'chunk_size'</span>: chunk_size, <span class=\"hljs-string\">'chunk_overlap'</span>: chunk_overlap}\n\n    <span class=\"hljs-comment\"># Save the parser type and parameters to the session state</span>\n    st.session_state[<span class=\"hljs-string\">'node_parser_type'</span>] = parser_type\n    st.session_state[<span class=\"hljs-string\">'node_parser_params'</span>] = parser_params\n    \n    <span class=\"hljs-keyword\">return</span> parser, parser_type</span></pre>\n <p>\n  Below the node parser selection, I have also included a preview of the first node of the text after splitting/parsing, just to give the users an idea of how the chunking is actually happening based the selected node parser and the relevant parameters.\n </p>\n <h2>\n  <code class=\"cw qa qb qc qd b\">\n   select_response_synthesis_method\n  </code>\n </h2>\n <p>\n  This function allows users to choose how the RAG pipeline synthesizes responses. I have included varioud response synthesis methods supported by Llamaindex including\n  <em class=\"re\">\n   refine\n  </em>\n  ,\n  <em class=\"re\">\n   tree_summarize\n  </em>\n  ,\n  <em class=\"re\">\n   compact\n  </em>\n  ,\n  <em class=\"re\">\n   simple_summarize\n  </em>\n  ,\n  <em class=\"re\">\n   accumulate\n  </em>\n  and\n  <em class=\"re\">\n   compact_accumulate.\n  </em>\n </p>\n <p>\n  Users can click on the more information link to get more details about response synthesis and the different types.\n </p>\n <pre><span class=\"qx on gt qd b bf qy qz l ra rb\" id=\"d551\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">select_response_synthesis_method</span>():\n    st.header(<span class=\"hljs-string\">\"Choose Response Synthesis Method\"</span>)\n    col1, col2 = st.columns([<span class=\"hljs-number\">4</span>,<span class=\"hljs-number\">1</span>])\n    <span class=\"hljs-keyword\">with</span> col2:\n        st.markdown(<span class=\"hljs-string\">\"\"\"\n                    [More Information](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/response_synthesizers.html)\n                    \"\"\"</span>)\n    response_modes = [\n        <span class=\"hljs-string\">\"refine\"</span>,\n        <span class=\"hljs-string\">\"tree_summarize\"</span>,  \n        <span class=\"hljs-string\">\"compact\"</span>, \n        <span class=\"hljs-string\">\"simple_summarize\"</span>, \n        <span class=\"hljs-string\">\"accumulate\"</span>, \n        <span class=\"hljs-string\">\"compact_accumulate\"</span>\n    ]\n    selected_mode = st.selectbox(<span class=\"hljs-string\">\"Select Response Mode\"</span>, response_modes, on_change=reset_pipeline_generated)\n    response_mode = selected_mode\n    <span class=\"hljs-keyword\">return</span> response_mode, selected_mode</span></pre>\n <h2>\n  <code class=\"cw qa qb qc qd b\">\n   select_vector_store\n  </code>\n </h2>\n <p>\n  Enables users to choose a vector store, which is a critical component for storing and retrieving embeddings in the RAG pipeline. This function supports the selection from multiple vector store options including Simple (Llamaindex default), Pinecone and Qdrant.\n </p>\n <pre><span class=\"qx on gt qd b bf qy qz l ra rb\" id=\"3b35\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">select_vector_store</span>():\n    st.header(<span class=\"hljs-string\">\"Choose Vector Store\"</span>)\n    vector_stores = [<span class=\"hljs-string\">\"Simple\"</span>, <span class=\"hljs-string\">\"Pinecone\"</span>, <span class=\"hljs-string\">\"Qdrant\"</span>]\n    selected_store = st.selectbox(<span class=\"hljs-string\">\"Select Vector Store\"</span>, vector_stores, on_change=reset_pipeline_generated)\n\n    vector_store = <span class=\"hljs-literal\">None</span>\n    <span class=\"hljs-keyword\">if</span> selected_store == <span class=\"hljs-string\">\"Pinecone\"</span>:\n        pc = Pinecone(api_key=os.environ[<span class=\"hljs-string\">'PINECONE_API_KEY'</span>])\n        index = pc.Index(<span class=\"hljs-string\">\"test\"</span>)\n        vector_store = PineconeVectorStore(pinecone_index=index)\n    <span class=\"hljs-keyword\">elif</span> selected_store == <span class=\"hljs-string\">\"Qdrant\"</span>:\n        client = qdrant_client.QdrantClient(location=<span class=\"hljs-string\">\":memory:\"</span>)\n        vector_store = QdrantVectorStore(client=client, collection_name=<span class=\"hljs-string\">\"sampledata\"</span>)\n    st.write(selected_store)\n    <span class=\"hljs-keyword\">return</span> vector_store, selected_store</span></pre>\n <h2>\n  generate_rag_pipeline Function\n </h2>\n <p>\n  This core function ties together the selected components to generate a RAG pipeline. It initializes the pipeline with the chosen LLM, embedding model, node parser, response synthesis method, and vector store. It is triggered by pressing the \u2018Generate RAG Pipeline\u2019 button.\n </p>\n <pre><span class=\"qx on gt qd b bf qy qz l ra rb\" id=\"0fa5\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">generate_rag_pipeline</span>(<span class=\"hljs-params\">file, llm, embed_model, node_parser, response_mode, vector_store</span>):\n    <span class=\"hljs-keyword\">if</span> vector_store <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n        <span class=\"hljs-comment\"># Set storage context if vector_store is not None</span>\n        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n    <span class=\"hljs-keyword\">else</span>:\n        storage_context = <span class=\"hljs-literal\">None</span>\n\n    <span class=\"hljs-comment\"># Create the service context</span>\n    service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, node_parser=node_parser)\n\n    <span class=\"hljs-comment\"># Create the vector index</span>\n    vector_index = VectorStoreIndex.from_documents(documents=file, storage_context=storage_context, service_context=service_context, show_progress=<span class=\"hljs-literal\">True</span>)\n    <span class=\"hljs-keyword\">if</span> storage_context:\n        vector_index.storage_context.persist(persist_dir=<span class=\"hljs-string\">\"persist_dir\"</span>)\n\n    <span class=\"hljs-comment\"># Create the query engine</span>\n    query_engine = vector_index.as_query_engine(\n        response_mode=response_mode,\n        verbose=<span class=\"hljs-literal\">True</span>,\n    )\n\n    <span class=\"hljs-keyword\">return</span> query_engine</span></pre>\n <h2>\n  generate_code_snippet Function\n </h2>\n <p>\n  This function is the culmination of the user\u2019s selections, generating the Python code necessary to implement the configured RAG pipeline. It dynamically constructs the code snippet based on the chosen LLM, embedding model, node parser, response synthesis method, and vector store, including the parameters set for the node parser.\n </p>\n <pre><span class=\"qx on gt qd b bf qy qz l ra rb\" id=\"b12e\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">generate_code_snippet</span>(<span class=\"hljs-params\">llm_choice, embed_model_choice, node_parser_choice, response_mode, vector_store_choice</span>):\n    node_parser_params = st.session_state.get(<span class=\"hljs-string\">'node_parser_params'</span>, {})\n    <span class=\"hljs-built_in\">print</span>(node_parser_params)\n    code_snippet = <span class=\"hljs-string\">\"from llama_index.llms import OpenAI, Gemini, Cohere\\n\"</span>\n    code_snippet += <span class=\"hljs-string\">\"from llama_index.embeddings import HuggingFaceEmbedding\\n\"</span>\n    code_snippet += <span class=\"hljs-string\">\"from llama_index import ServiceContext, VectorStoreIndex, StorageContext\\n\"</span>\n    code_snippet += <span class=\"hljs-string\">\"from llama_index.node_parser import SentenceSplitter, CodeSplitter, SemanticSplitterNodeParser, TokenTextSplitter\\n\"</span>\n    code_snippet += <span class=\"hljs-string\">\"from llama_index.node_parser.file import HTMLNodeParser, JSONNodeParser, MarkdownNodeParser\\n\"</span>\n    code_snippet += <span class=\"hljs-string\">\"from llama_index.vector_stores import MilvusVectorStore, QdrantVectorStore\\n\"</span>\n    code_snippet += <span class=\"hljs-string\">\"import qdrant_client\\n\\n\"</span>\n\n    <span class=\"hljs-comment\"># LLM initialization</span>\n    <span class=\"hljs-keyword\">if</span> llm_choice == <span class=\"hljs-string\">\"GPT-3.5\"</span>:\n        code_snippet += <span class=\"hljs-string\">\"llm = OpenAI(temperature=0.1, model='gpt-3.5-turbo-1106')\\n\"</span>\n    <span class=\"hljs-keyword\">elif</span> llm_choice == <span class=\"hljs-string\">\"GPT-4\"</span>:\n        code_snippet += <span class=\"hljs-string\">\"llm = OpenAI(temperature=0.1, model='gpt-4-1106-preview')\\n\"</span>\n    <span class=\"hljs-keyword\">elif</span> llm_choice == <span class=\"hljs-string\">\"Gemini\"</span>:\n        code_snippet += <span class=\"hljs-string\">\"llm = Gemini(model='models/gemini-pro')\\n\"</span>\n    <span class=\"hljs-keyword\">elif</span> llm_choice == <span class=\"hljs-string\">\"Cohere\"</span>:\n        code_snippet += <span class=\"hljs-string\">\"llm = Cohere(model='command', api_key='&amp;lt;YOUR_API_KEY&amp;gt;')  # Replace &amp;lt;YOUR_API_KEY&amp;gt; with your actual API key\\n\"</span>\n\n    <span class=\"hljs-comment\"># Embedding model initialization</span>\n    code_snippet += <span class=\"hljs-string\">f\"embed_model = HuggingFaceEmbedding(model_name='<span class=\"hljs-subst\">{embed_model_choice}</span>')\\n\\n\"</span>\n\n    <span class=\"hljs-comment\"># Node parser initialization</span>\n    node_parsers = {\n        <span class=\"hljs-string\">\"SentenceSplitter\"</span>: <span class=\"hljs-string\">f\"SentenceSplitter(chunk_size=<span class=\"hljs-subst\">{node_parser_params.get(<span class=\"hljs-string\">'chunk_size'</span>, <span class=\"hljs-number\">1024</span>)}</span>, chunk_overlap=<span class=\"hljs-subst\">{node_parser_params.get(<span class=\"hljs-string\">'chunk_overlap'</span>, <span class=\"hljs-number\">20</span>)}</span>)\"</span>,\n        <span class=\"hljs-string\">\"CodeSplitter\"</span>: <span class=\"hljs-string\">f\"CodeSplitter(language=<span class=\"hljs-subst\">{node_parser_params.get(<span class=\"hljs-string\">'language'</span>, <span class=\"hljs-string\">'python'</span>)}</span>, chunk_lines=<span class=\"hljs-subst\">{node_parser_params.get(<span class=\"hljs-string\">'chunk_lines'</span>, <span class=\"hljs-number\">40</span>)}</span>, chunk_lines_overlap=<span class=\"hljs-subst\">{node_parser_params.get(<span class=\"hljs-string\">'chunk_lines_overlap'</span>, <span class=\"hljs-number\">15</span>)}</span>, max_chars=<span class=\"hljs-subst\">{node_parser_params.get(<span class=\"hljs-string\">'max_chars'</span>, <span class=\"hljs-number\">1500</span>)}</span>)\"</span>,\n        <span class=\"hljs-string\">\"SemanticSplitterNodeParser\"</span>: <span class=\"hljs-string\">f\"SemanticSplitterNodeParser(buffer_size=<span class=\"hljs-subst\">{node_parser_params.get(<span class=\"hljs-string\">'buffer_size'</span>, <span class=\"hljs-number\">1</span>)}</span>, breakpoint_percentile_threshold=<span class=\"hljs-subst\">{node_parser_params.get(<span class=\"hljs-string\">'breakpoint_percentile_threshold'</span>, <span class=\"hljs-number\">95</span>)}</span>, embed_model=embed_model)\"</span>,\n        <span class=\"hljs-string\">\"TokenTextSplitter\"</span>: <span class=\"hljs-string\">f\"TokenTextSplitter(chunk_size=<span class=\"hljs-subst\">{node_parser_params.get(<span class=\"hljs-string\">'chunk_size'</span>, <span class=\"hljs-number\">1024</span>)}</span>, chunk_overlap=<span class=\"hljs-subst\">{node_parser_params.get(<span class=\"hljs-string\">'chunk_overlap'</span>, <span class=\"hljs-number\">20</span>)}</span>)\"</span>,\n        <span class=\"hljs-string\">\"HTMLNodeParser\"</span>: <span class=\"hljs-string\">f\"HTMLNodeParser(tags=<span class=\"hljs-subst\">{node_parser_params.get(<span class=\"hljs-string\">'tags'</span>, [<span class=\"hljs-string\">'p'</span>, <span class=\"hljs-string\">'h1'</span>])}</span>)\"</span>,  \n        <span class=\"hljs-string\">\"JSONNodeParser\"</span>: <span class=\"hljs-string\">\"JSONNodeParser()\"</span>,\n        <span class=\"hljs-string\">\"MarkdownNodeParser\"</span>: <span class=\"hljs-string\">\"MarkdownNodeParser()\"</span>\n    }\n    code_snippet += <span class=\"hljs-string\">f\"node_parser = <span class=\"hljs-subst\">{node_parsers[node_parser_choice]}</span>\\n\\n\"</span>\n\n    <span class=\"hljs-comment\"># Response mode</span>\n    code_snippet += <span class=\"hljs-string\">f\"response_mode = '<span class=\"hljs-subst\">{response_mode}</span>'\\n\\n\"</span>\n\n    <span class=\"hljs-comment\"># Vector store initialization</span>\n    <span class=\"hljs-keyword\">if</span> vector_store_choice == <span class=\"hljs-string\">\"Pinecone\"</span>:\n        code_snippet += <span class=\"hljs-string\">\"pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\\n\"</span>\n        code_snippet += <span class=\"hljs-string\">\"index = pc.Index('test')\\n\"</span>\n        code_snippet += <span class=\"hljs-string\">\"vector_store = PineconeVectorStore(pinecone_index=index)\\n\"</span>\n    <span class=\"hljs-keyword\">elif</span> vector_store_choice == <span class=\"hljs-string\">\"Qdrant\"</span>:\n        code_snippet += <span class=\"hljs-string\">\"client = qdrant_client.QdrantClient(location=':memory:')\\n\"</span>\n        code_snippet += <span class=\"hljs-string\">\"vector_store = QdrantVectorStore(client=client, collection_name='sampledata')\\n\"</span>\n    <span class=\"hljs-keyword\">elif</span> vector_store_choice == <span class=\"hljs-string\">\"Simple\"</span>:\n        code_snippet += <span class=\"hljs-string\">\"vector_store = None  # Simple in-memory vector store selected\\n\"</span>\n\n    code_snippet += <span class=\"hljs-string\">\"\\n# Finalizing the RAG pipeline setup\\n\"</span>\n    code_snippet += <span class=\"hljs-string\">\"if vector_store is not None:\\n\"</span>\n    code_snippet += <span class=\"hljs-string\">\"    storage_context = StorageContext.from_defaults(vector_store=vector_store)\\n\"</span>\n    code_snippet += <span class=\"hljs-string\">\"else:\\n\"</span>\n    code_snippet += <span class=\"hljs-string\">\"    storage_context = None\\n\\n\"</span>\n\n    code_snippet += <span class=\"hljs-string\">\"service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, node_parser=node_parser)\\n\\n\"</span>\n\n    code_snippet += <span class=\"hljs-string\">\"_file = 'path_to_your_file'  # Replace with the path to your file\\n\"</span>\n    code_snippet += <span class=\"hljs-string\">\"vector_index = VectorStoreIndex.from_documents(documents=_file, storage_context=storage_context, service_context=service_context, show_progress=True)\\n\"</span>\n    code_snippet += <span class=\"hljs-string\">\"if storage_context:\\n\"</span>\n    code_snippet += <span class=\"hljs-string\">\"    vector_index.storage_context.persist(persist_dir='persist_dir')\\n\\n\"</span>\n\n    code_snippet += <span class=\"hljs-string\">\"query_engine = vector_index.as_query_engine(response_mode=response_mode, verbose=True)\\n\"</span>\n\n    <span class=\"hljs-keyword\">return</span> code_snippet</span></pre>\n <h1>\n  Conclusion\n </h1>\n <p>\n  RAGArch stands at the intersection of innovation and practicality, offering a streamlined no-code approach to RAG pipeline development. It\u2019s designed to demystify the complexities of AI configurations. With RAGArch, both seasoned developers and AI enthusiasts can craft custom pipelines with ease, accelerating the journey from idea to implementation.\n </p>\n <p>\n  Your insights and contributions are invaluable as I continue to evolve this tool. Check out RAGArch on Github and let\u2019s start a conversation on Linkedin. I\u2019m always eager to collaborate and share knowledge with fellow tech adventurers.\n </p>\n <p>\n  <a href=\"https://github.com/AI-ANK/RAGArch\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   GitHub Repo\n  </a>\n </p>\n <p>\n  <a href=\"https://www.linkedin.com/in/harshadsuryawanshi/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Connect with Me on LinkedIn\n  </a>\n </p>\n <p>\n  <a href=\"https://huggingface.co/spaces/AI-ANK/RAGArch\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Live Demo\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 33239, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9383589f-e840-4b67-b2a0-56c8ddc6bdbc": {"__data__": {"id_": "9383589f-e840-4b67-b2a0-56c8ddc6bdbc", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/retrieving-privacy-safe-documents-over-a-network.html", "file_name": "retrieving-privacy-safe-documents-over-a-network.html", "file_type": "text/html", "file_size": 28688, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/retrieving-privacy-safe-documents-over-a-network.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/retrieving-privacy-safe-documents-over-a-network.html", "file_name": "retrieving-privacy-safe-documents-over-a-network.html", "file_type": "text/html", "file_size": 28688, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "d601f5f6a3f967fa14019796cf5e93383088c229753d73f56f6f3db56d7b4467", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In a\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f\" rel=\"noreferrer noopener\">\n   recent blog post\n  </a>\n  , we introduced our\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   llama-index-networks\n  </code>\n  library extension that makes it possible to build a network of RAG systems, which users can query. The benefits of such a network are clear: connecting to a diverse set of knowledge stores\u2014that one may not otherwise have access to\u2014means more accurate responses to an even wider breadth of queries.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  A main caveat to these networks though is that the data being shared across the network ought to be privacy safe. In this blog post, we demonstrate how to turn private, sensitive data into privacy-safe versions that can be subsequently and safely shared across a network. To do so, we\u2019ll be relying on some recent developments in the area of Privacy-Enhancing Techniques.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  The story of Alex, Bob and Beth continues\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  To illustrate all of this, we will again make use of our three made-up characters Alex, Bob and Beth. As a quick reminder, Alex is a data consumer who wants to access the data sources that Bob and Beth possess and are willing to supply.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We showed then how such data a collaboration could be permitted through\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   llama-index-networks\n  </code>\n  by taking the following steps:\n </p>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Bob and Beth both build their respective QueryEngine\u2019s (RAG in llama-index lingo)\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Bob and Beth both expose their QueryEngine behind a ContributorService\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Alex builds a NetworkQueryEngine that connects to Bob and Beth\u2019s ContributorService\u2019s\n  </li>\n </ol>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In part two of this story, we add the wrinkle that Bob and Beth possess private, sensitive data that must be carefully protected before to sharing to Alex. Or, put in another way, we need to add a step 0. to the above steps which applies protective measures to the private datasets.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Measures for protecting data (or more specifically the data subjects) depends on the use-case factors such as what the data involves and how its intended to be shared and ultimately processed. De-anonymizing techniques such as wiping PII (i.e., personal identifiable indicators) are often applied. However, in this blog post we highlight another privacy-enhancing technique called Differential Privacy.\n </p>\n <figure>\n  <figcaption>\n   Part 2: of Alex, Bob and Beth. This time Bob and Beth have sensitive data that they want to share, but can\u2019t unless protective measures are applied before sharing across the network.\n  </figcaption>\n </figure>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Part 2: of Alex, Bob and Beth. This time Bob and Beth have sensitive data that they want to share, but can\u2019t unless protective measures are applied before sharing across the network.\n </p>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  Sidebar: differential privacy primer\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In short, differential privacy is a method that provides mathematical guarantees (up to a certain level of chance) that an adversary would not be able to learn that a specific individual belonged to a private dataset after only seeing the output of running this private dataset through a protected data processing step. In other words, an individual\u2019s inclusion in the private dataset cannot be learned from the output of a differentially-private algorithm.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  By protecting against the threat of dataset inclusion, we mitigate the risk that an adversary is able to link the private data with their external sources to learn more about the data subject and potentially cause more privacy harms (such as distortion).\n </p>\n <figure>\n  <figcaption>\n   A light introduction to differential privacy.\n  </figcaption>\n </figure>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  A light introduction to differential privacy.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Coming back to the story of Alex, Bob and Beth, in order to protect Bob and Beth\u2019s data, we will make use of an algorithm that uses a pre-trained LLM to create synthetic copies of private data that satisfies the differential private mathematical guarantees. This algorithm was introduced in the paper entitled \u201cPrivacy-preserving in-context learning with differentially private few-shot generation\u201d by Xinyu Tang et al., which appeared in ICLR 2024. It is the synthetic copies that we can use to share across the network!\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  There we have it, the added privacy wrinkle and our differentially privacy approach means that we have to take the following steps to facilitate this data collaboration.\n </p>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Bob and Beth create privacy-safe synthetic copies of their private datasets\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Bob and Beth both build their respective QueryEngine\u2019s over their synthetic datasets\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Bob and Beth both expose their QueryEngine behind a ContributorService\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Alex builds a NetworkQueryEngine that connects to Bob and Beth\u2019s ContributorService\u2019s\n  </li>\n </ol>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  Creating differentially private synthetic copies of a private dataset\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Fortunately, for step 0., we can make use of the\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   DiffPrivateSimpleDataset\n  </code>\n  pack.\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index.core.llama_datasets.simple <span class=\"hljs-keyword\">import</span> LabelledSimpleDataset\n<span class=\"hljs-keyword\">from</span> llama_index.packs.diff_private_simple_dataset.base <span class=\"hljs-keyword\">import</span> PromptBundle\n<span class=\"hljs-keyword\">from</span> llama_index.packs.diff_private_simple_dataset <span class=\"hljs-keyword\">import</span> DiffPrivateSimpleDatasetPack\n<span class=\"hljs-keyword\">from</span> llama_index.llms.openai <span class=\"hljs-keyword\">import</span> OpenAI\n<span class=\"hljs-keyword\">import</span> tiktoken\n\n<span class=\"hljs-comment\"># Beth uses `DiffPrivateSimpleDatasetPack` to generate synthetic copies</span>\n\nllm = OpenAI(\n    model=<span class=\"hljs-string\">\"gpt-3.5-turbo-instruct\"</span>,\n    max_tokens=<span class=\"hljs-number\">1</span>,\n    logprobs=<span class=\"hljs-literal\">True</span>,\n    top_logprobs=<span class=\"hljs-number\">5</span>,  <span class=\"hljs-comment\"># OpenAI only allows for top 5 next token</span>\n)                    <span class=\"hljs-comment\"># as opposed to entire vocabulary</span>\ntokenizer = tiktoken.encoding_for_model(<span class=\"hljs-string\">\"gpt-3.5-turbo-instruct\"</span>)\n\nbeth_private_dataset: LabelledSimpleDataset = ... <span class=\"hljs-comment\"># a dataset that contains</span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"hljs-comment\"># examples with two attributes</span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"hljs-comment\"># `text` and `reference_label`</span>\n\nbeth_synthetic_generator = DiffPrivateSimpleDatasetPack(\n    llm=llm,\n    tokenizer=tokenizer,\n    prompt_bundle=prompt_bundle,    <span class=\"hljs-comment\"># params for preparing required prompts</span>\n    simple_dataset=simple_dataset,  <span class=\"hljs-comment\"># to generate the synthetic examples </span>\n)\n\nbeth_synthetic_dataset = <span class=\"hljs-keyword\">await</span> beth_synthetic_generator.arun(\n\t\tsize=<span class=\"hljs-number\">3</span>,  <span class=\"hljs-comment\"># number of synthetic observations to create</span>\n\t\tsigma=<span class=\"hljs-number\">0.5</span>  <span class=\"hljs-comment\"># param that determines the level of privacy</span>\n)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  With the synthetic dataset in hand, Bob and Beth can apply the steps introduced in our previous post to build their privacy-safe QueryEngine. It\u2019s worthwhile to mention here that as mentioned by the authors of the paper, the synthetic copies can be used as many times as one would like in a downstream task and it would incur no additional privacy cost! (This is due to the post-processing property of differential privacy.)\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Example: Symptom2Disease\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In this section of the blog post, we go over an actual example application of the privacy-safe networks over the\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.kaggle.com/datasets/niyarrbarman/symptom2disease/data\" rel=\"noreferrer noopener\">\n   Symptom2Disease\n  </a>\n  dataset. This dataset consists of 1,200 examples each containing a \u201csymptoms\u201d description as well as the associated \u201cdisease\u201d label \u2014 the dataset contains observations for 24 distinct disease labels. We split the dataset into two disjoint subsets, one for training and the other for testing. Moreover, we consider this original dataset to be private, requiring protective measures before being shared across a network.\n </p>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  Generate privacy-safe synthetic observations of Symptom2Disease\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We use the training subset and apply the\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   DiffPrivateSimpleDatasetPack\n  </code>\n  on it in order to generate privacy-safe, synthetic observations. But in order to do so, we first need to turn the raw Symptom2Disease dataset into a\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   LabelledSimpleDataset\n  </code>\n  object.\n </p>\n <pre><code><span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\">as</span> pd\n<span class=\"hljs-keyword\">from</span> sklearn.model_selection <span class=\"hljs-keyword\">import</span> train_test_split\n<span class=\"hljs-keyword\">from</span> llama_index.core.llama_dataset.simple <span class=\"hljs-keyword\">import</span> (\n    LabelledSimpleDataExample,\n    LabelledSimpleDataset,\n)\n<span class=\"hljs-keyword\">from</span> llama_index.core.llama_dataset.base <span class=\"hljs-keyword\">import</span> CreatedBy, CreatedByType\n\n<span class=\"hljs-comment\"># load the Symptom2Disease.csv file</span>\ndf = pd.read_csv(<span class=\"hljs-string\">\"Symptom2Disease.csv\"</span>)\ntrain, test = train_test_split(df, test_size=<span class=\"hljs-number\">0.2</span>)\n\n<span class=\"hljs-comment\"># create a LabelledSimpleDataset (which is what the pack works with)</span>\nexamples = []\n<span class=\"hljs-keyword\">for</span> index, row <span class=\"hljs-keyword\">in</span> df.iterrows():\n    example = LabelledSimpleDataExample(\n        reference_label=row[<span class=\"hljs-string\">\"label\"</span>],\n        text=row[<span class=\"hljs-string\">\"text\"</span>],\n        text_by=CreatedBy(<span class=\"hljs-built_in\">type</span>=CreatedByType.HUMAN),\n    )\n    examples.append(example)\n\nsimple_dataset = LabelledSimpleDataset(examples=examples)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Now we can use the llama-pack to create our synthetic observations.\n </p>\n <pre><code><span class=\"hljs-keyword\">import</span> llama_index.core.instrumentation <span class=\"hljs-keyword\">as</span> instrument\n<span class=\"hljs-keyword\">from</span> llama_index.core.llama_dataset.simple <span class=\"hljs-keyword\">import</span> LabelledSimpleDataset\n<span class=\"hljs-keyword\">from</span> llama_index.packs.diff_private_simple_dataset.base <span class=\"hljs-keyword\">import</span> PromptBundle\n<span class=\"hljs-keyword\">from</span> llama_index.packs.diff_private_simple_dataset <span class=\"hljs-keyword\">import</span> DiffPrivateSimpleDatasetPack\n<span class=\"hljs-keyword\">from</span> llama_index.llms.openai <span class=\"hljs-keyword\">import</span> OpenAI\n<span class=\"hljs-keyword\">import</span> tiktoken\n<span class=\"hljs-keyword\">from</span> .event_handler <span class=\"hljs-keyword\">import</span> DiffPrivacyEventHandler\n<span class=\"hljs-keyword\">import</span> asyncio\n<span class=\"hljs-keyword\">import</span> os\n\nNUM_SPLITS = <span class=\"hljs-number\">3</span>\nT_MAX = <span class=\"hljs-number\">150</span>\n\nllm = OpenAI(\n    model=<span class=\"hljs-string\">\"gpt-3.5-turbo-instruct\"</span>,\n    max_tokens=<span class=\"hljs-number\">1</span>,\n    logprobs=<span class=\"hljs-literal\">True</span>,\n    top_logprobs=<span class=\"hljs-number\">5</span>,\n)\ntokenizer = tiktoken.encoding_for_model(<span class=\"hljs-string\">\"gpt-3.5-turbo-instruct\"</span>)\n\nprompt_bundle = PromptBundle(\n    instruction=(\n        <span class=\"hljs-string\">\"You are a patient experiencing symptoms of a specific disease. \"</span>\n        <span class=\"hljs-string\">\"Given a label of disease type, generate the chosen type of symptoms accordingly.\\n\"</span>\n        <span class=\"hljs-string\">\"Start your answer directly after 'Symptoms: '. Begin your answer with [RESULT].\\n\"</span>\n    ),\n    label_heading=<span class=\"hljs-string\">\"Disease\"</span>,\n    text_heading=<span class=\"hljs-string\">\"Symptoms\"</span>,\n)\n\ndp_simple_dataset_pack = DiffPrivateSimpleDatasetPack(\n    llm=llm,\n    tokenizer=tokenizer,\n    prompt_bundle=prompt_bundle,\n    simple_dataset=simple_dataset,\n)\n\nsynthetic_dataset = <span class=\"hljs-keyword\">await</span> dp_simple_dataset_pack.arun(\n    sizes=<span class=\"hljs-number\">3</span>,\n    t_max=T_MAX,\n    sigma=<span class=\"hljs-number\">1.5</span>,\n    num_splits=NUM_SPLITS,\n    num_samples_per_split=<span class=\"hljs-number\">8</span>,  <span class=\"hljs-comment\"># number of private observations to create a</span>\n)                             <span class=\"hljs-comment\"># synthetic obsevation</span>\nsynthetic_dataset.save_json(<span class=\"hljs-string\">\"synthetic_dataset.json\"</span>)</code></pre>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  Create a network with two contributors\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Next, we imagine that there are two contributors that each have their own set of Symptom2Disease datasets. In particular, we split the 24 categories of diseases into two disjoint sets and consider each Contributor to possess only one of the two sets. Note that we created the synthetic observations on the full training set, though we could have easily done this on the split datasets as well.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Now that we have the synthetic observations, we can follow a slightly modified version of steps 1. through 3. defined in the story of Alex, Bob and Beth. The modification here is that we\u2019re using Retrievers instead of QueryEngine (the choice of Retriever or QueryEngine is completely up to the user).\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   Step 1:\n  </strong>\n  Contributor\u2019s build their Retriever over their synthetic datasets.\n </p>\n <pre><code><span class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\">from</span> llama_index.core <span class=\"hljs-keyword\">import</span> VectorStoreIndex\n<span class=\"hljs-keyword\">from</span> llama_index.core.llama_dataset.simple <span class=\"hljs-keyword\">import</span> LabelledSimpleDataset\n<span class=\"hljs-keyword\">from</span> llama_index.core.schema <span class=\"hljs-keyword\">import</span> TextNode\n\n\n<span class=\"hljs-comment\"># load the synthetic dataset</span>\nsynthetic_dataset = LabelledSimpleDataset.from_json(\n    <span class=\"hljs-string\">\"./data/contributor1_synthetic_dataset.json\"</span>\n)\n\n\nnodes = [\n    TextNode(text=el.text, metadata={<span class=\"hljs-string\">\"reference_label\"</span>: el.reference_label})\n    <span class=\"hljs-keyword\">for</span> el <span class=\"hljs-keyword\">in</span> synthetic_dataset[:]\n]\n\nindex = VectorStoreIndex(nodes=nodes)\nsimilarity_top_k = <span class=\"hljs-built_in\">int</span>(os.environ.get(<span class=\"hljs-string\">\"SIMILARITY_TOP_K\"</span>))\nretriever = index.as_retriever(similarity_top_k=similarity_top_k)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   Step 2:\n  </strong>\n  Contributor\u2019s expose their Retrievers behind a ContributorRetrieverService\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index.networks.contributor.retriever.service <span class=\"hljs-keyword\">import</span> (\n    ContributorRetrieverService,\n    ContributorRetrieverServiceSettings,\n)\n\nsettings = ContributorRetrieverServiceSettings() <span class=\"hljs-comment\"># loads from .env file</span>\nservice = ContributorRetrieverService(config=settings, retriever=retriever)\napp = service.app</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   Step 3:\n  </strong>\n  Define the NetworkRetriever that connects to the ContributorRetrieverServices\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index.networks.network.retriever <span class=\"hljs-keyword\">import</span> NetworkRetriever\n<span class=\"hljs-keyword\">from</span> llama_index.networks.contributor.retriever <span class=\"hljs-keyword\">import</span> ContributorRetrieverClient\n<span class=\"hljs-keyword\">from</span> llama_index.postprocessor.cohere_rerank <span class=\"hljs-keyword\">import</span> CohereRerank\n\n<span class=\"hljs-comment\"># ContributorRetrieverClient's connect to the ContributorRetrieverService</span>\ncontributors = [\n    ContributorRetrieverClient.from_config_file(\n        env_file=<span class=\"hljs-string\">f\"./client-env-files/.env.contributor_<span class=\"hljs-subst\">{ix}</span>.client\"</span>\n    )\n    <span class=\"hljs-keyword\">for</span> ix <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">3</span>)\n]\nreranker = CohereRerank(top_n=<span class=\"hljs-number\">5</span>)\nnetwork_retriever = NetworkRetriever(\n    contributors=contributors, node_postprocessors=[reranker]\n)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  With the\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   NetworkRetriever\n  </code>\n  established, we can retrieve synthetic observations from the two contributors data against a query.\n </p>\n <pre><code>related_records = network_retriever.aretrieve(<span class=\"hljs-string\">\"Vomitting and nausea\"</span>)\n<span class=\"hljs-built_in\">print</span>(related_records) <span class=\"hljs-comment\"># contain symptoms/disease records that are similar to</span>\n\t\t\t\t\t\t\t\t\t\t\t <span class=\"hljs-comment\"># to the queried symptoms.</span></code></pre>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  Evaluating the\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   NetworkRetriever\n  </code>\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  To evaluate the efficacy of the\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   NetworkRetriever\n  </code>\n  we make use of our test set in order to compute two traditional retrieval metrics, namely: hit rate and mean reciprocal rank.\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    hit rate:\n   </strong>\n   a hit occurs if any of the retrieved nodes share the same disease label as the test query (symptoms). The hit rate then is the total number of hits divided by the size of the test set.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    mean reciprocal rank:\n   </strong>\n   similar to hit rate, but now we take into account the position of the first retrieved node that shares the same disease label as the test query. If there is no such retrieved node, then the reciprocal rank of the test is equal to 0. The mean reciprocal rank is then merely the average of all reciprocal ranks across the test set.\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In addition to evaluating the\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   NetworkRetriever\n  </code>\n  we consider the two baselines that represent Retrieving only over the individual Contributor\u2019s synthetic datasets.\n </p>\n <figure>\n  <figcaption>\n   Retriever evaluations, with sigma equal to 1.5.\n  </figcaption>\n </figure>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In the image above, we observe that the NetworkRetriever outperforms both the individual contributor Retriever\u2019s in the test set. This shouldn\u2019t be hard to grasp however since the network retriever has access to more data since it has access to both the Contributor\u2019s synthetic observations\u2014this is the point after all of a network!\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Another important observation can be made upon inspection of these results. That is, the privacy-safe synthetic observations do indeed do the job of protecting privacy while still maintaining utility in the original dataset. This is often the concern when applying privacy measures such as differential privacy, where noise is incorporated to protect the data. Too much noise will provide high levels of privacy, but at the same time, may render the data useless in downstream tasks. From the table above, we see that at least for this example (though it does corroborate the results of the paper) that the synthetic observations still do match well with the test set, which are indeed real observations (i.e. not synthetically generated).\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Finally, this level of privacy can be controlled via the noise parameter\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   sigma\n  </code>\n  . In the example above we used a\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   sigma\n  </code>\n  of 1.5, which for this dataset amounts to an\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   epsilon\n  </code>\n  (i.e., privacy-loss measure) value of 1.3. (Privacy loss levels between 0 and 1 are\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.notion.so/Standups-8d1e478a8ce54ffa9788eef1fd416042?pvs=21\" rel=\"noreferrer noopener\">\n   generally considered to be quite private\n  </a>\n  .) Below, we share the evaluations that result from using a\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   sigma\n  </code>\n  of 0.5, which amounts to an\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   epsilon\n  </code>\n  of 15.9\u2014higher values of\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   epsilon\n  </code>\n  or privacy-loss means less privacy.\n </p>\n <pre><code><span class=\"hljs-comment\"># use the `DiffPrivacySimpleDatasetPack` to get the value of epsilon</span>\nepsilon = dp_simple_dataset_pack.sigma_to_eps(\n\t\tsigma=<span class=\"hljs-number\">0.5</span>,\n\t\tmechanism=<span class=\"hljs-string\">\"gaussian\"</span>,\n\t\tsize=<span class=\"hljs-number\">3</span>*<span class=\"hljs-number\">24</span>,\n\t\tmax_token_cnt=<span class=\"hljs-number\">150</span>  <span class=\"hljs-comment\"># number of max tokens to generate per synthetic example</span>\n)</code></pre>\n <figure>\n  <figcaption>\n   Retriever evaluations with less noise and thus less privacy i.e., sigma equal to 0.5.\n  </figcaption>\n </figure>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  So we see after comparing the evaluation metrics with different levels of privacy that when we use the synthetic observations that have higher levels of privacy, we take a bit of a hit in the performance as seen in the decrease in both the hit rate and mean reciprocal rank. This indeed is an illustration of the privacy tradeoff. If we take a look at some of the examples from the synthetic datasets, we can perhaps gain insight as to why this may be happening.\n </p>\n <pre><code><span class=\"hljs-comment\"># synthetic example epsilon = 1.3</span>\n{\n    <span class=\"hljs-string\">\"reference_label\"</span>: <span class=\"hljs-string\">\"Psoriasis\"</span>,\n    <span class=\"hljs-string\">\"text\"</span>: <span class=\"hljs-string\">\"[RESULTS] red, scalloped patches on skin; itching and burning sensation; thick, pitted nails on fingers and toes; joint discomfort; swollen and stiff joints; cracked and painful skin on palms and feet\"</span>,\n    <span class=\"hljs-string\">\"text_by\"</span>: {\n        <span class=\"hljs-string\">\"model_name\"</span>: <span class=\"hljs-string\">\"gpt-3.5-turbo-instruct\"</span>,\n        <span class=\"hljs-string\">\"type\"</span>: <span class=\"hljs-string\">\"ai\"</span>\n    }\n},\n\n<span class=\"hljs-comment\"># synthetic example epsilon = 15.9</span>\n{\n  <span class=\"hljs-string\">\"reference_label\"</span>: <span class=\"hljs-string\">\"Migraine\"</span>,\n  <span class=\"hljs-string\">\"text\"</span>: <span class=\"hljs-string\">\"Intense headache, sensitivity to light and sound, nausea, vomiting, vision changes, and fatigue.\"</span>,\n  <span class=\"hljs-string\">\"text_by\"</span>: {\n    <span class=\"hljs-string\">\"model_name\"</span>: <span class=\"hljs-string\">\"gpt-3.5-turbo-instruct\"</span>,\n    <span class=\"hljs-string\">\"type\"</span>: <span class=\"hljs-string\">\"ai\"</span>\n  }\n},</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We can see that synthetic datasets with higher level of privacy are not as clean in terms of punctuation symbols in the text when compared to those with lower levels of privacy. This makes sense because the differential privacy algorithm adds noise to the mechanics of next-token generation. Thus, perturbing this process greatly has affect on the instruction-following capabilities of the LLM.\n </p>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  In summary\n </h3>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We used differential privacy to create privacy-safe, synthetic observations in order to permit the data collaboration of private data that may not be otherwise possible.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We demonstrated the benefits of the NetworkRetriever that has access to more data than what the individual Contributor Retriever may have access to.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We demonstrated the affects of varying degrees of privacy on the synthetic observations, and by extension, the NetworkRetriever.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Learn more!\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  To delve deeper into the materials of this blog post, we share a few links below:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Source code for the privacy-safe networks retriever demo. With this, you can try the above all out yourself! (\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/tree/main/llama-index-networks/examples/privacy_safe_retrieval\" rel=\"noreferrer noopener\">\n    link\n   </a>\n   )\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Demo notebooks for the\n   <code class=\"SanityPortableText_inlineCode__cI85z\">\n    DiffPrivateSimpleDataset\n   </code>\n   (\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-diff-private-simple-dataset/examples/basic_demo/demo_usage.ipynb\" rel=\"noreferrer noopener\">\n    link\n   </a>\n   )\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   The source code for creating the synthetic Symptom2Disease observations using the\n   <code class=\"SanityPortableText_inlineCode__cI85z\">\n    DiffPrivateSimpleDataset\n   </code>\n   (\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llama_index/tree/main/llama-index-packs/llama-index-packs-diff-private-simple-dataset/examples/symptom_2_disease\" rel=\"noreferrer noopener\">\n    link\n   </a>\n   )\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 28627, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9858923-6720-4415-a97f-908b5f503af9": {"__data__": {"id_": "b9858923-6720-4415-a97f-908b5f503af9", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.html", "file_name": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.html", "file_type": "text/html", "file_size": 15104, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.html", "file_name": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.html", "file_type": "text/html", "file_size": 15104, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "e42498454830e4e968baac3fab3c02800b2accac3cd51897409080666e421f79", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  You may have heard the fuss about the latest release from European AI powerhouse\n  <a href=\"https://mistral.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Mistral AI\n  </a>\n  : it\u2019s called Mixtral 8x7b, a \u201cmixture of experts\u201d model \u2014 eight of them, each trained with 7 billion parameters, hence the name. Released originally as a\n  <a href=\"https://twitter.com/MistralAI/status/1733150512395038967\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   mic-drop tweet\n  </a>\n  they followed up a few days later with a\n  <a href=\"https://mistral.ai/news/mixtral-of-experts/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   blog post\n  </a>\n  that showed it matching or exceeding GPT-3.5 as well as the much larger Llama2 70b on a number of benchmarks.\n </p>\n <p>\n  Here at LlamaIndex we\u2019re naturally fans of open source software, so open models with permissive licenses like Mixtral are right up our alley. We\u2019ve had a few questions about how to get Mixtral working with LlamaIndex, so this post is here to get you up and running with a totally local model.\n </p>\n <h1>\n  Step 1: Install Ollama\n </h1>\n <p>\n  Previously getting a local model installed and working was a huge pain, but with the release of\n  <a href=\"https://ollama.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Ollama\n  </a>\n  , it\u2019s suddenly a snap! Available for MacOS and Linux (and soon on Windows, though you can use it today on Windows via\n  <a href=\"https://learn.microsoft.com/en-us/windows/wsl/install\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Windows Subsystem For Linux\n  </a>\n  ), it is itself open source and a\n  <a href=\"https://ollama.ai/download\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   free download\n  </a>\n  .\n </p>\n <p>\n  Once downloaded, you can get Mixtral with a single command:\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"2746\">ollama run mixtral</span></pre>\n <p>\n  The first time you run this command it will have to download the model, which can take a long time, so go get a snack. Also note that it requires a hefty 48GB of RAM to run smoothly! If that\u2019s too much for your machine, consider using its smaller but still very capable cousin\n  <strong>\n   Mistral 7b\n  </strong>\n  , which you install and run the same way:\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"9686\">ollama run mistral</span></pre>\n <p>\n  We\u2019ll assume you\u2019re using Mixtral for the rest of this tutorial, but Mistral will also work.\n </p>\n <p>\n  Once the model is running Ollama will automatically let you chat with it. That\u2019s fun, but what\u2019s the point of having a model if it can\u2019t work with your data? That\u2019s where LlamaIndex comes in. The next few steps will take you through the code line by line, but if you\u2019d prefer to save all the copying and pasting, all of this code is available in an\n  <a href=\"https://github.com/run-llama/mixtral_ollama\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   open-source repo\n  </a>\n  that you can clone to follow along there.\n </p>\n <h1>\n  Step 2: Install your dependencies\n </h1>\n <p>\n  You\u2019re going to need LlamaIndex installed, obviously! We\u2019ll also get you going with a handful of other dependencies that are about to come in handy:\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"6d42\">pip install llama-index qdrant_client torch transformers</span></pre>\n <h1>\n  Step 3: Smoke test\n </h1>\n <p>\n  If you\u2019ve got Ollama running and LlamaIndex properly installed, the following quick script will make sure everything is in order by asking it a quick \u201csmoke test\u201d question in a script all by itself:\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"d12c\"><span class=\"hljs-comment\"># Just runs .complete to make sure the LLM is listening</span>\n<span class=\"hljs-keyword\">from</span> llama_index.llms <span class=\"hljs-keyword\">import</span> Ollama\n\nllm = Ollama(model=<span class=\"hljs-string\">\"mixtral\"</span>)\nresponse = llm.complete(<span class=\"hljs-string\">\"Who is Laurie Voss?\"</span>)\n<span class=\"hljs-built_in\">print</span>(response)</span></pre>\n <h1>\n  Step 4: Load some data and index it\n </h1>\n <p>\n  Now you\u2019re ready to load in some real data! You can use any data you want; in this case I\u2019m using a\n  <a href=\"https://www.dropbox.com/scl/fi/6sos49fluvfilj3sqcvoj/tinytweets.json?rlkey=qmxlaqp000kmx8zktvaj4u1vh&amp;dl=0\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   small collection of my own tweets\n  </a>\n  which you can download, or use your own! We\u2019re going to be storing our data in the nifty, open source\n  <a href=\"https://github.com/qdrant/qdrant\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Qdrant\n  </a>\n  vector database (which is why we got you to install it earlier). Create a new python file, and load in all our dependencies:\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"0599\"><span class=\"hljs-keyword\">from</span> pathlib <span class=\"hljs-keyword\">import</span> Path\n<span class=\"hljs-keyword\">import</span> qdrant_client\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> (\n    VectorStoreIndex,\n    ServiceContext,\n    download_loader,\n)\n<span class=\"hljs-keyword\">from</span> llama_index.llms <span class=\"hljs-keyword\">import</span> Ollama\n<span class=\"hljs-keyword\">from</span> llama_index.storage.storage_context <span class=\"hljs-keyword\">import</span> StorageContext\n<span class=\"hljs-keyword\">from</span> llama_index.vector_stores.qdrant <span class=\"hljs-keyword\">import</span> QdrantVectorStore</span></pre>\n <p>\n  Then load our tweets out of our JSON file using a nifty JSONReader from\n  <a href=\"https://llamahub.ai/l/file-json?from=all\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaHub\n  </a>\n  , our convenient collection of open source data connectors. This will give you a pile of documents ready to be embedded and indexed:\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"9a4d\">JSONReader = download_loader(\"JSONReader\")\nloader = JSONReader()\ndocuments = loader.load_data(Path('./data/tinytweets.json'))</span></pre>\n <p>\n  Get Qdrant ready for action by initializing it and passing it into a Storage Context we\u2019ll be using later:\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"dcc0\">client = qdrant_client.QdrantClient(\n    path=\"./qdrant_data\"\n)\nvector_store = QdrantVectorStore(client=client, collection_name=\"tweets\")\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)</span></pre>\n <p>\n  Now set up our Service Context. We\u2019ll be passing it Mixtral as the LLM so we can test that things are working once we\u2019ve finished indexing; indexing itself doesn\u2019t need Mixtral. By passing\n  <code class=\"cw qi qj qk qa b\">\n   embed_model=\"local\"\n  </code>\n  we\u2019re specifying that LlamaIndex will embed your data locally, which is why you needed\n  <code class=\"cw qi qj qk qa b\">\n   torch\n  </code>\n  and\n  <code class=\"cw qi qj qk qa b\">\n   transformers\n  </code>\n  .\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"b93a\">llm = Ollama(model=\"mixtral\")\nservice_context = ServiceContext.from_defaults(llm=llm,embed_model=\"local\")</span></pre>\n <p>\n  Now bring it all together: build the index from the documents you loaded using the service and storage contexts you already set up, and give it a query:\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"0767\">index = VectorStoreIndex.from_documents(documents,service_context=service_context,storage_context=storage_context)\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(<span class=\"hljs-string\">\"What does the author think about Star Trek? Give details.\"</span>)\n<span class=\"hljs-built_in\">print</span>(response)</span></pre>\n <p>\n  Ollama will need to fire up Mixtral to answer the query, which can take a little while, so be patient! You should get output something like this (but with more details):\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"607d\">Based on the provided context information, the author has a mixed opinion about Star Trek.</span></pre>\n <h1>\n  Verify our index\n </h1>\n <p>\n  Now to prove it\u2019s not all smoke and mirrors, let\u2019s use our pre-built index. Start a new python file and load in dependencies again:\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"b12a\"><span class=\"hljs-keyword\">import</span> qdrant_client\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> (\n    VectorStoreIndex,\n    ServiceContext,\n)\n<span class=\"hljs-keyword\">from</span> llama_index.llms <span class=\"hljs-keyword\">import</span> Ollama\n<span class=\"hljs-keyword\">from</span> llama_index.vector_stores.qdrant <span class=\"hljs-keyword\">import</span> QdrantVectorStore</span></pre>\n <p>\n  This time we won\u2019t need to load the data, that\u2019s already done! We will need the Qdrant client and of course Mixtral again:\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"48fb\">client = qdrant_client.QdrantClient(\n    path=\"./qdrant_data\"\n)\nvector_store = QdrantVectorStore(client=client, collection_name=\"tweets\")\n\nllm = Ollama(model=\"mixtral\")\nservice_context = ServiceContext.from_defaults(llm=llm,embed_model=\"local\")</span></pre>\n <p>\n  This time instead of creating our index from documents we load it directly from the vector store using\n  <code class=\"cw qi qj qk qa b\">\n   from_vector_store\n  </code>\n  . We\u2019re also passing\n  <code class=\"cw qi qj qk qa b\">\n   similarity_top_k=20\n  </code>\n  to the query engine; this will mean it will fetch 20 tweets at a time (the default is 2) to get more context and better answer the question.\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"b083\">index = VectorStoreIndex.from_vector_store(vector_store=vector_store,service_context=service_context)\nquery_engine = index.as_query_engine(similarity_top_k=<span class=\"hljs-number\">20</span>)\nresponse = query_engine.query(<span class=\"hljs-string\">\"Does the author like SQL? Give details.\"</span>)\n<span class=\"hljs-built_in\">print</span>(response)</span></pre>\n <h1>\n  Build a little web service\n </h1>\n <p>\n  It\u2019s no good having an index that just runs as a script! Let\u2019s make an API out of this thing. We\u2019ll need two new dependencies:\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"bd6a\">pip install flask flask-cors</span></pre>\n <p>\n  Load in our dependencies as before into a new file:\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"0499\"><span class=\"hljs-keyword\">from</span> flask <span class=\"hljs-keyword\">import</span> Flask, request, jsonify\n<span class=\"hljs-keyword\">from</span> flask_cors <span class=\"hljs-keyword\">import</span> CORS, cross_origin\n<span class=\"hljs-keyword\">import</span> qdrant_client\n<span class=\"hljs-keyword\">from</span> llama_index.llms <span class=\"hljs-keyword\">import</span> Ollama\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> (\n    VectorStoreIndex,\n    ServiceContext,\n)\n<span class=\"hljs-keyword\">from</span> llama_index.vector_stores.qdrant <span class=\"hljs-keyword\">import</span> QdrantVectorStore</span></pre>\n <p>\n  Get the vector store, the LLM and the index loaded:\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"b229\"># re-initialize the vector store\nclient = qdrant_client.QdrantClient(\n    path=\"./qdrant_data\"\n)\nvector_store = QdrantVectorStore(client=client, collection_name=\"tweets\")\n\n# get the LLM again\nllm = Ollama(model=\"mixtral\")\nservice_context = ServiceContext.from_defaults(llm=llm,embed_model=\"local\")\n# load the index from the vector store\nindex = VectorStoreIndex.from_vector_store(vector_store=vector_store,service_context=service_context)</span></pre>\n <p>\n  Set up a really basic Flask server:\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"3c1d\">app = Flask(__name__)\ncors = CORS(app)\napp.config[<span class=\"hljs-string\">'CORS_HEADERS'</span>] = <span class=\"hljs-string\">'Content-Type'</span>\n\n<span class=\"hljs-comment\"># This is just so you can easily tell the app is running</span>\n<span class=\"hljs-meta\">@app.route(<span class=\"hljs-params\"><span class=\"hljs-string\">'/'</span></span>)</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">hello_world</span>():\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">'Hello, World!'</span></span></pre>\n <p>\n  And add a route that accepts a query (as form data), queries the LLM and returns the response:\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"8542\"><span class=\"hljs-meta\">@app.route(<span class=\"hljs-params\"><span class=\"hljs-string\">'/process_form'</span>, methods=[<span class=\"hljs-string\">'POST'</span>]</span>)</span>\n<span class=\"hljs-meta\">@cross_origin()</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">process_form</span>():\n    query = request.form.get(<span class=\"hljs-string\">'query'</span>)\n    <span class=\"hljs-keyword\">if</span> query <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n        query_engine = index.as_query_engine(similarity_top_k=<span class=\"hljs-number\">20</span>)\n        response = query_engine.query(query)\n        <span class=\"hljs-keyword\">return</span> jsonify({<span class=\"hljs-string\">\"response\"</span>: <span class=\"hljs-built_in\">str</span>(response)})\n    <span class=\"hljs-keyword\">else</span>:\n        <span class=\"hljs-keyword\">return</span> jsonify({<span class=\"hljs-string\">\"error\"</span>: <span class=\"hljs-string\">\"query field is missing\"</span>}), <span class=\"hljs-number\">400</span>\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">'__main__'</span>:\n    app.run()</span></pre>\n <p>\n  Note those last two lines, they\u2019re important!\n  <code class=\"cw qi qj qk qa b\">\n   flask run\n  </code>\n  is incompatible with the way LlamaIndex loads dependencies, so you will need to run this API directly like so (assuming your file is called\n  <code class=\"cw qi qj qk qa b\">\n   app.py\n  </code>\n  )\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"c3fa\">python app.py</span></pre>\n <p>\n  With your API up and running, you can use cURL to send a request and verify it:\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"6bda\">curl --location '<span class=\"hljs-symbol\">&amp;lt;</span>http://127.0.0.1:5000/process_form<span class=\"hljs-symbol\">&amp;gt;</span>' \\\\\n--form 'query=\"What does the author think about Star Trek?\"'</span></pre>\n <h1>\n  You\u2019re done!\n </h1>\n <p>\n  We covered a few things here:\n </p>\n <ul>\n  <li>\n   Getting Ollama to run Mixtral locally\n  </li>\n  <li>\n   Using LlamaIndex to query Mixtral 8x7b\n  </li>\n  <li>\n   Building and querying an index over your data using Qdrant vector store\n  </li>\n  <li>\n   Wrapping your index into a very simple web API\n  </li>\n  <li>\n   All open-source, free, and running locally!\n  </li>\n </ul>\n <p>\n  I hope this was a fun, quick introduction to running local models with LlamaIndex!\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 15027, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a037895-5207-4bfc-a218-4686c6572581": {"__data__": {"id_": "8a037895-5207-4bfc-a218-4686c6572581", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.html", "file_name": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.html", "file_type": "text/html", "file_size": 21244, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.html", "file_name": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.html", "file_type": "text/html", "file_size": 21244, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "3f32afd136785eb4b02d83ac5c183812322acdad9141dd558d06283fe986f7b0", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Over the holidays, I was running some retrieval benchmarks with LlamaIndex. I found myself rebuilding an index repeatedly with 30K documents, and finding waiting 10\u201320 minutes each time was too grating.\n </p>\n <p>\n  So to solve this, issue, I decided to bite the bullet and figure out how to deploy LlamaIndex to AWS, and create a scalable ETL pipeline for indexing my data. This brought the processing time down to around 5 minutes!\n </p>\n <figure>\n  <figcaption class=\"om fe on ny nz oo op be b bf z dt\">\n   Proposed system architecture\n  </figcaption>\n </figure>\n <p>\n  If you want to skip the detailed steps, you can jump to the code at the following repository:\n </p>\n <p>\n  <a href=\"https://github.com/run-llama/llamaindex_aws_ingestion\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://github.com/run-llama/llamaindex_aws_ingestion\n  </a>\n </p>\n <p>\n  <strong>\n   NOTE:\n  </strong>\n  I am not an AWS expert, and had zero experience with it before this project. There are likely ways to improve upon the design I came up with. This blog merely documents my first foray into getting a system working on AWS. My hope is that this helps other people get started, and opens the door for other engineers deploying more scale-able systems.\n </p>\n <h1>\n  Step 1: Figuring out how AWS works\n </h1>\n <p>\n  To use AWS effectively, there are several packages and tools that you will need:\n </p>\n <ol>\n  <li>\n   <a href=\"https://portal.aws.amazon.com/billing/signup#/start/email\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    AWS account signup\n   </a>\n  </li>\n  <li>\n   <a href=\"https://docs.aws.amazon.com/eks/latest/userguide/setting-up.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Install AWS CLI\n   </a>\n  </li>\n  <li>\n   Used to authenticate your AWS account for CLI tools\n  </li>\n  <li>\n   <a href=\"https://eksctl.io/installation/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Install eksctl\n   </a>\n  </li>\n  <li>\n   Used to create\n   <code class=\"cw qc qd qe qf b\">\n    EKS\n   </code>\n   clusters easily\n  </li>\n  <li>\n   <a href=\"https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Install kubectl\n   </a>\n  </li>\n  <li>\n   Used to configure and debug deployments, pods, services, etc.\n  </li>\n  <li>\n   <a href=\"https://www.docker.com/products/docker-desktop/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Install Docker\n   </a>\n  </li>\n </ol>\n <p>\n  As you will see, nearly all AWS deployments revolve around\n  <code class=\"cw qc qd qe qf b\">\n   yaml\n  </code>\n  files that describe what you are deploying and how they connect together, as well as some CLI commands to actually run the deployment.\n </p>\n <p>\n  If at any time you aren\u2019t sure what\u2019s going on, I found it helpful to visit the AWS dashboard and explore the resources I had actually deployed. Usually, you will want to visit. I had the pages below favourited in AWS. Also, remember to set your region properly in the top right!\n </p>\n <figure>\n  <figcaption class=\"om fe on ny nz oo op be b bf z dt\">\n   My AWS console favourites\n  </figcaption>\n </figure>\n <h2>\n  Note on how deployments work\n </h2>\n <p>\n  For a majority of deployments, you will typically have\n </p>\n <ol>\n  <li>\n   The cluster\n  </li>\n  <li>\n   The deployed app, scaled to X replicas\n  </li>\n  <li>\n   A load balancer, to balance the incoming requests between X replicas\n  </li>\n </ol>\n <p>\n  In the examples below, most will have a\n  <code class=\"cw qc qd qe qf b\">\n   yaml\n  </code>\n  for the deployed app, a\n  <code class=\"cw qc qd qe qf b\">\n   yaml\n  </code>\n  for the load balancer, and a command to create the cluster you want to run on.\n </p>\n <h2>\n  Helpful CLI Commands\n </h2>\n <p>\n  A few CLI commands proved to be extremely helpful for debugging and monitoring deployments.\n </p>\n <pre><span class=\"qz os gt qf b bf ra rb l rc rd\" id=\"d9d5\"># get the state of pods/deployments\nkubectl get pods\nkubectl get deployments\n\n# useful for seeing logs/events of pods + full yaml config\nkubectl describe pod &lt;pod name&gt;\nkubectl logs &lt;pod name&gt;\n\n# list clusters kubectl knows about\nkubectl config get-contexts\n\n# switch kubectl to another cluster\nkubectl config use-context &lt;context name&gt;\n\n# delete things\nkubectl delete &lt;pod/deployment/service&gt; &lt;name&gt;</span></pre>\n <h1>\n  Step 2: Deploying Text Embeddings Interface\n </h1>\n <p>\n  In order to run embeddings fast, we will deploy an embeddings server using HuggingFace\u2019s\n  <a href=\"https://github.com/huggingface/text-embeddings-inference\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Text Embedding Interface\n  </a>\n  (TEI). This server has production-level features and optimizations out-of-the-box, including continuous batching, flash-attention, rust implementation, and more. HuggingFace provides prebuilt docker images to simplify deployment.\n </p>\n <p>\n  However, the first step to running embeddings fast is to have a GPU. If you just signed up for AWS, you will have to request a quota increase. For me, I requested a few times for G5 instances (which run an Nvidia A10G GPU), and after a few days of testing on CPU, AWS gave me access to use up to 4 G5 instances.\n </p>\n <p>\n  Once you have a quota for GPU instances (like G5 nodes), you can create your cluster and deploy\n </p>\n <pre><span class=\"qz os gt qf b bf ra rb l rc rd\" id=\"bfc5\">eksctl create cluster --name embeddings --node-type=g5.xlarge --nodes 1\nsleep 5\nkubectl create -f ./tei-deployment.yaml\nsleep 5\nkubectl create -f ./tei-service.yaml\nsleep 5\necho \"Embeddings URL is: <span class=\"hljs-symbol\">&amp;lt;</span>http://$<span class=\"hljs-symbol\">&amp;gt;</span>(kubectl get svc tei-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\"</span></pre>\n <p>\n  The code above will create a cluster, a deployment (i.e. our TEI server) and a load balancer server.\n </p>\n <p>\n  You can see the yaml configs in\n  <a href=\"https://github.com/run-llama/llamaindex_aws_ingestion/tree/main/tei\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   the repo\n  </a>\n  , and you can edit them as needed.\n </p>\n <p>\n  <strong>\n   NOTE:\n  </strong>\n  Make sure to write down the URL printed at the end! If you forget, you can get the URL in the\n  <code class=\"cw qc qd qe qf b\">\n   EKS\n  </code>\n  page on AWS. You\u2019ll want the external IP for the load balancer.\n </p>\n <h1>\n  Step 3: Deploying RabbitMQ\n </h1>\n <p>\n  RabbitMQ is where we will queue documents to be ingested. RabbitMQ is a message broker system that allows for powerful yet simple queuing of tasks. Since some ingestion tasks (like metadata extraction, embeddings) can be slow, the more naive approach of a REST API would leave connections open while data is processed. Instead, using a queue allows us to quickly upload data and offload processing to scalable message consumer(s). It also allows us to add parallelism with ease, where in our system, each\n  <code class=\"cw qc qd qe qf b\">\n   Document\n  </code>\n  object is processed independently by a consumer.\n </p>\n <p>\n  Deploying RabbitMQ on\n  <code class=\"cw qc qd qe qf b\">\n   EKS\n  </code>\n  was a little tricky, but using the RabbitMQ operator installed with\n  <code class=\"cw qc qd qe qf b\">\n   krew\n  </code>\n  , many things are abstracted away.\n </p>\n <p>\n  First, you need to create your cluster. For whatever reason, this didn\u2019t work unless I also specified the zones\n </p>\n <pre><span class=\"qz os gt qf b bf ra rb l rc rd\" id=\"4d82\">eksctl create cluster \\\n  --name mqCluster \\\n  --zones us-east-1a,us-east-1b,us-east-1c,us-east-1d,us-east-1f</span></pre>\n <p>\n  Since RabbitMQ needs storage, and each replica needs to share the same storage, we should give our cluster permission to provision and use\n  <code class=\"cw qc qd qe qf b\">\n   EBS\n  </code>\n  for storage. This was a frustrating step to figure out since most existing guides skip this detail!\n </p>\n <pre><span class=\"qz os gt qf b bf ra rb l rc rd\" id=\"fb1b\">eksctl utils associate-iam-oidc-provider \\\n  --cluster=mqCluster \\\n  --region us-east<span class=\"hljs-number\">-1</span> \\\n  --approve\nsleep <span class=\"hljs-number\">5</span>\neksctl create iamserviceaccount \\\n    --name ebs-csi-controller-sa \\\n    --<span class=\"hljs-keyword\">namespace</span> kube-system \\\n    --cluster mqCluster \\\n    --role-name AmazonEKS_EBS_CSI_DriverRole \\\n    --role-only \\\n    --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \\\n    --approve\nsleep <span class=\"hljs-number\">5</span>\neksctl create addon \\\n  --name aws-ebs-csi-driver \\\n  --cluster mqCluster \\\n  --service-account-role-arn arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):role/AmazonEKS_EBS_CSI_DriverRole \\\n  --force</span></pre>\n <p>\n  From there, we can install the RabbitMQ operator and create our deployment\n </p>\n <pre><span class=\"qz os gt qf b bf ra rb l rc rd\" id=\"4f79\">kubectl apply -f <span class=\"hljs-symbol\">&amp;lt;</span>https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml<span class=\"hljs-symbol\">&amp;gt;</span>\nsleep 5\nkubectl apply -f rabbitmqcluster.yaml\nsleep 5\necho \"RabbitMQ URL is: $(kubectl get svc production-rabbitmqcluster -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\"</span></pre>\n <p>\n  As usual, the code for all this can be found in the\n  <a href=\"https://github.com/run-llama/llamaindex_aws_ingestion/tree/main/rabbitmq\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   git repo\n  </a>\n  .\n </p>\n <p>\n  <strong>\n   NOTE:\n  </strong>\n  Make sure to write down the URL printed at the end! If you forget, you can get the URL in the\n  <code class=\"cw qc qd qe qf b\">\n   EKS\n  </code>\n  page on AWS. You\u2019ll want the external IP for the load balancer.\n </p>\n <p>\n  You can monitor your RabbitMQ queues by visiting \u201c&lt;rabbitmq_url&gt;:15672\u201d and signing in with \u201cguest\u201d/\u201dguest\u201d.\n </p>\n <h1>\n  Step 4: Deploying IngestionPipeline Workers\n </h1>\n <p>\n  This is where the real meat of work comes in. We need to create a\n  <code class=\"cw qc qd qe qf b\">\n   consumer\n  </code>\n  that will endlessly pull from our\n  <code class=\"cw qc qd qe qf b\">\n   RabbitMQ\n  </code>\n  queue, ingest data with the help of TEI, and then put that data into our vector db.\n </p>\n <p>\n  To do this, we can make a FastAPI server that does two things\n </p>\n <ol>\n  <li>\n   Starts a thread to consume from our queue\n  </li>\n  <li>\n   Starts a webserver, to enable us to specify a readiness check, and gives us room to add more features in the future (i.e. probing queue status, logs, etc.)\n  </li>\n </ol>\n <p>\n  First, we write our code, as you can see in\n  <a href=\"https://github.com/run-llama/llamaindex_aws_ingestion/blob/main/worker/worker.py\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   worker.py\n  </a>\n </p>\n <p>\n  Then, we dockerize our app by creating a simple\n  <a href=\"https://github.com/run-llama/llamaindex_aws_ingestion/blob/main/worker/Dockerfile\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Dockerfile\n  </a>\n  and running:\n </p>\n <pre><span class=\"qz os gt qf b bf ra rb l rc rd\" id=\"5eee\">docker build -t &lt;image_name&gt; .\ndocker tag &lt;image_name&gt;:latest &lt;image_name&gt;:&lt;version&gt;\ndocker push &lt;image_name&gt;:&lt;version&gt;</span></pre>\n <p>\n  With our app dockerized, we can complete the\n  <code class=\"cw qc qd qe qf b\">\n   worker-deployment.yaml\n  </code>\n  file by filling in\n </p>\n <ul>\n  <li>\n   Our embeddings URL under\n   <code class=\"cw qc qd qe qf b\">\n    TEI_URL\n   </code>\n  </li>\n  <li>\n   Our rabbit-mq URL under\n   <code class=\"cw qc qd qe qf b\">\n    RABBITMQ_URL\n   </code>\n  </li>\n  <li>\n   Our image name under container image\n  </li>\n  <li>\n   Our cluster details (in this case, a weaviate URL and API key)\n  </li>\n </ul>\n <p>\n  With the\n  <code class=\"cw qc qd qe qf b\">\n   yaml\n  </code>\n  file complete, now we can properly deploy the worker\n </p>\n <pre><span class=\"qz os gt qf b bf ra rb l rc rd\" id=\"2b4a\">eksctl create cluster --name mq-workers --zones us-east-1a,us-east-1b,us-east-1c,us-east-1d,us-east-1f\nsleep 5\nkubectl create -f ./worker-deployment.yaml\nsleep 5\nkubectl create -f ./worker-service.yaml</span></pre>\n <h1>\n  Step 5: Making a User-Facing Lambda Function\n </h1>\n <p>\n  Our lambda function will rely on a single external dependency \u2014\n  <code class=\"cw qc qd qe qf b\">\n   pika\n  </code>\n  \u2014 which is used to communicate with RabbitMQ.\n </p>\n <p>\n  Create a python file called\n  <code class=\"cw qc qd qe qf b\">\n   lambda_function.py\n  </code>\n  with the following code:\n </p>\n <pre><span class=\"qz os gt qf b bf ra rb l rc rd\" id=\"830d\"><span class=\"hljs-keyword\">import</span> pika\n<span class=\"hljs-keyword\">import</span> json\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">lambda_handler</span>(<span class=\"hljs-params\">event, context</span>):\n    <span class=\"hljs-keyword\">try</span>:\n        body = json.loads(event.get(<span class=\"hljs-string\">'body'</span>, <span class=\"hljs-string\">'{}'</span>))\n    <span class=\"hljs-keyword\">except</span>:\n        body = event.get(<span class=\"hljs-string\">'body'</span>, {})\n        \n    user = body.get(<span class=\"hljs-string\">'user'</span>, <span class=\"hljs-string\">''</span>)\n    documents = body.get(<span class=\"hljs-string\">'documents'</span>, [])\n    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> user <span class=\"hljs-keyword\">or</span> <span class=\"hljs-keyword\">not</span> documents:\n        <span class=\"hljs-keyword\">return</span> {\n            <span class=\"hljs-string\">'statusCode'</span>: <span class=\"hljs-number\">400</span>,\n            <span class=\"hljs-string\">'body'</span>: json.dumps(<span class=\"hljs-string\">'Missing user or documents'</span>)\n        }\n    \n    credentials = pika.PlainCredentials(<span class=\"hljs-string\">\"guest\"</span>, <span class=\"hljs-string\">\"guest\"</span>)\n    parameters = pika.ConnectionParameters(\n        host=<span class=\"hljs-string\">\"hostname.amazonaws.com\"</span>, \n        port=<span class=\"hljs-number\">5672</span>, \n        credentials=credentials\n    )\n    \n    connection = pika.BlockingConnection(parameters=parameters)\n    channel = connection.channel()\n    channel.queue_declare(queue=<span class=\"hljs-string\">'etl'</span>)\n\n    <span class=\"hljs-keyword\">for</span> document <span class=\"hljs-keyword\">in</span> documents:\n        data = {\n            <span class=\"hljs-string\">'user'</span>: user,\n            <span class=\"hljs-string\">'documents'</span>: [document]\n        }\n        channel.basic_publish(\n            exchange=<span class=\"hljs-string\">\"\"</span>, \n            routing_key=<span class=\"hljs-string\">'etl'</span>, \n            body=json.dumps(data)\n        )\n\n    <span class=\"hljs-keyword\">return</span> {\n        <span class=\"hljs-string\">'statusCode'</span>: <span class=\"hljs-number\">200</span>,\n        <span class=\"hljs-string\">'body'</span>: json.dumps(<span class=\"hljs-string\">'Documents queued for ingestion'</span>)\n    }</span></pre>\n <p>\n  The function above processes incoming requests, and publishes each document as a single message in our rabbitmq cluster.\n </p>\n <p>\n  To deploy a lambda file with dependencies, we need to create a zip of our lambda function + all dependencies. To do this, we can create a\n  <code class=\"cw qc qd qe qf b\">\n   requirements.txt\n  </code>\n  file with our dependencies and run:\n </p>\n <pre><span class=\"qz os gt qf b bf ra rb l rc rd\" id=\"9737\">pip install -r requirements.txt -t .\nzip -r9 ../ingestion_lambda.zip . -x \"*.git*\" \"*setup.sh*\" \"*requirements.txt*\" \"*.zip*\"</span></pre>\n <p>\n  With our code and zip file in hand, head over to the Lambda AWS page in your browser.\n </p>\n <ol>\n  <li>\n   Select\n   <code class=\"cw qc qd qe qf b\">\n    Create function\n   </code>\n  </li>\n  <li>\n   Give it a name, select a python runtime (I used Python 3.11)\n  </li>\n  <li>\n   Click\n   <code class=\"cw qc qd qe qf b\">\n    Create function\n   </code>\n   at the bottom\n  </li>\n  <li>\n   In the code editor, you\u2019ll see an\n   <code class=\"cw qc qd qe qf b\">\n    Upload from\n   </code>\n   button \u2014 click that, and upload your zip file\n  </li>\n  <li>\n   Click test, give the test a name, and paste the following JSON\n  </li>\n </ol>\n <pre><span class=\"qz os gt qf b bf ra rb l rc rd\" id=\"8f73\">{\n    <span class=\"hljs-string\">\"body\"</span>: {<span class=\"hljs-string\">\"user\"</span>: <span class=\"hljs-string\">\"Test\"</span>, <span class=\"hljs-string\">\"documents\"</span>: [{<span class=\"hljs-string\">\"text\"</span>: <span class=\"hljs-string\">\"test\"</span>}]}\n}</span></pre>\n <p>\n  Once the test works, the\n  <code class=\"cw qc qd qe qf b\">\n   Deploy\n  </code>\n  button will not be grayed out, and you can click it.\n </p>\n <p>\n  Your public URL will be listed in the upper right pane under\n  <code class=\"cw qc qd qe qf b\">\n   Function URL\n  </code>\n  \u2014 this is the URL you can use to call your lambda function from anywhere!\n </p>\n <h1>\n  Step 6: Reap the Scaling Benefits\n </h1>\n <p>\n  Now, we can run our system end-to-end!\n </p>\n <p>\n  To ingest data, you can run:\n </p>\n <pre><span class=\"qz os gt qf b bf ra rb l rc rd\" id=\"977a\"><span class=\"hljs-keyword\">import</span> requests\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> Document, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(<span class=\"hljs-string\">\"./data\"</span>).load_data()\n\n<span class=\"hljs-comment\"># this will also be the namespace for the vector store </span>\n<span class=\"hljs-comment\"># -- for weaviate, it needs to start with a captial and only alpha-numeric</span>\nuser = <span class=\"hljs-string\">\"Loganm\"</span> \n\n<span class=\"hljs-comment\"># upload in batches</span>\n<span class=\"hljs-keyword\">for</span> batch_idx <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\">0</span>, <span class=\"hljs-built_in\">len</span>(documents), <span class=\"hljs-number\">30</span>):\n  documents_batch = documents[batch_idx:batch_idx+<span class=\"hljs-number\">30</span>]\n  body = {\n    <span class=\"hljs-string\">'user'</span>: user,\n    <span class=\"hljs-string\">'documents'</span>: [doc.json() <span class=\"hljs-keyword\">for</span> doc <span class=\"hljs-keyword\">in</span> documents_batch]\n  }\n\n <span class=\"hljs-comment\"># use the URL of our lambda function here</span>\n response = requests.post(<span class=\"hljs-string\">\"&amp;lt;lambda_url&amp;gt;\"</span>, json=body)\n <span class=\"hljs-built_in\">print</span>(response.text)</span></pre>\n <p>\n  Then, to use our data:\n </p>\n <pre><span class=\"qz os gt qf b bf ra rb l rc rd\" id=\"2fdb\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> VectorStoreIndex\n<span class=\"hljs-keyword\">from</span> llama_index.vector_stores <span class=\"hljs-keyword\">import</span> WeaviateVectorStore\n<span class=\"hljs-keyword\">import</span> weaviate\n\nauth_config = weaviate.AuthApiKey(api_key=<span class=\"hljs-string\">\"...\"</span>)\nclient = weaviate.Client(url=<span class=\"hljs-string\">\"...\"</span>, auth_client_secret=auth_config)\nvector_store = WeaviateVectorStore(weaviate_client=client, class_prefix=<span class=\"hljs-string\">\"&amp;lt;user&amp;gt;\"</span>)\nindex = VectorStoreIndex.from_vector_store(vector_store)</span></pre>\n <h1>\n  Step 7: Clean-up\n </h1>\n <p>\n  AWS doesn\u2019t make it easy to estimate costs of all this. But after running and testing things for a few days, I had only spent ~$40CAD. This included leaving some services running overnight (whoops!).\n </p>\n <p>\n  When you are done with your deployment, you\u2019ll want to delete the resources so that you aren\u2019t charged for things you aren\u2019t using. To delete my clusters, I ran the following:\n </p>\n <pre><span class=\"qz os gt qf b bf ra rb l rc rd\" id=\"90e4\">eksctl delete cluster embeddings\neksctl delete cluster mq-worker\nkubectl rabbitmq delete production-rabbitmqcluster</span></pre>\n <p>\n  Then, in the AWS UI console, I deleted any remaining resources on the\n  <code class=\"cw qc qd qe qf b\">\n   EC2\n  </code>\n  and\n  <code class=\"cw qc qd qe qf b\">\n   CloudFormation\n  </code>\n  pages, as well as double-checking that everything was deleted on the\n  <code class=\"cw qc qd qe qf b\">\n   EKS\n  </code>\n  page.\n </p>\n <h1>\n  Conclusion\n </h1>\n <p>\n  Using this setup, I was able to reduce index-construction times for creating large indexes dramatically. Before, it would take about 10\u201320 minutes to create the index for 25K documents, and with this setup (2 rabbitmq nodes, 2 workers, 2 embeddings), it was down to 5 minutes! And with more scaling, it could be even faster.\n </p>\n <h1>\n  Next Steps\n </h1>\n <p>\n  From here, there are several improvements that I can think of\n </p>\n <ul>\n  <li>\n   better secrets management\n  </li>\n  <li>\n   adding auto-scaling\n  </li>\n  <li>\n   adding a retrieval lambda function (would require making a docker image for lambda + llama-index)\n  </li>\n  <li>\n   adding queue stats to the fastapi server\n  </li>\n  <li>\n   deploying redis for document management on the IngestionPipeline\n  </li>\n </ul>\n <p>\n  I encourage anyone to take this work and build off it. Be sure share any improvement on the github repository as well!\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 21197, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d0b0051-1b7f-4ed8-951c-f4fcd00755af": {"__data__": {"id_": "8d0b0051-1b7f-4ed8-951c-f4fcd00755af", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.html", "file_name": "secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.html", "file_type": "text/html", "file_size": 13622, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.html", "file_name": "secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.html", "file_type": "text/html", "file_size": 13622, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "209cc9fd6966436241836dcca4c9c02439fc09503b384bd4b9bbc351274b2593", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  One of the many amazing feats that LLMs are capable of is generating executable code. This can be used to solve a variety of complex problems that require calculations and fixed logic that traditional computing excels at but LLMs can struggle to perform directly. When building agents to perform complex tasks, equipping your agent with code execution as an available tool can be a powerful strategy.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  However, this strategy comes with a major drawback: executable code can be flawed or even dangerous to execute, and detecting whether code will be problematic prior to executing it is arguably an expression of the\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://en.wikipedia.org/wiki/Halting_problem\" rel=\"noreferrer noopener\">\n   Halting Problem\n  </a>\n  , making it impossible to guarantee success at detection.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The solution is\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://en.wikipedia.org/wiki/Sandbox_(computer_security)\" rel=\"noreferrer noopener\">\n   sandboxing\n  </a>\n  , to isolate potentially problematic code from the host environment. Now, thanks to dynamic sessions in Azure Container Apps, the ability to execute sandboxed code generated by an LLM is simple directly from LlamaIndex. It\u2019s implemented as a tool that can be used by any LlamaIndex agent.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In this blog post we\u2019ll show you exactly how to use the new Azure Code Interpreter tool and walk you through a couple of examples of how to make the most of it. You can see the full code in\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/examples/tools/azure_code_interpreter/\" rel=\"noreferrer noopener\">\n   this notebook\n  </a>\n  and read more in the\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://llamahub.ai/l/tools/llama-index-tools-azure-code-interpreter\" rel=\"noreferrer noopener\">\n   tool documentation\n  </a>\n  on LlamaHub and on\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://learn.microsoft.com/en-us/azure/container-apps/sessions-code-interpreter?tabs=azure-cli\" rel=\"noreferrer noopener\">\n   learn.microsoft.com\n  </a>\n  .\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Set up Azure Container Apps dynamic sessions\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  First, install our python packages including the tool:\n </p>\n <pre><code>pip install llama-index\npip install llama-index-llms-azure\npip install llama-index-tools-azure-code-interpreter</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/latest/examples/tools/azure_code_interpreter/\" rel=\"noreferrer noopener\">\n   the notebook\n  </a>\n  we\u2019re using GPT 3.5 Turbo hosted on Azure as the LLM, but you can use any LLM capable of tool use:\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index.llms.azure_openai <span class=\"hljs-keyword\">import</span> AzureOpenAI\nllm = AzureOpenAI(\n    model=<span class=\"hljs-string\">\"gpt-35-turbo\"</span>,\n    deployment_name=<span class=\"hljs-string\">\"gpt-35-deploy\"</span>,\n    api_key=api_key,\n    azure_endpoint=azure_endpoint,\n    api_version=api_version,\n)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Once you\u2019ve got your LLM set up, you\u2019ll need to\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://learn.microsoft.com/en-us/azure/container-apps/sessions-code-interpreter?tabs=azure-cli#code-interpreter-session-pool\" rel=\"noreferrer noopener\">\n   create a session pool\n  </a>\n  to host your executions. Doing this will give you a pool management endpoint URL that you can provide to LlamaIndex like this:\n </p>\n <pre><code><span class=\"hljs-comment\"># Import the AzureCodeInterpreterToolSpec from llama_index</span>\n<span class=\"hljs-keyword\">from</span> llama_index.tools.azure_code_interpreter <span class=\"hljs-keyword\">import</span> (\n    AzureCodeInterpreterToolSpec,\n)\n\n<span class=\"hljs-comment\"># Create the AzureCodeInterpreterToolSpec with the pool_managment_endpoint set to your session management endpoint</span>\n<span class=\"hljs-comment\"># It is optional to set the local_save_path, but it is recommended to set it to a path where the tool can automatically save any intermediate data generated from Python code's output.</span>\nazure_code_interpreter_spec = AzureCodeInterpreterToolSpec(\n    pool_managment_endpoint=<span class=\"hljs-string\">\"your-pool-management-endpoint\"</span>,\n    local_save_path=<span class=\"hljs-string\">\"local-file-path-to-save-intermediate-data\"</span>,\n)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  This sets up a tool ready to be used with LlamaIndex. You\u2019re now ready to set up your agent:\n </p>\n <pre><code><span class=\"hljs-comment\"># Import the ReActAgent</span>\n<span class=\"hljs-keyword\">from</span> llama_index.core.agent <span class=\"hljs-keyword\">import</span> ReActAgent\n\n<span class=\"hljs-comment\"># Create the ReActAgent and inject the tools defined in the AzureDynamicSessionsToolSpec</span>\nagent = ReActAgent.from_tools(\n    azure_code_interpreter_spec.to_tool_list(), llm=llm, verbose=<span class=\"hljs-literal\">True</span>\n)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In this example we\u2019re providing only a single tool, but you could provide any other tools you like to your ReAct agent. Now you\u2019ve got an agent, you\u2019re ready to ask it to perform tasks!\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Dynamic sessions code interpreter in action\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In our first example, we\u2019re going to ask the agent the time in Seattle. This is usually a tricky task for LLMs, which don\u2019t know what time it is anywhere!\n </p>\n <pre><code><span class=\"hljs-comment\"># Test the agent with simple answers that could leverage Python codes</span>\n<span class=\"hljs-built_in\">print</span>(agent.chat(<span class=\"hljs-string\">\"Tell me the current time in Seattle.\"</span>))</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The agent generates python code to determine the time and convert it to the correct time zone. It passes this code to Azure Container Apps dynamic sessions, which execute the code and return the answer:\n </p>\n <pre><code>Thought: To provide the current time in Seattle, I need to calculate it based on the current UTC time and adjust for Seattle's time zone, which is Pacific Daylight Time (PDT) during daylight saving time and Pacific Standard Time (PST) outside of daylight saving time. PDT is UTC-7, and PST is UTC-8. I can use the code interpreter tool to get the current UTC time and adjust it accordingly.\nAction: code_interpreter\nAction Input: {'python_code': \"from datetime import datetime, timedelta; import pytz; utc_now = datetime.now(pytz.utc); seattle_time = utc_now.astimezone(pytz.timezone('America/Los_Angeles')); seattle_time.strftime('%Y-%m-%d %H:%M:%S %Z%z')\"}\nObservation: {'$id': '1', 'status': 'Success', 'stdout': '', 'stderr': '', 'result': '2024-05-04 13:54:09 PDT-0700', 'executionTimeInMilliseconds': 120}\nThought: I can answer without using any more tools. I'll use the user's language to answer.\nAnswer: The current time in Seattle is 2024-05-04 13:54:09 PDT.\nThe current time in Seattle is 2024-05-04 13:54:09 PDT.\n</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  You can also use the tool to safely inspect and manipulate data, as in this example where we ask it to open a CSV file and answer questions about it:\n </p>\n <pre><code><span class=\"hljs-comment\"># Upload a sample temperature file of a day in Redmond Washington and ask a question about it</span>\nres = azure_code_interpreter_spec.upload_file(\n    local_file_path=<span class=\"hljs-string\">\"./TemperatureData.csv\"</span>\n)\n<span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(res) != <span class=\"hljs-number\">0</span>:\n    <span class=\"hljs-built_in\">print</span>(\n        agent.chat(<span class=\"hljs-string\">\"Find the highest temperature in the file that I uploaded.\"</span>)\n    )</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  It doesn\u2019t just read data from the CSV, it performs math on it to determine the highest temperature:\n </p>\n <pre><code>Thought: I need to use the list_files tool to get the metadata for the uploaded file, and then use python to read the file and find the highest temperature.\nAction: list_files\nAction Input: {}\nObservation: [RemoteFileMetadata(filename='TemperatureData.csv', size_in_bytes=514, file_full_path='/mnt/data/TemperatureData.csv')]\nThought: I have the metadata for the file. I need to use python to read the file and find the highest temperature.\nAction: code_interpreter\nAction Input: {'python_code': \"import csv\\n\\nwith open('/mnt/data/TemperatureData.csv', 'r') as f:\\n    reader = csv.reader(f)\\n    next(reader)\\n    highest_temp = float('-inf')\\n    for row in reader:\\n        temp = float(row[1])\\n        if temp &gt; highest_temp:\\n            highest_temp = temp\\nprint(highest_temp)\"}\nObservation: {'$id': '1', 'status': 'Success', 'stdout': '12.4\\n', 'stderr': '', 'result': '', 'executionTimeInMilliseconds': 26}\nThought: I have the highest temperature. I can answer the question.\nAnswer: The highest temperature in the file is 12.4 degrees.\nThe highest temperature in the file is 12.4 degrees.</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The tool can also safely modify your uploaded files, as in this example where we get it to sort the data in the CSV:\n </p>\n <pre><code><span class=\"hljs-comment\"># Ask the LLM to perform modifications on the sample temperature file</span>\n<span class=\"hljs-built_in\">print</span>(\n    agent.chat(\n        <span class=\"hljs-string\">\"Rearrange the temperature data in a descending order and save it back to the original csv file.\"</span>\n    )\n)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In this example, you\u2019ll see that the LLM initially produces code with a syntax error, but the agent is smart enough to figure out what went wrong and correct itself:\n </p>\n <pre><code>Thought: To proceed, I need to first list the files in the session pool to identify the uploaded temperature data file.\nAction: list_files\nAction Input: {}\nObservation: [RemoteFileMetadata(filename='TemperatureCurve.png', size_in_bytes=35512, file_full_path='/mnt/data/TemperatureCurve.png'), RemoteFileMetadata(filename='TemperatureData.csv', size_in_bytes=514, file_full_path='/mnt/data/TemperatureData.csv')]\nThought: The temperature data file is identified as 'TemperatureData.csv'. I need to read its contents, rearrange the temperature data in descending order, and then save the changes back to the original CSV file.\nAction: code_interpreter\nAction Input: {'python_code': \"import pandas as pd\\n\\ndf = pd.read_csv('/mnt/data/TemperatureData.csv')\\ndf_sorted = df.sort_values(by='Temperature', ascending=False)\\ndf_sorted.to_csv('/mnt/data/TemperatureData.csv', index=False)\\n'filename': 'TemperatureData.csv'\"}\nObservation: {'$id': '1', 'status': 'Failure', 'stdout': '', 'stderr': \"illegal target for annotation (3407605186.py, line 6)  Cell In[2363], line 6\\n    'filename': 'TemperatureData.csv'\\n    ^\\nSyntaxError: illegal target for annotation\\n\\n\", 'result': None, 'executionTimeInMilliseconds': 9}\nThought: There was a syntax error in the Python code due to an incorrect line. I need to correct the code and execute it again to sort the temperature data and save it back to the CSV file.\nAction: code_interpreter\nAction Input: {'python_code': \"import pandas as pd\\n\\ndf = pd.read_csv('/mnt/data/TemperatureData.csv')\\ndf_sorted = df.sort_values(by='Temperature', ascending=False)\\ndf_sorted.to_csv('/mnt/data/TemperatureData.csv', index=False)\"}\nObservation: {'$id': '1', 'status': 'Success', 'stdout': '', 'stderr': '', 'result': '', 'executionTimeInMilliseconds': 91}\nThought: I can answer without using any more tools. I'll use the user's language to answer.\nAnswer: The temperature data has been successfully rearranged in descending order and saved back to the original CSV file, 'TemperatureData.csv'.\nThe temperature data has been successfully rearranged in descending order and saved back to the original CSV file, 'TemperatureData.csv'.</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Modifying files would not be useful if you couldn\u2019t retrieve them after modification, which is done like this:\n </p>\n <pre><code><span class=\"hljs-comment\"># Download the modified file</span>\nazure_code_interpreter_spec.download_file_to_local(\n    remote_file_path=<span class=\"hljs-string\">\"TemperatureData.csv\"</span>,\n    local_file_path=<span class=\"hljs-string\">\"/.../SortedTemperatureData.csv\"</span>,\n)</code></pre>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Endless possibilities\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The scope of tasks that you can achieve with sandboxed code execution is as broad as programming itself, and having safe execution guaranteed allows you to confidently hand agents tasks that previously you might have been hesitant about. We think this is an amazing addition to our LLM agent capabilities and we\u2019re excited to see what you build with it.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 13591, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a85b9a85-1bb3-4a38-bcfd-07da5bc554f0": {"__data__": {"id_": "a85b9a85-1bb3-4a38-bcfd-07da5bc554f0", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.html", "file_name": "secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.html", "file_type": "text/html", "file_size": 7372, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.html", "file_name": "secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.html", "file_type": "text/html", "file_size": 7372, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "6af6fd50a8567d6491d7a9f5f2398eaf8e00a9009055770d72adda46cab2835b", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <em>\n   This is a guest post by Protect AI.\n  </em>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We believe that RAG will be one of the preferred approaches for enterprises when developing LLM applications to generate prompt responses that are more relevant, and accurate, tailored to and based on company-specific content. However, while analyzing web pages with ChatGPT may leave the LLM vulnerable to injections embedded within the webpage, it is crucial to recognize that injections may also be concealed within the vector database or knowledge graph where data is retrieved and injected into the LLM.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  That is why we\u2019re thrilled to describe how LLM Guard by Protect AI can secure your data sources accessed for context in your LLM application, built with LlamaIndex.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://llm-guard.com/\" rel=\"noreferrer noopener\">\n   LLM Guard\n  </a>\n  is an open source solution by\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://protectai.com/llm-guard\" rel=\"noreferrer noopener\">\n   Protect AI\n  </a>\n  designed to fortify the security of Large Language Models (\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.helpnetsecurity.com/2023/05/10/security-privacy-risks-large-language-models-video/\" rel=\"noreferrer noopener\">\n   LLMs\n  </a>\n  ). It is designed for easy integration and deployment in production environments. It provides extensive security scanners for both prompts and responses of LLMs to detect, redact, and sanitize against adversarial prompt attacks, data leakage, and integrity breaches (e.g. offensive content, hallucination).\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  LLM Guard was built for a straightforward purpose: despite the potential of LLMs, corporate adoption has been hesitant. This reluctance stems from the significant security risks and a lack of control and observability of implementing these technologies. With over 2.5M downloads of its models, and a Google Patch Reward, LLM Guard is the open source standard and market leader in LLM security at inference.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   Secure RAG with LlamaIndex\n  </strong>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In the following\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://llm-guard.com/usage/notebooks/llama_index_rag/\" rel=\"noreferrer noopener\">\n   example\n  </a>\n  , we showcase a practical approach to improve the security of your RAG application. Specifically, we will explore a RAG application designed to facilitate the automated screening of candidate CVs by HR teams. Within the batch of CVs, there exists a diverse pool of candidates, including one who lacks experience and consequently is not the most suitable candidate. The nature of the attack manifests as an embedded prompt injection within the CV of this particular candidate, concealed in white text, rendering it challenging to detect with the naked eye.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://llm-guard.com/usage/notebooks/llama_index_rag/\" rel=\"noreferrer noopener\">\n   In the notebook example\n  </a>\n  , we conducted the attack initially and then repeated the process subsequent to fortifying the application with LLM Guard. With this example, we show how you can use LLM Guard and LlamaIndex for both input and output scanning of documents to detect any malicious content. Although ideally, we should scan documents before the\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/transformations.html\" rel=\"noreferrer noopener\">\n   ingestion\n  </a>\n  , for simplicity in the example, we chose to do scanning during\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/root.html\" rel=\"noreferrer noopener\">\n   retrieval\n  </a>\n  . In real-use cases, it's critical to do the scanning both during retrieval of real-time data from APIs (not vector stores) which we still need to verify as it can contain poisoned sources of information. For output scanning, it can simply be done by taking the results generated by LlamaIndex and running them through LLM Guard.\n </p>\n <pre><code>llm = OpenAI(model=<span class=\"hljs-string\">\"gpt-3.5-turbo\"</span>, temperature=<span class=\"hljs-number\">0.1</span>, output_parser=output_parser)\n\nservice_context = ServiceContext.from_defaults(\n    llm=llm, \n    transformations=transformations,\n    callback_manager=callback_manager,\n)\nindex = VectorStoreIndex.from_documents(\n    documents, service_context=service_context\n)\n\ninput_scanners = [\n    Anonymize(vault, entity_types=[<span class=\"hljs-string\">\"PERSON\"</span>, <span class=\"hljs-string\">\"EMAIL_ADDRESS\"</span>, <span class=\"hljs-string\">\"EMAIL_ADDRESS_RE\"</span>, <span class=\"hljs-string\">\"PHONE_NUMBER\"</span>]), \n    Toxicity(), \n    PromptInjection(),\n    Secrets()\n]\n\nllm_guard_postprocessor = LLMGuardNodePostProcessor(\n    scanners=input_scanners,\n    fail_fast=<span class=\"hljs-literal\">False</span>,\n    skip_scanners=[<span class=\"hljs-string\">\"Anonymize\"</span>],\n)\n\nquery_engine = index.as_query_engine(\n    similarity_top_k=<span class=\"hljs-number\">3</span>,\n    node_postprocessors=[llm_guard_postprocessor]\n)\nresponse = query_engine.query(<span class=\"hljs-string\">\"I am screening candidates for adult caregiving opportunity. Please recommend me an experienced person. Return just a name\"</span>)\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-built_in\">str</span>(response))\n</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   LLM Guard protects your LLM applications\n  </strong>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  As demonstrated in the practical example of securing an HR screening application with\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://llm-guard.com/\" rel=\"noreferrer noopener\">\n   LLM Guard\n  </a>\n  , the significance of mitigating potential attacks, cannot be overstated. Besides that, as LLMs evolve rapidly and embed advanced capabilities like agency and multi-modality, the complexity and impact of potential breaches escalate significantly. Thus, prioritizing RAG security becomes not just a necessity but rather fundamental in safeguarding against increasingly sophisticated threats and ensuring the integrity of critical enterprise LLM applications.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Try out LLM Guard by going to our\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/protectai/llm-guard\" rel=\"noreferrer noopener\">\n   library\n  </a>\n  or\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://llm-guard.com/\" rel=\"noreferrer noopener\">\n   documentation\n  </a>\n  . Also,\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://join.slack.com/t/laiyerai/shared_invite/zt-28jv3ci39-sVxXrLs3rQdaN3mIl9IT~w\" rel=\"noreferrer noopener\">\n   join our Slack\n  </a>\n  channel for any questions!\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 7369, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8e89f7d7-9a58-4ca0-b468-42d0d14c0287": {"__data__": {"id_": "8e89f7d7-9a58-4ca0-b468-42d0d14c0287", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d.html", "file_name": "shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d.html", "file_type": "text/html", "file_size": 5407, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d.html", "file_name": "shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d.html", "file_type": "text/html", "file_size": 5407, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "b214253c5be161d00cfe5b50646c37966cabb3fd832cb15000900d42c7a1153e", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <figure>\n  <figcaption class=\"no fe np na nb nq nr be b bf z dt\">\n   It\u2019s a llama on a ship, geddit?\n  </figcaption>\n </figure>\n <p>\n  Last week\n  <a href=\"/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   we released create-llama\n  </a>\n  , a command-line tool to generate a full-stack LlamaIndex application for Retrieval-Augmented Generation (RAG). The response was fantastic, so we\u2019ll be following up with more templates and more features. We also wanted to show you just how easy it is to get your generated app all the way to production. So here\u2019s a step by step guide, for each of the three backends we currently support: Next.js serverless, Express, and Python.\n </p>\n <h1>\n  Next.js backend\n </h1>\n <p>\n  The serverless full-stack Next.js application is the simplest version to deploy as you only have one artifact to deploy. Because it\u2019s a Next.js app we\u2019ll be deploying to\n  <a href=\"https://vercel.com\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Vercel\n  </a>\n  , the home of Next.js.\n </p>\n <h2>\n  Step 1: run create-llama\n </h2>\n <p>\n  First run create-llama to generate your app. We strongly recommend generating a new\n  <a href=\"https://platform.openai.com/api-keys\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   OpenAI API key\n  </a>\n  and supplying it at generation time (create-llama apps can be customized to use other LLMs but that\u2019s out of scope for this tutorial).\n </p>\n <h2>\n  Step 2: create a GitHub repository and push your app to it\n </h2>\n <p>\n  The easiest way to deploy on Vercel is from a linked GitHub repository. Your generated app is already set up as a git repo, so all you have to do after creating a new empty repo is follow the instructions to push it up. This should give you a repo that looks a bit like this:\n </p>\n <h2>\n  Step 3: import your repo into Vercel\n </h2>\n <p>\n  Select the option to create a new project from a git repo:\n </p>\n <p>\n  and select the repo you just created after authorizing:\n </p>\n <h2>\n  Step 4: configure your project\n </h2>\n <p>\n  Because this is a Next.js app and this is Vercel, there\u2019s very little you need to do! The only thing you need to remember is to click \u201cenvironment variables\u201d and create a variable called\n  <code class=\"cw qs qt qu qv b\">\n   OPENAI_API_KEY\n  </code>\n  with your key.\n </p>\n <h2>\n  Step 5: Deploy!\n </h2>\n <p>\n  That\u2019s it! Deploying a Next.js app to Vercel is pretty easy.\n </p>\n <p>\n  Your deployed app should look like this:\n </p>\n <p>\n  Congratulations, you\u2019ve deployed a full-stack RAG application!\n </p>\n <h1>\n  Express backend\n </h1>\n <p>\n  If you chose to generate an Express backend with a Next.js frontend instead, let\u2019s get you into production with those. We\u2019ll be deploying both frontend and backend to\n  <a href=\"https://render.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Render\n  </a>\n  , a fantastic service for both static sites and dynamic web applications.\n </p>\n <p>\n  There will be 3 big things to do here:\n </p>\n <ul>\n  <li>\n   Deploy the static frontend\n  </li>\n  <li>\n   Deploy the backend, and give the frontend permission to call it\n  </li>\n  <li>\n   Tell the frontend where the backend is located\n  </li>\n </ul>\n <p>\n  We promise you\u2019ll be production in no time.\n </p>\n <h2>\n  Step 1: run create-llama\n </h2>\n <p>\n  This is just like the same step in Next.js\n </p>\n <h2>\n  Step 2: push the code to a new GitHub repo\n </h2>\n <p>\n  Like Vercel, the easiest way to push a site to production is from a linked git repo. Your generated app already has a git repo initiated with\n  <code class=\"cw qs qt qu qv b\">\n   frontend\n  </code>\n  and\n  <code class=\"cw qs qt qu qv b\">\n   backend\n  </code>\n  folders, so you can go ahead and push them both to a single GitHub repository just as in the Next.js backend.\n </p>\n <h2>\n  Step 3: Start a new static site\n </h2>\n <p>\n  We\u2019ll be deploying your frontend first as a static site. After authorizing, select the repository where you pushed your frontend and backend; we\u2019ll specify that we\u2019re deploying the frontend in the next step.\n </p>\n <h2>\n  Step 4: configure your static site\n </h2>\n <p>\n  There are several changes you need to make to the default configuration to successfully publish your static frontend:\n </p>\n <ul>\n  <li>\n   Name your site something memorable, it will become the URL of your site once it\u2019s deployed\n  </li>\n  <li>\n   Set your root directory to\n   <code class=\"cw qs qt qu qv b\">\n    frontend\n   </code>\n  </li>\n  <li>\n   Set your build command to\n   <code class=\"cw qs qt qu qv b\">\n    npm install; npm run build\n   </code>\n  </li>\n  <li>\n   Set your publish directory to\n   <code class=\"cw qs qt qu qv b\">\n    out\n   </code>\n  </li>\n  <li>\n   Finally, click \u201cAdvanced\u201d and set an environment variable called\n   <code class=\"cw qs qt qu qv b\">\n    NODE_VERSION\n   </code>\n   to\n   <code class=\"cw qs qt qu qv b\">\n    20\n   </code>\n   . The default on Render is a much older version of Node so don\u2019t skip this step!\n  </li>\n </ul>\n <h2>\n  Step 5: deploy your static frontend\n </h2>\n <p>\n  Click the Deploy button and watch your site build! You should now have a site live at a URL something like\n  <code class=\"cw qs qt qu qv b\">\n   frontend-name-you-picked.onrender.com\n  </code>\n  .\n </p>\n <h2>\n  Step 6: start a new web service\n </h2>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5364, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d44d1b66-a6d3-4de4-97e6-449624b3fce1": {"__data__": {"id_": "d44d1b66-a6d3-4de4-97e6-449624b3fce1", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/simplify-your-rag-application-architecture-with-llamaindex-postgresml.html", "file_name": "simplify-your-rag-application-architecture-with-llamaindex-postgresml.html", "file_type": "text/html", "file_size": 12017, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/simplify-your-rag-application-architecture-with-llamaindex-postgresml.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/simplify-your-rag-application-architecture-with-llamaindex-postgresml.html", "file_name": "simplify-your-rag-application-architecture-with-llamaindex-postgresml.html", "file_type": "text/html", "file_size": 12017, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "1de78c55852abf8b2153ecf54590a46c70224e35c4599bc73e1d24b4b7ebf835", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We\u2019re happy to announce the recent integration of LlamaIndex with PostgresML \u2014 a comprehensive machine learning platform built on PostgreSQL. The PostgresML Managed Index allows LlamaIndex users to seamlessly manage document storage, splitting, embedding, and retrieval. By using PostgresML as the backend, users benefit from a streamlined and optimized process for Retrieval-Augmented Generation (RAG). This integration unifies embedding, vector search, and text generation into a single network call, resulting in faster, more reliable, and easier-to-manage RAG workflows.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   The problem with typical RAG workflows\n  </strong>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Typical Retrieval-Augmented Generation (RAG) workflows come with significant drawbacks, particularly for users.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Poor performance is a major issue, as these workflows involve multiple network calls to different services for embedding, vector storage, and text generation, leading to increased latency. Additionally, there are privacy concerns when sensitive data is sent to various LLM providers. These user-centric issues are compounded by other challenges:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Increased dev time to master new technologies\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Complicated maintenance and scalability issues due to multiple points of failure\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Costly vendors required for multiple services\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The diagram above illustrates the complexity, showing how each component interacts across different services \u2014 exacerbating these problems.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   Solution\n  </strong>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The PostgresML Managed Index offers a comprehensive solution to the challenges of typical RAG workflows.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  By managing document storage, splitting, embedding generation, and retrieval all within a single system, PostgresML significantly reduces dev time, scaling costs, and overall spend when you eliminate the need for multiple point solutions. Most importantly, it enhances the user experience by consolidating embedding, vector search, and text generation into a single network call \u2014 resulting in improved performance and reduced latency. Additionally, the use of open-source models ensures transparency and flexibility, while operating within the database addresses privacy concerns and provides users with a secure and efficient RAG workflow.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   About PostgresML\n  </strong>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  PostgresML [\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/postgresml/postgresml\" rel=\"noreferrer noopener\">\n   github\n  </a>\n  ||\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://postgresml.org/\" rel=\"noreferrer noopener\">\n   website\n  </a>\n  ||\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://postgresml.org/docs\" rel=\"noreferrer noopener\">\n   docs\n  </a>\n  ] allows users to take advantage of the fundamental relationship between data and models, by moving the models to your database rather than constantly moving data to the models. This in-database approach to AI architecture results in more scalable, reliable and efficient applications. On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-time outputs in one process, directly where your data resides.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Key highlights:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Model Serving - GPU accelerated inference engine for interactive applications, with no additional networking latency or reliability costs\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Model Store - Access to open-source models including state of the art LLMs from Hugging Face, and track changes in performance between versions\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Model Training - Train models with your application data using more than 50 algorithms for regression, classification or clustering tasks; fine tune pre-trained models like Llama and BERT to improve performance\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Feature Store - Scalable access to model inputs, including vector, text, categorical, and numeric data: vector database, text search, knowledge graph and application data all in one low-latency system\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Python and JavaScript SDKs - SDK clients can perform advanced ML/AI tasks in a single SQL request without having to transfer additional data, models, hardware or dependencies to your application\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Serverless deployments - Enjoy instant autoscaling, so your applications can handle peak loads without overprovisioning\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  PostgresML has a range of capabilities. In the following sections, we\u2019ll guide you through just one use case \u2013 RAG \u2013 and how to use the PostgresML Managed Index on LlamaIndex to build a better RAG app.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   How it works in LlamaIndex\n  </strong>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Let\u2019s look at a simple question-answering example using the PostgresML Managed Index. For this example, we will be using Paul Graham\u2019s essays.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   Step 1: Get Your Database Connection String\n  </strong>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  If you haven\u2019t already,\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://postgresml.org/signup\" rel=\"noreferrer noopener\">\n   create your PostgresML account\n  </a>\n  . You\u2019ll get $100 in free credits when you complete your profile.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Set the PGML_DATABASE_URL environment variable:\n </p>\n <pre><code><span class=\"hljs-built_in\">export</span> PGML_DATABASE_URL=<span class=\"hljs-string\">\"{YOUR_CONNCECTION_STRING}\"</span></code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Alternatively, you can pass the pgml_database_url argument when creating the index.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   Step 2: Create the PostgresML Managed Index\n  </strong>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  First install Llama_index and the PostgresML Managed Index component:\n </p>\n <pre><code>pip install llama_index llama-index-indices-managed-postgresml</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Then load in the data:\n </p>\n <pre><code><span class=\"hljs-built_in\">mkdir</span> data\ncurl -o data/paul_graham_essay.txt https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Finally create the PostgresML Managed Index:\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index.core.readers <span class=\"hljs-keyword\">import</span> SimpleDirectoryReader\n<span class=\"hljs-keyword\">from</span> llama_index.indices.managed.postgresml <span class=\"hljs-keyword\">import</span> PostgresMLIndex\n\n\ndocuments = SimpleDirectoryReader(<span class=\"hljs-string\">\"data\"</span>).load_data()\nindex = PostgresMLIndex.from_documents(\n    documents, collection_name=<span class=\"hljs-string\">\"llama-index-example\"</span>\n)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Note the collection_name is used to uniquely identify the index you are working with.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Here we are using the SimpleDirectoryReader to load in the documents and then we construct the PostgresMLIndex from those documents.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  This workflow does not require document preprocessing. Instead, the documents are sent directly to PostgresML where they are stored, split, and embedded per the pipeline specification. For more information on pipelines see:\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://postgresml.org/docs/api/client-sdk/pipelines\" rel=\"noreferrer noopener\">\n   https://postgresml.org/docs/api/client-sdk/pipelines\n  </a>\n  Custom Pipelines can be passed into the PostgresML index at creation, but by default documents are split using the recursive_character splitter and embedded with intfloat/e5-small-v2 .\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   Step 3: Querying\n  </strong>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Now that we have created our index we can use it for retrieval and querying:\n </p>\n <pre><code>retriever = index.as_retriever()\ndocs = retriever.retrieve(<span class=\"hljs-string\">\"Was the author puzzled by the IBM 1401?\"</span>)\n<span class=\"hljs-keyword\">for</span> doc <span class=\"hljs-keyword\">in</span> docs:\n    <span class=\"hljs-built_in\">print</span>(doc)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  PostgreML does embedding and retrieval in a single network call. Compare this query against other common LlamaIndex embedding and vector storage configurations and you will notice a significant speed up.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Using the PostgresML Index as a query_engine is just as easy:\n </p>\n <pre><code>response = index.as_query_engine().query(<span class=\"hljs-string\">\"Was the author puzzled by the IBM 1401?\"</span>)\n<span class=\"hljs-built_in\">print</span>(response)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Once again, notice how fast the response was! The PostgresML Managed Index is doing embedding, retrieval, and augmented generation in one network call. The speed up becomes even more apparent when streaming:\n </p>\n <pre><code>query_engine = index.as_query_engine(streaming=<span class=\"hljs-literal\">True</span>)\nresults = query_engine.query(<span class=\"hljs-string\">\"Was the author puzzled by the IBM 1401?\"</span>)\n<span class=\"hljs-keyword\">for</span> text <span class=\"hljs-keyword\">in</span> results.response_gen:\n    <span class=\"hljs-built_in\">print</span>(text, end=<span class=\"hljs-string\">\"\"</span>, flush=<span class=\"hljs-literal\">True</span>)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Note that by default the query_engine uses meta-llama/Meta-Llama-3-8B-Instruct but this is completely configurable.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   Key takeaways\n  </strong>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The PostgresML Managed Index uniquely unifies embedding, vector search, and text generation into a single network call. LlamaIndex users can expect faster, more reliable, and easier-to-manage RAG workflows by using PostgresML as the backend.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  To get started with PostgresML and LlamaIndex, you can follow the PostgresML intro\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://postgresml.org/docs/introduction/getting-started/\" rel=\"noreferrer noopener\">\n   guide\n  </a>\n  to setup your account, and the examples above with your own data.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 11994, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ba07468-3aef-4f59-b2e4-a4fc9643f4d0": {"__data__": {"id_": "8ba07468-3aef-4f59-b2e4-a4fc9643f4d0", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.html", "file_name": "special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.html", "file_type": "text/html", "file_size": 18153, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.html", "file_name": "special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.html", "file_type": "text/html", "file_size": 18153, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "dda22db7a59e83713cf318592ff12150d63fe022f082a82064a4a064cf84e7b4", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  We had an awesome time at the Berkeley Hackathon two weeks ago (6/17\u20136/18). The attendance stats were impressive:\n </p>\n <ul>\n  <li>\n   1200 hackers\n  </li>\n  <li>\n   262 submitted projects\n  </li>\n  <li>\n   2 real-life llamas \ud83e\udd99\n  </li>\n </ul>\n <p>\n  LlamaIndex sponsored a \u201cBest Knowledge-Intensive LLM App\u201d prize series at the hackathon. The criteria was an app that leveraged a knowledge base of custom data to build innovative new application experiences.\n </p>\n <p>\n  We announced three prize winners along with an honorable mention. We are excited to feature each project in a special highlight below. In each highlight, the creators describe the project mission and what it solves, the implementation+tech stack, challenges, and future directions. Check it out! \ud83d\udc47\n </p>\n <h1>\n  First Prize Winner: Helmet AI\n </h1>\n <p>\n  Creators: Jaiveer Singh, Devin Mui, Ethan Mehta, Manav Rathod\n </p>\n <p>\n  Devpost:\n  <a href=\"https://devpost.com/software/helmet-ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://devpost.com/software/helmet-ai\n  </a>\n </p>\n <h2>\n  Introduction\n </h2>\n <p>\n  In today\u2019s rapidly evolving business landscape, staying ahead of the competition is paramount for success. However, the deluge of information and the ever-changing market dynamics can make it challenging for business leaders to make informed decisions. In this blog post, we introduce Helmet AI, a cutting-edge market intelligence tool designed to empower leadership teams with real-time insights and a competitive edge. Join us as we explore the capabilities, technology stack, and future prospects of Helmet AI.\n </p>\n <h2>\n  Unveiling Helmet AI\n </h2>\n <p>\n  Helmet AI is an innovative market intelligence tool that harnesses the power of advanced technologies to provide leaders with actionable insights and an unparalleled understanding of the global business landscape. With its context-aware Ingestion Engine and Insight Extractor powered by OpenAI\u2019s GPT models, Helmet AI offers a comprehensive solution for tracking breaking news, uncovering hidden relationships, and extracting valuable, personalized insights from vast amounts of data. For ease of use, Helmet AI displays these insights in a familiar, Twitter-like \u201cFeed\u201d interface. Additionally, Helmet AI offers a Chat interface for users to ask questions about a particular news story to Helmet\u2019s knowledgeable chat agent.\n </p>\n <h2>\n  Key Features and Technology Stack\n </h2>\n <p>\n  <strong>\n   Context-Aware Ingestion Engine:\n  </strong>\n </p>\n <ul>\n  <li>\n   Helmet AI\u2019s Ingestion Engine continuously monitors the vast landscape of breaking news and global events. By leveraging techniques such as subscribing to RSS feeds for up to date news data and processing documents with LlamaIndex and LangChain, the engine builds a complete understanding of real-time events and their implications on various user profiles. Embeddings are stored in a Pinecone Vector Database.\n  </li>\n </ul>\n <p>\n  <strong>\n   Insight Extractor with OpenAI\u2019s GPT Models:\n  </strong>\n </p>\n <ul>\n  <li>\n   The Insight Extractor component of Helmet AI utilizes the power of OpenAI\u2019s GPT models to identify and concisely explain the intricate relationships between seemingly disparate topics surfaced in your feed. By transforming raw data into actionable insights with intelligent explanations, leaders can make informed decisions based on an understanding of market trends and complex dynamics.\n  </li>\n </ul>\n <p>\n  <strong>\n   Scalable Infrastructure:\n  </strong>\n </p>\n <ul>\n  <li>\n   Helmet AI is built on Azure\u2019s robust infrastructure, utilizing a range of services such as App Services, a PostgreSQL Database, and Github Actions for orchestrating Deployments. The implementation also incorporates GraphQL for efficient data retrieval and processing.\n  </li>\n </ul>\n <h2>\n  Challenges Overcome and Accomplishments\n </h2>\n <p>\n  During the development of Helmet AI, our team encountered various challenges, including integrating MindsDB with Azure and overcoming limitations with Gmail authentication. However, we were able to overcome these obstacles and successfully implemented Helmet AI in just 36 hours during the Berkeley AI Hackathon. Additionally, we established a seamless deployment process using GitHub Actions, automating manual service orchestration. The experience was particularly rewarding for the first-time hackers on the team.\n </p>\n <h2>\n  Key Learnings\n </h2>\n <p>\n  Throughout the development process, our team gained valuable insights. We discovered the importance of setting up deployment flows early on to reduce stress during crunch time. Embracing best practices in software engineering proved crucial. Furthermore, we realized the potential of leveraging advanced language models as implicit knowledge graphs, expanding their applications beyond traditional embeddings.\n </p>\n <h2>\n  Future Prospects\n </h2>\n <p>\n  Looking ahead, Helmet AI aims to scale up the Ingestion Engine to handle the entirety of the web, leveraging technologies like AnyScale. The team plans to collaborate with enterprise business development teams to initiate pilot programs and gather feedback for further refinement. With a solid foundation in place, Helmet AI hopes to have an impact on the way leaders gather insights and make strategic decisions.\n </p>\n <h2>\n  Conclusion\n </h2>\n <p>\n  Helmet AI represents a solid attempt at a game-changing solution for business leaders seeking to stay ahead in today\u2019s fast-paced business world. By leveraging cutting-edge technologies, including AI-powered insight extraction and explanation and real-time data analysis, Helmet AI empowers leaders to confidently navigate market challenges and seize emerging opportunities. As the tool continues to evolve and expand its capabilities, the future of market intelligence looks promising. Stay tuned for more updates on Helmet AI\u2019s journey towards transforming the way we approach gathering information and strategic decision-making.\n </p>\n <h1>\n  Winner: Split\n </h1>\n <p>\n  Creators: Aditya Ariyur, Nikhil Patel, Ronit Nagarapu\n </p>\n <p>\n  Devpost:\n  <a href=\"https://devpost.com/software/split-pv4hn7\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://devpost.com/software/split-pv4hn7\n  </a>\n </p>\n <h2>\n  Background/Motivation\n </h2>\n <p>\n  We wanted to develop an easy-to-use workflow that allowed users to generate personalized emails with the assistance of AI, while retaining the user\u2019s unique writing style and emotion inflections.\n </p>\n <h2>\n  What It Is\n </h2>\n <p>\n  Our product learns from your previous emails and trains a custom LLM that will draft emails that sound like you, not like a robot. It learns from your writing style and how you respond to specific people. Then, it generates emails from user prompts that match that style.\n </p>\n <h2>\n  How We Built It\n </h2>\n <p>\n  We used the Google API and LlamaIndex to parse through a user\u2019s old emails and develop an LLM model built on OpenAI\u2019s text-davinci-003. Then, we use Hume to understand the user\u2019s tone and emotion in their emails, and associate it with specific subjects and recipients so future\nemails can be fine-tuned to fit the user\u2019s emailing habits. The current interface was developed using React.js for the website and a Flask API to interact with the backend LLM model.\n </p>\n <h2>\n  Challenges + What We Learned\n </h2>\n <p>\n  It was quite difficult to get all of the different aspects of our model working together in unison, especially establishing the connection between the parsed emails and Hume emotion tags to the LlamaIndex model. We had to experiment with many different tools and prompt styles to get an accurate email generation. However, with a lot of dedication and troubleshooting, we were able to develop a working model to demonstrate our concept and its potential functionality. We learned how rewarding it was to train our own LLM using LlamaIndex. Base LLMs like ChatGPT are already so powerful, so the functionality of training a custom LLM based on your\nown data unlocks endless possibilities.\n </p>\n <h2>\n  What\u2019s Next\n </h2>\n <p>\n  We hope to completely integrate the code and workflow into a Google plugin or extension so users can easily implement it into their daily emailing. We want to ensure the privacy and security of the user\u2019s data, so we want to experiment with methods to reduce how much data is\nsent to third-party services like OpenAI. We also want to dedicate further development to the emotion training, as this could boost the effectiveness of our product and add to our main value proposition of personalized, user-specific email generation.\n </p>\n <h1>\n  Winner: Prosper AI\n </h1>\n <p>\n  Creators: Alan Yang, Ashay Changwani, Punit Sai Arani, Vedant Tapadia\n </p>\n <p>\n  Devpost:\n  <a href=\"https://devpost.com/software/prosper-ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://devpost.com/software/prosper-ai\n  </a>\n </p>\n <p>\n  <a href=\"https://prosperai.vercel.app/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Vercel Demo\n  </a>\n  /\n  <a href=\"https://www.youtube.com/watch?v=_-v0BhFPjAQ\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   YouTube Video\n  </a>\n </p>\n <h2>\n  Overview\n </h2>\n <p>\n  Prosper AI is a trailblazer in utilizing Artificial Intelligence to unlock your full financial potential. It serves as an accessible and smart virtual financial advisor, armed with precise insights and personalized advice. Our mission is to democratize financial expertise. By bridging the resource gap, Prosper AI aims to level the playing field for all.\n </p>\n <h2>\n  The Genesis of Prosper AI\n </h2>\n <p>\n  The spark that ignited Prosper AI was a simple observation of the wealth disparity among different social classes. The rich have always had access to knowledge and resources that help in growing and safeguarding their wealth. In contrast, those from modest backgrounds often lack the necessary knowledge and tools to utilize what they earn effectively. Many resort to social media for financial advice, which is often generic and occasionally unreliable as it comes from unqualified influencers. Hiring a financial advisor, on the other hand, could be exorbitant and impractical for those with a limited budget.\n </p>\n <p>\n  This is where Prosper AI steps in. We embraced the challenge to develop an innovative solution utilizing state-of-the-art technology and models to help digest and simplify complex financial data.\n </p>\n <h2>\n  The Prosperity Engine: How Prosper AI Works\n </h2>\n <p>\n  Prosper AI sources your financial data from any number and type of bank or investment account to provide qualified financial advice that adheres to regulatory policies, to give you personalized tips, advice and explanations.\n </p>\n <p>\n  Prosper AI achieves this by leveraging an open finance provider such as Plaid to source users financial information. Then Prosper AI will ask a series of financial goal questions to help contextualize the ideal outcomes for the user. Using this combination of personal financial data and goals, Prosper AI will provide a set of optimal and personalized recommendations on how to achieve these goals.\n </p>\n <p>\n  The beauty of Prosper AI lies in its interactivity and support. Users have the liberty to pose questions at any juncture if they find something perplexing. This is particularly invaluable for demystifying complicated charts or financial jargon. Furthermore, Prosper AI goes beyond just answering questions about the current recommendations. It\u2019s like having an expert financial advisor at your beck and call, ready to generate insights, charts, and suggestions for any aspect of your financial landscape. Whether it\u2019s planning for retirement, optimizing investments, or understanding tax liabilities, Prosper AI stands ready to guide users with precision and personalized insights to cultivate financial acumen and empower smarter financial decision-making.\n </p>\n <h2>\n  The Building Blocks of Prosper AI: A Look into Our Tech Stack\n </h2>\n <p>\n  <strong>\n   Backend:\n  </strong>\n  Our backend, the engine that powers Prosper AI, is written in Python and based on a FastAPI server. We chose Python because of its agility and the vast availability of open-source libraries that expedite the development process. Additionally, Python\u2019s native packages provided by OpenAI and Plaid seamlessly integrate with our backend, ensuring both development and runtime efficiency.\n </p>\n <p>\n  One of the cornerstones of Prosper AI\u2019s backend is a powerful prompting pipeline which simulates a fine-tuned model. To achieve this, we tap into the capabilities of OpenAI\u2019s GPT-4, enhanced with function calling, and interlink it with Pinecone\u2019s vector database using additional tools like LlamaIndex. This fusion forges a streamlined yet powerful interface.\n </p>\n <p>\n  <strong>\n   Frontend:\n  </strong>\n  When we started out, especially during the hackathon phase, we developed the web application frontend using Next.js, which was our comfort zone. However, as we progressed and aimed for higher benchmarks, we recognized the need to migrate to a more performant framework. We decided on SvelteKit, which stands out for its simplicity and performance, significantly accelerating the development process.\n </p>\n <p>\n  One of our key objectives is to make Prosper AI accessible and user-friendly. We crafted a minimalist user interface, which declutters the screen while maintaining the essence of information. Moreover, we supplemented this with visualizations, which are crucial in translating complex financial data into understandable and actionable insights for the user. Through this combination of a robust backend and an intuitive frontend, Prosper AI is poised to revolutionize personal financial management.\n </p>\n <h2>\n  Overcoming Challenges: The Journey of Prosper AI\u2019s Development\n </h2>\n <p>\n  The primary challenge we encountered during the initial stages was the creation of a pipeline to ingest and process years of financial data analytically. The sheer volume of data was not just overwhelming to handle all at once, but it was also crucial to process it responsibly and meaningfully.\n </p>\n <p>\n  To tackle this, we had to design a system that dissected the vast financial data into digestible segments, structuring it in an orderly manner that enabled logical understanding and actionable insights. Although crafting such a system under time pressure was strenuous, it offered us a valuable insight into the magnitude of data we were dealing with. It further emphasized the significance of our mission: to efficiently and comprehensively process such vast data for the benefit of our users.\n </p>\n <p>\n  Another demanding task was incorporating the complexities of tax code into our platform. Thousands of pages of tax regulations had to be converted into intelligent code, capable of offering savvy financial suggestions. Despite the enormous effort this task required, it was crucial in creating a comprehensive wealth management system. The result is a platform that delivers an optimized, personalized financial plan tailored to each user\u2019s specific goals and needs, as well as future plans. Our platform not only identifies the type of accounts and the cash flow strategies that would minimize tax liabilities but also charts a roadmap for maximizing net worth growth over the next 30 years.\n </p>\n <p>\n  This is the essence of Prosper AI \u2014 using technology to simplify complex financial management and facilitate the path towards prosperity.\n </p>\n <h2>\n  The Road Ahead for Prosper AI\n </h2>\n <p>\n  As we set our sights on the future, the Prosper AI team is more determined than ever to make strides in revolutionizing personal wealth management. Our immediate focus is to transition into full-time startup mode, which entails delving deeper into the development of feature functionalities and solidifying the foundation of our platform.\n </p>\n <p>\n  A key milestone on our roadmap is engaging in pilot use cases with our initial group of customers who have eagerly joined our waitlist. This phase is critical, as it allows us to validate the effectiveness and impact of Prosper AI in real-world scenarios. Through feedback and insights gathered from this initial group, we\u2019ll be able to refine and enhance the platform to ensure it not only meets but surpasses the expectations of our users.\n </p>\n <p>\n  But we won\u2019t stop there. The learnings from the pilot phase will serve as the springboard for subsequent developments and innovations. As we continue to harness cutting-edge technology and data analytics, Prosper AI aims to democratize access to financial knowledge and tools that can empower individuals to unlock their financial potential.\n </p>\n <p>\n  Stay tuned as Prosper AI embarks on this exciting journey towards transforming the landscape of personal finance, making it more accessible, intelligent, and personalized for all.\n </p>\n <p>\n  Together with Prosper AI, let\u2019s cultivate the seeds of financial growth and harvest the fruits of prosperity.\n </p>\n <h2>\n  Video/screenshots/links to material.\n </h2>\n <p>\n  Learn more and join our waitlist for a chance to win a $50 Amazon voucher:\n </p>\n <div>\n  <a href=\"https://prosperai.vercel.app/?source=post_page-----c135681bb6f0--------------------------------\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <div class=\"ra ab jb\">\n    <div class=\"rb ab cn ca rc rd\">\n     <h2 class=\"be gu is z jj re jl jm rf jo jq gs bj\">\n      Unleash The Power of Comparison\n     </h2>\n     <div class=\"rg l\">\n      <h3 class=\"be b is z jj re jl jm rf jo jq dt\">\n       Tap into Prosper AI, your intelligent sidekick for personalized &amp; optimal financial advice.\n      </h3>\n     </div>\n     <div class=\"rh l\">\n      <p class=\"be b du z jj re jl jm rf jo jq dt\">\n       prosperai.vercel.app\n      </p>\n     </div>\n    </div>\n   </div>\n  </a>\n </div>\n <p>\n  <a href=\"https://www.youtube.com/watch?v=_-v0BhFPjAQ\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   https://www.youtube.com/watch?v=_-v0BhFPjAQ\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 18080, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "07e7f65a-092e-4a2f-8f4a-db3039b3e8de": {"__data__": {"id_": "07e7f65a-092e-4a2f-8f4a-db3039b3e8de", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.html", "file_name": "streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.html", "file_type": "text/html", "file_size": 17345, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.html", "file_name": "streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.html", "file_type": "text/html", "file_size": 17345, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "7d5d24270f903f1af56392ded4464e9bf761653864cd8fe747d8cad1386c9638", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <em>\n   This is a guest post from Team CLAB, the winners of \"Best Use of LlamaIndex\" at our recent hackathon with MongoDB.\n  </em>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Imagine this: you\u2019re deep in a coding project, and a critical question pops up about a specific tool or library. You start the dreaded documentation shuffle\u200a\u2014\u200asearching through wikis, FAQs, maybe even firing up a separate chatbot for specific tools (like those from LlamaIndex, FireworksAI or anyone else). It\u2019s frustrating! \ud83e\udd2f We wanted to change that.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  That\u2019s why Team CLAB built\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://clab-ui.vercel.app/\" rel=\"noreferrer noopener\">\n   LlamaWorksDB (try it out!)\n  </a>\n  , your friendly AI-powered doc wizard \u2728. No more scattered searches! It taps into the knowledge of multiple sponsors of our hackathon including LlamaIndex, Fireworks.ai, and MongoDB, all through a single chatbot interface. Need something explained from MongoDB\u2019s docs? Got it! Want a code example from Fireworks.ai? Easy!\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  The foundation: LlamaIndex and data ingestion\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  LlamaIndex was the heart and soul of LlamaWorksDB. It\u2019s like a super versatile toolbox for handling all kinds of documentation! We primarily used their open-source readers to grab info straight from websites. A cool hack we did was customize the\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   SimpleWebPageReader\n  </code>\n  . We taught it to ignore website navigation bars, saving us a ton of precious tokens. \ud83d\udcaa While this worked great for the documentation sites, we used LlamaIndex\u2019s\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   GithubRepositoryReader\n  </code>\n  to easily read through each repo.\n </p>\n <pre><code><span class=\"hljs-keyword\">from</span> llama_index.readers.web <span class=\"hljs-keyword\">import</span> SimpleWebPageReader\n<span class=\"hljs-keyword\">import</span> re\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">LlamaDocsPageReader</span>(<span class=\"hljs-title class_ inherited__\">SimpleWebPageReader</span>):\n   <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">load_data</span>(<span class=\"hljs-params\">self, urls</span>):\n       documents = <span class=\"hljs-built_in\">super</span>().load_data(urls)\n       processed_documents = []\n       <span class=\"hljs-keyword\">for</span> doc <span class=\"hljs-keyword\">in</span> documents:\n           processed_doc = self.process_document(doc)\n           processed_documents.append(processed_doc)\n       <span class=\"hljs-keyword\">return</span> processed_documents\n\n   <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">process_document</span>(<span class=\"hljs-params\">self, document</span>):\n       <span class=\"hljs-comment\"># Split the document text by \"Table of Contents\"</span>\n       pattern = <span class=\"hljs-string\">r'(?i)\\n\\n*table\\s*of\\s*contents\\n\\n*'</span>\n       parts = re.split(pattern, document.text, maxsplit=<span class=\"hljs-number\">1</span>)\n       <span class=\"hljs-comment\"># If there is a part after \"Table of Contents\", use it as the document text</span>\n       <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(parts) &gt; <span class=\"hljs-number\">1</span>:\n           document.text = <span class=\"hljs-string\">\"Table of contents\"</span>.join(parts[<span class=\"hljs-number\">1</span>:])\n       <span class=\"hljs-keyword\">return</span> document</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Choosing how to split up the docs was interesting. LlamaIndex has options ranging from the basic\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   SentenceSplitter\n  </code>\n  to their\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   SemanticNodeParser\n  </code>\n  , which uses AI to group similar ideas. We went with the latter for those perfectly sized, meaningful chunks.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Finally, we embedded each \u2018node\u2019 and sent each as a document to MongoDB. Talk about streamlined! MongoDB stored the text, metadata,\n  <em>\n   and\n  </em>\n  our embeddings\u200a\u2014\u200aideal for the kind of search we wanted to build. We used Nomic\u2019s flexible embedding model via Fireworks, which let us fine-tune the dimensions for maximum efficiency.\n </p>\n <pre><code><span class=\"hljs-comment\"># FireworksEmbedding defaults to using model</span>\nembed_model = FireworksEmbedding(api_key=os.getenv(<span class=\"hljs-string\">'FIREWORKS_API_KEY'</span>),\n                                model=<span class=\"hljs-string\">\"nomic-ai/nomic-embed-text-v1.5\"</span>,\n                                embed_batch_size=<span class=\"hljs-number\">10</span>,\n                                dimensions=<span class=\"hljs-number\">768</span> <span class=\"hljs-comment\"># can range from 64 to 768</span>\n                                )\n\n<span class=\"hljs-comment\"># the tried and true sentence splitter</span>\ntext_splitter = SentenceSplitter(chunk_size=<span class=\"hljs-number\">1000</span>, chunk_overlap=<span class=\"hljs-number\">200</span>)\n<span class=\"hljs-comment\"># the semantic splitter uses our embedding model to group semantically related sentences together</span>\nsemantic_parser = SemanticSplitterNodeParser(embed_model=embed_model)\n\n<span class=\"hljs-comment\"># we set up MongoDB as our document and vector database</span>\nvector_store = MongoDBAtlasVectorSearch(\n   pymongo.MongoClient(os.getenv(<span class=\"hljs-string\">'MONGO_URI'</span>)),\n   db_name=<span class=\"hljs-string\">\"fireParse\"</span>,\n   collection_name=<span class=\"hljs-string\">\"llamaIndexDocs\"</span>,\n   index_name=<span class=\"hljs-string\">\"llama_docs_index\"</span>\n)\n\n<span class=\"hljs-comment\">#finally we use LlamaIndex's pipeline to string this all together</span>\npipeline = IngestionPipeline(\n   transformations=[\n       semantic_parser, <span class=\"hljs-comment\">#can replace with text_splitter</span>\n       embed_model,\n   ],\n   vector_store=vector_store,\n)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Once we have everything set up, we can create documents from URLs in MongoDB! Below is an example of using three URLs but we used hundreds.\n </p>\n <pre><code>example_urls = [\n   <span class=\"hljs-string\">\"https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook\"</span>,\n   <span class=\"hljs-string\">\"https://docs.llamaindex.ai/en/stable/examples/cookbooks/anthropic_haiku/\"</span>,\n  <span class=\"hljs-string\">\"https://docs.llamaindex.ai/en/stable/examples/vector_stores/MongoDBAtlasVectorSearch/\"</span>\n]\n\n<span class=\"hljs-comment\"># read in the documents and pass them through our pipeline</span>\ndocuments = LlamaDocsPageReader(html_to_text=<span class=\"hljs-literal\">True</span>).load_data(example_urls)\npipeline.run(documents=documents, show_progress=<span class=\"hljs-literal\">True</span>)</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  You can see in MongoDB how our documents have text, embedding (with 768 dimensions), and metadata.\n </p>\n <figure>\n  <figcaption>\n   Example document in MongoDB Atlas that resulted from the pipeline\n  </figcaption>\n </figure>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  MongoDB Atlas for vector search\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  MongoDB Atlas was our go-to for storing both the documentation text and the embeddings themselves. It\u2019s incredibly versatile! Setting up vector search within Atlas is a breeze, allowing us to quickly find the most relevant document chunks. Plus, LlamaIndex\u2019s metadata parsing played perfectly with Atlas\u200a\u2014\u200awe could easily filter results based on things like document source or topic.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   Setting up Vector Search:\n  </strong>\n  It was remarkably simple! We just specified these few things:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Path to the embedding field within our documents.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Embedding dimension size.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Similarity metric (e.g., cosine similarity).\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   That it\u2019s a vector index.\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   Filtering Power (Optional):\n  </strong>\n  For even finer control, we could add paths to fields we wanted to filter our searches by (like the company\u2019s name).\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Whether you\u2019re building a complex web app or a quick Streamlit prototype, LlamaIndex ChatEngines have you covered. They effortlessly manage conversation history, let you perform lightning-fast vector searches, and unlock a whole suite of powerful tools.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We built our ChatEngine directly from our trusty MongoDB index. This integration was surprisingly simple:\n </p>\n <pre><code><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">get_index</span>():\n   logger.info(<span class=\"hljs-string\">\"Connecting to index from MongoDB...\"</span>)\n   store = MongoDBAtlasVectorSearch(\n       db_name=os.environ[<span class=\"hljs-string\">\"MONGODB_DATABASE\"</span>],\n       collection_name=os.environ[<span class=\"hljs-string\">\"MONGODB_VECTORS\"</span>],\n       index_name=os.environ[<span class=\"hljs-string\">\"MONGODB_VECTOR_INDEX\"</span>],\n   )\n   index = VectorStoreIndex.from_vector_store(store)\n   logger.info(<span class=\"hljs-string\">\"Finished connecting to index from MongoDB.\"</span>)\n   <span class=\"hljs-keyword\">return</span> index\n\nindex = get_index()\nindex.as_chat_engine(\n    llm = Fireworks(\n             api_key=env_vars[<span class=\"hljs-string\">'FIREWORKS_API_KEY'</span>],\n             model=<span class=\"hljs-string\">\"accounts/fireworks/models/mixtral-8x22b-instruct\"</span> <span class=\"hljs-comment\">#Can be changed out for Llama3</span>\n             )\n    chat_mode=<span class=\"hljs-string\">\"best\"</span>, \n    context_prompt=(\n           <span class=\"hljs-string\">\"\"\" You are a software developer bot that is an expert at reading over documentation to answer questions.\n           Use the relevant documents for context:\n           {context_str}\n           \\nInstruction: Use the previous chat history, or the context above, to interact and help the user.\n           \"\"\"</span>\n           ),\n    verbose=<span class=\"hljs-literal\">True</span>\n    )</code></pre>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <code class=\"SanityPortableText_inlineCode__cI85z\">\n   create-llama\n  </code>\n  : from idea to app in record time\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We were seriously impressed by Create-Llama. Usually, building a full-stack app takes time, but Create-Llama had us up and running in under 15 minutes! All we did was point it towards our vector database and give a few basic details. Honestly, it made development a joy!\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191\" rel=\"noreferrer noopener\">\n   This blog post goes into more detail about how to use create-llama.\n  </a>\n </p>\n <figure>\n  <figcaption>\n   The create-llama setup screen\n  </figcaption>\n </figure>\n <figure>\n  <figcaption>\n   The create-llama app, customized and ready to go\n  </figcaption>\n </figure>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Deployment: Render and Vercel\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  To make LlamaWorksDB production-ready and easily accessible, we turned to\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://render.com/\" rel=\"noreferrer noopener\">\n   Render\n  </a>\n  and\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://vercel.com/\" rel=\"noreferrer noopener\">\n   Vercel\n  </a>\n  . Render was a perfect fit for our Python FastAPI backend, as it focuses on ease of deployment and scalability. Vercel seamlessly handled our Next.js frontend\u200a\u2014\u200awe loved its developer-centric approach and the effortless build process. Both platforms made deployment a breeze, letting us focus on coding rather than complex infrastructure setup.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Future directions\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Our hackathon success is just the beginning. We envision LlamaWorksDB evolving into a powerhouse for developers seeking answers within technical documentation. Here\u2019s how we see it growing:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Enhanced Retrieval:\n   </strong>\n   We\u2019re excited to experiment with LlamaIndex\u2019s powerful capabilities like MultiVectorSearch to further refine our results. Integrating different LLMs will open up new possibilities for how LlamaWorksDB understands and interacts with technical content.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    A Focus on Documentation:\n   </strong>\n   We want to double down on making LlamaWorksDB the ultimate tool for navigating documentation. This means exploring specialized techniques and tools designed specifically for understanding complex technical information.\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  LlamaWorksDB is an open-source project in Beta, and we believe in the power of collaboration! If you\u2019re passionate about AI-powered documentation tools, we invite you to:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Try it out:\n   </strong>\n   Explore our\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/clab2024/clab/tree/main\" rel=\"noreferrer noopener\">\n    GitHub repo\n   </a>\n   and give\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://clab-ui.vercel.app/\" rel=\"noreferrer noopener\">\n    LlamaWorksDB a spin\n   </a>\n   !\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Contribute:\n   </strong>\n   Help us build new features, test integrations, and refine our search capabilities.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Share your feedback:\n   </strong>\n   Let us know how we can make LlamaWorksDB even better.\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Together, let\u2019s revolutionize the way developers interact with documentation!\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  \ud83d\udd17 Explore our project and join the innovation:\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/clab2024/clab/\" rel=\"noreferrer noopener\">\n   https://github.com/clab2024/clab/\n  </a>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://clab-ui.vercel.app/\" rel=\"noreferrer noopener\">\n   https://clab-ui.vercel.app/\n  </a>\n  (front-end) (leverages free credits responds late)\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://clab.onrender.com/docs\" rel=\"noreferrer noopener\">\n   https://clab.onrender.com/docs\n  </a>\n  (back-end)\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Meet Team CLAB! \ud83c\udf89\n </h2>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.linkedin.com/in/chris-wood-b9282a22a/\" rel=\"noreferrer noopener\">\n    <strong>\n     Chris Wood\n    </strong>\n   </a>\n   : Up-and-coming tech whiz, ready to graduate with valuable insights from his internship at Tutello.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.linkedin.com/in/leowalker/\" rel=\"noreferrer noopener\">\n    <strong>\n     Leo Walker\n    </strong>\n   </a>\n   : Data Scientist with the discipline and precision of a Military Veteran.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.linkedin.com/in/andrew-g-townsend/\" rel=\"noreferrer noopener\">\n    <strong>\n     Andrew Townsend\n    </strong>\n   </a>\n   : A Computer Science graduate from SJSU, bringing fresh academic perspectives.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.linkedin.com/in/barathsubramaniam/\" rel=\"noreferrer noopener\">\n    <strong>\n     Barath Subramaniam\n    </strong>\n   </a>\n   : The strategic mind behind Product Security AI and Data engineering at Adobe. Twitter:\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/baraths84\" rel=\"noreferrer noopener\">\n    @baraths84\n   </a>\n  </li>\n </ul>\n <figure>\n  <figcaption>\n   Team CLAB (plus Laurie)\n  </figcaption>\n </figure>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 17268, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79bd7357-de68-4075-b944-67130bc2e1c3": {"__data__": {"id_": "79bd7357-de68-4075-b944-67130bc2e1c3", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.html", "file_name": "supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.html", "file_type": "text/html", "file_size": 16870, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.html", "file_name": "supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.html", "file_type": "text/html", "file_size": 16870, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "beb66d0ebec5818e2eaafd47edcd70963f00c48795962a04da2126e0feba1bbf", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <em>\n   This is a guest post from Uptrain.\n  </em>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We are excited to announce the recent integration of LlamaIndex with UpTrain - an open-source LLM evaluation framework to evaluate your RAG pipelines and experiment with different configurations. As an increasing number of companies are graduating their LLM prototypes to production-ready systems, robust evaluations provide a systematic framework to make decisions rather than going with the \u2018vibes\u2019. By combining LlamaIndex's flexibility and UpTrain's evaluation framework, developers can experiment with different configurations, fine-tuning their LLM-based applications for optimal performance.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   About UpTrain\n  </strong>\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   UpTrain\n  </strong>\n  [\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/uptrain-ai/uptrain\" rel=\"noreferrer noopener\">\n   github\n  </a>\n  ||\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://uptrain.ai/\" rel=\"noreferrer noopener\">\n   website\n  </a>\n  ||\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.uptrain.ai/getting-started/introduction\" rel=\"noreferrer noopener\">\n   docs\n  </a>\n  ] is an open-source platform to evaluate and improve LLM applications. It provides grades for 20+ preconfigured checks (covering language, code, embedding use cases), performs root cause analyses on instances of failure cases and provides guidance for resolving them.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   Key Highlights:\n  </strong>\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Data Security:\n   </strong>\n   As an open-source solution, UpTrain conducts all evaluations and analyses locally, ensuring that your data remains within your secure environment (except for the LLM calls).\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Custom Evaluator LLMs:\n   </strong>\n   UpTrain allows for\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/uptrain-ai/uptrain/blob/main/examples/open_source_evaluator_tutorial.ipynb\" rel=\"noreferrer noopener\">\n    customisation of your evaluator LLM\n   </a>\n   , offering options among several endpoints, including OpenAI, Anthropic, Llama, Mistral, or Azure.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Insights that help with model improvement:\n   </strong>\n   Beyond mere evaluation, UpTrain performs\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/uptrain-ai/uptrain/blob/main/examples/root_cause_analysis/rag_with_citation.ipynb\" rel=\"noreferrer noopener\">\n    root cause analysis\n   </a>\n   to pinpoint the specific components of your LLM pipeline, that are underperforming, as well as identifying common patterns among failure cases, thereby helping in their resolution.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Diverse Experimentations:\n   </strong>\n   The platform enables\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/uptrain-ai/uptrain/tree/main/examples/experiments\" rel=\"noreferrer noopener\">\n    experimentation\n   </a>\n   with different prompts, LLM models, RAG modules, embedding models, etc. and helps you find the best fit for your specific use case.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Compare open-source LLMs:\n   </strong>\n   With UpTrain, you can compare your fine-tuned open-source LLMs against proprietary ones (such as GPT-4), helping you to find the most cost-effective model without compromising quality.\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In the following sections, we will illustrate how you can use UpTrain to evaluate your LlamaIndex pipeline. The evaluations demonstrated here will help you quickly find what\u2019s affecting the quality of your responses, allowing you to take appropriate corrective actions.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   LlamaIndex x UpTrain Callback Handler\n  </strong>\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We introduce an UpTrain Callback Handler which makes evaluating your existing LlamaIndex Pipeline seamless. By adding just a few lines of code, UpTrain will automatically perform a series of checks - evaluating the quality of generated responses, the quality of contextual data retrieved by the RAG pipeline as well as the performance of all the interim steps.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  If you wish to skip right ahead to the tutorial, check it out\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/callbacks/UpTrainCallback.ipynb\" rel=\"noreferrer noopener\">\n   here.\n  </a>\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   Evals across the board: From Vanilla to Advanced RAG\n  </strong>\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Vanilla RAG involves a few steps. You need to embed the documents and store them in a vector database. When the user asks questions, the framework embeds them and uses similarity search to find the most relevant documents. The content of these retrieved documents, and the original query, are then passed on to the LLM to generate the final response.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  While the above is a great starting point, there have been a lot of improvements to achieve better results. Advanced RAG applications have many additional steps that improve the quality of the retrieved documents, which in turn improve the quality of your responses.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  But as Uncle Ben famously said to Peter Parker in the GenAI universe:\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <em>\n   \u201cWith increased complexity comes more points of failure.\u201d.\n  </em>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Most of the LLM evaluation tools only evaluate the final context-response pair and fail to take into consideration the intermediary steps of an advanced RAG pipeline. Let\u2019s look at all the evaluations provided by UpTrain.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   Addressing Points of Failure in RAG Pipelines\n  </strong>\n </h2>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  <strong>\n   1. RAG Query Engine Evaluation\n  </strong>\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Let's first take a Vanilla RAG Pipeline and see how you can test its performance. UpTrain provides three operators curated for testing both the retrieved context as well as the LLM's response.\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-relevance\" rel=\"noreferrer noopener\">\n    <strong>\n     Context Relevance\n    </strong>\n   </a>\n   : However informative the documents retrieved might be, if they are not relevant to your query, you will likely not get a response that answers your query. The Context Relevance operator determines if the documents fetched from the vector store contain information that can be used to answer your query.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.uptrain.ai/predefined-evaluations/context-awareness/factual-accuracy\" rel=\"noreferrer noopener\">\n    <strong>\n     Factual Accuracy\n    </strong>\n   </a>\n   : Now that we have checked if the context contains information to answer our query, we will check if the response provided by the LLM is backed by the information present in the context. The Factual Accuracy operator assesses if the LLM is hallucinating or providing information that is not present in the context.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.uptrain.ai/predefined-evaluations/response-quality/response-completeness\" rel=\"noreferrer noopener\">\n    <strong>\n     Response Completeness\n    </strong>\n   </a>\n   : Not all queries are straightforward. Some of them have multiple parts to them. A good response should be able to answer all the aspects of the query. The Response Completeness operator checks if the response contains all the information requested by the query.\n  </li>\n </ul>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  <strong>\n   2. Sub-Question Query Engine Evaluation\n  </strong>\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Let's say you tried out a Vanilla RAG pipeline and got consistently low Response Completeness scores. This means that the LLM is not answering all aspects of your query. One of the ways to solve this is by splitting the query into multiple smaller sub-queries that the LLM can answer more easily. To do this, you can use the SubQuestionQueryGeneration operator provided by LlamaIndex. This operator decomposes a question into sub-questions, generating responses for each using an RAG query engine.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  If you include this SubQuery module in your RAG pipeline, it introduces another point of failure, e.g. what if the sub-questions that we split our original question aren't good representations of it? UpTrain automatically adds new evaluations to check how well the module performs:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.uptrain.ai/predefined-evaluations/sub-query/sub-query-completeness\" rel=\"noreferrer noopener\">\n    <strong>\n     Sub Query Completeness\n    </strong>\n   </a>\n   : It evaluates whether the sub-questions accurately and comprehensively cover the original query.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Context Relevance, Factual Accuracy and Response Completeness for each of the sub-queries.\n  </li>\n </ul>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  <strong>\n   3. Reranking Evaluations\n  </strong>\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We looked at a way of dealing with low Response Completeness scores. Now, let's look at a way of dealing with low Context Relevance scores.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  RAG pipelines retrieve documents based on semantic similarity. These documents are ordered based on how similar they are to the query asked. However, recent research [\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://arxiv.org/pdf/2307.03172.pdf\" rel=\"noreferrer noopener\">\n   Lost in the Middle: How Language Model Uses Long Contexts\n  </a>\n  ] has shown that the LLMs are sensitive to the placement of the most critical information within the retrieved context. To solve this, you might want to add a reranking block.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Reranking involves using a semantic search model (specially tuned for the reranking task) that breaks down the retrieved context into smaller chunks, finds the semantic similarity between them and the query and rewrites the context by ranking them in order of their similarity.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We observed that when using the reranking operators in LlamaIndex, two scenarios can occur. These scenarios differ based on the number of nodes before and after the reranking process:\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   a. Same Number of Nodes Before and After Reranking:\n  </strong>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  If the number of nodes after the reranking remains the same, then we need to check if the new order is such that nodes higher in rank are more relevant to the query as compared to the older order. To check for this, UpTrain provides a Context Reranking operator.\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-reranking\" rel=\"noreferrer noopener\">\n    <strong>\n     Context Reranking\n    </strong>\n   </a>\n   : Checks if the order of reranked nodes is more relevant to the query than the original order.\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   b. Fewer Number of Nodes After Reranking:\n  </strong>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Reducing the number of nodes can help the LLM give better responses. This is because the LLMs process smaller context lengths better. However, we need to make sure that we don't lose information that would have been useful in answering the question. Therefore, during the process of reranking, if the number of nodes in the output is reduced, we provide a Context Conciseness operator.\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-conciseness\" rel=\"noreferrer noopener\">\n    <strong>\n     Context Conciseness\n    </strong>\n   </a>\n   : Examines whether the reduced number of nodes still provides all the required information.\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   Key Takeaways: Enhancing RAG Pipelines Through Advanced Techniques and Evaluation\n  </strong>\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Let's do a quick recap here. We started off with a Vanilla RAG pipeline and evaluated the quality of the generated response and retrieved context. Then, we moved to advanced RAG concepts like the SubQuery technique (used to combat cases with low Response Completeness scores) and the Reranking technique (used to improve the quality of retrieved context) and looked at advanced evaluations to quantify their performance.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  This essentially provides a framework to systematically test the performance of different modules as well as evaluate if they actually lead to better quality responses by making data-driven decisions.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Much of the success in the field of Artificial intelligence can be attributed to experimentation with different architectures, hyperparameters, datasets, etc., and our integration with UpTrain allows you to import those best practices while building RAG pipelines. Get started with uptrain with this\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.uptrain.ai/getting-started/quickstart\" rel=\"noreferrer noopener\">\n   quickstart tutorial\n  </a>\n  .\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  <strong>\n   References\n  </strong>\n </h2>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/callbacks/UpTrainCallback.ipynb\" rel=\"noreferrer noopener\">\n    UpTrain Callback Handler Tutorial\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/uptrain-ai/uptrain\" rel=\"noreferrer noopener\">\n    UpTrain GitHub Repository\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6\" rel=\"noreferrer noopener\">\n    Advanced RAG Techniques: an Illustrated Overview\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://arxiv.org/pdf/2307.03172.pdf\" rel=\"noreferrer noopener\">\n    Lost in the Middle: How Language Models Use Long Contexts\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/community/integrations/uptrain.html\" rel=\"noreferrer noopener\">\n    UpTrainCallbackHandler documentation\n   </a>\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://uptrain.ai/\" rel=\"noreferrer noopener\">\n    UpTrain Website\n   </a>\n  </li>\n </ol>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 16857, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "512f2072-c2a8-4b01-844c-5a4905be82a4": {"__data__": {"id_": "512f2072-c2a8-4b01-844c-5a4905be82a4", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.html", "file_name": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.html", "file_type": "text/html", "file_size": 30335, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.html", "file_name": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.html", "file_type": "text/html", "file_size": 30335, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "d311a4f51b97b2034e31ce7fba884a7bdaf10612a2db32813f2883ffd0efdaa1", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Anthropic\u2019s\n  <a href=\"https://www.anthropic.com/index/100k-context-windows\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   100K Context Window\n  </a>\n  expansion, just released yesterday, has taken the AI community by storm. A 100k token limit is approximately 75k words (~3x GPT4\u201332k\u2019s context window, ~25x that of GPT-3.5/ChatGPT); this means that you can now fit 300 pages of text in a\n  <em class=\"on\">\n   single\n  </em>\n  inference call\n  <em class=\"on\">\n   .\n  </em>\n </p>\n <p>\n  One of the core use cases highlighted in the Anthropic blog is\n  <a href=\"https://vimeo.com/825669443\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   analyzing an SEC 10-K filing\n  </a>\n  ; the model is capable of ingesting the entire report, and producing answers to different questions.\n </p>\n <p>\n  Coincidentally, we\n  <a href=\"https://medium.com/@jerryjliu98/how-unstructured-and-llamaindex-can-help-bring-the-power-of-llms-to-your-own-data-3657d063e30d\" rel=\"noopener\">\n   published a tutorial\n  </a>\n  a few months ago showing how LlamaIndex + Unstructured + GPT3 could help you perform different queries over UBER SEC 10-k filings. LlamaIndex provides a comprehensive toolkit to help manage external data on top of any LLM with limited context windows, and we show that we can execute a diverse range of queries, from questions over a single document to comparing sections across documents.\n </p>\n <p>\n  How well does Anthropic\u2019s 100k model do over UBER SEC 10-k filings? Moreover, how well does it do\n  <em class=\"on\">\n   without\n  </em>\n  the help of any of LlamaIndex\u2019s more advanced data structures? In this blog we show the performance of Anthropic\u2019s model on different queries, using the simplest data structure available: the list index.\n </p>\n <h1>\n  High-Level Findings\n </h1>\n <p>\n  Where Anthropic\u2019s 100k model does well:\n </p>\n <ul>\n  <li>\n   <strong>\n    Holistic understanding of the data (kind of, after some prompt tuning):\n   </strong>\n   Anthropic\u2019s model does demonstrate an impressive capability to synthesize insights across the entire context window to answer the question at hand (assuming we set\n   <code class=\"cw pu pv pw px b\">\n    response_mode=\"tree_summarize\"\n   </code>\n   , see below). It can miss details though; see below!\n  </li>\n  <li>\n   <strong>\n    Latency:\n   </strong>\n   This one was surprising to us. Anthropic\u2019s model is able to crunch an entire UBER 10-k filing in ~60\u201390 seconds, which seems long but is much faster than repeated API calls to GPT-3 (which when added up can take minutes).\n  </li>\n </ul>\n <p>\n  Where Anthropic\u2019s 100k model doesn\u2019t do well:\n </p>\n <ul>\n  <li>\n   <strong>\n    Cost:\n   </strong>\n   This one is obvious. Every query we ran processed hundreds of thousands of tokens. At\n   <a href=\"https://console.anthropic.com/account/pricing\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    $11 per million tokens for Claude-v1\n   </a>\n   , this equates to $1 per query, which can quickly add up.\n  </li>\n  <li>\n   <strong>\n    Reasoning over more complicated prompts:\n   </strong>\n   Anthropic\u2019s model demonstrated a surprising lack of ability to understand our refine prompt for\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/how_to/query/response_synthesis.html#refine\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    \u201ccreate-and-refine\u201d response synthesis\n   </a>\n   , returning incorrect/irrelevant results. We ended up switching to\n   <a href=\"https://gpt-index.readthedocs.io/en/latest/how_to/query/response_synthesis.html#tree-summarize\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    \u201ctree summarization\u201d instead\n   </a>\n   . See below for results.\n  </li>\n </ul>\n <h1>\n  Overview of Methodology\n </h1>\n <p>\n  We want to test the capabilities of Anthropic\u2019s 100K model on top of UBER 10-k filings from 2019\u20132022. We also want to do this while using as little retrieval/synthesis constructs as possible. This means no embeddings, and no fancy retrieval mechanisms.\n </p>\n <p>\n  Ideally, we can directly insert an entire 10-k filing (or even all four 10-k filings) into the prompt. However, we found that a single UBER 10-k filing actually consists of ~\n  <strong>\n   160k tokens, which is greater than the 100k context window.\n  </strong>\n  This means that we still have to chunk up each filing!\n </p>\n <p>\n  We end up using our\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/guides/primer/index_guide.html#querying\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   list index data structure\n  </a>\n  \u2014 we split each text up into massive ~100k token chunks, and use our\n  <strong>\n   response synthesis strategies\n  </strong>\n  to synthesize an answer across multiple chunks.\n </p>\n <p>\n  We run some queries over each filing as well as over multiple filings, similar to our original blog post. We report the results below.\n </p>\n <h1>\n  Tutorial Setup\n </h1>\n <p>\n  Our data ingestion is the same as the LlamaIndex + Unstructured blog post. We use Unstructured\u2019s HTML parser to parse the HTML DOM into nicely formatted text. We then create a Document object for each SEC filing.\n </p>\n <p>\n  You can access Unstructured data loaders on\n  <a href=\"https://llamahub.ai/l/file-unstructured\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaHub\n  </a>\n  .\n </p>\n <pre><span class=\"qm op gt px b bf qn qo l qp qq\" id=\"e8d2\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> download_loader\n<span class=\"hljs-keyword\">from</span> pathlib <span class=\"hljs-keyword\">import</span> Path\n\nUnstructuredReader = download_loader(<span class=\"hljs-string\">\"UnstructuredReader\"</span>, refresh_cache=<span class=\"hljs-literal\">True</span>)\n\nloader = UnstructuredReader()\ndoc_set = {}\nall_docs = []\nyears = [<span class=\"hljs-number\">2022</span>, <span class=\"hljs-number\">2021</span>, <span class=\"hljs-number\">2020</span>, <span class=\"hljs-number\">2019</span>]\n<span class=\"hljs-keyword\">for</span> year <span class=\"hljs-keyword\">in</span> years:\n    year_doc = loader.load_data(file=Path(<span class=\"hljs-string\">f'./data/UBER/UBER_<span class=\"hljs-subst\">{year}</span>.html'</span>), split_documents=<span class=\"hljs-literal\">False</span>)[<span class=\"hljs-number\">0</span>]\n    <span class=\"hljs-comment\"># insert year metadata into each year</span>\n    year_doc.extra_info = {<span class=\"hljs-string\">\"year\"</span>: year}\n    doc_set[year] = year_doc\n    all_docs.append(year_doc)</span></pre>\n <p>\n  Next, we want to setup the Anthropic LLM. We\u2019re using claude-v1 by default. We also want to manually define the new 100k-token input size within our\n  <code class=\"cw pu pv pw px b\">\n   PromptHelper\n  </code>\n  object; this will help us figure out how to \u201ccompact\u201d context into the input prompt space during response synthesis.\n </p>\n <p>\n  We set the\n  <code class=\"cw pu pv pw px b\">\n   max_input_size\n  </code>\n  to 100k and the output length to 2048. We also set the context chunk size to a high value (95k, leaving some buffer room for rest of the prompt). Context will only be chunked if the number of tokens exceeds this limit.\n </p>\n <pre><span class=\"qm op gt px b bf qn qo l qp qq\" id=\"fa10\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> PromptHelper, LLMPredictor, ServiceContext\n<span class=\"hljs-keyword\">from</span> langchain.llms <span class=\"hljs-keyword\">import</span> Anthropic\n\n<span class=\"hljs-comment\"># define prompt helper</span>\n<span class=\"hljs-comment\"># set maximum input size</span>\nmax_input_size = <span class=\"hljs-number\">100000</span>\n<span class=\"hljs-comment\"># set number of output tokens</span>\nnum_output = <span class=\"hljs-number\">2048</span>\n<span class=\"hljs-comment\"># set maximum chunk overlap</span>\nmax_chunk_overlap = <span class=\"hljs-number\">20</span>\nprompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n\nllm_predictor = LLMPredictor(llm=Anthropic(model=<span class=\"hljs-string\">\"claude-v1.3-100k\"</span>, temperature=<span class=\"hljs-number\">0</span>, max_tokens_to_sample=num_output))\nservice_context = ServiceContext.from_defaults(\n    llm_predictor=llm_predictor, prompt_helper=prompt_helper,\n    chunk_size_limit=<span class=\"hljs-number\">95000</span>\n)</span></pre>\n <h1>\n  Analyzing a Single Document\n </h1>\n <p>\n  Let\u2019s first analyze queries over a single document. We build a list index over the 2019 UBER 10-K:\n </p>\n <pre><span class=\"qm op gt px b bf qn qo l qp qq\" id=\"8434\">list_index = GPTListIndex.from_documents([doc_set[<span class=\"hljs-number\">2019</span>]], service_context=service_context)\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-built_in\">len</span>(list_index.index_struct.nodes))</span></pre>\n <p>\n  As mentioned, the 10-K exceeds the 100k token limit, and so there are two nodes within the list index.\n </p>\n <p>\n  We then ask a query: \u201cWhat were some of the biggest risk factors in 2019?\u201d\n </p>\n <p>\n  Recall that there are two approaches within LlamaIndex for response synthesis across multiple nodes (where the total context exceeds the context window): a \u201ccreate-and-refine\u201d strategy, and a \u201ctree summarize\u201d strategy.\n </p>\n <ul>\n  <li>\n   <strong>\n    Create-and-Refine:\n   </strong>\n   sequentially go through each retrieved\n   <code class=\"cw pu pv pw px b\">\n    Node\n   </code>\n   . Use a Question-Answer Prompt for the first Node, and use a Refine Prompt for subsequent Nodes. Make a separate LLM call per Node.\n  </li>\n  <li>\n   <strong>\n    Tree Summarize:\n   </strong>\n   Given a set of\n   <code class=\"cw pu pv pw px b\">\n    Node\n   </code>\n   objects and the query, recursively construct a tree using the Question Answer Prompt and return the root node as the response. Good for summarization purposes.\n  </li>\n </ul>\n <p>\n  Claude-v1 100K does not do well with the \u201ccreate-and-refine\u201d strategy (which is the default).\n </p>\n <pre><span class=\"qm op gt px b bf qn qo l qp qq\" id=\"bc70\"># NOTE: the default create/refine approach does not give good answers\nquery = \"What were some of the biggest risk factors in 2019?\"\nquery_engine = list_index.as_query_engine(service_context=service_context)\nresponse = query_engine.query(query)</span></pre>\n <p>\n  Here is the retrieved answer:\n </p>\n <pre><span class=\"qm op gt px b bf qn qo l qp qq\" id=\"a70c\">Original answer:\n\nSome of the biggest risk factors <span class=\"hljs-keyword\">for</span> Uber <span class=\"hljs-keyword\">in</span> <span class=\"hljs-number\">2019</span> included:\n\n\u2022 Regulatory challenges <span class=\"hljs-keyword\">and</span> uncertainty. Uber faced regulatory challenges <span class=\"hljs-keyword\">and</span> uncertainty <span class=\"hljs-keyword\">in</span> many markets, including restrictions on its products <span class=\"hljs-keyword\">and</span> services, caps on pricing, <span class=\"hljs-keyword\">and</span> licensing requirements. For example, California<span class=\"hljs-string\">'s AB5 law and other similar laws increased the risk of Drivers being classified as employees. Uber also faced regulatory scrutiny and bans in London, Barcelona, and other markets.\n\n\u2022 Competition. The markets in which Uber operates are highly competitive, and Uber faced significant competition from well-established and low-cost alternatives in 2019. Competitors also aggressively competed for Drivers and consumers by offering significant incentives and discounts. \n\n\u2022 Safety and security. There were risks related to the safety and security of Uber'</span>s platform, including risks <span class=\"hljs-keyword\">from</span> vehicle <span class=\"hljs-keyword\">or</span> scooter accidents, assaults, <span class=\"hljs-keyword\">and</span> other incidents. Uber released a safety report <span class=\"hljs-keyword\">in</span> <span class=\"hljs-number\">2019</span> detailing reports of sexual assaults on its platform which brought additional scrutiny.\n\n\u2022 Financial performance <span class=\"hljs-keyword\">and</span> profitability. Uber continued to experience significant losses <span class=\"hljs-keyword\">in</span> <span class=\"hljs-number\">2019</span> <span class=\"hljs-keyword\">as</span> it invested <span class=\"hljs-keyword\">in</span> growth. There was no guarantee that Uber would achieve <span class=\"hljs-keyword\">or</span> sustain profitability <span class=\"hljs-keyword\">in</span> the future.  \n\n\u2022 Reliance on third parties. Uber relied on third parties <span class=\"hljs-keyword\">for</span> key elements of its business like cloud computing services, payment processing, <span class=\"hljs-keyword\">and</span> background checks. If these third parties failed to provide services <span class=\"hljs-keyword\">or</span> increased costs, it could adversely impact Ube<span class=\"hljs-string\">r's business.\n\n\u2022 Macroeconomic conditions. Uber'</span>s business was sensitive to economic conditions <span class=\"hljs-keyword\">and</span> changes that impacted discretionary consumer spending. A decline <span class=\"hljs-keyword\">in</span> the economy could reduce demand <span class=\"hljs-keyword\">for</span> Ube<span class=\"hljs-string\">r's products and services.  \n\n\u2022 Reliance on Drivers and Restaurants. Uber'</span>s success depended on attracting <span class=\"hljs-keyword\">and</span> retaining enough Drivers <span class=\"hljs-keyword\">and</span> Restaurants to its platform. If Uber was unable to attract <span class=\"hljs-keyword\">or</span> retain enough Drivers <span class=\"hljs-keyword\">and</span> Restaurants, it could negatively impact its operations.\n\n\u2022 Intellectual <span class=\"hljs-built_in\">property</span>. If Uber was unable to protect its intellectual <span class=\"hljs-built_in\">property</span>, <span class=\"hljs-keyword\">or</span> <span class=\"hljs-keyword\">if</span> third parties claimed Uber was infringing on their intellectual <span class=\"hljs-built_in\">property</span>, it could harm Ube<span class=\"hljs-string\">r's business. Uber relied on a combination of copyright, trademark, patent, and trade secret laws to establish its intellectual property rights.\n\nRefined answer:\n\nSome of the biggest risk factors for Uber in 2019 included:\n\n\u2022 Regulatory challenges and uncertainty. Uber faced significant regulatory challenges in 2019, including AB5 in California which increased the risk of Drivers being classified as employees and regulatory scrutiny and temporary bans in London and other markets. These regulatory issues created uncertainty and posed risks to Uber'</span>s business model <span class=\"hljs-keyword\">and</span> financial results.\n\n\u2022 Competition. The ridesharing <span class=\"hljs-keyword\">and</span> meal delivery markets are highly competitive, <span class=\"hljs-keyword\">and</span> competitors offered significant incentives <span class=\"hljs-keyword\">and</span> discounts to take market share <span class=\"hljs-keyword\">from</span> Uber <span class=\"hljs-keyword\">in</span> <span class=\"hljs-number\">2019.</span> This competition could negatively impact Ube<span class=\"hljs-string\">r's growth and profitability.\n\n\u2022 Safety and security. There were risks related to the safety and security of Uber'</span>s platform, including risks <span class=\"hljs-keyword\">from</span> vehicle <span class=\"hljs-keyword\">or</span> scooter accidents, assaults, <span class=\"hljs-keyword\">and</span> other incidents. Ube<span class=\"hljs-string\">r's safety report detailing sexual assault reports brought additional scrutiny and risks. Any failure to ensure safety could significantly damage Uber'</span>s reputation <span class=\"hljs-keyword\">and</span> business.\n\n\u2022 Financial performance <span class=\"hljs-keyword\">and</span> profitability. Uber continued to experience substantial losses <span class=\"hljs-keyword\">in</span> <span class=\"hljs-number\">2019</span> due to investments <span class=\"hljs-keyword\">in</span> growth, <span class=\"hljs-keyword\">and</span> there was no guarantee of future profitability. Ube<span class=\"hljs-string\">r's path to profitability was uncertain, and failure to become profitable could threaten its business model and access to capital.  \n\n\u2022 Reliance on third parties. Uber relied on third parties for services like cloud computing, payment processing, and background checks. If these third parties failed to provide services or increased costs, it could negatively impact Uber'</span>s offerings, growth, <span class=\"hljs-keyword\">and</span> profitability.  \n\n\u2022 Macroeconomic conditions. Ube<span class=\"hljs-string\">r's business was sensitive to the economy and consumer discretionary spending. An economic downturn could reduce demand for Uber'</span>s offerings <span class=\"hljs-keyword\">and</span> threaten its growth <span class=\"hljs-keyword\">and</span> financial performance.\n\n\u2022 Reliance on Drivers <span class=\"hljs-keyword\">and</span> Restaurants. Ube<span class=\"hljs-string\">r's success depended on attracting and retaining enough Drivers and Restaurants to support growth and meet consumer demand. Failure to attract or retain Drivers and Restaurants could significantly impact Uber'</span>s business.  \n\n\u2022 Intellectual <span class=\"hljs-built_in\">property</span>. Failure to protect intellectual <span class=\"hljs-built_in\">property</span> <span class=\"hljs-keyword\">or</span> infringement of others<span class=\"hljs-string\">' intellectual property could damage Uber'</span>s competitive position <span class=\"hljs-keyword\">and</span> business. Uber relied on laws <span class=\"hljs-keyword\">and</span> policies to protect intellectual <span class=\"hljs-built_in\">property</span>, but monitoring infringement <span class=\"hljs-keyword\">and</span> unauthorized use was difficult.</span></pre>\n <p>\n  As shown, the answer contains the prompt within it; the point of the refine prompt was to produce just the refined answer, not produce both the original and refined answers.\n </p>\n <p>\n  For some reason, using the Tree Summarize strategy works much better.\n </p>\n <pre><span class=\"qm op gt px b bf qn qo l qp qq\" id=\"e9b4\"># NOTE: tree_summarize gives better answers\nquery = \"What were some of the biggest risk factors in 2019?\"\nquery_engine = list_index.as_query_engine(service_context=service_context, response_mode=\"tree_summarize\")\nresponse = query_engine.query(query)</span></pre>\n <p>\n  Returned response:\n </p>\n <pre><span class=\"qm op gt px b bf qn qo l qp qq\" id=\"e996\">\u2022 Regulatory challenges and uncertainty: Uber faced significant regulatory challenges and uncertainty in <span class=\"hljs-number\">2019</span>, including AB5 in California which codified a <span class=\"hljs-keyword\">new</span> <span class=\"hljs-title class_\">test</span> <span class=\"hljs-keyword\">for</span> determining whether workers should be classified as employees or independent contractors. Uber also faced regulatory scrutiny and bans in other markets like London, UK. These regulatory issues created uncertainty and risk around Uber<span class=\"hljs-string\">'s business model and operations.\n\n\u2022 Safety and security: Uber received negative publicity around safety incidents on its platform which could damage its brand and reputation. Uber released a safety report in 2019 on sexual assaults and other incidents which led to additional scrutiny. Safety and security risks remain an ongoing issue for Uber'</span>s business.\n\n\u2022 Competition: The markets in which Uber competes are intensely competitive, and Uber faces competition from <span class=\"hljs-keyword\">new</span> <span class=\"hljs-title class_\">and</span> existing companies in the various segments it operates in like ridesharing, food delivery, and logistics. Increased competition could make it difficult <span class=\"hljs-keyword\">for</span> Uber to attract and retain platform users.\n\n\u2022 Financial losses: Uber continued to experience significant losses from operations in <span class=\"hljs-number\">2019</span> as it invested in growth. Uber may not achieve or sustain profitability in the future which could negatively impact its business and stock price.\n\n\u2022 Reliance on third-parties: Uber relies on third-party data centers, cloud computing services, and software which exposes it to risks <span class=\"hljs-keyword\">if</span> these third-parties experience any disruptions or interference. Any disruption of third-party services could negatively impact Uber<span class=\"hljs-string\">'s business.\n\n\u2022 Macroeconomic conditions: Uber'</span>s business is sensitive to macroeconomic conditions, and any downturn could reduce discretionary consumer spending which would harm Uber<span class=\"hljs-string\">'s business. Issues like a recession, increased unemployment, or reduced consumer confidence could all potentially impact Uber'</span>s business.\n\n\u2022 Reliance on Drivers and Restaurants: Uber<span class=\"hljs-string\">'s business depends on attracting and retaining Drivers and Restaurants to its platform. If Uber is unable to attract or maintain a critical mass of Drivers and Restaurants, its platform will become less appealing to platform users and its financial results would be adversely impacted.</span></span></pre>\n <p>\n  The answer is much better. It goes into details about the US-China trade war, slowing economic growth, Brexit, and more (keep in mind 2019 is pre-COVID).\n </p>\n <h2>\n  Token Usage and Latency\n </h2>\n <p>\n  The document contains around ~170K tokens. For some reason, this number is not reflected on the Anthropic usage logs (the \u201cPrompt Tokens\u201d section seems capped at 10240). But the Prompt Length (in characters) is logged, as well as the model latency.\n </p>\n <p>\n  Given the pricing, ~170K tokens would be equivalent to $1.5\u20132 USD.\n </p>\n <p>\n  A query through one Uber SEC-10K takes around\n  <strong>\n   150\n  </strong>\n  <strong>\n   seconds\n  </strong>\n  , including all LLM calls. This is actually a bit faster than repeated calls to ChatGPT/davinci. Each ChatGPT/davinci call (with the 4K token window maximized), empirically can take 6\u201310 seconds to complete \u2192\n  <strong>\n   125\u2013250 seconds (\n  </strong>\n  or more).\n </p>\n <h1>\n  Analyzing Multiple Documents\n </h1>\n <p>\n  A popular example in our\n  <a href=\"https://medium.com/@jerryjliu98/how-unstructured-and-llamaindex-can-help-bring-the-power-of-llms-to-your-own-data-3657d063e30d\" rel=\"noopener\">\n   previous blog post\n  </a>\n  was showcasing that you could compare/contrast different documents with LlamaIndex graph structures.\n </p>\n <p>\n  We test whether we can do that here as well, by feeding in multiple SEC reports into Claude-v1 100k.\n </p>\n <p>\n  <strong>\n   Caveat:\n  </strong>\n  Considering that one UBER SEC-10K filing doesn\u2019t even fit in the context window, we\u2019ll of course also need to implement response synthesis strategies in order to handle ingesting multiple 10K filings.\n </p>\n <p>\n  We build a list index over all 4 10K filings: 2019, 2020, 2021, and 2022.\n </p>\n <pre><span class=\"qm op gt px b bf qn qo l qp qq\" id=\"5912\">list_index = GPTListIndex.from_documents(all_docs, service_context=service_context)\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-built_in\">len</span>(list_index.index_struct.nodes))</span></pre>\n <p>\n  We then ask our question using our Tree Summarize response mode.\n </p>\n <pre><span class=\"qm op gt px b bf qn qo l qp qq\" id=\"feee\">query = \"How are the risk factors changing across years? Compare/contrast the risk factors across the SEC filings.\"\nquery_engine = list_index.as_query_engine(response_mode=\"tree_summarize\")\nresponse = query_engine.query(query)</span></pre>\n <p>\n  The full answer is given below:\n </p>\n <pre><span class=\"qm op gt px b bf qn qo l qp qq\" id=\"a606\">The risk factors disclosed in Uber<span class=\"hljs-number\">'</span>s SEC filings have evolved over time based on Uber<span class=\"hljs-number\">'</span>s business <span class=\"hljs-keyword\">and</span> industry changes. Some of the key differences in risk factors across the filings are:\n\n<span class=\"hljs-number\">2017</span> <span class=\"hljs-number\">10</span>-K:\n- Focused heavily on risks related to negative publicity, competition, dependence on independent contractors, <span class=\"hljs-keyword\">and</span> regulatory challenges as Uber was still facing backlash from various PR crises <span class=\"hljs-keyword\">and</span> regulatory pushback. \n- Also highlighted risks from intellectual property litigation given various IP disputes at the time.\n\n<span class=\"hljs-number\">2018</span> <span class=\"hljs-number\">10</span>-K:\n- Added more risks related to autonomous vehicles as Uber ramped up its self-driving car efforts. Specifically called out risks from accidents, technical challenges, <span class=\"hljs-keyword\">and</span> competition in the AV space.\n- Removed some risks related to negative publicity <span class=\"hljs-keyword\">and</span> PR crises as those issues had subsided. But added risks related to corporate culture <span class=\"hljs-keyword\">and</span> workplace environment given the Fowler scandal.\n\n<span class=\"hljs-number\">2019</span> <span class=\"hljs-number\">10</span>-K: \n- Further expanded AV risks to include risks from partnerships <span class=\"hljs-keyword\">and</span> third party relationships as Uber relied more on partnerships with other companies like Toyota <span class=\"hljs-keyword\">and</span> Volvo. \n- Added risks related to environmental regulations <span class=\"hljs-keyword\">and</span> climate change as those issues received more mainstream attention. \n- Removed risks related to initial scaling challenges as Uber had matured as a company. But added risks related to growth into <span class=\"hljs-keyword\">new</span> offerings <span class=\"hljs-keyword\">and</span> markets as Uber expanded into freight shipping, scooters, etc.\n\nIn summary, Uber<span class=\"hljs-number\">'</span>s risk factors have evolved to reflect the key issues <span class=\"hljs-keyword\">and</span> challenges facing the company at any point in time. The risks highlight both external factors like regulations as well as internal factors related to Uber<span class=\"hljs-number\">'</span>s business, technology, <span class=\"hljs-keyword\">and</span> operations. The changes over time show how an innovative company<span class=\"hljs-number\">'</span>s risks can shift quite quickly.\n\nSome of the key trends in how the risk factors are changing include:\n\n<span class=\"hljs-number\">1.</span> Risks related to negative publicity <span class=\"hljs-keyword\">and</span> PR crises decreased over time as those issues subsided, <span class=\"hljs-keyword\">while</span> risks related to other issues like AV technology, workplace culture, <span class=\"hljs-keyword\">and</span> <span class=\"hljs-keyword\">new</span> initiatives increased. This shows how Uber<span class=\"hljs-number\">'</span>s risks evolved as the company matured.\n\n<span class=\"hljs-number\">2.</span> Risks tend to increase in areas where Uber is actively investing <span class=\"hljs-keyword\">or</span> expanding into. For example, risks related to AVs, partnerships, <span class=\"hljs-keyword\">and</span> <span class=\"hljs-keyword\">new</span> mobility offerings increased as Uber ramped up efforts in those areas. This highlights how risk profiles change with a company<span class=\"hljs-number\">'</span>s strategic priorities.\n\n<span class=\"hljs-number\">3.</span> External risks like regulations <span class=\"hljs-keyword\">and</span> climate change were added as those issues gained more mainstream attention <span class=\"hljs-keyword\">and</span> importance. This shows how companies have to adapt their risk factors to account <span class=\"hljs-keyword\">for</span> changes in the overall business environment. \n\n<span class=\"hljs-number\">4.</span> Certain foundational risks around competition, growth, <span class=\"hljs-keyword\">and</span> reliance on independent contractors persisted over time. But the specifics <span class=\"hljs-keyword\">and</span> details provided <span class=\"hljs-keyword\">for</span> those risks evolved based on Uber<span class=\"hljs-number\">'</span>s current challenges <span class=\"hljs-keyword\">and</span> priorities. So <span class=\"hljs-keyword\">while</span> the themes remained, the risks were actively updated.\n\nIn summary, Uber<span class=\"hljs-number\">'</span>s risk factors changed over time to provide more details on the company<span class=\"hljs-number\">'</span>s priorities, challenges, partnerships, <span class=\"hljs-keyword\">and</span> external environment at any given point. But certain core risks inherent to Uber<span class=\"hljs-number\">'</span>s business model have also remained consistently highlighted, demonstrating how those foundational risks are <span class=\"hljs-type\">long</span>-term in nature. The changes in risks over time provide a glimpse into how an innovative company<span class=\"hljs-number\">'</span>s risk profile is constantly evolving.</span></pre>\n <p>\n  This response only contains risk refactor analysis over the 2019 10-K (which in turn contains risk refactors for 2017 and 2018). It does not contain the years from 2020 onwards. Part of this is potentially due to our tree summarize response synthesis strategy. Nevertheless, it shows that trying to naively \u201cstuff\u201d documents into big 100K token chunks with simple response synthesis strategies still does not produce the optimal answers.\n </p>\n <h2>\n  <strong>\n   Token Usage and Latency\n  </strong>\n </h2>\n <p>\n  As expected, feeding all four documents into Anthropic necessitates many more chained LLM calls, which consumes way more tokens and takes a lot longer (on the order of 9\u201310 minutes).\n </p>\n <h1>\n  Conclusion\n </h1>\n <p>\n  In general, the new 100K context window is incredibly exciting and offers developers a new mode of feeding in data into the LLM for different tasks/queries. It offers coherent analysis with a marginal token cost that is much cheaper than that of GPT-4.\n </p>\n <p>\n  That said, trying to maximize this context window with each inference call does come with tradeoffs in terms of latency and cost.\n </p>\n <p>\n  We look forward to doing more experiments/comparisons/thought pieces on top of Claude! Let us know your feedback.\n </p>\n <h2>\n  Resources\n </h2>\n <p>\n  You can check out our\n  <a href=\"https://colab.research.google.com/drive/1uuqvPI2_WNFMd7g-ahFoioSHV7ExB2GR?usp=sharing\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   full Colab notebook here\n  </a>\n  .\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 30200, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56225273-6ced-4d10-a708-beb42b52bc19": {"__data__": {"id_": "56225273-6ced-4d10-a708-beb42b52bc19", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/the-latest-updates-to-llamacloud.html", "file_name": "the-latest-updates-to-llamacloud.html", "file_type": "text/html", "file_size": 5697, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/the-latest-updates-to-llamacloud.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/the-latest-updates-to-llamacloud.html", "file_name": "the-latest-updates-to-llamacloud.html", "file_type": "text/html", "file_size": 5697, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "d63a623374096b01b56660202c55c793a48d3993b62eaaaf3e5fb15806b9c83a", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  To build a production-quality LLM agent over your data, you need a production-quality data processing layer. LlamaCloud is that data processing and management layer for your AI knowledge assistants. Since\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/llamacloud-built-for-enterprise-llm-app-builders\" rel=\"noreferrer noopener\">\n   launching a LlamaCloud waitlist last week\n  </a>\n  , we\u2019ve gotten hundreds of signups and published case studies showing how it cuts\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/blog/case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud\" rel=\"noreferrer noopener\">\n   production development hours by 50%\n  </a>\n  .\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  On top of that, our team has shipped a slew of new features at a breakneck pace in the past week. We\u2019re excited to highlight these new features that collectively help you\n  <strong>\n   set up a chat interface in minutes,\n  </strong>\n  <strong>\n   increase developer collaboration within your team, and access more data and metadata.\n  </strong>\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Set up a Chat Interface in Minutes\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We are releasing\n  <strong>\n   LlamaCloud Chat,\n  </strong>\n  which gives you an easy-to-use chat interface over your data. This chat interface is a conversational RAG pipeline built over the advanced retrieval interface that a given pipeline provides, and has out-of-the-box support for streaming and citations - it\u2019s powered by the same DNA as\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/create-llama\" rel=\"noreferrer noopener\">\n   <code class=\"SanityPortableText_inlineCode__cI85z\">\n    create-llama\n   </code>\n   , our fully open-source set up tool\n  </a>\n  for LLM applications.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The LlamaCloud UI already lets you set up a data pipeline over any data in minutes, and now you get a full-blown ChatGPT over your data in minutes. Besides the chat UI, you also have additional flexibility:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   You can customize metadata filters in the retrieval parameters\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   You can view retrieved nodes and their source files\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Besides chunk-level retrieval, you can now do\n   <strong>\n    file-level retrieval\n   </strong>\n   (more on this soon!)\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  LlamaCloud is fundamentally a developer tool: with these updates, we enable developers to spend less time on data pipeline setup and iteration, and more time on writing the orchestration logic on top of this interface.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Increased Developer Collaboration\n </h2>\n <figure>\n  <figcaption>\n   The team selection interface\n  </figcaption>\n </figure>\n <figure>\n  <figcaption>\n   Organization settings\n  </figcaption>\n </figure>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We\u2019ve added\n  <strong>\n   organizational features\n  </strong>\n  into LlamaCloud, enabling any individual user to create an organization and add other users to the organization. Any user within an organization will have a view of all the organization\u2019s projects and indexes within each project.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  This allows your team to have a single-source of truth for your data pipelines. In the past each developer would spend time re-indexing/experimenting with the same sources of data. This feature enables transparency, re-use, and generally more rapid development velocity.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Improved Data and Metadata Access\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We\u2019ve made several updates here - we\u2019ve added more data connectors and added features to let you more easily access and customize metadata.\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   We added a Notion, Slack, and Jira Connector\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Our Sharepoint connector now natively integrates with user IDs that you can filter for, enabling you to build LLM applications with access control.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   You can now attach metadata to any uploaded file as a CSV - do this through the UI or our API!\n  </li>\n </ul>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Want to see what LlamaCloud can do for you?\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Come sign up on our\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://bit.ly/llamacloud\" rel=\"noreferrer noopener\">\n   waitlist\n  </a>\n  for access. If you\u2019re interested in chatting about enterprise plans,\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.llamaindex.ai/contact\" rel=\"noreferrer noopener\">\n   get in touch.\n  </a>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  If you\u2019ve gotten access to LlamaCloud, check out our\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/run-llama/llamacloud-demo\" rel=\"noreferrer noopener\">\n   rich repository of demonstrations and examples\n  </a>\n  on how to build different LLM application use cases.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5678, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ab1d3a1-cacc-48e3-afd0-efeed2a51579": {"__data__": {"id_": "8ab1d3a1-cacc-48e3-afd0-efeed2a51579", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.html", "file_name": "timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.html", "file_type": "text/html", "file_size": 34118, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.html", "file_name": "timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.html", "file_type": "text/html", "file_size": 34118, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "fc40ff6dc2c470bccf3c37c98d71f267750a5535e6f219689f1218af738b0e77", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Authors: Avthar Sewrathan, Matvey Arye, Jerry Liu, Yi Ding\n </p>\n <p>\n  Introducing the\n  <a href=\"https://www.timescale.com/ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Timescale Vector\n  </a>\n  integration for LlamaIndex. Timescale Vector enables LlamaIndex developers to build better AI applications with PostgreSQL as their vector database: with faster vector similarity search, efficient time-based search filtering, and the operational simplicity of a single, easy-to-use cloud PostgreSQL database for not only vector embeddings but an AI application\u2019s relational and time-series data too.\n </p>\n <p>\n  PostgreSQL is the world\u2019s most loved database, according to the\n  <a href=\"https://survey.stackoverflow.co/2023/#section-most-popular-technologies-databases\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Stack Overflow 2023 Developer Survey\n  </a>\n  . And for a good reason: it\u2019s been battle-hardened by production use for over three decades, it\u2019s robust and reliable, and it has a rich ecosystem of tools, drivers, and connectors.\n </p>\n <p>\n  And while pgvector, the open-source extension for vector data on PostgreSQL, is a wonderful extension (and all its features are offered as part of Timescale Vector), it is just one piece of the puzzle in providing a production-grade experience for AI application developers on PostgreSQL. After speaking with numerous developers at nimble startups and established industry giants, we saw the need to enhance pgvector to cater to the performance and operational needs of developers building AI applications.\n </p>\n <p>\n  Here\u2019s the TL;DR on how Timescale Vector helps you build better AI applications with LlamaIndex:\n </p>\n <ul>\n  <li>\n   <strong>\n    Faster similarity search on millions of vectors:\n   </strong>\n   Thanks to the introduction of a new search index inspired by the DiskANN algorithm,\n   <a href=\"https://www.timescale.com/blog/how-we-made-postgresql-the-best-vector-database/?utm_campaign=vectorlaunch&amp;utm_source=llamaindex&amp;utm_medium=referral\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Timescale Vector achieves 3X faster search speed at ~99% recall than a specialized database\n   </a>\n   and outperforms all existing PostgreSQL search indexes by between 39.39% and 1,590.33% on a dataset of one million OpenAI embeddings. Plus, enabling product quantization yields a\n   <a href=\"https://www.timescale.com/blog/how-we-made-postgresql-the-best-vector-database/?utm_campaign=vectorlaunch&amp;utm_source=llamaindex&amp;utm_medium=referral\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    10x index space savings compared to pgvector\n   </a>\n   . Timescale Vector also offers pgvector\u2019s Hierarchical Navigable Small Worlds (HNSW) and Inverted File Flat (IVFFlat) indexing algorithms.\n  </li>\n  <li>\n   <strong>\n    Efficient similarity search with time-based filtering:\n   </strong>\n   Timescale Vector optimizes time-based vector search queries, leveraging the automatic time-based partitioning and indexing of\n   <a href=\"https://docs.timescale.com/use-timescale/latest/hypertables/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Timescale\u2019s hypertables\n   </a>\n   to efficiently find recent embeddings, constrain vector search by a time range or document age, and store and retrieve large language model (LLM) response and chat history with ease. Time-based semantic search also enables you to use Retrieval Augmented Generation (RAG) with time-based context retrieval to give users more useful LLM responses.\n  </li>\n  <li>\n   <strong>\n    Simplified AI infra stack:\n   </strong>\n   By combining vector embeddings, relational data, and time-series data in one PostgreSQL database, Timescale Vector eliminates the operational complexity that comes with managing multiple database systems at scale.\n  </li>\n  <li>\n   <strong>\n    Simplified metadata handling and multi-attribute filtering:\n   </strong>\n   Developers can leverage all PostgreSQL data types to store and filter metadata and JOIN vector search results with relational data for more contextually relevant responses. In future releases, Timescale Vector will further optimize rich multi-attribute filtering, enabling even faster similarity searches when filtering on metadata.\n  </li>\n </ul>\n <p>\n  On top of these innovations for vector workloads, Timescale Vector provides a robust, production-ready cloud PostgreSQL platform with flexible pricing, enterprise-grade security, and free expert support.\n </p>\n <p>\n  In the rest of this post, we\u2019ll dive deeper (with code!) into the unique capabilities Timescale Vector enables for developers wanting to use PostgreSQL as their vector database with LlamaIndex:\n </p>\n <ul>\n  <li>\n   Faster similarity search with DiskANN, HNSW and IVFFlat index types.\n  </li>\n  <li>\n   Efficient similarity search when filtering vectors by time.\n  </li>\n  <li>\n   Retrieval Augmented Generation (RAG) with time-based context retrieval.\n  </li>\n </ul>\n <p>\n  (If you want to jump straight to the code, explore\n  <a href=\"https://gpt-index.readthedocs.io/en/stable/examples/vector_stores/Timescalevector.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   this tutorial\n  </a>\n  ).\n </p>\n <p>\n  <strong>\n   \ud83c\udf89\n  </strong>\n  <a href=\"https://console.cloud.timescale.com/signup?utm_campaign=vectorlaunch&amp;utm_source=llamaindex&amp;utm_medium=referral\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    LlamaIndex Users Get 3 Months of Timescale Vector for Free\n   </strong>\n  </a>\n </p>\n <p>\n  We\u2019re giving LlamaIndex users an extended 90-day trial of Timescale Vector. This makes it easy to test and develop your applications on Timescale Vector, as you won\u2019t be charged for any cloud PostgreSQL databases you spin up during your trial period.\n  <a href=\"https://console.cloud.timescale.com/signup?utm_campaign=vectorlaunch&amp;utm_source=llamaindex&amp;utm_medium=referral\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Try Timescale Vector for free today\n  </a>\n  .\n </p>\n <h1>\n  Faster Vector Similarity Search in PostgreSQL\n </h1>\n <p>\n  Timescale Vector speeds up Approximate Nearest Neighbor (ANN) search on large-scale vector datasets, enhancing pgvector with a state-of-the-art ANN index inspired by the\n  <a href=\"https://www.microsoft.com/en-us/research/publication/diskann-fast-accurate-billion-point-nearest-neighbor-search-on-a-single-node/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   DiskANN\n  </a>\n  algorithm. Timescale Vector also offers pgvector\u2019s HNSW and IVFFlat indexing algorithms, giving developers the flexibility to choose the right index for their use case.\n </p>\n <p>\n  Our performance benchmarks using the\n  <a href=\"https://github.com/erikbern/ann-benchmarks/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   ANN benchmarks\n  </a>\n  suite show that Timescale Vector achieves between 39.43% and 1,590.33% faster search speed at ~99% recall than all existing PostgreSQL search indexes, and 3X faster search speed at ~99% recall than specialized vector databases on a dataset of one million OpenAI embeddings. You can\n  <a href=\"https://www.timescale.com/blog/how-we-made-postgresql-the-best-vector-database/?utm_campaign=vectorlaunch&amp;utm_source=llamaindex&amp;utm_medium=referral\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   read more about the performance benchmark methodology, the databases compared and results here\n  </a>\n  .\n </p>\n <figure>\n  <figcaption class=\"pz fe qa ny nz qb qc be b bf z dt\">\n   <em class=\"qd\">\n    Timescale Vector\u2019s new DiskANN-inspired index outperforms all existing PostgreSQL index types when performing approximate nearest neighbor searches at 99 % recall on 1 million OpenAI embeddings.\n   </em>\n  </figcaption>\n </figure>\n <p>\n  Using Timescale Vector\u2019s DiskANN, HNSW, or IVFFLAT indexes in LlamaIndex is incredibly straightforward.\n </p>\n <p>\n  Simply create a Timescale Vector vector store and add the\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/data_modules/documents_and_nodes/usage_nodes.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   data nodes\n  </a>\n  you want to query as shown below:\n </p>\n <pre><span class=\"qi ow gt qf b bf qj qk l ql qm\" id=\"e55c\"><span class=\"hljs-keyword\">from</span> llama_index.vector_stores <span class=\"hljs-keyword\">import</span> TimescaleVectorStore\n\n<span class=\"hljs-comment\"># Create a timescale vector store with specified params</span>\nts_vector_store = TimescaleVectorStore.from_params(\n   service_url=TIMESCALE_SERVICE_URL,\n   table_name=<span class=\"hljs-string\">\"your_table_name\"</span>,\n   time_partition_interval= timedelta(days=<span class=\"hljs-number\">7</span>),\n)\nts_vector_store.add(nodes)</span></pre>\n <p>\n  Then run:\n </p>\n <pre><span class=\"qi ow gt qf b bf qj qk l ql qm\" id=\"b587\"># Create a timescale vector index (DiskANN)\nts_vector_store.create_index()</span></pre>\n <p>\n  This will create a timescale-vector index with the default parameters.\n </p>\n <p>\n  We should point out that the term \u201cindex\u201d is a bit overloaded. For many VectorStores, an index is the thing that stores your data (in relational databases this is often called a table), but in the PostgreSQL world an index is something that speeds up search, and we are using the latter meaning here.\n </p>\n <p>\n  We can also specify the exact parameters for index creation in the\n  <code class=\"cw qn qo qp qf b\">\n   create_index\n  </code>\n  command as follows:\n </p>\n <pre><span class=\"qi ow gt qf b bf qj qk l ql qm\" id=\"69b5\"><span class=\"hljs-meta\"># create new timescale vector index (DiskANN) with specified parameters</span>\nts_vector_store.<span class=\"hljs-built_in\">create_index</span>(<span class=\"hljs-string\">\"tsv\"</span>, max_alpha=<span class=\"hljs-number\">1.0</span>, num_neighbors=<span class=\"hljs-number\">50</span>)</span></pre>\n <p>\n  Advantages to this Timescale Vector\u2019s new DiskANN-inspired vector search index include the following:\n </p>\n <ul>\n  <li>\n   Faster vector search at 99% accuracy in PostgreSQL.\n  </li>\n  <li>\n   Optimized for running on disks, not only in memory use.\n  </li>\n  <li>\n   Quantization optimization compatible with PostgreSQL, reducing the vector size and consequently shrinking the index size (\n   <a href=\"https://www.timescale.com/blog/how-we-made-postgresql-the-best-vector-database/?utm_campaign=vectorlaunch&amp;utm_source=llamaindex&amp;utm_medium=referral\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    by 10x in some cases\n   </a>\n   !) and expediting searches.\n  </li>\n  <li>\n   Efficient hybrid search or filtering additional dimensions.\n  </li>\n </ul>\n <p>\n  For more on how Timescale Vector\u2019s new index works,\n  <a href=\"https://www.timescale.com/blog/how-we-made-postgresql-the-best-vector-database/?utm_campaign=vectorlaunch&amp;utm_source=llamaindex&amp;utm_medium=referral\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   see this blog post\n  </a>\n  .\n </p>\n <p>\n  Pgvector is packaged as part of Timescale Vector, so you can also access pgvector\u2019s HNSW and IVFFLAT indexing algorithms in your LlamaIndex applications. The ability to conveniently create ANN search indexes from your LlamaIndex application code makes it easy to create different indexes and compare their performance:\n </p>\n <pre><span class=\"qi ow gt qf b bf qj qk l ql qm\" id=\"57fe\"># Create an HNSW index\n# Note: You don't need to specify m and ef_construction parameters as we set smart defaults.\nts_vector_store.create_index(\"hnsw\", m=16, ef_construction=64)\n\n# Create an IVFFLAT index\n# Note: You don't need to specify num_lists and num_records parameters as we set smart defaults.\nts_vector_store.create_index(\"ivfflat\", num_lists=20, num_records=1000)</span></pre>\n <h1>\n  Add Efficient Time-Based Search Functionality to Your LlamaIndex AI Application\n </h1>\n <p>\n  Timescale Vector optimizes time-based vector search,\n  leveraging the automatic time-based partitioning and indexing of\n  <a href=\"https://docs.timescale.com/use-timescale/latest/hypertables/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Timescale\u2019s hypertables\n  </a>\n  to efficiently search vectors by time and similarity.\n </p>\n <p>\n  Time is often an important metadata component for vector embeddings. Sources of embeddings, like documents, images, and web pages, often have a timestamp associated with them, for example, their creation date, publishing date, or the date they were last updated, to name but a few.\n </p>\n <p>\n  We can take advantage of this time metadata in our collections of vector embeddings to enrich the quality and applicability of search results by retrieving vectors that are not just semantically similar but also pertinent to a specific time frame.\n </p>\n <p>\n  Here are some examples where time-based retrieval of vectors can improve your LlamaIndex applications:\n </p>\n <ul>\n  <li>\n   <strong>\n    Finding recent embeddings:\n   </strong>\n   Finding the most recent embeddings that are semantically similar to a query vector. For example, finding the most recent news, documents, or social media posts related to elections.\n  </li>\n  <li>\n   <strong>\n    Search within a time-range:\n   </strong>\n   Constraining similarity search to only vectors within a relevant time range. For example, asking time-based questions about a knowledge base (\u201cWhat new features were added between January and March 2023?\u201d).\n  </li>\n  <li>\n   <strong>\n    Chat history:\n   </strong>\n   Storing and retrieving LLM response history. For example, chatbot chat history.\n  </li>\n </ul>\n <p>\n  Let\u2019s take a look at an example of performing time-based searches on a\n  <a href=\"https://chat.openai.com/share/0612295b-a408-4e02-9ac2-3dc231fa89d1\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   git log dataset\n  </a>\n  . In a git log, each entry has a timestamp, an author, and some information about the commit.\n </p>\n <p>\n  To illustrate how to use TimescaleVector\u2019s time-based vector search functionality, we\u2019ll ask questions about the git log history for TimescaleDB. Each git commit entry has a timestamp associated with it, as well as a message and other metadata (e.g., author).\n </p>\n <p>\n  We\u2019ll illustrate how to create nodes with a time-based UUID and how to run similarity searches with time range filters using the Timescale Vector vector store..\n </p>\n <h1>\n  Create nodes from each commit in the gitlog\n </h1>\n <p>\n  First, we load the git log entries from the\n  <a href=\"https://s3.amazonaws.com/assets.timescale.com/ai/commit_history.csv\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   demo CSV file\n  </a>\n  using Pandas:\n </p>\n <pre><span class=\"qi ow gt qf b bf qj qk l ql qm\" id=\"a7b2\"><span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\">as</span> pd\n<span class=\"hljs-keyword\">from</span> pathlib <span class=\"hljs-keyword\">import</span> Path\n\n\n<span class=\"hljs-comment\"># Read the CSV file into a DataFrame</span>\nfile_path = Path(<span class=\"hljs-string\">\"../data/csv/commit_history.csv\"</span>)\ndf = pd.read_csv(file_path)</span></pre>\n <p>\n  Next, we\u2019ll create nodes of type `TextNode` for each commit in our git log dataset, extracting the relevant information and assigning it to the node\u2019s text and metadata, respectively.\n </p>\n <pre><span class=\"qi ow gt qf b bf qj qk l ql qm\" id=\"e107\"><span class=\"hljs-keyword\">from</span> llama_index.schema <span class=\"hljs-keyword\">import</span> TextNode, NodeRelationship, RelatedNodeInfo\n<span class=\"hljs-comment\"># Create a Node object from a single row of data</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">create_node</span>(<span class=\"hljs-params\">row</span>):\n   record = row.to_dict()\n   record_name = split_name(record[<span class=\"hljs-string\">\"author\"</span>])\n   record_content = <span class=\"hljs-built_in\">str</span>(record[<span class=\"hljs-string\">\"date\"</span>]) + <span class=\"hljs-string\">\" \"</span> + record_name + <span class=\"hljs-string\">\" \"</span> + <span class=\"hljs-built_in\">str</span>(record[<span class=\"hljs-string\">\"change summary\"</span>]) + <span class=\"hljs-string\">\" \"</span> + <span class=\"hljs-built_in\">str</span>(record[<span class=\"hljs-string\">\"change details\"</span>])\n   node = TextNode(\n       id_=create_uuid(record[<span class=\"hljs-string\">\"date\"</span>]),\n       text= record_content,\n       metadata={\n           <span class=\"hljs-string\">'commit'</span>: record[<span class=\"hljs-string\">\"commit\"</span>],\n           <span class=\"hljs-string\">'author'</span>: record_name,\n           <span class=\"hljs-string\">'date'</span>: create_date(record[<span class=\"hljs-string\">\"date\"</span>]),\n       }\n   )\n   <span class=\"hljs-keyword\">return</span> node\n\nnodes = [create_node(row) <span class=\"hljs-keyword\">for</span> _, row <span class=\"hljs-keyword\">in</span> df.iterrows()]</span></pre>\n <p>\n  <strong>\n   Note:\n  </strong>\n  The code above references two helper functions to get things in the right format (`split_name()` and `create_date()`), which we\u2019ve omitted for brevity. The full code is included in the tutorial linked in the Resources section at the end of this post.\n </p>\n <h1>\n  Create UUIDs for each node based on the date of each git commit\n </h1>\n <p>\n  We will take a closer look at a helper function we use to create each node\u2019s\n  <code class=\"cw qn qo qp qf b\">\n   id_\n  </code>\n  . For time-based search in LlamaIndex, Timescale Vector uses the \u2018datetime\u2019 portion of a UUID v1 to place vectors in the correct time partition.\n  <a href=\"https://github.com/timescale/python-vector\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Timescale Vector\u2019s Python client library\n  </a>\n  provides a simple-to-use function named `uuid_from_time` to create a UUID v1 from a Python DateTime object, which we\u2019ll then use as our `ids` for the TextNodes.\n </p>\n <pre><span class=\"qi ow gt qf b bf qj qk l ql qm\" id=\"19d5\"><span class=\"hljs-keyword\">from</span> timescale_vector <span class=\"hljs-keyword\">import</span> client\n<span class=\"hljs-comment\"># Function to take in a date string in the past and return a uuid v1</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">create_uuid</span>(<span class=\"hljs-params\">date_string: <span class=\"hljs-built_in\">str</span></span>):\n   <span class=\"hljs-keyword\">if</span> date_string <span class=\"hljs-keyword\">is</span> <span class=\"hljs-literal\">None</span>:\n       <span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">None</span>\n   time_format = <span class=\"hljs-string\">'%a %b %d %H:%M:%S %Y %z'</span>\n   datetime_obj = datetime.strptime(date_string, time_format)\n   uuid = client.uuid_from_time(datetime_obj)\n   <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">str</span>(uuid)</span></pre>\n <p>\n  Since we are dealing with timestamps in the past, we take advantage of the `uuid_from_time` function to help generate the correct UUIDs for each node. If you want the current date and time associated with your Nodes (or Documents) for time-based search, you can skip this step. A UUID associated with the current date and time will be automatically generated as the nodes are added to the table in Timescale Vector by default.\n </p>\n <p>\n  Let\u2019s take a look at the contents of a node:\n </p>\n <pre><span class=\"qi ow gt qf b bf qj qk l ql qm\" id=\"7381\">print(nodes[0].get_content(metadata_mode=\"all\"))</span></pre>\n <pre><span class=\"qi ow gt qf b bf qj qk l ql qm\" id=\"7200\">commit: 44e41c12ab25e36c202f58e068ced262eadc8d16\nauthor: Lakshmi Narayanan Sreethar\ndate: 2023-09-5 21:03:21+0850\n\nTue Sep 5 21:03:21 2023 +0530 Lakshmi Narayanan Sreethar Fix segfault in set_integer_now_func When an invalid function oid is passed to set_integer_now_func, it finds out that the function oid is invalid but before throwing the error, it calls ReleaseSysCache on an invalid tuple causing a segfault. Fixed that by removing the invalid call to ReleaseSysCache.  Fixes #6037</span></pre>\n <h1>\n  Create vector embeddings for the text of each node\n </h1>\n <p>\n  Next, we\u2019ll create vector embeddings of the content of each node so that we can perform similarity searches on the text associated with each node. We\u2019ll use the `OpenAIEmbedding` model to create the embeddings.\n </p>\n <pre><span class=\"qi ow gt qf b bf qj qk l ql qm\" id=\"0a73\"><span class=\"hljs-comment\"># Create embeddings for nodes</span>\n<span class=\"hljs-keyword\">from</span> llama_index.embeddings <span class=\"hljs-keyword\">import</span> OpenAIEmbedding\nembedding_model = OpenAIEmbedding()\n\n<span class=\"hljs-keyword\">for</span> node <span class=\"hljs-keyword\">in</span> nodes:\n   node_embedding = embedding_model.get_text_embedding(\n       node.get_content(metadata_mode=<span class=\"hljs-string\">\"all\"</span>)\n   )\n   node.embedding = node_embedding</span></pre>\n <h1>\n  Load nodes into Timescale Vector vector store\n </h1>\n <p>\n  Next, we\u2019ll create a `TimescaleVectorStore` instance and add the nodes we created to it.\n </p>\n <pre><span class=\"qi ow gt qf b bf qj qk l ql qm\" id=\"550c\"># Create a timescale vector store and add the newly created nodes to it\nts_vector_store = TimescaleVectorStore.from_params(\n   service_url=TIMESCALE_SERVICE_URL,\n   table_name=\"li_commit_history\",\n   time_partition_interval= timedelta(days=7),\n)\nts_vector_store.add(nodes)</span></pre>\n <p>\n  To take advantage of Timescale Vector\u2019s efficient time-based search, we need to specify the `time_partition_interval` argument when instantiating a Timescale Vector vector store. This argument represents the length of each interval for partitioning the data by time. Each partition will consist of data that falls within the specified length of time.\n </p>\n <p>\n  In the example above, we use seven days for simplicity, but you can pick whatever value makes sense for the queries used by your application \u2014 for example, if you query recent vectors frequently, you might want to use a smaller time delta like one day, or if you query vectors over a decade-long time period, then you might want to use a larger time delta like six months or one year. As a rule of thumb, common queries should touch only a couple of partitions and at the same time your full dataset should fit within a 1000 partitions, but don\u2019t stress too much \u2014 the system is not very sensitive to this value.\n </p>\n <h1>\n  Similarity search with time filters\n </h1>\n <p>\n  Now that we\u2019ve loaded our nodes that contain vector embeddings data and metadata into a Timescale Vector vector store, and enabled automatic time-based partitioning on the table our vectors and metadata are stored in, we can query our vector store with time-based filters as follows:\n </p>\n <pre><span class=\"qi ow gt qf b bf qj qk l ql qm\" id=\"62ab\"># Query the <span class=\"hljs-built_in\">vector</span> database\nvector_store_query = VectorStoreQuery(query_embedding = query_embedding, similarity_top_k=<span class=\"hljs-number\">5</span>)\n\n# Time filter variables <span class=\"hljs-keyword\">for</span> query\nstart_dt = datetime(<span class=\"hljs-number\">2023</span>, <span class=\"hljs-number\">8</span>, <span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">22</span>, <span class=\"hljs-number\">10</span>, <span class=\"hljs-number\">35</span>) # Start date = <span class=\"hljs-number\">1</span> August <span class=\"hljs-number\">2023</span>, <span class=\"hljs-number\">22</span>:<span class=\"hljs-number\">10</span>:<span class=\"hljs-number\">35</span>\nend_dt = datetime(<span class=\"hljs-number\">2023</span>, <span class=\"hljs-number\">8</span>, <span class=\"hljs-number\">30</span>, <span class=\"hljs-number\">22</span>, <span class=\"hljs-number\">10</span>, <span class=\"hljs-number\">35</span>) # End date = <span class=\"hljs-number\">30</span> August <span class=\"hljs-number\">2023</span>, <span class=\"hljs-number\">22</span>:<span class=\"hljs-number\">10</span>:<span class=\"hljs-number\">35</span>\n\n<span class=\"hljs-meta\"># return most similar vectors to query between start date and end date date range</span>\n<span class=\"hljs-meta\"># returns a VectorStoreQueryResult object</span>\nquery_result = ts_vector_store.query(vector_store_query, start_date = start_dt, end_date = end_dt)</span></pre>\n <p>\n  Let\u2019s take a look at the date and contents of the nodes returned by our query:\n </p>\n <pre><span class=\"qi ow gt qf b bf qj qk l ql qm\" id=\"666f\"><span class=\"hljs-comment\"># for each node in the query result, print the node metadata date</span>\n<span class=\"hljs-keyword\">for</span> node <span class=\"hljs-keyword\">in</span> query_result.nodes:\n   <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"-\"</span> * <span class=\"hljs-number\">80</span>)\n   <span class=\"hljs-built_in\">print</span>(node.metadata[<span class=\"hljs-string\">\"date\"</span>])\n   <span class=\"hljs-built_in\">print</span>(node.get_content(metadata_mode=<span class=\"hljs-string\">\"all\"</span>))</span></pre>\n <pre><span class=\"qi ow gt qf b bf qj qk l ql qm\" id=\"c98b\">--------------------------------------------------------------------------------\n2023-08-3 14:30:23+0500\ncommit:  7aeed663b9c0f337b530fd6cad47704a51a9b2ec\nauthor: Dmitry Simonenko\ndate: 2023-08-3 14:30:23+0500\n\nThu Aug 3 14:30:23 2023 +0300 Dmitry Simonenko Feature flags for TimescaleDB features This PR adds..\n--------------------------------------------------------------------------------\n2023-08-29 18:13:24+0320\ncommit:  e4facda540286b0affba47ccc63959fefe2a7b26\nauthor: Sven Klemm\ndate: 2023-08-29 18:13:24+0320\n\nTue Aug 29 18:13:24 2023 +0200 Sven Klemm Add compatibility layer for _timescaledb_internal functions With timescaledb 2.12 all the functions present in _timescaledb_internal were\u2026\n--------------------------------------------------------------------------------\n2023-08-22 12:01:19+0320\ncommit:  cf04496e4b4237440274eb25e4e02472fc4e06fc\nauthor: Sven Klemm\ndate: 2023-08-22 12:01:19+0320\n\nTue Aug 22 12:01:19 2023 +0200 Sven Klemm Move utility functions to _timescaledb_functions schema To increase schema security we do not want to mix\u2026\n--------------------------------------------------------------------------------\n2023-08-29 10:49:47+0320\ncommit:  a9751ccd5eb030026d7b975d22753f5964972389\nauthor: Sven Klemm\ndate: 2023-08-29 10:49:47+0320\n\nTue Aug 29 10:49:47 2023 +0200 Sven Klemm Move partitioning functions to _timescaledb_functions schema To increase schema security\u2026\n--------------------------------------------------------------------------------\n2023-08-9 15:26:03+0500\ncommit:  44eab9cf9bef34274c88efd37a750eaa74cd8044\nauthor: Konstantina Skovola\ndate: 2023-08-9 15:26:03+0500\n\nWed Aug 9 15:26:03 2023 +0300 Konstantina Skovola Release 2.11.2 This release contains bug fixes since the 2.11.1 release\u2026</span></pre>\n <p>\n  Success! Notice how only vectors with timestamps within the specified start and end date ranges of 1 August, 2023, and 30 August, 2023, are included in the results.\n </p>\n <p>\n  Here\u2019s some intuition for why Timescale Vector\u2019s time-based partitioning speeds up ANN queries with time-based filters.\n </p>\n <p>\n  Timescale Vector partitions the data by time and creates ANN indexes on each partition individually. Then, during search, we perform a three-step process:\n </p>\n <ul>\n  <li>\n   Step 1: filter our partitions that don\u2019t match the time predicate.\n  </li>\n  <li>\n   Step 2: perform the similarity search on all matching partitions.\n  </li>\n  <li>\n   Step 3: combine all the results from each partition in step 2, rerank, and filter out results by time.\n  </li>\n </ul>\n <p>\n  Timescale Vector leverages\n  <a href=\"https://docs.timescale.com/use-timescale/latest/hypertables/about-hypertables/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   TimescaleDB\u2019s hypertables\n  </a>\n  , which automatically partition vectors and associated metadata by a timestamp. This enables efficient querying on vectors by both similarity to a query vector and time, as partitions not in the time window of the query are ignored, making the search a lot more efficient by filtering out whole swaths of data in one go.\n </p>\n <p>\n  When performing a vector similarity search on `TimescaleVectorStore`, rather than specifying the start and end dates for our search, we can also specify a time filter with a provided start date and time delta later:\n </p>\n <pre><span class=\"qi ow gt qf b bf qj qk l ql qm\" id=\"8eba\"># return most similar vectors to query from start date and a time delta later\nquery_result = ts_vector_store.query(vector_store_query, start_date = start_dt, time_delta = td)</span></pre>\n <p>\n  And we can specify a time filter within a provided end_date and time delta earlier. This syntax is very useful for filtering your search results to contain vectors before a certain date cutoff.\n </p>\n <pre><span class=\"qi ow gt qf b bf qj qk l ql qm\" id=\"4769\"># return most similar vectors to query from end date and a time delta earlier\nquery_result = ts_vector_store.query(vector_store_query, end_date = end_dt, time_delta = td)</span></pre>\n <h1>\n  Powering Retrieval Augmented Generation With Time-Based Context Retrieval in LlamaIndex Applications With Timescale Vector\n </h1>\n <p>\n  Let\u2019s put everything together and look at how to use the TimescaleVectorStore to power RAG on the git log dataset we examined above.\n </p>\n <p>\n  To do this, we can use the TimescaleVectorStore as a\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/api_reference/query/query_engines.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   QueryEngine\n  </a>\n  . When creating the query engine, we use TimescaleVector\u2019s time filters to constrain the search to a relevant time range by passing our time filter parameters as `vector_strore_kwargs`.\n </p>\n <pre><span class=\"qi ow gt qf b bf qj qk l ql qm\" id=\"a136\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> VectorStoreIndex\n<span class=\"hljs-keyword\">from</span> llama_index.storage <span class=\"hljs-keyword\">import</span> StorageContext\n\nindex = VectorStoreIndex.from_vector_store(ts_vector_store)\nquery_engine = index.as_query_engine(vector_store_kwargs = ({<span class=\"hljs-string\">\"start_date\"</span>: start_dt, <span class=\"hljs-string\">\"end_date\"</span>:end_dt}))\n\nquery_str = <span class=\"hljs-string\">\"What's new with TimescaleDB functions? When were these changes made and by whom?\"</span>\nresponse = query_engine.query(query_str)\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-built_in\">str</span>(response))</span></pre>\n <p>\n  We asked the LLM a question about our gitlog, namely, \u201cWhat\u2019s new with TimescaleDB functions. When were these changes made and by whom?\u201d\n </p>\n <p>\n  Here\u2019s the response we get, which synthesizes the nodes returned from semantic search with time-based filtering on the Timescale VectorStore:\n </p>\n <pre><span class=\"qi ow gt qf b bf qj qk l ql qm\" id=\"48aa\">TimescaleDB functions have undergone changes recently. These changes include the addition of several GUCs (Global User Configuration) that allow for enabling or disabling major TimescaleDB features. Additionally, a compatibility layer has been added for the \"_timescaledb_internal\" functions, which were moved into the \"_timescaledb_functions\" schema to enhance schema security. These changes were made by Dmitry Simonenko and Sven Klemm. The specific dates of these changes are August 3, 2023, and August 29, 2023, respectively.</span></pre>\n <p>\n  This is a simple example of a powerful concept \u2014 using time-based context retrieval in your RAG applications can help provide more relevant answers to your users. This time-based context retrieval can be helpful to any dataset with a natural language and time component. Timescale Vector uniquely enables this thanks to its efficient time-based similarity search capabilities, and taking advantage of it in your LlamaIndex application is easy thanks to the Timescale Vector integration.\n </p>\n <h1>\n  Resources and next steps\n </h1>\n <p>\n  Now that you\u2019ve learned how Timescale Vector can help you power better AI applications with PostgreSQL, it\u2019s your turn to dive in. Take the next step in your learning journey by following one of the tutorials or reading one of the blog posts in the resource set below:\n </p>\n <ul>\n  <li>\n   <a href=\"https://gpt-index.readthedocs.io/en/stable/examples/vector_stores/Timescalevector.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     Up and Running Tutorial\n    </strong>\n   </a>\n   <strong>\n    :\n   </strong>\n   learn how to use Timescale Vector in LlamaIndex using a real-world dataset. You\u2019ll learn how to use Timescale Vector as a Vectorstore, Retriever, and QueryEngine and perform time-based similarity search on vectors.\n  </li>\n  <li>\n   <a href=\"https://www.timescale.com/blog/how-we-made-postgresql-the-best-vector-database/?utm_campaign=vectorlaunch&amp;utm_source=llamaindex&amp;utm_medium=referral\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     Timescale Vector explainer\n    </strong>\n   </a>\n   : learn more about the internals of Timescale Vector.\n  </li>\n  <li>\n   <a href=\"https://www.timescale.com/ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    <strong>\n     Timescale Vector website\n    </strong>\n   </a>\n   <strong>\n    :\n   </strong>\n   learn more about Timescale Vector and Timescale\u2019s AI Launch Week.\n  </li>\n </ul>\n <p>\n  <strong>\n   \ud83c\udf89 And a reminder:\n  </strong>\n  <a href=\"https://console.cloud.timescale.com/signup?utm_campaign=vectorlaunch&amp;utm_source=llamaindex&amp;utm_medium=referral\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    LlamaIndex Users get Timescale Vector free for 90 days\n   </strong>\n  </a>\n </p>\n <p>\n  We\u2019re giving LlamaIndex users an extended 90-day trial of Timescale Vector. This makes it easy to test and develop your applications on Timescale Vector, as you won\u2019t be charged for any cloud PostgreSQL databases you spin up during your trial period.\n  <a href=\"https://console.cloud.timescale.com/signup?utm_campaign=vectorlaunch&amp;utm_source=llamaindex&amp;utm_medium=referral\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Try Timescale Vector for free today\n  </a>\n  .\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 33983, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "886b75ed-9414-4389-9e46-fed3f2f5e0ac": {"__data__": {"id_": "886b75ed-9414-4389-9e46-fed3f2f5e0ac", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.html", "file_name": "tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.html", "file_type": "text/html", "file_size": 22826, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.html", "file_name": "tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.html", "file_type": "text/html", "file_size": 22826, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "e75e88705fa4b67503227f352849fa71c5269748facc18fe9c9ef825c1aa23d4", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  <em class=\"ol\">\n   In this technical walkthrough, we\u2019ll highlight the functionality of Tonic Validate and its integration with LlamaIndex. Sign up for a free account\n  </em>\n  <a href=\"https://www.tonic.ai/validate\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <em class=\"ol\">\n    here\n   </em>\n  </a>\n  <em class=\"ol\">\n   before you start.\n  </em>\n </p>\n <h1>\n  Introduction\n </h1>\n <p>\n  As enterprise adoption of generative AI technologies continues, companies are turning to Retrieval Augmented Generation (RAG) systems to extend the application of large-language models (LLMs) to their private data (e.g., a chatbot that can answer questions based on internal technical documentation). Traditionally in software engineering, companies have placed a high emphasis on implementing continuous integration tests to ensure systems remain performant when updates are made. More recently, these same principles have been applied to machine learning models in production.\n </p>\n <p>\n  However, as a young technology, RAG currently lacks best practices for integration tests to ensure breaking changes aren\u2019t introduced to the production system. In this article, we will demonstrate how you can use Tonic Validate\u2019s RAG performance monitoring capabilities, LlamaIndex, and GitHub Actions to create novel integration tests that alert you to changes in RAG system performance. To make things easy, Tonic Validate is available natively within LlamaIndex\u2019s core library \u2014 you can read more about that\n  <a href=\"https://www.tonic.ai/blog/tonic-ai-and-llamaindex-join-forces-to-help-developers-build-more-performant-rag-systems\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   here\n  </a>\n  .\n </p>\n <h1>\n  What is Tonic Validate?\n </h1>\n <p>\n  Tonic Validate is a RAG benchmarking and evaluation platform that monitors performance of RAG systems in production. It provides comprehensive metrics for measuring the performance of each component in your RAG system, visualizations for comparing performance across time as the system changes, and workflows for creating benchmark question-answer sets and reviewing LLM responses. Tonic Validate shines a light on how your RAG system is truly performing, enabling continuous performance monitoring of your production RAG systems. You can\n  <a href=\"https://www.tonic.ai/validate\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   learn more\n  </a>\n  and\n  <a href=\"https://validate.tonic.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   sign up for a free account\n  </a>\n  .\n </p>\n <h1>\n  Setting up LlamaIndex\n </h1>\n <p>\n  To get started, let\u2019s create an example RAG system for us to test. In this case, LlamaIndex provides a tool called\n  <code class=\"cw pq pr ps pt b\">\n   create-llama\n  </code>\n  which can generate a full-stack RAG application for us. To install it, we need to make sure we have Node.JS installed and run the following command:\n </p>\n <pre><span class=\"qc oo gt pt b bf qd qe l qf qg\" id=\"3b21\">npx create-llama@latest</span></pre>\n <p>\n  This command will take you through a series of prompts. Here are the options to select for each prompt:\n </p>\n <pre><span class=\"qc oo gt pt b bf qd qe l qf qg\" id=\"9596\">What is your project named? \u00bb llama-validate-demo\nWhich template would you like to use? \u00bb Chat without streaming\nWhich framework would you like to use? \u00bb FastAPI (Python)\nWould you like to install dependencies automatically? \u00bb No\nWhich model would you like to use? \u00bb gpt-4\u20131106-preview\nWhich data source would you like to use? \u00bb Use an example PDF\nWould you like to use a vector database? \u00bb No, just store the data in the file system</span></pre>\n <p>\n  Once these options are selected, your project should be created in a folder called\n  <code class=\"cw pq pr ps pt b\">\n   llama-validate-demo\n  </code>\n  . For this demo, we are going to replace the example PDF\n  <code class=\"cw pq pr ps pt b\">\n   create-llama\n  </code>\n  provides with our own larger dataset. The dataset consists of a collection of essays from Paul Graham\u2019s blog. This should more closely replicate a real world scenario where a company runs RAG on a larger internal dataset. To add the essays, download them from\n  <a href=\"https://github.com/TonicAI/llama-validate-demo/blob/main/data.zip\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   our Github\n  </a>\n  and unzip them inside the root folder of your created project. Make sure the unzipped folder is named\n  <code class=\"cw pq pr ps pt b\">\n   data\n  </code>\n  . Be sure to delete any existing files in the folder before copying the new dataset.\n </p>\n <p>\n  After you have the essays in the right directory, you can set up your OpenAI API key by setting it as an environment variable called\n  <code class=\"cw pq pr ps pt b\">\n   OPENAI_API_KEY\n  </code>\n  . You can do this either via setting the environment variable system wide or by creating a\n  <code class=\"cw pq pr ps pt b\">\n   .env\n  </code>\n  file in the root folder of your\n  <code class=\"cw pq pr ps pt b\">\n   create-llama\n  </code>\n  project. Then you can run the following commands in the root folder for your\n  <code class=\"cw pq pr ps pt b\">\n   create-llama\n  </code>\n  project:\n </p>\n <pre><span class=\"qc oo gt pt b bf qd qe l qf qg\" id=\"c038\">poetry install\npoetry shell\npython app/engine/generate.py</span></pre>\n <p>\n  This will install the dependencies and generate the RAG embeddings for the Paul Graham essays. After this, you can run the chatbot with:\n </p>\n <pre><span class=\"qc oo gt pt b bf qd qe l qf qg\" id=\"c0ce\">python main.py</span></pre>\n <p>\n  To test the chatbot, you can send a request via curl:\n </p>\n <pre><span class=\"qc oo gt pt b bf qd qe l qf qg\" id=\"afef\">curl - location <span class=\"hljs-string\">'localhost:8000/api/chat'</span> \\\n - header <span class=\"hljs-string\">'Content-Type: application/json'</span> \\\n - data <span class=\"hljs-string\">'{ \"messages\": [{ \"role\": \"user\", \"content\": \"In the early days, how were the Airbnb founders financing their startup?\" }] }'</span></span></pre>\n <p>\n  LlamaIndex will then return a response:\n </p>\n <pre><span class=\"qc oo gt pt b bf qd qe l qf qg\" id=\"4c5c\">{\n    <span class=\"hljs-string\">\"result\"</span>: {\n        <span class=\"hljs-string\">\"role\"</span>: <span class=\"hljs-string\">\"assistant\"</span>,\n        <span class=\"hljs-string\">\"content\"</span>: <span class=\"hljs-string\">\"In the early days, the Airbnb founders financed their startup by creating and selling themed breakfast cereals. They created limited-edition cereal boxes, such as \\\"Obama O's\\\" and \\\"Cap'n McCain's,\\\" during the 2008 U.S. presidential election, which became a collectible and helped them raise funds for their company. This creative approach to funding allowed them to sustain the business in its initial phase before securing more traditional forms of investment.\"</span>\n    }\n}</span></pre>\n <p>\n  Finally, in\n  <code class=\"cw pq pr ps pt b\">\n   llama-validate-demo/app/api/routers/chat.py\n  </code>\n  we want to replace the\n  <code class=\"cw pq pr ps pt b\">\n   return _Result\n  </code>\n  line at the end of the chat function with the following.\n </p>\n <pre><span class=\"qc oo gt pt b bf qd qe l qf qg\" id=\"39ce\"><span class=\"hljs-keyword\">return</span> _Result(\n    result=_Message(\n        role=<span class=\"hljs-title class_\">MessageRole</span>.<span class=\"hljs-variable constant_\">ASSISTANT</span>,\n        content=response.response,\n        context=[x.text <span class=\"hljs-keyword\">for</span> x <span class=\"hljs-keyword\">in</span> response.source_nodes]\n    )\n)</span></pre>\n <p>\n  This allows the LlamaIndex API to return the RAG context that was used to answer the question asked. Now, we can move on to setting up Tonic Validate!\n </p>\n <h1>\n  Setting up Tonic Validate\n </h1>\n <p>\n  To set up Tonic Validate, first install it via poetry:\n </p>\n <pre><span class=\"qc oo gt pt b bf qd qe l qf qg\" id=\"9a05\">poetry add tonic-validate</span></pre>\n <p>\n  Now, we can create our tests for Tonic Validate. To get started, create a file inside\n  <code class=\"cw pq pr ps pt b\">\n   llama-validate-demo/tests\n  </code>\n  called\n  <code class=\"cw pq pr ps pt b\">\n   validate_test.py\n  </code>\n  . We will also need to create a list of test questions and answers which you can find\n  <a href=\"https://github.com/TonicAI/llama-validate-demo/blob/main/tests/qa_pairs.json\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   here\n  </a>\n  . Alternatively, you can also use the Tonic Validate UI to create the test set and call it via the SDK \u2014 we\u2019ll be adding a feature to help generate these benchmarks using synthetic data to make this process even easier. Download the\n  <code class=\"cw pq pr ps pt b\">\n   qa_pairs.json\n  </code>\n  file from the link and paste it into\n  <code class=\"cw pq pr ps pt b\">\n   llama-validate-demo/tests\n  </code>\n  . Once we have both of these files, we can add the following code into\n  <code class=\"cw pq pr ps pt b\">\n   validate_test.py\n  </code>\n  .\n </p>\n <pre><span class=\"qc oo gt pt b bf qd qe l qf qg\" id=\"2bc6\"><span class=\"hljs-keyword\">import</span> json\n<span class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\">from</span> tonic_validate <span class=\"hljs-keyword\">import</span> ValidateApi\n<span class=\"hljs-keyword\">from</span> tonic_validate.metrics <span class=\"hljs-keyword\">import</span> AnswerSimilarityMetric, RetrievalPrecisionMetric, AugmentationPrecisionMetric, AnswerConsistencyMetric\n<span class=\"hljs-keyword\">from</span> llama_index.evaluation <span class=\"hljs-keyword\">import</span> TonicValidateEvaluator\n<span class=\"hljs-keyword\">import</span> requests\n\n<span class=\"hljs-keyword\">from</span> dotenv <span class=\"hljs-keyword\">import</span> load_dotenv\n\nload_dotenv()\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">get_llm_response</span>(<span class=\"hljs-params\">prompt</span>):\n    url = <span class=\"hljs-string\">\"http://localhost:8000/api/chat\"</span>\n\n    payload = json.dumps({\n        <span class=\"hljs-string\">\"messages\"</span>: [\n            {\n                <span class=\"hljs-string\">\"role\"</span>: <span class=\"hljs-string\">\"user\"</span>,\n                <span class=\"hljs-string\">\"content\"</span>: prompt\n            }\n        ]\n    })\n    headers = { <span class=\"hljs-string\">'Content-Type'</span>: <span class=\"hljs-string\">'application/json'</span> }\n    response = requests.request(<span class=\"hljs-string\">\"POST\"</span>, url, headers=headers, data=payload).json()\n    result = response[<span class=\"hljs-string\">'result'</span>]\n    <span class=\"hljs-keyword\">return</span> result[<span class=\"hljs-string\">'content'</span>], result[<span class=\"hljs-string\">'context'</span>]</span></pre>\n <p>\n  This code sets up the dependency imports and also specifies a\n  <code class=\"cw pq pr ps pt b\">\n   get_llm_response\n  </code>\n  function which sends a request to the LlamaIndex API server we set up earlier to get a response. Now, let\u2019s create a function that gets the list of questions to ask LlamaIndex for our testing.\n </p>\n <pre><span class=\"qc oo gt pt b bf qd qe l qf qg\" id=\"d879\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">get_q_and_a</span>():\n    <span class=\"hljs-comment\"># Load qa_pairs.json</span>\n    qa_pairs = json.load(<span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">'./tests/qa_pairs.json'</span>))\n    <span class=\"hljs-keyword\">return</span> ([x[<span class=\"hljs-string\">'question'</span>] <span class=\"hljs-keyword\">for</span> x <span class=\"hljs-keyword\">in</span> qa_pairs], [x[<span class=\"hljs-string\">'answer'</span>] <span class=\"hljs-keyword\">for</span> x <span class=\"hljs-keyword\">in</span> qa_pairs])</span></pre>\n <p>\n  This function gets the question-answer pairs from our json file. The questions are what we will ask the RAG system and the answers are the correct answers for those questions. For instance, if the question was \u201cWhat is the capital of France?\u201d then the answer would be \u201cParis\u201d.\n </p>\n <p>\n  Next, we can add the code that queries LlamaIndex:\n </p>\n <pre><span class=\"qc oo gt pt b bf qd qe l qf qg\" id=\"100e\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">get_responses</span>(<span class=\"hljs-params\">questions</span>):\n    llm_answers = []\n    context_lists = []\n    <span class=\"hljs-keyword\">for</span> item <span class=\"hljs-keyword\">in</span> questions:\n        llm_answer, llm_context_list = get_llm_response(item)\n        llm_answers.append(llm_answer)\n        context_lists.append(llm_context_list)\n    <span class=\"hljs-keyword\">return</span> (llm_answers, context_lists)</span></pre>\n <p>\n  This code iterates over the questions, queries LlamaIndex, and logs each response into an array. We have two arrays. One is the actual answer from LlamaIndex. The other is a list of the snippets of text (called the context list) that LlamaIndex provided to help the LLM answer the question.\n </p>\n <p>\n  Now we have a list of LLM responses generated from a list of test questions. Let\u2019s score them:\n </p>\n <pre><span class=\"qc oo gt pt b bf qd qe l qf qg\" id=\"ff5c\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">score_run</span>(<span class=\"hljs-params\">questions, context_lists, reference_answers, llm_answers</span>):\n    metrics = [\n        AnswerSimilarityMetric(),\n        RetrievalPrecisionMetric(),\n        AugmentationPrecisionMetric(),\n        AnswerConsistencyMetric()\n    ]\n    scorer = TonicValidateEvaluator(metrics, model_evaluator=<span class=\"hljs-string\">\"gpt-4-1106-preview\"</span>)\n    run = scorer.evaluate_run(\n        questions, llm_answers, context_lists, reference_answers\n    )\n    <span class=\"hljs-keyword\">return</span> run, metrics</span></pre>\n <p>\n  We first need to define the metrics in Tonic Validate that we want to use. You can find a list of available metrics and their definitions\n  <a href=\"https://github.com/TonicAI/tonic_validate?tab=readme-ov-file#tonic-validate-metrics\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   here\n  </a>\n  . After we create the metrics, we can take advantage of Tonic Validate\u2019s integration with LlamaIndex. Since Tonic Validate is built into LlamaIndex\u2019s evaluation framework as an evaluator, all we need to do is create a\n  <code class=\"cw pq pr ps pt b\">\n   TonicValidateEvaluator\n  </code>\n  , which scores the LlamaIndex responses across the chosen metrics. Then we return the results along with the metrics.\n </p>\n <p>\n  Finally, we can create our test function for pytest which evaluates LlamaIndex.\n </p>\n <pre><span class=\"qc oo gt pt b bf qd qe l qf qg\" id=\"9875\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">test_llama_index</span>():\n    questions, reference_answers = get_q_and_a()\n    llm_answers, context_lists = get_responses(questions)\n    run, metrics = score_run(questions, context_lists, reference_answers, llm_answers)\n    <span class=\"hljs-comment\"># Upload results to web ui</span>\n    validate_api = ValidateApi()\n    <span class=\"hljs-comment\"># Get project id from env</span>\n    project_id = os.getenv(<span class=\"hljs-string\">\"PROJECT_ID\"</span>)\n    validate_api.upload_run(project_id, run)</span></pre>\n <p>\n  This runs all the code we\u2019ve written to get the scores and then sends them to Tonic Validate\u2019s API to visualize in the UI. In order to send the metrics for each run to the UI, you need to sign up for a free account, which you can do\n  <a href=\"https://validate.tonic.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   here\n  </a>\n  . I highly recommend utilizing the UI to make visualizing and monitoring performance changes a breeze. Once you sign up, you will be taken through a short onboarding process where you create an API key and a project. The API key should be stored in an environment variable called\n  <code class=\"cw pq pr ps pt b\">\n   TONIC_VALIDATE_API_KEY\n  </code>\n  and the project ID in an environment variable called\n  <code class=\"cw pq pr ps pt b\">\n   PROJECT_ID\n  </code>\n  .\n </p>\n <p>\n  Once you have set up your account and configured your environment variables, you can run the test via the following commands:\n </p>\n <pre><span class=\"qc oo gt pt b bf qd qe l qf qg\" id=\"0add\">poetry shell\npytest</span></pre>\n <p>\n  You can also make the test fail if the metrics score too low. This would be a pertinent step to add in if you want to avoid introducing breaking changes to a production RAG system; for example, if you update the model version and the answer similarity score suddenly drop below a certain threshold, you could have the test fail and issue a warning to debug the issue.\n </p>\n <pre><span class=\"qc oo gt pt b bf qd qe l qf qg\" id=\"3d49\"><span class=\"hljs-comment\"># Check none of the metrics scored too low    </span>\n<span class=\"hljs-keyword\">for</span> metric <span class=\"hljs-keyword\">in</span> metrics:\n    <span class=\"hljs-keyword\">if</span> metric.name == AnswerSimilarityMetric.name:\n        <span class=\"hljs-keyword\">assert</span> run.overall_scores[metric.name] &amp;gt;= <span class=\"hljs-number\">3.5</span>\n    <span class=\"hljs-keyword\">else</span>:\n        <span class=\"hljs-keyword\">assert</span> run.overall_scores[metric.name] &amp;gt;= <span class=\"hljs-number\">0.7</span></span></pre>\n <h1>\n  Setting up GitHub Actions\n </h1>\n <p>\n  With LlamaIndex and Tonic Validate configured, we have the ability to connect data to an LLM and measure the accuracy of LLM responses. You can push this setup into production and have a functional chatbot. As is common in modern software development practices, you will likely continue to fix bugs, make improvements, and add new data or features to your RAG system. Before pushing to production, QA testing is in place to catch any changes to your code that may introduce unintended effects. For example, adding a new dataset or updating an LLM to a new version could lead to changes in the quality of responses. One approach, the one that we recommend, for adding QA testing for your RAG system is to use GitHub Actions to establish an integration test using Tonic Validate that checks the LLM response quality of your RAG system, allowing you to catch and rectify any performance degradation before it is pushed into production.\n </p>\n <p>\n  To set up Tonic Validate to run in GitHub Actions, we can create a folder\n  <code class=\"cw pq pr ps pt b\">\n   llama-validate-demo/.github/workflows\n  </code>\n  with a file called\n  <code class=\"cw pq pr ps pt b\">\n   python-app.yml\n  </code>\n  . In this file, we will include the following code configuration that defines the integration test workflow:\n </p>\n <pre><span class=\"qc oo gt pt b bf qd qe l qf qg\" id=\"6437\"># This workflow will install Python dependencies and run tests with LlamaIndex\n\nname: Python application\n\non:\n  push:\n    branches: [ <span class=\"hljs-string\">\"main\"</span> ]\n  pull_request:\n    branches: [ <span class=\"hljs-string\">\"main\"</span> ]\n\npermissions:\n  contents: read\n\njobs:\n  build:\n\n    runs-on: ubuntu-latest\n    environment: Actions\n\n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python <span class=\"hljs-number\">3.11</span>\n      uses: actions/setup-python@v3\n      with:\n        python-version: <span class=\"hljs-string\">\"3.11\"</span>\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install poetry\n        poetry config virtualenvs.create <span class=\"hljs-literal\">false</span>\n        poetry install --no-root --no-dev --no-directory\n    - name: Set PYTHONPATH\n      run: echo <span class=\"hljs-string\">\"PYTHONPATH=$GITHUB_WORKSPACE\"</span> &amp;gt;&amp;gt; $GITHUB_ENV\n    - name: Set up vector index\n      env:\n        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n      run: |\n        python app/engine/generate.py\n    - name: Start up test server\n      env:\n        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        MODEL: gpt<span class=\"hljs-number\">-4</span><span class=\"hljs-number\">-1106</span>-preview\n      run: |\n        python main.py &amp;amp;\n        sleep <span class=\"hljs-number\">10</span>\n    - name: Test with pytest\n      env:\n        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        TONIC_VALIDATE_API_KEY: ${{ secrets.TONIC_VALIDATE_API_KEY }}\n        PROJECT_ID: ${{ secrets.PROJECT_ID }}\n      run: |\n        pytest</span></pre>\n <p>\n  This configures GitHub to run the tests defined with Tonic Validate upon every commit. The GitHub Actions configuration downloads the repo, sets up the dependencies, generates the embeddings, and then starts up the test server and runs the test.\n </p>\n <p>\n  After this file is set up, we just need to set our secrets in GitHub. In GitHub, go to\n  <code class=\"cw pq pr ps pt b\">\n   Settings &gt; Secrets and variables &gt; Actions\n  </code>\n  for your repo and create a secret called\n  <code class=\"cw pq pr ps pt b\">\n   OPENAI_API_KEY\n  </code>\n  ,\n  <code class=\"cw pq pr ps pt b\">\n   TONIC_VALIDATE_API_KEY\n  </code>\n  , and\n  <code class=\"cw pq pr ps pt b\">\n   PROJECT_ID\n  </code>\n  . These values will all be the same as the values you set earlier. Now your GitHub actions set up is complete and you can proactively monitor changes to your RAG system during development and before going into production.\n </p>\n <p>\n  Try pushing some commits to it and watch it run! To view the results, go to\n  <a href=\"https://validate.tonic.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Tonic Validate\u2019s web app\n  </a>\n  and navigate to your project. You should see a view like this that shows recent metrics and their evolution over time:\n </p>\n <p>\n  Now you and your team can track your RAG system\u2019s performance over time to make sure there aren\u2019t any dips in performance! Thank you for reading and make sure to check out Tonic Validate!\n </p>\n <p>\n  <em class=\"ol\">\n   For more information on Tonic Validate, visit our\n  </em>\n  <a href=\"https://www.tonic.ai/validate\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <em class=\"ol\">\n    website\n   </em>\n  </a>\n  <em class=\"ol\">\n   and sign up for a\n  </em>\n  <a href=\"https://validate.tonic.ai/signup\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <em class=\"ol\">\n    free account\n   </em>\n  </a>\n  <em class=\"ol\">\n   today. You can also visit our GitHub\n  </em>\n  <a href=\"https://github.com/TonicAI/llama-validate-demo\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <em class=\"ol\">\n    page\n   </em>\n  </a>\n  <em class=\"ol\">\n   to view all of the code used in this post and the rest of our SDK. Our LlamaIndex integration is available\n  </em>\n  <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/evaluation/TonicValidateEvaluators.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <em class=\"ol\">\n    here\n   </em>\n  </a>\n  <em class=\"ol\">\n   .\n  </em>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 22772, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89666c95-e8ad-4519-81cd-3bac4c737a3b": {"__data__": {"id_": "89666c95-e8ad-4519-81cd-3bac4c737a3b", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/towards-long-context-rag.html", "file_name": "towards-long-context-rag.html", "file_type": "text/html", "file_size": 21294, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/towards-long-context-rag.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/towards-long-context-rag.html", "file_name": "towards-long-context-rag.html", "file_type": "text/html", "file_size": 21294, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "af28f6d3ccf60dd832d4bdba18b08798ea5f9ccf70260a1933e9a12c285b681a", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Google recently released\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#gemini-15\" rel=\"noreferrer noopener\">\n   Gemini 1.5 Pro with a 1M context window\n  </a>\n  , available to a limited set of developers and enterprise customers. Its performance has caught the imagination of\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/alliekmiller/status/1760522046251962459?s=20\" rel=\"noreferrer noopener\">\n   AI Twitter\n  </a>\n  . It\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf\" rel=\"noreferrer noopener\">\n   achieves 99.7% recall in the \u201cNeedle in a Haystack\u201d\n  </a>\n  experiment popularized by\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/GregKamradt/status/1722386725635580292?s=20\" rel=\"noreferrer noopener\">\n   Greg Kamradt\n  </a>\n  . Early users have shared results feeding dozens of research papers, financial reports at once and report impressive results in terms of its ability to synthesize across vast troves of information.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Naturally, this begs the question - is RAG dead? Some\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/francis_yao_/status/1759962812229800012?s=46&amp;t=pfae6EnnrBq2o8ok0KpVqw\" rel=\"noreferrer noopener\">\n   folks think so\n  </a>\n  , while others\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/ptsi/status/1758511307433947625?s=20\" rel=\"noreferrer noopener\">\n   disagree\n  </a>\n  . Those in the first camp make valid points. Most small data use cases can fit within a 1-10M context window. Tokens will get cheaper and faster to process over time. Having an LLM natively interleave retrieval/generation via attention layers leads to a higher response quality compared to the one-shot retrieval present in naive RAG.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We were fortunate to have a preview of Gemini 1.5 Pro\u2019s capabilities, and through playing around with it developed a thesis for how context-augmented LLM applications will evolve. This blog post clarifies\n  <strong>\n   our mission as a data framework\n  </strong>\n  along with\n  <strong>\n   our view of what long-context LLM architectures will look like.\n  </strong>\n  Our view is that while long-context LLMs will simplify certain parts of the RAG pipeline (e.g. chunking), there will need to be evolved RAG architectures to handle the new use cases that long-context LLMs bring along. No matter what new paradigms emerge, our mission at LlamaIndex is to build tooling towards that future.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Our Mission Goes Beyond RAG\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The goal of LlamaIndex is very simple:\n  <strong>\n   enable developers to build LLM applications over their data.\n  </strong>\n  This mission goes beyond just RAG. To date we have invested a considerable amount of effort in advancing RAG techniques for existing LLMs, and we\u2019ve done so because it\u2019s enabled developers to unlock dozens of new use cases such as QA over semi-structured data, over complex documents, and agentic reasoning in a multi-doc setting.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  But we\u2019re also excited about Gemini Pro, and we will continue to advance LlamaIndex as a production data framework in a long-context LLM future.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   An LLM framework is intrinsically valuable.\n  </strong>\n  As an open-source data framework, LlamaIndex paves the cowpaths towards building any LLM use case from prototype to production. A framework makes it easier to build these use cases versus building from scratch. We enable\n  <em>\n   all\n  </em>\n  developers to build for these use cases, whether it\u2019s setting up the proper architecture using our core abstractions or leveraging the hundreds of integrations in our ecosystem. No matter what the underlying LLM advancements are and whether RAG continues to exist in its current form, we continue to make the framework production-ready, including watertight abstractions, first-class documentation, and consistency.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://blog.llamaindex.ai/introducing-llamacloud-and-llamaparse-af8cedf9006b\" rel=\"noreferrer noopener\">\n   We also launched LlamaCloud last week\n  </a>\n  . Our mission for LlamaCloud remains building the data infra enabling any enterprise to make their vast unstructured, semi-structured, and structured data sources production-ready for use with LLMs.\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Initial Gemini 1.5 Pro Observations\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  During our initial testing we played around with some PDFs: SEC 10K Filings, ArXiv papers, this monster\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.lowellhsproject.com/DocumentCenter/View/236/LHS-Schematic-Design-Binder\" rel=\"noreferrer noopener\">\n   Schematic Design Binder\n  </a>\n  , and more. We will do a lot more deeper analyses once the APIs are available, but in the meantime we share observations below.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Gemini results are impressive and consistent with what we\u2019ve seen in the technical report and on socials:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Gemini has impressive recall of specific details:\n   </strong>\n   We threw in 100k-1M tokens of context, and asked questions over very specific details in these documents (unstructured text and tabular data), and in all cases Gemini was able to recall the details. See above for Gemini comparing table results in the 2019 Uber 10K Filing.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Gemini has impressive summarization capabilities.\n   </strong>\n   The model can analyze large swaths of information across multiple documents and synthesize answers.\n  </li>\n </ul>\n <figure>\n  <figcaption>\n   This figure shows a question-response pair from Gemini over the 2019 Uber 10K filing. The question and answer is shown at the top and the source table is shown at the bottom. Gemini is able to return the correct answer.\n  </figcaption>\n </figure>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  There are some parts where we noticed Gemini struggles a bit.\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Gemini doesn\u2019t read all tables and charts correctly.\n   </strong>\n   Gemini Pro still has a hard time being able to read figures and complex tables.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Gemini can take a long time.\n   </strong>\n   Returning an answer over the Uber 10K Filing (~160k) takes ~20 seconds. Returning an answer over the LHS Schematic Design Binder (~890k) takes ~60+ seconds.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Gemini can hallucinate page numbers.\n   </strong>\n   When asked to give a summary but also with page number citations, Gemini hallucinated the sources.\n  </li>\n </ul>\n <figure>\n  <figcaption>\n   An example where Gemini 1.5 Pro still hallucinates. The model hallucinates a number when asked about the total number of gross bookings across all segments - the number is visible in the chart and can also be pieced together from the table.\n  </figcaption>\n </figure>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Directionally though it\u2019s an exciting glimpse of the future and warrants a bigger discussion on which RAG paradigms will fade and new architectures that will emerge. See below!\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Long Contexts Resolve Some Pain Points, but some Challenges Remain\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Gemini 1.5 Pro is just the first of many long-context LLMs to emerge, which will inevitably change how users are building RAG.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Here are some existing RAG pain points that we believe long-context LLMs will solve:\n </p>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Developers will worry less about how to precisely tune chunking algorithms.\n   </strong>\n   We honestly think this will be a huge blessing to LLM developers. Long-context LLMs enable native chunk sizes to be bigger. Assuming per-token cost and latency also go down, developers will no longer have to split hairs deciding how to split their chunks into granular strips through tuning chunking separators, chunk sizes, and careful metadata injection. Long-context LLMs enable chunks to be at the level of entire documents, or at the very least groups of pages.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Developers will need to spend less time tuning retrieval and chain-of-thought over single documents\n   </strong>\n   . An issue with small-chunk top-k RAG is that while certain questions may be answered over a specific snippet of the document, other questions require deep analysis between sections or between two documents (for instance comparison queries). For these use cases, developers will no longer have to rely on a chain-of-thought agent to do two retrievals against a weak retriever; instead, they can just one-shot prompt the LLM to obtain the answer.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Summarization will be easier.\n   </strong>\n   This is related to the above statement. A lot of summarization strategies over big documents involve \u201chacks\u201d such as sequential refinement or hierarchical summarization (see our\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html\" rel=\"noreferrer noopener\">\n    response synthesis modules\n   </a>\n   as a reference guide). This can now be alleviated with a single LLM call.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Personalized memory will be better and easier to build:\n   </strong>\n   A key issue for building conversational assistants is figuring out how to load sufficient conversational context into the prompt window. 4k tokens easily overflows this window for very basic web search agents - if it decides to load in a Wikipedia page for instance, that text will easily overflow the context. 1M-10M context windows will let developers more easily implement conversational memory with fewer compression hacks (e.g. vector search or automatic KG construction).\n  </li>\n </ol>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  There are, however, some lingering challenges:\n </p>\n <ol>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    10M tokens is not enough for large document corpuses - kilodoc retrieval is still a challenge.\n   </strong>\n   1M tokens is around ~7 Uber SEC 10K filings. 10M tokens would be around ~70 filings. 10M tokens is roughly bounded by 40MB of data. While this is enough for many \u201csmall\u201d document corpuses, many knowledge corpuses in the enterprise are in the gigabytes or terabytes. To build LLM-powered systems over these knowledge corpuses, developers will still need to build in some way of retrieving this data to augment language models with context.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Embedding models are lagging behind in context length.\n   </strong>\n   So far the largest context window we\u2019ve seen for embeddings are\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://hazyresearch.stanford.edu/blog/2024-01-11-m2-bert-retrieval\" rel=\"noreferrer noopener\">\n    32k from together.ai\n   </a>\n   . This means that even if the chunks used for synthesis with long-context LLMs can be big, any text chunks used for retrieval still need to be a lot smaller.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    Cost and Latency.\n   </strong>\n   Yes, all cost and latency concerns are alleviated with time. Nevertheless, stuffing a 1M context window takes ~60 seconds and can cost anywhere from $0.50 to $20 with current pricing. An solution to this that\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://twitter.com/Francis_YAO_\" rel=\"noreferrer noopener\">\n    Yao Fu\n   </a>\n   brought up is that a\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://x.com/Francis_YAO_/status/1759962812229800012?s=20\" rel=\"noreferrer noopener\">\n    KV Cache\n   </a>\n   can cache the document activations, so that any subsequent generations can reuse the same cache. Which leads to our next point below.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   <strong>\n    A KV Cache takes up a significant amount of GPU memory, and has sequential dependencies\n   </strong>\n   . We chatted with Yao and he mentioned that at the moment, caching 1M tokens worth of activations would use up approximately 100GB of GPU memory, or 2 H100s. There are also interesting challenges on how to best manage the cache especially when the underlying corpus is big - since each activation is a function of all tokens leading up to it, replacing any document in the KV cache would affect all activations following the document.\n  </li>\n </ol>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Towards New RAG Architectures\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Proper usage of long-context LLMs will necessitate new architectures to best take advantage of their capabilities, while working around their remaining constraints. We outline some proposals below.\n </p>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  1. Small to Big Retrieval over Documents\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  To the extent that long-context LLMs need retrieval augmentation over big knowledge bases (e.g. in the gigabytes), we will need\n  <strong>\n   small-to-big retrieval:\n  </strong>\n  index and retrieve small chunks, but have each chunk link to big chunks that will ultimately be fed to LLMs during synthesis.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  This architecture already exists in LlamaIndex in different forms (\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/MetadataReplacementDemo.html\" rel=\"noreferrer noopener\">\n   sentence window retriever\n  </a>\n  and\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://docs.llamaindex.ai/en/stable/examples/retrievers/recursive_retriever_nodes.html\" rel=\"noreferrer noopener\">\n   recursive retrieval over chunk sizes\n  </a>\n  ), but can be scaled up even more for long-context LLMs - embed document summaries, but link to entire documents.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  One reason we want to embed and index smaller chunks is due to the fact that current embedding models are not keeping up with LLMs in terms of context length. Another reason is that there can actually be retrieval benefits in having multiple granular embedding representations compared to a single document-level embedding for a document. If there is a single embedding for a document, then that embedding has the burden of encoding all information throughout the entire document. On the other hand, we\u2019ve found that embedding many smaller chunks and having each small chunk link to a bigger chunk, will lead to better retrieval of the relevant information.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Check out the diagram above for an illustration of two flavors of small-to-big retrieval. One is indexing document summaries and linking them to documents, and the other is indexing smaller chunks within a document and linking them to the document. Of course, you could also do both - a general best practice for improving retrieval is to just try out multiple techniques at once and fuse the results later.\n </p>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  2. Intelligent Routing for Latency/Cost Tradeoffs\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The arrival of long-context LLMs will inevitably raise questions on the amount of context that is suitable for each use case. Injecting LLMs with long context comes with real cost and latency tradeoffs and isn\u2019t suitable for every use case or even every question. Although cost and latency will decrease in the future, we anticipate users will need to think carefully about this tradeoff for the next year or two.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Certain questions that are asking about specific details are well suited for existing RAG techniques of top-k retrieval and synthesis.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  More complex questions require more context from disparate pieces of different documents, and in those settings it is less clear how to correctly answer these questions while optimizing for latency and cost:\n </p>\n <ul>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Summarization questions require going over entire documents.\n  </li>\n  <li class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n   Multi-part questions can be solved by doing chain-of-thought and interleaving retrieval and reasoning; they can also be solved by shoving all context into the prompt.\n  </li>\n </ul>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We imagine an intelligent routing layer that operates on top of multiple RAG and LLM synthesis pipelines over a knowledge base. Given a question, the router can ideally choose an optimal strategy in terms of cost and latency in terms of retrieving context to answer the question. This ensures that a single interface can handle different types of questions while not becoming prohibitively expensive.\n </p>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  3. Retrieval Augmented KV Caching\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  An optimization that Google and other companies are certainly working on is resolving latency and cost concerns through a\n  <strong>\n   KV Cache.\n  </strong>\n  At a high-level, a KV cache stores activations from pre-existing key and query vectors in an attention layer, preventing the need to recompute activations across the entire text sequence during LLM generation (we found\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://medium.com/@plienhar/llm-inference-series-3-kv-caching-unveiled-048152e461c8\" rel=\"noreferrer noopener\">\n   this\n  </a>\n  to be a nice intro reference to how a KV Cache works).\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Using a KV Cache to cache all document tokens within the context window prevents the need to recompute activations for these tokens on subsequent conversations, bringing down latency and cost significantly.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  But this leads to interesting retrieval strategies on how to best use the cache, particularly for knowledge corpuses that exceed the context length. We imagine a \u201c\n  <strong>\n   retrieval augmented caching\n  </strong>\n  \u201d paradigm emerging, where we want to retrieve the most relevant documents that the user would want to answer, with the expectation that they will continue to use the documents that are in the cache.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  This could involve interleaving retrieval strategies with\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://www.notion.so/Long-Context-Window-Blog-Post-e2e3faaac23140eabc0e066ce2783890?pvs=21\" rel=\"noreferrer noopener\">\n   traditional caching algorithms\n  </a>\n  such as LRU caching. But a difference with existing KV cache architectures is that the position matters, since the cached vector is a function of all tokens leading up to that position, not just the tokens in the document itself. This means that you can\u2019t just swap out a chunk from the KV cache without affecting all cached tokens that occur after it positionally.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In general the API interface for using a KV Cache is up in the air. It\u2019s also up in the air as to whether the nature of the cache itself will evolve or algorithms will evolve to best leverage the cache.\n </p>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  What\u2019s Next\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We believe the future of LLM applications is bright, and we are excited to be at the forefront of this rapidly evolving field. We invite developers and researchers to join us in exploring the possibilities of long-context LLMs and building the next generation of intelligent applications.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 21249, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b3d12de-a059-49a6-875c-0b6d7afdd4c6": {"__data__": {"id_": "9b3d12de-a059-49a6-875c-0b6d7afdd4c6", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.html", "file_name": "transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.html", "file_type": "text/html", "file_size": 14000, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.html", "file_name": "transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.html", "file_type": "text/html", "file_size": 14000, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "1b3cbb1db33a74502176df46f79a2bad28549e2a0365ddcc09859d6b6ff2c07a", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  In the dynamic world of AI and data analytics, the ability to bridge the gap between complex data queries and non-technical users is a game-changer. My latest project, Na2SQL, showcases this exciting advancement. Leveraging the power of LlamaIndex and OpenAI\u2019s GPT-3.5, this app allows users, regardless of their SQL knowledge, to derive valuable insights from a database using simple natural language.\n </p>\n <h1>\n  Features\n </h1>\n <ul>\n  <li>\n   <strong>\n    Intuitive Natural Language Queries:\n   </strong>\n   The core of this application is its ability to understand and process natural language queries. Users can ask questions in plain English and receive SQL queries and insights in return, all without any prior SQL experience.\n  </li>\n  <li>\n   <strong>\n    Advanced Data Processing:\n   </strong>\n   The app doesn\u2019t just stop at generating SQL queries; it executes these queries and analyzes the results to provide meaningful insights, making it a powerful tool for data analysis.\n  </li>\n  <li>\n   <strong>\n    User-Friendly Interface with Streamlit:\n   </strong>\n   I chose Streamlit for its simplicity and effectiveness in creating interactive web applications. The app\u2019s interface is straightforward, ensuring a smooth user experience.\n  </li>\n  <li>\n   <strong>\n    Database Viewer:\n   </strong>\n   An interactive database viewer in the sidebar on the left allows users to explore the database structure, enhancing their understanding and interaction with the data.\n  </li>\n </ul>\n <h1>\n  The Tech Stack\n </h1>\n <p>\n  This project harmoniously integrates several advanced technologies:\n </p>\n <ol>\n  <li>\n   <strong>\n    OpenAI\u2019s GPT-3.5:\n   </strong>\n   At the heart of the application is GPT-3.5, enabling the app to understand natural natural language queries and transform them into valid SQL queries. Furthermore, it also generates the final analysis considering both the user\u2019s query and the SQL output, thereby providing a comprehensive and relevant response.\n  </li>\n  <li>\n   <strong>\n    LlamaIndex:\n   </strong>\n   A pivotal component of the app is LlamaIndex\u2019s SQLTableQueryEngine. This powerful tool translates natural language queries into SQL, handles the execution of these queries, and plays a significant role in the subsequent analysis using GPT 3.5. Its integration ensures a smooth transition from user inputs to database insights, culminating in a meaningful final analysis that encapsulates the entire natural language-to-SQL-to-execution process.\n  </li>\n  <li>\n   <strong>\n    LlamaIndex\u2019s Streamlit LlamaPack:\n   </strong>\n   Using LlamaIndex\u2019s Streamlit LlamaPack, we quickly assemble and highly functional Streamlit UI. This framework significantly simplifies the UI development process, allowing for rapid deployment and an enhanced user experience.\n  </li>\n  <li>\n   <strong>\n    SQLite Database:\n   </strong>\n   The app interacts with an dummy SQLite ecommerce database, showcasing its ability to work with real-world data.\n  </li>\n </ol>\n <h1>\n  Deep Dive into the Code\n </h1>\n <p>\n  In the heart of the application lies\n  <code class=\"cw qf qg qh qi b\">\n   app.py\n  </code>\n  , a script that brings to life the seamless interaction between natural language processing and SQL query generation.\n </p>\n <p>\n  This code is an evolution of the\n  <strong>\n   Streamlit chatbot LlamaPack\n  </strong>\n  available on\n  <a href=\"https://github.com/run-llama/llama-hub/blob/main/llama_hub/llama_packs/streamlit_chatbot/base.py\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Llama Hub\n  </a>\n  , further tailored to meet the specific needs of ecommerce data analytics. Let's dive into some key portions of the\n  <code class=\"cw qf qg qh qi b\">\n   app.py\n  </code>\n  script:\n </p>\n <h2>\n  1. Initial Imports and Setup\n </h2>\n <p>\n  The script begins by importing necessary modules such as Streamlit, SQLAlchemy for database interaction, LlamaIndex for language model services, and other essential libraries.\n </p>\n <pre><span class=\"rc om gt qi b bf rd re l rf rg\" id=\"4f1e\"><span class=\"hljs-keyword\">import</span> streamlit <span class=\"hljs-keyword\">as</span> st\n<span class=\"hljs-keyword\">from</span> sqlalchemy <span class=\"hljs-keyword\">import</span> create_engine, inspect\n<span class=\"hljs-keyword\">from</span> typing <span class=\"hljs-keyword\">import</span> <span class=\"hljs-type\">Dict</span>, <span class=\"hljs-type\">Any</span>\n\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> (\n    VectorStoreIndex,\n    ServiceContext,\n    download_loader,\n)\n<span class=\"hljs-keyword\">from</span> llama_index.llama_pack.base <span class=\"hljs-keyword\">import</span> BaseLlamaPack\n<span class=\"hljs-keyword\">from</span> llama_index.llms <span class=\"hljs-keyword\">import</span> OpenAI\n<span class=\"hljs-keyword\">import</span> openai\n<span class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\">as</span> pd</span></pre>\n <h2>\n  2. StreamlitChatPack Class\n </h2>\n <p>\n  The\n  <code class=\"cw qf qg qh qi b\">\n   StreamlitChatPack\n  </code>\n  class extends the base LlamaPack, setting up the page and modules necessary for the app's functionality.\n </p>\n <pre><span class=\"rc om gt qi b bf rd re l rf rg\" id=\"b3e3\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">StreamlitChatPack</span>(<span class=\"hljs-title class_ inherited__\">BaseLlamaPack</span>):\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">\n        self,\n        page: <span class=\"hljs-built_in\">str</span> = <span class=\"hljs-string\">\"Natural Language to SQL Query\"</span>,\n        run_from_main: <span class=\"hljs-built_in\">bool</span> = <span class=\"hljs-literal\">False</span>,\n        **kwargs: <span class=\"hljs-type\">Any</span>,\n    </span>) -&amp;gt; <span class=\"hljs-literal\">None</span>:\n        <span class=\"hljs-string\">\"\"\"Init params.\"\"\"</span>\n        self.page = page\n\n    <span class=\"hljs-comment\"># ... other methods ...</span></span></pre>\n <h2>\n  3. The\n  <code class=\"cw qf qg qh qi b\">\n   run\n  </code>\n  Method\n </h2>\n <p>\n  This method is where the magic happens. It sets up the Streamlit page configuration and initializes the chat functionality.\n </p>\n <pre><span class=\"rc om gt qi b bf rd re l rf rg\" id=\"30a8\">def run(self, *args: Any, **kwargs: Any) -<span class=\"hljs-symbol\">&amp;gt;</span> Any:\n    \"\"\"Run the pipeline.\"\"\"\n    import streamlit as st\n\n    st.set_page_config(\n        page_title=f\"{self.page}\",\n        layout=\"centered\",\n        initial_sidebar_state=\"auto\",\n        menu_items=None,\n    )\n    \n    # ... rest of the run method ...</span></pre>\n <h2>\n  4. Database Schema Viewer in the Sidebar\n </h2>\n <p>\n  A helpful feature is the Database Schema Viewer, conveniently located in the sidebar. This viewer serves as a reference tool, allowing users to see the structure and content of the database tables, enhancing their understanding of the data they\u2019re querying.\n </p>\n <pre><span class=\"rc om gt qi b bf rd re l rf rg\" id=\"5f71\"><span class=\"hljs-comment\"># Sidebar for database schema viewer</span>\nst.sidebar.markdown(<span class=\"hljs-string\">\"## Database Schema Viewer\"</span>)\n\n<span class=\"hljs-comment\"># Create an inspector object</span>\ninspector = inspect(engine)\n\n<span class=\"hljs-comment\"># Get list of tables in the database</span>\ntable_names = inspector.get_table_names()\n\n<span class=\"hljs-comment\"># Sidebar selection for tables</span>\nselected_table = st.sidebar.selectbox(<span class=\"hljs-string\">\"Select a Table\"</span>, table_names)\n\ndb_file = <span class=\"hljs-string\">'ecommerce_platform1.db'</span>\nconn = sqlite3.connect(db_file)\n\n<span class=\"hljs-comment\"># Display the selected table</span>\n<span class=\"hljs-keyword\">if</span> selected_table:\n    df = get_table_data(selected_table, conn)\n    st.sidebar.text(<span class=\"hljs-string\">f\"Data for table '<span class=\"hljs-subst\">{selected_table}</span>':\"</span>)\n    st.sidebar.dataframe(df)\n\n<span class=\"hljs-comment\"># Close the connection</span>\nconn.close()</span></pre>\n <h2>\n  5. Database Interaction and LLM Integration\n </h2>\n <p>\n  This part of the code loads the database from disk and initializes the LLM and the service context for use with Llamaindex. I\u2019ve used GPT3.5 here, but you can easily swap it out with any other LLM of your choice.\n </p>\n <pre><span class=\"rc om gt qi b bf rd re l rf rg\" id=\"c889\"><span class=\"hljs-comment\"># Function to load database and LLM</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">load_db_llm</span>():\n    engine = create_engine(<span class=\"hljs-string\">\"sqlite:///ecommerce_platform1.db\"</span>)\n    sql_database = SQLDatabase(engine)  <span class=\"hljs-comment\"># Include all tables</span>\n    llm2 = OpenAI(temperature=<span class=\"hljs-number\">0.1</span>, model=<span class=\"hljs-string\">\"gpt-3.5-turbo-1106\"</span>)\n    service_context = ServiceContext.from_defaults(llm=llm2)\n    <span class=\"hljs-keyword\">return</span> sql_database, service_context, engine\n\nsql_database, service_context, engine = load_db_llm()</span></pre>\n <h2>\n  6. Initializing the NLSQLTableQueryEngine\n </h2>\n <p>\n  One of the most critical aspects of the application is the initialization of the\n  <code class=\"cw qf qg qh qi b\">\n   NLSQLTableQueryEngine\n  </code>\n  . This is where the app sets up the engine responsible for converting natural language queries into SQL queries, executing them and generating the final response, all with the help of GPT 3.5.\n </p>\n <pre><span class=\"rc om gt qi b bf rd re l rf rg\" id=\"8999\"><span class=\"hljs-comment\"># Initializing the query engine</span>\n<span class=\"hljs-keyword\">if</span> <span class=\"hljs-string\">\"query_engine\"</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span> st.session_state:\n    st.session_state[<span class=\"hljs-string\">\"query_engine\"</span>] = NLSQLTableQueryEngine(\n        sql_database=sql_database,\n        synthesize_response=<span class=\"hljs-literal\">True</span>,\n        service_context=service_context\n    )</span></pre>\n <h2>\n  7. User Interaction and Displaying Results\n </h2>\n <p>\n  The script provides an interactive interface for users to input natural language queries, which are then translated into SQL queries and executed.\n </p>\n <p>\n  The app concludes by displaying the SQL queries and responses, offering an informative and engaging user experience\n </p>\n <pre><span class=\"rc om gt qi b bf rd re l rf rg\" id=\"9339\"><span class=\"hljs-keyword\">if</span> prompt := st.chat_input(\n    <span class=\"hljs-string\">\"Enter your natural language query about the database\"</span>\n):  <span class=\"hljs-comment\"># Prompt for user input and save to chat history</span>\n    <span class=\"hljs-keyword\">with</span> st.chat_message(<span class=\"hljs-string\">\"user\"</span>):\n        st.write(prompt)\n    add_to_message_history(<span class=\"hljs-string\">\"user\"</span>, prompt)\n\n<span class=\"hljs-comment\"># If last message is not from assistant, generate a new response</span>\n<span class=\"hljs-keyword\">if</span> st.session_state[<span class=\"hljs-string\">\"messages\"</span>][-<span class=\"hljs-number\">1</span>][<span class=\"hljs-string\">\"role\"</span>] != <span class=\"hljs-string\">\"assistant\"</span>:\n    <span class=\"hljs-keyword\">with</span> st.spinner():\n        <span class=\"hljs-keyword\">with</span> st.chat_message(<span class=\"hljs-string\">\"assistant\"</span>):\n            response = st.session_state[<span class=\"hljs-string\">\"query_engine\"</span>].query(<span class=\"hljs-string\">\"User Question:\"</span>+prompt+<span class=\"hljs-string\">\". \"</span>)\n            sql_query = <span class=\"hljs-string\">f\"```sql\\n<span class=\"hljs-subst\">{response.metadata[<span class=\"hljs-string\">'sql_query'</span>]}</span>\\n```\\n**Response:**\\n<span class=\"hljs-subst\">{response.response}</span>\\n\"</span>\n            response_container = st.empty()\n            response_container.write(sql_query)\n            add_to_message_history(<span class=\"hljs-string\">\"assistant\"</span>, sql_query)</span></pre>\n <h1>\n  Wrapping Up\n </h1>\n <p>\n  This app is more than just a tool; it\u2019s a step towards making data analytics accessible to a broader audience. It embodies the potential of AI in simplifying complex data interactions. I invite you to explore this application, witness its capabilities, and join me in this journey towards a more inclusive data-driven future.\n </p>\n <p>\n  <a href=\"https://github.com/AI-ANK/Na2SQL/tree/main\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Link to Github Repo\n  </a>\n </p>\n <p>\n  <a href=\"https://www.linkedin.com/in/harshadsuryawanshi/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Connect with Me on LinkedIn\n  </a>\n </p>\n <p>\n  <a href=\"https://www.linkedin.com/posts/harshadsuryawanshi_ai-llamaindex-streamlit-activity-7141801596006440960-mCjT?utm_source=combined_share_message&amp;utm_medium=member_desktop\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Linkedin Post:\n  </a>\n </p>\n <div>\n  <a href=\"https://www.linkedin.com/posts/harshadsuryawanshi_ai-llamaindex-streamlit-activity-7141801596006440960-mCjT?utm_source=combined_share_message&amp;utm_medium=member_desktop&amp;source=post_page-----e08edefa21f9--------------------------------\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <div class=\"rn ab jb\">\n    <div class=\"ro ab cn ca rp rq\">\n     <h2 class=\"be gu is z jj rr jl jm rs jo jq gs bj\">\n      Harshad S. on LinkedIn: #ai #llamaindex #streamlit #largelanguagemodels\u2026\n     </h2>\n     <div class=\"rt l\">\n      <h3 class=\"be b is z jj rr jl jm rs jo jq dt\">\n       AI Prototype 6: Transforming Natural Language to SQL and Insights for E-commerce with LlamaIndex, OpenAI GPT3.5, and\u2026\n      </h3>\n     </div>\n     <div class=\"ru l\">\n      <p class=\"be b du z jj rr jl jm rs jo jq dt\">\n       www.linkedin.com\n      </p>\n     </div>\n    </div>\n   </div>\n  </a>\n </div>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 13973, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44d7b787-127b-4136-8076-08c9bc871bd5": {"__data__": {"id_": "44d7b787-127b-4136-8076-08c9bc871bd5", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.html", "file_name": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.html", "file_type": "text/html", "file_size": 14281, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.html", "file_name": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.html", "file_type": "text/html", "file_size": 14281, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "5438c45a743a5eaaa81bcabc6d5fc7f280e932f14ebfe982835f2b3c20db0604", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  (Authored by Andrei Fajardo at LlamaIndex)\n </p>\n <figure>\n  <figcaption class=\"ol fe om nx ny on oo be b bf z dt\">\n   The llama-dataset collection. Each labelled llama-dataset is comprised of its associated labelled examples. With examples, we make predictions with the appropriate object depending on the task. After making predictions, we can evaluate the performance of the object by measuring some distance between predictions and the corresponding references.\n  </figcaption>\n </figure>\n <h1>\n  Intro\n </h1>\n <p>\n  A few weeks back, we\n  <a href=\"/introducing-llama-datasets-aadb9994ad9e\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   launched\n  </a>\n  our very first set of llama-datasets, namely the\n  <code class=\"cw pt pu pv pw b\">\n   LabelledRagDataset\n  </code>\n  . The main purpose of these llama-datasets is to provide builders with the means to benchmark their LLM systems in an effective and efficient manner. In the couple of weeks since that launch date, we\u2019ve amassed over a dozen\n  <code class=\"cw pt pu pv pw b\">\n   LabelledRagDataset\n  </code>\n  s via both staff and community contributions (all of which are available for download through\n  <a href=\"https://llamahub.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaHub\n  </a>\n  )!\n </p>\n <p>\n  The fun doesn\u2019t stop there though: today we\u2019re introducing two new llama-dataset types:\n  <code class=\"cw pt pu pv pw b\">\n   LabelledEvaluatorDataset\n  </code>\n  and the\n  <code class=\"cw pt pu pv pw b\">\n   LabelledPairwiseEvaluatorDataset\n  </code>\n  . These new llama-dataset types are meant for evaluating or benchmarking an LLM evaluator. Indeed, the adopted standard for evaluating LLM responses is to use a strong LLM as an evaluator. This approach is certainly more scalable, faster, and cheaper than using human evaluators via crowdsourcing. However, these LLM evaluators themselves must also be continuously evaluated rather than blindly trusted.\n </p>\n <p>\n  In this post, we provide a brief overview of the new llama-datasets as well as provide some very interesting results from benchmarking Google\u2019s Gemini and OpenAI\u2019s GPT models as LLM evaluators on the MT-Bench datasets which we\u2019ve converted into the new llama-dataset types.\n </p>\n <h1>\n  A primer on the new llama-datasets\n </h1>\n <p>\n  Before getting into the new llama-datasets, recall that with\n  <code class=\"cw pt pu pv pw b\">\n   LabelledRagDataset\n  </code>\n  our end goal was to use it to evaluate or benchmark a Retrieval-Augmented Generation (RAG) system. The way to do that with our llama-dataset abstractions is to build a\n  <code class=\"cw pt pu pv pw b\">\n   QueryEngine\n  </code>\n  (i.e., a RAG system) and then use it to make \u201cpredictions\u201d over the\n  <code class=\"cw pt pu pv pw b\">\n   LabelledRagDataset\n  </code>\n  . With the predictions in hand, we can evaluate the quality of these predictions by comparing it to the corresponding reference attributes of the\n  <code class=\"cw pt pu pv pw b\">\n   LabelledRagDataset\n  </code>\n  .\n </p>\n <figure>\n  <figcaption class=\"ol fe om nx ny on oo be b bf z dt\">\n   Benchmarking flow with LabelledRagDataset. With a query engine, predictions are made over every labelled example. We can then compare predicted responses and contexts with the reference versions (i.e., labels). This flow is conveniently handled via the RagEvaluatorPack.\n  </figcaption>\n </figure>\n <p>\n  In a similar vein, the new llama-datasets are meant to benchmark an LLM evaluator. Let\u2019s go through the first kind, the\n  <code class=\"cw pt pu pv pw b\">\n   LabelledEvaluatorDataset\n  </code>\n  . Here, instead of the RAG system making predictions on a\n  <code class=\"cw pt pu pv pw b\">\n   LabelledRagDataset\n  </code>\n  we have an LLM evaluator making \u201cpredictions\u201d over a\n  <code class=\"cw pt pu pv pw b\">\n   LabelledEvaluatorDataset\n  </code>\n  \u2014 predictions in this context means that the LLM evaluator is evaluating the response produced by another LLM model to a given query. As before, with the predictions in hand, we can measure the goodness of the LLM evaluator\u2019s evaluations by comparing it to the corresponding reference attributes of the\n  <code class=\"cw pt pu pv pw b\">\n   LabelledEvaluatorDataset\n  </code>\n  .\n </p>\n <figure>\n  <figcaption class=\"ol fe om nx ny on oo be b bf z dt\">\n   Benchmarking flow with LabelledEvaluatorDataset. With a supplied evaluator, predictions are made over every example. In this context, a prediction is an evaluation of the answer to the query and optional contexts and ground truth answer. With these predictions in hand, we can evaluate how good the evaluations are by comparing them to the reference feedbacks and scores. A llama-pack called EvaluatorBenchmarkerPack makes benchmarking a one-liner.\n  </figcaption>\n </figure>\n <p>\n  The second llama-dataset we\u2019re introducing today can be considered an extension of the first one. The\n  <code class=\"cw pt pu pv pw b\">\n   LabelledPairwiseEvaluatorDataset\n  </code>\n  is similarly used for benchmarking an LLM evaluator. However, there is a subtle difference in the evaluation task as here the LLM evaluator compares two generated answers from two separate LLMs. Outside of this difference, the flow for using this llama-dataset to benchmark an evaluator remains the same.\n </p>\n <figure>\n  <figcaption class=\"ol fe om nx ny on oo be b bf z dt\">\n   Benchmarking flow with LabelledPairwiseEvaluatorDataset. With a supplied evaluator, predictions are made over every example. In this context, a prediction is an evaluation of two answers to the query and optional contexts and ground truth answer. That is, the LLM evaluator ranks the two answers to determine the superior one. With these predictions in hand, we can evaluate how good the evaluations are by comparing them to the reference feedbacks and scores. A llama-pack called EvaluatorBenchmarkerPack makes benchmarking a one-liner.\n  </figcaption>\n </figure>\n <h1>\n  Benchmarking Gemini and GPT models as LLM evaluators: Gemini achieves GPT-3.5 performance!\n </h1>\n <p>\n  In this section, we will put our new llama-dataset types to use in order to pit Gemini Pro against GPT models. For this, we\u2019re going to use slightly adapted versions of the MT-Bench dataset. These adapted versions have been made available for download and use through\n  <a href=\"https://llamahub.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaHub\n  </a>\n  along with today\u2019s release!\n </p>\n <h1>\n  Mini MT-Bench Single Grading Dataset\n </h1>\n <p>\n  This llama-dataset is a\n  <code class=\"cw pt pu pv pw b\">\n   LabelledEvaluatorDataset\n  </code>\n  and is a miniature version of the MT-Bench single-grading dataset. In particular, we consider all of the 160 original questions (i.e., 80 x 2, since MT Bench is a two-turn question dataset), but only the responses produced by Llama2-70b. For the reference evaluations, we use GPT-4. As with the original\n  <code class=\"cw pt pu pv pw b\">\n   LabelledRagDataset\n  </code>\n  , we\u2019ve produced a new llama-pack\n  <code class=\"cw pt pu pv pw b\">\n   EvaluatorBenchmarkerPack\n  </code>\n  (of course, also made available in today\u2019s release!) to make benchmarking an LLM evaluator on the new llama-datasets relatively effortless. The below snippet of code is how you can replicate the results of this benchmark\n </p>\n <pre><span class=\"qd oq gt pw b bf qe qf l qg qh\" id=\"d4f1\"><span class=\"hljs-keyword\">from</span> llama_index.llama_dataset <span class=\"hljs-keyword\">import</span> download_llama_dataset\n<span class=\"hljs-keyword\">from</span> llama_index.llama_pack <span class=\"hljs-keyword\">import</span> download_llama_pack\n<span class=\"hljs-keyword\">from</span> llama_index.evaluation <span class=\"hljs-keyword\">import</span> CorrectnessEvaluator\n<span class=\"hljs-keyword\">from</span> llama_index.llms <span class=\"hljs-keyword\">import</span> Gemini\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> ServiceContext</span></pre>\n <pre><span class=\"qm oq gt pw b is qn qo l jh qh\" id=\"382e\"># download dataset\nevaluator_dataset, _ = download_llama_dataset(\n    \"MiniMtBenchSingleGradingDataset\", \"./mini_mt_bench_data\"\n)</span><span class=\"qm oq gt pw b is qp qo l jh qh\" id=\"069b\"># define evaluator\ngemini_pro_context = ServiceContext.from_defaults(\n    llm = Gemini(model=\"models/gemini-pro\", temperature=0)\n)\nevaluator = CorrectnessEvaluator(service_context=gemini_pro_context)</span><span class=\"qm oq gt pw b is qp qo l jh qh\" id=\"f1a4\"># download EvaluatorBenchmarkerPack and define the benchmarker\nEvaluatorBenchmarkerPack = download_llama_pack(\"EvaluatorBenchmarkerPack\", \"./pack\")\nevaluator_benchmarker = EvaluatorBenchmarkerPack(\n    evaluator=evaluators[\"gpt-3.5\"],\n    eval_dataset=evaluator_dataset,\n    show_progress=True,\n)</span><span class=\"qm oq gt pw b is qp qo l jh qh\" id=\"8f78\"># produce the benchmark result\nbenchmark_df = await evaluator_benchmarker.arun(\n\t\tbatch_size=5,\n\t\tsleep_time_in_seconds=0.5\n)</span></pre>\n <h1>\n  Benchmark Results\n </h1>\n <figure>\n  <figcaption class=\"ol fe om nx ny on oo be b bf z dt\">\n   Invalid_predictions occurs whenever the LLM evaluator fails to produce the desired output structure and as well as other exceptions. Correlations represent the correlations with the scores produced by each of the evaluators with the reference scores produced by the reference evaluator GPT-4. Similarly, the remaining two metrics, MAE (i.e., mean absolute error, which is a sum of the absolute differences between each pair of evaluator and reference scores) and Hamming (i.e., which counts how many times evaluator and reference scores are equivalent), are computed with the scores produced by the evaluators and those from the reference evaluator.\n  </figcaption>\n </figure>\n <p>\n  <strong>\n   Observations\n  </strong>\n </p>\n <ul>\n  <li>\n   It seems that Gemini-Pro and GPT-3.5 are quite close in terms of their closeness to the reference evaluator GPT-4!\n  </li>\n  <li>\n   As for GPT-4 versus the reference GPT-4, this is mostly used for assessing self-consistency of the LLM, for which we see it does a fairly good job at that.\n  </li>\n </ul>\n <h1>\n  MT-Bench Human Judgement Dataset\n </h1>\n <p>\n  For this benchmark, we\u2019ll evaluate the LLM evaluators on the task of ranking two LLM responses, to determine which of the two is the better one. And it is for this such task that\n  <code class=\"cw pt pu pv pw b\">\n   LabelledPairwiseEvaluatorDataset\n  </code>\n  exists. The llama-dataset that we\u2019ve curated here is a slightly adapted version of the original MT-Bench Human Judgement dataset. Specifically, in the original dataset, there are some replications with respect to the triple (query, model A, model B) examples since for some of these more than one human evaluation was provided. Since our prompt allows the LLM evaluator to deem a tie, and to our knowledge, this wasn\u2019t made an option for the human evaluators, we have aggregated the results across the different human evaluations to get the proportion of times model A wins versus model B for each triple (query, model A, model B). We then say that human evaluators deem a tie if the proportion lies between 0.4 and 0.6. It should be emphasized here that the reference evaluations are provided by humans, and so the benchmark metrics that we produce and share here represent the LLM agreement with humans.\n </p>\n <p>\n  (We skip showing the code snipped to produce the results here, because they\u2019re essentially the same as the previously shared code snipper with the exception of requiring a\n  <code class=\"cw pt pu pv pw b\">\n   PairwiseComparisonEvaluator\n  </code>\n  instead of a\n  <code class=\"cw pt pu pv pw b\">\n   CorrectnessEvaluator\n  </code>\n  .)\n </p>\n <h1>\n  Benchmark Results\n </h1>\n <figure>\n  <figcaption class=\"ol fe om nx ny on oo be b bf z dt\">\n   Invalid_predictions are as defined in the previous benchmark. Inconclusive\u2019s represent the case when an LLM evaluator flips its winner after prompting it with the same evaluation task but instead flipping the order of presentation of the two answers (i.e. to mitigate against position bias). Two agreement rates, with the inclusion and exclusion of ties, are also produced \u2014 note that these are both conditional in the event that the prediction (or evaluation) is valid.\n  </figcaption>\n </figure>\n <p>\n  <strong>\n   Observations\n  </strong>\n </p>\n <ul>\n  <li>\n   In terms of agreement rates, all three models seem quite close. Note again that these are conditional on the prediction/evaluation being valid. And so, one should \u201cdiscount\u201d these with the invalid and inconclusive counts.\n  </li>\n  <li>\n   Gemini Pro and GPT-3.5 seem to be a bit more assertive than GPT-4 resulting in only 50\u201360 ties to GPT-4\u2019s 100 ties.\n  </li>\n  <li>\n   Overall, it seems that Gemini Pro is up to snuff with GPT models, and would say that it outperforms GPT-3.5!\n  </li>\n </ul>\n <h1>\n  Go now and evaluate your evaluators (and eat your veggies)!\n </h1>\n <p>\n  It is, for obvious reasons, important to evaluate your LLM evaluators, as these are now being relied upon to evaluate the performance of our LLM systems \u2014 a broken compass is not really helpful! With these newly introduced llama-datasets, we hope that it is easy for you to compile your own benchmark datasets on your own data, and then even easier to produce your benchmark metrics. As mentioned before, the two llama-datasets discussed in this blog are available for download and use through\n  <a href=\"https://llamahub.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaHub\n  </a>\n  . Be sure to visit and make use of the datasets there to build an exhaustive benchmark suite! (We welcome contributed llama-datasets as well!)\n </p>\n <h1>\n  Related Links\n </h1>\n <ul>\n  <li>\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/evaluation/mt_bench_human_judgement.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    MT-Bench Human Judgement Benchmarking Notebook\n   </a>\n  </li>\n  <li>\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/evaluation/mt_bench_single_grading.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    MT-Bench Single Grading Benchmarking Notebook\n   </a>\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 14222, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "781d1d55-0b3b-450b-a735-1f91508a309d": {"__data__": {"id_": "781d1d55-0b3b-450b-a735-1f91508a309d", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/unlocking-the-3rd-dimension-for-generative-ai-part-1.html", "file_name": "unlocking-the-3rd-dimension-for-generative-ai-part-1.html", "file_type": "text/html", "file_size": 8072, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/unlocking-the-3rd-dimension-for-generative-ai-part-1.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/unlocking-the-3rd-dimension-for-generative-ai-part-1.html", "file_name": "unlocking-the-3rd-dimension-for-generative-ai-part-1.html", "file_type": "text/html", "file_size": 8072, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "1737d321ca023ec6ac6d78108c2965b651747fff0014368694c4d60c6810c528", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  It would be an understatement to say that generative AI has been taking the world by storm over the past couple of years. While text (1D) and image (2D) models are reaching a level of quality that is truly transforming the way we create digital content, the same cannot be said for 3D models.\n </p>\n <p>\n  3D generative AI has made significant strides in creating 3D digital assets, with techniques such as Neural Radiance Fields (NeRFs) showing promising results for applications such as video game development. However, the limitations of current generative AI technologies become apparent when it comes to applications beyond the digital realm, especially in the context of engineering and manufacturing. After spending more than 2 years actively engaged with the state-of-the-art tools for 3D generative AI, I personally have not been able to generate a single model that I would actually want to have manufactured into a physical object.\n </p>\n <p>\n  To put it simply: the current state of 3D generative AI is not very useful for engineers.\n </p>\n <h2>\n  The Motivation Behind neThing.xyz\n </h2>\n <p>\n  Our mission at\n  <a href=\"https://polyspectra.com/\">\n   polySpectra\n  </a>\n  is to help engineers make their ideas real. The key insight that led to the invention of\n  <a href=\"http://nething.xyz/\">\n   neThing.xyz\n  </a>\n  (pronounced \u201canything dot x,y,z\u201d) was that AI is actually quite good at writing code, and by training a \u201ccodegen\u201d AI on domain specific languages for \u201ccode CAD\u201d, our AI can produce code that can be rapidly converted into 3D CAD models.\n </p>\n <p>\n  For some quick context, AI code generation tools are now achieving ~95% evaluation benchmarks against human programmers. At the current pace, I wouldn\u2019t be surprised if an AI hits 100% in the next three weeks. (\n  <a href=\"https://paperswithcode.com/sota/code-generation-on-humaneval\">\n   See this leaderboard\n  </a>\n  for more details.)\n </p>\n <p>\n  In 3D modeling, there is a growing buzz around \u201ccode CAD\u201d \u2014 a term that signifies a paradigm shift in how we approach computer-aided design. Unlike traditional graphical CAD interfaces, which rely heavily on visual tools and manual user input, code CAD leverages programming to create and manipulate 3D models. This approach offers a more direct and potentially more powerful method for generating complex designs, as it allows for precision and automation that can be difficult to achieve with mouse-driven interfaces.\n </p>\n <p>\n  A testament to the rising prominence of code CAD was its debut this month in the Too Tall Toby speed CAD competition. In the second match of the video below,\n  <a href=\"https://github.com/jdegenstein/\">\n   Jern\n  </a>\n  competes using the code CAD package\n  <a href=\"https://build123d.readthedocs.io/en/latest/\">\n   Build123d\n  </a>\n  , against \u201cMr. Alex\u201d who is using the traditional CAD tool SolidWorks.\n </p>\n <p>\n  <a href=\"https://www.youtube.com/watch?v=-coWKJhwQbM&amp;t=4666s\">\n   (Jump to 1:18 for competition-grade Code CAD!)\n  </a>\n </p>\n <p>\n  <i>\n   So my idea was simple: if AI can code, and code can CAD, why can\u2019t AI CAD?\n  </i>\n </p>\n <h2>\n  Why RAG?\n </h2>\n <p>\n  Trying to create a 3D generative AI with this code generation approach led us to confront a significant challenge: the need for incredibly long prompts to provide the AI with enough context about our code CAD domain-specific languages. This was essential for it to stand any chance of producing working code, let alone something useful.\n </p>\n <p>\n  I first studied Retrieval-Augmented Generation (RAG) under the tutelage of Mayo Oshin. (\n  <a href=\"https://maven.com/ai-chat-with-data/chatgpt-your-data\">\n   Check out his RAG course!\n  </a>\n  ). I knew that RAG was going to be a necessary part of the strategy for making neThing.xyz \u201csmarter\u201d, and through Mayo\u2019s course I had the opportunity to meet Jerry Liu and ask him some questions about the more nuanced elements of retrieval.\n </p>\n <p>\n  The initial version of neThing.xyz was ok, but I really had to wrangle the LLM: each query involved about 10,000 tokens. So if a user asks for \u201ca box\u201d, it is not 2 input tokens I\u2019m paying for, it is 10,002 tokens. I knew I needed a more scalable approach\u2026\n </p>\n <h2>\n  Enter the RAG-a-thon\n </h2>\n <p>\n  The announcement of the \u201c\n  <a href=\"https://rag-a-thon.devpost.com/\">\n   RAG-a-thon\n  </a>\n  \u201d presented the perfect opportunity to quickly integrate RAG into neThing.xyz. For those familiar with the whirlwind nature of hackathons, you\u2019ll understand when I say that time always seems to be in shorter supply than anticipated, often leading one to overestimate what can be accomplished. (For me, I usually overestimate what I can achieve by 3\u201310x!) With this in mind, I tried to set a modest goal for the weekend: add LlamaIndex to neThing.xyz.\n </p>\n <p>\n  My ultimate aim was to leverage LlamaIndex to dramatically expand the corpus of documentation available to neThing.xyz. But in the theme of setting a low bar, I started by just breaking down my very large system prompt into a set of documents that LlamaIndex could retrieve from AstraDB, bringing back only the most relevant example code for a given user\u2019s query.\n </p>\n <p>\n  With significant assistance from Logan Markewich from the LlamaIndex team, I managed to implement RAG via LlamaIndex and AstraDB in a single day. This immediately reduced the average number of tokens per user query from about 10,000 to roughly 2,000. The impact of this was huge, resulting in an 80% cost reduction in our OpenAI bill in just one day \u2014 a change that was incredibly meaningful for us as a small business.\n </p>\n <p>\n  While I am personally passionate about the myriad ways in which RAG can make LLMs smarter, I want to emphasize the immediate ROI that RAG provided. By simply reorganizing the same set of information for retrieval through LlamaIndex, we achieved significant cost savings with minimal effort. As an entrepreneur, an 80% reduction in costs with just eight hours of work is a deal I\u2019d take any day of the week.\n </p>\n <p>\n  On the final Sunday of the RAG-a-thon, I dedicated most of my time to ensuring that my demo would function correctly. The \u201cdemo gods\u201d blessed me that day: I won first place in the \u201ccontinuous innovation\u201d track!\n </p>\n <h2>\n  Examples of neThing.xyz in Action:\n </h2>\n <p>\n  Text:\n </p>\n <p>\n  Curves:\n </p>\n <p>\n  Threads:\n </p>\n <p>\n  Pipes:\n </p>\n <p>\n  Lattices:\n </p>\n <h2>\n  What\u2019s Next?\n </h2>\n <p>\n  My goal with neThing.xyz is to make the best 3D generative AI for engineers, with a focus on \u201ctext-to-CAD\u201d. This is a really hard problem, and I shared some of these challenges with LlamaIndex in our recent webinar, and in more detail on\n  <a href=\"https://www.wevolver.com/article/teaching-ai-cad-is-hard\">\n   Wevolver\n  </a>\n  .\n </p>\n <p>\n  Our key objectives are to make neThing.xyz faster, smarter, and cheaper. We are leveraging LlamaIndex to orchestrate the entire RAG pipeline, and we are really excited about the amazing pace of developments in this open source community.\n </p>\n <p>\n  <em>\n   What would it take to get a part like this from a natural language prompt?\n  </em>\n </p>\n <p>\n  Honestly, I don\u2019t know how to do it.\n </p>\n <p>\n  We are just getting things off the ground and I would be tremendously excited to have you join our community. Our AI will only ever be as smart as the sum of the community that trained it, and we are excited to see what you will create!\n </p>\n <p>\n  Please give\n  <a href=\"https://nething.xyz/\">\n   neThing.xyz\n  </a>\n  a try today and share your honest feedback with us via our\n  <a href=\"https://forum.nething.xyz/\">\n   new community forum\n  </a>\n  . Your input will play a crucial role in our ongoing development efforts, helping us to refine and improve the tool.\n </p>\n <p>\n  Make it real.\n </p>\n <p>\n  Raymond\n </p>\n <p>\n  P.S. \u2014\n  <em>\n   This article is titled Part 1 for a reason. What do you want to see in Part 2? Tell me below!\n  </em>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 7999, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29e5f7c9-ebbf-4ae8-a2e8-3e2dd022921c": {"__data__": {"id_": "29e5f7c9-ebbf-4ae8-a2e8-3e2dd022921c", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.html", "file_name": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.html", "file_type": "text/html", "file_size": 19393, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.html", "file_name": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.html", "file_type": "text/html", "file_size": 19393, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "62cde2b61ee8fa5b8732f7e9769965c12a284d359ef363f153012a749440c889", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <em>\n   This is a guest post from our friends at Mozilla about\n   <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://future.mozilla.org/news/llamafile-four-months-of-progress-towards-democratizing-ai/\" rel=\"noreferrer noopener\">\n    Llamafile\n   </a>\n  </em>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/Mozilla-Ocho/llamafile\" rel=\"noreferrer noopener\">\n   llamafile\n  </a>\n  , an open source project from Mozilla, is one of the simplest ways to run a large language model (LLM) on your laptop. All you have to do is download a llamafile from\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://huggingface.co/models?sort=trending&amp;search=llamafile\" rel=\"noreferrer noopener\">\n   HuggingFace\n  </a>\n  then run the file. That's it.\n  <strong>\n   On most computers, you won't need to install anything.\n  </strong>\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  There are a few reasons why you might want to run an LLM on your laptop, including:\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  1. Privacy: Running locally means you won't have to share your data with third parties.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  2. High availability: Run your LLM-based app without an internet connection.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  3. Bring your own model: You can easily test many different open-source LLMs (anything available on HuggingFace) and see which one works best for your task.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  4. Free debugging/testing: Local LLMs allow you to test many parts of an LLM-based system without paying for API calls.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In this blog post, we'll show how to set up a llamafile and use it to run a local LLM on your computer. Then, we'll show how to use LlamaIndex with your llamafile as the LLM &amp; embedding backend for a local RAG-based research assistant. You won't have to sign up for any cloud service or send your data to any third party--everything will just run on your laptop.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Note: You can also get all of the example code below as a Jupyter notebook from our\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/Mozilla-Ocho/llamafile-llamaindex-examples\" rel=\"noreferrer noopener\">\n   GitHub repo\n  </a>\n  .\n </p>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Download and run a llamafile\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  First, what is a llamafile? A llamafile is an executable LLM that you can run on your own computer. It contains the weights for a given open source LLM, as well as everything needed to actually run that model on your computer. There's nothing to install or configure (with a few caveats, discussed\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file#gotchas\" rel=\"noreferrer noopener\">\n   here\n  </a>\n  ).\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Each llamafile bundles 1) model weights &amp; metadata in gguf format + 2) a copy of\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/ggerganov/llama.cpp\" rel=\"noreferrer noopener\">\n   `llama.cpp`\n  </a>\n  specially compiled using [Cosmopolitan Libc](https://github.com/jart/cosmopolitan). This allows the models to run on most computers without additional installation. llamafiles also come with a ChatGPT-like browser interface, a CLI, and an OpenAI-compatible REST API for chat models.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  There are only 2 steps to setting up a llamafile:\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  1. Download a llamafile\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  2. Make the llamafile executable\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  We'll go through each step in detail below.\n </p>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  Step 1: Download a llamafile\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  There are many llamafiles available on the\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://huggingface.co/models?sort=trending&amp;search=llamafile\" rel=\"noreferrer noopener\">\n   HuggingFace model hub\n  </a>\n  (just search for 'llamafile') but for the purpose of this walkthrough, we'll use\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile\" rel=\"noreferrer noopener\">\n   TinyLlama-1.1B\n  </a>\n  (0.67 GB,\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile\" rel=\"noreferrer noopener\">\n   model info\n  </a>\n  ). To download the model, you can either click this download link:\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile?download=true\" rel=\"noreferrer noopener\">\n   TinyLlama-1.1B\n  </a>\n  or open a terminal and use something like `wget`. The download should take 5-10 minutes depending on the quality of your internet connection.\n </p>\n <pre><code>wget https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile </code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  This model is small and won't be very good at actually answering questions but, since it's a relatively quick download and its inference speed will allow you to index your vector store in just a few minutes, it's good enough for the examples below. For a higher-quality LLM, you may want to use a larger model like\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://huggingface.co/Mozilla/Mistral-7B-Instruct-v0.2-llamafile/resolve/main/mistral-7b-instruct-v0.2.Q4_0.llamafile?download=true\" rel=\"noreferrer noopener\">\n   Mistral-7B-Instruct\n  </a>\n  (5.15 GB,\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://huggingface.co/Mozilla/Mistral-7B-Instruct-v0.2-llamafile\" rel=\"noreferrer noopener\">\n   model info\n  </a>\n  ).\n </p>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  Step 2: Make the llamafile executable\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  If you didn't download the llamafile from the command line, figure out where your browser stored your downloaded llamafile.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Now, open your computer's terminal and, if necessary, go to the directory where your llamafile is stored: `cd path/to/downloaded/llamafile`\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   If you're using macOS, Linux, or BSD\n  </strong>\n  , you'll need to grant permission for your computer to execute this new file. (You only need to do this once.):\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  <strong>\n   If you're on Windows, instead just rename the file by adding \".exe\" on the end\n  </strong>\n  e.g. rename `TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile` to `TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile.exe`\n </p>\n <pre><code>chmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile</code></pre>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  Kick the tires\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Now, your llamafile should be ready to go. First, you can check which version of the llamafile library was used to build the llamafile binary you should downloaded:\n </p>\n <pre><code>./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --version\n\nllamafile v0.7.0</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  This post was written using a model built with `llamafile v0.7.0`. If your llamafile displays a different version and some of the steps below don't work as expected, please\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/Mozilla-Ocho/llamafile/issues\" rel=\"noreferrer noopener\">\n   post an issue on the llamafile issue tracker\n  </a>\n  .\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  The easiest way to use your llamafile is via its built-in chat interface. In a terminal, run\n </p>\n <pre><code>./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Your browser should open automatically and display a chat interface. (If it doesn't, just open your browser and point it at http://localhost:8080). When you're done chatting, return to your terminal and hit `Control-C` to shut down llamafile. If you're running these commands inside a notebook, just interrupt the notebook kernel to stop the llamafile.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In the rest of this walkthrough, we'll be using the llamafile's built-in inference server instead of the browser interface. The llamafile's server provides a REST API for interacting with the TinyLlama LLM via HTTP. Full server API documentation is available\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/Mozilla-Ocho/llamafile/blob/main/llama.cpp/server/README.md#api-endpoints\" rel=\"noreferrer noopener\">\n   here\n  </a>\n  . To start the llamafile in server mode, run:\n </p>\n <pre><code>./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding</code></pre>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  Summary: Download and run a llamafile\n </h3>\n <pre><code><span class=\"hljs-comment\"># 1. Download the llamafile-ized model</span>\nwget https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile\n\n<span class=\"hljs-comment\"># 2. Make it executable (you only need to do this once)</span>\n<span class=\"hljs-built_in\">chmod</span> +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile\n\n<span class=\"hljs-comment\"># 3. Run in server mode</span>\n./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding</code></pre>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Build a research assistant using LlamaIndex and llamafile\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Now, we'll show how to use LlamaIndex with your llamafile to build a research assistant to help you learn about some topic of interest--for this post, we chose\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://en.wikipedia.org/wiki/Homing_pigeon\" rel=\"noreferrer noopener\">\n   homing pigeons\n  </a>\n  . We'll show how to prepare your data, index into a vector store, then query it.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  One of the nice things about running an LLM locally is privacy. You can mix both \"public data\" like Wikipedia pages and \"private data\" without worrying about sharing your data with a third party. Private data could include e.g. your private notes on a topic or PDFs of classified content. As long as you use a local LLM (and a local vector store), you won't have to worry about leaking data. Below, we'll show how to combine both types of data. Our vector store will include Wikipedia pages, an Army manual on caring for homing pigeons, and some brief notes we took while we were reading about this topic.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  To get started, download our example data:\n </p>\n <pre><code><span class=\"hljs-built_in\">mkdir</span> data\n\n<span class=\"hljs-comment\"># Download 'The Homing Pigeon' manual from Project Gutenberg</span>\nwget https://www.gutenberg.org/cache/epub/55084/pg55084.txt -O data/The_Homing_Pigeon.txt\n\n<span class=\"hljs-comment\"># Download some notes on homing pigeons</span>\nwget https://gist.githubusercontent.com/k8si/edf5a7ca2cc3bef7dd3d3e2ca42812de/raw/24955ee9df819e21975b1dd817938c1bfe955634/homing_pigeon_notes.md -O data/homing_pigeon_notes.md</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Next, we'll need to install LlamaIndex and a few of its integrations:\n </p>\n <pre><code><span class=\"hljs-comment\"># Install llama-index</span>\npip install llama-index-core\n<span class=\"hljs-comment\"># Install llamafile integrations and SimpleWebPageReader</span>\npip install llama-index-embeddings-llamafile llama-index-llms-llamafile llama-index-readers-web</code></pre>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  Start your llamafile server and configure LlamaIndex\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In this example, we'll use the same llamafile to both produce the embeddings that will get indexed in our vector store and as the LLM that will answer queries later on. (However, there is no reason you can't use one llamafile for the embeddings and separate llamafile for the LLM functionality--you would just need to start the llamafile servers on different ports.)\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  To start the llamafile server, open a terminal and run:\n </p>\n <pre><code>./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding --port 8080</code></pre>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Now, we'll configure LlamaIndex to use this llamafile:\n </p>\n <pre><code><span class=\"hljs-comment\"># Configure LlamaIndex</span>\n<span class=\"hljs-keyword\">from</span> llama_index.core <span class=\"hljs-keyword\">import</span> Settings\n<span class=\"hljs-keyword\">from</span> llama_index.embeddings.llamafile <span class=\"hljs-keyword\">import</span> LlamafileEmbedding\n<span class=\"hljs-keyword\">from</span> llama_index.llms.llamafile <span class=\"hljs-keyword\">import</span> Llamafile\n<span class=\"hljs-keyword\">from</span> llama_index.core.node_parser <span class=\"hljs-keyword\">import</span> SentenceSplitter\n\nSettings.embed_model = LlamafileEmbedding(base_url=<span class=\"hljs-string\">\"http://localhost:8080\"</span>)\n\nSettings.llm = Llamafile(\n\tbase_url=<span class=\"hljs-string\">\"http://localhost:8080\"</span>,\n\ttemperature=<span class=\"hljs-number\">0</span>,\n\tseed=<span class=\"hljs-number\">0</span>\n)\n\n<span class=\"hljs-comment\"># Also set up a sentence splitter to ensure texts are broken into semantically-meaningful chunks (sentences) that don't take up the model's entire</span>\n<span class=\"hljs-comment\"># context window (2048 tokens). Since these chunks will be added to LLM prompts as part of the RAG process, we want to leave plenty of space for both</span>\n<span class=\"hljs-comment\"># the system prompt and the user's actual question.</span>\nSettings.transformations = [\n\tSentenceSplitter(\n    \tchunk_size=<span class=\"hljs-number\">256</span>,\n    \tchunk_overlap=<span class=\"hljs-number\">5</span>\n\t)\n]</code></pre>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  Prepare your data and build a vector store\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Now, we'll load our data and index it.\n </p>\n <pre><code><span class=\"hljs-comment\"># Load local data</span>\n<span class=\"hljs-keyword\">from</span> llama_index.core <span class=\"hljs-keyword\">import</span> SimpleDirectoryReader\nlocal_doc_reader = SimpleDirectoryReader(input_dir=<span class=\"hljs-string\">'./data'</span>)\ndocs = local_doc_reader.load_data(show_progress=<span class=\"hljs-literal\">True</span>)\n\n<span class=\"hljs-comment\"># We'll load some Wikipedia pages as well</span>\n<span class=\"hljs-keyword\">from</span> llama_index.readers.web <span class=\"hljs-keyword\">import</span> SimpleWebPageReader\nurls = [\n\t<span class=\"hljs-string\">'https://en.wikipedia.org/wiki/Homing_pigeon'</span>,\n\t<span class=\"hljs-string\">'https://en.wikipedia.org/wiki/Magnetoreception'</span>,\n]\nweb_reader = SimpleWebPageReader(html_to_text=<span class=\"hljs-literal\">True</span>)\ndocs.extend(web_reader.load_data(urls))\n\n<span class=\"hljs-comment\"># Build the index</span>\n<span class=\"hljs-keyword\">from</span> llama_index.core <span class=\"hljs-keyword\">import</span> VectorStoreIndex\n\nindex = VectorStoreIndex.from_documents(\n\tdocs,\n\tshow_progress=<span class=\"hljs-literal\">True</span>,\n)\n\n<span class=\"hljs-comment\"># Save the index</span>\nindex.storage_context.persist(persist_dir=<span class=\"hljs-string\">\"./storage\"</span>)</code></pre>\n <h3 class=\"Text_text__zPO0D Text_text-size-40__fIyvA\">\n  Query your research assistant\n </h3>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  Finally, we're ready to ask some questions about homing pigeons.\n </p>\n <pre><code>query_engine = index.as_query_engine()\n<span class=\"hljs-built_in\">print</span>(query_engine.query(<span class=\"hljs-string\">\"What were homing pigeons used for?\"</span>))</code></pre>\n <pre><code>\tHoming pigeons were used for a variety of purposes, including military reconnaissance, communication, and transportation. They were also used for scientific research, such as studying the behavior of birds in flight and their migration patterns. In addition, they were used for religious ceremonies and as a symbol of devotion and loyalty. Overall, homing pigeons played an important role in the history of aviation and were a symbol of the human desire for communication and connection.</code></pre>\n <pre><code><span class=\"hljs-built_in\">print</span>(query_engine.query(<span class=\"hljs-string\">\"When were homing pigeons first used?\"</span>))</code></pre>\n <pre><code>The context information provided in the given context is that homing pigeons were first used in the 19th century. However, prior knowledge would suggest that homing pigeons have been used for navigation and communication for centuries.</code></pre>\n <h2 class=\"Text_text__zPO0D Text_text-size-48__A2f8Q\">\n  Conclusion\n </h2>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  In this post, we've shown how to download and set up an LLM running locally via llamafile. Then, we showed how to use this LLM with LlamaIndex to build a simple RAG-based research assistant for learning about homing pigeons. Your assistant ran 100% locally: you didn't have to pay for API calls or send data to a third party.\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  As a next step, you could try running the examples above with a better model like\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://huggingface.co/Mozilla/Mistral-7B-Instruct-v0.2-llamafile/resolve/main/mistral-7b-instruct-v0.2.Q4_0.llamafile?download=true\" rel=\"noreferrer noopener\">\n   Mistral-7B-Instruct\n  </a>\n  . You could also try building a research assistant for different topic like \"semiconductors\" or \"how to bake bread\".\n </p>\n <p class=\"Text_text__zPO0D Text_text-size-16__PkjFu\">\n  To find out more about llamafile, check out the project on\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://github.com/Mozilla-Ocho/llamafile\" rel=\"noreferrer noopener\">\n   GitHub\n  </a>\n  , read this\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://justine.lol/oneliners/\" rel=\"noreferrer noopener\">\n   blog post\n  </a>\n  on bash one-liners using LLMs, or say hi to the community on\n  <a class=\"SanityPortableText_link__QA4Ze\" href=\"https://discord.com/invite/teDuGYVTB2\" rel=\"noreferrer noopener\">\n   Discord\n  </a>\n  .\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 19392, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a7eb7f5e-5364-48ce-aa1f-77e9aa159c2c": {"__data__": {"id_": "a7eb7f5e-5364-48ce-aa1f-77e9aa159c2c", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/using-llms-for-retrieval-and-reranking-23cf2d3a14b6.html", "file_name": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.html", "file_type": "text/html", "file_size": 17641, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/using-llms-for-retrieval-and-reranking-23cf2d3a14b6.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/using-llms-for-retrieval-and-reranking-23cf2d3a14b6.html", "file_name": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.html", "file_type": "text/html", "file_size": 17641, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "1b88e4f8f641450fb09a9fad4d906ee35f926083d5bd2fa7ed1c5ea32ba20e9e", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h1>\n  <strong>\n   Summary\n  </strong>\n </h1>\n <p>\n  This blog post outlines some of the core abstractions we have created in\n  <a href=\"https://github.com/jerryjliu/llama_index\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaIndex\n  </a>\n  around LLM-powered retrieval and reranking, which helps to create enhancements to document retrieval beyond naive top-k embedding-based lookup.\n </p>\n <p>\n  LLM-powered retrieval can return more relevant documents than embedding-based retrieval, with the tradeoff being much higher latency and cost. We show how using embedding-based retrieval as a first-stage pass, and second-stage retrieval as a reranking step can help provide a happy medium. We provide results over the Great Gatsby and the Lyft SEC 10-k.\n </p>\n <figure>\n  <figcaption class=\"pq fe pr pc pd ps pt be b bf z dt\">\n   Two-stage retrieval pipeline: 1) Top-k embedding retrieval, then 2) LLM-based reranking\n  </figcaption>\n </figure>\n <h1>\n  <strong>\n   Introduction and Background\n  </strong>\n </h1>\n <p>\n  There has been a wave of \u201cBuild a chatbot over your data\u201d applications in the past few months, made possible with frameworks like\n  <a href=\"https://github.com/jerryjliu/llama_index\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaIndex\n  </a>\n  and\n  <a href=\"https://github.com/hwchase17/langchain\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LangChain\n  </a>\n  . A lot of these applications use a standard stack for retrieval augmented generation (RAG):\n </p>\n <ul>\n  <li>\n   Use a vector store to store unstructured documents (knowledge corpus)\n  </li>\n  <li>\n   Given a query, use a\n   <strong>\n    retrieval model\n   </strong>\n   to retrieve relevant documents from the corpus, and a\n   <strong>\n    synthesis model\n   </strong>\n   to generate a response.\n  </li>\n  <li>\n   The\n   <strong>\n    retrieval model\n   </strong>\n   fetches\n   the top-k documents by embedding similarity to the query.\n  </li>\n </ul>\n <p>\n  In this stack, the retrieval model is not a novel idea; the concept of top-k embedding-based semantic search has been around for at least a decade, and doesn\u2019t involve the LLM at all.\n </p>\n <p>\n  There are a lot of benefits to embedding-based retrieval:\n </p>\n <ul>\n  <li>\n   It\u2019s very fast to compute dot products. Doesn\u2019t require any model calls during query-time.\n  </li>\n  <li>\n   Even if not perfect, embeddings can encode the semantics of the document and query reasonably well. There\u2019s a class of queries where embedding-based retrieval returns very relevant results.\n  </li>\n </ul>\n <p>\n  Yet for a variety of reasons, embedding-based retrieval can be imprecise and return irrelevant context to the query, which in turn degrades the quality of the overall RAG system, regardless of the quality of the LLM.\n </p>\n <p>\n  This is also not a new problem: one approach to resolve this in existing IR and recommendation systems is to create a\n  <strong>\n   two stage process\n  </strong>\n  . The first stage uses embedding-based retrieval with a high top-k value to maximize recall while accepting a lower precision. Then the second stage uses a slightly more computationally expensive process that is higher precision and lower recall (for instance with BM25) to \u201crerank\u201d the existing retrieved candidates.\n </p>\n <p>\n  Covering the downsides of embedding-based retrieval is worth an entire series of blog posts. This blog post is an initial exploration of an alternative retrieval method and how it can (potentially) augment embedding-based retrieval methods.\n </p>\n <h1>\n  <strong>\n   LLM Retrieval and Reranking\n  </strong>\n </h1>\n <p>\n  Over the past week, we\u2019ve developed a variety of initial abstractions around the concept of \u201cLLM-based\u201d retrieval and reranking. At a high-level, this approach uses the LLM to decide which document(s) / text chunk(s) are relevant to the given query. The input prompt would consist of a set of candidate documents, and the LLM is tasked with selecting the relevant set of documents as well as scoring their relevance with an internal metric.\n </p>\n <figure>\n  <figcaption class=\"pq fe pr pc pd ps pt be b bf z dt\">\n   Simple diagram of how LLM-based retrieval works\n  </figcaption>\n </figure>\n <p>\n  An example prompt would look like the following:\n </p>\n <pre><code>A list of documents is shown below. Each document has a number next to it along with a summary of the document. A question is also provided.\n  Respond with the numbers of the documents you should consult to answer the question, in order of relevance, as well\n  as the relevance score. The relevance score is a number from 1\u201310 based on how relevant you think the document is to the question.\n  Do not include any documents that are not relevant to the question.\n  Example format:\n  Document 1:\n  &lt;summary of document 1&gt;\n  Document 2:\n  &lt;summary of document 2&gt;\n  \u2026\n  Document 10:\n  &lt;summary of document 10&gt;\n  Question: &lt;question&gt;\n  Answer:\n  Doc: 9, Relevance: 7\n  Doc: 3, Relevance: 4\n  Doc: 7, Relevance: 3\n  Let's try this now:\n  {context_str}\n  Question: {query_str}\n  Answer:</code></pre>\n <p>\n  The prompt format implies that the text for each document should be relatively concise. There are two ways of feeding in the text to the prompt corresponding to each document:\n </p>\n <ul>\n  <li>\n   You can directly feed in the raw text corresponding to the document. This works well if the document corresponds to a bite-sized text chunk.\n  </li>\n  <li>\n   You can feed in a condensed summary for each document. This would be preferred if the document itself corresponds to a long-piece of text. We do this under the hood with our new\n   <a href=\"https://medium.com/llamaindex-blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec\" rel=\"noopener\">\n    document summary index\n   </a>\n   , but you can also choose to do it yourself.\n  </li>\n </ul>\n <p>\n  Given a collection of documents, we can then create document \u201cbatches\u201d and send each batch into the LLM input prompt. The output of each batch would be the set of relevant documents + relevance scores within that batch. The final retrieval response would aggregate relevant documents from all batches.\n </p>\n <p>\n  You can use our abstractions in two forms: as a standalone retriever module (\n  <code class=\"cw qm qn qo qe b\">\n   ListIndexLLMRetriever\n  </code>\n  ) or a reranker module (\n  <code class=\"cw qm qn qo qe b\">\n   LLMRerank\n  </code>\n  ). The remainder of this blog primarily focuses on the reranker module given the speed/cost.\n </p>\n <h2>\n  <strong>\n   LLM Retriever\n  </strong>\n  <code class=\"cw qm qn qo qe b\">\n   (ListIndexLLMRetriever)\n  </code>\n </h2>\n <p>\n  This module is defined over a list index, which simply stores a set of nodes as a flat list. You can build the list index over a set of documents and then use the LLM retriever to retrieve the relevant documents from the index.\n </p>\n <pre><span class=\"qh nb gt qe b bf qi qj l qk ql\" id=\"6811\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> GPTListIndex\n<span class=\"hljs-keyword\">from</span> llama_index.indices.<span class=\"hljs-built_in\">list</span>.retrievers <span class=\"hljs-keyword\">import</span> ListIndexLLMRetriever\nindex = GPTListIndex.from_documents(documents, service_context=service_context)\n<span class=\"hljs-comment\"># high - level API</span>\nquery_str = <span class=\"hljs-string\">\"What did the author do during his time in college?\"</span>\nretriever = index.as_retriever(retriever_mode=<span class=\"hljs-string\">\"llm\"</span>)\nnodes = retriever.retrieve(query_str)\n<span class=\"hljs-comment\"># lower-level API</span>\nretriever = ListIndexLLMRetriever()\nresponse_synthesizer = ResponseSynthesizer.from_args()\nquery_engine = RetrieverQueryEngine(retriever=retriever, response_synthesizer=response_synthesizer)\nresponse = query_engine.query(query_str)</span></pre>\n <p>\n  <strong>\n   Use Case:\n  </strong>\n  This could potentially be used in place of our vector store index. You use the LLM instead of embedding-based lookup to select the nodes.\n </p>\n <h2>\n  <strong>\n   LLM Reranker (LLMRerank)\n  </strong>\n </h2>\n <p>\n  This module is defined as part of our\n  <code class=\"cw qm qn qo qe b\">\n   NodePostprocessor\n  </code>\n  abstraction, which is defined for second-stage processing after an initial retrieval pass.\n </p>\n <p>\n  The postprocessor can be used on its own or as part of a\n  <code class=\"cw qm qn qo qe b\">\n   RetrieverQueryEngine\n  </code>\n  call. In the below example we show how to use the postprocessor as an independent module after an initial retriever call from a vector index.\n </p>\n <pre><span class=\"qh nb gt qe b bf qi qj l qk ql\" id=\"0ef3\"><span class=\"hljs-keyword\">from</span> llama_index.indices.query.schema <span class=\"hljs-keyword\">import</span> QueryBundle\nquery_bundle = QueryBundle(query_str)\n<span class=\"hljs-comment\"># configure retriever</span>\nretriever = VectorIndexRetriever(\nindex=index,\nsimilarity_top_k=vector_top_k,\n)\nretrieved_nodes = retriever.retrieve(query_bundle)\n<span class=\"hljs-comment\"># configure reranker</span>\nreranker = LLMRerank(choice_batch_size=<span class=\"hljs-number\">5</span>, top_n=reranker_top_n, service_context=service_context)\nretrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)</span></pre>\n <h2>\n  <strong>\n   Limitations/Caveats\n  </strong>\n </h2>\n <p>\n  There are certain limitations and caveats to LLM-based retrieval, especially with this initial version.\n </p>\n <ul>\n  <li>\n   LLM-based retrieval is orders of magnitude slower than embedding-based retrieval. Embedding search over thousands or even millions of embeddings can take less than a second. Each LLM prompt of 4000 tokens to OpenAI can take minutes to complete.\n  </li>\n  <li>\n   Using third-party LLM API\u2019s costs money.\n  </li>\n  <li>\n   The current method of batching documents may not be optimal, because it relies on an assumption that document batches can be scored independently of each other. This lacks a global view of the ranking for all documents.\n  </li>\n </ul>\n <p>\n  Using the LLM to retrieve and rank every node in the document corpus can be prohibitively expensive. This is why using the LLM as a second-stage reranking step, after a first-stage embedding pass, can be helpful.\n </p>\n <h1>\n  <strong>\n   Initial Experimental Results\n  </strong>\n </h1>\n <p>\n  Let\u2019s take a look at how well LLM reranking works!\n </p>\n <p>\n  We show some comparisons between naive top-k embedding-based retrieval as well as the two-stage retrieval pipeline with a first-stage embedding-retrieval filter and second-stage LLM reranking. We also showcase some results of pure LLM-based retrieval (though we don\u2019t showcase as many results given that it tends to run a lot slower than either of the first two approaches).\n </p>\n <p>\n  We analyze results over two very different sources of data: the Great Gatsby and the 2021 Lyft SEC 10-k. We only analyze results over the \u201cretrieval\u201d portion and not synthesis to better isolate the performance of different retrieval methods.\n </p>\n <p>\n  The results are presented in a qualitative fashion. A next step would definitely be more comprehensive evaluation over an entire dataset!\n </p>\n <h2>\n  <strong>\n   The Great Gatsby\n  </strong>\n </h2>\n <p>\n  In our first example, we load in the Great Gatsby as a\n  <code class=\"cw qm qn qo qe b\">\n   Document\n  </code>\n  object, and build a vector index over it (with chunk size set to 512).\n </p>\n <pre><span class=\"qh nb gt qe b bf qi qj l qk ql\" id=\"78ed\"># LLM Predictor (gpt-3.5-turbo) + service context\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size_limit=512)\n# load documents\ndocuments = SimpleDirectoryReader('../../../examples/gatsby/data').load_data()\nindex = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)</span></pre>\n <p>\n  We then define a\n  <code class=\"cw qm qn qo qe b\">\n   get_retrieved_nodes\n  </code>\n  function \u2014 this function can either do just embedding-based retrieval over the index, or embedding-based retrieval + reranking.\n </p>\n <pre><span class=\"qh nb gt qe b bf qi qj l qk ql\" id=\"d46b\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">get_retrieved_nodes</span>(<span class=\"hljs-params\">\n    query_str, vector_top_k=<span class=\"hljs-number\">10</span>, reranker_top_n=<span class=\"hljs-number\">3</span>, with_reranker=<span class=\"hljs-literal\">False</span>\n</span>):\n  query_bundle = QueryBundle(query_str)\n  <span class=\"hljs-comment\"># configure retriever</span>\n  retriever = VectorIndexRetriever(\n    index=index,\n    similarity_top_k=vector_top_k,\n  )\n  retrieved_nodes = retriever.retrieve(query_bundle)\n  <span class=\"hljs-keyword\">if</span> with_reranker:\n    <span class=\"hljs-comment\"># configure reranker</span>\n    reranker = LLMRerank(choice_batch_size=<span class=\"hljs-number\">5</span>, top_n=reranker_top_n, service_context=service_context)\n    retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n  <span class=\"hljs-keyword\">return</span> retrieved_nodes</span></pre>\n <p>\n  We then ask some questions. With embedding-based retrieval we set k=3. With two-stage retrieval we set k=10 for embedding retrieval and n=3 for LLM-based reranking.\n </p>\n <p>\n  <strong>\n   Question: \u201dWho was driving the car that hit Myrtle?\u201d\n  </strong>\n </p>\n <p>\n  (For those of you who are not familiar with the Great Gatsby, the narrator finds out later on from Gatsby that Daisy was actually the one driving the car, but Gatsby takes the blame for her).\n </p>\n <p>\n  The top retrieved contexts are shown in the images below. We see that in embedding-based retrieval, the top two texts contain semantics of the car crash but give no details as to who was actually responsible. Only the third text contains the proper answer.\n </p>\n <figure>\n  <figcaption class=\"pq fe pr pc pd ps pt be b bf z dt\">\n   Retrieved context using top-k embedding lookup (baseline)\n  </figcaption>\n </figure>\n <p>\n  In contrast, the two-stage approach returns just one relevant context, and it contains the correct answer.\n </p>\n <figure>\n  <figcaption class=\"pq fe pr pc pd ps pt be b bf z dt\">\n   Retrieved context using two-stage pipeline (embedding lookup then rerank)\n  </figcaption>\n </figure>\n <h2>\n  <strong>\n   2021 Lyft SEC 10-K\n  </strong>\n </h2>\n <p>\n  We want to ask some questions over the 2021 Lyft SEC 10-K, specifically about the COVID-19 impacts and responses. The Lyft SEC 10-K is 238 pages long, and a ctrl-f for \u201cCOVID-19\u201d returns 127 matches.\n </p>\n <p>\n  We use a similar setup as the Gatsby example above. The main differences are that we set the chunk size to 128 instead of 512, we set k=5 for the embedding retrieval baseline, and an embedding k=40 and reranker n=5 for the two-stage approach.\n </p>\n <p>\n  We then ask the following questions and analyze the results.\n </p>\n <p>\n  <strong>\n   Question: \u201cWhat initiatives are the company focusing on independently of COVID-19?\u201d\n  </strong>\n </p>\n <p>\n  Results for the baseline are shown in the image above. We see that results corresponding to indices 0, 1, 3, 4, are about measures directly in response to Covid-19, even though the question was specifically about company initiatives that were independent of the COVID-19 pandemic.\n </p>\n <figure>\n  <figcaption class=\"pq fe pr pc pd ps pt be b bf z dt\">\n   Retrieved context using top-k embedding lookup (baseline)\n  </figcaption>\n </figure>\n <p>\n  We get more relevant results in approach 2, by widening the top-k to 40 and then using an LLM to filter for the top-5 contexts. The independent company initiatives include \u201cexpansion of Light Vehicles\u201d (1), \u201cincremental investments in brand/marketing\u201d (2), international expansion (3), and accounting for misc. risks such as natural disasters and operational risks in terms of financial performance (4).\n </p>\n <figure>\n  <figcaption class=\"pq fe pr pc pd ps pt be b bf z dt\">\n   Retrieved context using two-stage pipeline (embedding lookup then rerank)\n  </figcaption>\n </figure>\n <h1>\n  <strong>\n   Conclusion\n  </strong>\n </h1>\n <p>\n  That\u2019s it for now! We\u2019ve added some initial functionality to help support LLM-augmented retrieval pipelines, but of course there\u2019s a ton of future steps that we couldn\u2019t quite get to. Some questions we\u2019d love to explore:\n </p>\n <ul>\n  <li>\n   How our LLM reranking implementation compares to other reranking methods (e.g. BM25, Cohere Rerank, etc.)\n  </li>\n  <li>\n   What the optimal values of embedding top-k and reranking top-n are for the two stage pipeline, accounting for latency, cost, and performance.\n  </li>\n  <li>\n   Exploring different prompts and text summarization methods to help determine document relevance\n  </li>\n  <li>\n   Exploring if there\u2019s a class of applications where LLM-based retrieval on its own would suffice, without embedding-based filtering (maybe over smaller document collections?)\n  </li>\n </ul>\n <h2>\n  Resources\n </h2>\n <p>\n  You can play around with the notebooks yourself!\n </p>\n <p>\n  <a href=\"https://github.com/jerryjliu/llama_index/blob/main/docs/examples/node_postprocessor/LLMReranker-Gatsby.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Great Gatsby Notebook\n  </a>\n </p>\n <p>\n  <a href=\"https://github.com/jerryjliu/llama_index/blob/main/docs/examples/node_postprocessor/LLMReranker-Lyft-10k.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   2021 Lyft 10-K Notebook\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 17566, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5612d787-8f42-4b63-90d7-ab5d6e37df5c": {"__data__": {"id_": "5612d787-8f42-4b63-90d7-ab5d6e37df5c", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/vellum-llamaindex-integration-58b476a1e33f.html", "file_name": "vellum-llamaindex-integration-58b476a1e33f.html", "file_type": "text/html", "file_size": 9037, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/vellum-llamaindex-integration-58b476a1e33f.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/vellum-llamaindex-integration-58b476a1e33f.html", "file_name": "vellum-llamaindex-integration-58b476a1e33f.html", "file_type": "text/html", "file_size": 9037, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "5fafb46bc77becfed63072af8c423cf799d5d2142ca41be59e18b800dd4e1d46", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  <strong>\n   Co-Authors:\n  </strong>\n </p>\n <ul>\n  <li>\n   Akash Sharma, founder and CEO, Vellum\n  </li>\n  <li>\n   Jerry Liu, co-founder and CEO, LlamaIndex\n  </li>\n </ul>\n <h1>\n  About Us\n </h1>\n <p>\n  The central mission of\n  <a href=\"https://github.com/jerryjliu/llama_index\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaIndex\n  </a>\n  is to provide an interface between Large Language Models (LLM\u2019s), and your private, external data. Over the past few months, it has become one of the most popular open-source frameworks for LLM data augmentation (context-augmented generation), for a variety of use cases: question-answering, summarization, structured queries, and more.\n </p>\n <p>\n  <a href=\"http://vellum.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Vellum\n  </a>\n  is a developer platform to build high quality LLM applications. The platform provides best-in-class tooling for prompt engineering, unit testing, regression testing, monitoring &amp; versioning of in-production traffic and model fine tuning. Vellum\u2019s platform helps companies save countless engineering hours to build internal tooling and instead use that time to build end user facing applications.\n </p>\n <h1>\n  Why we partnered on this integration\n </h1>\n <p>\n  Until recently, LlamaIndex users did not have a way to do prompt engineering and unit testing pre-production and versioning/monitoring the prompts post production. Prompt engineering and unit testing is key to ensure that your LLM feature is producing reliable results in production. Here\u2019s an example of simple prompt that produces vastly different results between GPT-3, GPT-3.5 and GPT-4:\n </p>\n <h2>\n  Unit testing your prompts\n </h2>\n <p>\n  Creating a unit test bank is a proactive approach to ensure prompt reliability \u2014 it\u2019s best practice to run 50\u2013100 test cases before putting prompts in production. The test bank should comprise scenarios &amp; edge cases anticipated in production, think of this as QAing your feature before it goes to production. The prompts should \u201cpass\u201d these test cases based on your evaluation criteria. Use Vellum Test Suites to upload test cases in bulk via CSV upload.\n </p>\n <h2>\n  Regression testing in production\n </h2>\n <p>\n  Despite how well you test before sending a prompt in production, edge cases can appear when in production. This is expected, so no stress! Through the Vellum integration, LlamaIndex users can change prompts and get prompt versioning without making any code changes. While doing that, however, it\u2019s best practice to run historical inputs that were sent to the prompt in production to the new prompt and confirm it doesn\u2019t break any existing behavior. LLMs are sometimes unpredictable, even changing the word \u201cgood\u201d to \u201cgreat\u201d in a prompt can result in differing outputs!\n </p>\n <h1>\n  Best practices to leverage the integration\n </h1>\n <h2>\n  How to access the integration\n </h2>\n <p>\n  <a href=\"https://github.com/jerryjliu/llama_index/blob/main/examples/vellum/Vellum%20Integration%20Demo.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   This\n  </a>\n  demo notebook goes into detail on how you can use Vellum to manage prompts within LlamaIndex.\n </p>\n <p>\n  <strong>\n   Prerequisites\n  </strong>\n </p>\n <ol>\n  <li>\n   Sign up for a free Vellum account at\n   <a href=\"https://app.vellum.ai/signup\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    app.vellum.ai/signup\n   </a>\n  </li>\n  <li>\n   Go to\n   <a href=\"https://app.vellum.ai/api-keys\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    app.vellum.ai/api-keys\n   </a>\n   and generate a Vellum API key. Note it down somewhere.\n  </li>\n </ol>\n <p>\n  <strong>\n   Auto-Register Prompts &amp; Make Predictions Through Vellum\n  </strong>\n </p>\n <p>\n  If you import a prompt in LlamaIndex, the VellumPredictor class will used to auto-register a prompt with Vellum to make predictions.\n </p>\n <p>\n  By registering a prompt with Vellum, Vellum will create:\n </p>\n <ol>\n  <li>\n   A \u201cSandbox\u201d \u2014 an environment where you can iterate on the prompt, it\u2019s model, provider, params, etc.; and\n  </li>\n  <li>\n   A \u201cDeployment\u201d \u2014 a thin API proxy between you and LLM providers and offering prompt versioning, request monitoring, and more\n  </li>\n </ol>\n <p>\n  You can use VellumPromptRegistry to retrieve information about the registered prompt and get links to open its corresponding Sandbox and Deployment in Vellum\u2019s UI. More details about Vellum\u2019s Sandbox and Deployment features can be found\n  <a href=\"https://www.notion.so/Vellum-LlamaIndex-integration-096fb3f141ac49c695b7fcb6c70c0519?pvs=21\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   here\n  </a>\n </p>\n <h2>\n  Prompt engineering tips in context augmented use cases\n </h2>\n <p>\n  Think of the Large Language Model as a smart college graduate that needs instructions if the task at hand is not clear. If you\u2019re not getting good results with the default prompt templates, follow these instructions:\n </p>\n <ol>\n  <li>\n   Add use case specific details to the prompt to guide what the model focuses on.\n  </li>\n  <li>\n   Create 5\u201310 input scenarios to test performance\n  </li>\n  <li>\n   Iterate a few times: (i) Tweak the prompt by adding more specific instructions or examples for the scenarios with bad results, (ii) Evaluate against the target response for each scenario\n  </li>\n  <li>\n   In parallel, test out different foundation models and model providers using Vellum\u2019s Sandbox. Maybe Claude or PaLM does better than GPT-4 for your use case.\n  </li>\n  <li>\n   If you would like additional reasoning or explanation, use a more prescriptive approach:\n  </li>\n </ol>\n <ul>\n  <li>\n   Add detailed step by step instructions to the end of the prompt and ask the LLM to walk though those steps when creating it\u2019s answer:\n  </li>\n  <li>\n   e.g. (1) \u2026 (2) \u2026 (3) \u2026 \u2026 (6) Output a JSON with the following typescript schema\n  </li>\n  <li>\n   This is convenient because it\u2019s simple to parse out the JSON blob from the LLM output\n  </li>\n  <li>\n   However this causes more tokens to be generated so is slower and costs more, but it\u2019s not nearly as expensive and slow as chaining multiple calls\n  </li>\n </ul>\n <h2>\n  Measuring prompt quality, before production\n </h2>\n <p>\n  One of the common reasons why evaluating LLM model quality is hard is that there\u2019s no defined framework. The evaluation metric depends on your use case. This\n  <a href=\"https://www.vellum.ai/blog/how-to-evaluate-the-quality-of-large-language-models-for-production-use-cases\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   blog\n  </a>\n  goes in more detail, but in summary, the evaluation approach depends on type of use case:\n </p>\n <ul>\n  <li>\n   <strong>\n    Classification:\n   </strong>\n   accuracy, recall, precision, F score and confusion matrices for a deeper evaluation\n  </li>\n  <li>\n   <strong>\n    Data extraction:\n   </strong>\n   Validate that the output is syntactically valid and the expected keys are present in the generated response\n  </li>\n  <li>\n   <strong>\n    SQL/Code generation:\n   </strong>\n   Validate that the output is syntactically valid and running it will return the expected values\n  </li>\n  <li>\n   <strong>\n    Creative output:\n   </strong>\n   Semantic similarity between model generated response and target response using cross-encoders\n  </li>\n </ul>\n <p>\n  Vellum\u2019s Sandbox and Test Suites offer\n  <strong>\n   Exact Match, Regex Match, Semantic Similarity &amp; Webhook\n  </strong>\n  as evaluation criteria. You get a clear indication of which test cases \u201cpass\u201d, given your evaluation criteria\n </p>\n <p>\n  <strong>\n   Testing in Vellum Sandbox\n  </strong>\n </p>\n <p>\n  <strong>\n   Testing in Vellum Test Suites\n  </strong>\n </p>\n <h2>\n  Measuring prompt quality, once in production\n </h2>\n <p>\n  User feedback is the ultimate source of truth for model quality \u2014 if there\u2019s a way for your users to either implicitly or explicitly tell you whether they the response is \u201cgood\u201d or \u201cbad,\u201d that\u2019s what you should track and improve!\n </p>\n <p>\n  Explicit user feedback is collected when your users respond with something like a \ud83d\udc4d or \ud83d\udc4e in your UI when interacting with the LLM output. Asking explicitly may not result in enough volume of feedback to measure overall quality. If your feedback collection rates are low, we suggest using implicit feedback if possible.\n </p>\n <p>\n  Implicit feedback is based on how users react to the output generated by the LLM. For example, if you generate a first draft of en email for a user and they send it without making edits, that\u2019s likely a good response! If they hit regenerate, or re-write the whole thing, that\u2019s probably not a good response. Implicit feedback collection may not be possible for all use cases, but it can be a powerful gauge of quality.\n </p>\n <p>\n  Use Vellum\u2019s Actuals endpoint to track the quality of each completion and track results in the Completions and Monitoring tabs of your Deployment.\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 8936, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fbb41713-78d4-4c48-87be-05927fc3dba6": {"__data__": {"id_": "fbb41713-78d4-4c48-87be-05927fc3dba6", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc.html", "file_name": "zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc.html", "file_type": "text/html", "file_size": 8815, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc.html", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc.html", "file_name": "zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc.html", "file_type": "text/html", "file_size": 8815, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "15e093f14b9f38934cc59942b4153a40a80aa2eb227f7efa3cd010aeec3c849f", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Editor\u2019s Note: This article was written by\n  <a href=\"https://twitter.com/danielchalef\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Daniel\n  </a>\n  at Zep for the LlamaIndex blog.\n </p>\n <p>\n  Zep is a\n  <a href=\"https://www.getzep.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   long-term memory store for LLM applications\n  </a>\n  . With Zep, developers can easily add relevant documents, chat history memory &amp; rich user data to LLM app prompts. Document and chat history storage, embedding, enrichment, and more are taken care of by the Zep service.\n </p>\n <p>\n  In this article, we demonstrate how to use Zep\u2019s new Document Vector Store with the (also new) ZepVectorStore for LlamaIndex.\n </p>\n <h1>\n  Installing Zep and some important concepts\n </h1>\n <p>\n  <a href=\"https://github.com/getzep/zep\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Zep is open source\n  </a>\n  and may be\n  <a href=\"https://docs.getzep.com/deployment/quickstart/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   installed via Docker\n  </a>\n  , or to Kubernetes and hosting platforms such as Render. SDKs are available for Python and TypeScript, and frameworks such as LangChain and LlamaIndex ship with support for Zep.\n </p>\n <p>\n  Zep stores documents in Collections, with the document text, embeddings, and metadata all colocated. This enables hybrid semantic search over a collection, with results filtered by JSONPath queries against document metadata. When using Zep with LlamaIndex, LlamaIndex filters are translated for use by Zep.\n </p>\n <p>\n  A document or document chunk is equivalent to a LlamaIndex TextNode or NodeWithEmbedding.\n </p>\n <p>\n  Collections can be optionally set to automatically embed texts using a service such as OpenAI or locally using an embedding model of your choice. However, when using Zep with LlamaIndex, we rely on LlamaIndex\u2019s integrations with embedded services and libraries.\n </p>\n <h1>\n  Creating a ZepVectorStore and Document Collection\n </h1>\n <p>\n  You will need to have\n  <a href=\"https://docs.getzep.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   installed Zep\n  </a>\n  and have your API URL and, optionally, authentication key handy.\n </p>\n <pre><span class=\"pt on gt pq b bf pu pv l pw px\" id=\"5267\"><span class=\"hljs-keyword\">from</span> llama_index.vector_stores <span class=\"hljs-keyword\">import</span> ZepVectorStore\nzep_api_url = <span class=\"hljs-string\">\"http://localhost:8000\"</span>\nzep_api_key = <span class=\"hljs-string\">\"&amp;lt;optional_jwt_token&amp;gt;\"</span>\ncollection_name = <span class=\"hljs-string\">\"babbage\"</span> <span class=\"hljs-comment\"># The name of a new or existing collection</span>\nembedding_dimensions = <span class=\"hljs-number\">1536</span> <span class=\"hljs-comment\"># the dimensions of the embedding model you intend to use</span>\nvector_store = ZepVectorStore(\n  api_url=zep_api_url,\n  api_key=zep_api_key,\n  collection_name=collection_name,\n  embedding_dimensions=embedding_dimensions\n)</span></pre>\n <p>\n  The collection name is a unique identifier for your vector index and should only contain alphanumeric characters. If the collection does not exist, Zep will automatically create one for you.\n </p>\n <h1>\n  Creating and populating an Index\n </h1>\n <p>\n  Below we\u2019ll use a common LlamaIndex pattern for loading content and adding it to an index. After loading the text data, we create a StorageContext backed by the ZepVectorStore.\n </p>\n <p>\n  We then create the index using our loaded documents and Zep-backed storage context.\n </p>\n <pre><span class=\"pt on gt pq b bf pu pv l pw px\" id=\"23b8\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> VectorStoreIndex, SimpleDirectoryReader\n<span class=\"hljs-keyword\">from</span> llama_index.storage.storage_context <span class=\"hljs-keyword\">import</span> StorageContext\n\ndocuments = SimpleDirectoryReader(<span class=\"hljs-string\">\"./babbages_calculating_engine/\"</span>).load_data()\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n\nquery = <span class=\"hljs-string\">\"the sun and stars\"</span>\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(query)\n\n\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-built_in\">str</span>(response))</span></pre>\n <pre><span class=\"pt on gt pq b bf pu pv l pw px\" id=\"22cb\">But one of the most signal examples of this kind, of which we are aware, is related by Mr Baily. The catalogue of stars published by the Astronomical Society was computed by two separate and independent persons, and was afterwards compared and examined with great care and attention by Mr Stratford. On examining this catalogue, and recalculating a portion of it, Mr Baily discovered an error in the case of the star</span></pre>\n <p>\n  Finally, we run a simple text query against the index and print the resulting node\u2019s text.\n </p>\n <h1>\n  Hybrid search with metadata filters\n </h1>\n <p>\n  As mentioned above, Zep also supports associating rich metadata with documents. This metadata can be an arbitrarily deep JSON structure. When working with LlamaIndex, we currently support filtering on top-level keys in the map.\n </p>\n <p>\n  The code below demonstrates running a vector search over an index and filtering on metadata using LlamaIndex\u2019s MetadataFilters. We print the result and the normalized cosine similarity for the matching result.\n </p>\n <pre><span class=\"pt on gt pq b bf pu pv l pw px\" id=\"d794\"><span class=\"hljs-keyword\">from</span> llama_index.schema <span class=\"hljs-keyword\">import</span> TextNode\n<span class=\"hljs-keyword\">from</span> llama_index.vector_stores.types <span class=\"hljs-keyword\">import</span> ExactMatchFilter, MetadataFilters\n\nnodes = [\n   TextNode(\n       text=<span class=\"hljs-string\">\"Not aware that tables of these squares existed, Bouvard, who calculated the tides for Laplace, underwent the labour of calculating the square of each individual sine in every case in which it occurred.\"</span>,\n       metadata={\n           <span class=\"hljs-string\">\"topic\"</span>: <span class=\"hljs-string\">\"math\"</span>,\n           <span class=\"hljs-string\">\"entities\"</span>: <span class=\"hljs-string\">\"laplace\"</span>,\n       },\n   ),\n   TextNode(\n       text=<span class=\"hljs-string\">\"Within the limits of the lunar orbit there are not less than one thousand stars, which are so situated as to be in the moon's path, and therefore to exhibit, at some period or other, those desirable occultations.\"</span>,\n       metadata={\n           <span class=\"hljs-string\">\"topic\"</span>: <span class=\"hljs-string\">\"astronomy\"</span>,\n           <span class=\"hljs-string\">\"entities\"</span>: <span class=\"hljs-string\">\"moon\"</span>,\n       },\n   ),\n]\n\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex(nodes, storage_context=storage_context)\n\n\nfilters = MetadataFilters(filters=[ExactMatchFilter(key=<span class=\"hljs-string\">\"topic\"</span>, value=<span class=\"hljs-string\">\"astronomy\"</span>)])\n\n\nretriever = index.as_retriever(filters=filters)\nresult = retriever.retrieve(<span class=\"hljs-string\">\"What is the structure of our galaxy?\"</span>)\n\n\n<span class=\"hljs-keyword\">for</span> r <span class=\"hljs-keyword\">in</span> result:\n   <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"\\n\"</span>, r.node.text, r.score)</span></pre>\n <pre><span class=\"pt on gt pq b bf pu pv l pw px\" id=\"54ba\">Within the limits of the lunar orbit there are not less than one thousand stars, which are so situated as to be in the moon's path, and therefore to exhibit, at some period or other, those desirable occultations.  0.6456785674</span></pre>\n <h1>\n  Summing it up\n </h1>\n <p>\n  Zep offers a single API for vector search over documents and chat history, allowing developers to populate prompts with both forms of long-term memory. LlamaIndex makes it extremely easy to populate Zep with content from a broad set of documents and data sources and query these sources when building prompts and other functionality for LLM apps.\n </p>\n <h1>\n  Next Steps\n </h1>\n <ul>\n  <li>\n   Read the\n   <a href=\"https://docs.getzep.com/deployment/quickstart/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Zep Quick Start Guide\n   </a>\n  </li>\n  <li>\n   <a href=\"https://github.com/jerryjliu/llama_index/tree/main/docs/examples/vector_stores/ZepIndexDemo.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Zep and LlamaIndex Walkthrough Notebook\n   </a>\n  </li>\n  <li>\n   <a href=\"https://gpt-index.readthedocs.io/en/stable/index.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Getting Started with LllamaIndex\n   </a>\n  </li>\n </ul>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 8802, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83001b14-fb2d-400e-b31b-3a48501408ce": {"__data__": {"id_": "83001b14-fb2d-400e-b31b-3a48501408ce", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.html", "file_name": "a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.html", "file_type": "text/html", "file_size": 24708, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about using the llama-index library to evaluate the performance of RAG systems based on seven measurement aspects outlined in a survey paper by Gao et al. The text highlights the evaluation notebook guides provided by the llama-index library and explains the concept of faithfulness, which is further explained in a Notion document. This text can answer questions related to the evaluation capabilities of the llama-index library, the measurement aspects outlined by Gao et al., and how to use the library to assess the performance of RAG systems in relation to these aspects.\n\nSome potential questions that this text can answer include:\n- What is the llama-index library and how can it be used to evaluate RAG systems?\n- What measurement aspects are outlined in the survey paper by Gao et al.?\n- How can the evaluation notebook guides provided by the llama-index library be used to assess the performance of RAG systems in relation to these measurement aspects?\n- What is faithfulness and how is it related to RAG systems?\n- How can the Notion document help in understanding the concept of faithfulness in relation to RAG systems?\n- What are some sophisticated techniques for building Advanced RAG systems, and how can they be applied using the llama-index library?\n\nOverall, this text provides insights into using the llama-index library to evaluate RAG systems based on the measurement aspects outlined by Gao et al., and can help builders assess the level to which their RAG system meets success requirements through these measurement aspects.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90d0dad3-83e4-48ff-969a-41a450639f14": {"__data__": {"id_": "90d0dad3-83e4-48ff-969a-41a450639f14", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.html", "file_name": "a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.html", "file_type": "text/html", "file_size": 11715, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text introduces a new index called Document Summary Index in the LLamaIndex data structure. This index allows for the extraction and indexing of unstructured text summaries for each document, which can then be used for retrieval and response synthesis. By storing summaries instead of text chunks, this approach can offer better retrieval performance compared to traditional semantic search. The index can be built using a ResponseSynthesizer and can be queried using a LLM-based or embedding-based retrieval approach. Some questions that this text can answer include:\n\n- What is the Document Summary Index in LLamaIndex?\n- How does the Document Summary Index differ from traditional semantic search approaches?\n- How can the Document Summary Index be built and queried using LLamaIndex?\n- What are some benefits of using the Document Summary Index for retrieval and response synthesis?\n- How can the Document Summary Index be extended to summarize larger text chunks or one-liners?\n- What are some next steps for exploring autosummarization in different layers and LLM-based retrieval using the Document Summary Index?\n\nOverall, the text describes the Document Summary Index as a \"middle ground\" between semantic search and brute-force summarization across all documents, as it allows for context beyond what is indexed in a specific text chunk, while also being more flexible and automatic than traditional approaches.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8459bd37-a59b-4eb1-b346-49c451ffd64b": {"__data__": {"id_": "8459bd37-a59b-4eb1-b346-49c451ffd64b", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/agentic-rag-with-llamaindex-2721b8a49ff6.html", "file_name": "agentic-rag-with-llamaindex-2721b8a49ff6.html", "file_type": "text/html", "file_size": 7713, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/agentic-rag-with-llamaindex-2721b8a49ff6.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about the concept of Agentic RAG, which is an implementation of RAG (Retrieval-as-a-Service) using agent technology. It explains how Agentic RAG can be incorporated into existing RAG pipelines for enhanced, conversational search and retrieval. The text provides an example implementation of Agentic RAG using LlamaIndex's approach, which creates a scaling architecture with smaller worker-agents managing subsets of documents. The implementation allows for the selection of relevant documents based on a user query, executes an agentic loop over the documents, including chain-of-thought, summarisation, and reranking. Some questions that this text can answer include:\n\n- What is Agentic RAG and how does it enhance RAG implementations?\n- How does the architecture of Agentic RAG allow for scaling and optimisation of agent functionality?\n- What principles are illustrated by this implementation of Agentic RAG?\n- How does the architecture of Agentic RAG serve as a reference framework for optimising agent functionality in scaling an agent?\n- How does Agentic RAG add intelligence and resilience to RAG implementations, and what are some sought-after enterprise LLM implementation types?\n- What are some key benefits of following an agent approach for RAG implementations, and how does this architecture expand over an organisation with more sub bots being added?\n- Where can one follow for updates on large language models related to Agentic RAG? (Answer: LinkedIn)", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78e2927e-d1d5-46d0-9f7d-a83732e7369b": {"__data__": {"id_": "78e2927e-d1d5-46d0-9f7d-a83732e7369b", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.html", "file_name": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.html", "file_type": "text/html", "file_size": 18536, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text introduces a voice-activated assistant named C3, which utilizes Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) applications to enhance accessibility in AI. C3 can answer general queries and document-specific inquiries through voice commands, making it suitable for individuals with typing challenges or accessibility issues. Some of the questions that this text can answer include requests for summaries, definitions, further information, solutions, and resource recommendations related to LLMs, RAG, and AI applications. The text also provides guidance on how to build a web-based AI voice assistant using JavaScript and Python, including setting up a backend server using Create-Llama, integrating a LLM and a wake word recognition model, implementing speech synthesis, and rendering a UI. The text emphasizes the importance of making AI tools accessible and user-friendly and encourages feedback and collaboration to further democratize AI.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77ba3a80-549e-47e9-bbf3-f70fbaa39686": {"__data__": {"id_": "77ba3a80-549e-47e9-bbf3-f70fbaa39686", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/announcing-llamaindex-0-9-719f03282945.html", "file_name": "announcing-llamaindex-0-9-719f03282945.html", "file_type": "text/html", "file_size": 18985, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/announcing-llamaindex-0-9-719f03282945.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is an announcement from the developers of LlamaIndex, a Python library used for building search engines. It highlights several significant updates and changes in LlamaIndex's latest release, version 0.9. These updates include flattening the interface for node parsing and metadata extraction, improved defaults for tokenization and token counting, reduced bloat in packaging, more consistent and predictable import paths for user-facing concepts, and the introduction of new modules for supporting multi-modal use cases with multi-modal LLMs, embeddings, and RAG. This text can answer questions such as what major changes have been made to LlamaIndex in version 0.9, why flattening the interface is significant, how tokenization and token counting have been improved, how users can install LangChain as part of their LlamaIndex installation, and what import path changes have been made in version 0.9 for user-facing concepts.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c07eb866-c51d-4e7a-b9af-1df89655b2d6": {"__data__": {"id_": "c07eb866-c51d-4e7a-b9af-1df89655b2d6", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications.html", "file_name": "arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications.html", "file_type": "text/html", "file_size": 3887, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about a strategic alliance and joint product launch between Arize AI and LlamaIndex. The text explains that the new joint offering, called LlamaTrace, is a hosted version of Arize OSS Phoenix that helps teams overcome technical challenges in deploying modern LLM systems for real-world use cases. The text highlights that 47.7% of AI engineers and developers are currently leveraging retrieval in their LLM applications, and that orchestration frameworks like LlamaIndex can accelerate generative AI development. The text also mentions that LlamaTrace works natively with the LlamaIndex and Arize ecosystems, and that it offers a fully hosted, online, persistent deployment option for teams that do not want to self-host. The text also includes quotes from the CEOs of Arize and LlamaIndex, discussing their shared vision for enabling the reduction of time it takes to deploy generative AI into production while ensuring business-critical use cases. \n\nThis text can answer questions such as:\n- Who are Arize AI and LlamaIndex, and what is their joint offering called LlamaTrace?\n- Why is Arize AI's CEO discussing a shared vision with LlamaIndex?\n- How can orchestration frameworks like LlamaIndex help accelerate generative AI development?\n- What technical challenges does LlamaTrace help teams overcome in deploying modern LLM systems?\n- How can LlamaTrace be accessed, and what options does it offer for deployment?\n- How does LlamaTrace work natively with the LlamaIndex and Arize ecosystems?\n- What benefits does LlamaTrace offer for AI engineers and developers in terms of experimentation, iteration, collaboration, and data processing?\n- How does LlamaTrace complement the data platform and orchestration framework offered by LlamaCloud and LlamaIndex?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c94c9c1-8eaa-45d4-befa-54e84f2ecebe": {"__data__": {"id_": "6c94c9c1-8eaa-45d4-befa-54e84f2ecebe", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/automate-online-tasks-with-multion-and-llamaindex.html", "file_name": "automate-online-tasks-with-multion-and-llamaindex.html", "file_type": "text/html", "file_size": 10641, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/automate-online-tasks-with-multion-and-llamaindex.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text describes a technical walkthrough on integrating MultiOn, an AI agents platform designed to facilitate autonomous completion of tasks in any web environment, with LlamaIndex, an orchestration framework that facilitates data ingestion, indexing, and querying. This integration allows developers to build AI agents that can manage online activities from start to finish, handling everything from simple data retrieval to complex interactions. The text provides a practical example where this technology is used to automate email interactions and web browsing, specifically with Gmail and Google Meet. Some of the questions that this text can answer include how to set up the AI agent with necessary configurations and API keys, how to integrate the Gmail search tool, how to initialize the agent with tools and a system prompt, and how to send email responses through MultiOn. It also mentions resources for further exploration, such as the LlamaHub page and a notebook on GitHub. Overall, the text discusses the potential of these technologies to automate and streamline online tasks, significantly impacting how developers interact with digital environments and manage data.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f32cd777-600a-4a8e-91f1-c5340cfd85ea": {"__data__": {"id_": "f32cd777-600a-4a8e-91f1-c5340cfd85ea", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/batch-inference-with-mymagic-ai-and-llamaindex.html", "file_name": "batch-inference-with-mymagic-ai-and-llamaindex.html", "file_type": "text/html", "file_size": 15092, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/batch-inference-with-mymagic-ai-and-llamaindex.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text consists of Amazon reviews summarized by a machine learning model. It highlights themes such as horror, comedy, drama, and thriller elements, as well as aspects like well-playing parents, decent dialogue, lack of horror elements, inconsistent genre, positive feedback, Woody Allen's signature style, and Scarlett Johansson's performance. The text can answer questions related to these themes and aspects, as well as provide insights into the reviews' overall ratings and criticisms. The second text describes the process of embedding and storing a document using LlamaIndex's query engine, explaining how to configure the embedding model and Llama3 model, create an index, and query engine. It provides an example query and explains how to optimize settings for the indexing pipeline to improve query performance. The text can answer questions related to embedding text, optimizing query performance, and storing and retrieving large amounts of text data using LlamaIndex. Some examples of questions that this text can answer include:\n\n- What are the least favorite movies among a set of reviews?\n- How can I efficiently store and retrieve large amounts of text data using LlamaIndex?\n- How can I configure the embedding model and Llama3 model for my specific use case?\n- How can I optimize the settings for the indexing pipeline to improve query performance?\n- How can I integrate MyMagic AI's API into my project for real-time inference and analysis of text data?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51cf26f6-53c6-47e1-8ff7-3b845c7744cc": {"__data__": {"id_": "51cf26f6-53c6-47e1-8ff7-3b845c7744cc", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/becoming-proficient-in-document-extraction-32aa13046ed5.html", "file_name": "becoming-proficient-in-document-extraction-32aa13046ed5.html", "file_type": "text/html", "file_size": 14667, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/becoming-proficient-in-document-extraction-32aa13046ed5.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses the revolutionary impact of two tools, Zephyr 7b LLM and LlamaIndex, on the field of document handling, specifically in terms of image-based document extraction. It highlights the limitations and obstacles of traditional OCR technology and explains how these tools address these issues through advanced machine learning algorithms, contextual comprehension, and adaptive image processing techniques. The text provides step-by-step instructions on how to use these tools to read and analyze receipts and extract data from images, and addresses limitations such as multilingual complexity, handwritten text, and cursive fonts. Some questions that this text can answer include how to extract data from images, how to handle documents in different languages, and how to address limitations of traditional OCR technology. The text also provides a list of links and resources for further exploration and support, including the author's Patreon page, Medium articles, Kaggle and Hugging Face profiles, YouTube channel, and LinkedIn profile.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e1d8446-49c6-4183-ad1a-6f5dfc6bc6da": {"__data__": {"id_": "2e1d8446-49c6-4183-ad1a-6f5dfc6bc6da", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.html", "file_name": "boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.html", "file_type": "text/html", "file_size": 21486, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is discussing the results of a study on the use of various open source models for embedding, retrieval, and reranking in a search engine called Cohere. The study found that a combination of OpenAI and JinaAI-Base embeddings, along with the CohereRerank/bge-reranker-large model, yielded the best hit rate and mean reciprocal rank (MRR) for search results. The text provides information on how to use these models in Retrieval Augmented Generation (RAG) pipelines, and how to determine the optimal combination of these models for top-notch retrieval performance using the Retrieval Evaluation module from LlamaIndex. Some of the questions that this text can answer include: Which embeddings and rerankers work best together for search results? How significant is the role of rerankers in improving search results? What is the impact of choosing the right embedding for the initial search? How can we find the right mix of embeddings and rerankers for optimal performance?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71360fb6-350f-4905-aef8-19f79ee01ff5": {"__data__": {"id_": "71360fb6-350f-4905-aef8-19f79ee01ff5", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.html", "file_name": "bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.html", "file_type": "text/html", "file_size": 11333, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about a winning solution, called Counselor Copilot, that was developed during a hackathon called RAG-a-thon. This solution is an AI copilot designed to assist crisis counselors in their work by automating administrative tasks and providing real-time suggestions for replies based on contact context and chat history. The text explains that this technology can help address the pressing issue of counselor shortage by maximizing the impact of existing resources, and ultimately lead to higher-quality conversations with patients. Some questions that this text can answer include:\n- What is the Counselor Copilot and how does it work?\n- How does Counselor Copilot help address the pressing issue of counselor shortage?\n- What tasks does Counselor Copilot automate for crisis counselors?\n- What tools does Counselor Copilot deploy based on the chat history and contact context?\n- What are some possible extensions for Counselor Copilot, and how can they improve the quality of suggested responses and closed-loop feedback cycle?\n- What is the Trevor Project and how does it relate to the Counselor Copilot solution?\n- What challenges do TrevorText counselors face, and how does Counselor Copilot address these challenges?\n- Where can more information about Counselor Copilot and its development be found?\n- What organizations and resources were cited in the text, and how do they relate to the Counselor Copilot solution?\n- How can others build on the work of the Counselor Copilot solution, and what ideas are suggested for further development?\n- What is the future potential of Counselor Copilot, and how can it improve the efficiency and effectiveness of crisis care?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1003737-974b-4885-96e1-a46fd31a3e22": {"__data__": {"id_": "d1003737-974b-4885-96e1-a46fd31a3e22", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.html", "file_name": "bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.html", "file_type": "text/html", "file_size": 8278, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about a new tool called AutoTranslateDoc, which aims to democratize access to technical documentation by breaking down language barriers. The tool collects documentation from GitHub, chunks it for translation using LLMs like GPT-3.5 and GPT-4, verifies the accuracy of translations using techniques like strategic document splitting and rigorous verification methods, and consolidates the chunks back into a cohesive document. The text also mentions the tool's ability to manage documentation updates efficiently and its plans for future enhancements, such as manual change integration and a GUI for translation management. This text can answer questions about the features and benefits of AutoTranslateDoc, how it works, how it improves translation accuracy and consistency, how it handles documentation updates, and the future developments of the tool.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d6dc69f-9dae-4907-a8c0-14331d223732": {"__data__": {"id_": "4d6dc69f-9dae-4907-a8c0-14331d223732", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.html", "file_name": "build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.html", "file_type": "text/html", "file_size": 13116, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text explains a workflow for connecting private knowledge sources using LlamaIndex connectors, ingesting documents, parsing documents into node objects, constructing indexes from nodes, and querying the index to answer questions using a Large Language Model (LLM) like OpenAI's text-davinci-003 model. This workflow can be used with MongoDB as the datastore, either storing documents and indexes as part of a collection or persisting the indexes using the MongoDBIndexStore class. Some examples of questions that this text can answer include:\n\n- \"How does GPT4 do on the bar exam?\"\n- \"What issues were observed after fine-tuning GPT-4 with RHLF?\"\n- \"What is RBRM?\"\n\nThe text also discusses RBRM (Rule-Based Reward Model) in the context of GPT-4 policy models during fine-tuning and explains how a PDF document is converted into LlamaIndex nodes and indices and persisted into MongoDB. Resources for reading data from MongoDB and various indexes in LlamaIndex are also provided. The text does not prior knowledge, but it can answer questions about RBRM, how to convert PDF documents into LlamaIndex nodes and indices, and where to find resources for working with MongoDB and LlamaIndex. It does not explicitly answer questions about undesired behaviors during fine-tuning, but it does mention that RBRM can help address these issues.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5fb4319-54c6-4233-91cc-2367005a7202": {"__data__": {"id_": "e5fb4319-54c6-4233-91cc-2367005a7202", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.html", "file_name": "build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.html", "file_type": "text/html", "file_size": 8801, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about the collaboration between the LlamaIndex and TruLens teams to enable rapid building, evaluation, and iteration of LLM apps. It explains how to use LlamaIndex and TruLens to build LLM apps, wrap them with TruLens to trace intermediate steps, and add feedback functions to evaluate their behavior. The text also mentions some of the questions that this technology can help answer, such as what the author did growing up, where the author was born, and whether the model is accurately summarizing the context provided. The text encourages iterating on app prompts, models, and chunking approaches to optimize app performance, and provides an example of a TruLens dashboard for tracking app versions and identifying failure modes. Overall, this text describes the potential benefits of using LlamaIndex and TruLens for LLM app development and evaluation.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf82d9b0-25ef-4a65-959c-bc0f5589ced8": {"__data__": {"id_": "cf82d9b0-25ef-4a65-959c-bc0f5589ced8", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.html", "file_name": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.html", "file_type": "text/html", "file_size": 27200, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses the use of LlamaIndex and Ray, two open-source frameworks, to build a scalable data pipeline and query engine. LlamaIndex is used for data loading, parsing, embedding, and indexing, while Ray ensures efficient and fast parallel execution. The text explains how to deploy the application using Ray Serve, which provides a web interface for metrics, charts, and other features for debugging Ray applications. Some questions that this text can answer include:\n\n- What is Ray Serve and how is it used to deploy LlamaIndex applications?\n- What resources are provided in the Ray docs and the Ray blogs to learn about Ray Serve?\n- How does the Ray docs present Ray Serve compared to the Ray blogs?\n- How can LlamaIndex be used to perform semantic search over a single document or combine results across multiple documents?\n- Where can I learn more about LlamaIndex and Ray to build and deploy scalable LLM apps?\n- How can I join the Ray and LlamaIndex communities to connect with other users and developers?\n- Is there an upcoming event related to Ray that I should attend? If so, where can I register?\n\nThe Ray docs and the Ray blogs both provide resources for learning about Ray Serve, including guides for Quick Start, User, Production, Performance Tuning, Development Workflow, API Reference, experimental Java API, and experimental gRPC support. The Ray blogs also provide Quick Start, User, and Advanced Guides for Ray Serve. The Ray docs provide more detailed information than the Ray blogs, but both resources can help users learn about Ray Serve.\n\nLlamaIndex can be used to perform semantic search over a single document or combine results across multiple documents. Users can join the Ray and LlamaIndex communities to connect with other users and developers. An upcoming event related to Ray is not mentioned in the provided text.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "288844f4-181a-4fb5-813b-81b6f7fe1b2f": {"__data__": {"id_": "288844f4-181a-4fb5-813b-81b6f7fe1b2f", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.html", "file_name": "building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.html", "file_type": "text/html", "file_size": 8818, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about building a fully open source retriever using LlamaIndex and Nomic Embed, a new embedding model that exceeds OpenAI Ada performance on both short and long context benchmarks. The text explains the need for open source embedding models in AI deployment, as closed source models have deliberately obfuscated training protocols and cannot be audited. It then walks through the steps of downloading and preparing data, setting up a database using SimpleDirectoryReader and NomicEmbedding, and building a retriever using LlamaIndex. The text also touches on visualizing the retrieval database using Nomic Atlas and connecting it to a generative model for full RAG using LlamaIndex.\n\nThis text can answer questions such as:\n- What is a retriever and how does it work in RAG systems?\n- Why are open source embedding models important for safe AI deployment?\n- How can we build a fully open source retriever using LlamaIndex and Nomic Embed?\n- How can we visualize our retrieval database using Nomic Atlas?\n- How can we connect our database to a generative model using LlamaIndex for full RAG?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c22a9a7-21f8-4e51-9b36-542b2a213711": {"__data__": {"id_": "7c22a9a7-21f8-4e51-9b36-542b2a213711", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-a-multi-agent-concierge-system.html", "file_name": "building-a-multi-agent-concierge-system.html", "file_type": "text/html", "file_size": 22858, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/building-a-multi-agent-concierge-system.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about a multi-agent concierge system that allows for multiple agents to complete various tasks based on user input. The system consists of an orchestration agent, known as the concierge agent, which determines which agent to run based on the user's current state and request. Sub-agents are responsible for specific tasks, such as looking up stock prices, authenticating users, checking account balances, and transferring money between accounts. The system uses a continuation agent to manage the chains of agents and a global state to keep track of user input and agent output. This text can answer questions about how the system selects which agent to run, how the sub-agents interact with each other, and how the continuation agent manages the chains of agents. It also touches on the importance of natural language instructions and the implicit \"chains\" of agents that are created through them. Overall, the text describes a novel approach to coordinating multiple agents simultaneously and managing their dependencies through natural language instructions.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "650a5a60-f82f-432e-9069-94ced09b1840": {"__data__": {"id_": "650a5a60-f82f-432e-9069-94ced09b1840", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.html", "file_name": "building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.html", "file_type": "text/html", "file_size": 30765, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text outlines the process of building and deploying a Slackbot using the Bolt framework, LlamaIndex, and Qdrant. It explains how to create a Slack app, install it to a workspace, join a channel, and reply to messages using LlamaIndex for natural language processing. This text can answer questions about setting up a Slack app, creating a Slack bot, and using LlamaIndex for NLP. It also touches on how to make the bot reply only to messages that mention it, how to store and retrieve facts using Qdrant, and how to implement the concept of recency in fact retrieval. Some additional questions that this text can answer include how to add more features to the bot, such as remembering who spoke, understanding threaded conversations, and adding metadata to nodes. However, it does not provide detailed information on more advanced features like joining every channel or adding multi-modal abilities.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5761f8d9-2484-4561-80f6-fb3eb3b80d7c": {"__data__": {"id_": "5761f8d9-2484-4561-80f6-fb3eb3b80d7c", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.html", "file_name": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.html", "file_type": "text/html", "file_size": 14878, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about OpenLLM, an open-source tool for training, evaluating, and deploying lifelong learning models using a variety of datasets and algorithms. It allows users to customize AI tools to fit specific needs by demonstrating how to create an AI-powered system that understands and processes queries using OpenLLM and LlamaIndex, a tool for managing data using LLMs. This text can answer questions related to OpenLLM, such as what it is, how it can be used to deploy and manage LLMs in different environments, how to fine-tune, serve, deploy, and monitor LLMs using OpenLLM, and how to manage data using LLMs using LlamaIndex for a specific project or application. It also touches on potential use cases for OpenLLM, such as image classification, natural language processing, and time-series data analysis, and provides evaluation and deployment tools for use in research, education, and industry. Some specific questions that this text can answer include how to simplify the end-to-end deployment workflow for LLMs using OpenLLM, how to improve output quality using OpenLLM and LlamaIndex by defining a SentenceSplitter, and how to explore more capabilities and use cases of OpenLLM and LlamaIndex.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f12b488-aa4a-44fe-8351-c959e234c53e": {"__data__": {"id_": "1f12b488-aa4a-44fe-8351-c959e234c53e", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.html", "file_name": "building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.html", "file_type": "text/html", "file_size": 10568, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is discussing the capabilities and features of LlamaIndex, an open-source library for building and evaluating question-answering (QA) systems using Large Language Models (LLMs). It explains how LlamaIndex offers various data structures for indexing data, such as the list index, vector index, keyword index, and tree index, and provides both a high-level and low-level API for customization. The text also mentions that LlamaIndex provides Question Generation and label-free Evaluation modules for evaluating the overall system's performance, without the need for ground-truth labels. The text provides examples of how these modules can be used to answer queries, including Response + Source Nodes (Context) evaluation, which checks if the response generated and source nodes (context) are matching, Query + Response + Source Nodes (Context) evaluation, which checks if the response + source context answers the query, and Query + Response + Individual Source Nodes (Context) evaluation, which checks which source nodes of the retrieved source nodes are relevant and show those documents to the users. Overall, LlamaIndex can be used to build accurate and reliable QA systems for various applications. Some of the questions that this text can answer include how to index data using LlamaIndex, how to use the Question Generation and Evaluation modules for evaluation, and how to customize various aspects of retrieval and synthesis using the low-level API.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f9f3383-657e-4718-868e-2beea6e45e25": {"__data__": {"id_": "7f9f3383-657e-4718-868e-2beea6e45e25", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-better-tools-for-llm-agents-f8c5a6714f11.html", "file_name": "building-better-tools-for-llm-agents-f8c5a6714f11.html", "file_type": "text/html", "file_size": 26798, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/building-better-tools-for-llm-agents-f8c5a6714f11.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses various aspects related to the creation and usage of tools for large language model (LLM) agents. It touches upon the LlamaHub Tools library, which enables LLMs such as ChatGPT to connect to APIs and perform actions on behalf of users. The author highlights the importance of writing informative and useful tool prompts and making them tolerant of partial inputs to minimize errors. The text also discusses the development of tools for Google Workspace applications, specifically focusing on Google Calendar and Gmail APIs. It emphasizes the need to provide simple deterministic functions for agents to use and return prompts from functions that perform mutations. The text also mentions the consideration of the size of the context window when building tools and making them compatible with the LoadAndSearchTool. The author suggests asking the agent about its own tools to debug them during development. The text also provides an example conversation between an Agent and a user to demonstrate how to debug tools. Overall, the text offers insights into creating and utilizing tools for LLamaIndex, as well as tips and techniques for optimizing their functionality and usability. Some of the questions that this text can answer include how to create tolerant tools, how to provide simple deterministic functions for agents, how to return prompts from functions that perform mutations, how to make tools compatible with the LoadAndSearchTool, and how to consider the size of the context window when building tools.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "46aef392-a42f-4159-846f-713d933c31d9": {"__data__": {"id_": "46aef392-a42f-4159-846f-713d933c31d9", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.html", "file_name": "building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.html", "file_type": "text/html", "file_size": 9815, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about building a multi-tenancy RAG (Retriever-Augmented Generation) system using LlamaIndex. It explains the concept of multi-tenancy in RAG systems, where each user's interaction with the system is isolated to prevent accidental or unauthorized cross-referencing of private information between different users. The text provides instructions for downloading and loading data, creating an empty index, ingesting documents, defining query engines, and querying the system. It also mentions the use of the IngestionPipeline for data ingestion and performing transformations. Some of the questions that this text can answer include: what are propositions mentioned in the paper, what are steps involved in LLMCompiler, and what are proposals mentioned in the paper regarding dense x retrieval.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "efdb4bc2-a61c-4fa3-9b1f-3ee7a9fd6f02": {"__data__": {"id_": "efdb4bc2-a61c-4fa3-9b1f-3ee7a9fd6f02", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.html", "file_name": "building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.html", "file_type": "text/html", "file_size": 12899, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a blog post about the author's creation of a prototype for a multi-modal visual-language application using AI models from companies like Microsoft and Google, as well as the open-source LLamaIndex library. The post explains the features and technical details of the prototype, which includes real-time image interaction with KOSMOS-2 and PaLM for captioning and conversation, respectively, and the role of LlamaIndex in orchestrating these elements. The post also mentions the use of Streamlit for the user interface and explains the limitations of the demo version and how user input is handled. Overall, this text can answer questions about the author's development process, the AI models used, the technical details of the prototype, and how user input is managed.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29dde6e0-b00a-4ec8-8fc9-91c1a11cced8": {"__data__": {"id_": "29dde6e0-b00a-4ec8-8fc9-91c1a11cced8", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf.html", "file_name": "building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf.html", "file_type": "text/html", "file_size": 8785, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about building RAG (Retrieval Augmented Generation) applications using LlamaIndex and Zilliz Cloud Pipelines. It explains how developers new to search and index can benefit from these technologies as they abstract away the technical complexity behind a few function calls, allowing developers to focus on the core user experience of their RAG applications. The text also provides a step-by-step guide on how to use Zilliz Cloud Pipelines to build a high-quality RAG chatbot that supports multi-tenancy through metadata filtering. Some of the questions that this text can answer include how to set up Zilliz Cloud Pipelines, how to ingest documents, and how to query the documents using ZillizCloudPipelineIndex as a query engine. The text also mentions advanced customization options for ZillizCloudPipelineIndex and encourages readers to ask questions in the provided user groups if they have any queries.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc5bc645-280c-44b1-8f55-a923ad5a53b4": {"__data__": {"id_": "fc5bc645-280c-44b1-8f55-a923ad5a53b4", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-the-data-framework-for-llms-bca068e89e0e.html", "file_name": "building-the-data-framework-for-llms-bca068e89e0e.html", "file_type": "text/html", "file_size": 13028, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/building-the-data-framework-for-llms-bca068e89e0e.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about LlamaIndex, a data framework designed to connect a user's private data with language models, also known as LLMs. The text explains that LLMs are increasingly being used in various fields such as GANs, sensor compression, and Transformers, and that as these models become more advanced, they evolve from knowledge generators to intelligent engines that can reason and act over new information. The text also highlights that while calling an LLM API is easy, setting up a software system that can extract insights from private data is harder, and that LlamaIndex aims to solve these data problems. Some of the questions that this text can answer include what is LlamaIndex, what problems does it aim to solve, who are some of the notable figures in the AI community who have contributed to its development, and what are some of the technical challenges in the space of LLMs and data. The text also provides information about the seed funding that LlamaIndex has received and the companies that are using it in their data-powered LLM apps. It explains some of the features that LlamaIndex offers, including data management, data querying, integrations with other storage providers, and integrations with downstream applications. The text also mentions some of the areas where LlamaIndex aims to improve, such as handling complex queries, better evaluation of LLM data systems, and ease of use for both beginner users and advanced users. Additionally, the text mentions some of the pain points that enterprises face in building and deploying data-powered LLM apps to production and LlamaIndex's plans to address these issues. Finally, the text provides information about how to learn more about LlamaIndex and how to join the Llama(Index) gang. It also mentions some job openings for founding engineers with experience in AI, data systems, and full-stack/front-end.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc255221-4c45-42cd-9448-37ce4bf85975": {"__data__": {"id_": "bc255221-4c45-42cd-9448-37ce4bf85975", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud.html", "file_name": "case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud.html", "file_type": "text/html", "file_size": 6199, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about Scaleport AI, a company that specializes in deploying AI across key industries such as Legal, eCommerce, Real Estate, and Finance, providing tailored generative AI solutions for production applications. The text discusses the challenges that Scaleport AI faced in areas such as development timelines, sales processes, data management, and OCR performance. It then goes on to explain how Scaleport AI turned to LlamaCloud, a comprehensive AI development platform, to address these challenges. The text highlights the benefits that Scaleport AI has gained from using LlamaCloud, such as accelerated development timelines, enhanced OCR performance, and flexible data handling. It also includes quotes from Teemu Lahdenper\u00e4, CTO of Scaleport AI, discussing the significant time savings and improved sales outcomes that have resulted from using LlamaCloud. Overall, this text can answer questions about the challenges faced by Scaleport AI, how they addressed these challenges, the benefits gained from using LlamaCloud, and the perspectives of Scaleport AI's CTO on these matters.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec2945bc-f663-4011-96cb-9f322642710a": {"__data__": {"id_": "ec2945bc-f663-4011-96cb-9f322642710a", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex.html", "file_name": "case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex.html", "file_type": "text/html", "file_size": 6724, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about Lyzr, a full-stack agent framework that specializes in building autonomous AI agents for enterprises. It explains how LlamaIndex, a retrieval augmentation system (RAG), plays a crucial role in Lyzr's technology stack by providing essential context, custom data access, and flexible retrieval for Lyzr's agents. The text discusses the positive impact of integrating LlamaIndex into Lyzr's framework, including rapid revenue growth, enhanced agent accuracy, and scalability. It also highlights the positive reception of Lyzr's agents by customers, with 75% of Lyzr's customers using two or more AI agents and a partnership with SurePeople. The text concludes by mentioning Lyzr's future plans, including the development of new agents and a framework called Lyzr AgentMesh. \n\nThis text can answer questions such as:\n\n- What is Lyzr and what does it specialize in?\n- How does LlamaIndex contribute to Lyzr's technology stack?\n- What benefits have resulted from integrating LlamaIndex into Lyzr's framework?\n- How have Lyzr's agents been received by customers?\n- What future plans does Lyzr have for expanding its offerings?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad5e5614-fd2d-40b8-b5fd-77e2351f5c1c": {"__data__": {"id_": "ad5e5614-fd2d-40b8-b5fd-77e2351f5c1c", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c.html", "file_name": "chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c.html", "file_type": "text/html", "file_size": 4649, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about the two-year knowledge cutoff of ChatGPT, an AI language model developed by OpenAI. It explains why OpenAI cannot simply update the model's knowledge and the potential limitations and costs of doing so. The text suggests some solutions for building applications that require more recent data, such as Retrieval Augmented Generation (RAG) and fine tuning. It also discusses some open-source tools and projects that address this issue. The text mentions some examples of applications that do not require more recent data and some chatbots that use RAG to provide more recent information. This text can answer questions about the reasons behind ChatGPT's knowledge cutoff, the techniques for providing more recent data in applications, and some open-source tools for addressing this issue.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13840a23-c7d3-466a-a5ef-3ca13ad3c289": {"__data__": {"id_": "13840a23-c7d3-466a-a5ef-3ca13ad3c289", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.html", "file_name": "combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.html", "file_type": "text/html", "file_size": 21543, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses the development of a query engine called SQLAutoVectorQueryEngine, which combines the capabilities of structured analytics from a SQL database and semantic search from a vector database to provide more comprehensive and detailed answers to complex natural language queries. This engine can join information across both structured and unstructured data, allowing for more accurate and efficient queries. By initializing the engine with a SQL query engine and a query engine that uses a vector store auto-retriever module, during query-time the engine chooses whether to query the SQL database or the vector database based on a selector prompt. If the vector database is chosen, the engine performs retrieval and synthesizes a natural language output using a RetrieverQueryEngine with the VectorIndexAutoRetriever module. If the SQL database is chosen, the engine executes a text-to-SQL query operation against the database and synthesizes a natural language output, optionally with additional query transformation steps. \n\nThis text highlights the potential of combining LLMs with both structured and unstructured data to unlock new retrieval and query capabilities. Some examples of queries that can be answered using this approach include queries about the population, country, and history of specific cities, as well as queries that leverage both structured and unstructured data. In summary, this text describes a new approach for combining the capabilities of LLMs for structured and unstructured data using a SQL database for structured data and a vector database for unstructured data, as well as an auto-retrieval module that simulates a join between the two databases.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ac9b548-a6ce-44f0-a298-8b1bc6870b58": {"__data__": {"id_": "4ac9b548-a6ce-44f0-a298-8b1bc6870b58", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191.html", "file_name": "create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191.html", "file_type": "text/html", "file_size": 4343, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is introducing a new command-line tool called \"create-llama\" that makes it easier to use LlamaIndex, a tool for loading, indexing, and chatting with data using LLMs like GPT-4. The tool generates a full-stack app that can be customized with the user's own data and OpenAI API key. The app can be deployed using different back-end options, including Next.js, Express, or Python FastAPI. The user is asked a series of questions, such as the type of backend to use, whether to use streaming or non-streaming, and which chat engine to use. The text also mentions some technical details, such as the use of the shadcn/ui library for styling in the Next.js front-end, and the ability to customize the app further by modifying the LLM used. Some possible answers to queries about this text could include:\n- What is \"create-llama\" and how does it simplify using LlamaIndex?\n- How do I generate a full-stack app using \"create-llama\"?\n- What types of data can be loaded and indexed using LlamaIndex?\n- How do I customize the app generated by \"create-llama\"?\n- Which back-end options are available for the generated app?\n- How do I deploy the generated app using Vercel or Render?\n- How do I modify the LLM used in the generated app?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6f9d733b-3654-42e7-8055-4a5b1cc7929b": {"__data__": {"id_": "6f9d733b-3654-42e7-8055-4a5b1cc7929b", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/customizing-property-graph-index-in-llamaindex.html", "file_name": "customizing-property-graph-index-in-llamaindex.html", "file_type": "text/html", "file_size": 30133, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/customizing-property-graph-index-in-llamaindex.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses two different topics. Firstly, it explains how a dataset of 2250 news articles can be used to construct a knowledge graph that can answer a variety of questions related to entities such as people, organizations, products, events, and locations. Some examples of potential questions that this text can answer include identifying individuals or organizations mentioned in the text, understanding the context and relationships between entities, and retrieving information about concepts and locations discussed in the text. The knowledge graph can also provide information about text chunks and embeddings, as well as relationships between these text chunks and entities. To improve the quality of the constructed graph, entity deduplication or disambiguation is necessary to merge duplicate nodes and improve graph structural integrity. The retriever options for querying the knowledge graph include the LLMSynonymRetriever, VectorContextRetriever, TextToCypherRetriever, and CypherTemplateRetriever, as well as the possibility of implementing a custom retriever.\n\nSecondly, the text explains how to customize the property graph index within LlamaIndex, specifically focusing on implementing entity deduplication and designing custom retrieval methods to enhance GraphRAG accuracy. It provides an example of how to use a custom retriever to answer a specific question using the RetrieverQueryEngine. This custom retriever first identifies entities in the input query and then executes the VectorContextRetriever for each identified entity separately. The entity extraction model and prompt for this custom retriever are defined, and the prompt is used to extract named entities such as names of people, organizations, concepts, and locations from the text.\n\nSome examples of potential questions that this custom retriever can answer include requests for specific details about named entities, such as their career statistics, political positions, or opinions on certain issues. The provided text also mentions that the knowledge graph can provide information about text chunks and embeddings, as well as relationships between these text chunks and entities, which can be used to answer questions related to these text chunks and entities. \n\nIn summary, the provided text describes how to construct a knowledge graph using a news dataset and how to customize the property graph index within LlamaIndex for improved GraphRAG accuracy. It also explains how to use a custom retriever to answer questions related to named entities and text chunks from the news dataset.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0c3b479-4e27-488f-96bb-2f7958531f88": {"__data__": {"id_": "c0c3b479-4e27-488f-96bb-2f7958531f88", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/data-agents-eed797d7972f.html", "file_name": "data-agents-eed797d7972f.html", "file_type": "text/html", "file_size": 32843, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/data-agents-eed797d7972f.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about the launch of two new components in the open-source LLamaIndex project: Data Agent components and the LlamaHub Tool repository. Data Agent components include reasoning loops and tool abstractions for properly indexing, retrieving, and querying data, while the LlamaHub Tool repository provides a collection of tools for loading and indexing data from various sources. These components aim to create automated knowledge workers that can reason over and interact with data using agent frameworks like LangChain. Some questions that this text can answer include: what are Data Agent components and how do they help with properly indexing, retrieving, and querying data? What is the LlamaHub Tool repository and what types of tools does it provide for loading and indexing data? How can these tools be integrated into agent frameworks like LangChain?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b89fc93f-8fe1-455b-b2e2-3f039fc4c821": {"__data__": {"id_": "b89fc93f-8fe1-455b-b2e2-3f039fc4c821", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/data-agents-zapier-nla-67146395ce1.html", "file_name": "data-agents-zapier-nla-67146395ce1.html", "file_type": "text/html", "file_size": 1249, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/data-agents-zapier-nla-67146395ce1.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about the introduction of LlamaIndex data agents, which can access data and perform tasks using Zapier NLA. The text explains that with just five lines of code, users can access the third-party apps and actions on Zapier's platform, which has over 30,000 options. The text also mentions the potential for having a personal assistant that can access data and perform tasks, and describes the benefits of using LlamaIndex data agents in conjunction with Zapier NLA. Some questions that this text can answer include:\n\n- What are LlamaIndex data agents, and how can they be used in conjunction with Zapier NLA?\n- How many third-party apps and actions are available on Zapier's platform, and how can they be accessed using LlamaIndex data agents?\n- What benefits does using LlamaIndex data agents in conjunction with Zapier NLA offer, and how can this technology be used to perform tasks and access data?\n- Can users access their own data using LlamaIndex data agents and Zapier NLA, and if so, how can they do so?\n- How can users implement the provided example code to use LlamaIndex data agents and Zapier NLA to summarize unread emails and send them to Slack?\n", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc3aae11-61f2-45bc-97a1-6485c18dcfbe": {"__data__": {"id_": "dc3aae11-61f2-45bc-97a1-6485c18dcfbe", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.html", "file_name": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.html", "file_type": "text/html", "file_size": 38052, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses two separate topics: financial reports from Uber for the years 2022, and the performance of ReAct-based agents using language models like GPT-3 and GPT-4 to solve complex tasks over data sources. The text about Uber's financial reports explains that there are three financial reports available for each quarter of 2022, and these reports can be queried and compared using various tools and techniques to answer questions about Uber's financial results, trends, and patterns. Some possible questions that this text can answer include: what were Uber's financial results for each quarter in 2022, how do these results compare to each other and to previous years, and what factors may have contributed to any changes or fluctuations in Uber's financial metrics?\n\nThe text about ReAct-based agents describes an experiment involving LLM models in LangChain, a popular AI framework for natural language processing. The authors compared the performance of GPT-3 and GPT-4 ReAct agents, as well as a simpler router agent, in analyzing Uber's financial statements using a zero-shot ReAct approach. The text highlights some limitations and areas for further analysis and discussion related to the effectiveness of ReAct-based agents using different language models and the need for interaction constraints with less sophisticated models. Some possible questions that this text can answer include: how did Uber's revenue grow over the last few quarters, what were the specific changes in Uber's risk factors over the past three quarters, and how reliable are the decisions made by less sophisticated models like GPT-3?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d53a5a5-a61b-48d1-9726-c37659c594b0": {"__data__": {"id_": "8d53a5a5-a61b-48d1-9726-c37659c594b0", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.html", "file_name": "easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.html", "file_type": "text/html", "file_size": 20138, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a tutorial on how to improve the ability of the Llama 2 model, which is a significant milestone in the advancement of open-source LLMs, to generate accurate SQL outputs from natural language using the process of finetuning. The tutorial explains that the smallest Llama 2 model, with 7B parameters, is not very good at generating SQL, making it impractical for structured analytics use cases. The text describes how finetuning, which involves training the model on a specific task or dataset, can be used to improve its performance in this area. The tutorial also explains how to use the Modal framework, PEFT, and LlamaIndex to fine-tune the model and make it useful for structured analytics against any SQL database. Some of the questions that this text can answer include how to fine-tune the Llama 2 model for SQL generation, how to evaluate its performance, and how to integrate it with LlamaIndex. Additionally, the text provides resources such as a GitHub repo, a Jupyter notebook guide, and a list of required packages.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a4b10b9-3792-4e0b-9b47-a409b6735b7f": {"__data__": {"id_": "3a4b10b9-3792-4e0b-9b47-a409b6735b7f", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.html", "file_name": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.html", "file_type": "text/html", "file_size": 34831, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text consists of different sources with varying topics. The first text is a CSV file containing information about countries, including their names, capital cities, and types. This text can help answer questions related to the locations and political structures of various countries around the world, such as the names of capital cities and which countries in the CSV file are recognized as independent political entities.\n\nThe second text is a summary of a movie called \"Raise the Red Lantern\" (Da hong deng long gao gao gua), which mentions the four wives of a wealthy lord and the strict rules and tensions within the household. This text can provide insights into the plot and cast of the movie, as well as some specific questions that it can help answer, such as the names of the main character and the other wives.\n\nThe third text provides instructions and code snippets for using the Hugging Face (HF) model and the Transformers library in Python to perform text summarization and question answering tasks. This text can answer questions about how to use these models and libraries for tasks such as summarizing news articles, identifying important points in scientific papers, or answering quesion-based requests in customer service.\n\nThe fourth text is about using the LlamaHub library in Python to load data from different sources and create vector databases for natural language processing tasks. This text specifically demonstrates how to use GraphQL to query data from an API and create a vector database for answering questions related to that data, such as finding continents with countries and capitals or the number of countries and capitals in North America.\n\nIn summary, the provided text encompasses different topics and can help answer questions related to countries, movies, natural language processing, and Python libraries. Some specific questions that these texts can help answer include the names of capital cities, the plot and cast of a movie, how to use Hugging Face and Transformers libraries, and how to create vector databases for natural language processing tasks using GraphQL.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c239bd6c-3955-4c0a-a528-3ec9f4e865bf": {"__data__": {"id_": "c239bd6c-3955-4c0a-a528-3ec9f4e865bf", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.html", "file_name": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.html", "file_type": "text/html", "file_size": 10471, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses the evaluation of multi-modal retrieval-augmented generation (RAG) systems, which involve both text and images as inputs. It explains how evaluation of such systems differs from traditional text-only RAG systems and provides a framework for evaluating multi-modal RAG systems. Some questions that this text can answer include: how do we evaluate the relevance and faithfulness of multi-modal generated responses, how can we separate out the retrieval evaluations per modalities for increased visibility, and how can we use LLMs as judges for these evaluations? The text also provides resources for building and evaluating multi-modal RAG systems using the llama-index library.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c6a1117-be96-4783-aa30-8d8d0f1f0fc7": {"__data__": {"id_": "5c6a1117-be96-4783-aa30-8d8d0f1f0fc7", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.html", "file_name": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.html", "file_type": "text/html", "file_size": 17554, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The first provided text is regarding Uber's financial performance in 2021, as disclosed in their SEC filings for the year. It can answer questions related to Uber's revenue, net income, regional growth, active user base, strategic initiatives, stock price performance, risks and challenges, cash flow, liquidity position, and future plans and projections.\n\nThe second provided text is about implementing a RAG system using the LlamaIndex library in Python. It explains how to load a dataset, define a service context for a specific LLM, evaluate the responses generated by the LLM using faithfulness and relevancy, and optimize chunk sizes for better performance. This text can answer questions related to loading datasets, defining service contexts, evaluating responses using faithfulness and relevancy, and optimizing chunk sizes for a RAG system.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "492d2827-5f2e-4694-b90e-02beba3aab66": {"__data__": {"id_": "492d2827-5f2e-4694-b90e-02beba3aab66", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.html", "file_name": "fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.html", "file_type": "text/html", "file_size": 13624, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses the implementation of a linear adapter for fine-tuning embedding models in LlamaIndex, a module for efficient text indexing and searching. It explains that while existing embedding models like SBERT and OpenAI can be fine-tuned using the SentenceTransformersFinetuneEngine, this approach has limitations, such as the requirement to re-embed the document corpus after fine-tuning and the inability to freeze document embeddings. The author explores a general approach that allows document embeddings to be frozen during training by implementing a linear adapter that is trained on top of query embeddings produced by any model. The text provides a guide on how to generate a synthetic dataset, fine-tune the linear adapter, and evaluate its performance using the EmbeddingAdapterFinetuneEngine abstraction. Some questions that this text can answer include how to fine-tune a linear adapter on top of any embedding model, how to generate a synthetic dataset for training and evaluation, how to use the EmbeddingAdapterFinetuneEngine abstraction for fine-tuning, and how to compare the performance of the fine-tuned model against other embedding models using two ranking metrics. The text also discusses how the module allows for the transformation of queries while keeping document embeddings fixed, and provides quantitative metrics for comparing the performance of different models, showing small but noticeable performance bumps. Overall, the text suggests that this module can help in eking out marginal improvement in retrieval metrics by allowing the transformation of queries while keeping document embeddings fixed, and that it could be a cheap and easy-to-try solution for users to decide whether it makes sense for their specific use case.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c818674-12f6-4454-98fd-f5ca4e575a4a": {"__data__": {"id_": "5c818674-12f6-4454-98fd-f5ca4e575a4a", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.html", "file_name": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.html", "file_type": "text/html", "file_size": 15144, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses the process of improving the performance of embedding models, which are used for text representation in downstream natural language processing (NLP) tasks like question answering and summarization, by finetuning them on unlabeled and unstructured data. The text explains how to generate a dataset for finetuning, how to use the Hugging Face Transformers library to finetune the model, and how to evaluate its performance using the InformationRetrievalEvaluator. Some questions that this text can answer include how to finetune an embedding model, how to generate a dataset, how to finetune the model using Hugging Face Transformers, and how to evaluate its performance using metrics like hit-rate and evaluation suite. The text also compares the performance of the finetuned model to the base model and the OpenAI embedding model.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e24625a9-19a9-4658-aead-0413b1fa8d86": {"__data__": {"id_": "e24625a9-19a9-4658-aead-0413b1fa8d86", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.html", "file_name": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.html", "file_type": "text/html", "file_size": 27896, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses the capabilities of a language model called GPT-4V in analyzing charts, tables, and graphs related to AI model performance. It compares the model's performance on specific questions versus general questions and explores techniques such as chain of thought prompting to enhance precision. The text also evaluates the performance of other models like Llama2, Vicuna, and Mistral across various NLP tasks and metrics. Some of the questions that this text can answer include which model performs better in specific tasks, how well Mistral performs compared to other models, and which model shows better scaling with model size. However, some metrics and tasks are not explicitly defined, making it difficult to provide a complete understanding of the text's content. Overall, the text aims to inform the community about the performance of these models and encourages testing with similar questions on one's own dataset before drawing conclusions.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b12f7641-553c-46e2-b38a-96edbfdf383e": {"__data__": {"id_": "b12f7641-553c-46e2-b38a-96edbfdf383e", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.html", "file_name": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.html", "file_type": "text/html", "file_size": 19580, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses a tool called FinSight's Annual Report Analyzer, which uses natural language processing and large language models (LLMs) to analyze annual reports more efficiently and insightfully. This tool is designed for portfolio managers, financial analysts, and shareholders to save time and improve decision-making by generating structured insights from the annual reports. The tool can answer questions related to key performance metrics, financial stats, major events, challenges encountered, strategic initiatives, market outlook, product roadmap, risk factors, and risk mitigation. It also provides insights into innovation, R&D activities, and innovation focus. The text explains how the tool uses a RAG (Retrieval Augmented Generation) pipeline, prompt engineering, and tools such as LlamaIndex and Streamlit to generate insights based on the information in a company's annual report. The author also discusses upcoming features such as the ability to select and save insights and the addition of more profession-specific insights. In summary, the text is about how LLMs can be used to analyze annual reports for financial analysis, making it easier and more insightful for professionals in the finance industry.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d400bfc-be64-4be7-91c4-cee2a8590dfc": {"__data__": {"id_": "4d400bfc-be64-4be7-91c4-cee2a8590dfc", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa.html", "file_name": "how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa.html", "file_type": "text/html", "file_size": 16248, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about using LLM (Large Language Model) technologies, specifically LlamaIndex and OpenAI, to create autonomous agents that can execute tasks without much or fewer instructions. These agents can solve tasks related to questions and answering, using tools to achieve desired behavior, or even planning tasks. The text provides examples of using agents with personal or private data, such as answering questions about software developer Dan Abramov. Some of the questions that this text can answer include \"Where did Dan Abramov work in his 20s?\" or \"What was Dan Abramov's salary in his 20s?\" by setting up tools for agents and creating a query engine tool from a vector index. Overall, the text is about utilizing LLM technologies to create autonomous agents that can answer specific questions and automate workflows.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0e32d09-7d5e-4924-a122-f574b006ce43": {"__data__": {"id_": "a0e32d09-7d5e-4924-a122-f574b006ce43", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.html", "file_name": "how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.html", "file_type": "text/html", "file_size": 8975, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses how to create customized AI chatbots using the LlamaIndex library and the EmbedAI platform. It explains how to train ChatGPT on various types of data sources such as website content, PDF documents, YouTube videos, and Notion documents. The text provides step-by-step instructions and code examples for each scenario. It also highlights some potential use cases for these chatbots, including customer support, company search engines, personalized learning assistance, technical support, healthcare assistance, and finance chatbots. Additionally, the text touches upon some challenges that may arise while building custom chatbots, such as handling data that changes frequently and dealing with tabular data in PDFs. Overall, this text provides a comprehensive guide for those interested in creating AI chatbots tailored to their specific data needs, whether through coding with LlamaIndex or using the no-code EmbedAI platform.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f3c8fa1-e2ab-4f59-84db-9579abf12516": {"__data__": {"id_": "3f3c8fa1-e2ab-4f59-84db-9579abf12516", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.html", "file_name": "improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.html", "file_type": "text/html", "file_size": 8911, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about a research paper presented by an AI team at Meta, titled \"RA-DIT: RETRIEVAL-AUGMENTED DUAL INSTRUCTION TUNING\". The paper proposes a method called RA-DIT that allows any large language model (LLM) to incorporate retrieval features without requiring expensive modifications during pre-training or integrating a data store after training, which leads to suboptimal performance in existing approaches. The paper describes how RA-DIT works through two distinct fine-tuning steps: updating the pre-trained LLM to better use retrieved information and updating the retriever to return more relevant results. The text also discusses the datasets and metrics used to test the suggested method, as well as the results obtained from evaluating three models: LLAMA 65B, LLAMA 65B REPLUG, and RA-DIT 65B, in both knowledge-intensive and commonsense reasoning tasks. This text can answer questions about the RA-DIT approach, its capabilities, and its performance on various datasets and metrics, as well as questions about the LLamaIndex implementation of this method.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "076ff5b2-eeac-4b00-a2d3-a69736af563f": {"__data__": {"id_": "076ff5b2-eeac-4b00-a2d3-a69736af563f", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.html", "file_name": "improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.html", "file_type": "text/html", "file_size": 20086, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about fine-tuning a Cohere reranker (custom reranker) using LlamaIndex to improve retrieval performance metrics. It demonstrates this process for a specific dataset related to Uber rides in New York City. Some questions that this text can answer include how to fine-tune a Cohere reranker model using LlamaIndex, how to evaluate retrieval performance metrics, how to choose between random or cosine sampling for selecting hard negatives, and the importance of empirical evidence in choosing the optimal number of hard negatives for Fine-tuned rerankers (custom rerankers). This text does not have a specific topic or context beyond this process and dataset. Other potential questions that this text may be able to answer include how the fine-tuned reranker performs in comparison to a baseline reranker, and how the retrieval performance metrics change with different hyperparameter configurations.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7182dfc4-59d1-40bb-a441-0eaae03678b7": {"__data__": {"id_": "7182dfc4-59d1-40bb-a441-0eaae03678b7", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/improving-vector-search-reranking-with-postgresml-and-llamaindex.html", "file_name": "improving-vector-search-reranking-with-postgresml-and-llamaindex.html", "file_type": "text/html", "file_size": 16145, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/improving-vector-search-reranking-with-postgresml-and-llamaindex.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses two separate topics. The first text describes how to implement a simple reranking example using LlamaIndex and the PostgresML managed index. It explains the differences between traditional reranking and cross-encoders, which excel at handling new, unseen data without the need for extensive user interaction data for fine-tuning. The text also mentions the Paul Graham dataset and how to set up dependencies, create a SimpleDirectoryReader, construct a PostgresMLIndex, and configure a retriever. Some questions that this text can answer include how to implement reranking using LlamaIndex and the PostgresML managed index, how to create a retriever from documents, and how to configure a retriever with limit and rerank parameters.\n\nThe second text describes the author's background in writing and programming as a child, as well as their interest in art. The author visited Carnegie Institute and reflected on the potential of making a living as an artist through painting. This text can answer queries related to the author's background in writing and programming, their interest in art, and their thought process behind visiting Carnegie Institute. Additionally, queries related to the use of punch cards and Fortran in the past can also be answered using this text.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6c2f380-dae5-46d1-b859-51ecf810ab63": {"__data__": {"id_": "e6c2f380-dae5-46d1-b859-51ecf810ab63", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-airbyte-sources-within-llamaindex-42209071722f.html", "file_name": "introducing-airbyte-sources-within-llamaindex-42209071722f.html", "file_type": "text/html", "file_size": 12780, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/introducing-airbyte-sources-within-llamaindex-42209071722f.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses two separate features in the context of data integration and vector databases. \n\nFirstly, the text explains that Airbyte, a data integration platform, has integrated with LlamaIndex, an open-source framework for vector databases and LLM (Language Model) systems. This integration allows users to load data directly into LlamaIndex using Airbyte's loaders, which are compatible with various sources such as Gong, Hubspot, Salesforce, Shopify, Stripe, Typeform, and Zendesk Support. Users can choose to run the loaders locally or use the hosted Airbyte service for additional features such as a user interface, event notifications, and scale-out capabilities. The integration also supports incremental loads, custom sources, and mapping Airbyte records to LlamaIndex documents. This feature enables users to move data from various sources to their LLM-based applications without the need for additional services or API calls. Some questions that this text can answer include how to load data directly into LlamaIndex using Airbyte's loaders, which sources are available for integration, and how to map Airbyte records to LlamaIndex documents.\n\nSecondly, the text introduces a feature called AirbyteCDKReader, which allows for the creation and use of custom data sources in Airbyte. The text explains how to use this feature by importing a custom data source (in this case, GitHub issues) and loading the data into a stream using the AirbyteCDKReader class. Some questions that this text can answer include what AirbyteCDKReader is, how to import a custom data source, what configuration is required to use AirbyteCDKReader with a custom data source, and how to load data from a custom data source into a stream using AirbyteCDKReader. The text also provides links to resources for learning more about creating custom sources and using Airbyte. Finally, the text mentions how to provide feedback on the priorities of features for Airbyte and how to get help with using existing custom data sources or integrating new ones with Airbyte.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60ff6c04-bd0a-4e31-926c-93f341377cab": {"__data__": {"id_": "60ff6c04-bd0a-4e31-926c-93f341377cab", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html", "file_name": "introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html", "file_type": "text/html", "file_size": 18790, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is introducing a Python library called llama-agents, which allows for the creation of complex multi-agent AI systems. The text explains how to use this library to construct a Query Rewriting RAG system, which combines query rewriting with RAG to enhance question-answering capabilities. This is an alpha release, and the developers are seeking feedback from the public to improve the library's features for use in production. Some potential questions that this text can answer include inquiries about the uniqueness of llama-agents in comparison to other AI libraries, examples of multi-agent AI systems that can be built using this library, how the Query Rewriting RAG system functions, and how to set up and utilize the multi-agent system with llama-agents. The text also mentions the availability of examples and resources in the llama-agents repository and explains how users can contribute to the public roadmap by providing feedback on the features. Overall, the text serves as a resource for acquiring knowledge about the library's capabilities and potential applications.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abfec326-12d7-4050-8535-fb19b5e8d1ec": {"__data__": {"id_": "abfec326-12d7-4050-8535-fb19b5e8d1ec", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-llama-datasets-aadb9994ad9e.html", "file_name": "introducing-llama-datasets-aadb9994ad9e.html", "file_type": "text/html", "file_size": 16032, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/introducing-llama-datasets-aadb9994ad9e.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about using the LLamaIndex, a framework for building semantic search and question answering systems, to extract insights and generate datasets from various sources. It explains how to use LLamaIndex to extract insights from text, generate a dataset using LLamaIndex, and contribute a dataset to the LLamaHub platform. This text can answer questions such as how to download and use a LLama dataset, how to create a LLama dataset, how to contribute a LLama dataset, and how to use LLamaIndex to extract insights and generate datasets. Some other questions that this text can answer include how to create a template for a LLama dataset, how to generate a baseline evaluation dataset, and how to prepare a dataset card and README.md file. Overall, the text provides information on how to utilize LLamaIndex to extract insights and generate datasets from multiple sources.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "123f9c77-f764-495f-b50a-463c23d9ef6d": {"__data__": {"id_": "123f9c77-f764-495f-b50a-463c23d9ef6d", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-llama-packs-e14f453b913a.html", "file_name": "introducing-llama-packs-e14f453b913a.html", "file_type": "text/html", "file_size": 12815, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/introducing-llama-packs-e14f453b913a.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text explains how to use the LlamaIndex and LlamaHub to create customizable Llama Packs for performing text analysis and retrieval tasks using the LLM (Large Language Model) framework. It provides examples on how to import and run a pre-existing LlamaPack, as well as how to create a new one by extending the BaseLlamaPack class. Some of the questions that this text can answer include how to use LlamaIndex and LlamaHub to create customizable Llama Packs, how to import and run a pre-existing LlamaPack, how to create a new LlamaPack by extending the BaseLlamaPack class, where to find all available Llama Packs on LlamaHub, and how to contribute a new LlamaPack by copying/pasting existing code into the BaseLlamaPack subclass.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d700344-f592-4c7e-ad65-58c9a2b23dae": {"__data__": {"id_": "1d700344-f592-4c7e-ad65-58c9a2b23dae", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-llamacloud-and-llamaparse-af8cedf9006b.html", "file_name": "introducing-llamacloud-and-llamaparse-af8cedf9006b.html", "file_type": "text/html", "file_size": 17273, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/introducing-llamacloud-and-llamaparse-af8cedf9006b.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about LlamaIndex, a company focused on large language models (LLMs) and generative AI. It announces the release of two new tools: LlamaParse, a tool for parsing natural language into structured data, and LlamaCloud, a service for storing and indexing large volumes of text. The text explains how these tools can be used to build real-world applications, such as parsing building codes for Accessory Dwelling Unit (ADU) planning and parsing real-estate disclosures for home buying. It also introduces some partners and collaborators in the LLM and AI ecosystem, including Datastax, MongoDB, Qdrant, and NVIDIA, who are integrating LlamaParse and LlamaCloud into their offerings. The text provides information on how to access these tools, with LlamaParse available for public preview and LlamaCloud in a private preview mode. Some questions that this text can answer include: what is LlamaIndex and what tools have they released? how can these tools be used in practical applications, and who are some of the partners and collaborators involved? how can I access these tools, and what are the usage limitations?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93df4a84-3e21-488e-be5c-574732ab057f": {"__data__": {"id_": "93df4a84-3e21-488e-be5c-574732ab057f", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-llamaindex-ts-89f41a1f24ab.html", "file_name": "introducing-llamaindex-ts-89f41a1f24ab.html", "file_type": "text/html", "file_size": 3898, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/introducing-llamaindex-ts-89f41a1f24ab.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about the release of version 0.0.1 of LlamaIndexTS, a TypeScript library focused on helping developers integrate their private data with large language models like ChatGPT. It explains what LlamaIndex is, provides a backstory, describes the library's design, and mentions a playground where developers can test it out. Some questions that this text can answer include: What is LlamaIndex and how does it help developers integrate their private data with large language models? Who developed LlamaIndexTS and why was it created? How does LlamaIndexTS differ from the Python version of the library, and what runtime environments does it support? How can developers contribute to the library's development?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23c0c342-b5ff-4097-9372-42a689349c3d": {"__data__": {"id_": "23c0c342-b5ff-4097-9372-42a689349c3d", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-query-pipelines-025dc2bb0537.html", "file_name": "introducing-query-pipelines-025dc2bb0537.html", "file_type": "text/html", "file_size": 17799, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/introducing-query-pipelines-025dc2bb0537.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about the QueryPipeline feature introduced in the LLamaIndex library, which allows for the creation of customizable query workflows using a set of modules. It explains how to define relationships between modules, run the pipeline, and define custom components. Some questions that this text can answer include: how to use the QueryPipeline feature, what modules are supported, how to run the pipeline with multiple inputs, and how to define custom components. It also touches on the difference between a QueryPipeline and an IngestionPipeline. Overall, this text provides guidance on how to utilize this feature in LLamaIndex to efficiently and declaratively process and query information from multiple sources without prior knowledge.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6dc427b-b915-4647-a282-6d68bc6fc466": {"__data__": {"id_": "f6dc427b-b915-4647-a282-6d68bc6fc466", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.html", "file_name": "introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.html", "file_type": "text/html", "file_size": 7788, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about the introduction of a new tool called RAGs, which allows users to create and customize their own RAG pipeline using natural language. The text explains that RAGs is a Streamlit app that simplifies the process of setting up a \"ChatGPT over your data\" experience without the need for coding. The app has three simple steps for users to follow: (1) easily describe a task and define parameters, (2) dive into the configuration view to alter specific settings, and (3) interact with the RAG agent to ask questions based on the user's data. The app is designed for both less-technical and technical users, with technical users having the freedom to inspect and customize specific parameter settings. The text also provides a high-level overview of the app's architecture and explains how the builder agent and builder tools are used to construct a RAG pipeline. The text concludes with instructions on how to install and set up RAGs, as well as information on how to contribute to the project. Some of the questions that this text can answer include how to build a RAG agent, how to view and edit generated configuration parameters, and how to test the app's features by asking specific and summarization questions.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f8bbe65-f50d-4736-91b8-f2c2d95f664c": {"__data__": {"id_": "3f8bbe65-f50d-4736-91b8-f2c2d95f664c", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41.html", "file_name": "introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41.html", "file_type": "text/html", "file_size": 4155, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text introduces a new command-line tool called llamaindex-cli, which allows users to try out retrieval-augmented generation (RAG) without the need to write any code. The tool uses Chroma under the hood and requires the installation of chromadb as well. The text explains how to use the tool, including setting the OPENAI_API_KEY environment variable and ingesting local files into the local vector database. The text also describes how to ask questions using the tool and provides an example question. The text mentions that the tool can be customized to use any LLM model, including local models like Mixtral 8x7b through Ollama. The documentation is suggested for further details on how to use the tool and build more advanced query and retrieval techniques. Some of the questions that this text can answer include \"What is LlamaIndex?\" and \"How can I customize llamaindex-cli to use local LLM models like Mixtral 8x7b through Ollama?\"", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b34bddd8-397b-49b8-aee0-35fa2312c3c7": {"__data__": {"id_": "b34bddd8-397b-49b8-aee0-35fa2312c3c7", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms.html", "file_name": "introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms.html", "file_type": "text/html", "file_size": 14933, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about the introduction of the Property Graph Index feature in LlamaIndex, an open-source project for building search engines. It explains that this feature allows for labels and properties to be assigned to nodes and relationships, and that text nodes are represented as vector embeddings. The text also mentions that the index supports both vector and symbolic retrieval, and can be constructed and queried using various methods. Some potential questions that this text can answer include how to use the Property Graph Index with LlamaIndex, how to store and manage data using different backing stores, and how to perform advanced queries using Neo4j. The text provides links to resources for learning more about the indexing process, basic and advanced usage examples, and using the Neo4j store directly. Overall, the text highlights the benefits and capabilities of the Property Graph Index and how it can be integrated into LlamaIndex for more advanced search and indexing functionality.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88fb8b71-9417-463e-9594-a800a6bd7002": {"__data__": {"id_": "88fb8b71-9417-463e-9594-a800a6bd7002", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb.html", "file_name": "join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb.html", "file_type": "text/html", "file_size": 6164, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about a free course on advanced retrieval augmented generation (RAG) for production, developed in collaboration with Activeloop, TowardsAI, and the Intel Disruptor Initiative. The course is designed to enable individuals to apply RAG techniques across various industries, including legal, biomedical, healthcare, e-commerce, and finance, through interactive projects and video content. The course covers challenges with naive RAG, advanced RAG methods using LlamaIndex, RAG agents, and production-grade apps. It also includes a free trial of one month for Activeloop's Starter and Growth plans, as well as a certificate upon completion. The text can answer questions about the course content, the industries it covers, the technologies used, the benefits of RAG, and the instructors involved.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "edde25d6-2068-483d-9f6c-d1efc27c2289": {"__data__": {"id_": "edde25d6-2068-483d-9f6c-d1efc27c2289", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/launching-the-first-genai-native-document-parsing-platform.html", "file_name": "launching-the-first-genai-native-document-parsing-platform.html", "file_type": "text/html", "file_size": 9532, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/launching-the-first-genai-native-document-parsing-platform.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is an announcement from LlamaIndex, a company focused on connecting the world's data to the power of LLMs. The announcement introduces LlamaParse, a GenAI-native document parsing platform that enables users to provide natural-language instructions to the parser, resulting in significantly better parsing results. Some examples of instructions include rich table support, parsing mathematical equations, and parsing comic books. The platform also offers JSON mode, which provides a rich programmatic format that allows users to extract images and build custom RAG strategies. The text also mentions the expansion of supported document types, including Microsoft Word, PowerPoint, and ePub books, as well as the availability of a free tier and paid plans for additional pages. Overall, this text can answer questions related to the capabilities and features of LlamaParse, such as how to use natural-language instructions, how to extract images, and how to build custom RAG strategies. It can also provide information on the supported document types and pricing options.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a331bec-d6d4-44e9-b437-bc33c4cbcdf0": {"__data__": {"id_": "0a331bec-d6d4-44e9-b437-bc33c4cbcdf0", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llama-index-prem-ai-join-forces-51702fecedec.html", "file_name": "llama-index-prem-ai-join-forces-51702fecedec.html", "file_type": "text/html", "file_size": 9535, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llama-index-prem-ai-join-forces-51702fecedec.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about the integration of Prem App and Llama Index, two AI development tools. It explains how the integration enables developers to connect custom data sources to large language models easily, simplifying the overall AI development cycle. The text provides step-by-step instructions on how to build a simple talk to your data use case using Prem and Llama Index, as well as links to further resources such as documentation and tutorials. Some questions that this text can answer include how to install and run Prem services, how to load data and create documents using Prem, how to instantiate LLMs and embeddings using Langchain, how to configure the vector store, and how to perform a simple query using the query engine. It also provides information on how to join the community of developers who are on a similar journey towards creating open, composable, and privacy-centric AI applications.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01310167-4545-4da6-832e-2377043f67a9": {"__data__": {"id_": "01310167-4545-4da6-832e-2377043f67a9", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamacloud-built-for-enterprise-llm-app-builders.html", "file_name": "llamacloud-built-for-enterprise-llm-app-builders.html", "file_type": "text/html", "file_size": 10150, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamacloud-built-for-enterprise-llm-app-builders.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about a new product called LlamaCloud, which is designed to help developers manage complex data sources for LLM (Language Model) and RAG (Reasoning About Groups) applications in production environments. LlamaCloud offers a range of features, including a state-of-the-art parser called LlamaParse, managed ingestion, advanced retrieval, a playground for testing and refining strategies, and scalability and security options. The text also highlights some of the challenges facing LLM and RAG applications, such as data quality issues, scalability hurdles, accuracy concerns, and configuration overload. It provides examples of how LlamaCloud can be used, as well as information on how to join a waitlist for official access and stay updated on the product. The text also touches on the strengths of LlamaCloud, which include easy integration into existing code and complementary layers to vector storage providers. Some of the questions that this text can answer include:\n\n- What is LlamaCloud, and how can it help with LLM and RAG applications in production environments?\n- What features does LlamaCloud offer, and how can they improve the accuracy and efficiency of LLM and RAG applications?\n- How can developers join the waitlist for official access to LlamaCloud, and what other resources are available for learning more about the product?\n- How does LlamaCloud differ from vector databases in terms of focus and functionality?\n- What challenges do LLM and RAG applications face, and how does LlamaCloud address these challenges?\n- What examples are available for using LlamaCloud, and how can they help developers get started with the product?\n- How can LlamaCloud be integrated into existing code, and what resources are available for learning how to do this?\n- What scalability and security options are available for LlamaCloud, and how can they be configured to meet specific needs?\n- How can developers stay updated on the latest features and developments related to LlamaCloud?\n- What integrations are planned for LlamaCloud with different vector storage providers, based on customer requests?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "871bde9f-8afd-44ee-9e8a-778424484ad6": {"__data__": {"id_": "871bde9f-8afd-44ee-9e8a-778424484ad6", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.html", "file_name": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.html", "file_type": "text/html", "file_size": 23681, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses updates and improvements in the open-source project called llama_index. It highlights the introduction of a new module called RAG, which allows for ranking, aggregation, and generation of content, and provides more flexibility in specifying consecutive messages with the same role. The text also mentions exposing the chat history state as a property and supporting overriding the chat history in chat and achat endpoints. Additionally, the text explains the removal of some previously deprecated arguments. Some questions that this text can answer include: what is RAG, why was it introduced, how does it work, how can the chat history be accessed as a property, and how can it be overridden in chat and achat endpoints. Overall, the text explains several updates and changes to the llama_index library, including the removal of certain deprecated functionalities and the replacement of certain objects with new ones. It also provides guidance on how to migrate existing code to use these new features.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0413acd1-7b0e-4b3d-bd40-f5b40cc74b56": {"__data__": {"id_": "0413acd1-7b0e-4b3d-bd40-f5b40cc74b56", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim.html", "file_name": "llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim.html", "file_type": "text/html", "file_size": 3079, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about the integration of LlamaIndex, an open-source tool for connecting data to LLMs and extracting insights, with NVIDIA NIM, a part of NVIDIA AI Enterprise software platform that optimizes inference on popular AI models. This integration aims to help enterprises seamlessly deploy generative AI at scale, especially for use cases like enterprise search, question answering, analytics, and more. It allows enterprises to connect their proprietary data sources to generative AI models hosted using NVIDIA NIM, search across structured and unstructured enterprise data, and build data-enriched generative AI applications. This integration provides a seamless path from experimentation to production, as developers can access NVIDIA NIM microservices from the NVIDIA API catalog using industry-standard protocols and then easily transition those applications to work on their self-hosted NVIDIA NIM instance for enhanced security, customization, and cost effectiveness at scale. Some questions that this text can answer include:\n\n- How can enterprises ensure the protection of intellectual property, security, and compliance while deploying generative AI at scale?\n- How can enterprises access industry-standard protocols for building AI applications using NVIDIA NIM microservices?\n- How can enterprises search across and extract insights from both structured and unstructured enterprise data to enhance the knowledge and accuracy of AI models?\n- How can developers easily transition their AI applications from experimentation to production using NVIDIA NIM and LlamaIndex?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1fc794d0-dd4b-4232-ac2d-bcfdd2f737c9": {"__data__": {"id_": "1fc794d0-dd4b-4232-ac2d-bcfdd2f737c9", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-and-transformers-agents-67042ee1d8d6.html", "file_name": "llamaindex-and-transformers-agents-67042ee1d8d6.html", "file_type": "text/html", "file_size": 14762, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-and-transformers-agents-67042ee1d8d6.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses the use of Large Language Models (LLMs) and agents, which are programs that can perform tasks and make decisions. It mentions the release of Transformers Agents, a popular use-case for LLMs, as well as the LlamaIndex tool, which can augment agents' image-generation capabilities by suggesting better prompts. The text also introduces a Text2Image Prompt Assistant tool, which can re-write prompts to generate more beautiful images. The text suggests that LlamaIndex can be used by agents and provides some context on the capabilities and limitations of LLMs and agents, as well as the challenges and opportunities associated with their use. Some potential questions that this text can answer include:\n\n- What are LLMs and agents, and how are they being used in practice?\n- How can LLMs and agents be integrated into specific applications, such as image generation?\n- What are some of the key benefits and drawbacks of using LLMs and agents, and how are these factors influencing their adoption and use?\n- How are LLMs and agents being developed and improved over time, and what are some of the key trends and innovations in this area?\n- How can LLMs and agents be made more robust and reliable, particularly in the face of uncertainty and ambiguity in input data?\n- How can LLMs and agents be made more efficient and scalable, particularly in terms of resource usage and computational complexity?\n- How can LLMs and agents be made more transparent and interpretable, particularly in terms of their decision-making processes and outcomes?\n- How can LLMs and agents be made more secure and trustworthy, particularly in terms of their privacy and confidentiality concerns?\n- How can LLMs and agents be made more adaptable and flexible, particularly in terms of their ability to handle new and diverse types of input data?\n- How can LlamaIndex be used with LLM agents, and how can custom tools in Transformers Agents be distributed and shared using Hugging Face Spaces?\n\nThe text also provides examples of how the Text2Image Prompt Assistant tool can be used to generate different images based on varying prompts and temperatures.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e80d500-411f-412e-8c7d-0ba3aabb3930": {"__data__": {"id_": "1e80d500-411f-412e-8c7d-0ba3aabb3930", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-and-weaviate-ba3ff1cbf5f4.html", "file_name": "llamaindex-and-weaviate-ba3ff1cbf5f4.html", "file_type": "text/html", "file_size": 9644, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-and-weaviate-ba3ff1cbf5f4.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is an overview and demo of the integration between LlamaIndex and Weaviate. LlamaIndex is a data framework for building LLM applications, while Weaviate is a vector database that acts as an external storage provider. The integration allows for the creation of powerful and reliable RAG stacks for LLM applications. The text describes the capabilities of both tools, including data ingestion, indexing, and querying. It also provides a demo notebook for building a simple QA system over Weaviate's blog posts. Some of the questions that this text can answer include:\n\n- What is the intersection between LLMs and search?\n- How can LLMs improve search capabilities?\n- How can LLMs be used in re-ranking and search result compression?\n- How can LLMs be used to manage document updates and rank search results?\n- How can LLMs be used to prompt the language model to extract or formulate a question based on a prompt or description of a search engine tool?\n- How can LLMs be used to prompt the language model to rank search results according to their relevance with the query?\n- How can LLMs be used to classify the most likely answer span given a question and text passage as input?\n\nIn summary, the text explains how LlamaIndex and Weaviate can be used to build LLM applications, particularly for search and QA systems.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90128250-545b-4fcd-92b9-504e7d59be68": {"__data__": {"id_": "90128250-545b-4fcd-92b9-504e7d59be68", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.html", "file_name": "llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.html", "file_type": "text/html", "file_size": 9573, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about a solution called LlamaIndex, which aims to simplify the process of knowledge transfer (KT) in the IT and software development industry. The solution involves four stages: code parsing, summary and explanation generation with LlamaIndex, video creation with D-ID, and video-code integration. The authors explain how LlamaIndex and D-ID are used to generate summaries, detailed explanations, and videos for individual code snippets, respectively. The text also touches upon the challenges of KT in the industry and how LlamaIndex and D-ID can help address them. The text mentions a hackathon where the authors secured the first prize for their solution. Overall, the text can answer questions such as:\n\n- What is LlamaIndex and how is it used in the context of KT?\n- How does LlamaIndex generate summaries and explanations for code snippets?\n- How does D-ID create videos for code snippets?\n- How do LlamaIndex and D-ID address the challenges of KT in the industry?\n- What is the hackathon mentioned in the text, and what was the outcome for the authors' solution?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e2a7e039-d3fb-49dc-a063-62b9bcf32e48": {"__data__": {"id_": "e2a7e039-d3fb-49dc-a063-62b9bcf32e48", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.html", "file_name": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.html", "file_type": "text/html", "file_size": 28117, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text refers to three distinct sources: a scientific paper titled \"LLM Compiler Paper,\" instructions for creating a question answering system using Weaviate's vector search capabilities, and a blog post discussing the tuning of alpha in a hybrid search system.\n\nThe scientific paper, titled \"LLM Compiler Paper,\" focuses on the compilation of LLMs (Large Language Models), and explores their features, applications, challenges, limitations, and potential future directions for scientific research. The paper discusses the implications of LLMs in various fields, such as natural language processing, artificial intelligence, machine learning, and cognitive science. Some questions that this text can answer include: its significance in the field of scientific research, the key features and applications of LLMs, the challenges and limitations of LLMs, and how it compares and contrasts with other related papers in terms of methodology, results, and conclusions.\n\nThe instructions for creating a question answering system using Weaviate's vector search capabilities provide a step-by-step guide on how to load data, create nodes, set up a Weaviate client, and define a custom retriever with a reranker. This text answers questions such as: how to load data into Weaviate's vector search system, how to create nodes using Weaviate's vector search capabilities, how to set up a Weaviate client for vector search, and how to define a custom retriever with a reranker using Weaviate's vector search capabilities.\n\nThe blog post discusses the tuning of alpha in a hybrid search system for various query types and document indexing scenarios. It presents graphs and results from experiments conducted on two sets of data and highlights the impact of using a reranker, as well as the hybrid search system itself. This text answers questions such as: what is alpha tuning in a hybrid search system, how does it affect retrieval evaluation metrics for different query types and document indexing scenarios, and what are some best practices for optimizing alpha values for specific use cases. Additionally, the text provides references for further reading on hybrid search and its capabilities.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd573d2c-d04b-4397-9d42-579528d43f12": {"__data__": {"id_": "bd573d2c-d04b-4397-9d42-579528d43f12", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-gemini-8d7c3b9ea97e.html", "file_name": "llamaindex-gemini-8d7c3b9ea97e.html", "file_type": "text/html", "file_size": 16217, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-gemini-8d7c3b9ea97e.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses the release and support of Google's new language model, Gemini, in LlamaIndex. It explains the different features and capabilities of both the text-only and multi-modal (text and images) variants, as well as the Semantic Retriever API, which bundles storage, embedding models, retrieval, and LLM in a RAG pipeline. This text can answer questions about the performance and benchmarks of Gemini's Ultra variants, how to use Gemini and Semantic Retriever in LlamaIndex, and what safety settings are included in the LLM for grounded-output production.\n\nAdditionally, the text briefly introduces a semantic retriever tool called \"GoogleDemo\" which can be used to retrieve relevant information based on a query. This tool can provide different answering styles and safety settings for the results, and it can be decomposed into different components for advanced RAG (Retrieval Augmentation from Generation) use cases such as Google Retriever + Reranking, Multi-Query + Google Retriever, and HyDE + Google Retriever.\n\nSeparately, the text also provides information about a restaurant called \"SeaWorld Orlando's Aquatica\" with a rating of 4 and a review score of 4.3 on Yelp. It explains that the restaurant offers a unique underwater-themed dining experience with a view of Universal Studios' Inland Sea, and it provides the address and nearby popular tourist places. Some questions that this text can answer about SeaWorld Orlando's Aquatica include its location, rating, review score, and the type of dining experience it offers.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2a9fcbf-4e45-4a09-9280-2ab03dbd2b91": {"__data__": {"id_": "f2a9fcbf-4e45-4a09-9280-2ab03dbd2b91", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.html", "file_name": "llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.html", "file_type": "text/html", "file_size": 17065, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text consists of product reviews for items such as the iPhone 13, SamsungTV, and an ergonomic chair. This text can provide insights into consumer sentiments regarding these products, including their features, performance, and overall satisfaction. Some questions that this text can answer include: what are the benefits and drawbacks of each product, which product has the best battery life and camera quality, and what are the most comfortable chairs available. This text can also serve as a source of information for answering questions related to these products, such as how user sentiment is perceived based on their reviews, whether users are generally happy with a specific product, and how user feedback has evolved over time for a particular product. Additionally, the text provides information about a technique called Text2SQL+RAG, which is a methodology for efficiently processing and analyzing large volumes of textual data using a combination of SQL queries and natural language processing. This technique allows for the extraction of insights and summaries from text, such as user reviews, to help businesses make informed decisions. Overall, the provided text offers a wealth of information that can be used to answer questions related to these products and to rapidly analyze and interpret vast swaths of textual data in the age of e-commerce, where user reviews play a crucial role in product success or failure.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0855239-4e1a-41e7-8feb-593b16c4512d": {"__data__": {"id_": "d0855239-4e1a-41e7-8feb-593b16c4512d", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff.html", "file_name": "llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff.html", "file_type": "text/html", "file_size": 4196, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is an announcement by Laurie Voss, who has recently joined LlamaIndex as their VP of Developer Relations. The text discusses Laurie's background in the tech industry and her previous roles, including founding npm Inc. The text also explains the significance of LLMs (Large Language Models) and how they are changing the landscape of software development. Laurie shares her belief that LLMs are the future of software and describes the potential applications of LLMs that were previously impossible. The text also mentions LlamaIndex, a Python and JavaScript framework that allows for the creation of customizable, production-class applications that use LLMs. Some of the questions that this text can answer include:\n- Who is Laurie Voss and what is her role at LlamaIndex?\n- What is the significance of LLMs in software development and how are they changing the industry?\n- What is LlamaIndex and how can it be used to create customizable, production-class applications that use LLMs?\n- How can SEC Insights, an app created by LlamaIndex, be used to ingest regulatory documents and ask questions about them?\n- What is an alpaca and why does Laurie Voss use it as her personal mascot?\n", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "155d642c-6e3b-4c3b-9178-080677b69062": {"__data__": {"id_": "155d642c-6e3b-4c3b-9178-080677b69062", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.html", "file_name": "llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.html", "file_type": "text/html", "file_size": 26130, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses two technologies, LlamIndex and Metaphor, and their integration to enhance the capabilities of LLMs and RAG systems. LlamIndex provides data agents that can retrieve information from various sources, while Metaphor allows for highly semantic searches over the internet. This integration allows LLMs to perform fully neural, highly semantic searches over the internet and also get clean, HTML content from the results. Some of the questions that this text can answer include:\n\n1. What are the capabilities of LlamIndex data agents and how are they being combined with Metaphor?\n2. What is the Metaphor API and how does it connect LLMs to the internet?\n3. What types of tasks can LLMs perform using this tool, such as scheduling meetings or sending emails?\n4. How is the Metaphor API trained to predict links on the internet, and what benefits does this provide for highly semantic searches over the internet?\n\nAdditionally, the text briefly mentions superconductors, which are materials that can conduct electricity without any resistance or energy loss, even at extremely low temperatures. Some possible questions that this text can answer related to superconductors include:\n\n1. What are superconductors?\n2. How do superconductors work?\n3. What are the different types of superconductors?\n4. What are some real-world applications of superconductors?\n5. How have recent developments in superconductors impacted their properties or uses?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b94ada67-94c1-4eb7-a4dd-6f203bb98d8c": {"__data__": {"id_": "b94ada67-94c1-4eb7-a4dd-6f203bb98d8c", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-news-special-edition-openai-developer-day-e955f16db4e2.html", "file_name": "llamaindex-news-special-edition-openai-developer-day-e955f16db4e2.html", "file_type": "text/html", "file_size": 3202, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-news-special-edition-openai-developer-day-e955f16db4e2.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about the recent developments in the world of LLMs (Large Language Models) by OpenAI. It highlights the features released during their developer day conference, such as support for two new models - \"gpt-4-1106-preview\" and \"gpt-4-vision-preview\". The text also mentions the integration of Azure OpenAI endpoints, new embeddings abstractions, and function calling. The text concludes by mentioning an update to their demo, SEC Insights, which now uses the latest version of GPT-4. Some questions that this text can answer include:\n\n- What are the new LLMs, \"gpt-4-1106-preview\" and \"gpt-4-vision-preview\", released by OpenAI during their developer day conference?\n- What are the features of these new LLMs?\n- How can I use these new LLMs in my applications?\n- What is the difference between \"gpt-4-1106-preview\" and \"gpt-4-vision-preview\"?\n- How can I integrate Azure OpenAI endpoints into my applications?\n- What are the new embeddings abstractions released by OpenAI?\n- How can I use these new embeddings abstractions in my applications?\n- How can I use function calling with OpenAI's LLMs?\n- How can I access the updated SEC Insights demo using the latest version of GPT-4?\n\nOverall, the text provides insights into the latest developments in the LLM space by OpenAI, which can be useful for developers and researchers working in this field.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49f1def6-1d5e-41b6-9a7a-8bcb23d9a250": {"__data__": {"id_": "49f1def6-1d5e-41b6-9a7a-8bcb23d9a250", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-02-13-26fa79601ba5.html", "file_name": "llamaindex-newsletter-2023-02-13-26fa79601ba5.html", "file_type": "text/html", "file_size": 10106, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-02-13-26fa79601ba5.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a newsletter from LlamaIndex, a company focused on language model indexing and retrieval. It highlights recent developments and releases from LlamaIndex, including the latest open-source release, Self-RAG, and the integration of LlamaIndex with FlowiseAI. The newsletter also features guides, tutorials, and webinars related to LlamaIndex and RAG (Retrieval as a Service) workflows, as well as updates on partnerships and enterprise-ready initiatives. Some questions that this text can answer include:\n\n- What is Self-RAG and how is it integrated into LlamaIndex?\n- How can LlamaIndex be used with FlowiseAI for RAG app development?\n- What is MistralAI's RAG guide with LlamaIndex, and what embedding models are used?\n- How can agents be built in LlamaIndex.TS, and what features are included?\n- What are some popular tutorials and guides related to LlamaIndex and RAG workflows?\n- Who are some notable speakers from recent webinars, and what topics were covered?\n- How can enterprises get more information about LlamaIndex's enterprise-ready initiatives, and what upcoming products are available for partners?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db379d9e-d881-4f08-bb6a-bf7ea6c03076": {"__data__": {"id_": "db379d9e-d881-4f08-bb6a-bf7ea6c03076", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-17-33514cbc04a2.html", "file_name": "llamaindex-newsletter-2023-10-17-33514cbc04a2.html", "file_type": "text/html", "file_size": 15358, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-17-33514cbc04a2.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a newsletter from LlamaIndex, highlighting various updates and developments related to their framework for building search engines using LLMs. The text covers topics such as hackathons, guides, integrations, features, webinars, tutorials, and demos. Some of the questions that this text can answer include: who spoke at the AI.Engineer Summit and what topics were covered in their talks, which projects won at the AGI House Hackathon, what new features and developments were introduced, what guides and tutorials are available for evaluation and demonstration purposes, who built YC Bot and what is its functionality, what is Multi-Document Agents and how does it improve query methods beyond basic vector indexing, how does UnstructuredIO enhance LLM/RAG applications and what is the UnstructuredElementNodeParser, what is Cross-Encoder Fine-Tuning and how can it be used to refine post-embedding search results, which new data readers have been integrated into LLamaIndex, and what types of data can they handle. The text also mentions webinars and podcasts, such as a webinar on Time-based retrieval for RAG and a podcast given by Jerry Liu on a specific topic.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae3fa659-adca-476a-a932-11719039fdc4": {"__data__": {"id_": "ae3fa659-adca-476a-a932-11719039fdc4", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-24-4a76204eeaa3.html", "file_name": "llamaindex-newsletter-2023-10-24-4a76204eeaa3.html", "file_type": "text/html", "file_size": 10205, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-24-4a76204eeaa3.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a newsletter highlighting recent developments and updates related to LlamaIndex, an open-source project focused on improving the performance of LLMs (Large Language Models) through RAG (Recursive Auto-Regressive) and other techniques. The newsletter covers feature releases, guides, tutorials, integrations, webinars, and collaborations. Some of the questions that this text can answer include:\n\n- What are some recent feature releases by LlamaIndex, and how do they enhance the performance and functionality of LLMs?\n- What is the QueryFusionRetriever, and how does it allow users to generate multiple queries with LLMs and apply reciprocal rank fusion for improved results?\n- What is the Router Fine-Tuning approach, and how has it achieved a 99% match rate, outperforming both the gpt-3.5\u2019s 65% and the base model\u2019s 12%?\n- What is the SQLRetriever, and how does it merge Text-to-SQL and RAG to enable a RAG pipeline setup over SQL databases for structured table node retrieval and response synthesis?\n- Who are some of the individuals and organizations that have contributed to LlamaIndex through tutorials, guides, and collaborations, and what are some of their noteworthy efforts?\n- What are some of the recent integrations and collaborations that LlamaIndex has established with other organizations and projects, such as Gradient AI, PrivateGPT, VectorFlow, and AI21 Labs LLMs?\n- What are some of the webinars and events related to LLM development and application that LlamaIndex has been involved in, and what are some of the key takeaways from these events?\n\nOverall, the newsletter provides insights into the latest advancements and trends related to LLM research and development, and how they are being applied and integrated into real-world applications and projects.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8eb79a05-ff5d-4057-86bf-9f1bbd6bc8b3": {"__data__": {"id_": "8eb79a05-ff5d-4057-86bf-9f1bbd6bc8b3", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-31-36244e2b3f0c.html", "file_name": "llamaindex-newsletter-2023-10-31-36244e2b3f0c.html", "file_type": "text/html", "file_size": 11836, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-31-36244e2b3f0c.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is an update on recent developments and releases related to the open-source LLMs Llama 2 and LlamaIndex, as well as Retrieval Augmented Generation (RAG). It discusses fine-tuning of these LLMs, integrating them into LlamaIndex pipelines, accessing a vast collection of models through the HuggingFace Inference API, and provides links to documentation and examples. Some of the questions that this text can answer include how to fine-tune Llama 2 and LlamaIndex, how to integrate RAG into an LLM pipeline, how to access a wide range of models through the HuggingFace Inference API for conversational, text generation, and feature extraction endpoints, and where to find documentation and examples related to these topics. Additionally, the text mentions a webinar on Unlocking ChatGPT for Business and a workshop on Retrieval Augmented Generation with LlamaIndex, providing further resources for those interested in learning more.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "369ac48f-7909-45e3-b24d-7c27e92212db": {"__data__": {"id_": "369ac48f-7909-45e3-b24d-7c27e92212db", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-07-cf20b9a833aa.html", "file_name": "llamaindex-newsletter-2023-11-07-cf20b9a833aa.html", "file_type": "text/html", "file_size": 11074, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-07-cf20b9a833aa.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a newsletter from LlamaIndex, a popular LLM (Large Language Model) framework for text and multimedia search and analysis. It summarizes the latest developments and releases from the past week, including feature releases, demos, guides, tutorials, integrations, and collaborations. The text covers topics such as LlamaIndex Chat, Evaluator Fine-Tuning, ParamTuner, CohereAI Embed v3, Voyage AI Integration, Activeloop's Deep Memory, Emotion Prompting, MongoDB starter kit, HuggingFace's text-embeddings-inference server, Zephyr-7b-beta, Small-to-Big Retrieval, Router Query Engine, and Tavily AI research API. Some of the questions that this text can answer include how to create a customizable LLM chatbot template, enhance LLM output assessment, refine RAG pipeline performance, integrate various tools and APIs, deploy LLM technology on an AWS EC2 GPU instance, set up multiple indices/query engines for a dataset, and build advanced RAG systems. Overall, the text highlights the ongoing advancements and contributions to the LLM field by the LlamaIndex community.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "91d0debf-63e5-4b72-b5de-658f6d9bbcdf": {"__data__": {"id_": "91d0debf-63e5-4b72-b5de-658f6d9bbcdf", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-14-dad06ae4284a.html", "file_name": "llamaindex-newsletter-2023-11-14-dad06ae4284a.html", "file_type": "text/html", "file_size": 11683, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-14-dad06ae4284a.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a newsletter from LlamaIndex, a vector database that enables users to index, search, and query knowledge graphs, documents, and databases utilizing state-of-the-art large-language models (LLMs). The newsletter highlights recent releases and explorations by the company, including a multi-modal RAG stack for complex document and image Q&A, OpenAIAssistantAgent abstractions, parallel function calling, and the MechGPT project by Professor Markus J. Buehler from MIT. The newsletter also includes tutorials, guides, and demos on topics such as building advanced RAG with the Assistants API, evaluating the OpenAI Assistant API vs RAG, and crafting a GPT Builder. Notable individuals in the LLM and AI space, including Jerry J. Li\u00fa, Bhavesh Bhat, David Garnitz, Harshad Suryawanshi, Sudarshan Koirala, and Ravi Theja, have contributed tutorials and guides to the newsletter. Some questions that this text can answer include how to use LLamaIndex to index, search, and query knowledge graphs, documents, and databases utilizing LLMs, how to build advanced RAG with the Assistants API, how to evaluate the OpenAI Assistant API vs RAG, and how to craft a GPT Builder. Additionally, the text provides links to tutorials and webinars on Building My Own ChatGPT Vision, Creating OpenAI Assistant Agent, Boosting RAG, and analyzing tables using GPT-4, as well as names of experts in the field of AI and natural language processing that individuals can follow on Twitter to stay updated with the latest developments and insights.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e56ce492-07a5-4612-a6fe-5677c2b2ecf0": {"__data__": {"id_": "e56ce492-07a5-4612-a6fe-5677c2b2ecf0", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-21-aa3a71e339f8.html", "file_name": "llamaindex-newsletter-2023-11-21-aa3a71e339f8.html", "file_type": "text/html", "file_size": 10593, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-21-aa3a71e339f8.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a newsletter from LlamaIndex, a company focused on developing and deploying open-source LLM indexing and search technologies. The newsletter discusses recent developments, including the increasing popularity of their technology, various feature releases and enhancements, upcoming events, and guides and tutorials for using their products. Some questions that this text can answer include:\n\n- What is the Retool.com State of AI 2023 survey and how significant is LlamaIndex's presence in it?\n- What are some of the new features introduced in the LlamaIndex 0.9 release, such as streamlined data handling, automated caching, improved text processing interfaces, tokenizer updates, PyPi packaging enhancements, consistent import paths, and the beta version of MultiModal RAG Modules?\n- What is the difference between multi-modal evaluation and traditional text evaluation, and how can MultiModalRelevancyEvaluator and MultiModalFaithfulnessEvaluator be applied in multi-modal settings?\n- What is the CLI tool \"create-llama\" and how can it be used to build full-stack LLM apps with options like FastAPI, ExpressJS, and Next.js for backends and a Next.js frontend with Vercel AI SDK components?\n- What is the fine-tuning of the Cohere reranker and how does it improve retrieval performance in the RAG pipeline?\n- What is the integration with Chroma's multi-modal collections and how does it allow for indexing both text and images in a single collection, enhancing RAG pipelines by combining text and image information for use with multi-modal models like GPT-4V, LLaVa, and Fuyu?\n- What are some of the guides and tutorials available for using LlamaIndex, including Multi-Modal Retrieval using GPT text embedding and CLIP image embedding for Wikipedia Articles, using the index to chat with data, and building a full-stack financial analysis bot using \"create-llama\" and Llama Index's RAG?\n- Who are some of the people who have created tutorials on LlamaIndex, such as Wenqi Glantz, Glenn Parham, Sudarshan Koirala, and Ravi Theja?\n- What is the CEO of Ll", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f199c3b-e433-46ed-b4ec-67d697c45bb8": {"__data__": {"id_": "0f199c3b-e433-46ed-b4ec-67d697c45bb8", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-28-a31be430a786.html", "file_name": "llamaindex-newsletter-2023-11-28-a31be430a786.html", "file_type": "text/html", "file_size": 9884, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-28-a31be430a786.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a newsletter from LlamaIndex, a company specializing in LLM (Large Language Model) indexing and search technology. The newsletter highlights recent developments and updates from LlamaIndex, such as the release of Llama Packs, a series of prepackaged modules and templates for LLM app development, and the introduction of the RAGs project for programming AI agents using natural language. The newsletter also covers feature releases and enhancements, integrations, guides, and tutorials related to LlamaIndex and LLMs. Some questions that this text can answer include:\n- What is LlamaIndex and what services does it offer?\n- What are Llama Packs and how do they simplify LLM app development?\n- What is the RAGs project and how does it allow for programming AI agents using natural language?\n- What are some recent feature releases and enhancements related to LlamaIndex and LLMs?\n- Who are some individuals and organizations that have utilized LlamaIndex technology in their work or projects?\n- Where can I find guides and tutorials related to LLMs and LlamaIndex?\n- What are some upcoming events or webinars related to LLM and AI technology?\n- How can I sign up for a workshop on Building an Open Source RAG Application Using LlamaIndex?\n- How can I provide feedback or contribute to the development of LlamaIndex technology?\n- How can I stay informed about future updates and releases from LlamaIndex?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ace7cd41-9c8e-45ae-a906-cbd7bdea2de3": {"__data__": {"id_": "ace7cd41-9c8e-45ae-a906-cbd7bdea2de3", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-05-faf5ab930264.html", "file_name": "llamaindex-newsletter-2023-12-05-faf5ab930264.html", "file_type": "text/html", "file_size": 13772, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-05-faf5ab930264.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a newsletter from the LlamaIndex community, highlighting recent developments and updates related to the LlamaIndex framework for integrating large language models (LLMs) into applications. It includes announcements about collaborations, new product launches, and speed enhancements in structured metadata extraction. Some of the questions that this text can answer include how to simplify building advanced RAG systems using LlamaPacks, how to evaluate RAG systems using the OpenAI Cookbook, how the speed enhancement in structured metadata extraction has improved RAG performance, how to set up a RAG + Streamlit app using the StreamlitChatPack, who developed the AInimal Go app and what features it offers, how the Table Transformer model is being used with GPT-4V for advanced RAG applications in parsing tables from PDFs, how to simplify complex app development using the core guide for full-stack app development, who created the Hands-On RAG guide for personal data with Vespa and LLamaIndex, and who created the tutorial on Llama Packs: The Low-Code Solution to Building Your LLM Apps. The text also mentions upcoming hackathons related to LLamaIndex and resources available for learning how to use it for various applications such as video content libraries and AI observability.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "393e9985-d181-4b5c-b17a-e2641a6d865d": {"__data__": {"id_": "393e9985-d181-4b5c-b17a-e2641a6d865d", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-12-4a5d542fbb1e.html", "file_name": "llamaindex-newsletter-2023-12-12-4a5d542fbb1e.html", "file_type": "text/html", "file_size": 14752, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-12-4a5d542fbb1e.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a compilation of various resources related to LlamaIndex, an open-source framework for semantic search and natural language processing. It highlights recent developments and offerings related to LlamaIndex, such as the launch of Llama Datasets, RAGs v5, and AutoTranslateDoc, as well as updates to the production RAG pipeline and LlamaHub. This text can answer questions related to the features and capabilities of LlamaDatasets, the enhancements in the production RAG pipeline, the benefits of using Ollama LlamaPack, and the availability of tutorials and guides for specific tasks, as well as integrations and webinars featuring LlamaIndex and related technologies. It also provides information on how to get in touch with the LlamaIndex team if interested in becoming an enterprise partner.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eec58ba9-7ab0-455a-9b62-b68a9faf700d": {"__data__": {"id_": "eec58ba9-7ab0-455a-9b62-b68a9faf700d", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-19-2965a2d03726.html", "file_name": "llamaindex-newsletter-2023-12-19-2965a2d03726.html", "file_type": "text/html", "file_size": 15040, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-19-2965a2d03726.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a newsletter from LlamaIndex, a company focused on LLM/RAG technologies. This newsletter highlights recent advancements, partnerships, and resources related to LlamaIndex. Some of the questions that this text can answer include:\n\n1. What recent advancements has LlamaIndex made in the field of LLM/RAG technologies?\n2. Which prominent organizations have partnered with LlamaIndex to enhance their LLM/RAG pipelines?\n3. What new datasets has LlamaIndex introduced to support RAG research and evaluation?\n4. What new features have been added to the Create-llama template in LlamaIndex?\n5. What new QA capabilities has LlamaIndex added through their partnership with Google Gemini?\n6. What new multi-modal and LLM integrations has LlamaIndex introduced through their partnerships with MistralAI and Docugami, respectively?\n7. What new community demos have been showcased using LlamaIndex's technology, such as Mozilla's MemoryCache and OpenBB Finance's enhanced chat widget in Terminal Pro?\n8. What new guides and resources has LlamaIndex provided for enhancing RAG pipelines with advanced LLMs and RAG workflows?\n\nSome questions that this text cannot answer include:\n\n1. What specific improvements have resulted from the partnership between LlamaIndex and Google Gemini in terms of LLM/RAG capabilities?\n2. How have the new datasets introduced by LlamaIndex advanced RAG research and evaluation, and what specific question complexities do they offer?\n3. How have the new multi-modal and LLM integrations introduced by LlamaIndex through their partnerships with MistralAI and Docugami, respectively, enhanced QA performance and capabilities?\n4. How has the SharePoint data loader introduced by LlamaIndex improved the integration of SharePoint files into LLM/RAG pipelines?\n5. How have the new community demos showcased using LlamaIndex's technology, such as Mozilla's MemoryCache and OpenBB Finance's enhanced chat widget in Terminal Pro, specifically improved personal knowledge management and accuracy in large context management?\n6. How have the new guides and resources provided by LlamaIndex for enhancing RAG pipelines with advanced LLMs and R", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2b6eec6-c895-492e-8b70-45b0c98b4436": {"__data__": {"id_": "f2b6eec6-c895-492e-8b70-45b0c98b4436", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-02-f349db8c1842.html", "file_name": "llamaindex-newsletter-2024-01-02-f349db8c1842.html", "file_type": "text/html", "file_size": 17293, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-02-f349db8c1842.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a newsletter from the LlamaIndex team that highlights their recent developments and releases. It discusses various new tools and techniques such as LLMCompiler, MultiDocAutoRetrieverPack, Structured Hierarchical RAG, and a new lower-level agent API. These tools and techniques are designed to improve the efficiency and scalability of retrieval and generation tasks, as well as provide better structured retrieval and dynamic responses to large documents and metadata. The text also mentions the integration with OpenRouterAI and the introduction of a new agent API for enhanced transparency, debuggability, and control. Some of the questions that this text can answer include how to build custom agents for complex queries, how to use LLMCompiler for faster, efficient handling of complex queries, how to implement MultiDocAutoRetrieverPack for structured retrieval and dynamic responses to large documents and metadata, how to use Structured Hierarchical RAG for optimized retrieval over multiple documents, and how to build a devbot that understands and writes code using RAG. The text also provides links to various resources related to natural language processing and AI models, including guides, tutorials, webinars, and enterprise-related information related to LlamaIndex and related technologies such as RAG, LLMs, and multi-modal settings.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab81fa3d-da8f-42cf-8b30-fa58fab75ffb": {"__data__": {"id_": "ab81fa3d-da8f-42cf-8b30-fa58fab75ffb", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-09-6209000da2e6.html", "file_name": "llamaindex-newsletter-2024-01-09-6209000da2e6.html", "file_type": "text/html", "file_size": 13781, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-09-6209000da2e6.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a newsletter from LlamaIndex, an open-source library for semantic search and question answering using large language models (LLMs). The newsletter highlights recent updates and developments in the LlamaIndex ecosystem, including new features such as Query Pipelines, ETL Pipeline, Multimodal ReAct Agent, RAGatouille LlamaPack, and the open-source roadmap for 2024. It also includes community demos, guides, and tutorials related to LlamaIndex and its associated tools and technologies. Some of the questions that this text can answer include explaining what LlamaIndex is, introducing new features and releases, discussing community demos, and providing resources for learning how to use LlamaIndex in various applications, such as building an AI shopping assistant, improving Advanced RAG, and building a chatbot. It also provides information on upcoming products for LlamaIndex in enterprise settings and webinars on AI data management with Weights & Biases.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4f4106f-d3e0-42c8-97a3-3014f377c37e": {"__data__": {"id_": "b4f4106f-d3e0-42c8-97a3-3014f377c37e", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-16-752195bed96d.html", "file_name": "llamaindex-newsletter-2024-01-16-752195bed96d.html", "file_type": "text/html", "file_size": 9794, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-16-752195bed96d.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a newsletter from LlamaIndex, a company that provides a framework for building advanced question-answering and search applications using LLMs (Large Language Models). The newsletter highlights the latest developments, features, and enhancements in LlamaIndex, such as Chain-of-Table Framework, LLM Self-Consistency Mechanism, Semantic Text Splitting in RAG, Parallel RAG Ingestion, TogetherAI\u2019s Embeddings Support, AgentSearch-v1, and Ensembling and Fusion in Advanced RAG. The newsletter also includes guides, tutorials, events, and calls to action for enterprises interested in using LlamaIndex. Some of the questions that this text can answer include:\n\n- What is LlamaIndex and what is it used for?\n- What are some of the recent developments in LlamaIndex, such as Chain-of-Table Framework, LLM Self-Consistency Mechanism, Semantic Text Splitting in RAG, Parallel RAG Ingestion, TogetherAI\u2019s Embeddings Support, AgentSearch-v1, and Ensembling and Fusion in Advanced RAG?\n- Where can I find guides and tutorials on using LlamaIndex for building full-stack RAG applications, understanding the importance of reranking in advanced RAG pipelines, transforming invoice data into JSON, and building AI apps with local LLMs running on Windows with NVIDIA?\n- Who gave a talk on building multi-tenancy RAG systems with LlamaIndex and Qdrant at FOSS United, Bangalore, India?\n- How can enterprises get in touch with LlamaIndex to learn more about its upcoming products available for partners?\n\nThe newsletter also includes links to the sources of information, which can provide further details and context on the topics covered.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5eabadc2-9f73-4400-81df-e9167e1c1595": {"__data__": {"id_": "5eabadc2-9f73-4400-81df-e9167e1c1595", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-23-11ee2c211bab.html", "file_name": "llamaindex-newsletter-2024-01-23-11ee2c211bab.html", "file_type": "text/html", "file_size": 10910, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-23-11ee2c211bab.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a newsletter from LlamaIndex, a company focused on large language models (LLMs) and related technologies. The newsletter highlights recent updates and developments from LlamaIndex, including announcements for an upcoming hackathon and a webinar featuring Sehoon Kim and Amir Gholami. It also features guides, tutorials, and demos on topics such as advanced query transformations, long-context embedding models, and composable retrievers. The newsletter introduces new features such as RankGPT, a technique that leverages GPT-3.5 and GPT-4 for document ranking, and Composable Retrievers, an interface centralizing advanced retrieval and RAG techniques. It also includes demos, such as RAG-Maestro for ArXiv Research, and guides on topics such as Multi-Stock Ticker Analysis and Advanced QA over Tabular Data. The newsletter also includes feature releases and enhancements, such as LITS support for streaming on all endpoints and the integration with Tonic Validate for LLM-powered evaluations. Overall, the text provides insights into the latest advancements and developments in the field of LLMs and related technologies by LlamaIndex. Some of the questions that this text can answer include: \n- What are some recent updates and developments from LlamaIndex?\n- What new features has LlamaIndex introduced?\n- What guides, tutorials, and demos is LlamaIndex currently offering?\n- What are some of the recent announcements from LlamaIndex, such as the upcoming hackathon and webinar featuring Sehoon Kim and Amir Gholami?\n- What are some of the latest advancements and developments in the field of LLMs and related technologies that LlamaIndex is currently working on?\n- Who has developed RAG-Maestro for ArXiv Research, and what is it used for?\n- What is Composable Retrievers, and how does it simplify creating complex RAG setups by allowing you to define IndexNodes to link different retrievers or RAG pipelines?\n- What is RankGPT, and how does it leverage GPT-3.5 and GPT-4 for efficient document ranking, featuring a unique sliding window strategy for handling large contexts?\n- What is Long-Context Embedding Models, and how do they offer a solution to the", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6916b918-0b45-4117-bab9-1c322353fb32": {"__data__": {"id_": "6916b918-0b45-4117-bab9-1c322353fb32", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-30-0d01eb0d8cef.html", "file_name": "llamaindex-newsletter-2024-01-30-0d01eb0d8cef.html", "file_type": "text/html", "file_size": 11180, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-30-0d01eb0d8cef.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is primarily a newsletter from LlamaIndex, a company focused on building efficient and scalable knowledge extraction and search engines using large-scale language models (LLMs). The newsletter highlights recent developments and updates, including new features, tutorials, guides, and webinars. Some questions that this text can answer include:\n\n- What are some of the latest feature releases and enhancements from LlamaIndex, such as RAG CLI, JSONalyze, and support for OpenAI embeddings?\n- How can I use LlamaIndex to build a ReAct agent, and what are the essential components for creating agents, including reasoning prompts, output parsing, tool selection, and memory management?\n- What is a Slack bot that learns from conversations and accurately answers organizational queries, and how can I build one using LlamaIndex, Qdrant Engine, and Render?\n- What is LLMCompiler, and how can I use it to jump-start my RAG pipelines with advanced retrieval LlamaPacks and benchmark with Lighthouz AI?\n- How can I efficiently summarize large JSON datasets using JSONalyze, and what are the advanced SQL queries that can be performed on them?\n- What is the RAG CLI, and how can I use it for local file indexing and search with advanced integration and customization features?\n- What are some of the upcoming products that are being developed for enterprise-ready LlamaIndex, and how can I get in touch with the company to learn more?\n- How can I attend the upcoming LlamaIndex hackathon, and what are some of the prizes and resources that will be provided to participants?\n- What is the LlamaPack with Vanna AI, and how can it be used to create advanced text-to-SQL tools using RAG for storing, indexing, and generating SQL queries?\n- How can I integrate LlamaIndex with Neutrino to take advantage of its GPT-4 level performance at significantly reduced costs by smartly allocating queries to the most suitable model from a diverse range?\n- How can I attend the LlamaIndex hackathon, and what are some of the prizes and resources that will be provided to participants?\n- What is the LlamaIndex hackathon, and how can I participate in it to collaborate with other developers and win", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "df738da3-01fa-4c8f-accf-3057e7d28913": {"__data__": {"id_": "df738da3-01fa-4c8f-accf-3057e7d28913", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-06-9a303130ad9f.html", "file_name": "llamaindex-newsletter-2024-02-06-9a303130ad9f.html", "file_type": "text/html", "file_size": 9110, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-06-9a303130ad9f.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about the latest updates and developments related to LlamaIndex, an open-source RAG (Retrieval Augmented Generation) system based on LLM (Large Language Models) technology. The text highlights various feature releases, guides, and tutorials related to LlamaIndex, as well as community contributions and educational resources. It also announces a new $2,000 bounty program with Replit for advanced RAG with LlamaIndex. The text answers questions such as what are the latest updates and releases related to LlamaIndex, what are some of the guides and tutorials available for learning and using LlamaIndex, who are some of the community contributors and what are their projects, and how can enterprises get involved with LlamaIndex. It also mentions various demos and events related to LlamaIndex. Overall, the text provides a comprehensive overview of the latest happenings in the LlamaIndex ecosystem.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3bdedc09-2d20-4d3c-b0e7-5048dc2d3a23": {"__data__": {"id_": "3bdedc09-2d20-4d3c-b0e7-5048dc2d3a23", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4.html", "file_name": "llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4.html", "file_type": "text/html", "file_size": 10623, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is an email newsletter from LlamaIndex, a company specializing in large-scale knowledge representation and reasoning systems. The newsletter announces the launch of LlamaCloud, a suite of managed parsing, ingestion, and retrieval services tailored for production-grade context augmentation in LLM and RAG applications. It also highlights recent developments, feature releases, demos, guides, and tutorials related to LlamaIndex and RAG (Retrieval-Augmented Generation). The text can answer questions such as:\n- What is LlamaIndex and what services does it offer?\n- What is RAG and how is it related to LlamaIndex?\n- What are the recent developments and feature releases related to LlamaIndex and RAG?\n- Who are some of the companies and individuals that have used LlamaIndex and RAG, and what are their applications?\n- What are some of the practical tips and tricks for building production-ready RAG, as inspired by Sisil Mehta from JasperAI?\n- What are some of the recent demos and guides related to LlamaIndex and RAG?\n- Who are some of the people who have provided tutorials related to LlamaIndex and RAG, and what are their topics?\n- What is the upcoming webinar related to LlamaIndex and Flowise, and what will it cover?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a921b43b-bc2d-4860-9b27-e223b02236fe": {"__data__": {"id_": "a921b43b-bc2d-4860-9b27-e223b02236fe", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-27-4b9102a0f824.html", "file_name": "llamaindex-newsletter-2024-02-27-4b9102a0f824.html", "file_type": "text/html", "file_size": 12685, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-27-4b9102a0f824.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text consists of various resources related to RAG (Recorded Action Grammar), a framework used for document comparison and verification. It includes links to guides, tutorials, and a webinar on RAG development, simplification, and productionization. Some of the questions that this text can answer include where to find resources on simplifying advanced RAG development, how to build an interactive chatbot using RAG with React, understanding the 12 RAG pain points and solutions in the RAG pipeline, and learning practical tips and tricks for productionizing RAG from Sisil at JasperAI through a webinar. Overall, the text provides resources for individuals interested in learning about RAG development, simplification, and productionization.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a547101-75f8-4471-92bf-c1b454ce94fb": {"__data__": {"id_": "7a547101-75f8-4471-92bf-c1b454ce94fb", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-05.html", "file_name": "llamaindex-newsletter-2024-03-05.html", "file_type": "text/html", "file_size": 13864, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-05.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a newsletter from LlamaIndex, a company offering tools and services for natural language processing, highlighting recent developments and features in their products. These features include the release of LlamaParse, a PDF parsing service, the introduction of llama-index-networks for combining answers from independent RAG apps over the network, improved parsing and OCR support for 81+ languages in LlamaParse, first-class support for the Groq inference engine, and a new llama-index-packs feature for RAPTOR, a tree-structured technique for advanced RAG. The text also mentions the release of LlamaCloud, a world-beating PDF parsing service, and the expansion of the daily usage cap for LlamaParse. This text can answer questions related to using these features, such as how to use LlamaParse and LlamaCloud, how to combine answers from independent RAG apps over the network using llama-index-networks, and how to use new techniques like RAPTOR with LlamaIndex's tools and services. The text also includes links to resources and guides related to LlamaIndex and its usage, such as a notebook on building Basic RAG with LlamaIndex, webinars on building RAG applications, and leveling up LLM applications with observability. Other questions that this text can answer include who presented a notebook on building Basic RAG with LlamaIndex and who did a webinar with Traceloop on leveling up LLM applications with observability.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "57234cb7-d09a-4463-bbcc-fa30974cfa17": {"__data__": {"id_": "57234cb7-d09a-4463-bbcc-fa30974cfa17", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-12.html", "file_name": "llamaindex-newsletter-2024-03-12.html", "file_type": "text/html", "file_size": 11140, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-12.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a blog post from LlamaIndex, a project that aims to provide a simple-to-use, efficient, and scalable framework for applying LLMs to a range of tasks. The post provides an update on recent developments in the LlamaIndex project, including feature releases, enhancements, demos, guides, and tutorials. Some of the questions that this text can answer include:\n\n- What are some new features that have been released in LlamaIndex, such as LlamaParse JSON Mode and Hierarchical Code Splitting?\n- Who are some notable individuals and organizations that have contributed to recent developments in the project, such as ryanpeach, Raymond Weitekamp, and Anthropic?\n- What are some practical applications of LlamaIndex, such as building local LLM agents for step-wise execution, creating a real-time RAG chatbot using Google Drive and Sharepoint, or leveraging LLamaindex for efficient document handling?\n- Who are some organizations and individuals that have used LlamaIndex in their projects, such as Nething.xyz, AI Makerspace, and mithril-security?\n- What are some upcoming events related to LlamaIndex, such as a RAG meetup in Paris on March 27th?\n\nOverall, the text provides a comprehensive overview of recent developments in the LlamaIndex project, highlighting its diverse applications and contributions from various individuals and organizations.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6678a088-8c15-4de7-bc3a-1644f9402804": {"__data__": {"id_": "6678a088-8c15-4de7-bc3a-1644f9402804", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-19.html", "file_name": "llamaindex-newsletter-2024-03-19.html", "file_type": "text/html", "file_size": 11259, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-19.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a newsletter from LlamaIndex, a company focused on advancing the state-of-the-art in RAG (Retrieval-Augmented Generation). It includes updates on new releases and features, such as the launch of LlamaParse, a GenAI native document parsing and interpretation tool, and the introduction of natural language parsing instructions, allowing for the extraction of math snippets from PDFs into LaTeX. The text also highlights recent developments, including the release of LlamaIndex v0.10.20, which introduces a new Instrumentation module for observability, and the integration of open-source observability for RAG pipelines through collaboration with langfuse. Additionally, the text mentions demos, such as Home AI, which uses LLMs to automate the parsing of complex property disclosures, and tutorials, such as those on using LlamaIndex and Mathpix to parse, index, and query complex mathematics within scientific papers. The text also includes guides, such as one on using LlamaIndex and Mathpix to parse, index, and query complex mathematics within scientific papers, and webinars, such as one with Charles Packer on Long-Term, Self-Editing Memory with MemGPT. Overall, this text provides updates on advancements in the field of RAG and related technologies. Some questions that this text can answer include how to properly extract text from complex documents using LlamaParse, how to match candidates to jobs based on their CV using LlamaParse and Qdrant, and how to use LlamaIndex and Mathpix to parse, index, and query complex mathematics within scientific papers.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d67b4d2-3a1f-44eb-8833-57deaecf65db": {"__data__": {"id_": "8d67b4d2-3a1f-44eb-8833-57deaecf65db", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-26.html", "file_name": "llamaindex-newsletter-2024-03-26.html", "file_type": "text/html", "file_size": 13007, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-26.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a newsletter from LlamaIndex that highlights recent developments and advancements in their technology, particularly regarding privacy-preserving techniques and integrations with other tools. It covers topics such as privacy-preserving techniques for LlamaPack and RAG Network, the introduction of a privacy-preserving RAG Network, the launch of a LlamaPack based on a paper by Xinyu Tang, and the integration of Databricks Vector Search into LlamaIndex by BAM Elevate. The text also includes guides and tutorials for advanced RAG and agents with MistralAI, integrating custom models with LlamaIndex, and secure RAG with LLM-guard by Protect AI. Some of the questions that this text can answer include how to securely ingest and retrieve data using LLM-powered systems against deceptive manipulations, how to prototype on patient data safely, and how to develop a production-grade RAG pipeline with LlamaParse and LlamaIndex, as well as creating an advanced PDF RAG agent, showcasing RAG with LlamaIndex on 15 Indian languages, and registering for a webinar featuring LaVague. Additionally, it mentions a Panel discussion on 'Why RAG Will Never Die - The Context Window Myth' and the topics that will be covered in the RAG meetup in Paris on March 27th.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75952d41-8f05-41a6-9a17-8a13e46d162e": {"__data__": {"id_": "75952d41-8f05-41a6-9a17-8a13e46d162e", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-02.html", "file_name": "llamaindex-newsletter-2024-04-02.html", "file_type": "text/html", "file_size": 13087, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-02.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a weekly update from the LlamaIndex community that highlights recent developments and releases related to natural language processing (NLP) technologies. Some of the topics covered in this update include the launch of RAFTDatasetPack, the integration of Cohere's Int8 and binary embeddings, the release of Python docs with accessible example notebooks, advanced search, and comprehensive API details. This text also discusses collaborations such as the DeepLearningAI course on integrating RAG into a full-stack application and the RAFTDatasetPack LlamaPack for dataset generation. By using RAG, which combines information retrieval with text generation to improve the accuracy and relevance of answers to complex questions, applications such as insurance policy interpretation, financial news summarization, and PDF text extraction can be realized. Some of the questions that this text can answer include how to construct a backend API, how to deploy a RAG server for real-time use, how to improve LLM responses on coverage queries, how to design efficient RAG systems, how to handle concurrent requests, and how to fail-resiliently. The resources provided in this update cover efficient embedding serving, concurrent request handling, failure resilience, and real-time usage, making RAG a powerful tool for tasks such as legal research, medical diagnosis, and customer support.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d1d146f-234d-4722-9a44-7cae1d22ebeb": {"__data__": {"id_": "3d1d146f-234d-4722-9a44-7cae1d22ebeb", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-09.html", "file_name": "llamaindex-newsletter-2024-04-09.html", "file_type": "text/html", "file_size": 11147, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-09.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a weekly update from the LlamaIndex community, outlining recent developments and releases. It includes highlights such as the integration of RankLLM into LlamaIndex, the launch of a LlamaIndex and MistralAI Cookbook Series, and the introduction of the Anthropic\u2019s Claude Function Calling Agent. The text also mentions feature releases and enhancements, demos, guides, tutorials, and webinars. Some questions that this text can answer include: What is the Anthropic\u2019s Claude Function Calling Agent and how does it enhance QA and workflow automation? What is RankLLM and how has it been integrated into LlamaIndex? Who launched the LlamaIndex and MistralAI Cookbook Series and for what purpose? What demos have been released, and for what use cases? What guides and tutorials are available, and what topics do they cover? Who presented webinars, and what were their topics? By reading this text, you can gain a better understanding of the current state of LlamaIndex and related developments.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da7a008c-194b-4127-8013-c0b28d2ac838": {"__data__": {"id_": "da7a008c-194b-4127-8013-c0b28d2ac838", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-16.html", "file_name": "llamaindex-newsletter-2024-04-16.html", "file_type": "text/html", "file_size": 13388, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-16.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a newsletter from LlamaIndex that highlights recent developments and releases related to RAG (Retrieval-Augmented Generation) and its applications in LLM (Language Modeling). Some of the questions that this text can answer include:\n\n1. What is the Chain of Abstraction Technique and how is it being implemented in LLM applications?\n2. What is the Create-tsi Toolkit and what features does it offer for building full-stack RAG applications?\n3. What is the Return_Direct feature and how does it enhance agent controllability in tools?\n4. What are some recent demos and guides related to RAG systems and LLM-generated knowledge graphs?\n5. What is the Best RAG Techniques paper and what findings does it highlight regarding various RAG methods?\n6. Who is Akash Mathur and what tutorial does he have on data management in LlamaIndex?\n7. Who is Leonie and what interactive tutorial does she have related to creating an app for conversing with code from a GitHub repository?\n8. What is the purpose of the tutorial by Hamza Gharbi on Building and Evaluating Advanced RAG and what topics does it cover?\n9. Who is Prof. Markus J. Buehler and what study does he have related to using LLM-generated knowledge graphs to accelerate biomaterials discovery?\n10. What is the purpose of the tutorial by Kingzzm and what features does it offer for building an agent capable of advanced document retrieval and maintaining conversation memory?\n\nThe provided text also mentions various sources that provide more information on RAG and its applications, such as tutorials, webinars, and social media profiles. Some of the questions that these sources can answer include:\n\n- What is RAG and how is it used in LLM applications?\n- How can RAG be used to create apps that allow for conversing with code from GitHub repositories?\n- How can RAG be enhanced to overcome the issue of 'broken' context in RAG construction?\n- How can RAG be used for pill search, including identifying unknown pills, checking drug interactions and side effects, and confirming proper dosage amounts?\n- Who are some of the key contributors to RAG, such as Tianjun Zhang and Shishir Patil, and what did they", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "969e0723-8b7a-400b-85b0-14e539747a3f": {"__data__": {"id_": "969e0723-8b7a-400b-85b0-14e539747a3f", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-23.html", "file_name": "llamaindex-newsletter-2024-04-23.html", "file_type": "text/html", "file_size": 10277, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-23.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a weekly update from LlamaWorld, highlighting various releases, demos, guides, and tutorials related to LlamaIndex, an open-source natural language indexing and querying engine. Some of the questions that this text can answer include:\n\n- What are some recent releases from LlamaWorld, and what are their features and benefits?\n- How can I use MistralAI's 8x22b model for RAG, query routing, and tool applications?\n- What is a new Llama 3 model, and where can I access it directly from Hugging Face?\n- What is a create-llama template, and how can I use it to quickly start building full-stack LLM applications using the nextjs-llama3 template?\n- What is the new LlamaParse document parser, and how can I use it for PDF parsing?\n- What is the DREAM framework, and how can I use it for optimizing RAG setups in a distributed environment?\n- How can I integrate Qdrant Hybrid Cloud with LlamaIndex, featuring JinaAI embeddings and MistralAI's Mixtral 8x7b?\n- How can I build a RAG application using completely open and free components from Elastic, featuring Ollama and MistralAI?\n- How can I create an agent that writes code by reading my documentation, and what tools can I use for this purpose?\n- How can I fine-tune Hugging Face models using LlamaIndex's finetuning techniques, including steps from quantization to fine-tuning with QLoRA?\n- How can I use LlamaIndex with Azure's AI Search to create powerful RAG applications, including Hybrid Search, Query Rewriting, and SubQuestionQuery Engine?\n- How can I build a Finance Agent with LlamaIndex to query public companies and look up stock prices, summarize financial news, and plot stock data?\n- How can I enhance a RAG pipeline with \"state\" storage for a more personalized, conversational assistant using LlamaIndex's custom agent and query pipeline abstractions?\n- How can I fine-tune embedding models for RAG with LoRA using LlamaIndex's finetuning abstractions?\n\nSome of the questions that this text cannot answer", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f10e29e1-4b5a-4907-b71e-704f75f98031": {"__data__": {"id_": "f10e29e1-4b5a-4907-b71e-704f75f98031", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-30.html", "file_name": "llamaindex-newsletter-2024-04-30.html", "file_type": "text/html", "file_size": 8240, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-30.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a newsletter summarizing recent updates and developments related to LlamaIndex, a project focused on building context-aware applications using large language models (LLMs). Some of the questions that this text can answer include:\n\n- What are the highlights of the latest newsletter from LlamaIndex?\n- What new features and releases have been announced for LlamaIndex and related projects?\n- Who appeared on a security podcast related to LlamaIndex?\n- What tutorials and guides are available for building LLM applications using LlamaIndex?\n- What upcoming webinars are related to LlamaIndex?\n- Where can I find a new user group for LlamaIndex in Korea?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0a1109d-4e46-4423-b755-80a3bb4af515": {"__data__": {"id_": "c0a1109d-4e46-4423-b755-80a3bb4af515", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-07.html", "file_name": "llamaindex-newsletter-2024-05-07.html", "file_type": "text/html", "file_size": 11641, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-07.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a newsletter from LlamaIndex, highlighting recent updates and developments in their open-source project. It discusses new releases for both LlamaIndex.ts and LlamaIndex Python, including feature enhancements and support for various web streams and agents. The text also introduces the LlamaPack for the Reflection Agentic Pattern (RAG) and provides links to demos, guides, and tutorials related to agentic RAG, semantic search, and workflow automation. Some of the questions that this text can answer include how to filter AirBnB listings using natural language, how to set up a local RAG pipeline using Llama 3, Ollama, and LlamaIndex, how to optimize RAG supervised embeddings using reranking with your data, and how to build agents dedicated to workflow automation, as well as topics related to agentic RAG, semantic caching, and production-ready techniques, as discussed by Divyanshu Dixit and Tyler Hutcherson. Other resources mentioned in the text include Cleanlab's tutorial on getting trustworthiness scores from your RAG pipeline to avoid hallucinations and course-correct, and webinars on deploying AI applications to AWS and RAG, covering topics from basic RAG to handling long-context RAG and evaluating the RAG pipeline.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f1ff09e-313f-4b07-9807-32dc5a2b9fc2": {"__data__": {"id_": "3f1ff09e-313f-4b07-9807-32dc5a2b9fc2", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-14.html", "file_name": "llamaindex-newsletter-2024-05-14.html", "file_type": "text/html", "file_size": 8288, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-14.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a weekly update from LlamaIndex, a company that offers tools and resources for natural language processing and language modeling. The update highlights recent developments, guides, and tutorials related to LlamaIndex and its associated technologies, such as RAG (Retrieval-as-generation) and Llama3 (an open-source, research-grade language model). Some of the questions that this text can answer include:\n\n- What is the new LlamaIndex course in collaboration with DeepLearningAI, and how can it help improve RAG skills?\n- What are some recent feature releases and enhancements in LlamaIndex, including support for GPT-4o and Llama3 cookbooks?\n- What guides and tutorials are available for building agents, using RAG for content moderation, and advanced PDF parsing with LlamaParse?\n- Who are some individuals and organizations that have contributed to the development of LlamaIndex and its associated technologies, and what are their areas of expertise?\n- How can evaluation libraries like TruLens, Ragas, UpTrain, and DeepEval be used to assess RAG systems using metrics like faithfulness, relevance, and answer correctness?\n\nBy providing a summary of recent developments and resources, this text can help individuals and organizations interested in language modeling and natural language processing stay up-to-date with the latest advancements and tools available through LlamaIndex and its associated technologies.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b086fcf1-f08b-4afc-8097-9471a965c1ce": {"__data__": {"id_": "b086fcf1-f08b-4afc-8097-9471a965c1ce", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-21.html", "file_name": "llamaindex-newsletter-2024-05-21.html", "file_type": "text/html", "file_size": 9430, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-21.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a weekly update from LlamaIndex, a company focused on generative AI. It highlights recent developments and releases from LlamaIndex, including integration with Google Cloud's Vertex AI and the introduction of GPT-4o in LlamaParse. The text also includes guides, tutorials, and webinars on various topics related to generative AI, such as Text-to-SQL, document parsing, and chunk sizes for RAG evaluation. Some questions that this text can answer include:\n\n- What are the latest feature releases and enhancements from LlamaIndex, and how do they improve document parsing and structured image extraction?\n- How can I use GPT-4o in LlamaParse for enhanced document parsing, and what are the increased costs associated with this?\n- How can I optimize chunk sizes for RAG evaluation in production environments, and what is the impact on evaluation metrics?\n- What is the memary architecture and its role in long-term memory for autonomous systems? How can I learn more about this topic through a webinar with LlamaIndex and the memary team?\n- How can I attend the first-ever LlamaIndex meetup in San Francisco and connect with the company's team and partners from Activeloop and Tryolabs?\n", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "097d486c-14e7-46cd-819f-029c8fdedf6f": {"__data__": {"id_": "097d486c-14e7-46cd-819f-029c8fdedf6f", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-28.html", "file_name": "llamaindex-newsletter-2024-05-28.html", "file_type": "text/html", "file_size": 9205, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-28.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about the latest weekly updates from LlamaIndex, a company that provides tools for natural language processing and generative AI. The text highlights various integration updates, detailed guides, demos, educational tutorials, and informative webinars. It mentions several feature releases and enhancements, including secure code execution with AzureCodeInterpreterTool, integration with Nomic embed, and support for Vespa vector store. The text also introduces new projects like LlamaFS for organized files and RAGApp for no-code chatbots. Some questions that this text can answer include:\n\n- What are the latest updates from LlamaIndex and what new features have been released?\n- How can I build an automated Email Agent using MultiOn and LlamaIndex?\n- What is LlamaFS and how does it automatically organize messy file directories?\n- How can I integrate DSPy bootstrapping models with a LlamaIndex RAG pipeline powered by LlamaParse?\n- How can I enhance image search using LlamaIndex and Qdrant Engine's capabilities?\n- How can I build a RAG chatbot using Llamaindex, Groq with Llama3 & Chainlit?\n- How can I learn to build an Open-Source Coding Assistant using OpenDevin?\n\nOverall, the text provides insights into the latest developments in the field of generative AI and natural language processing, as well as practical examples and tutorials for building various applications and tools.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "124f07cf-be0c-49f5-bd03-4d3dc22ff9eb": {"__data__": {"id_": "124f07cf-be0c-49f5-bd03-4d3dc22ff9eb", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-04.html", "file_name": "llamaindex-newsletter-2024-06-04.html", "file_type": "text/html", "file_size": 10632, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-04.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a newsletter from LlamaIndex, a framework for building AI applications. The newsletter covers updates, content, and resources related to LlamaIndex, particularly for working with knowledge graphs. Some of the questions that this text can answer include:\n\n1. What are the highlights of the latest LlamaIndex release?\n2. What new features and enhancements have been introduced in the latest release?\n3. How can LlamaIndex be used to build knowledge graphs with LLMs, and what new tools and techniques are available for this purpose?\n4. What is the Property Graph Index, and how does it transform how knowledge graphs are built and queried?\n5. How can LlamaParse be used to convert complex spreadsheet files into LLM-friendly tables for improved performance and data handling?\n6. What is Codestral, and how does it support over 80 programming languages for code generation?\n7. What is the Milvus Lite integration, and how can it be used to provide an easy start to vector search?\n8. What is the PostgresML integration, and how can it be used to build AI applications?\n9. What are some guides, demos, and tutorials available for working with LlamaIndex, including building custom graph retrievers, constructing knowledge graphs using local models and Neo4j, and building GenAI applications using NVIDIA's NIM inference microservices?\n10. What is the FinTextQA benchmark dataset, and how was it evaluated using LlamaIndex's Auto-Merging and Sentence Window Retrievers, along with various embeddings, rerankers, and LLMs?\n11. Who presented a tutorial on Serving A LlamaIndex RAG App as REST APIs, and where can I find this tutorial?\n12. What is Sherlock Xu's tutorial from BentoML on Serving A LlamaIndex RAG App as REST APIs, and where can I find it?\n13. Who introduced a new benchmark dataset for long-form financial question answering, and what was the name of this benchmark?\n14. What is the memary benchmark, and who introduced it?\n15. Where can I find a webinar with authors of memary?\n16. Where can I find more", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "afd66fa0-1373-44e1-af17-88f0aeb7d1c6": {"__data__": {"id_": "afd66fa0-1373-44e1-af17-88f0aeb7d1c6", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-11.html", "file_name": "llamaindex-newsletter-2024-06-11.html", "file_type": "text/html", "file_size": 11257, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-11.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a newsletter from LlamaIndex, highlighting recent updates, guides, demos, educational tutorials, and webinars designed to enhance the understanding and experience with their platforms and tools. Some of the questions that this text can answer include:\n\n- What are the new memory modules in LlamaIndex and how do they boost agentic RAG capabilities?\n- How was the integration of Create-llama and E2B turned into an advanced data analysis feature for agents?\n- How can LlamaParse and Knowledge Graphs be used to develop RAG pipelines and agents for complex query handling?\n- What is Prometheus-2 RAG Evaluation and how can it be used to effectively evaluate RAG applications?\n- What is the difference between RAG retrieval and agentic RAG retrieval, and how can agents be transformed into powerful data analysts using Python coding?\n- What is Nomic-Embed-Vision and how does it transform Nomic-Embed-Text into a multimodal embedding for handling image, text, and combined tasks?\n- What are some techniques for query rewriting to enhance RAG pipelines, such as sub-question decomposition, HyDE for aligning questions with embedding semantics, and step-back prompting?\n- Who are some experts that have shared tutorials on Agentic RAG Systems and what insights do they offer into advanced system building, including router query engines, function calling, and multi-step reasoning across complex documents?\n- What is a RAG pipeline for securing RAG apps using Azure for application security, including identity management, secure key storage, and managed Qdrant?\n- Who is presenting a webinar with LlamaIndex on LlamaIndex property graph for insights into high-level and low-level graph construction, retrieval, and knowledge graph agents?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aad60b80-c0fb-4533-a3d9-887157de0f0a": {"__data__": {"id_": "aad60b80-c0fb-4533-a3d9-887157de0f0a", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-18.html", "file_name": "llamaindex-newsletter-2024-06-18.html", "file_type": "text/html", "file_size": 12216, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-18.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text updates us on various developments related to LlamaIndex, a tool for building web agents. It highlights integrations with other technologies such as MoA, TiDB, and SingleStoreDB, as well as the release of a cookbook for building RAG and agents. The text also mentions real-world use cases of LlamaIndex, tutorials on building web agents, and a webinar on the future of web agents with MultiOn. Some of the questions that this text can answer include the identities and contributions of developers like Mervin Praison, Akriti Upadhya, and Kingzzm, the tools and technologies used for building web agents like local models, chainlit, GroqInc, LlamaParse, and Nougat, as well as advanced RAG patterns and the agentification of the internet through MultiOn. Overall, the text provides insights into the versatility and applicability of LlamaIndex in various use cases, from research and development to enterprise and industry applications.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c70e20ef-07ef-40b0-b508-debf9e34f01c": {"__data__": {"id_": "c70e20ef-07ef-40b0-b508-debf9e34f01c", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-25.html", "file_name": "llamaindex-newsletter-2024-06-25.html", "file_type": "text/html", "file_size": 7557, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-25.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a newsletter from LlamaIndex, highlighting recent updates, feature releases, demos, guides, and tutorials related to their tools. This newsletter covers various topics such as integrating CrewAI and MistralAI, building agents, using LlamaIndex pipelines with MLflow, CRAG for financial analysis, automating GitHub commits, and creating custom text-to-SQL pipelines. It can answer questions related to these topics, such as how to integrate CrewAI and MistralAI, how to build agents using LlamaIndex, how to use CRAG for financial analysis, how to automate GitHub commits using Composio and LlamaIndex Tools, and how to create custom text-to-SQL pipelines using LlamaIndex's DAG capabilities.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "07f91239-eab0-4b77-9d44-83bbfed20201": {"__data__": {"id_": "07f91239-eab0-4b77-9d44-83bbfed20201", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-02.html", "file_name": "llamaindex-newsletter-2024-07-02.html", "file_type": "text/html", "file_size": 9058, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-02.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a newsletter from LlamaIndex, highlighting recent developments and updates in their platform. Some of the questions that this text can answer include:\n\n- What is the significance of the launch of LlamaCloud, the fully-managed ingestion service for LLM applications?\n- What is the difference between the alpha-release framework llama-agents and traditional LLM applications?\n- How can create-llama be integrated with LlamaCloud for efficient system maintenance and data pipeline setup?\n- What is DSPy's role in enhancing query pipelines, optimizing prompts, or repurposing DSPy predictors using LLM applications?\n- How can automating code reviews with LlamaIndex be achieved using Composio's AI agent?\n- What is the process of building an agentic RAG service with LlamaIndex, as outlined in the provided guide?\n- Who are some of the content creators featured in the newsletter, and what tutorials and guides do they offer?\n\nOverall, the text provides insights into recent developments in LLM applications, highlighting new frameworks, integration opportunities, and use cases. It also showcases various guides, demos, and tutorials that offer practical examples for building LLM applications, optimizing prompts, and integrating AI agents into production environments.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "87813008-0ec9-4f71-b345-d58f4909ecd6": {"__data__": {"id_": "87813008-0ec9-4f71-b345-d58f4909ecd6", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-09.html", "file_name": "llamaindex-newsletter-2024-07-09.html", "file_type": "text/html", "file_size": 12456, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-09.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a compilation of resources and tutorials related to building AI-enabled trading assistants, conducting semantic information searches, and evaluating RAG pipelines using tools like LlamaIndex, Giskard, and ReAct. It covers various topics such as portfolio tracking, stock order management, question generation for RAG evaluation, and financial analysis of SEC documents. Some of the questions that this text can answer include how to use LlamaIndex and Giskard for diverse question generation, how to build a Multi-Document Financial Analyst Agent using LlamaIndex and Ollama, how to convert datasets to BEIR format for RAG evaluation, and how to evaluate RAG pipelines using essential metrics like precision@K and NDCG. Overall, this text provides links to demos, guides, and tutorials related to these topics.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "442318f5-bd6e-4b42-8c8d-905dde904aca": {"__data__": {"id_": "442318f5-bd6e-4b42-8c8d-905dde904aca", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-16.html", "file_name": "llamaindex-newsletter-2024-07-16.html", "file_type": "text/html", "file_size": 12112, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-16.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a newsletter from LlamaIndex, a company that offers tools and services for natural language processing using LLMs (Large Language Models). The newsletter announces the launch of LlamaCloud, a data processing layer that enhances RAG workflows with advanced parsing, indexing, and retrieval capabilities. It also introduces LlamaTrace, a collaboration with Arize AI that provides tracing, observability, and evaluation capabilities for LLM application workflows. The newsletter highlights feature releases and enhancements, including the integration of Redis Queue with llama-agents and the implementation of GraphRAG concepts with LlamaIndex. It provides links to demos, guides, tutorials, and events, some of which have achieved over $1M ARR using LlamaIndex. This text can answer questions about the latest developments and offerings from LlamaIndex, including its products, services, and collaborations. It can also provide insights into natural language processing using LLMs and how LlamaIndex's tools and services can be used for various applications. \n\nAdditionally, the text mentions a Llama 3 AI hackathon being hosted by multiple organizations, and it provides information about participating organizations and the prizes and credits being offered. The text also includes links to tutorials on using LlamaIndex and llama-agents, as well as a mention of a tutorial by Mervin Praison on using llama-agents. Overall, this text provides insights into LlamaIndex's products, services, and collaborations, as well as related events and tutorials.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "54c4d011-d894-42de-8b06-3f68bfdfae0a": {"__data__": {"id_": "54c4d011-d894-42de-8b06-3f68bfdfae0a", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.html", "file_name": "llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.html", "file_type": "text/html", "file_size": 11234, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a summary of a podcast episode where the speaker, Sam Charrington, interviewed Jerry Chen about LlamaIndex, a toolkit for connecting language models to data. The summary highlights the key points discussed during the podcast, such as the origin story of LlamaIndex, its evolution from a fun tool to a set of useful tools and instructions, and the advanced primitives it offers beyond top-k retrieval. The summary also touches on the future of LlamaIndex, including the automation of decision-making and the unification of everything under a single query interface. The text also provides instructions on how to ask questions using LlamaIndex over the podcast transcript. Some questions that this text can answer include:\n\n- What are the three key points described in the podcast episode about LlamaIndex?\n- What is the origin story of LlamaIndex, and how did it come about?\n- What advanced primitives does LlamaIndex offer beyond top-k retrieval, and how do they work?\n- How does LlamaIndex plan to automate decision-making and unify everything under a single query interface?\n- What is the interface between LLM-based data processing systems and data sources of record, and how does it evolve?\n- How can I ask my own questions over the podcast using LlamaIndex? Where can I find the resources I need to do this?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e550df24-500a-47da-871e-45c9c237c6c5": {"__data__": {"id_": "e550df24-500a-47da-871e-45c9c237c6c5", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.html", "file_name": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.html", "file_type": "text/html", "file_size": 44532, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses the evaluation process of the Prometheus model, an open-source alternative to the traditional reliance on GPT-4 for RAG (Retrieval-Augmented Generation) evaluation tasks. It compares the effectiveness of Prometheus and GPT-4 in three distinct evaluation metrics: Correctness, Faithfulness, and Context Relevancy. The text provides a step-by-step guide on how to set up an evaluation pipeline using the Prometheus model and the LlamaIndex framework, while simultaneously comparing its performance with GPT-4. The text also mentions the cost analysis of using these models. Some potential questions that this text can answer include:\n\n1. How does the Prometheus model compare to GPT-4 in terms of evaluation metrics?\n2. What is the Prometheus model and how is it being used for evaluation tasks?\n3. How can one set up an evaluation pipeline using the Prometheus model and the LlamaIndex framework?\n4. How does Prometheus perform compared to GPT-4 in terms of feedback accuracy and context relevance?\n5. Are there any potential hallucinations or wrong interpretations in the feedback provided by Prometheus?\n6. What is the cost of using Prometheus and GPT-4 for evaluation tasks?\n7. How does the Prometheus model compare to GPT-4 in terms of cost analysis?\n8. What further research or investigation might be needed to fully understand the effectiveness and limitations of the Prometheus model in comparison to GPT-4?\n9. How can the insights gained from this evaluation process be applied in practice?\n10. What implications or consequences might be associated with the use of Prometheus and GPT-4 for evaluation tasks, particularly in terms of accuracy, context relevance, and cost?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7122d6bc-8686-4963-8965-c45b7dcc93da": {"__data__": {"id_": "7122d6bc-8686-4963-8965-c45b7dcc93da", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-turns-1-f69dcdd45fe3.html", "file_name": "llamaindex-turns-1-f69dcdd45fe3.html", "file_type": "text/html", "file_size": 8172, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-turns-1-f69dcdd45fe3.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a blog post celebrating the one-year anniversary of LlamaIndex, an open-source library for retrieval-augmented generation (RAG) with LLMs. It highlights the significant growth and impact of LlamaIndex in the past year, including the number of contributors, projects, and downloads, as well as its usage in popular applications and enterprise settings. The post also mentions some major milestones, such as the launch of GPT Tree Index, support for indexing embeddings, and the introduction of ChatGPT API and plugins. It also announces the company's incorporation and recent funding. Some questions that this text can answer include:\n\n- How many contributors, projects, and downloads does LlamaIndex have after one year?\n- Which popular applications and enterprise settings are using LlamaIndex?\n- What major milestones has LlamaIndex achieved in the past year?\n- What new features has LlamaIndex introduced, such as GPT Tree Index, embedding indexing, and support for ChatGPT API and plugins?\n- What recent developments has LlamaIndex announced, such as its incorporation and recent funding?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f20eb78-358b-4525-950a-d014c9b5a4e4": {"__data__": {"id_": "4f20eb78-358b-4525-950a-d014c9b5a4e4", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-update-07-10-2023-4ceebdab96cb.html", "file_name": "llamaindex-update-07-10-2023-4ceebdab96cb.html", "file_type": "text/html", "file_size": 14119, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-update-07-10-2023-4ceebdab96cb.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is an update on the latest features, integrations, and advancements in the open-source project, LlamaIndex. LlamaIndex is a toolkit for embedding, indexing, and querying text data using large language models (LLMs). It enables the creation of customizable and scalable indices, query engines, and agents for efficient and accurate text retrieval, reasoning, and generation. The text highlights various ways to use LlamaIndex, including loading data from different databases, refreshing private data sources, enriching LLM models, automatically extracting metadata, and integrating with tools like Chainlit.io, Github Issues reader, and DePlot model for interpreting charts and plots. Some specific questions that the text can answer include how to load data into Weaviate and connect LlamaIndex to a Weaviate instance, how to use LlamaIndex with LLM apps using PydanticProgram, how to use LlamaIndex with SQL querying process, and how to use LlamaIndex with code-based extraction for efficient data extraction. Additionally, the text provides resources such as tutorials, video tutorials, blog posts, webinars, and podcasts for learning how to use LlamaIndex for tasks like refreshing private data sources, loading data from graph databases, customizing LLMs, prompts, and embeddings, and building LLM workflows, agents, and assistants for various scenarios.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a7ff547-14ef-496b-a8e6-4ed239047a07": {"__data__": {"id_": "7a7ff547-14ef-496b-a8e6-4ed239047a07", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-update-08-01-2023-185514d9b897.html", "file_name": "llamaindex-update-08-01-2023-185514d9b897.html", "file_type": "text/html", "file_size": 18864, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-update-08-01-2023-185514d9b897.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is an update on recent developments in the LlamaIndex project, which is a text indexing and retrieval library built on top of Python's Pandas and Scikit-Learn. It allows for the creation of advanced query engines, powered by LLMs, to perform semantic search through numerical text representations. Some of the questions that this text can answer include how to use Llama2 with LlamaIndex and Weaviate to perform semantic search on external data, how to build and evaluate advanced query engines with LlamaIndex, how to integrate LLMs into investment research workflows using OpenBB and LlamaIndex, how to use LlamaIndex to build a QA system, how to use LlamaIndex in combination with Arize and TruLens to build an LLM app, how to process a wide variety of data from both structured and unstructured sources using LlamaIndex, how to leverage LlamaIndex from concept to production, and how to participate in events, workshops, and demos related to LlamaIndex to learn more about its capabilities and how to use it effectively. The text covers topics such as the launch of Data Agents, which combine AI agents with data and offer more than 15 tool specs for easy integration; LlamaIndex's support for over 20 vector databases; the integration of Llama2 models and the launch of two new LLMs, Anthropic Claude 2.0 and Chroma v0.4.0; the launch of ContextChatEngine, which addresses the issue of conversational agents hallucinating information by ensuring retrieval of context with every user interaction; and the launch of LlamaIndex's BEIR integration and its support for BEIR, an Information Retrieval benchmark. The text also mentions the newly launched Data Agents' ability to automatically interact with any API defined via an OpenAPI spec, which facilitates easy integration of the OpenAPI Tool, which helps the data agent concentrate on tool selection and action orchestration. The text also notes several tutorials for LlamaIndex, including Adam Hofmann's blog post on building better tools for LLM agents and Weaviate iate's tutorial on using LlamaIndex with Weaviate.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e35d539-6390-48bb-9205-e3b17e1cd9e0": {"__data__": {"id_": "5e35d539-6390-48bb-9205-e3b17e1cd9e0", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-update-09-03-2023-4a7c21c0f60b.html", "file_name": "llamaindex-update-09-03-2023-4a7c21c0f60b.html", "file_type": "text/html", "file_size": 34061, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-update-09-03-2023-4a7c21c0f60b.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is an update on recent developments and features in the LlamaIndex and TypeScript-based LLM framework. It highlights advancements in areas such as integration with Metaphor, support for OpenAI's fine-tuned models, the Prompt system, the Recursive Dreaming technique for enhancing RAG, integration with BagelDB and AskMarvinAI, simplification of retrieval evaluations, and callback handling support. This text can answer questions such as how LlamaIndex integrates with Metaphor, how to use callback handling support, how to deploy LLMs for fine-tuned model support, how to use metadata for structured retrieval over large documents, and how to improve RAG pipelines using summaries and the Recursive Dreaming technique. Additionally, the text provides resources for further exploration of LlamaIndex's capabilities, including events, demos, and papers. Some of the organizations and applications that have used LlamaIndex include those in medicine, SEC document analysis, and building a startup with a 3D interface.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3285e0fd-d0b5-48f4-b79e-57eb2c3e4b26": {"__data__": {"id_": "3285e0fd-d0b5-48f4-b79e-57eb2c3e4b26", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-update-20-09-2023-86ed66f78bac.html", "file_name": "llamaindex-update-20-09-2023-86ed66f78bac.html", "file_type": "text/html", "file_size": 15447, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-update-20-09-2023-86ed66f78bac.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is an update on the progress of the LlamaIndex project, which allows for the creation of data agents with LLMs (Language Models). It highlights various features and enhancements, such as the open-sourcing of the RAG platform, the release of Replit templates, and the launch of LlamaIndex.TS with MongoDBReader and enhanced keyword indexing. The text also provides tips for improving RAG retrieval, offers tutorials and guides for building RAG from scratch, and mentions events, webinars, and integrations with external platforms like PortkeyAI, Elastic, MultiOn, Vectara, LiteLLM, and MonsterAPI. Some of the questions that this text can answer include how to improve RAG retrieval, how to build RAG from scratch, which platforms LLamaIndex integrates with, who spoke about LLM applications at an Arize AI event, who conducted a workshop on LLamaIndex at a meetup, and what webinars are available related to LLamaIndex and LLM challenges in production.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9bb11259-1760-43ad-b6f5-00a7d237f2fb": {"__data__": {"id_": "9bb11259-1760-43ad-b6f5-00a7d237f2fb", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-update-2023-10-10-3718a3d19fb9.html", "file_name": "llamaindex-update-2023-10-10-3718a3d19fb9.html", "file_type": "text/html", "file_size": 19541, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-update-2023-10-10-3718a3d19fb9.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a weekly update on recent advancements in the LLM (Large Language Models) and RAG (Retrieval-Augmented Generation) fields, specifically highlighting releases and enhancements from the LlamaIndex library and related papers. This text includes information about new features, such as full observability with Arize AI Phoenix and improved extraction through OpenAI's latest function calling fine-tuning. It also mentions winners of a recent Streamlit Hackathon, new modules like RetrieverEvaluator and LongContextReorder, and a streamlined deployment option for the secinsights.ai open-sourced RAG app template. This text can answer questions related to using LlamaIndex for low-level module ingestion and retrieval, building multi-document chatbots, query strategies for knowledge graphs, evaluating chunk sizes for RAG, and integrating LlamaIndex with various tools and databases. It also provides links to documentation, tutorials, webinars, and events related to LlamaIndex. However, the text mainly provides summaries and links, rather than detailed explanations or answers to specific questions. Some additional resources, such as notebooks or slides, may also be available at the linked locations.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e10a12c9-04f9-4687-8409-d2b28ecf0a17": {"__data__": {"id_": "e10a12c9-04f9-4687-8409-d2b28ecf0a17", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-update-6-26-2023-ed30a9d45f84.html", "file_name": "llamaindex-update-6-26-2023-ed30a9d45f84.html", "file_type": "text/html", "file_size": 16810, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-update-6-26-2023-ed30a9d45f84.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is an update on the progress of the LlamaIndex project, which aims to create a data framework for Large Language Models (LLMs). It highlights recent developments such as the integration of Prem App with LlamaIndex for privacy in AI development, the ability to extract tabular data frames from unstructured text, tutorials and webinars on how to use LlamaIndex, hackathons and events related to LlamaIndex, and encourages feedback and suggestions from the community. Some questions that this text can answer include:\n\n- What is LlamaIndex and what is its purpose?\n- What recent developments have occurred in the LlamaIndex project?\n- Where can I find tutorials and resources on how to use LlamaIndex?\n- Are there any upcoming events or webinars related to LlamaIndex that I can attend?\n- How can I provide feedback or suggestions to the LlamaIndex team?\n\nThis text can also answer questions related to new features and integrations for the LlamaIndex platform, such as the support for a new stack with NebulaGraph for unique retrieval-augmented generation techniques, the enhancement of transparency in LlamaIndex applications with the new CitationQueryEngine, the direct prompting of JSON keys and conversion of Pydantic objects into the Guidance format, the selection of relevant choices given a query for complex data collections with the MultiSelector object in the new multi-router feature, the enhancement of the functionality of LLM indices over various data types with the new Object Index wrapper, and the new tracing feature of the TruLens team, which allows developers to evaluate and track their experiments more efficiently.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "081baa8f-e7a1-4804-b75e-e6c7702709b3": {"__data__": {"id_": "081baa8f-e7a1-4804-b75e-e6c7702709b3", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-v0-10-838e735948f8.html", "file_name": "llamaindex-v0-10-838e735948f8.html", "file_type": "text/html", "file_size": 26572, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-v0-10-838e735948f8.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is announcing the release of version 0.10.0 of the Python library llama_index, which aims to simplify the development of LLM-powered applications and improve the user experience. The new version introduces a new core architecture, enables LLM and vector store indexing, and deprecates the ServiceContext. It also introduces a new central hub called LlamaHub, which will serve as the main hub for all third-party integrations. The provided text explains the reasons behind these updates, such as dealing with breaking changes, the use of ServiceContext, and the deprecation of the llama-hub repo. It also provides examples of how to use the library with different integrations and packages, as well as how to contribute to the library's core refactors or integrations/packs. Some of the questions that this text can answer include how to upgrade to version 0.10, how to contribute integrations and packs, where to find the new package registry, how to report bugs, and where to seek help. It also provides links to the updated documentation, migration guide, and repository, as well as examples of notebooks.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a382cabf-7150-4c27-8e29-9158df4d8cd8": {"__data__": {"id_": "a382cabf-7150-4c27-8e29-9158df4d8cd8", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-vectara-7a3889cd34cb.html", "file_name": "llamaindex-vectara-7a3889cd34cb.html", "file_type": "text/html", "file_size": 13681, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-vectara-7a3889cd34cb.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses two technologies, LlamaIndex and Vectara. LlamaIndex is a data framework for building LLM (Large Language Model) applications, which allows users to define a data pipeline for their application using composable modules. Vectara, on the other hand, is an end-to-end platform that offers powerful generative AI capabilities for developers, including data processing, vector and text storage, query flow, and security and privacy. Some questions that this text can answer related to LlamaIndex include how to ingest data into a VectorStoreIndex, how to set up LlamaIndex, and how to use chat-engines with LlamaIndex. Regarding Vectara, this text can answer questions such as how to create a VectaraIndex instance, how to use Vectara for querying, and how Vectara simplifies the complexities of large language models, embedding models, vector databases, and MLOps for developers.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd19d281-9b4e-442f-8f4f-c9a6007f3ff6": {"__data__": {"id_": "dd19d281-9b4e-442f-8f4f-c9a6007f3ff6", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.html", "file_name": "llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.html", "file_type": "text/html", "file_size": 15546, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text explains how to create an agent using the LlamaIndex and Waii tools, along with the PDF Loader, for data analysis tasks. This agent can comprehend and respond to natural language inquiries, generate SQL queries, describe databases and datasets, and extract information from both databases and PDF files. Some of the questions that this text can answer include:\n\n1. What is the provided text about? (description query)\n2. Can you summarize the instructions for creating the agent? (summarize query)\n3. How do I use the Waii tool to generate SQL queries? (waii tool usage query)\n4. Can you describe the retail_data schema in more detail? (retail_data schema description query)\n5. Can you provide me with the top 10 item categories sold during Christmas time across all years? (top 10 item categories sold during Christmas time query)\n6. Can you retrieve the page from the holiday retailer report by Deloitte that shows the top gift categories during the holiday? (retrieve pdf page query)\n\nIn general, this text explains how to create an agent using a combination of text analysis, SQL queries, and PDF files to perform data analysis tasks. This agent can assist with tasks such as generating SQL queries, describing databases and datasets, and retrieving information from both databases and PDF files, which can be used to answer a variety of data analysis-related questions.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6530c364-f6dc-4b04-9631-9c18b4241caf": {"__data__": {"id_": "6530c364-f6dc-4b04-9631-9c18b4241caf", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.html", "file_name": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.html", "file_type": "text/html", "file_size": 14272, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about a technique called LongLLMLingua, which is designed to enhance the performance of large language models (LLMs) in scenarios involving retrieval-augmented generation (RAG) and long contexts. LongLLMLingua addresses issues such as performance drop, high costs, and context window limitations in RAG or long context scenarios via prompt compression. It can improve accuracy by up to 21.4% while only using \u00bc of the tokens and save $28 for every 1000 examples in long context situations. LongLLMLingua offers solutions to these problems through two main approaches: Re-ranking and Fine-grained prompt compression. The text also provides references for further reading. Some of the questions that this text can answer include: What are some issues that arise in RAG or long context scenarios and how does LongLLMLingua address them? How does LongLLMLingua improve accuracy in these scenarios? How does LongLLMLingua address the context window limitation in RAG scenarios? How does LongLLMLingua reduce costs in these scenarios? How does LongLLMLingua save costs in long context scenarios? How does LongLLMLingua compare to other retrieval methods in terms of accuracy? How does LongLLMLingua improve the integrity of key information in long context scenarios? How does LongLLMLingua address the \"lost in the middle\" situation in RAG scenarios? How does LongLLMLingua improve efficiency in these scenarios? How can LongLLMLingua be used in the widely used RAG framework, LlamaIndex?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9260d03a-a64d-44d8-837a-371c97930488": {"__data__": {"id_": "9260d03a-a64d-44d8-837a-371c97930488", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.html", "file_name": "mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.html", "file_type": "text/html", "file_size": 9736, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses the challenges of parsing PDFs, specifically text-only layered PDFs, due to their complex layouts, font encoding issues, non-linear text storage, and inconsistent use of spaces. It explores the limitations of LLMs in processing large documents and the role of retrieval-augmented generation (RAG) in overcoming these limitations. The text introduces LayoutPDFReader, a tool that can parse PDFs while preserving hierarchical layout information, including section and subsection hierarchy, paragraph merging, connections between sections, table recognition, and nested list structures. It also explains how the tool employs intelligent chunking to maintain the cohesion of related text. The text highlights that LLMSherpa leverages a cost-free and open API server, and it provides examples of how the tool can be used to generate a LLamaIndex query engine from the document chunks produced by LayoutPDFReader. Some questions that this text can answer include how to efficiently parse complex PDFs, how to overcome LLM limitations in processing large documents, and how to use RAG to improve information retrieval. The text also provides references for further reading on the topic.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec91c3cb-204a-4184-b27c-60145bb5ea4c": {"__data__": {"id_": "ec91c3cb-204a-4184-b27c-60145bb5ea4c", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/multi-modal-rag-621de7525fea.html", "file_name": "multi-modal-rag-621de7525fea.html", "file_type": "text/html", "file_size": 11704, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/multi-modal-rag-621de7525fea.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about Porsche, a well-known German automobile manufacturer. Some of the questions that this text can answer include:\n\n- What is Porsche?\n- What company is Porsche?\n- What type of product does Porsche produce?\n- What country is Porsche from?\n- What industry does Porsche belong to?\n\nIn more detail, Porsche is a company that produces high-performance sports cars, SUVs, and sedans. It is based in Germany and is a significant player in the automotive industry. The text provides basic information about Porsche, and further questions could include details about specific models, historical background, and technological innovations.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "713febfb-2255-496c-9648-087c01d69fcf": {"__data__": {"id_": "713febfb-2255-496c-9648-087c01d69fcf", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/multimodal-rag-building-ainimal-go-fecf8404ed97.html", "file_name": "multimodal-rag-building-ainimal-go-fecf8404ed97.html", "file_type": "text/html", "file_size": 16030, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/multimodal-rag-building-ainimal-go-fecf8404ed97.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about a project called 'AInimal Go!' which combines a specialized vision model, ResNet18, with a large language model (LLM) using LlamaIndex as the orchestration layer and Wikipedia articles as the knowledge base. The app allows users to upload or capture images of animals, with ResNet18 swiftly classifying the animal. The Cohere LLM API, adeptly orchestrated by LlamaIndex, takes over and enables users to engage in unique conversations about and with the identified animal, informed and enriched by a knowledge base of nearly 200 Wikipedia articles. This text can answer questions related to the functionality of the 'AInimal Go!' project, such as how it combines a specialized vision model with a large language model, how it uses LlamaIndex as the orchestration layer, and how it utilizes Wikipedia articles as a knowledge base. It can also answer questions about the specific vision model, ResNet18, and how it swiftly classifies animals in images. Additionally, it can answer questions about the Cohere LLM API and how it is orchestrated by LlamaIndex to provide accurate and relevant responses to user queries, as well as questions about the identified animal, such as its characteristics, behaviors, and habitat.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b398c01-24b6-4aa4-afda-d1b0fab6dd83": {"__data__": {"id_": "1b398c01-24b6-4aa4-afda-d1b0fab6dd83", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.html", "file_name": "multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.html", "file_type": "text/html", "file_size": 22025, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses two distinct topics: (1) the impacts of climate change on polar bears, and (2) the development of a multimodal RAG architecture using OpenAI's GPT4V and LanceDB to process and analyze video content. In terms of polar bears and climate change, the text explains how the Gaussian function, a mathematical concept that describes how rapidly something is changing, helps us understand the rate and magnitude of the changes that polar bears are experiencing due to climate change. It can answer questions related to how climate change impacts polar bears, such as how the Gaussian function helps explain these impacts, what insights can be gained about polar bears and climate change through the use of the Gaussian function, and what information can be obtained about the rate and magnitude of the changes that polar bears are experiencing due to climate change through the use of the Gaussian function. The text also touches upon the Central Limit Theorem, convolution of random variables, Gaussian function, and convolution of two Gaussians in relation to the Gaussian function's properties and significance in probability and statistics. \n\nIn terms of the multimodal RAG architecture using OpenAI's GPT4V and LanceDB, the text explains how this approach simplifies the video analysis process and enhances its accuracy and relevance. It describes how this architecture processes and analyzes video content, and how it can improve the video analysis process. Some possible questions that this text can answer related to this topic include: how this approach simplifies the video analysis process, how it enhances its accuracy and relevance, and what insights can be gained about video content analysis through the use of this architecture. The text also provides visual examples and explanations to clarify the mathematical concepts related to the Gaussian function and its significance in probability and statistics.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7516225c-ac9f-4846-b9df-7b9fd66a796e": {"__data__": {"id_": "7516225c-ac9f-4846-b9df-7b9fd66a796e", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.html", "file_name": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.html", "file_type": "text/html", "file_size": 14311, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses implementing multimodal RAG applications using LlamaIndex and Neo4j. It explains how to preprocess articles from Medium using BeautifulSoup, extract text and images, and then index the resulting vectors using Neo4j and LlamaIndex. The article also discusses creating a multimodal LLM using OpenAI's GPT-4-Vision model and demonstrating how to use this multimodal LLM to answer queries. The text can answer questions related to summarizing the content of specific articles, providing insights on the author's writing style, and identifying key themes or topics within the text, as well as how to initiate vector stores for text and images, how to create a multimodal LLM, and how to use the prompt template for querying the system.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bdb955cb-3791-4a33-bb8b-9b49ae4dc880": {"__data__": {"id_": "bdb955cb-3791-4a33-bb8b-9b49ae4dc880", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.html", "file_name": "newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.html", "file_type": "text/html", "file_size": 15857, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about a project called NewsGPT that utilizes LLamaIndex and Streamlit to create a chat application that can summarize and compare news articles based on different perspectives. This application, which won a hackathon, aims to save time by providing predefined prompts for questions and allowing for streaming of responses. Some of the questions that this text can answer include how NewsGPT uses LLamaIndex and Streamlit to create the chat application, what predefined prompts are provided for asking questions, how users can join and support the production app associated with NewsGPT called Neotice, and who the individuals behind NewsGPT are and how to connect with them on LinkedIn. Overall, the text is about a project that uses advanced technologies to simplify the process of summarizing and comparing news articles, making it more accessible and time-saving for users.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "003fa2ae-c366-4f7e-b6d1-2b03b19a8c23": {"__data__": {"id_": "003fa2ae-c366-4f7e-b6d1-2b03b19a8c23", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/nvidia-research-rag-with-long-context-llms-7d94d40090c4.html", "file_name": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.html", "file_type": "text/html", "file_size": 10854, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/nvidia-research-rag-with-long-context-llms-7d94d40090c4.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses a study that explores the effectiveness of combining retrieval augmentation and long context extension methods with large language models (LLMs) for long-context question answering and summarization tasks. The study compares the performance of LLMs with and without retrieval augmentation, as well as with different context window sizes, using a variety of datasets. The text also touches on the concept of the \"lost-in-the-middle\" phenomenon, which can negatively impact the performance of LLMs when handling long contexts. Some questions that this text can answer include:\n\n- How does retrieval augmentation impact the performance of LLMs with different context window sizes?\n- Are longer context LLMs always better for long-context tasks, or can shorter LLMs with retrieval augmentation achieve similar performance levels?\n- Which LLMs and datasets were used in the study, and what metrics were used to evaluate their performance?\n- How does the study's findings differ from those of other studies, such as Longbench's findings?\n- What are some best practices for implementing retrieval augmentation, such as how many chunks should be retrieved and how does the number of retrieved chunks impact performance?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "097b6b16-d85c-42fc-863c-cf2d6a082ee2": {"__data__": {"id_": "097b6b16-d85c-42fc-863c-cf2d6a082ee2", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/one-click-open-source-rag-observability-with-langfuse.html", "file_name": "one-click-open-source-rag-observability-with-langfuse.html", "file_type": "text/html", "file_size": 9729, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/one-click-open-source-rag-observability-with-langfuse.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about integrating Langfuse, an open source LLM engineering platform, with LlamaIndex, a popular library for building language models. This integration allows for detailed tracing and analysis of RAG (Retrieval-as-a-Service) applications built with LlamaIndex. The text highlights the ease of use and flexibility of this integration, as well as the additional features provided by Langfuse, such as session tracking, prompt management, and evaluation. By using this integration, users can gain insights into the performance, latency, and resource usage of their RAG applications, as well as identify areas for optimization and improvement. Some specific questions that this text can answer include:\n\n- What context is being retrieved from the index to answer a user's question?\n- How is chat memory being managed in a RAG application built with LlamaIndex?\n- Which steps in a RAG application built with LlamaIndex are adding the most latency, and how can they be optimized?\n- How can sessions be grouped and analyzed in a conversational RAG application?\n- How can prompts be version controlled, collaborated on, and evaluated using Langfuse?\n- How can RAG traces be evaluated using the RAGAS eval suite?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79033d51-6e72-4407-bcd3-8a1c1ea243f3": {"__data__": {"id_": "79033d51-6e72-4407-bcd3-8a1c1ea243f3", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/openai-cookbook-evaluating-rag-systems-fe393c61fb93.html", "file_name": "openai-cookbook-evaluating-rag-systems-fe393c61fb93.html", "file_type": "text/html", "file_size": 2220, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/openai-cookbook-evaluating-rag-systems-fe393c61fb93.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about the release of the OpenAI Cookbook, which is a guide to evaluating Retrieval-Augmented Generation (RAG) systems using LlamaIndex. The cookbook has three sections: understanding RAG systems, building RAG systems with LlamaIndex, and evaluating the performance of RAG systems in the areas of retrieval and response generation. The text mentions using a synthetic dataset generation method to conduct thorough evaluations. By reading this text, one can answer questions such as what is the OpenAI Cookbook, what are RAG systems, how to build RAG systems using LlamaIndex, and how to evaluate the performance of RAG systems in retrieval and response generation.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c78b9be-10aa-4203-b3a5-95c6452201a0": {"__data__": {"id_": "4c78b9be-10aa-4203-b3a5-95c6452201a0", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/pii-detector-hacking-privacy-in-rag.html", "file_name": "pii-detector-hacking-privacy-in-rag.html", "file_type": "text/html", "file_size": 17508, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/pii-detector-hacking-privacy-in-rag.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text contains personal information such as names, credit card numbers, email addresses, and locations, as well as details about individuals' medical licenses and last names. It also mentions attending a concert, browsing a website, and posting a photo online. This text raises concerns about privacy and potential data breaches. Some questions that this text can answer include identifying specific individuals by name or license number, providing details about their personal information, financial transactions, and website access. However, without prior knowledge, it is unclear whether the text provides further context or details about the specific individuals or entities mentioned. Overall, the text appears to provide a mix of personal and sensitive information that could potentially reveal individuals' identities, financial information, and personal interests. Some questions that this text can answer include who is the author of the text and what is their name? what is the credit card number and email address mentioned in the text? where does the author live? who is robo@presidio.site and have they been to a p\u00e1lmi einarsson concert before? what is the limit for a specific credit card number mentioned in the text? what is the author's last name? what is the URL that the author recently posted a photo to? These questions provide insights into the author's identity, financial information, and personal interests, but for privacy and security reasons, it is recommended to anonymize this type of sensitive information before sharing or processing it.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c990777c-dce7-4e5b-bb02-54cdd9180314": {"__data__": {"id_": "c990777c-dce7-4e5b-bb02-54cdd9180314", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.html", "file_name": "pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.html", "file_type": "text/html", "file_size": 14744, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses Accessory Dwelling Units (ADUs), also known as granny flats or in-law units, which are secondary living spaces on a property. The text highlights the challenges involved in creating ADUs, including traditional land survey and vendor consultation processes becoming increasingly difficult. It introduces an award-winning innovation, an ADU planning system, that simplifies the ADU creation process for users. This system navigates through city building codes, connects users with local vendors, and presents viable floor plan options within a user-friendly app. This text can answer questions about the benefits and potential of ADUs, the challenges and solutions involved in their creation process, the ADU market and its projected growth, and how the new application simplifies the process of constructing ADUs at home. It also mentions the commitment to personalizing the service further by adapting recommendations to align with individual budget constraints and specific layout preferences.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2716650b-a624-4ddc-8de6-9664b0a43286": {"__data__": {"id_": "2716650b-a624-4ddc-8de6-9664b0a43286", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.html", "file_name": "querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.html", "file_type": "text/html", "file_size": 13717, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text introduces a new library extension called llama-index-networks, which allows data suppliers to package their data in the form of RAGs (Rapidly-Aggregated Graphs) and share them with data consumers to expand their query systems' knowledge. This library extension enables the creation of RAG marketplaces, where data suppliers can prepare their data for consumption, while data consumers can easily access the required knowledge via a standard interface provided by the ContributorService. The text also touches on the potential benefits of connecting related RAGs for companies with multiple franchises or operations, as well as the importance of privacy and security in data sharing. Some questions that this text can answer include how connecting related RAGs benefits companies with multiple franchises or operations, what are some potential use cases for such collaboration, and how necessary features and capabilities can be developed to facilitate in-compliance RAG networks.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a8628c5-14f9-493f-83fc-5ec0da7827f1": {"__data__": {"id_": "3a8628c5-14f9-493f-83fc-5ec0da7827f1", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.html", "file_name": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.html", "file_type": "text/html", "file_size": 33310, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text describes various tools and applications related to Retrieval-Augmented Generation (RAG), a technique in natural language processing that combines retrieval and generation to answer complex questions. These tools include RAGArch, a no-code tool for creating custom RAG pipelines, and a web application for configuring and testing RAG pipelines using a range of deep learning models, embedding models, node parsers, response synthesis methods, and vector stores. The text also mentions Llamaindex, a tool for generating responses to questions using embedding models, node parsers, response synthesis methods, and vector stores. The provided text can answer questions about how to select and test different components of RAG pipelines, how to configure language models, embedding models, node parsers, response synthesis methods, and vector stores, and how to generate custom RAG pipelines using Python code. It can also provide information about the features and functionalities of these tools, as well as their impact on the accuracy and relevance of generated responses.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e63a095-cf8f-4bfc-922d-5ae7cee288a2": {"__data__": {"id_": "4e63a095-cf8f-4bfc-922d-5ae7cee288a2", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/retrieving-privacy-safe-documents-over-a-network.html", "file_name": "retrieving-privacy-safe-documents-over-a-network.html", "file_type": "text/html", "file_size": 28688, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/retrieving-privacy-safe-documents-over-a-network.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses the use of differential privacy to create synthetic observations from real data, with the aim of enabling data collaboration while preserving privacy. The text explains that multiple contributors have their own sets of Symptom2Disease datasets, which are disjoint. Synthetic observations are created using a DiffPrivateSimpleDatasetPack, and a network is built using a VectorStoreIndex. The NetworkRetriever, which connects to the ContributorRetrieverServices, can be used to retrieve synthetic observations from the two contributors' data against a query. The text evaluates the NetworkRetriever using hit rate and mean reciprocal rank metrics and demonstrates that synthetic observations still match well with the test set, which are indeed real observations. The text also touches upon controlling the level of privacy via the noise parameter sigma and notes that too much noise may render the data useless in downstream tasks. The text concludes by stating that the level of privacy can be controlled via the noise parameter sigma, and higher values of sigma correspond to higher values of privacy loss. Some questions that this text can answer include how differential privacy works to create privacy-safe synthetic observations, how the network retriever with access to more data benefits data collaboration, and how varying levels of privacy affect the synthetic observations and network retriever. The text also provides links to source code and demo notebooks for further exploration of these concepts.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8cee370-e834-4710-af6e-1c4d640d1d22": {"__data__": {"id_": "c8cee370-e834-4710-af6e-1c4d640d1d22", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.html", "file_name": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.html", "file_type": "text/html", "file_size": 15104, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses a technology product called Mixtral 8x7b, which is a combination of eight trained language models with a total parameter size of 56 billion. Mixtral 8x7b has demonstrated comparable or better performance than GPT-3.5 and Llama2 on certain benchmarks. The author provides instructions for setting up a local instance of Mixtral using Ollama and integrating it with LlamaIndex, an open-source library for working with LLMs. This setup allows for querying the text index built using LlamaIndex and Mixtral using natural language questions, potentially answering queries such as \"What does the author think about Star Trek?\" or \"Can you recommend a specific book based on my reading history?\" The author also mentions building a simple web service using Flask and Flask-CORS to expose the LlamaIndex and Mixtral capabilities to external clients. Overall, the text provides instructions for localizing LLMs with LlamaIndex and building a searchable index using Qdrant vector store.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28df59df-30a7-4aee-90b7-cb990c758c92": {"__data__": {"id_": "28df59df-30a7-4aee-90b7-cb990c758c92", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.html", "file_name": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.html", "file_type": "text/html", "file_size": 21244, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text outlines a process for deploying a scalable ETL pipeline for indexing and embedding data using AWS. This involves creating a cluster, deploying Text Embeddings Interface (TEI) and RabbitMQ, and creating ingestion pipeline workers. TEI is used to run embeddings fast, while RabbitMQ is used to queue documents for processing. The workers consume from the RabbitMQ queue, ingest data with the help of TEI, and put that data into a vector db. Some questions that this text can help answer include:\n\n1. How to create a scalable ETL pipeline for indexing and embedding data using AWS?\n2. What is Text Embeddings Interface (TEI) and how is it used to run embeddings fast?\n3. What is RabbitMQ and how is it used to queue documents for processing?\n4. How to deploy and configure a RabbitMQ cluster using AWS and Docker?\n5. How to create a Docker image for LLM using Docker and Python?\n6. How to create a FastAPI server using FastAPI and Docker?\n7. How to deploy a Lambda function using AWS and Python?\n8. How to scale the serverless search engine using AWS and Kubernetes?\n9. How to clean up the resources after use to avoid unnecessary costs?\n\nThe text also touches on optimizing the performance of the RabbitMQ deployment on AWS and provides code snippets for creating the required resources and deploying the applications. Overall, this text provides a detailed and practical guide on how to create a serverless search engine using LLM, FastAPI, and AWS, covering various stages of the process, from data preparation to deployment, and providing insights into scaling and cleanup.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7e6bcfe0-e8f2-47bc-975f-c4e0a1ec6170": {"__data__": {"id_": "7e6bcfe0-e8f2-47bc-975f-c4e0a1ec6170", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.html", "file_name": "secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.html", "file_type": "text/html", "file_size": 13622, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses a new feature that allows language learning models (LLMs) to safely execute sandboxed code using dynamic sessions in Azure Container Apps. This capability is made possible by the Azure Code Interpreter tool, which enables tasks like inspecting and manipulating data using LLMs. Some potential use cases of this feature include calculating current times in different locations and sorting temperature data. The text explains how to set up this functionality, including installing necessary packages and creating an agent using the ReActAgent class. The text also highlights the benefits of this feature, such as secure execution and reduced hesitation when delegating tasks to the agent. Some questions that the text can answer include how to sort and save data from a CSV file using Code Interpreter, how to retrieve the modified file, and what other tasks can be achieved using sandboxed code execution within an LLM agent. The text describes this feature as an \"amazing addition\" to the agent's capabilities because it provides confidence and safety in executing tasks that involve modifying and manipulating files.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7312d12-c832-4281-a316-83f147e4a5b0": {"__data__": {"id_": "c7312d12-c832-4281-a316-83f147e4a5b0", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.html", "file_name": "secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.html", "file_type": "text/html", "file_size": 7372, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a guest post by Protect AI on the topic of securing LLM applications using their open source solution, LLM Guard. It explains the risks associated with LLMs and highlights the hesitance of corporate adoption due to security concerns and lack of control and observability. The post then demonstrates how LLM Guard can be used to improve the security of RAG applications, specifically in the context of HR screening, by implementing security scanners for input and output scanning during both ingestion and retrieval. The text encourages readers to try out LLM Guard by visiting their library or documentation, and also suggests joining their Slack channel for further questions. The text can answer questions related to the risks of LLMs, the benefits and limitations of LLM Guard, and how to implement LLM Guard in RAG applications.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8e3da68c-661c-4d88-80dd-4a6b639985f2": {"__data__": {"id_": "8e3da68c-661c-4d88-80dd-4a6b639985f2", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d.html", "file_name": "shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d.html", "file_type": "text/html", "file_size": 5407, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a step-by-step guide on deploying a Retrieval-Augmented Generation (RAG) application generated by the command-line tool called create-llama to three different backends: Next.js serverless, Express, and Python. The text also mentions the services Vercel and Render for deployment purposes. The guide explains how to create a GitHub repository, configure the project, and deploy the generated application to production. This text can answer questions such as how to deploy a RAG application using create-llama, which services are recommended for deployment, and how to set up environment variables for OpenAI API keys. Additionally, the text provides instructions on how to deploy a static frontend and a backend to Render, as well as how to configure the project for both.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d713bde2-c216-45ca-a50c-4d82447ddcc9": {"__data__": {"id_": "d713bde2-c216-45ca-a50c-4d82447ddcc9", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/simplify-your-rag-application-architecture-with-llamaindex-postgresml.html", "file_name": "simplify-your-rag-application-architecture-with-llamaindex-postgresml.html", "file_type": "text/html", "file_size": 12017, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/simplify-your-rag-application-architecture-with-llamaindex-postgresml.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is an announcement from LlamaIndex, a text generation platform, about their recent integration with PostgresML, a machine learning platform built on PostgreSQL. This integration allows LlamaIndex users to seamlessly manage document storage, splitting, embedding, and retrieval using PostgresML as the backend. This results in faster, more reliable, and easier-to-manage Retrieval-Augmented Generation (RAG) workflows. By using PostgresML as the backend, users benefit from a streamlined and optimized process for RAG, as embedding, vector storage, and text generation are all consolidated into a single network call. This integration unifies embedding, vector search, and text generation into a single network call, resulting in faster, more reliable, and easier-to-manage RAG workflows. Some of the questions that this text can answer include how typical RAG workflows come with significant drawbacks, particularly for users, such as poor performance and increased dev time to master new technologies. Additionally, this text explains the user-centric issues, such as privacy concerns when sensitive data is sent to various LLM providers, as well as other challenges, such as higher costs required for multiple services. Overall, this text discusses how typical RAG workflows come with significant drawbacks, particularly for users, such as poor performance and increased dev time to master new technologies, as well as other challenges, such as higher costs required for multiple services. It also explains how the PostgresML Managed Index offers a comprehensive solution to the challenges of typical RAG workflows, such as improved performance and reduced latency. This results in improved performance and reduced latency for users. Some of the questions that this text can answer include how typical RAG workflows come with significant drawbacks, particularly for users, such as poor performance and increased dev time to master new technologies, as well as other challenges, such as higher costs required for multiple services. It also explains how the PostgresML Managed Index offers a comprehensive solution to the challenges of typical RAG workflows, such as improved performance and reduced latency. This results in improved performance and reduced latency for users. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed5e1673-d674-4279-8727-32d03c128bf7": {"__data__": {"id_": "ed5e1673-d674-4279-8727-32d03c128bf7", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.html", "file_name": "special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.html", "file_type": "text/html", "file_size": 18153, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses three prize winners of a \"Best Knowledge-Intensive LLM App\" prize series at the Berkeley AI Hackathon. Each project is briefly outlined, including their mission, implementation, challenges, and future prospects. The three projects are Helmet AI, Split, and Prosper AI. \n\nHelmet AI is a market intelligence tool that provides real-time insights and a competitive edge to leadership teams. Split is a workflow that uses AI to help users generate personalized emails while maintaining their unique writing style and emotion inflections. Prosper AI is a wealth management platform that utilizes AI to offer personalized financial plans tailored to each user's specific goals and needs. It also helps users minimize tax liabilities through intelligent financial suggestions. The text provides insights into the challenges faced by the teams during development, as well as future plans and innovations. \n\nThis text can answer questions about the three prize winners, such as what they are, what they do, and what their future prospects are. It can also provide insights into the technologies and tools used in each project, as well as the challenges faced during development. For Prosper AI, it can answer questions about the challenges of processing and structuring large amounts of data, minimizing tax liabilities through intelligent financial suggestions, and future developments and innovations. Additionally, the text provides links to learn more about Prosper AI and join a waitlist for a chance to win a $50 Amazon voucher.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d49bc2cf-8eb4-4a55-8753-f055713a20c0": {"__data__": {"id_": "d49bc2cf-8eb4-4a55-8753-f055713a20c0", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.html", "file_name": "streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.html", "file_type": "text/html", "file_size": 17345, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about a project called LlamaWorksDB, created by Team CLAB during a hackathon with MongoDB. LlamaWorksDB utilizes LlamaIndex, a language model developed by Meta AI, to create a documentation tool that searches for information using vector search within MongoDB Atlas. The text explains the setup process, including using LlamaIndex's pipeline to create documents from URLs, and building a full-stack app using Create-Llama. Some questions that this text can answer include how to use LlamaIndex's pipeline, how to set up vector search within MongoDB Atlas, and how to build a full-stack app using Create-Llama. Additionally, the text touches on deployment using Render and Vercel, and invites collaboration and feedback on the project.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d3bea69b-d7cf-4192-b5f1-bda08ed5f075": {"__data__": {"id_": "d3bea69b-d7cf-4192-b5f1-bda08ed5f075", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.html", "file_name": "supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.html", "file_type": "text/html", "file_size": 16870, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses integrating LlamaIndex with UpTrain's evaluation framework to improve the performance of RAG (Retrieval-Augmented Generation) pipelines. RAG pipelines involve retrieving relevant context from a large corpus, augmenting it with a language model (LLM), and generating a response to a given query. The text explains the limitations of the Vanilla RAG pipeline and how to use advanced techniques like the SubQuery and Reranking techniques to improve the pipeline's performance. It also explains how to import best practices from UpTrain to build RAG pipelines and provides references for further reading on the topic. Some of the questions that this text can answer include how to use the SubQuery technique to combat cases with low Response Completeness scores, how to use the Reranking technique to improve the quality of retrieved context, and how to evaluate the performance of different modules in a RAG pipeline using advanced evaluations. The text also provides resources to learn more about advanced RAG concepts and techniques. Overall, the text aims to help users quickly find what's affecting the quality of responses and take appropriate corrective actions.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dca7137f-1b4b-49cf-b8e1-365b6ee4f349": {"__data__": {"id_": "dca7137f-1b4b-49cf-b8e1-365b6ee4f349", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.html", "file_name": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.html", "file_type": "text/html", "file_size": 30335, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a tutorial on how to use LlamaIndex and Unstructured to analyze UBER SEC 10-K filings using Anthropic's 100k context window. It discusses two approaches for response synthesis across multiple nodes when the total context exceeds the context window and compares the performance of Anthropic's 100k model over UBER SEC 10-K filings using both approaches. The text can answer questions related to risk factors, regulatory challenges, competition, safety and security, financial performance, and profitability of UBER in 2019. By analyzing the risk factors disclosed in Uber's SEC filings over several years, the text highlights the evolution of these risk factors based on changes in Uber's business and industry. Some of the questions that this text can answer include: what are the key differences in risk factors disclosed in Uber's SEC filings over several years, how have Uber's risk factors evolved based on changes in the company's business and industry, what external and internal factors have influenced Uber's risk factors over time, and how do Uber's risk factors reflect both external factors like regulations and internal factors related to the company's operations?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8bf758c5-7993-4480-86e4-31efc1b5b2e1": {"__data__": {"id_": "8bf758c5-7993-4480-86e4-31efc1b5b2e1", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/the-latest-updates-to-llamacloud.html", "file_name": "the-latest-updates-to-llamacloud.html", "file_type": "text/html", "file_size": 5697, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/the-latest-updates-to-llamacloud.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about the launch of LlamaCloud, a data processing and management layer for building AI knowledge assistants. It discusses recent updates to LlamaCloud, including the release of LlamaCloud Chat, which provides a conversational RAG pipeline over data, and organizational features that enable developer collaboration. The text also mentions improvements to data and metadata access, such as the addition of new data connectors and the ability to attach metadata to uploaded files. The text invites readers to sign up for access to LlamaCloud and provides resources for learning how to build LLM application use cases. Some questions that this text can answer include:\n\n- What is LlamaCloud and what is it used for?\n- What recent updates have been made to LlamaCloud, and what benefits do they provide?\n- How can I sign up for access to LlamaCloud and learn how to use it for building LLM application use cases?\n- What resources are available for learning how to use LlamaCloud for different LLM application use cases?\n- How does LlamaCloud enable developer collaboration and rapid development velocity?\n- What data connectors have been added to LlamaCloud, and how can they be used to access and customize metadata?\n- How does the Sharepoint connector integrate with user IDs in LlamaCloud?\n- How can I attach metadata to uploaded files using LlamaCloud?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7aeff5aa-425a-4a5d-b684-c669ed5b53ad": {"__data__": {"id_": "7aeff5aa-425a-4a5d-b684-c669ed5b53ad", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.html", "file_name": "timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.html", "file_type": "text/html", "file_size": 34118, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is related to using Timescale Vector, a vector database for AI applications powered by PostgreSQL, to improve the efficiency and relevance of vector search and similarity calculations. It explains how Timescale Vector's time-based partitioning and similarity search capabilities enable faster and more accurate vector search queries with time-based filters. This text can answer questions related to optimizing vector search queries for time-based data, using LLamaIndex with Timescale Vector, and improving the efficiency and relevance of vector search and similarity calculations. Some other questions that this text can help answer include how to use Timescale Vector with real-world datasets and integrations with popular vector search libraries like FAISS and GPT-Index, as well as how Timescale Vector compares to other vector databases in terms of scalability, accuracy, and cost. Overall, this text provides insights into how to leverage Timescale Vector for AI applications that require vector search and similarity calculations, with a focus on time-based data.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68cacd2a-53b3-4748-87b4-3daca98a318b": {"__data__": {"id_": "68cacd2a-53b3-4748-87b4-3daca98a318b", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.html", "file_name": "tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.html", "file_type": "text/html", "file_size": 22826, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text explains how to use Tonic Validate, a benchmarking and evaluation platform, to create integration tests for LlamaIndex, an open-source library for building real-time assistants (RAG) using large language models. It provides a technical walkthrough on how to set up LlamaIndex and Tonic Validate, replace example datasets, and use them to test RAG system performance. Tonic Validate can also monitor RAG system performance in production and provide comprehensive metrics for measuring the performance of each component in the system. Some questions that this text can answer include how to load question-answer pairs, how to generate responses using LlamaIndex, how to score responses using Tonic Validate, and how to set up GitHub Actions for integration tests. Overall, the text describes how to create RAG systems using LlamaIndex and Tonic Validate for measuring and monitoring LLM response accuracy.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "907c913e-6863-4e67-ba21-99654c410ecd": {"__data__": {"id_": "907c913e-6863-4e67-ba21-99654c410ecd", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/towards-long-context-rag.html", "file_name": "towards-long-context-rag.html", "file_type": "text/html", "file_size": 21294, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/towards-long-context-rag.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses the release of Google's long-context LLM, Gemini 1.5 Pro, and its impact on the RAG (Retrieval as a Service) paradigm. It explores the limitations and opportunities presented by long-context language models (LLMs) in natural language processing applications. The text highlights the challenges of working with LLMs, such as the cost and latency issues associated with generating long contexts, and proposes solutions for addressing these challenges, such as small-to-big retrieval, intelligent routing, and retrieval augmented KV caching. The text invites developers and researchers to explore the possibilities of LLMs and build intelligent applications in this rapidly evolving field. \n\nThis text can answer questions related to the limitations and opportunities of LLMs, such as the cost and latency issues associated with generating long contexts, as well as proposed solutions for addressing these challenges. It can also provide insights into the future of LLM applications and the potential impact of LLMs on intelligent applications. Specifically, it can answer questions related to the impact of LLMs on the RAG paradigm, the future of RAG architectures, and the potential benefits and drawbacks of using LLMs for different types of questions.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e713d08-09cb-465a-84ad-f47c210ab63d": {"__data__": {"id_": "2e713d08-09cb-465a-84ad-f47c210ab63d", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.html", "file_name": "transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.html", "file_type": "text/html", "file_size": 14000, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about two projects that utilize AI technologies to simplify complex data queries for non-technical users. The first project, Na2SQL, uses OpenAI's GPT-3.5 and LlamaIndex to allow users with no prior SQL experience to derive valuable insights from a database using simple natural language queries. The second project is an AI prototype that uses LlamaIndex and OpenAI GPT3.5, along with Streamlit, to transform natural language into SQL queries and provide insights for e-commerce. Both projects aim to simplify data interactions for users who do not have SQL knowledge. Questions that the texts can answer include how these projects work, what technologies they use, what features they have, and what benefits they provide for users who do not have SQL knowledge. The texts also provide links to the GitHub repositories and invitations to connect with the authors on LinkedIn. Some additional questions that the texts can answer include what kind of insights these projects provide for e-commerce and what technologies are used to transform natural language into SQL queries.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a277129-75eb-4cf1-8762-14f409a7daba": {"__data__": {"id_": "9a277129-75eb-4cf1-8762-14f409a7daba", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.html", "file_name": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.html", "file_type": "text/html", "file_size": 14281, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about introducing new llama-dataset types and packs for benchmarking LLM evaluators, specifically LabelledEvaluatorDataset and LabelledPairwiseEvaluatorDataset. These datasets are meant for evaluating LLM evaluators instead of LLMs themselves. The text also introduces two new llama-packs, EvaluatorBenchmarkerPack and PairwiseComparisonEvaluatorPack, which make benchmarking these new datasets easier. The text provides benchmark results for Google's Gemini and OpenAI's GPT models as LLM evaluators using slightly adapted versions of the MT-Bench dataset. It encourages the reader to evaluate their own LLM evaluators using the provided datasets and packs. Some questions that this text can answer include which LLM evaluator performs best, how accurate are the LLM evaluators, and how do Gemini Pro, GPT-3.5, and GPT-4 compare in terms of agreement rates and performance as LLM evaluators.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb7b2a61-3cdd-4d17-adcf-7f7dee63829f": {"__data__": {"id_": "bb7b2a61-3cdd-4d17-adcf-7f7dee63829f", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/unlocking-the-3rd-dimension-for-generative-ai-part-1.html", "file_name": "unlocking-the-3rd-dimension-for-generative-ai-part-1.html", "file_type": "text/html", "file_size": 8072, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/unlocking-the-3rd-dimension-for-generative-ai-part-1.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about a tool called neThing.xyz, which aims to help engineers turn their ideas into physical objects using AI. The author explains that while AI has made significant strides in generating 2D and 3D digital assets, the same cannot be said for 3D models beyond the digital realm, particularly in the context of engineering and manufacturing. The author introduces the concept of \"code CAD\", which uses programming to create and manipulate 3D models, and explains how AI can be used to generate code for this purpose. The author also discusses the challenges of using AI for 3D generative modeling and how they have been addressed using a technique called Retrieval-Augmented Generation (RAG). The text provides examples of neThing.xyz in action for different types of objects, such as curves, threads, pipes, and lattices. The author also discusses the future goals of neThing.xyz, which include making the AI faster, smarter, and cheaper. Some questions that this text can answer include:\n- What is neThing.xyz and how can it help engineers turn their ideas into physical objects using AI?\n- What is code CAD and how can AI be used to generate code for this purpose?\n- What challenges have been faced in using AI for 3D generative modeling beyond the digital realm, particularly in the context of engineering and manufacturing?\n- How has Retrieval-Augmented Generation (RAG) been used to address these challenges in neThing.xyz?\n- What are some examples of objects that can be generated using neThing.xyz, such as curves, threads, pipes, and lattices?\n- What are the future goals of neThing.xyz, including making the AI faster, smarter, and cheaper?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a2f8bfb-56cc-4800-b16e-bddb54595e21": {"__data__": {"id_": "0a2f8bfb-56cc-4800-b16e-bddb54595e21", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.html", "file_name": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.html", "file_type": "text/html", "file_size": 19393, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text explains how to build a research assistant using LlamaIndex and llamafile to gather information on a specific topic, in this case, homing pigeons. It details the process of downloading and setting up an LLM running locally via llamafile, preparing data, and then querying the research assistant with specific questions. This research assistant can answer questions related to homing pigeons, such as \"What is homing behavior?\" \"Are wild homing pigeons exhibiting homing behavior?\" \"How do homing pigeons navigate to their original nest or loft?\" \"What factors influence homing behavior in homing pigeons?\" \"How do homing pigeons learn to navigate to their original nest or loft?\" and \"How long does it take for homing pigeons to return to their original nest or loft?\" The text also mentions that different topics, such as semiconductors or baking bread, can be learned using a different LLM and data.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6de8c2f-dc98-46a4-be20-8f3960df6968": {"__data__": {"id_": "b6de8c2f-dc98-46a4-be20-8f3960df6968", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/using-llms-for-retrieval-and-reranking-23cf2d3a14b6.html", "file_name": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.html", "file_type": "text/html", "file_size": 17641, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/using-llms-for-retrieval-and-reranking-23cf2d3a14b6.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text consists of two separate pieces of information. \n\nThe first piece of information is a summary of the novel \"The Great Gatsby\" by F. Scott Fitzgerald. This text provides background information on the main characters, explores themes related to wealth, class, and the American Dream during the Jazz Age in the 1920s, and highlights the significance of the green light at the end of the dock and the role of the setting of Long Island in the story. This text can help answer questions related to the main characters, the meaning of the green light, Jay Gatsby's place in the narrative, and the significance of the setting.\n\nThe second piece of information describes the implementation of LLM-augmented retrieval pipelines using the Hugging Face Transformers library and the OpenSearch Rust SDK. This text explains how LLMs can be integrated into retrieval pipelines and the benefits they offer, and provides examples using the Great Gatsby and the 2021 Lyft SEC 10-K as datasets to demonstrate their functionality. This text can help answer questions related to the car crash in \"The Great Gatsby\" and the initiatives of Lyft, both related to COVID-19 and independent of it.\n\nOverall, this text provides an overview of two distinct topics: a summary of \"The Great Gatsby\" and the implementation of LLM-augmented retrieval pipelines. It highlights the questions that can be answered using this information.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c957df3-76e6-4ecd-9cb6-7c86f1819cdc": {"__data__": {"id_": "9c957df3-76e6-4ecd-9cb6-7c86f1819cdc", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/vellum-llamaindex-integration-58b476a1e33f.html", "file_name": "vellum-llamaindex-integration-58b476a1e33f.html", "file_type": "text/html", "file_size": 9037, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/vellum-llamaindex-integration-58b476a1e33f.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is an announcement about a partnership between LlamaIndex, an open-source framework for LLM data augmentation, and Vellum, a developer platform for building LLM applications. The partnership aims to provide an interface between LLM models and private, external data, as well as tools for prompt engineering, unit testing, regression testing, monitoring, and model fine-tuning. The text explains the benefits and use cases of the integration, including unit and regression testing, prompt engineering tips, and measuring prompt quality in both development and production environments. The text also provides instructions on how to access and use the integration, as well as some best practices for leveraging it. The text can answer questions such as:\n\n- What is the purpose of the integration between LlamaIndex and Vellum?\n- What tools and features are provided by the integration for prompt engineering, unit testing, regression testing, monitoring, and model fine-tuning?\n- How can I access and use the integration?\n- What best practices should I follow to leverage the integration effectively?\n- What are some common reasons why evaluating LLM model quality is hard, and how can I measure prompt quality in development and production environments?\n- How can I collect user feedback to improve LLM output quality?\n- How can I track the quality of each completion using Vellum's Actuals endpoint?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d4d5ede-c2ae-41d9-b9a9-8b0394f363af": {"__data__": {"id_": "5d4d5ede-c2ae-41d9-b9a9-8b0394f363af", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc.html", "file_name": "zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc.html", "file_type": "text/html", "file_size": 8815, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "/workspace/projects/LlamindexHelper/data/zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc.html", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is a demonstration of how to use the new Document Vector Store with Zep's new VectorStore for LlamaIndex. It explains how to install Zep, create a VectorStore and Document Collection, create and populate an Index, and perform hybrid search with metadata filters using LlamaIndex. This text can answer questions related to using Zep and LlamaIndex for long-term memory storage, document and chat history memory, document embedding, enrichment, and filtering using metadata. It also provides examples of using LlamaIndex's MetadataFilters for filtering search results based on metadata. Some potential questions that this text can answer include: how to add relevant documents and chat history memory to LLM app prompts using Zep, how to store documents in Collections with document text, embeddings, and metadata, how to perform hybrid semantic search over a collection with results filtered by metadata filters, and how to use LlamaIndex's MetadataFilters for filtering search results based on metadata.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"/workspace/projects/LlamindexHelper/data/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.html": {"node_ids": ["d7be752a-0e06-424d-8aee-3a98e5e85a5c", "83001b14-fb2d-400e-b31b-3a48501408ce"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.html", "file_name": "a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.html", "file_type": "text/html", "file_size": 24708, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.html": {"node_ids": ["fd4001fc-71fa-4139-a074-30a1e7623240", "90d0dad3-83e4-48ff-969a-41a450639f14"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.html", "file_name": "a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.html", "file_type": "text/html", "file_size": 11715, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/agentic-rag-with-llamaindex-2721b8a49ff6.html": {"node_ids": ["c8449782-e685-40ae-b9e3-7444396543c4", "8459bd37-a59b-4eb1-b346-49c451ffd64b"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/agentic-rag-with-llamaindex-2721b8a49ff6.html", "file_name": "agentic-rag-with-llamaindex-2721b8a49ff6.html", "file_type": "text/html", "file_size": 7713, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.html": {"node_ids": ["e0af1e9c-54d5-4fc6-b29c-200c841ef247", "78e2927e-d1d5-46d0-9f7d-a83732e7369b"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.html", "file_name": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.html", "file_type": "text/html", "file_size": 18536, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/announcing-llamaindex-0-9-719f03282945.html": {"node_ids": ["2d74700e-cd3e-4388-a062-02ab00aef449", "77ba3a80-549e-47e9-bbf3-f70fbaa39686"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/announcing-llamaindex-0-9-719f03282945.html", "file_name": "announcing-llamaindex-0-9-719f03282945.html", "file_type": "text/html", "file_size": 18985, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications.html": {"node_ids": ["b21d0b46-68a2-4661-a1d0-00ed2b425b11", "c07eb866-c51d-4e7a-b9af-1df89655b2d6"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications.html", "file_name": "arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications.html", "file_type": "text/html", "file_size": 3887, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/automate-online-tasks-with-multion-and-llamaindex.html": {"node_ids": ["7dafcb22-17ae-4dd3-8487-0b01dcb3f669", "6c94c9c1-8eaa-45d4-befa-54e84f2ecebe"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/automate-online-tasks-with-multion-and-llamaindex.html", "file_name": "automate-online-tasks-with-multion-and-llamaindex.html", "file_type": "text/html", "file_size": 10641, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/batch-inference-with-mymagic-ai-and-llamaindex.html": {"node_ids": ["542e4fa9-6577-4c98-a8c7-72ca69e69993", "f32cd777-600a-4a8e-91f1-c5340cfd85ea"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/batch-inference-with-mymagic-ai-and-llamaindex.html", "file_name": "batch-inference-with-mymagic-ai-and-llamaindex.html", "file_type": "text/html", "file_size": 15092, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/becoming-proficient-in-document-extraction-32aa13046ed5.html": {"node_ids": ["4f6ca503-a707-49a5-8bc7-a5e0d6a923be", "51cf26f6-53c6-47e1-8ff7-3b845c7744cc"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/becoming-proficient-in-document-extraction-32aa13046ed5.html", "file_name": "becoming-proficient-in-document-extraction-32aa13046ed5.html", "file_type": "text/html", "file_size": 14667, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.html": {"node_ids": ["721d9c47-002c-446a-b063-8586ab40dfe4", "2e1d8446-49c6-4183-ad1a-6f5dfc6bc6da"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.html", "file_name": "boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.html", "file_type": "text/html", "file_size": 21486, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.html": {"node_ids": ["ab682f7b-a39a-4347-a397-ea949f7645c2", "71360fb6-350f-4905-aef8-19f79ee01ff5"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.html", "file_name": "bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.html", "file_type": "text/html", "file_size": 11333, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.html": {"node_ids": ["677eaae0-49f8-42b6-aeef-77f6c9a29fe7", "d1003737-974b-4885-96e1-a46fd31a3e22"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.html", "file_name": "bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.html", "file_type": "text/html", "file_size": 8278, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.html": {"node_ids": ["91bbd0bf-7804-48e4-b8f1-c45ebe769478", "4d6dc69f-9dae-4907-a8c0-14331d223732"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.html", "file_name": "build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.html", "file_type": "text/html", "file_size": 13116, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.html": {"node_ids": ["eb53e9aa-77c0-4337-b93c-8d845533424c", "e5fb4319-54c6-4233-91cc-2367005a7202"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.html", "file_name": "build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.html", "file_type": "text/html", "file_size": 8801, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.html": {"node_ids": ["aa501d43-7ee2-4310-a6bb-bac4f0169314", "cf82d9b0-25ef-4a65-959c-bc0f5589ced8"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.html", "file_name": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.html", "file_type": "text/html", "file_size": 27200, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.html": {"node_ids": ["bcdb8818-7166-47e9-adb9-9b7fcacdd629", "288844f4-181a-4fb5-813b-81b6f7fe1b2f"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.html", "file_name": "building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.html", "file_type": "text/html", "file_size": 8818, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/building-a-multi-agent-concierge-system.html": {"node_ids": ["9e41746c-026f-4163-9c90-e76d8317896c", "7c22a9a7-21f8-4e51-9b36-542b2a213711"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-a-multi-agent-concierge-system.html", "file_name": "building-a-multi-agent-concierge-system.html", "file_type": "text/html", "file_size": 22858, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.html": {"node_ids": ["1c6e6f5a-fd22-479c-aa6f-883a608eff98", "650a5a60-f82f-432e-9069-94ced09b1840"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.html", "file_name": "building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.html", "file_type": "text/html", "file_size": 30765, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.html": {"node_ids": ["53114f4b-875f-4c80-a26b-dade1c3fb97b", "5761f8d9-2484-4561-80f6-fb3eb3b80d7c"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.html", "file_name": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.html", "file_type": "text/html", "file_size": 14878, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.html": {"node_ids": ["cb3fad68-987a-4b78-8d97-36528341f73f", "1f12b488-aa4a-44fe-8351-c959e234c53e"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.html", "file_name": "building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.html", "file_type": "text/html", "file_size": 10568, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/building-better-tools-for-llm-agents-f8c5a6714f11.html": {"node_ids": ["72caf566-b9af-486b-aa5a-5519e5509e53", "7f9f3383-657e-4718-868e-2beea6e45e25"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-better-tools-for-llm-agents-f8c5a6714f11.html", "file_name": "building-better-tools-for-llm-agents-f8c5a6714f11.html", "file_type": "text/html", "file_size": 26798, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.html": {"node_ids": ["a75e40b3-ef51-4482-a252-f4cb730e78c6", "46aef392-a42f-4159-846f-713d933c31d9"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.html", "file_name": "building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.html", "file_type": "text/html", "file_size": 9815, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.html": {"node_ids": ["3c2a141f-c9f0-4abd-b500-8cb1106bc427", "efdb4bc2-a61c-4fa3-9b1f-3ee7a9fd6f02"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.html", "file_name": "building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.html", "file_type": "text/html", "file_size": 12899, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf.html": {"node_ids": ["4bf5fad7-81d8-42a0-8759-88e33e229195", "29dde6e0-b00a-4ec8-8fc9-91c1a11cced8"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf.html", "file_name": "building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf.html", "file_type": "text/html", "file_size": 8785, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/building-the-data-framework-for-llms-bca068e89e0e.html": {"node_ids": ["7603167c-80bd-4ae6-ba83-be688bcc6ff2", "fc5bc645-280c-44b1-8f55-a923ad5a53b4"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/building-the-data-framework-for-llms-bca068e89e0e.html", "file_name": "building-the-data-framework-for-llms-bca068e89e0e.html", "file_type": "text/html", "file_size": 13028, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud.html": {"node_ids": ["b27e9bc5-89de-4fee-aaaf-8bd4a02091b6", "bc255221-4c45-42cd-9448-37ce4bf85975"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud.html", "file_name": "case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud.html", "file_type": "text/html", "file_size": 6199, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex.html": {"node_ids": ["ef7ab3d9-ad12-44f8-81e5-a733fe37ed70", "ec2945bc-f663-4011-96cb-9f322642710a"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex.html", "file_name": "case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex.html", "file_type": "text/html", "file_size": 6724, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c.html": {"node_ids": ["eccad25c-8678-4c75-be21-1b172414e304", "ad5e5614-fd2d-40b8-b5fd-77e2351f5c1c"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c.html", "file_name": "chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c.html", "file_type": "text/html", "file_size": 4649, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.html": {"node_ids": ["43c11e43-a9e1-4e20-a578-35a234992fc0", "13840a23-c7d3-466a-a5ef-3ca13ad3c289"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.html", "file_name": "combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.html", "file_type": "text/html", "file_size": 21543, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191.html": {"node_ids": ["093dd541-ece9-485b-b41d-4ae80b733e86", "4ac9b548-a6ce-44f0-a298-8b1bc6870b58"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191.html", "file_name": "create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191.html", "file_type": "text/html", "file_size": 4343, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/customizing-property-graph-index-in-llamaindex.html": {"node_ids": ["1ebeacf6-ead5-42b9-8080-9a1316efd4d6", "6f9d733b-3654-42e7-8055-4a5b1cc7929b"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/customizing-property-graph-index-in-llamaindex.html", "file_name": "customizing-property-graph-index-in-llamaindex.html", "file_type": "text/html", "file_size": 30133, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/data-agents-eed797d7972f.html": {"node_ids": ["a56f2b2d-eb47-487d-b108-690cae3ec9f6", "c0c3b479-4e27-488f-96bb-2f7958531f88"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/data-agents-eed797d7972f.html", "file_name": "data-agents-eed797d7972f.html", "file_type": "text/html", "file_size": 32843, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/data-agents-zapier-nla-67146395ce1.html": {"node_ids": ["45d275d9-debd-445a-aae0-6fc18f97f963", "b89fc93f-8fe1-455b-b2e2-3f039fc4c821"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/data-agents-zapier-nla-67146395ce1.html", "file_name": "data-agents-zapier-nla-67146395ce1.html", "file_type": "text/html", "file_size": 1249, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.html": {"node_ids": ["1298488c-751b-4e4b-8609-0fc822bbeda4", "dc3aae11-61f2-45bc-97a1-6485c18dcfbe"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.html", "file_name": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.html", "file_type": "text/html", "file_size": 38052, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.html": {"node_ids": ["58797dbc-24cc-4e0c-911a-c75a82476b66", "8d53a5a5-a61b-48d1-9726-c37659c594b0"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.html", "file_name": "easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.html", "file_type": "text/html", "file_size": 20138, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.html": {"node_ids": ["10abc301-d5d8-4f47-98f0-448a326fae47", "3a4b10b9-3792-4e0b-9b47-a409b6735b7f"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.html", "file_name": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.html", "file_type": "text/html", "file_size": 34831, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.html": {"node_ids": ["cf9e1e55-3c01-4ad7-9212-ab5716ef2216", "c239bd6c-3955-4c0a-a528-3ec9f4e865bf"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.html", "file_name": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.html", "file_type": "text/html", "file_size": 10471, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.html": {"node_ids": ["2ab33830-349c-4cfd-a9df-27023e133c9c", "5c6a1117-be96-4783-aa30-8d8d0f1f0fc7"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.html", "file_name": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.html", "file_type": "text/html", "file_size": 17554, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.html": {"node_ids": ["1fb1c0dd-614c-4cce-a613-fa39c6634069", "492d2827-5f2e-4694-b90e-02beba3aab66"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.html", "file_name": "fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.html", "file_type": "text/html", "file_size": 13624, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.html": {"node_ids": ["e5133220-b12b-46a4-bbea-dcdc289b1280", "5c818674-12f6-4454-98fd-f5ca4e575a4a"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.html", "file_name": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.html", "file_type": "text/html", "file_size": 15144, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.html": {"node_ids": ["cb09bc93-1d03-429b-bf6b-157bf6d0e135", "e24625a9-19a9-4658-aead-0413b1fa8d86"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.html", "file_name": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.html", "file_type": "text/html", "file_size": 27896, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.html": {"node_ids": ["fcd4f869-7e6b-4140-8092-7f0fded29c81", "b12f7641-553c-46e2-b38a-96edbfdf383e"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.html", "file_name": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.html", "file_type": "text/html", "file_size": 19580, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa.html": {"node_ids": ["7e016859-6da8-439e-859d-93470687a784", "4d400bfc-be64-4be7-91c4-cee2a8590dfc"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa.html", "file_name": "how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa.html", "file_type": "text/html", "file_size": 16248, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.html": {"node_ids": ["015332b0-8eea-4d0a-8af8-d3ec7fdc0849", "a0e32d09-7d5e-4924-a122-f574b006ce43"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.html", "file_name": "how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.html", "file_type": "text/html", "file_size": 8975, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.html": {"node_ids": ["19be6fb5-5454-421a-a25c-65cd25babeb4", "3f3c8fa1-e2ab-4f59-84db-9579abf12516"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.html", "file_name": "improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.html", "file_type": "text/html", "file_size": 8911, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.html": {"node_ids": ["b0411a47-97c5-43ae-98c4-efb40e22c9bf", "076ff5b2-eeac-4b00-a2d3-a69736af563f"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.html", "file_name": "improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.html", "file_type": "text/html", "file_size": 20086, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/improving-vector-search-reranking-with-postgresml-and-llamaindex.html": {"node_ids": ["d4a76c0d-8011-4cfa-ba89-4bd18f779538", "7182dfc4-59d1-40bb-a441-0eaae03678b7"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/improving-vector-search-reranking-with-postgresml-and-llamaindex.html", "file_name": "improving-vector-search-reranking-with-postgresml-and-llamaindex.html", "file_type": "text/html", "file_size": 16145, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/introducing-airbyte-sources-within-llamaindex-42209071722f.html": {"node_ids": ["ad700478-309e-467d-bfcc-15ed542fa03e", "e6c2f380-dae5-46d1-b859-51ecf810ab63"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-airbyte-sources-within-llamaindex-42209071722f.html", "file_name": "introducing-airbyte-sources-within-llamaindex-42209071722f.html", "file_type": "text/html", "file_size": 12780, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html": {"node_ids": ["c18d9bd8-763e-4a8c-b32f-2e8c87b5c3be", "60ff6c04-bd0a-4e31-926c-93f341377cab"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html", "file_name": "introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html", "file_type": "text/html", "file_size": 18790, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/introducing-llama-datasets-aadb9994ad9e.html": {"node_ids": ["0f337657-1fcf-4602-80ec-e2765f1ca312", "abfec326-12d7-4050-8535-fb19b5e8d1ec"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-llama-datasets-aadb9994ad9e.html", "file_name": "introducing-llama-datasets-aadb9994ad9e.html", "file_type": "text/html", "file_size": 16032, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/introducing-llama-packs-e14f453b913a.html": {"node_ids": ["0d551e45-4afb-475f-84cf-6e916d1bb625", "123f9c77-f764-495f-b50a-463c23d9ef6d"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-llama-packs-e14f453b913a.html", "file_name": "introducing-llama-packs-e14f453b913a.html", "file_type": "text/html", "file_size": 12815, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/introducing-llamacloud-and-llamaparse-af8cedf9006b.html": {"node_ids": ["75b64a9d-5435-4697-a5a1-e6f68fd72ee7", "1d700344-f592-4c7e-ad65-58c9a2b23dae"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-llamacloud-and-llamaparse-af8cedf9006b.html", "file_name": "introducing-llamacloud-and-llamaparse-af8cedf9006b.html", "file_type": "text/html", "file_size": 17273, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/introducing-llamaindex-ts-89f41a1f24ab.html": {"node_ids": ["67a39a17-1d98-4e44-b55e-0c34fbe4931a", "93df4a84-3e21-488e-be5c-574732ab057f"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-llamaindex-ts-89f41a1f24ab.html", "file_name": "introducing-llamaindex-ts-89f41a1f24ab.html", "file_type": "text/html", "file_size": 3898, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/introducing-query-pipelines-025dc2bb0537.html": {"node_ids": ["29b4215a-bbc5-47b4-8d27-5747dc7f4dcd", "23c0c342-b5ff-4097-9372-42a689349c3d"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-query-pipelines-025dc2bb0537.html", "file_name": "introducing-query-pipelines-025dc2bb0537.html", "file_type": "text/html", "file_size": 17799, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.html": {"node_ids": ["0d8709fd-deca-456e-8637-22e1d560e2d5", "f6dc427b-b915-4647-a282-6d68bc6fc466"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.html", "file_name": "introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.html", "file_type": "text/html", "file_size": 7788, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41.html": {"node_ids": ["45b64eb6-f8fd-4097-9e2e-1cf2b25b01d7", "3f8bbe65-f50d-4736-91b8-f2c2d95f664c"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41.html", "file_name": "introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41.html", "file_type": "text/html", "file_size": 4155, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms.html": {"node_ids": ["5da260ff-81af-437d-bbc7-e67593307b87", "b34bddd8-397b-49b8-aee0-35fa2312c3c7"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms.html", "file_name": "introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms.html", "file_type": "text/html", "file_size": 14933, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb.html": {"node_ids": ["6f6bbd07-b237-4787-a3ce-fab279884a70", "88fb8b71-9417-463e-9594-a800a6bd7002"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb.html", "file_name": "join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb.html", "file_type": "text/html", "file_size": 6164, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/launching-the-first-genai-native-document-parsing-platform.html": {"node_ids": ["9252e368-47eb-4114-922a-b9098ed2a1e4", "edde25d6-2068-483d-9f6c-d1efc27c2289"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/launching-the-first-genai-native-document-parsing-platform.html", "file_name": "launching-the-first-genai-native-document-parsing-platform.html", "file_type": "text/html", "file_size": 9532, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llama-index-prem-ai-join-forces-51702fecedec.html": {"node_ids": ["b4d5e3eb-45fb-4b5e-8bfb-ccecefd2ab7b", "0a331bec-d6d4-44e9-b437-bc33c4cbcdf0"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llama-index-prem-ai-join-forces-51702fecedec.html", "file_name": "llama-index-prem-ai-join-forces-51702fecedec.html", "file_type": "text/html", "file_size": 9535, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamacloud-built-for-enterprise-llm-app-builders.html": {"node_ids": ["7ba8e8f5-55df-4d69-9171-68459d3b96eb", "01310167-4545-4da6-832e-2377043f67a9"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamacloud-built-for-enterprise-llm-app-builders.html", "file_name": "llamacloud-built-for-enterprise-llm-app-builders.html", "file_type": "text/html", "file_size": 10150, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.html": {"node_ids": ["eaf39a01-bafe-4c52-85af-37d412ae586f", "871bde9f-8afd-44ee-9e8a-778424484ad6"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.html", "file_name": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.html", "file_type": "text/html", "file_size": 23681, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim.html": {"node_ids": ["e84ce598-1e29-4f9b-b8b8-d1a63678ac26", "0413acd1-7b0e-4b3d-bd40-f5b40cc74b56"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim.html", "file_name": "llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim.html", "file_type": "text/html", "file_size": 3079, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-and-transformers-agents-67042ee1d8d6.html": {"node_ids": ["c71a83e3-f32a-4445-9ee3-cad21e9a0778", "1fc794d0-dd4b-4232-ac2d-bcfdd2f737c9"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-and-transformers-agents-67042ee1d8d6.html", "file_name": "llamaindex-and-transformers-agents-67042ee1d8d6.html", "file_type": "text/html", "file_size": 14762, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-and-weaviate-ba3ff1cbf5f4.html": {"node_ids": ["3e5d98e6-d512-4b30-b531-d13042572fd3", "1e80d500-411f-412e-8c7d-0ba3aabb3930"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-and-weaviate-ba3ff1cbf5f4.html", "file_name": "llamaindex-and-weaviate-ba3ff1cbf5f4.html", "file_type": "text/html", "file_size": 9644, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.html": {"node_ids": ["83e4306e-ad51-472d-8de8-5db4fe1e12e9", "90128250-545b-4fcd-92b9-504e7d59be68"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.html", "file_name": "llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.html", "file_type": "text/html", "file_size": 9573, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.html": {"node_ids": ["a368acbf-5e8a-4f67-90fb-681d10d26379", "e2a7e039-d3fb-49dc-a063-62b9bcf32e48"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.html", "file_name": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.html", "file_type": "text/html", "file_size": 28117, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-gemini-8d7c3b9ea97e.html": {"node_ids": ["d4312cad-4695-4e8f-84d4-793758a8c966", "bd573d2c-d04b-4397-9d42-579528d43f12"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-gemini-8d7c3b9ea97e.html", "file_name": "llamaindex-gemini-8d7c3b9ea97e.html", "file_type": "text/html", "file_size": 16217, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.html": {"node_ids": ["75c83f20-cfc9-4004-a2f1-3b5844923fdc", "f2a9fcbf-4e45-4a09-9280-2ab03dbd2b91"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.html", "file_name": "llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.html", "file_type": "text/html", "file_size": 17065, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff.html": {"node_ids": ["b2394993-0632-41dc-9b85-fb6398ac127d", "d0855239-4e1a-41e7-8feb-593b16c4512d"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff.html", "file_name": "llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff.html", "file_type": "text/html", "file_size": 4196, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.html": {"node_ids": ["ec34043d-40a0-4bd1-9d3f-3c1d34b74d6f", "155d642c-6e3b-4c3b-9178-080677b69062"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.html", "file_name": "llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.html", "file_type": "text/html", "file_size": 26130, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-news-special-edition-openai-developer-day-e955f16db4e2.html": {"node_ids": ["fd9bd377-d076-4e2f-ad94-4c70fb203431", "b94ada67-94c1-4eb7-a4dd-6f203bb98d8c"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-news-special-edition-openai-developer-day-e955f16db4e2.html", "file_name": "llamaindex-news-special-edition-openai-developer-day-e955f16db4e2.html", "file_type": "text/html", "file_size": 3202, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-02-13-26fa79601ba5.html": {"node_ids": ["286dc06c-44d3-4ddb-81fe-c5e32f246a8e", "49f1def6-1d5e-41b6-9a7a-8bcb23d9a250"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-02-13-26fa79601ba5.html", "file_name": "llamaindex-newsletter-2023-02-13-26fa79601ba5.html", "file_type": "text/html", "file_size": 10106, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-17-33514cbc04a2.html": {"node_ids": ["1b13ab53-9b38-4ac2-b634-9361051cd562", "db379d9e-d881-4f08-bb6a-bf7ea6c03076"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-17-33514cbc04a2.html", "file_name": "llamaindex-newsletter-2023-10-17-33514cbc04a2.html", "file_type": "text/html", "file_size": 15358, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-24-4a76204eeaa3.html": {"node_ids": ["de3e81b8-63b3-4ea1-8dbd-0e9608cd55c7", "ae3fa659-adca-476a-a932-11719039fdc4"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-24-4a76204eeaa3.html", "file_name": "llamaindex-newsletter-2023-10-24-4a76204eeaa3.html", "file_type": "text/html", "file_size": 10205, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-31-36244e2b3f0c.html": {"node_ids": ["ffec9c1b-f29d-4596-89ee-323ae052143e", "8eb79a05-ff5d-4057-86bf-9f1bbd6bc8b3"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-31-36244e2b3f0c.html", "file_name": "llamaindex-newsletter-2023-10-31-36244e2b3f0c.html", "file_type": "text/html", "file_size": 11836, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-07-cf20b9a833aa.html": {"node_ids": ["bb45192e-23b8-4f4a-adb4-ebc82f1f3440", "369ac48f-7909-45e3-b24d-7c27e92212db"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-07-cf20b9a833aa.html", "file_name": "llamaindex-newsletter-2023-11-07-cf20b9a833aa.html", "file_type": "text/html", "file_size": 11074, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-14-dad06ae4284a.html": {"node_ids": ["dae6e131-5d00-4a87-8cee-c2e54de14693", "91d0debf-63e5-4b72-b5de-658f6d9bbcdf"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-14-dad06ae4284a.html", "file_name": "llamaindex-newsletter-2023-11-14-dad06ae4284a.html", "file_type": "text/html", "file_size": 11683, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-21-aa3a71e339f8.html": {"node_ids": ["1c6b3573-c71f-4c67-a579-7be2e27c9846", "e56ce492-07a5-4612-a6fe-5677c2b2ecf0"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-21-aa3a71e339f8.html", "file_name": "llamaindex-newsletter-2023-11-21-aa3a71e339f8.html", "file_type": "text/html", "file_size": 10593, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-28-a31be430a786.html": {"node_ids": ["8b11322e-cfc5-490a-9095-8a596889d6dc", "0f199c3b-e433-46ed-b4ec-67d697c45bb8"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-28-a31be430a786.html", "file_name": "llamaindex-newsletter-2023-11-28-a31be430a786.html", "file_type": "text/html", "file_size": 9884, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-05-faf5ab930264.html": {"node_ids": ["b917ee22-46b2-4ba6-b9f4-499ee3e46976", "ace7cd41-9c8e-45ae-a906-cbd7bdea2de3"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-05-faf5ab930264.html", "file_name": "llamaindex-newsletter-2023-12-05-faf5ab930264.html", "file_type": "text/html", "file_size": 13772, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-12-4a5d542fbb1e.html": {"node_ids": ["2896da82-3211-4bde-8115-d8d376dfeeb2", "393e9985-d181-4b5c-b17a-e2641a6d865d"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-12-4a5d542fbb1e.html", "file_name": "llamaindex-newsletter-2023-12-12-4a5d542fbb1e.html", "file_type": "text/html", "file_size": 14752, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-19-2965a2d03726.html": {"node_ids": ["5fcab154-b832-4485-b490-a1fc906a44ce", "eec58ba9-7ab0-455a-9b62-b68a9faf700d"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-19-2965a2d03726.html", "file_name": "llamaindex-newsletter-2023-12-19-2965a2d03726.html", "file_type": "text/html", "file_size": 15040, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-02-f349db8c1842.html": {"node_ids": ["982f24c2-20a2-49d4-9f3a-cb216cb40c75", "f2b6eec6-c895-492e-8b70-45b0c98b4436"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-02-f349db8c1842.html", "file_name": "llamaindex-newsletter-2024-01-02-f349db8c1842.html", "file_type": "text/html", "file_size": 17293, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-09-6209000da2e6.html": {"node_ids": ["547215de-f1fe-402d-a7cb-bce82b5fb107", "ab81fa3d-da8f-42cf-8b30-fa58fab75ffb"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-09-6209000da2e6.html", "file_name": "llamaindex-newsletter-2024-01-09-6209000da2e6.html", "file_type": "text/html", "file_size": 13781, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-16-752195bed96d.html": {"node_ids": ["b293f6a2-825e-4e15-9fa8-bacd8a1a6255", "b4f4106f-d3e0-42c8-97a3-3014f377c37e"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-16-752195bed96d.html", "file_name": "llamaindex-newsletter-2024-01-16-752195bed96d.html", "file_type": "text/html", "file_size": 9794, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-23-11ee2c211bab.html": {"node_ids": ["d0215726-4154-4f22-abca-96c226bdf066", "5eabadc2-9f73-4400-81df-e9167e1c1595"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-23-11ee2c211bab.html", "file_name": "llamaindex-newsletter-2024-01-23-11ee2c211bab.html", "file_type": "text/html", "file_size": 10910, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-30-0d01eb0d8cef.html": {"node_ids": ["befbcdfe-273d-447f-a8a8-443643f2518e", "6916b918-0b45-4117-bab9-1c322353fb32"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-30-0d01eb0d8cef.html", "file_name": "llamaindex-newsletter-2024-01-30-0d01eb0d8cef.html", "file_type": "text/html", "file_size": 11180, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-06-9a303130ad9f.html": {"node_ids": ["651f6397-c19d-4a2f-9fdb-2a657699aa69", "df738da3-01fa-4c8f-accf-3057e7d28913"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-06-9a303130ad9f.html", "file_name": "llamaindex-newsletter-2024-02-06-9a303130ad9f.html", "file_type": "text/html", "file_size": 9110, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4.html": {"node_ids": ["c292cf04-4202-46ef-9615-ebd8b4f691e1", "3bdedc09-2d20-4d3c-b0e7-5048dc2d3a23"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4.html", "file_name": "llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4.html", "file_type": "text/html", "file_size": 10623, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-27-4b9102a0f824.html": {"node_ids": ["8b59e0f9-8a67-446d-a31a-47f64aabb86c", "a921b43b-bc2d-4860-9b27-e223b02236fe"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-27-4b9102a0f824.html", "file_name": "llamaindex-newsletter-2024-02-27-4b9102a0f824.html", "file_type": "text/html", "file_size": 12685, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-05.html": {"node_ids": ["309f00b5-e781-4d35-998a-88bcadea4915", "7a547101-75f8-4471-92bf-c1b454ce94fb"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-05.html", "file_name": "llamaindex-newsletter-2024-03-05.html", "file_type": "text/html", "file_size": 13864, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-12.html": {"node_ids": ["5507c3b5-7743-46ce-85e1-540285feec65", "57234cb7-d09a-4463-bbcc-fa30974cfa17"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-12.html", "file_name": "llamaindex-newsletter-2024-03-12.html", "file_type": "text/html", "file_size": 11140, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-19.html": {"node_ids": ["fae755c6-64eb-4e29-b14e-ec1f5fe151b0", "6678a088-8c15-4de7-bc3a-1644f9402804"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-19.html", "file_name": "llamaindex-newsletter-2024-03-19.html", "file_type": "text/html", "file_size": 11259, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-26.html": {"node_ids": ["ee8f4315-d714-4901-be24-598390cde4ae", "8d67b4d2-3a1f-44eb-8833-57deaecf65db"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-26.html", "file_name": "llamaindex-newsletter-2024-03-26.html", "file_type": "text/html", "file_size": 13007, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-02.html": {"node_ids": ["2645d982-aadc-4e88-8d16-a0cda3215a7c", "75952d41-8f05-41a6-9a17-8a13e46d162e"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-02.html", "file_name": "llamaindex-newsletter-2024-04-02.html", "file_type": "text/html", "file_size": 13087, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-09.html": {"node_ids": ["309648a9-822c-44ba-aa22-09311c2de138", "3d1d146f-234d-4722-9a44-7cae1d22ebeb"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-09.html", "file_name": "llamaindex-newsletter-2024-04-09.html", "file_type": "text/html", "file_size": 11147, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-16.html": {"node_ids": ["231ef033-a676-4456-9ec2-d998a21fddba", "da7a008c-194b-4127-8013-c0b28d2ac838"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-16.html", "file_name": "llamaindex-newsletter-2024-04-16.html", "file_type": "text/html", "file_size": 13388, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-23.html": {"node_ids": ["fa6018c4-71e9-4a2b-a80d-629135ec97a9", "969e0723-8b7a-400b-85b0-14e539747a3f"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-23.html", "file_name": "llamaindex-newsletter-2024-04-23.html", "file_type": "text/html", "file_size": 10277, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-30.html": {"node_ids": ["017f7ad5-1379-4285-8155-0a6ccf3c4800", "f10e29e1-4b5a-4907-b71e-704f75f98031"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-30.html", "file_name": "llamaindex-newsletter-2024-04-30.html", "file_type": "text/html", "file_size": 8240, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-07.html": {"node_ids": ["60ecb273-470b-431a-a3ec-512ae5c68015", "c0a1109d-4e46-4423-b755-80a3bb4af515"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-07.html", "file_name": "llamaindex-newsletter-2024-05-07.html", "file_type": "text/html", "file_size": 11641, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-14.html": {"node_ids": ["64c022dc-191e-4e0c-b783-6676845d219e", "3f1ff09e-313f-4b07-9807-32dc5a2b9fc2"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-14.html", "file_name": "llamaindex-newsletter-2024-05-14.html", "file_type": "text/html", "file_size": 8288, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-21.html": {"node_ids": ["e40d8b2d-e098-40e0-88d3-a53b9ae8308b", "b086fcf1-f08b-4afc-8097-9471a965c1ce"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-21.html", "file_name": "llamaindex-newsletter-2024-05-21.html", "file_type": "text/html", "file_size": 9430, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-28.html": {"node_ids": ["4d064c70-90b1-4465-97e2-2eabafc0feed", "097d486c-14e7-46cd-819f-029c8fdedf6f"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-28.html", "file_name": "llamaindex-newsletter-2024-05-28.html", "file_type": "text/html", "file_size": 9205, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-04.html": {"node_ids": ["8180bf62-60e1-450e-9589-4b0667a566bf", "124f07cf-be0c-49f5-bd03-4d3dc22ff9eb"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-04.html", "file_name": "llamaindex-newsletter-2024-06-04.html", "file_type": "text/html", "file_size": 10632, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-11.html": {"node_ids": ["ed9e395e-b9b9-4821-a4f9-06cc2b7e5329", "afd66fa0-1373-44e1-af17-88f0aeb7d1c6"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-11.html", "file_name": "llamaindex-newsletter-2024-06-11.html", "file_type": "text/html", "file_size": 11257, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-18.html": {"node_ids": ["3d42b824-bce9-4a63-9053-cb9e91c8801f", "aad60b80-c0fb-4533-a3d9-887157de0f0a"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-18.html", "file_name": "llamaindex-newsletter-2024-06-18.html", "file_type": "text/html", "file_size": 12216, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-25.html": {"node_ids": ["51420421-c965-4215-85a4-5d3e334431ab", "c70e20ef-07ef-40b0-b508-debf9e34f01c"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-25.html", "file_name": "llamaindex-newsletter-2024-06-25.html", "file_type": "text/html", "file_size": 7557, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-02.html": {"node_ids": ["9ab4246f-4d29-4686-abe8-2f0c710de25e", "07f91239-eab0-4b77-9d44-83bbfed20201"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-02.html", "file_name": "llamaindex-newsletter-2024-07-02.html", "file_type": "text/html", "file_size": 9058, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-09.html": {"node_ids": ["141f71f0-efff-4a47-8861-6b1a809a0a15", "87813008-0ec9-4f71-b345-d58f4909ecd6"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-09.html", "file_name": "llamaindex-newsletter-2024-07-09.html", "file_type": "text/html", "file_size": 12456, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-16.html": {"node_ids": ["257dc101-bd52-471d-9104-3ab57355740d", "442318f5-bd6e-4b42-8c8d-905dde904aca"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-16.html", "file_name": "llamaindex-newsletter-2024-07-16.html", "file_type": "text/html", "file_size": 12112, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.html": {"node_ids": ["94092881-2ff9-4817-a884-f349c4240baf", "54c4d011-d894-42de-8b06-3f68bfdfae0a"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.html", "file_name": "llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.html", "file_type": "text/html", "file_size": 11234, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.html": {"node_ids": ["4ddf56f3-b705-465d-984a-043fc1b59144", "e550df24-500a-47da-871e-45c9c237c6c5"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.html", "file_name": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.html", "file_type": "text/html", "file_size": 44532, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-turns-1-f69dcdd45fe3.html": {"node_ids": ["525538fd-c063-4fa1-bf29-c8f33d33b1fc", "7122d6bc-8686-4963-8965-c45b7dcc93da"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-turns-1-f69dcdd45fe3.html", "file_name": "llamaindex-turns-1-f69dcdd45fe3.html", "file_type": "text/html", "file_size": 8172, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-update-07-10-2023-4ceebdab96cb.html": {"node_ids": ["a900c947-cee8-4e8f-9915-97ae111d8b8a", "4f20eb78-358b-4525-950a-d014c9b5a4e4"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-update-07-10-2023-4ceebdab96cb.html", "file_name": "llamaindex-update-07-10-2023-4ceebdab96cb.html", "file_type": "text/html", "file_size": 14119, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-update-08-01-2023-185514d9b897.html": {"node_ids": ["c9abdf40-a41f-4099-8d3f-e24b5f91a5c3", "7a7ff547-14ef-496b-a8e6-4ed239047a07"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-update-08-01-2023-185514d9b897.html", "file_name": "llamaindex-update-08-01-2023-185514d9b897.html", "file_type": "text/html", "file_size": 18864, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-update-09-03-2023-4a7c21c0f60b.html": {"node_ids": ["1b725d51-5e7f-41e5-9688-ac6ad095544b", "5e35d539-6390-48bb-9205-e3b17e1cd9e0"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-update-09-03-2023-4a7c21c0f60b.html", "file_name": "llamaindex-update-09-03-2023-4a7c21c0f60b.html", "file_type": "text/html", "file_size": 34061, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-update-20-09-2023-86ed66f78bac.html": {"node_ids": ["3cae4701-e505-4058-9d4c-7ecef3082ea5", "3285e0fd-d0b5-48f4-b79e-57eb2c3e4b26"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-update-20-09-2023-86ed66f78bac.html", "file_name": "llamaindex-update-20-09-2023-86ed66f78bac.html", "file_type": "text/html", "file_size": 15447, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-update-2023-10-10-3718a3d19fb9.html": {"node_ids": ["b857c6d8-1c28-4c5e-ad15-aeb0017a4539", "9bb11259-1760-43ad-b6f5-00a7d237f2fb"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-update-2023-10-10-3718a3d19fb9.html", "file_name": "llamaindex-update-2023-10-10-3718a3d19fb9.html", "file_type": "text/html", "file_size": 19541, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-update-6-26-2023-ed30a9d45f84.html": {"node_ids": ["6c5f7ca6-b5b6-4fbe-8e7e-19da0ce2f118", "e10a12c9-04f9-4687-8409-d2b28ecf0a17"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-update-6-26-2023-ed30a9d45f84.html", "file_name": "llamaindex-update-6-26-2023-ed30a9d45f84.html", "file_type": "text/html", "file_size": 16810, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-v0-10-838e735948f8.html": {"node_ids": ["6fb7feeb-82a2-4dab-bf53-1e4295f8cd5e", "081baa8f-e7a1-4804-b75e-e6c7702709b3"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-v0-10-838e735948f8.html", "file_name": "llamaindex-v0-10-838e735948f8.html", "file_type": "text/html", "file_size": 26572, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-vectara-7a3889cd34cb.html": {"node_ids": ["ca5cd227-31a4-4889-a697-58fde080ff56", "a382cabf-7150-4c27-8e29-9158df4d8cd8"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-vectara-7a3889cd34cb.html", "file_name": "llamaindex-vectara-7a3889cd34cb.html", "file_type": "text/html", "file_size": 13681, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.html": {"node_ids": ["16f2c1f8-4c52-4e46-b267-afb162402d47", "dd19d281-9b4e-442f-8f4f-c9a6007f3ff6"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.html", "file_name": "llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.html", "file_type": "text/html", "file_size": 15546, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.html": {"node_ids": ["53468c8b-eb55-4386-8249-8e2dc11ff5a0", "6530c364-f6dc-4b04-9631-9c18b4241caf"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.html", "file_name": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.html", "file_type": "text/html", "file_size": 14272, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.html": {"node_ids": ["46618cc7-3c24-4ae8-a484-5105fe1c8c62", "9260d03a-a64d-44d8-837a-371c97930488"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.html", "file_name": "mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.html", "file_type": "text/html", "file_size": 9736, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/multi-modal-rag-621de7525fea.html": {"node_ids": ["2934dde5-632a-4630-bd67-e76f5036cedd", "ec91c3cb-204a-4184-b27c-60145bb5ea4c"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/multi-modal-rag-621de7525fea.html", "file_name": "multi-modal-rag-621de7525fea.html", "file_type": "text/html", "file_size": 11704, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/multimodal-rag-building-ainimal-go-fecf8404ed97.html": {"node_ids": ["5b6ae502-fa08-4814-a455-5bf4588d5787", "713febfb-2255-496c-9648-087c01d69fcf"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/multimodal-rag-building-ainimal-go-fecf8404ed97.html", "file_name": "multimodal-rag-building-ainimal-go-fecf8404ed97.html", "file_type": "text/html", "file_size": 16030, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.html": {"node_ids": ["4802a12a-b25c-4d24-8cdf-6a91a0f50268", "1b398c01-24b6-4aa4-afda-d1b0fab6dd83"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.html", "file_name": "multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.html", "file_type": "text/html", "file_size": 22025, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.html": {"node_ids": ["96a79d46-9c47-43d1-a537-51395ef4596b", "7516225c-ac9f-4846-b9df-7b9fd66a796e"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.html", "file_name": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.html", "file_type": "text/html", "file_size": 14311, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.html": {"node_ids": ["14357e15-3916-4aa0-aaa6-b3214b2a64d2", "bdb955cb-3791-4a33-bb8b-9b49ae4dc880"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.html", "file_name": "newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.html", "file_type": "text/html", "file_size": 15857, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/nvidia-research-rag-with-long-context-llms-7d94d40090c4.html": {"node_ids": ["0212bfaa-89d6-456c-a78f-aa8ce03094df", "003fa2ae-c366-4f7e-b6d1-2b03b19a8c23"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/nvidia-research-rag-with-long-context-llms-7d94d40090c4.html", "file_name": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.html", "file_type": "text/html", "file_size": 10854, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/one-click-open-source-rag-observability-with-langfuse.html": {"node_ids": ["3939f831-8ef2-4f7a-b240-ed12c62e4306", "097b6b16-d85c-42fc-863c-cf2d6a082ee2"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/one-click-open-source-rag-observability-with-langfuse.html", "file_name": "one-click-open-source-rag-observability-with-langfuse.html", "file_type": "text/html", "file_size": 9729, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/openai-cookbook-evaluating-rag-systems-fe393c61fb93.html": {"node_ids": ["80cb2875-d534-42ff-88b0-2b247c4a4365", "79033d51-6e72-4407-bcd3-8a1c1ea243f3"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/openai-cookbook-evaluating-rag-systems-fe393c61fb93.html", "file_name": "openai-cookbook-evaluating-rag-systems-fe393c61fb93.html", "file_type": "text/html", "file_size": 2220, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/pii-detector-hacking-privacy-in-rag.html": {"node_ids": ["d177694f-0d97-438b-a3ce-7696eaf89084", "4c78b9be-10aa-4203-b3a5-95c6452201a0"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/pii-detector-hacking-privacy-in-rag.html", "file_name": "pii-detector-hacking-privacy-in-rag.html", "file_type": "text/html", "file_size": 17508, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.html": {"node_ids": ["c7886207-4b88-4a1a-8e30-820391e9deaf", "c990777c-dce7-4e5b-bb02-54cdd9180314"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.html", "file_name": "pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.html", "file_type": "text/html", "file_size": 14744, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.html": {"node_ids": ["b2862724-1e15-4d32-bd5b-11992d50b4f7", "2716650b-a624-4ddc-8de6-9664b0a43286"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.html", "file_name": "querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.html", "file_type": "text/html", "file_size": 13717, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.html": {"node_ids": ["2d336599-7330-4600-95eb-3b84a5276fe7", "3a8628c5-14f9-493f-83fc-5ec0da7827f1"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.html", "file_name": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.html", "file_type": "text/html", "file_size": 33310, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/retrieving-privacy-safe-documents-over-a-network.html": {"node_ids": ["9383589f-e840-4b67-b2a0-56c8ddc6bdbc", "4e63a095-cf8f-4bfc-922d-5ae7cee288a2"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/retrieving-privacy-safe-documents-over-a-network.html", "file_name": "retrieving-privacy-safe-documents-over-a-network.html", "file_type": "text/html", "file_size": 28688, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.html": {"node_ids": ["b9858923-6720-4415-a97f-908b5f503af9", "c8cee370-e834-4710-af6e-1c4d640d1d22"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.html", "file_name": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.html", "file_type": "text/html", "file_size": 15104, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.html": {"node_ids": ["8a037895-5207-4bfc-a218-4686c6572581", "28df59df-30a7-4aee-90b7-cb990c758c92"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.html", "file_name": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.html", "file_type": "text/html", "file_size": 21244, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.html": {"node_ids": ["8d0b0051-1b7f-4ed8-951c-f4fcd00755af", "7e6bcfe0-e8f2-47bc-975f-c4e0a1ec6170"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.html", "file_name": "secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.html", "file_type": "text/html", "file_size": 13622, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.html": {"node_ids": ["a85b9a85-1bb3-4a38-bcfd-07da5bc554f0", "c7312d12-c832-4281-a316-83f147e4a5b0"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.html", "file_name": "secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.html", "file_type": "text/html", "file_size": 7372, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d.html": {"node_ids": ["8e89f7d7-9a58-4ca0-b468-42d0d14c0287", "8e3da68c-661c-4d88-80dd-4a6b639985f2"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d.html", "file_name": "shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d.html", "file_type": "text/html", "file_size": 5407, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/simplify-your-rag-application-architecture-with-llamaindex-postgresml.html": {"node_ids": ["d44d1b66-a6d3-4de4-97e6-449624b3fce1", "d713bde2-c216-45ca-a50c-4d82447ddcc9"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/simplify-your-rag-application-architecture-with-llamaindex-postgresml.html", "file_name": "simplify-your-rag-application-architecture-with-llamaindex-postgresml.html", "file_type": "text/html", "file_size": 12017, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.html": {"node_ids": ["8ba07468-3aef-4f59-b2e4-a4fc9643f4d0", "ed5e1673-d674-4279-8727-32d03c128bf7"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.html", "file_name": "special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.html", "file_type": "text/html", "file_size": 18153, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.html": {"node_ids": ["07e7f65a-092e-4a2f-8f4a-db3039b3e8de", "d49bc2cf-8eb4-4a55-8753-f055713a20c0"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.html", "file_name": "streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.html", "file_type": "text/html", "file_size": 17345, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.html": {"node_ids": ["79bd7357-de68-4075-b944-67130bc2e1c3", "d3bea69b-d7cf-4192-b5f1-bda08ed5f075"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.html", "file_name": "supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.html", "file_type": "text/html", "file_size": 16870, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.html": {"node_ids": ["512f2072-c2a8-4b01-844c-5a4905be82a4", "dca7137f-1b4b-49cf-b8e1-365b6ee4f349"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.html", "file_name": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.html", "file_type": "text/html", "file_size": 30335, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/the-latest-updates-to-llamacloud.html": {"node_ids": ["56225273-6ced-4d10-a708-beb42b52bc19", "8bf758c5-7993-4480-86e4-31efc1b5b2e1"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/the-latest-updates-to-llamacloud.html", "file_name": "the-latest-updates-to-llamacloud.html", "file_type": "text/html", "file_size": 5697, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.html": {"node_ids": ["8ab1d3a1-cacc-48e3-afd0-efeed2a51579", "7aeff5aa-425a-4a5d-b684-c669ed5b53ad"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.html", "file_name": "timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.html", "file_type": "text/html", "file_size": 34118, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.html": {"node_ids": ["886b75ed-9414-4389-9e46-fed3f2f5e0ac", "68cacd2a-53b3-4748-87b4-3daca98a318b"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.html", "file_name": "tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.html", "file_type": "text/html", "file_size": 22826, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/towards-long-context-rag.html": {"node_ids": ["89666c95-e8ad-4519-81cd-3bac4c737a3b", "907c913e-6863-4e67-ba21-99654c410ecd"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/towards-long-context-rag.html", "file_name": "towards-long-context-rag.html", "file_type": "text/html", "file_size": 21294, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.html": {"node_ids": ["9b3d12de-a059-49a6-875c-0b6d7afdd4c6", "2e713d08-09cb-465a-84ad-f47c210ab63d"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.html", "file_name": "transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.html", "file_type": "text/html", "file_size": 14000, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.html": {"node_ids": ["44d7b787-127b-4136-8076-08c9bc871bd5", "9a277129-75eb-4cf1-8762-14f409a7daba"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.html", "file_name": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.html", "file_type": "text/html", "file_size": 14281, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/unlocking-the-3rd-dimension-for-generative-ai-part-1.html": {"node_ids": ["781d1d55-0b3b-450b-a735-1f91508a309d", "bb7b2a61-3cdd-4d17-adcf-7f7dee63829f"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/unlocking-the-3rd-dimension-for-generative-ai-part-1.html", "file_name": "unlocking-the-3rd-dimension-for-generative-ai-part-1.html", "file_type": "text/html", "file_size": 8072, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.html": {"node_ids": ["29e5f7c9-ebbf-4ae8-a2e8-3e2dd022921c", "0a2f8bfb-56cc-4800-b16e-bddb54595e21"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.html", "file_name": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.html", "file_type": "text/html", "file_size": 19393, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/using-llms-for-retrieval-and-reranking-23cf2d3a14b6.html": {"node_ids": ["a7eb7f5e-5364-48ce-aa1f-77e9aa159c2c", "b6de8c2f-dc98-46a4-be20-8f3960df6968"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/using-llms-for-retrieval-and-reranking-23cf2d3a14b6.html", "file_name": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.html", "file_type": "text/html", "file_size": 17641, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/vellum-llamaindex-integration-58b476a1e33f.html": {"node_ids": ["5612d787-8f42-4b63-90d7-ab5d6e37df5c", "9c957df3-76e6-4ecd-9cb6-7c86f1819cdc"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/vellum-llamaindex-integration-58b476a1e33f.html", "file_name": "vellum-llamaindex-integration-58b476a1e33f.html", "file_type": "text/html", "file_size": 9037, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "/workspace/projects/LlamindexHelper/data/zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc.html": {"node_ids": ["fbb41713-78d4-4c48-87be-05927fc3dba6", "5d4d5ede-c2ae-41d9-b9a9-8b0394f363af"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc.html", "file_name": "zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc.html", "file_type": "text/html", "file_size": 8815, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}}}