{"docstore/data": {"a677fa02-6d9d-42e7-a2db-b9065adb193a": {"__data__": {"id_": "a677fa02-6d9d-42e7-a2db-b9065adb193a", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-08-06.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-08-06", "date": "Aug 6, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-08-06"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "79eb28d7-e2ec-4994-9d3b-20d8e9c18fc5", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-08-06.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-08-06", "date": "Aug 6, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-08-06"}, "hash": "bfff6c1560ab776b4b3292f63b8d4d5fd82ca579e0e9722ca935ad3a2e96bcc7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa39e8a5-b224-47bc-93ce-583a9dd22c98", "node_type": "1", "metadata": {}, "hash": "95078ce7c89b72e1e354415b6cb9657fe8d784ecda3e2379541180d42bffb80b", "class_name": "RelatedNodeInfo"}}, "text": "Greetings, Llama Lovers!\n\nWelcome to this week\u2019s edition of the LlamaIndex newsletter! We\u2019re excited to\nshare our latest updates including dynamic features like LlamaIndex Workflows\nand retrieval capabilities in LlamaCloud. Check out our in-depth guides,\ntutorials, and the upcoming webinars that will help you make the most of these\nnew developments.\n\n##  **The highlights:**\n\n  1. **LlamaIndex Workflows Launched:** LlamaIndex Workflows, a new event-driven architecture for building multi-agent applications, supports batching, async operations, and streaming. Agents subscribe to and emit events for complex, readable, Pythonic orchestration. [ Blogpost ](https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex) , [ Tweet ](https://x.com/llama_index/status/1819048068798616058) . \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 849, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa39e8a5-b224-47bc-93ce-583a9dd22c98": {"__data__": {"id_": "aa39e8a5-b224-47bc-93ce-583a9dd22c98", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-08-06.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-08-06", "date": "Aug 6, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-08-06"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "79eb28d7-e2ec-4994-9d3b-20d8e9c18fc5", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-08-06.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-08-06", "date": "Aug 6, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-08-06"}, "hash": "bfff6c1560ab776b4b3292f63b8d4d5fd82ca579e0e9722ca935ad3a2e96bcc7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a677fa02-6d9d-42e7-a2db-b9065adb193a", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-08-06.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-08-06", "date": "Aug 6, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-08-06"}, "hash": "bcc2a0d556c56229f19b10a2544d8ee8db6ab08e035c6405c1b782defa0275ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e039c69-ca59-404a-b1e8-77af83825991", "node_type": "1", "metadata": {}, "hash": "08f705fc950daac05e5e965f5b4b52b71a8aebe066ca53c48c5083adb9c1ffe9", "class_name": "RelatedNodeInfo"}}, "text": "2. **Dynamic Retrieval Feature in LlamaCloud:** A new feature in LlamaCloud now supports dynamic retrieval for QA assistants, enabling both chunk-level and file-level document retrieval based on query similarity to intelligently route queries. [ Blogpost ](https://www.llamaindex.ai/blog/dynamic-retrieval-with-llamacloud) , [ Notebook ](https://github.com/run-llama/llamacloud-demo/blob/main/examples/10k_apple_tesla/demo_file_retrieval.ipynb) , [ Tweet ](https://x.com/llama_index/status/1818337133746360623) . \n  ", "mimetype": "text/plain", "start_char_idx": 849, "end_char_idx": 1365, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e039c69-ca59-404a-b1e8-77af83825991": {"__data__": {"id_": "4e039c69-ca59-404a-b1e8-77af83825991", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-08-06.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-08-06", "date": "Aug 6, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-08-06"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "79eb28d7-e2ec-4994-9d3b-20d8e9c18fc5", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-08-06.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-08-06", "date": "Aug 6, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-08-06"}, "hash": "bfff6c1560ab776b4b3292f63b8d4d5fd82ca579e0e9722ca935ad3a2e96bcc7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa39e8a5-b224-47bc-93ce-583a9dd22c98", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-08-06.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-08-06", "date": "Aug 6, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-08-06"}, "hash": "559301b042a9116dd24bc0cd2fc231b7b62570faa37f24860993a609ae736bd2", "class_name": "RelatedNodeInfo"}}, "text": "3. **LongRAG LlamaPack:** LongRAG is now available as a LlamaPack in LlamaIndex, utilizing larger document chunks and long-context LLMs for more effective synthesis. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-longrag/examples/longrag.ipynb) , [ Tweet ](https://x.com/llama_index/status/1818802688274100578) . \n\n##  **Feature Releases and Enhancements:**\n\n  * We have launched LlamaIndex Workflows, a new event-driven way to build multi-agent applications where each agent acts as a component that subscribes to and emits events, allowing for complex, readable, and Pythonic orchestration with enhanced support for batching, async operations, and streaming. [ Blogpost ](https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex) , [ Tweet ](https://x.com/llama_index/status/1819048068798616058) . \n  * We have introduced a new feature in LlamaCloud to improve your QA assistant with our latest capability for dynamic retrieval, allowing both chunk-level and file-level retrieval. This feature enables the retrieval of entire documents based on query similarity, which supports building agents that can intelligently route queries based on their content. [ Blogpost ](https://www.llamaindex.ai/blog/dynamic-retrieval-with-llamacloud) , [ Notebook ](https://github.com/run-llama/llamacloud-demo/blob/main/examples/10k_apple_tesla/demo_file_retrieval.ipynb) , [ Tweet ](https://x.com/llama_index/status/1818337133746360623) . \n  * We have launched LongRAG as a LlamaPack in LlamaIndex. LongRAG simplifies retrieval by using larger document chunks and leveraging long-context LLMs for synthesis. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-longrag/examples/longrag.ipynb) , [ Tweet ](https://x.com/llama_index/status/1818802688274100578) . \n\n**Guides:**\n\n  * [ Guide ](https://docs.llamaindex.ai/en/latest/examples/workflow/react_agent/) to building a ReAct agent from scratch using LlamaIndex workflows. \n  * [ Guide ](https://docs.llamaindex.ai/en/latest/examples/workflow/rag/) to Building an Event-Driven RAG Pipeline with LlamaIndex, featuring distinct event-driven steps for retrieval, reranking, and synthesis, enhanced with graph tracing and async processing. \n  * [ Guide ](https://docs.llamaindex.ai/en/latest/module_guides/observability/#mlflow) to MLflow in LlamaIndex to manage, deploy, and monitor your genAI applications with MLflow's tracking, packaging, evaluation, and tracing capabilities. \n\n**Tutorials:**\n\n  * [ Pavan Kumar\u2019s ](https://x.com/pavan_mantha1) [ tutorial ](https://blog.gopenai.com/building-smarter-agents-using-llamaindex-agents-and-qdrants-hybrid-search-50c0ecbbfb0d) on Building Smarter Agents using LlamaIndex Agents and Qdrant\u2019s Hybrid Search. \n  * [ Farzad Sunavala\u2019s ](https://www.linkedin.com/in/farzadsunavala) [ tutorial ](https://farzzy.hashnode.dev/rag-observability-and-evaluation-with-azure-ai-search-azure-openai-llamaindex-and-arize-phoenix) on RAG Observability and Evaluation with Azure AI Search, Azure OpenAI, LlamaIndex, and Arize Phoenix. \n  * [ Composio\u2019s ](https://x.com/composiohq) [ tutorial ](https://github.com/ComposioHQ/composio/tree/master/python/examples/pr_agent/pr_agent_llama_index) on building a PR review agent using Composio's GitHub/Slack tools and LlamaIndex agent abstractions. \n  * [ Benito Martin\u2019s ](https://medium.com/@benitomartin) [ tutorial ](https://medium.com/@benitomartin/find-your-code-scaling-a-llamaindex-and-qdrant-application-with-google-kubernetes-engine-2db126f16344) on Scaling a LlamaIndex and Qdrant Application with Google Kubernetes Engine. \n  * [ Chew Loong Nian\u2019s ](https://medium.com/@chewloongnian) [ tutorial ](https://pub.towardsai.net/introducing-llamaextract-beta-transforming-metadata-extraction-for-enhanced-rag-queries-de3d74d34cd7) on Transforming Metadata Extraction for Enhanced RAG Queries using LlamaExtract. \n  * [ Pavan Kumar\u2019s ](https://x.com/pavan_mantha1) [ tutorial ](https://medium.com/@manthapavankumar11/practical-implementation-of-agentic-rag-workflows-with-llama-index-and-qdrant-3b6622cd3124) on Practical Implementation of Agentic RAG Workflows with Llama-Index and Qdrant. \n  * AI21 Labs [ tutorial ](https://www.llamaindex.ai/blog/jamba-instruct-s-256k-context-window-on-llamaindex) on using Jamba-Instruct Model with LlamaIndex. \n\n**Webinars And Hackathons:**\n\n  * [ Join us ](https://lu.ma/ka5xtyqo) for a webinar on August 8th with [ Dedy Kredo ](https://x.com/DedyKredo) from [ CodiumAI ](https://x.com/CodiumAI) on using RAG with LlamaIndex to help build a code generation solution that\u2019s contextually aware of the right elements of source code. \n  * [ Join us ](https://lu.ma/p13pkknm?tk=SsniSt) on RAG Hack Night at GitHub with [ Weaviate ](https://x.com/weaviate_io) , [ Neosync ](https://x.com/neosynccloud) , [ Arize AI ](https://x.com/arizeai) on August 13th. \n\n", "mimetype": "text/plain", "start_char_idx": 1365, "end_char_idx": 6336, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b26c34d-54af-4530-9762-9bbea0b23def": {"__data__": {"id_": "9b26c34d-54af-4530-9762-9bbea0b23def", "embedding": null, "metadata": {"filename": "introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex.md", "extension": ".md", "title": "Introducing workflows beta: a new way to create complex AI applications with LlamaIndex", "date": "Aug 1, 2024", "url": "https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0d9fb0a3-eb34-4516-992d-0ca9644c6eb5", "node_type": "4", "metadata": {"filename": "introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex.md", "extension": ".md", "title": "Introducing workflows beta: a new way to create complex AI applications with LlamaIndex", "date": "Aug 1, 2024", "url": "https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex"}, "hash": "4d78708f70d29e623ec4837a43bb5745585c866e225c4616a83578db1a599f01", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4db6b442-65b8-46b5-b631-69489f0eb0d9", "node_type": "1", "metadata": {}, "hash": "6333d7179ee7b794323e0111e668f87e208dcca95cd8acf04458b5b85d20b89b", "class_name": "RelatedNodeInfo"}}, "text": "We\u2019re pleased to be introducing a brand-new beta feature of LlamaIndex:\nworkflows, a mechanism for orchestrating actions in the increasingly-complex\nAI application we see our users building.\n\nWhat started as a trend with the advent of LLMs is now a de-facto standard: AI\napplications are made of multiple tasks implemented by different components.\nOpen source frameworks in the market strive to make the life of AI engineers\neasier by providing easy-to-use abstractions for foundational components like\ndata loaders, LLMs, vector databases, and rerankers, all the way up to\nexternal services. Meanwhile, all of those frameworks are also on a quest to\nfind what\u2019s the best abstraction to orchestrate such components, researching\nwhat\u2019s most intuitive and efficient for an AI developer in order to implement\nthe logic that keeps together a compound AI system.\n\nTwo of those potential orchestration patterns are chains and pipelines, both\nof which are implementations of the same Directed Acyclic Graph (DAG)\nabstraction. We took a stab at this with our [ Query Pipelines\n](https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537)\nrelease at the beginning of the year - it was a declarative API that let you\norchestrate simple-to-advanced query workflows over your data for different\nuse cases, like QA, structured extraction, and agentic automation. But as we\ntried to build upon it and experimented with adding cycles to better support\nmore complex workflows, we noticed several issues, causing us to reflect on\nwhy a DAG may not be the right fit for an agentic landscape, and what\nalternatives we could introduce in the framework.\n\n##  Limitations of a Graph-based UX\n\nA fundamental aspect of DAGs is the \u201cA\u201d in DAGs: they are acyclic, meaning\nthere are no loops. But in a world that\u2019s more and more agentic, the inability\nto perform loops in an AI application\u2019s logic is simply unacceptable. For\nexample, if one component provides bad results, an AI developer should have a\nway to tell the system to self-correct and try again.\n\nEven without adding cycles and loops to a DAG, the query pipeline suffered\nfrom a few noticeable issues:\n\n  * hard to debug when things go wrong \n  * they obscure how components and modules are being executed \n  * our pipeline orchestrator became increasingly extremely complex and had to handle a ton of different edge cases \n  * they were hard to read for complex pipelines \n\nOnce we added cycles to query pipelines, these developer UX issues around\ngraphs were amplified. We experienced first-hand developer pain in areas like:\n\n  * A lot of core orchestration logic like ` if-else ` statements and ` while ` loops get baked into the edges of the graph. Defining these edges becomes cumbersome and verbose. \n  * It became hard to handle edge cases around optional and default values. It was hard for us as a framework to figure out whether a parameter would get passed from upstream nodes. \n  * Defining graphs with cycles didn\u2019t always feel as natural to developers building agents. An agent encapsulates a general LLM-powered entity that can take in observations and generate responses. Here the graph UX enforced that \u201cagent\u201d node had the incoming edges and outgoing edges explicitly defined, forcing users to define verbose communication patterns with other nodes. \n\nWe asked: are graphs really the only abstraction we can use to orchestrate\ncomponents in a compound AI system?\n\n##  From Graphs to EDA: go event-driven\n\nA compound AI system can be implemented with a LlamaIndex _workflow_ . The\nworkflow dispatches events back and forth through a collection of Python\nfunctions called _steps_ . Each step can be seen as one component of your\nsystem: one to process a query, one to talk with an LLM, one to load data from\na vector database and so on. Every step receives one or more events to process\nand can optionally send back events that will be relayed to other components\nif needed.\n\nMoving to an event-driven architecture causes a fundamental shift in design.\nIn many graph implementations the graph traversal algorithm is responsible for\ndetermining what component should run next and what data should be passed. In\nan event-driven architecture, the component subscribes to a certain types of\nevents and it\u2019s ultimately responsible for deciding what to do based on the\ndata it received.\n\nIn an event-driven system, concepts like optionality of inputs and default\nvalues are sorted out at the component level, dramatically simplifying the\norchestration code.\n\n##  A workflow primer\n\nTo help clarify this idea, let\u2019s look at an example. A minimal LlamaIndex\nworkflow looks like this:\n\n    \n    \n    from llama_index.core.workflow import (\n        StartEvent,\n        StopEvent,\n        Workflow,\n        step,\n    )\n    \n    from llama_index.llms.openai import OpenAI\n    \n    class OpenAIGenerator(Workflow):\n        @step()\n        async def generate(self, ev: StartEvent) -> StopEvent:\n            query = ev.get(\"query\")\n            llm = OpenAI()\n            response = await llm.acomplete(query)\n            return StopEvent(result=str(response))\n    \n    w = OpenAIGenerator(timeout=10, verbose=False)\n    result = await w.run(query=\"What's LlamaIndex?\")\n    print(result)\n\nThe ` generate ` function is marked as a workflow step using the ` @step `\ndecorator and it declares which events it wants to receive and which events it\nwill send back using the method signature with proper typing annotations. In\norder to run a workflow, we create an instance of the ` OpenAIGenerator `\nclass passing some configuration parameters like the desired timeout and we\nthen call the ` run ` method. Any keyword argument passed to ` run ` will be\npacked into a special event of type ` StartEvent ` that will be relayed to the\nsteps that requested it (in this case, only the ` generate ` step). The `\ngenerate ` step returns a special event of type ` StopEvent ` that will signal\nthe workflow to gracefully halt its execution. A ` StopEvent ` carries any\ndata that we want to return to the caller as the workflow result, in this case\nthe LLM response.\n\n###  Workflows can loop\n\nIn event-driven architectures, loops have to do with communication rather than\ntopology. Any step can decide to call another step multiple times by crafting\nand sending the proper event. Let\u2019s see a self-correction loop for example\n(check the [ notebook\n](https://docs.llamaindex.ai/en/latest/examples/workflow/reflection/) for the\nfull code):\n\n    \n    \n    class ExtractionDone(Event):\n        output: str\n        passage: str\n    \n    \n    class ValidationErrorEvent(Event):\n        error: str\n        wrong_output: str\n        passage: str\n        \n        \n    class ReflectionWorkflow(Workflow):\n        @step()\n        async def extract(\n            self, ev: StartEvent | ValidationErrorEvent\n        ) -> StopEvent | ExtractionDone:\n            if isinstance(ev, StartEvent):\n                passage = ev.get(\"passage\")\n                if not passage:\n                    return StopEvent(result=\"Please provide some text in input\")\n                reflection_prompt = \"\"\n            elif isinstance(ev, ValidationErrorEvent):\n                passage = ev.passage\n                reflection_prompt = REFLECTION_PROMPT.format(\n                    wrong_answer=ev.wrong_output, error=ev.error\n                )\n    \n            llm = Ollama(model=\"llama3\", request_timeout=30)\n            prompt = EXTRACTION_PROMPT.format(\n                passage=passage, schema=CarCollection.schema_json()\n            )\n            if reflection_prompt:\n                prompt += reflection_prompt\n    \n            output = await llm.acomplete(prompt)\n    \n            return ExtractionDone(output=str(output), passage=passage)\n    \n        @step()\n        async def validate(\n            self, ev: ExtractionDone\n        ) -> StopEvent | ValidationErrorEvent:\n            try:\n                json.loads(ev.output)\n            except Exception as e:\n                print(\"Validation failed, retrying...\")\n                return ValidationErrorEvent(\n                    error=str(e), wrong_output=ev.output, passage=ev.passage\n                )\n    \n            return StopEvent(result=ev.output)\n    \n    w = ReflectionWorkflow(timeout=60, verbose=True)\n    result = await w.run(\n        passage=\"There are two cars available: a Fiat Panda with 45Hp and a Honda Civic with 330Hp.\"\n    )\n    print(result)\n\nIn this example, the ` validate ` step receives the result of the tentative\nschema extraction as an event and it can decide to try again by returning a `\nValidationErrorEvent ` that will be eventually delivered to the ` extract `\nstep which will perform another attempt. Note that in this example the\nworkflow might time out if this extract/validate loop keeps providing poor\nresults for too long, but another strategy might be giving up after a precise\nnumber of attempts, just to give an example.\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 8929, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4db6b442-65b8-46b5-b631-69489f0eb0d9": {"__data__": {"id_": "4db6b442-65b8-46b5-b631-69489f0eb0d9", "embedding": null, "metadata": {"filename": "introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex.md", "extension": ".md", "title": "Introducing workflows beta: a new way to create complex AI applications with LlamaIndex", "date": "Aug 1, 2024", "url": "https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0d9fb0a3-eb34-4516-992d-0ca9644c6eb5", "node_type": "4", "metadata": {"filename": "introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex.md", "extension": ".md", "title": "Introducing workflows beta: a new way to create complex AI applications with LlamaIndex", "date": "Aug 1, 2024", "url": "https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex"}, "hash": "4d78708f70d29e623ec4837a43bb5745585c866e225c4616a83578db1a599f01", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b26c34d-54af-4530-9762-9bbea0b23def", "node_type": "1", "metadata": {"filename": "introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex.md", "extension": ".md", "title": "Introducing workflows beta: a new way to create complex AI applications with LlamaIndex", "date": "Aug 1, 2024", "url": "https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex"}, "hash": "59a2976635cf0679d7577515ec647c33bb456c9cb2c0dc271a85962d73487967", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2999d86c-b90c-43df-821d-1814e7ff0b48", "node_type": "1", "metadata": {}, "hash": "d1cee397974635b587f4fca5e9b9076d9668d6f4ca2e2ed405f0b0ecec6df407", "class_name": "RelatedNodeInfo"}}, "text": "###  Workflows keep state\n\nWorkflows keep a global state during the execution, and this state can be\nshared and propagated to its steps upon request. This shared state is\nimplemented as a ` Context ` object and can be used by steps to store data in\nbetween iterations but also as an alternative form of communication among\ndifferent steps. Let\u2019s see an excerpt from a more complex RAG example as an\nexample showing how to use the global context (check [ notebook\n](https://docs.llamaindex.ai/en/latest/examples/workflow/rag/) for full code):\n\n    \n    \n    class RAGWorkflow(Workflow):\n        @step(pass_context=True)\n        async def ingest(self, ctx: Context, ev: StartEvent) -> Optional[StopEvent]:\n            dataset_name = ev.get(\"dataset\")\n            _, documents = download_llama_dataset(dsname, \"./data\")\n            ctx.data[\"INDEX\"] = VectorStoreIndex.from_documents(documents=documents)\n            return StopEvent(result=f\"Indexed {len(documents)} documents.\")\n            \n        ...\n\nIn this case the ` ingest ` step creates an index, and it wants to make it\navailable to any other step that might needed it later during workflow\nexecution. The idiomatic way of doing that in a LlamaIndex workflow is to\ndeclare the step requires an instance of the global context ( `\n@step(pass_context=True) ` does the trick) and store the index in the context\nitself with a predefined key that other steps might access later.\n\n###  Workflows can be customized\n\nAlongside Workflows, we\u2019ll be releasing a set of predefined workflows so that\nthe most common use cases can be implemented with a single line of code. ", "mimetype": "text/plain", "start_char_idx": 8929, "end_char_idx": 10547, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2999d86c-b90c-43df-821d-1814e7ff0b48": {"__data__": {"id_": "2999d86c-b90c-43df-821d-1814e7ff0b48", "embedding": null, "metadata": {"filename": "introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex.md", "extension": ".md", "title": "Introducing workflows beta: a new way to create complex AI applications with LlamaIndex", "date": "Aug 1, 2024", "url": "https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0d9fb0a3-eb34-4516-992d-0ca9644c6eb5", "node_type": "4", "metadata": {"filename": "introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex.md", "extension": ".md", "title": "Introducing workflows beta: a new way to create complex AI applications with LlamaIndex", "date": "Aug 1, 2024", "url": "https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex"}, "hash": "4d78708f70d29e623ec4837a43bb5745585c866e225c4616a83578db1a599f01", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4db6b442-65b8-46b5-b631-69489f0eb0d9", "node_type": "1", "metadata": {"filename": "introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex.md", "extension": ".md", "title": "Introducing workflows beta: a new way to create complex AI applications with LlamaIndex", "date": "Aug 1, 2024", "url": "https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex"}, "hash": "267d20159b19f0eadd4846631cfe57087b335d5f48f91a4bd8ca7333bf45daa7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3eb2234-5067-4718-a77d-01323fa73868", "node_type": "1", "metadata": {}, "hash": "56025550995c6851ed27923e8bb3dfd44c09f470b264dd25fbe570d5488028ad", "class_name": "RelatedNodeInfo"}}, "text": "Using\nthese predefined flows, users still might want to just _slightly_ change a\npredefined workflow to introduce some custom behavior without having to\nrewrite a whole workflow from scratch. Let\u2019s say you want to customize a RAG\nworkflow and use a custom re-ranking step, all you would need to do is\nsubclass a hypothetical built-in ` RAGWorkflow ` class and override the `\nrerank ` step like this:\n\n    \n    \n    class MyWorkflow(RAGWorkflow):\n        @step(pass_context=True)\n        def rerank(\n            self, ctx: Context, ev: Union[RetrieverEvent, StartEvent]\n        ) -> Optional[QueryResult]:\n            # my custom reranking logic here\n            \n     \n    w = MyWorkflow(timeout=60, verbose=True)\n    result = await w.run(query=\"Who is Paul Graham?\")\n\n###  Workflows can be debugged\n\nThe complexity of your workflows will grow with the complexity of your\napplication logic, and sometimes it can be hard to understand how events will\nflow during execution by just looking at the Python code. ", "mimetype": "text/plain", "start_char_idx": 10547, "end_char_idx": 11555, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3eb2234-5067-4718-a77d-01323fa73868": {"__data__": {"id_": "b3eb2234-5067-4718-a77d-01323fa73868", "embedding": null, "metadata": {"filename": "introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex.md", "extension": ".md", "title": "Introducing workflows beta: a new way to create complex AI applications with LlamaIndex", "date": "Aug 1, 2024", "url": "https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0d9fb0a3-eb34-4516-992d-0ca9644c6eb5", "node_type": "4", "metadata": {"filename": "introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex.md", "extension": ".md", "title": "Introducing workflows beta: a new way to create complex AI applications with LlamaIndex", "date": "Aug 1, 2024", "url": "https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex"}, "hash": "4d78708f70d29e623ec4837a43bb5745585c866e225c4616a83578db1a599f01", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2999d86c-b90c-43df-821d-1814e7ff0b48", "node_type": "1", "metadata": {"filename": "introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex.md", "extension": ".md", "title": "Introducing workflows beta: a new way to create complex AI applications with LlamaIndex", "date": "Aug 1, 2024", "url": "https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex"}, "hash": "40713d665aba8d333f39167d73b8f140aa8c04976545ff87a05cec418c56a070", "class_name": "RelatedNodeInfo"}}, "text": "To ease the\nunderstanding of complex workflows and to support the debugging of workflow\nexecutions, LlamaIndex provides two functions:\n\n  * ` draw_all_possible_flows ` produces a picture showing all the steps in a workflow and how events will possibly flow \n  * ` draw_most_recent_execution ` produces a similar picture, showing only the events that were actually sent during the last workflow execution \n\nOn top of that, workflows can be executed manually, by calling ` run_step() `\nmultiple times until all the steps have completed. After each ` run_step `\ncall, the workflow can be inspected, examining any intermediate results or\ndebug logs.\n\n##  Why you should use workflows today\n\nDespite being at an early stage of development, LlamaIndex workflows already\nrepresent a step forward compared to query pipelines, extending their\nfunctionalities and adding more flexibility. On top of that, workflows come\nwith a set of features that you would normally expect from a much more mature\nsoftware:\n\n  * Fully async with streaming support \n  * Instrumented by default, providing one-click observability with the supported integrations \n  * Step-by-step execution for easier debugging \n  * Validation and visualization of the event-driven dependencies \n  * Events are implemented as pydantic models to ease customization and further developments of new features \n\n##  Resources\n\nCheck out our [ workflow documentation\n](https://docs.llamaindex.ai/en/latest/module_guides/workflow/) and our [\nexamples ](https://github.com/run-\nllama/llama_index/tree/main/docs/docs/examples/workflow) including:\n\n  * [ RAG ](https://docs.llamaindex.ai/en/latest/examples/workflow/rag/)\n  * [ Reflection ](https://docs.llamaindex.ai/en/latest/examples/workflow/reflection/)\n  * [ Function calling ](https://docs.llamaindex.ai/en/latest/examples/workflow/function_calling_agent/)\n  * [ ReAct agent ](https://docs.llamaindex.ai/en/latest/examples/workflow/react_agent/)\n\n", "mimetype": "text/plain", "start_char_idx": 11555, "end_char_idx": 13504, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5f5f812-a8b0-40b3-8f51-66b2c49f53b4": {"__data__": {"id_": "d5f5f812-a8b0-40b3-8f51-66b2c49f53b4", "embedding": null, "metadata": {"filename": "jamba-instruct-s-256k-context-window-on-llamaindex.md", "extension": ".md", "title": "Jamba-Instruct's 256k context window on LlamaIndex", "date": "Jul 31, 2024", "url": "https://www.llamaindex.ai/blog/jamba-instruct-s-256k-context-window-on-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6e83ca1f-180e-4e5d-b417-df488eda2f0f", "node_type": "4", "metadata": {"filename": "jamba-instruct-s-256k-context-window-on-llamaindex.md", "extension": ".md", "title": "Jamba-Instruct's 256k context window on LlamaIndex", "date": "Jul 31, 2024", "url": "https://www.llamaindex.ai/blog/jamba-instruct-s-256k-context-window-on-llamaindex"}, "hash": "85f47676dd585ef6f69680f12e9ec734a66fd4f734864c01b001a01ce5068324", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dffdd78b-ca74-4469-a6e2-22902100b64b", "node_type": "1", "metadata": {}, "hash": "8d1020b9fdc2b2743cc49629da6dcb085e32de308e7127d3ca39ecd75ff3ead0", "class_name": "RelatedNodeInfo"}}, "text": "Build state-of-the-art RAG applications for the enterprise by leveraging\nLlamaIndex\u2019s market-leading RAG strategies with AI21 Labs\u2019 long context\nFoundation Model, Jamba-Instruct.\n\nWe at AI21 Labs are excited to announce that our groundbreaking Jamba-Instruct\nfoundation model is now available through leading data framework LlamaIndex.\nWith this integration, developers can now build powerful RAG enterprise\napplications with enhanced accuracy and cost-efficiency due to Jamba-\nInstruct\u2019s impressive 256K context window and LlamaIndex\u2019s sophisticated end-\nto-end offerings for RAG.\n\nWhile many models declare long context windows, researchers at NVIDIA found\nthat [ most falter under evaluation ](https://arxiv.org/pdf/2404.06654) ,\nrevealing a discrepancy between their claimed and effective context window\nlengths. Jamba-Instruct is one of the few models on the market to not only\nachieve parity between its declared and effective lengths, but to do so with a\nmuch longer context window length than any other model in its size class.\n\nBy offering a context window of 256K\u2014roughly equivalent to 800 pages of\ntext\u2014Jamba-Instruct increases the number of retrieved chunks and can vastly\nimprove the entire RAG system, rather than trying to improve the search\nmechanism or incorporating an additional reranking component. Using a long\ncontext foundation model like Jamba-Instruct makes querying private enterprise\ndata with RAG both more reliable and easier.\n\nIn the following notebook ( [ also available directly on colab\n](https://colab.research.google.com/drive/1ycpC1pfCty9bqCmHdrgvAtqQwP1o0lPg)\n), we\u2019ll walk through an example of querying a collection of financial\ndocuments, showing how Jamba-Instruct\u2019s 256K context window allows the RAG\npipeline to retrieve more chunks at once in order to deliver an accurate\nanswer.\n\n###  RAG Q&A on financial documents\n\nTo get started, these are the packages you need to install. You will also need\nAPI keys to set up OpenAI for embeddings and AI21 for Jamba-Instruct.\n\n    \n    \n    !pip install llama-index\n    !pip install -U ai21\n    !pip install llama-index-llms-ai21\n    \n    import os\n    from llama_index.core.llama_dataset import download_llama_dataset\n    from llama_index.core.llama_pack import download_llama_pack\n    from llama_index.core import VectorStoreIndex\n    from llama_index.core import SimpleDirectoryReader\n    from llama_index.llms.ai21 import AI21\n    \n    os.environ['OPENAI_API_KEY'] = 'YOUR_OPENAI_API_KEY' # For embeddings\n    os.environ['AI21_API_KEY'] = 'YOUR_AI21_API_KEY' # For the generation\n    \n    # Setup jamba instruct as the llm\n    llm = AI21(\n        model='jamba-instruct',\n        temperature=0,\n        max_tokens=2000\n    )\n\nNext, download 5 10-K forms from Amazon from [ Amazon\u2019s Investor Relations\npage. ](https://ir.aboutamazon.com/sec-filings/default.aspx)\n\n    \n    \n    # Get the data - download 10k forms from AMZN from the last five years\n    os.mkdir(\"data\")\n    !wget 'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/c7c14359-36fa-40c3-b3ca-5bf7f3fa0b96.pdf' -O 'data/amazon_2023.pdf'\n    !wget 'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/d2fde7ee-05f7-419d-9ce8-186de4c96e25.pdf' -O 'data/amazon_2022.pdf'\n    !wget 'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/f965e5c3-fded-45d3-bbdb-f750f156dcc9.pdf' -O 'data/amazon_2021.pdf'\n    !wget 'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/336d8745-ea82-40a5-9acc-1a89df23d0f3.pdf' -O 'data/amazon_2020.pdf'\n    !wget 'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/4d39f579-19d8-4119-b087-ee618abf82d6.pdf' -O 'data/amazon_2019.pdf'\n\nSet up your index and query engine to create the retrieval and generation\ncomponents of your RAG system.\n\n    \n    \n    ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3738, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dffdd78b-ca74-4469-a6e2-22902100b64b": {"__data__": {"id_": "dffdd78b-ca74-4469-a6e2-22902100b64b", "embedding": null, "metadata": {"filename": "jamba-instruct-s-256k-context-window-on-llamaindex.md", "extension": ".md", "title": "Jamba-Instruct's 256k context window on LlamaIndex", "date": "Jul 31, 2024", "url": "https://www.llamaindex.ai/blog/jamba-instruct-s-256k-context-window-on-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6e83ca1f-180e-4e5d-b417-df488eda2f0f", "node_type": "4", "metadata": {"filename": "jamba-instruct-s-256k-context-window-on-llamaindex.md", "extension": ".md", "title": "Jamba-Instruct's 256k context window on LlamaIndex", "date": "Jul 31, 2024", "url": "https://www.llamaindex.ai/blog/jamba-instruct-s-256k-context-window-on-llamaindex"}, "hash": "85f47676dd585ef6f69680f12e9ec734a66fd4f734864c01b001a01ce5068324", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5f5f812-a8b0-40b3-8f51-66b2c49f53b4", "node_type": "1", "metadata": {"filename": "jamba-instruct-s-256k-context-window-on-llamaindex.md", "extension": ".md", "title": "Jamba-Instruct's 256k context window on LlamaIndex", "date": "Jul 31, 2024", "url": "https://www.llamaindex.ai/blog/jamba-instruct-s-256k-context-window-on-llamaindex"}, "hash": "d38a432a2843293bd45246dfd1fecc4c1bb3d95fec44f7e2b8725189a54a4d6b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35f3b3de-27fd-4c37-96cc-c5ff7a053302", "node_type": "1", "metadata": {}, "hash": "14467b738a31a3de1e7218c940dec21221efbec1a7f0f4ae51bc50173c4b064c", "class_name": "RelatedNodeInfo"}}, "text": "# Setup the index\n    file_list = [os.path.join(\"data\", f) for f in os.listdir(\"data\")]\n    \n    amzn_10k_docs = SimpleDirectoryReader(input_files=file_list).load_data()\n    index = VectorStoreIndex.from_documents(documents=amzn_10k_docs)\n    \n    # Build a query engine\n    default_query_engine = index.as_query_engine(llm)\n\nLet\u2019s enter a query to make sure our RAG system is working.\n\n    \n    \n    answer = default_query_engine.query(\"What was the company's revenue in 2021?\")\n    print(answer.response)\n    \n    \n    The company's revenue in 2021 was $469,822 million.\n\nGreat! ", "mimetype": "text/plain", "start_char_idx": 3738, "end_char_idx": 4319, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35f3b3de-27fd-4c37-96cc-c5ff7a053302": {"__data__": {"id_": "35f3b3de-27fd-4c37-96cc-c5ff7a053302", "embedding": null, "metadata": {"filename": "jamba-instruct-s-256k-context-window-on-llamaindex.md", "extension": ".md", "title": "Jamba-Instruct's 256k context window on LlamaIndex", "date": "Jul 31, 2024", "url": "https://www.llamaindex.ai/blog/jamba-instruct-s-256k-context-window-on-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6e83ca1f-180e-4e5d-b417-df488eda2f0f", "node_type": "4", "metadata": {"filename": "jamba-instruct-s-256k-context-window-on-llamaindex.md", "extension": ".md", "title": "Jamba-Instruct's 256k context window on LlamaIndex", "date": "Jul 31, 2024", "url": "https://www.llamaindex.ai/blog/jamba-instruct-s-256k-context-window-on-llamaindex"}, "hash": "85f47676dd585ef6f69680f12e9ec734a66fd4f734864c01b001a01ce5068324", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dffdd78b-ca74-4469-a6e2-22902100b64b", "node_type": "1", "metadata": {"filename": "jamba-instruct-s-256k-context-window-on-llamaindex.md", "extension": ".md", "title": "Jamba-Instruct's 256k context window on LlamaIndex", "date": "Jul 31, 2024", "url": "https://www.llamaindex.ai/blog/jamba-instruct-s-256k-context-window-on-llamaindex"}, "hash": "7e05d4ca14adc7f10960dcacab807ef76ebce9b9191e8563f70d5b246e614264", "class_name": "RelatedNodeInfo"}}, "text": "It works. Now let\u2019s try a similar query to continue validating.\n\n    \n    \n    answer = default_query_engine.query(\"What was the company's revenue in 2023?\")\n    print(answer.response)\n    \n    \n    The company's revenue in 2023 was not explicitly mentioned in the provided context. However, it is mentioned that the company's operating income increased to $36.9 billion in 2023, compared to $12.2 billion in 2022.\n\nWe can see there\u2019s a problem\u2014we know that the answer to our question is most\ndefinitely included in our documents, yet our RAG system is claiming that it\ncannot find the answer. That\u2019s because the default amount of retrieved chunks\nis rather small (a few chunks). This makes the whole system prone to errors\nand failing to capture information that is indeed located in the documents.\n\nHowever, with Jamba-Instruct, a model which handles a 256K context window\neffectively, we can increase the number of retrieved chunks from just a few\n(default value) to 100 and vastly improve the entire RAG system.\n\nLet\u2019s build a new query engine on top of our existing index and try the query\nthat failed before.\n\n    \n    \n    # Large amount of chunks in the retrieval process\n    extended_query_engine = index.as_query_engine(llm,\n                                                  similarity_top_k=100)\n    \n    answer = extended_query_engine.query(\"What was the company's revenue in 2023?\")\n    print(answer.response)\n    \n    \n    The company's revenue in 2023 was $574.785 million.\n\nWe see that the RAG system, with the help of Jamba-Instruct\u2019s 256K context\nwindow, is now able to produce the accurate answer.\n\nLet\u2019s try one more answer to validate our new RAG system.\n\n    \n    \n    answer = default_query_engine.query(\"Was there a stock split in the last five years?\")\n    print(answer.response)\n    \n    \n    No, there was no stock split in the last five years.\n    \n    \n    answer = extended_query_engine.query(\"Was there a stock split in the last five years?\")\n    print(answer.response)\n    \n    \n    Yes, there was a stock split in the last five years. On May 27, 2022, Amazon.com, Inc. effected a 20-for-1 stock split of its common stock.\n\n###  Context is king\n\nOften, the debate is framed as \u201cRAG vs. long context.\u201d We at AI21 Labs believe\nthat\u2019s the wrong way to look at it. Rather, it\u2019s long context _plus_ RAG. When\npaired together in an AI system, a long context model enhances the quality and\naccuracy of a RAG system, especially useful in enterprise contexts that\ninvolve lengthy documents or vast databases of information.\n\nGoing forward, as RAG systems continue to scale, the number of documents and\nlengths of chunks will drastically increase. Only a long context model\u2014whose\ncontext length truly delivers\u2014can handle this amount of text.\n\n", "mimetype": "text/plain", "start_char_idx": 4319, "end_char_idx": 7084, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea2a5822-ed60-4d58-9781-b44c2a990ac4": {"__data__": {"id_": "ea2a5822-ed60-4d58-9781-b44c2a990ac4", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-07-30.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-30", "date": "Jul 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-30"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "62dafea5-0c5a-49ee-93da-d159adb5c9ce", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-07-30.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-30", "date": "Jul 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-30"}, "hash": "4eea636929a3835c68aaa7d29ee4e09c9a02df13e059fced72021b67e2c04a59", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6c514cb-bfff-4e25-bbac-19d0f27c3310", "node_type": "1", "metadata": {}, "hash": "e25e29070c20d03109d4774eaab9d65b90486df036780105f9f39757997b17a2", "class_name": "RelatedNodeInfo"}}, "text": "Hello, Llama Enthusiasts!\n\nWelcome to this week\u2019s edition of the LlamaIndex newsletter! We\u2019re excited to\nbring you the latest updates on our products, including LlamaCloud and\nLlamaExtract, comprehensive guides, detailed tutorials, and upcoming webinars.\n\n**LlamaIndex Office Hours:**\n\nBuilding agents or RAG applications? Join us for a 15-30 minute Zoom chat\nabout your projects, and we\u2019ll thank you with free LlamaIndex swag.\n\n[ **Sign up for our office hours today**\n](https://docs.google.com/forms/d/e/1FAIpQLSefrnmxQWD-1OhSP51kUKtdbw9EGDjrMLefkZFACKD19TKsuQ/viewform)\n\n##  **The highlights:**\n\n  1. **LlamaExtract Beta Launched:** Our managed service for structured data extraction from unstructured documents, enhancing RAG and agent pipelines via both UI and API. [ Blogpost ](https://www.llamaindex.ai/blog/introducing-llamaextract-beta-structured-data-extraction-in-just-a-few-clicks) , [ Tweet ](https://x.com/llama_index/status/1816570219734945944) . \n  2. **Structured Extraction for LLM-powered Pipelines:** Structured extraction capabilities for ETL, RAG, and agent workflows, featuring asynchronous operations and streaming. Integrate a Pydantic object with your LLM for structured extraction at the chunk or document level with real-time JSON output visualization. [ Docs ](https://docs.llamaindex.ai/en/latest/use_cases/extraction/) , [ Tweet ](https://x.com/llama_index/status/1816195731826565586) . \n\n##  **Feature Releases and Enhancements:**\n\n  1. We have launched LlamaExtract as an early preview of our managed service for structured data extraction from unstructured documents, enhancing RAG and agent pipelines, now available in beta via UI and API. [ Blogpost ](https://www.llamaindex.ai/blog/introducing-llamaextract-beta-structured-data-extraction-in-just-a-few-clicks) , [ Tweet ](https://x.com/llama_index/status/1816570219734945944) . \n  2. We have launched structured extraction for LLM-powered ETL, RAG, and agent pipelines, featuring full support for asynchronous operations and streaming. Simply integrate a Pydantic object with your LLM using as_structured_llm(\u2026), enabling chunk-level or document-level structured extraction and real-time JSON output visualization. [ Docs ](https://docs.llamaindex.ai/en/latest/use_cases/extraction/) , [ Tweet ](https://x.com/llama_index/status/1816195731826565586) . \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2343, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6c514cb-bfff-4e25-bbac-19d0f27c3310": {"__data__": {"id_": "f6c514cb-bfff-4e25-bbac-19d0f27c3310", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-07-30.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-30", "date": "Jul 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-30"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "62dafea5-0c5a-49ee-93da-d159adb5c9ce", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-07-30.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-30", "date": "Jul 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-30"}, "hash": "4eea636929a3835c68aaa7d29ee4e09c9a02df13e059fced72021b67e2c04a59", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea2a5822-ed60-4d58-9781-b44c2a990ac4", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-07-30.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-30", "date": "Jul 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-30"}, "hash": "a615d79559e952f942ce3726d6ed2958ab085e0b215a2119c95ba5a50ef5da22", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b67b4ca6-4385-4c60-8610-ca8bdcdf71ca", "node_type": "1", "metadata": {}, "hash": "e79c52e8e4a0448518121d94e18d41d7d83e66b4c86f172eccfe5513488f3722", "class_name": "RelatedNodeInfo"}}, "text": "3. ", "mimetype": "text/plain", "start_char_idx": 2343, "end_char_idx": 2346, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b67b4ca6-4385-4c60-8610-ca8bdcdf71ca": {"__data__": {"id_": "b67b4ca6-4385-4c60-8610-ca8bdcdf71ca", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-07-30.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-30", "date": "Jul 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-30"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "62dafea5-0c5a-49ee-93da-d159adb5c9ce", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-07-30.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-30", "date": "Jul 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-30"}, "hash": "4eea636929a3835c68aaa7d29ee4e09c9a02df13e059fced72021b67e2c04a59", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f6c514cb-bfff-4e25-bbac-19d0f27c3310", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-07-30.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-30", "date": "Jul 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-30"}, "hash": "8c11c1e67572244e7a3c81952aac0cf2f54af0c5cfadb244ff034bd944b1f999", "class_name": "RelatedNodeInfo"}}, "text": "We have integrated with Ollama for tool calling. This let\u2019s you build agents with local models like llama3.1. [ Docs ](https://docs.llamaindex.ai/en/latest/examples/llm/ollama/) , [ Tweet ](https://x.com/llama_index/status/1816637751175053610) . \n  4. We have day-0 support for building LLM applications with Mistral Large-2. [ Tweet ](https://x.com/llama_index/status/1816151296388522065) . \n\n##  **Guides:**\n\n  * [ Guide ](https://github.com/run-llama/llama_extract/blob/main/examples/rag/rag_metadata.ipynb) to Automated Structured Extraction for RAG to improve retrieval and synthesis in RAG pipelines with LlamaExtract by defining schemas and extracting metadata for richer context. \n\n##  **Tutorials:**\n\n  * [ Laurie Voss\u2019s ](https://x.com/seldo) [ tutorial ](https://www.youtube.com/watch?v=5Ahdg1DkPMc) on LlamaParse: simplified document parsing for generative AI applications. \n  * [ PromptEngineer\u2019s ](https://x.com/engineerrprompt) [ tutorial ](https://www.youtube.com/watch?v=Rg35oYuus-w) on Multi-modal RAG: Chat with Docs containing Images. \n  * [ kingzzm\u2019s ](https://x.com/kingzzm) [ tutorial ](https://medium.com/@zhaozhiming) on Evaluating RAG? LlamaIndex is All You Need. \n  * [ Avi Kumar Talaviya\u2019s ](https://x.com/avikumart_) [ tutorial ](https://medium.com/@avikumart_/building-natural-language-to-sql-applications-using-llamaindex-39280e90d52f) on Building Natural Language to SQL Applications using LlamaIndex. \n  * [ Pavan Belagatti\u2019s ](https://x.com/Pavan_Belagatti) [ tutorial ](https://www.youtube.com/watch?v=LBeFNSJbGFM) on Building Multi AI Agent Systems Using LlamaIndex and Crew AI. \n  * [ Fahd Mirza\u2019s ](https://www.youtube.com/watch?v=LUP7r3zko3o) [ tutorial ](https://www.youtube.com/watch?v=xnoEjczoqqE) on Implementing GraphRAG locally by LlamaIndex. \n\n##  **Webinars:**\n\n  * [ Webinar ](https://youtu.be/nzcBvba7mzI) with [ ColPali ](https://arxiv.org/abs/2407.01449) authors on Efficient Document Retrieval with Vision Language Models. \n\n", "mimetype": "text/plain", "start_char_idx": 2346, "end_char_idx": 4323, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59245926-175b-4550-9356-21d4787a5282": {"__data__": {"id_": "59245926-175b-4550-9356-21d4787a5282", "embedding": null, "metadata": {"filename": "dynamic-retrieval-with-llamacloud.md", "extension": ".md", "title": "Dynamic Retrieval with LlamaCloud", "date": "Jul 30, 2024", "url": "https://www.llamaindex.ai/blog/dynamic-retrieval-with-llamacloud"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6692003a-9368-461c-84d3-d7bd2c425d07", "node_type": "4", "metadata": {"filename": "dynamic-retrieval-with-llamacloud.md", "extension": ".md", "title": "Dynamic Retrieval with LlamaCloud", "date": "Jul 30, 2024", "url": "https://www.llamaindex.ai/blog/dynamic-retrieval-with-llamacloud"}, "hash": "105de37611c2a93fe217d88153f90f776c8a791f68deff29a4a5d90a8dafde32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cca9a457-90ff-4392-b8cb-1d835d8018fb", "node_type": "1", "metadata": {}, "hash": "554dfd28731794b90e224b899aa05a4d54055e3abe24ddf72d1d31bc425f2646", "class_name": "RelatedNodeInfo"}}, "text": "Building a robust question-answering assistant requires dynamically retrieving\nthe most relevant information for each query. Sometimes a short snippet\nprovides a complete answer, while other questions need the full context of an\nentire document (e.g. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 251, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cca9a457-90ff-4392-b8cb-1d835d8018fb": {"__data__": {"id_": "cca9a457-90ff-4392-b8cb-1d835d8018fb", "embedding": null, "metadata": {"filename": "dynamic-retrieval-with-llamacloud.md", "extension": ".md", "title": "Dynamic Retrieval with LlamaCloud", "date": "Jul 30, 2024", "url": "https://www.llamaindex.ai/blog/dynamic-retrieval-with-llamacloud"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6692003a-9368-461c-84d3-d7bd2c425d07", "node_type": "4", "metadata": {"filename": "dynamic-retrieval-with-llamacloud.md", "extension": ".md", "title": "Dynamic Retrieval with LlamaCloud", "date": "Jul 30, 2024", "url": "https://www.llamaindex.ai/blog/dynamic-retrieval-with-llamacloud"}, "hash": "105de37611c2a93fe217d88153f90f776c8a791f68deff29a4a5d90a8dafde32", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59245926-175b-4550-9356-21d4787a5282", "node_type": "1", "metadata": {"filename": "dynamic-retrieval-with-llamacloud.md", "extension": ".md", "title": "Dynamic Retrieval with LlamaCloud", "date": "Jul 30, 2024", "url": "https://www.llamaindex.ai/blog/dynamic-retrieval-with-llamacloud"}, "hash": "564f3e1065735df79e05f9b433af4a97be598558c0365dcfc897a30fe3d4da2a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f958558b-14a0-43a7-b64b-becb1a547737", "node_type": "1", "metadata": {}, "hash": "91510f2fb52c0397e86f9c2ba97d6f2074d48eca2f258af037fb24f3920a3845", "class_name": "RelatedNodeInfo"}}, "text": "\u201cGive me a summary of this book\u201d).\n\nFor questions that fall into the latter category, top-k vector search falls\nshort; rather than just retrieving the top-k chunks that match, you need to\nintentionally retrieve the entire document text and feed that into the context\nwindow of the LLM.\n\nTo address this, we\u2019re excited to release **file-level retrieval** for\nLlamaCloud, which is a separate retrieval API from our existing **chunk-level\nretrieval** capabilities. This retrieval interface then paves the way for us\nto build an agent that can dynamically route the question to the right\nretrieval interface depending on the properties of the question, which allows\nit to more robustly handle different user questions.\n\n##  File-Level Retrieval\n\nThis method retrieves the full content of a document, such as an entire SEC\n10K filing or an entire slide deck. There are two main ways to perform file-\nlevel retrieval:\n\n  * **By Metadata** : Selects files based on their metadata. \n  * **By Content** : Uses the document's content to determine the most relevant files. \n\nThe interface allows you to seamlessly toggle between retrieval modes, and\nadjust parameters like top-k results, and even filter by metadata when using\nfile-level retrieval.\n\n##  Building an Agent for Dynamic Retrieval\n\nTo demonstrate the power of dynamic retrieval, we've created [ a Jupyter\nnotebook ](https://github.com/run-llama/llamacloud-\ndemo/blob/main/examples/10k_apple_tesla/demo_file_retrieval.ipynb) that guides\nyou through building an agent capable of choosing between chunk-level and\nfile-level retrieval based on the question at hand. Let's walk through the key\nsteps:\n\n  1. ", "mimetype": "text/plain", "start_char_idx": 251, "end_char_idx": 1905, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f958558b-14a0-43a7-b64b-becb1a547737": {"__data__": {"id_": "f958558b-14a0-43a7-b64b-becb1a547737", "embedding": null, "metadata": {"filename": "dynamic-retrieval-with-llamacloud.md", "extension": ".md", "title": "Dynamic Retrieval with LlamaCloud", "date": "Jul 30, 2024", "url": "https://www.llamaindex.ai/blog/dynamic-retrieval-with-llamacloud"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6692003a-9368-461c-84d3-d7bd2c425d07", "node_type": "4", "metadata": {"filename": "dynamic-retrieval-with-llamacloud.md", "extension": ".md", "title": "Dynamic Retrieval with LlamaCloud", "date": "Jul 30, 2024", "url": "https://www.llamaindex.ai/blog/dynamic-retrieval-with-llamacloud"}, "hash": "105de37611c2a93fe217d88153f90f776c8a791f68deff29a4a5d90a8dafde32", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cca9a457-90ff-4392-b8cb-1d835d8018fb", "node_type": "1", "metadata": {"filename": "dynamic-retrieval-with-llamacloud.md", "extension": ".md", "title": "Dynamic Retrieval with LlamaCloud", "date": "Jul 30, 2024", "url": "https://www.llamaindex.ai/blog/dynamic-retrieval-with-llamacloud"}, "hash": "44a9f1e8a25260d5ad8a07f1cb0f29d52db042a5dd949dd45771e134b4ae0249", "class_name": "RelatedNodeInfo"}}, "text": "**Defining Retrievers** : We set up both chunk-level and file-level retrievers using LlamaCloud's ` as_retriever ` method. These can be converted into query engines that can synthesize an answer over retrieved context given a question. \n\n    \n    \n    # File-level retriever\n    doc_retriever = index.as_retriever(\n        retrieval_mode=\"files_via_content\",\n        files_top_k=1\n    )\n    \n    # Chunk-level retriever\n    chunk_retriever = index.as_retriever(\n        retrieval_mode=\"chunks\",\n        rerank_top_n=5\n    )\n\n  1. **Building the Agent** : The heart of the notebook is the agent that decides which query tool to use. It analyzes the input question and chooses the appropriate retrieval method based on the tool description. In this section, we keep it simple - tell the agent to choose file-level retrieval if the question is more for \u201chigh-level summarization\u201d, otherwise do chunk-level retrieval. \n  2. **Testing the Agent** : Even with these basic tool prompts, our agent can already intelligently decide to perform file-level retrieval for more \u201csummarization-esque\u201d questions. \n\n    \n    \n    # uses chunk-level retrieval twice\n    response = agent.chat(\"Tell me the revenue for Apple and Tesla in 2021?\")\n    print(response)\n    \n    # uses file-level retrieval twice\n    response = agent.chat(\"How was Tesla doing generally in 2021 and 2022?\")\n    print(response)\n\n  1. **[Advanced] File-level Retrieval** : The notebook also demonstrates how to set up more sophisticated file-level retrieval by inferring metadata filters and injecting few-shot examples of existing files, letting the agent better make a decision of file-level vs. chunk-level based on the actual files that have been uploaded instead of just a fixed prompt. \n\n##  Build more Robust QA Interfaces Starting Today\n\nLlamaCloud's dynamic retrieval capabilities represent a significant step\nforward in building more intelligent and context-aware LLM applications. By\noffering the flexibility to choose between chunk-level and file-level\nretrieval, developers can create more nuanced and accurate systems that adapt\nto the specific needs of each query.\n\nWe encourage you to explore these new features in the LlamaCloud UI and try\nout the notebook for yourself. As always, we're excited to see what you'll\nbuild with these enhanced retrieval capabilities!\n\n##  Want to see what LlamaCloud can do for you?\n\nCome sign up on our [ waitlist ](https://bit.ly/llamacloud) for access. If\nyou\u2019re interested in chatting about enterprise plans, [ get in touch.\n](https://www.llamaindex.ai/contact)\n\nIf you\u2019ve gotten access to LlamaCloud, check out our [ rich repository of\ndemonstrations and examples ](https://github.com/run-llama/llamacloud-demo) on\nhow to build different LLM application use cases.\n\n", "mimetype": "text/plain", "start_char_idx": 1905, "end_char_idx": 4681, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "043f5478-70af-4c13-94dd-f8f170d7f626": {"__data__": {"id_": "043f5478-70af-4c13-94dd-f8f170d7f626", "embedding": null, "metadata": {"filename": "introducing-llamaextract-beta-structured-data-extraction-in-just-a-few-clicks.md", "extension": ".md", "title": "Introducing LlamaExtract Beta: structured data extraction in just a few clicks", "date": "Jul 25, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamaextract-beta-structured-data-extraction-in-just-a-few-clicks"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81f29a86-fa6c-4ffa-b1e9-fdfaab10fba1", "node_type": "4", "metadata": {"filename": "introducing-llamaextract-beta-structured-data-extraction-in-just-a-few-clicks.md", "extension": ".md", "title": "Introducing LlamaExtract Beta: structured data extraction in just a few clicks", "date": "Jul 25, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamaextract-beta-structured-data-extraction-in-just-a-few-clicks"}, "hash": "10f4c4137dcdfd5260ccbba6c4a3b91d0148a1bb34a1b42ca57c4fb14ba14533", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee0e7d22-a211-4531-94e2-055e5692d80f", "node_type": "1", "metadata": {}, "hash": "5351614b91457ae71f984a73ffb507e2dfae1dd8663dc1e94391f427e159f581", "class_name": "RelatedNodeInfo"}}, "text": "Structured extraction from unstructured data is both a core use case for LLMs\nin its own right, as well as a key ingredient in data processing for retrieval\nand RAG use cases. Today we\u2019re excited to announce a beta release of\nLlamaExtract, which is a managed service that lets you perform structured\nextraction from unstructured documents.\n\nIt does the following:\n\n  1. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 370, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee0e7d22-a211-4531-94e2-055e5692d80f": {"__data__": {"id_": "ee0e7d22-a211-4531-94e2-055e5692d80f", "embedding": null, "metadata": {"filename": "introducing-llamaextract-beta-structured-data-extraction-in-just-a-few-clicks.md", "extension": ".md", "title": "Introducing LlamaExtract Beta: structured data extraction in just a few clicks", "date": "Jul 25, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamaextract-beta-structured-data-extraction-in-just-a-few-clicks"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81f29a86-fa6c-4ffa-b1e9-fdfaab10fba1", "node_type": "4", "metadata": {"filename": "introducing-llamaextract-beta-structured-data-extraction-in-just-a-few-clicks.md", "extension": ".md", "title": "Introducing LlamaExtract Beta: structured data extraction in just a few clicks", "date": "Jul 25, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamaextract-beta-structured-data-extraction-in-just-a-few-clicks"}, "hash": "10f4c4137dcdfd5260ccbba6c4a3b91d0148a1bb34a1b42ca57c4fb14ba14533", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "043f5478-70af-4c13-94dd-f8f170d7f626", "node_type": "1", "metadata": {"filename": "introducing-llamaextract-beta-structured-data-extraction-in-just-a-few-clicks.md", "extension": ".md", "title": "Introducing LlamaExtract Beta: structured data extraction in just a few clicks", "date": "Jul 25, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamaextract-beta-structured-data-extraction-in-just-a-few-clicks"}, "hash": "708a2f6a8ac7b9bece57b45a42a7f88bce2941e60db421bf7bd5befc9e9565f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "434b8bbf-a0c8-404a-bce0-4c88cbddd802", "node_type": "1", "metadata": {}, "hash": "f8e14b7873c825f81aa8a75fbf4928189d7b02620aa93417fca524dc126a9582", "class_name": "RelatedNodeInfo"}}, "text": "Infers a schema from an existing candidate set of documents. You can choose to edit this schema later. \n  2. ", "mimetype": "text/plain", "start_char_idx": 370, "end_char_idx": 479, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "434b8bbf-a0c8-404a-bce0-4c88cbddd802": {"__data__": {"id_": "434b8bbf-a0c8-404a-bce0-4c88cbddd802", "embedding": null, "metadata": {"filename": "introducing-llamaextract-beta-structured-data-extraction-in-just-a-few-clicks.md", "extension": ".md", "title": "Introducing LlamaExtract Beta: structured data extraction in just a few clicks", "date": "Jul 25, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamaextract-beta-structured-data-extraction-in-just-a-few-clicks"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81f29a86-fa6c-4ffa-b1e9-fdfaab10fba1", "node_type": "4", "metadata": {"filename": "introducing-llamaextract-beta-structured-data-extraction-in-just-a-few-clicks.md", "extension": ".md", "title": "Introducing LlamaExtract Beta: structured data extraction in just a few clicks", "date": "Jul 25, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamaextract-beta-structured-data-extraction-in-just-a-few-clicks"}, "hash": "10f4c4137dcdfd5260ccbba6c4a3b91d0148a1bb34a1b42ca57c4fb14ba14533", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee0e7d22-a211-4531-94e2-055e5692d80f", "node_type": "1", "metadata": {"filename": "introducing-llamaextract-beta-structured-data-extraction-in-just-a-few-clicks.md", "extension": ".md", "title": "Introducing LlamaExtract Beta: structured data extraction in just a few clicks", "date": "Jul 25, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamaextract-beta-structured-data-extraction-in-just-a-few-clicks"}, "hash": "087d05e1ea280f8ef4a8ee689826e10ed486a5b5238bd525c6d8f3921eec9a3c", "class_name": "RelatedNodeInfo"}}, "text": "Extracts values from a set of documents according to a specified schema (whether inferred from the previous step, specified by a human, or both). \n\nLlamaExtract is available to LlamaCloud users through both a UI and API.\nSchema inference currently comes with a cap of 5 files, with a max of 10 pages\nper file. Given an existing schema, schema extraction happens on a per-\ndocument level.\n\n> LlamaExtract is currently in _beta,_ meaning it is an experimental feature\n> that we\u2019re working hard to improve and make more generally scalable and\n> usable. Please report any issues to our [ Github ](https://github.com/run-\n> llama/llama_extract/issues/new) !\n\n##  Metadata Extraction is a Key Part of the LLM ETL Stack\n\nA new data ETL stack is needed for LLM applications. This data loading,\ntransformation, and indexing layer is crucial for downstream RAG and agent use\ncases over unstructured data.\n\nWe built LlamaParse and LlamaCloud to serve these ETL needs and power\nthousands of production pipelines over complex documents in production.\nThrough working with our users and customers, we realized that besides chunk-\nlevel embeddings, automated metadata extraction is an important part of the\ntransformation story (the \u201cT\u201d in ETL); it is a core necessary ingredient for\nincreasing transparency and control over broad swaths of unstructured data.\n\nThat led us to build the initial version of LlamaExtract - designed to\nautomate data transformation for your unstructured data.\n\n##  A Walkthrough\n\nLlamaExtract is an API, it also has a [ python client\n](https://github.com/run-llama/llama_extract) and of course a web UI within [\nLlamaCloud ](https://cloud.llamaindex.ai) .\n\n**Use the UI to Prototype**\n\nThe LlamaExtract UI allows you to prototype an extraction job. After clicking\nthe \u201cExtraction (beta)\u201d tab in the left-hand side, you can click \u201cCreate a new\nschema\u201d in order to define a new extraction job, which will take you to the\nschema creation screen:\n\nDrag and drop your files into the UI. Click \u201cNext\u201d to kick off schema\ninference - the inferred JSON schema will be shown in the \u201cSchema\u201d section\nbelow:\n\nThis follows the Pydantic JSON schema format; after inference you have full\nfreedom to customize and modify the inferred schema. The schema visualization\nwill reflect your changes.\n\nOnce you\u2019re happy with your schema, you can kick off extraction, which will\nreturn the final JSON object with extracted keys and values in adherence with\nthe schema.\n\n> There are some core improvements we are making to the UI, like decoupling\n> schema inference and extraction, allowing users to pre-define a schema, etc.\n> For more flexibility check out the API below.\n\n**Use the API to Create Extraction Workflows**\n\nThe API allows users to more flexibly integrate schema inference and\nextraction. To access the API via our client package, follow these steps:\n\n    \n    \n    pip install llama-extract\n\nYou can choose to either _infer_ a schema or specify your own schema (or infer\nfirst, and then modify it programmatically after to your liking). If you wish\nto use LlamaExtract\u2019s schema inference capability, do:\n\n    \n    \n    from llama_extract import LlamaExtract\n    \n    extractor = LlamaExtract()\n    \n    extraction_schema = extractor.infer_schema(\"Test Schema\", [\"./file1.pdf\",\"./file2.pdf\"])\n\nIf you prefer you can specify the schema directly rather than inferring it.\nThe easiest way is to define a Pydantic object and convert that to a JSON\nschema:\n\n    \n    \n    from pydantic import BaseModel, Field\n    \n    class ResumeMetadata(BaseModel):\n        \"\"\"Resume metadata.\"\"\"\n    \n        years_of_experience: int = Field(..., description=\"Number of years of work experience.\")\n        highest_degree: str = Field(..., description=\"Highest degree earned (options: High School, Bachelor's, Master's, Doctoral, Professional\")\n        professional_summary: str = Field(..., description=\"A general summary of the candidate's experience\")\n        \n    extraction_schema = extractor.create_schema(\"Test Schema\", ResumeMetadata)\n\nHowever you get your schema, you can now perform extraction:\n\n    \n    \n    extractions = extractor.extract(extraction_schema.id, [\"./file3.pdf\",\"./file4.pdf\"])\n\nYou can see the extracted data:\n\n    \n    \n    print(extractions[0].data)\n\n##  Some Example Use Cases\n\nLlamaExtract uses LlamaParse as its underlying parser and is able to handle\ncomplex document types (note: native multimodal extraction coming soon!)\nThrough our initial explorations, here are some initial datasets where\nLlamaExtract is valuable:\n\n  * Resumes: extract structured annotations like school, work experiences, YOE from a candidate\u2019s profile \n  * Receipts and Invoices: extract line items, total price, and other figures. \n  * Product pages: structure and categorize your products according to a user-defined schema. \n\nCheck out the [ examples section ](https://github.com/run-\nllama/llama_extract/tree/main/examples) of the LlamaExtract client repo to see\nmore possibilities.\n\n##  We\u2019re Rapidly Improving our Extraction Capabilities\n\nBy opening this up to the community, we\u2019re excited to rapidly improve the UX,\nscalability, and performance. Here\u2019s an feature roadmap:\n\n  * Multimodal extraction \n  * Decouple schema creation from extraction in the UI \n  * Support for human-in-the-loop schema creation as a first class UX \n  * Make automated schema inference and extraction more robust over longer documents (e.g. a 10K filing) \n\n##  Try it Out\n\nLlamaExtract is available to _all_ users. You don\u2019t need to be let off a\nwaitlist! Just create an account at [ cloud.llamaindex.ai\n](https://cloud.llamaindex.ai/) to use the UI; you can also check out our [\npython client ](https://github.com/run-llama/llama_extract) . To get started,\ntry one of these notebooks:\n\n  * [ LlamaExtract quick start ](https://github.com/run-llama/llama_extract/blob/main/examples/demo_basic.ipynb)\n  * [ LlamaExtract with Pydantic ](https://github.com/run-llama/llama_extract/blob/main/examples/demo_pydantic_model.ipynb)\n  * [ LlamaExtract + RAG ](https://github.com/run-llama/llama_extract/blob/main/examples/rag/rag_metadata.ipynb)\n\nIf you\u2019re interested in the broader LlamaCloud functionality, [ join our\nwaitlist\n](https://docs.google.com/forms/d/e/1FAIpQLSdehUJJB4NIYfrPIKoFdF4j8kyfnLhMSH_qYJI_WGQbDWD25A/viewform)\n.\n\n", "mimetype": "text/plain", "start_char_idx": 479, "end_char_idx": 6785, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8ec7259-830c-4ce7-beb7-faeeaea47e5e": {"__data__": {"id_": "d8ec7259-830c-4ce7-beb7-faeeaea47e5e", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-07-23.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-23", "date": "Jul 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-23"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "888d9985-99d0-4ab5-9412-e40a25f14143", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-07-23.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-23", "date": "Jul 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-23"}, "hash": "29d84de7c05fe169480c3b747d510d4e1c13e587f20227f66c8bcceb2b5d0bae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5793b9b4-e936-4c8c-a986-4dcd5d8ef53d", "node_type": "1", "metadata": {}, "hash": "3267fd20ae7d9540bb29f0c9064eca24882a54ac1691502f6ebe90b2612e6b9b", "class_name": "RelatedNodeInfo"}}, "text": "Hello, Llama Followers!\n\nWelcome to this week\u2019s edition of the LlamaIndex newsletter! We\u2019re thrilled to\nshare some exciting updates about our products, including LlamaCloud,\nLlamaParse, and LlamaAgents. You\u2019ll also find success stories with LlamaCloud,\nextensive guides, in-depth tutorials, and information about upcoming\nhackathons.\n\n##  **The highlights:**\n\n  1. **LlamaCloud Updates:** New features including LlamaCloud Chat, enhanced Teams collaboration, and expanded integrations with Notion, Slack, Jira, and SharePoint. [ Blogpost ](https://www.llamaindex.ai/blog/the-latest-updates-to-llamacloud) , [ Tweet ](https://x.com/llama_index/status/1814363518726222119) . \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 676, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5793b9b4-e936-4c8c-a986-4dcd5d8ef53d": {"__data__": {"id_": "5793b9b4-e936-4c8c-a986-4dcd5d8ef53d", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-07-23.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-23", "date": "Jul 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-23"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "888d9985-99d0-4ab5-9412-e40a25f14143", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-07-23.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-23", "date": "Jul 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-23"}, "hash": "29d84de7c05fe169480c3b747d510d4e1c13e587f20227f66c8bcceb2b5d0bae", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8ec7259-830c-4ce7-beb7-faeeaea47e5e", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-07-23.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-23", "date": "Jul 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-23"}, "hash": "42c7e38a191560ab848316284502630f388806aa51a79a0391be2924741c4fea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d4bf7ae-fcea-47d7-b709-bd3bec637ca5", "node_type": "1", "metadata": {}, "hash": "6d5d4c373b49a85ee12189dafa72ff933b09cb703da16a7936996a0a231d3818", "class_name": "RelatedNodeInfo"}}, "text": "2. **Scaleport AI\u2019s Accelerated Development with LlamaCloud:** Scaleport AI boosts development speed and sales with LlamaCloud and LlamaIndex, improving data handling and OCR accuracy across multiple industries. [ Blogpost ](https://www.llamaindex.ai/blog/case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud) . \n  3. **Claude Sonnet-3.5 Integration with LlamaParse:** Integration of Claude Sonnet-3.5 with LlamaParse improves chart understanding and data extraction capabilities. [ Notebook ](https://github.com/run-llama/llama_parse/blob/main/examples/multimodal/claude_parse.ipynb) , [ Tweet ](https://x.com/llama_index/status/1813249175817232782) . \n  4. **Multimodal RAG Cookbook:** A new guide for processing text, diagrams, charts, and tables in slide decks using LlamaParse, LlamaIndex, and GPT-4o. [ Notebook ](https://github.com/run-llama/llama_parse/blob/main/examples/multimodal/multimodal_rag_slide_deck.ipynb) , [ Tweet ](https://x.com/llama_index/status/1812963306032013586) . \n  5. **Human in the Loop with LlamaAgents:** Implementation includes HumanService for math queries and agent handling for other inquiries, managed via Gradio app and RabbitMQ. [ Code ](https://github.com/run-llama/llama-agents/tree/main/examples/human-in-the-loop) . \n\n##  **Feature Releases and Enhancements:**\n\n  1. We have released new features on LlamaCloud like LlamaCloud Chat for instant conversational data access, enhanced Teams functionality for collaboration, and expanded data integration with connectors for Notion, Slack, Jira, and improved SharePoint support. [ Blogpost ](https://www.llamaindex.ai/blog/the-latest-updates-to-llamacloud) , [ Tweet ](https://x.com/llama_index/status/1814363518726222119) . \n  ", "mimetype": "text/plain", "start_char_idx": 676, "end_char_idx": 2427, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d4bf7ae-fcea-47d7-b709-bd3bec637ca5": {"__data__": {"id_": "0d4bf7ae-fcea-47d7-b709-bd3bec637ca5", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-07-23.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-23", "date": "Jul 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-23"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "888d9985-99d0-4ab5-9412-e40a25f14143", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-07-23.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-23", "date": "Jul 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-23"}, "hash": "29d84de7c05fe169480c3b747d510d4e1c13e587f20227f66c8bcceb2b5d0bae", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5793b9b4-e936-4c8c-a986-4dcd5d8ef53d", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-07-23.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-23", "date": "Jul 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-23"}, "hash": "1350119c1d77260eceb347e84d5c365e84f2807015765d081dc5c776dc79fd39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3260b5a2-dace-438e-8877-ec7f3fc5c445", "node_type": "1", "metadata": {}, "hash": "46d22fcc0c0bc78a73a25e5960dabafcae3a43166256d35e77eedb6b755c7966", "class_name": "RelatedNodeInfo"}}, "text": "2. We integrated Claude Sonnet-3.5 with LlamaParse to enhance document parsing capabilities, offering advanced chart understanding and structured data extraction with improved validation and scalability. [ Notebook ](https://github.com/run-llama/llama_parse/blob/main/examples/multimodal/claude_parse.ipynb) , [ Tweet ](https://x.com/llama_index/status/1813249175817232782) . \n  3. We have released a cookbook on Multimodal RAG for processing slide decks rich in text, diagrams, charts, and tables using LlamaParse, LlamaIndex, and GPT-4o, blending text and image data for comprehensive document analysis. [ Notebook ](https://github.com/run-llama/llama_parse/blob/main/examples/multimodal/multimodal_rag_slide_deck.ipynb) , [ Tweet ](https://x.com/llama_index/status/1812963306032013586) . \n  4. We have implemented Human in the Loop with LlamaAgents in our new example that integrates a HumanService object for handling math queries and an agent for other queries, all managed through a Gradio app and RabbitMQ messaging. [ Code ](https://github.com/run-llama/llama-agents/tree/main/examples/human-in-the-loop) . \n  5. We have made huge improvements to markdown-based table reconstruction in LlamaParse, enabling the parsing of very complex tables while ensuring that rows and columns remain well-aligned. [ Notebook ](https://github.com/run-llama/llama_parse/blob/main/examples/demo_advanced.ipynb) , [ Tweet ](https://x.com/llama_index/status/1813355957491273936) . \n  ", "mimetype": "text/plain", "start_char_idx": 2427, "end_char_idx": 3900, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3260b5a2-dace-438e-8877-ec7f3fc5c445": {"__data__": {"id_": "3260b5a2-dace-438e-8877-ec7f3fc5c445", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-07-23.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-23", "date": "Jul 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-23"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "888d9985-99d0-4ab5-9412-e40a25f14143", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-07-23.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-23", "date": "Jul 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-23"}, "hash": "29d84de7c05fe169480c3b747d510d4e1c13e587f20227f66c8bcceb2b5d0bae", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d4bf7ae-fcea-47d7-b709-bd3bec637ca5", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-07-23.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-23", "date": "Jul 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-23"}, "hash": "4ca421bc13e241805e4ce37b45c15a8727b6fc97ea7cf3d47b3f023f823d3e2b", "class_name": "RelatedNodeInfo"}}, "text": "6. [ Marcus Schiesser ](https://x.com/MarcusSchiesser) has enhanced [ RAGapp ](https://github.com/ragapp/ragapp/tree/main) , a docker-deployable, enterprise-ready RAG application, with MistralAI and Groq support for rapid inference, and a Cohere reranker to boost result relevance. \n\n##  **Use-cases:**\n\n  * Scaleport AI accelerated development and improved sales with LlamaCloud and LlamaIndex to streamline AI development, reducing prototype timelines, simplifying data ingestion, and improving OCR accuracy across key industries like Legal, eCommerce, Real Estate, and Finance. [ Blogpost ](https://www.llamaindex.ai/blog/case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud) . \n  * Merlinn, an open-source LLM-powered on-call copilot community project features a Slack assistant that manages production incidents automatically, integrating with tools like Datadog, PagerDuty, GitHub, Notion, and Confluence using core components from LlamaIndex in both Python and TypeScript. [ Repository ](https://github.com/merlinn-co/merlinn) . \n\n##  **Guides:**\n\n  * Guide to Building a Multi-Agent Concierge System to demonstrate how to create a complex tree system with specialized sub-agents and meta-agents for efficient customer interaction handling. [ Blog ](https://www.llamaindex.ai/blog/building-a-multi-agent-concierge-system) , [ Tweet ](https://x.com/llama_index/status/1813618002405069173) . \n  * [ Guide ](https://www.llamaindex.ai/blog/improving-vector-search-reranking-with-postgresml-and-llamaindex) on Improving Vector Search - Reranking with PostgresML and LlamaIndex \n\n##  **Tutorials:**\n\n  * [ Laurie\u2019s ](https://x.com/seldo) [ video tutorial ](https://www.youtube.com/watch?v=lqRTCxsKBwc) on Building and deploying Multi-Agent RAG systems with LlamaIndex. \n  * [ Pavan Kumar ](https://x.com/pavan_mantha1) [ tutorial ](https://towardsdev.com/conversational-media-platform-chatting-with-podcasts-and-videos-using-openai-qdrant-and-gemma2-4208ab7e90ee) on Chatting with Podcasts and Videos using OpenAI, Qdrant and Gemma2. \n  * Lakshmi Narayana [ tutorial ](https://blog.stackademic.com/agentic-rag-enhancing-ai-systems-with-llamaindex-8c54bba41171) on Agentic RAG with LlamaIndex to enhance generative AI applications with intelligent routing, multi-step reasoning, and adaptive learning. \n\n##  **Talks, Webinars and Podcasts:**\n\n  * Jerry Liu\u2019s [ talk ](https://www.youtube.com/watch?v=zeAyuLc_f3Q) on advanced LlamaIndex capabilities, introducing \u2018Llama Agents\u2019 for deploying microservice-based agents that communicate via a unified API. \n  * Jerry Liu discusses high-quality data, prompt engineering, long context windows, and RAG on the [ StackOverflowPodcast ](https://stackoverflow.blog/2024/07/16/the-framework-helping-devs-build-llm-apps/) with Jerry Chen. \n  * [ Webinar ](https://www.youtube.com/watch?v=44h94AJgQoM) with Yixin Hu (VU Amsterdam) and Thomas Hulard (McDermott) on Evaluating RAG with LlamaIndex. \n  * [ Webinar ](https://www.youtube.com/watch?v=V_-WNJgTvgg) with the cofounders of Deasie (Reece, Leonard, Mikko) on improving RAG with advanced parsing and metadata. \n\n##  **Hackathons:**\n\n  * We\u2019re sponsoring a month-long hackathon with PingCAP for their #TiDB database! Join us, AWS Cloud, Anyscale Compute, Dify AI, Jina AI, Lepton AI, and NPI AI to compete for over $30,000 in prizes, including $12,000 in cash for the first-place winner. [ Sign up here ](https://tidbhackathon2024.devpost.com/) . \n\n", "mimetype": "text/plain", "start_char_idx": 3900, "end_char_idx": 7375, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2798414e-f31e-44a2-86de-03188775da06": {"__data__": {"id_": "2798414e-f31e-44a2-86de-03188775da06", "embedding": null, "metadata": {"filename": "improving-vector-search-reranking-with-postgresml-and-llamaindex.md", "extension": ".md", "title": "Improving Vector Search - Reranking with PostgresML and LlamaIndex", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/improving-vector-search-reranking-with-postgresml-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "033242bb-7f8d-4a19-a628-ab5236c05991", "node_type": "4", "metadata": {"filename": "improving-vector-search-reranking-with-postgresml-and-llamaindex.md", "extension": ".md", "title": "Improving Vector Search - Reranking with PostgresML and LlamaIndex", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/improving-vector-search-reranking-with-postgresml-and-llamaindex"}, "hash": "b051274d3600a91a43173fd3547b2410a90c87b4935e88606bb1ba784a1161e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "947cb5e3-20f3-42c5-b5fa-4283600e5ba9", "node_type": "1", "metadata": {}, "hash": "436b3a0eab40e3da80d334ee167451f68521c10a127e41a5ae11aa6a9afde904", "class_name": "RelatedNodeInfo"}}, "text": "##  Search and Reranking: Improving Result Relevance\n\nSearch systems typically employ two main methods: keyword and semantic.\nKeyword search matches exact query terms to indexed database content, while\nsemantic search uses NLP and machine learning to understand query context and\nintent. Many effective systems combine both approaches for optimal results.\n\nAfter initial retrieval, reranking can further improve result relevance.\nTraditional reranking relies on historical user interaction data, but this\napproach struggles with new content and requires substantial data to train\neffectively. An advanced alternative is using cross-encoders, which directly\ncompare query-result pairs for similarity.\n\nCross-encoders directly compare two pieces of text and compute a similarity\nscore. Unlike traditional semantic search methods, we cannot precompute\nembeddings for cross-encoders and reuse them later. Instead, we must run the\ncross-encoder for every pair of texts we want to compare, making this method\ncomputationally expensive and impractical for large-scale searches. However,\nit is highly effective for reranking a subset of our dataset because it excels\nat evaluating new, unseen data without the need for extensive user interaction\ndata for fine-tuning.\n\nCross-encoders complement and enhance traditional reranking systems by\naddressing their limitations in deep text analysis, particularly for novel or\nhighly specific content. They do not rely on large datasets of user\ninteractions for training (though such data can still be beneficial) and are\nadept at handling new and previously unseen data. This makes cross-encoders an\nexcellent choice for enhancing the relevance of search results in a reranking\ncontext.\n\n##  Implementing Reranking\n\nWe are going to implement a simple reranking example using LlamaIndex and the\nPostgresML managed index. For more info on the PostgresML managed index. Check\nout our announcement with LlamaIndex: [ Simplify your RAG application\narchitecture with LlamaIndex + PostgresML\n](https://www.llamaindex.ai/blog/simplify-your-rag-application-architecture-\nwith-llamaindex-postgresml) .\n\nInstall the required dependencies to get started:\n\n    \n    \n    pip install llama_index llama-index-indices-managed-postgresml\n\nWe will be using the Paul Graham dataset which can be downloaded with curl:\n\n    \n    \n    mkdir data\n    \n    curl -o data/paul_graham_essay.txt https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n\nThe PostgresML Managed Index will handle storing, splitting, embedding, and\nquerying our documents. All we need is a database connection string. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2667, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "947cb5e3-20f3-42c5-b5fa-4283600e5ba9": {"__data__": {"id_": "947cb5e3-20f3-42c5-b5fa-4283600e5ba9", "embedding": null, "metadata": {"filename": "improving-vector-search-reranking-with-postgresml-and-llamaindex.md", "extension": ".md", "title": "Improving Vector Search - Reranking with PostgresML and LlamaIndex", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/improving-vector-search-reranking-with-postgresml-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "033242bb-7f8d-4a19-a628-ab5236c05991", "node_type": "4", "metadata": {"filename": "improving-vector-search-reranking-with-postgresml-and-llamaindex.md", "extension": ".md", "title": "Improving Vector Search - Reranking with PostgresML and LlamaIndex", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/improving-vector-search-reranking-with-postgresml-and-llamaindex"}, "hash": "b051274d3600a91a43173fd3547b2410a90c87b4935e88606bb1ba784a1161e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2798414e-f31e-44a2-86de-03188775da06", "node_type": "1", "metadata": {"filename": "improving-vector-search-reranking-with-postgresml-and-llamaindex.md", "extension": ".md", "title": "Improving Vector Search - Reranking with PostgresML and LlamaIndex", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/improving-vector-search-reranking-with-postgresml-and-llamaindex"}, "hash": "bb08b0a83119819de51c3baeedc885d77fc71ccdcd160b54e5cecbd49edbc48c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b01e2db8-c6b6-40fa-96c6-aff5385ab659", "node_type": "1", "metadata": {}, "hash": "5323d552478a07d6230298018570c9d48bdba69a1582f685a8b66b860b94f6e2", "class_name": "RelatedNodeInfo"}}, "text": "If you\nhaven\u2019t already, [ create your PostgresML account\n](https://postgresml.org/signup) . You\u2019ll get $100 in free credits when you\ncomplete your profile.\n\nSet the PGML_DATABASE_URL environment variable:\n\n    \n    \n    export PGML_DATABASE_URL=\"{YOUR_CONNCECTION_STRING}\"\n\nLet\u2019s create our index:\n\n    \n    \n    from llama_index.core.readers import SimpleDirectoryReader\n    from llama_index.indices.managed.postgresml import PostgresMLIndex\n    \n    \n    documents = SimpleDirectoryReader(\"data\").load_data()\n    index = PostgresMLIndex.from_documents(\n        documents, collection_name=\"llama-index-rerank-example\"\n    )\n\nNote the collection_name is used to uniquely identify the index you are\nworking with.\n\nHere we are using the SimpleDirectoryReader to load in the documents and then\nwe construct the PostgresMLIndex from those documents.\n\nThis workflow does not require document preprocessing. Instead, the documents\nare sent directly to PostgresML where they are stored, split, and embedded per\nthe pipeline specification. This is a unique quality of using the PostgresML\nmanaged index.\n\nNow let\u2019s search! We can perform semantic search and get the top 2 results by\ncreating a retriever from our index.\n\n    \n    \n    retriever = index.as_retriever(limit=2)\n    docs = retriever.retrieve(\"What did the author do as a child?\")\n    for doc in docs:\n        print(\"---------\")\n        print(f\"Id: {doc.id_}\")\n        print(f\"Score: {doc.score}\")\n        print(f\"Text: {doc.text}\")\n    \n\nDoing this we get:\n\n    \n    \n    ---------\n    \n    Id: de01b7e1-95f8-4aa0-b4ec-45ef64816e0e\n    \n    Score: 0.7793415653313153\n    \n    Text: Wow, I thought, there's an audience. If I write something and put it on the web, anyone can read it. ", "mimetype": "text/plain", "start_char_idx": 2667, "end_char_idx": 4405, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b01e2db8-c6b6-40fa-96c6-aff5385ab659": {"__data__": {"id_": "b01e2db8-c6b6-40fa-96c6-aff5385ab659", "embedding": null, "metadata": {"filename": "improving-vector-search-reranking-with-postgresml-and-llamaindex.md", "extension": ".md", "title": "Improving Vector Search - Reranking with PostgresML and LlamaIndex", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/improving-vector-search-reranking-with-postgresml-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "033242bb-7f8d-4a19-a628-ab5236c05991", "node_type": "4", "metadata": {"filename": "improving-vector-search-reranking-with-postgresml-and-llamaindex.md", "extension": ".md", "title": "Improving Vector Search - Reranking with PostgresML and LlamaIndex", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/improving-vector-search-reranking-with-postgresml-and-llamaindex"}, "hash": "b051274d3600a91a43173fd3547b2410a90c87b4935e88606bb1ba784a1161e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "947cb5e3-20f3-42c5-b5fa-4283600e5ba9", "node_type": "1", "metadata": {"filename": "improving-vector-search-reranking-with-postgresml-and-llamaindex.md", "extension": ".md", "title": "Improving Vector Search - Reranking with PostgresML and LlamaIndex", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/improving-vector-search-reranking-with-postgresml-and-llamaindex"}, "hash": "8147e0df268bca1d4a0f2e25f523ec119ee6e64dfd361082383247273c008810", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "20c6f958-2273-459f-98e9-59241e26ca72", "node_type": "1", "metadata": {}, "hash": "05fb1d018799564189ccd67f8fae75d0057d62766f4f97d71652a69b473546cd", "class_name": "RelatedNodeInfo"}}, "text": "That may seem obvious now, but it was surprising then. In the print era there was a narrow channel to readers, guarded by fierce monsters known as editors. The only way to get an audience for anything you wrote was to get it published as a book, or in a newspaper or magazine. Now anyone could publish anything.\n    \n    \n    \n    This had been possible in principle since 1993, but not many people had realized it yet. I had been intimately involved with building the infrastructure of the web for most of that time, and a writer as well, and it had taken me 8 years to realize it. Even then it took me several years to understand the implications. It meant there would be a whole new generation of essays. [11]\n    \n    \n    \n    In the print era, the channel for publishing essays had been vanishingly small. Except for a few officially anointed thinkers who went to the right parties in New York, the only people allowed to publish essays were specialists writing about their specialties. There were so many essays that had never been written, because there had been no way to publish them. Now they could be, and I was going to write them. [12]\n    \n    \n    \n    I've worked on several different things, but to the extent there was a turning point where I figured out what to work on, it was when I started publishing essays online. ", "mimetype": "text/plain", "start_char_idx": 4405, "end_char_idx": 5744, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "20c6f958-2273-459f-98e9-59241e26ca72": {"__data__": {"id_": "20c6f958-2273-459f-98e9-59241e26ca72", "embedding": null, "metadata": {"filename": "improving-vector-search-reranking-with-postgresml-and-llamaindex.md", "extension": ".md", "title": "Improving Vector Search - Reranking with PostgresML and LlamaIndex", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/improving-vector-search-reranking-with-postgresml-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "033242bb-7f8d-4a19-a628-ab5236c05991", "node_type": "4", "metadata": {"filename": "improving-vector-search-reranking-with-postgresml-and-llamaindex.md", "extension": ".md", "title": "Improving Vector Search - Reranking with PostgresML and LlamaIndex", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/improving-vector-search-reranking-with-postgresml-and-llamaindex"}, "hash": "b051274d3600a91a43173fd3547b2410a90c87b4935e88606bb1ba784a1161e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b01e2db8-c6b6-40fa-96c6-aff5385ab659", "node_type": "1", "metadata": {"filename": "improving-vector-search-reranking-with-postgresml-and-llamaindex.md", "extension": ".md", "title": "Improving Vector Search - Reranking with PostgresML and LlamaIndex", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/improving-vector-search-reranking-with-postgresml-and-llamaindex"}, "hash": "d39a39bbcdf8660474c8a8466e6e1e4f114b7147cb62befcea208b86e848b282", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a546027d-ea6b-45e8-863d-f1dac57f8702", "node_type": "1", "metadata": {}, "hash": "a0102c73ba997852101283b849704cf706149059041df8d94219fa1cf3d8188f", "class_name": "RelatedNodeInfo"}}, "text": "From then on I knew that whatever else I did, I'd always write essays too.\n    \n    \n    \n    ---------\n    \n    Id: de01b7e1-95f8-4aa0-b4ec-45ef64816e0e\n    \n    Score: 0.7770352826735559\n    \n    Text: Asterix comics begin by zooming in on a tiny corner of Roman Gaul that turns out not to be controlled by the Romans. You can do something similar on a map of New York City: if you zoom in on the Upper East Side, there's a tiny corner that's not rich, or at least wasn't in 1993. ", "mimetype": "text/plain", "start_char_idx": 5744, "end_char_idx": 6227, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a546027d-ea6b-45e8-863d-f1dac57f8702": {"__data__": {"id_": "a546027d-ea6b-45e8-863d-f1dac57f8702", "embedding": null, "metadata": {"filename": "improving-vector-search-reranking-with-postgresml-and-llamaindex.md", "extension": ".md", "title": "Improving Vector Search - Reranking with PostgresML and LlamaIndex", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/improving-vector-search-reranking-with-postgresml-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "033242bb-7f8d-4a19-a628-ab5236c05991", "node_type": "4", "metadata": {"filename": "improving-vector-search-reranking-with-postgresml-and-llamaindex.md", "extension": ".md", "title": "Improving Vector Search - Reranking with PostgresML and LlamaIndex", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/improving-vector-search-reranking-with-postgresml-and-llamaindex"}, "hash": "b051274d3600a91a43173fd3547b2410a90c87b4935e88606bb1ba784a1161e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20c6f958-2273-459f-98e9-59241e26ca72", "node_type": "1", "metadata": {"filename": "improving-vector-search-reranking-with-postgresml-and-llamaindex.md", "extension": ".md", "title": "Improving Vector Search - Reranking with PostgresML and LlamaIndex", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/improving-vector-search-reranking-with-postgresml-and-llamaindex"}, "hash": "a65043886d52209c38c5a26aa7b54c0662d5c3b3ed58842a350ebad4c769693b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dca747ba-640f-4e21-87b1-5cf6e7d83654", "node_type": "1", "metadata": {}, "hash": "d1e0dd31593bcf7b300ffb39cfaf3f1fb0e45b68db0d179a50a28b5616ce47a6", "class_name": "RelatedNodeInfo"}}, "text": "It's called Yorkville, and that was my new home. Now I was a New York artist \u2014 in the strictly technical sense of making paintings and living in New York.\n    \n    \n    \n    I was nervous about money, because I could sense that Interleaf was on the way down. Freelance Lisp hacking work was very rare, and I didn't want to have to program in another language, which in those days would have meant C++ if I was lucky. So with my unerring nose for financial opportunity, I decided to write another book on Lisp. This would be a popular book, the sort of book that could be used as a textbook. I imagined myself living frugally off the royalties and spending all my time painting. (The painting on the cover of this book, ANSI Common Lisp, is one that I painted around this time.)\n    \n    \n    \n    The best thing about New York for me was the presence of Idelle and Julian Weber. Idelle Weber was a painter, one of the early photorealists, and I'd taken her painting class at Harvard. I've never known a teacher more beloved by her students. Large numbers of former students kept in touch with her, including me. After I moved to New York I became her de facto studio assistant.\n\nThese aren\u2019t bad results, but they aren\u2019t perfect. Let\u2019s try reranking with a\ncross-encoder.\n\n    \n    \n    retriever = index.as_retriever(\n        limit=2,\n        rerank={\n            \"model\": \"mixedbread-ai/mxbai-rerank-base-v1\",\n            \"num_documents_to_rerank\": 100\n        }\n    )\n    docs = retriever.retrieve(\"What did the author do as a child?\")\n    for doc in docs:\n        print(\"---------\")\n        print(f\"Id: {doc.id_}\")\n        print(f\"Score: {doc.score}\")\n        print(f\"Text: {doc.text}\")\n    \n\nHere, we configure our retriever to return the top two documents, but this\ntime, we add a rerank parameter to use the mixedbread-ai/mxbai-rerank-base-v1\nmodel. This means our initial semantic search will return 100 results, which\nwill then be reranked by the mixedbread-ai/mxbai-rerank-base-v1 model, and\nonly the top two results will be presented.\n\nRunning this outputs:\n\n    \n    \n    Id: de01b7e1-95f8-4aa0-b4ec-45ef64816e0e\n    Score: 0.17803585529327393\n    Text: What I Worked On\n    \n    February 2021\n    \n    Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n    \n    The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines \u2014 CPU, disk drives, printer, card reader \u2014 sitting up on a raised floor under bright fluorescent lights.\n    \n    The language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\n    \n    \n    ---------\n    Id: de01b7e1-95f8-4aa0-b4ec-45ef64816e0e\n    Score: 0.1057136133313179\n    Text: I wanted not just to build things, but to build things that would last.\n    \n    In this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where he was in grad school. ", "mimetype": "text/plain", "start_char_idx": 6227, "end_char_idx": 9876, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dca747ba-640f-4e21-87b1-5cf6e7d83654": {"__data__": {"id_": "dca747ba-640f-4e21-87b1-5cf6e7d83654", "embedding": null, "metadata": {"filename": "improving-vector-search-reranking-with-postgresml-and-llamaindex.md", "extension": ".md", "title": "Improving Vector Search - Reranking with PostgresML and LlamaIndex", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/improving-vector-search-reranking-with-postgresml-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "033242bb-7f8d-4a19-a628-ab5236c05991", "node_type": "4", "metadata": {"filename": "improving-vector-search-reranking-with-postgresml-and-llamaindex.md", "extension": ".md", "title": "Improving Vector Search - Reranking with PostgresML and LlamaIndex", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/improving-vector-search-reranking-with-postgresml-and-llamaindex"}, "hash": "b051274d3600a91a43173fd3547b2410a90c87b4935e88606bb1ba784a1161e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a546027d-ea6b-45e8-863d-f1dac57f8702", "node_type": "1", "metadata": {"filename": "improving-vector-search-reranking-with-postgresml-and-llamaindex.md", "extension": ".md", "title": "Improving Vector Search - Reranking with PostgresML and LlamaIndex", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/improving-vector-search-reranking-with-postgresml-and-llamaindex"}, "hash": "97adad11bfb77d9259f64586f6809b1465617723d6640d6379c03ff3706d4795", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c00c7960-382f-4d06-832c-f9b9c35391a1", "node_type": "1", "metadata": {}, "hash": "f8704768b4657172ac5485f2d21ee05eecee532bf3f19f3f1313d9c2bf787d71", "class_name": "RelatedNodeInfo"}}, "text": "One day I went to visit the Carnegie Institute, where I'd spent a lot of time as a kid. While looking at a painting there I realized something that might seem obvious, but was a big surprise to me. There, right on the wall, was something you could make that would last. Paintings didn't become obsolete. Some of the best ones were hundreds of years old.\n    \n    And moreover this was something you could make a living doing. Not as easily as you could by writing software, of course, but I thought if you were really industrious and lived really cheaply, it had to be possible to make enough to survive. And as an artist you could be truly independent. You wouldn't have a boss, or even need to get research funding.\n    \n    I had always liked looking at paintings. Could I make them? I had no idea. I'd never imagined it was even possible. I knew intellectually that people made art \u2014 that it didn't just appear spontaneously \u2014 but it was as if the people who made it were a different species. They either lived long ago or were mysterious geniuses doing strange things in profiles in Life magazine. The idea of actually being able to make art, to put that verb before that noun, seemed almost miraculous.\n    \n\nThese are much better results! ", "mimetype": "text/plain", "start_char_idx": 9876, "end_char_idx": 11122, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c00c7960-382f-4d06-832c-f9b9c35391a1": {"__data__": {"id_": "c00c7960-382f-4d06-832c-f9b9c35391a1", "embedding": null, "metadata": {"filename": "improving-vector-search-reranking-with-postgresml-and-llamaindex.md", "extension": ".md", "title": "Improving Vector Search - Reranking with PostgresML and LlamaIndex", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/improving-vector-search-reranking-with-postgresml-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "033242bb-7f8d-4a19-a628-ab5236c05991", "node_type": "4", "metadata": {"filename": "improving-vector-search-reranking-with-postgresml-and-llamaindex.md", "extension": ".md", "title": "Improving Vector Search - Reranking with PostgresML and LlamaIndex", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/improving-vector-search-reranking-with-postgresml-and-llamaindex"}, "hash": "b051274d3600a91a43173fd3547b2410a90c87b4935e88606bb1ba784a1161e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dca747ba-640f-4e21-87b1-5cf6e7d83654", "node_type": "1", "metadata": {"filename": "improving-vector-search-reranking-with-postgresml-and-llamaindex.md", "extension": ".md", "title": "Improving Vector Search - Reranking with PostgresML and LlamaIndex", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/improving-vector-search-reranking-with-postgresml-and-llamaindex"}, "hash": "192f8a322726bf930a310e0590287dc071beb903ba5500c252a53403bc237b2d", "class_name": "RelatedNodeInfo"}}, "text": "We can see that the top document has the answer\nto the user\u2019s question. Notice that we did not have to specify a third party\nAPI to use for reranking. Once again, PostgresML handles the reranking using\ncross-encoders in the database.\n\nWe can use re-ranking directly in RAG:\n\n    \n    \n    query_engine = index.as_query_engine(\n        streaming=True,\n        vector_search_limit=2,\n        vector_search_rerank={\n            \"model\": \"mixedbread-ai/mxbai-rerank-base-v1\",\n            \"num_documents_to_rerank\": 100,\n        },\n    )\n    results = query_engine.query(\"What did the author do as a child?\")\n    for text in results.response_gen:\n        print(text, end=\"\", flush=True)\n    \n\nRunning this outputs:\n\n    \n    \n    Based on the context information, as a child, the author worked on writing (writing short stories) and programming (on the IBM 1401 using Fortran) outside of school.\n\nThat is the exact answer we wanted!\n\n##  Reranking Leads to Better Results\n\nSearch can be complicated. Reranking with cross-encoders improves search by\ncomparing text pairs and effectively handling new data. Implementing reranking\nwith LlamaIndex and PostgresML improves search results, providing more precise\nanswers in retrieval-augmented generation applications.\n\nTo get started with PostgresML and LlamaIndex, you can follow the PostgresML\nintro [ guide ](https://postgresml.org/docs/introduction/getting-started/) to\nsetup your account, and use the examples above with your own data.\n\n", "mimetype": "text/plain", "start_char_idx": 11122, "end_char_idx": 12604, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3bb32e4f-f76a-4d26-823a-366091ae03e6": {"__data__": {"id_": "3bb32e4f-f76a-4d26-823a-366091ae03e6", "embedding": null, "metadata": {"filename": "the-latest-updates-to-llamacloud.md", "extension": ".md", "title": "The latest updates to LlamaCloud", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/the-latest-updates-to-llamacloud"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "645df4f8-7d44-4511-a5c0-8c55474e99b4", "node_type": "4", "metadata": {"filename": "the-latest-updates-to-llamacloud.md", "extension": ".md", "title": "The latest updates to LlamaCloud", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/the-latest-updates-to-llamacloud"}, "hash": "057e1b6eff5aa22091d45fb7ed88560d35160844925ea3919a47dd6656508fac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a3136c8-0786-462e-9ba2-feb87d836589", "node_type": "1", "metadata": {}, "hash": "f25d97df0156e29f2d0f45b9321fac9e4ad24697ed2587af4609997867f3b532", "class_name": "RelatedNodeInfo"}}, "text": "To build a production-quality LLM agent over your data, you need a production-\nquality data processing layer. LlamaCloud is that data processing and\nmanagement layer for your AI knowledge assistants. Since [ launching a\nLlamaCloud waitlist last week ](https://www.llamaindex.ai/blog/llamacloud-\nbuilt-for-enterprise-llm-app-builders) , we\u2019ve gotten hundreds of signups and\npublished case studies showing how it cuts [ production development hours by\n50% ](https://www.llamaindex.ai/blog/case-study-how-scaleport-ai-accelerated-\ndevelopment-and-improved-sales-with-llamacloud) .\n\nOn top of that, our team has shipped a slew of new features at a breakneck\npace in the past week. We\u2019re excited to highlight these new features that\ncollectively help you **set up a chat interface in minutes,** **increase\ndeveloper collaboration within your team, and access more data and metadata.**\n\n##  Set up a Chat Interface in Minutes\n\nWe are releasing **LlamaCloud Chat,** which gives you an easy-to-use chat\ninterface over your data. This chat interface is a conversational RAG pipeline\nbuilt over the advanced retrieval interface that a given pipeline provides,\nand has out-of-the-box support for streaming and citations - it\u2019s powered by\nthe same DNA as [ ` create-llama ` , our fully open-source set up tool\n](https://github.com/run-llama/create-llama) for LLM applications.\n\nThe LlamaCloud UI already lets you set up a data pipeline over any data in\nminutes, and now you get a full-blown ChatGPT over your data in minutes.\nBesides the chat UI, you also have additional flexibility:\n\n  * You can customize metadata filters in the retrieval parameters \n  * You can view retrieved nodes and their source files \n  * Besides chunk-level retrieval, you can now do **file-level retrieval** (more on this soon!) \n\nLlamaCloud is fundamentally a developer tool: with these updates, we enable\ndevelopers to spend less time on data pipeline setup and iteration, and more\ntime on writing the orchestration logic on top of this interface.\n\n##  Increased Developer Collaboration\n\nThe team selection interface  Organization settings\n\nWe\u2019ve added **organizational features** into LlamaCloud, enabling any\nindividual user to create an organization and add other users to the\norganization. Any user within an organization will have a view of all the\norganization\u2019s projects and indexes within each project.\n\nThis allows your team to have a single-source of truth for your data\npipelines. In the past each developer would spend time re-\nindexing/experimenting with the same sources of data. This feature enables\ntransparency, re-use, and generally more rapid development velocity.\n\n##  Improved Data and Metadata Access\n\nWe\u2019ve made several updates here - we\u2019ve added more data connectors and added\nfeatures to let you more easily access and customize metadata.\n\n  * We added a Notion, Slack, and Jira Connector \n  * Our Sharepoint connector now natively integrates with user IDs that you can filter for, enabling you to build LLM applications with access control. \n  * You can now attach metadata to any uploaded file as a CSV - do this through the UI or our API! \n\n##  Want to see what LlamaCloud can do for you?\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3186, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a3136c8-0786-462e-9ba2-feb87d836589": {"__data__": {"id_": "9a3136c8-0786-462e-9ba2-feb87d836589", "embedding": null, "metadata": {"filename": "the-latest-updates-to-llamacloud.md", "extension": ".md", "title": "The latest updates to LlamaCloud", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/the-latest-updates-to-llamacloud"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "645df4f8-7d44-4511-a5c0-8c55474e99b4", "node_type": "4", "metadata": {"filename": "the-latest-updates-to-llamacloud.md", "extension": ".md", "title": "The latest updates to LlamaCloud", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/the-latest-updates-to-llamacloud"}, "hash": "057e1b6eff5aa22091d45fb7ed88560d35160844925ea3919a47dd6656508fac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3bb32e4f-f76a-4d26-823a-366091ae03e6", "node_type": "1", "metadata": {"filename": "the-latest-updates-to-llamacloud.md", "extension": ".md", "title": "The latest updates to LlamaCloud", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/the-latest-updates-to-llamacloud"}, "hash": "08e47690b48de8169a288da5a54469dac8a018125101c1e47fcb457ea66b41fc", "class_name": "RelatedNodeInfo"}}, "text": "Come sign up on our [ waitlist ](https://bit.ly/llamacloud) for access. If\nyou\u2019re interested in chatting about enterprise plans, [ get in touch.\n](https://www.llamaindex.ai/contact)\n\nIf you\u2019ve gotten access to LlamaCloud, check out our [ rich repository of\ndemonstrations and examples ](https://github.com/run-llama/llamacloud-demo) on\nhow to build different LLM application use cases.\n\n", "mimetype": "text/plain", "start_char_idx": 3186, "end_char_idx": 3573, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fdf5cead-ba49-4d5d-83eb-3854d4234766": {"__data__": {"id_": "fdf5cead-ba49-4d5d-83eb-3854d4234766", "embedding": null, "metadata": {"filename": "case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud.md", "extension": ".md", "title": "Case Study: How Scaleport.ai Accelerated Development and Improved Sales with LlamaCloud", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1b0c2b01-a080-44e0-9dc3-e783628d2eee", "node_type": "4", "metadata": {"filename": "case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud.md", "extension": ".md", "title": "Case Study: How Scaleport.ai Accelerated Development and Improved Sales with LlamaCloud", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud"}, "hash": "59de37479e3b6554d013c4210bc0017011020ceaf846b1a103123fa3e55fab3b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d387f68-cc83-4a2a-b2a0-e73cb221e64f", "node_type": "1", "metadata": {}, "hash": "e261cb65dc62bf8cf98e0f3ac4eeb541749aa5a418c6de003d187ea0f6a858ad", "class_name": "RelatedNodeInfo"}}, "text": "##  The Challenge: Streamlining AI Development\n\nScaleport AI specializes in transforming emerging AI technology into tangible\nbusiness results. They possess deep expertise in deploying AI across key\nindustries such as Legal, eCommerce, Real Estate, and Finance, providing\ntailored generative AI solutions for production applications.\n\nBefore adopting LlamaCloud and LlamaIndex, Scaleport AI faced several\nchallenges:\n\n  * Long development timelines for creating technical prototypes \n  * Difficulty in demonstrating tangible value to clients during the sales process \n  * Complex setup requirements for ingestion pipelines and data processing \n  * Suboptimal OCR performance, as existing solutions were not meeting the required accuracy and efficiency standards \n\n##  The Solution: LlamaCloud's Comprehensive AI Development Platform\n\nScaleport AI turned to LlamaCloud to address these challenges. LlamaCloud\noffered:\n\n  * **Centralized Knowledge Interface:** Simplified data management and reduced time spent on data wrangling. \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1031, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d387f68-cc83-4a2a-b2a0-e73cb221e64f": {"__data__": {"id_": "5d387f68-cc83-4a2a-b2a0-e73cb221e64f", "embedding": null, "metadata": {"filename": "case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud.md", "extension": ".md", "title": "Case Study: How Scaleport.ai Accelerated Development and Improved Sales with LlamaCloud", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1b0c2b01-a080-44e0-9dc3-e783628d2eee", "node_type": "4", "metadata": {"filename": "case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud.md", "extension": ".md", "title": "Case Study: How Scaleport.ai Accelerated Development and Improved Sales with LlamaCloud", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud"}, "hash": "59de37479e3b6554d013c4210bc0017011020ceaf846b1a103123fa3e55fab3b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fdf5cead-ba49-4d5d-83eb-3854d4234766", "node_type": "1", "metadata": {"filename": "case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud.md", "extension": ".md", "title": "Case Study: How Scaleport.ai Accelerated Development and Improved Sales with LlamaCloud", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud"}, "hash": "76ed9f356feda8cd554bf4e71a784af7bcd35809552e5c026b4715e2829a59a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "519117a3-c9ca-4cb2-9c3f-d711d84d20b3", "node_type": "1", "metadata": {}, "hash": "f25baac2dea1ec3f92574af4813ed26fb29e7838dea942e2cc7760c958760332", "class_name": "RelatedNodeInfo"}}, "text": "* **LlamaParse:** Outperformed existing OCR solutions, offering superior accuracy and efficiency. \n  * **Advanced Indexing and Retrieval:** Enabled flexible integration with various data sources, enhancing data management and accessibility. \n  * **Rapid Prototyping and Easy Production Deployments:** LlamaCloud provides an intuitive UI for rapid prototyping and a seamless transition from UI to code for full-scale development. \n\n##  The Results: Accelerated Development and Enhanced Client Engagement\n\nLlamaCloud delivered remarkable improvements for [ Scaleport.ai\n](http://scaleport.ai/) :\n\n  * **Accelerated Development Timelines:** The team could build technical prototypes during the scoping phase, demonstrating tangible value instantly. This improved client engagement and sales outcomes. \n  * **Enhanced OCR Performance:** LlamaParse outperformed GPT-4 vision on several OCR tasks, providing superior accuracy and efficiency. \n  * **Flexible Data Handling:** LlamaCloud's integration with data sources and advanced indexing and retrieval capabilities allowed for quick delivery of high-quality results. \n\nTeemu Lahdenper\u00e4, CTO of Scaleport AI, shared his experience:\n\n> \"LlamaCloud has really sped up our development timelines - whether it's\n> prototyping or production deployments. Before LlamaCloud, building even a\n> simple application took forever because we needed to write our own\n> abstractions for everything. When building an app for a client, a LOT of the\n> work is building the ingestion pipelines. Doing that stuff with LlamaCloud\n> and LlamaParse is remarkably simpler.\n\n> This in turn has really helped our sales outcomes since we can show tangible\n> value instantly. We've also seen great results with LlamaParse!\n\n> Specifically, we spent about 50-60% less development hours for one of our\n> clients than we did for an equivalent application prior to LlamaCloud. The\n> main time savings were around Llamaparse; not having to build a custom\n> ingestion pipeline and having the indexing sorted. This helps our margins as\n> well\u201d\n\n##  Conclusion: A Game-Changer for AI Development\n\nLlamaCloud has proven to be a game-changer for [ Scaleport.ai\n](http://scaleport.ai/) , enabling them to develop apps faster and enhance\ntheir overall AI application performance. This has accelerated their sales\nprocess! By leveraging LlamaCloud's comprehensive suite of tools, [\nScaleport.ai ](http://scaleport.ai/) has positioned itself at the forefront of\nAI solution providers, ready to meet the evolving needs of their clients with\nspeed, flexibility, and cutting-edge technology.\n\n##  Want to see what LlamaCloud can do for you?\n\n", "mimetype": "text/plain", "start_char_idx": 1031, "end_char_idx": 3672, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "519117a3-c9ca-4cb2-9c3f-d711d84d20b3": {"__data__": {"id_": "519117a3-c9ca-4cb2-9c3f-d711d84d20b3", "embedding": null, "metadata": {"filename": "case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud.md", "extension": ".md", "title": "Case Study: How Scaleport.ai Accelerated Development and Improved Sales with LlamaCloud", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1b0c2b01-a080-44e0-9dc3-e783628d2eee", "node_type": "4", "metadata": {"filename": "case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud.md", "extension": ".md", "title": "Case Study: How Scaleport.ai Accelerated Development and Improved Sales with LlamaCloud", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud"}, "hash": "59de37479e3b6554d013c4210bc0017011020ceaf846b1a103123fa3e55fab3b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d387f68-cc83-4a2a-b2a0-e73cb221e64f", "node_type": "1", "metadata": {"filename": "case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud.md", "extension": ".md", "title": "Case Study: How Scaleport.ai Accelerated Development and Improved Sales with LlamaCloud", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud"}, "hash": "80bef5bdc95178d11a850a74584ba484f228d7bef988de8c99669bd66d89e708", "class_name": "RelatedNodeInfo"}}, "text": "[ Sign up for LlamaCloud ](https://cloud.llamaindex.ai) and [ get on the\nwaitlist\n](https://docs.google.com/forms/d/e/1FAIpQLSdehUJJB4NIYfrPIKoFdF4j8kyfnLhMSH_qYJI_WGQbDWD25A/viewform)\nfor full access!\n\n", "mimetype": "text/plain", "start_char_idx": 3672, "end_char_idx": 3875, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02305182-6743-4f03-8342-d6461b02c6c5": {"__data__": {"id_": "02305182-6743-4f03-8342-d6461b02c6c5", "embedding": null, "metadata": {"filename": "building-a-multi-agent-concierge-system.md", "extension": ".md", "title": "Building a multi-agent concierge system", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/building-a-multi-agent-concierge-system"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c26ea0e7-6cd9-4a86-bcff-9ff279215e36", "node_type": "4", "metadata": {"filename": "building-a-multi-agent-concierge-system.md", "extension": ".md", "title": "Building a multi-agent concierge system", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/building-a-multi-agent-concierge-system"}, "hash": "1c1bcded2311b3bf3b83b277223a74fe78e34955fa9683f403b6960bc5a3828b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89dc47e1-3765-42a2-aa08-9c3ba27702c7", "node_type": "1", "metadata": {}, "hash": "d03b7b292d89c5c44dca0becbbeb29470f1293c7a3cdec85b33eb3309155eebc", "class_name": "RelatedNodeInfo"}}, "text": "##  Why build this?\n\nInteractive chat bots are by this point a familiar solution to customer\nservice, and agents are a frequent component of chat bot implementations. They\nprovide memory, introspection, tool use and other features necessary for a\ncompetent bot.\n\nWe have become interested in larger-scale chatbots: ones that can complete\ndozens of tasks, some of which have dependencies on each other, using hundreds\nof tools. What would that agent look like? It would have an enormous system\nprompt and a huge number of tools to choose from, which can be confusing for\nan agent.\n\nImagine a bank implementing a system that can:\n\n  * Look up the price of a specific stock \n  * Authenticate a user \n  * Check your account balance \n    * Which requires the user be authenticated \n  * Transfer money between accounts \n    * Which requires the user be authenticated \n    * And also that the user checks their account balance first \n\nEach of these top-level tasks has sub-tasks, for instance:\n\n  * The stock price lookup might need to look up the stock symbol first \n  * The user authentication would need to gather a username and a password \n  * The account balance would need to know which of the user's accounts to check \n\nComing up with a single primary prompt for all of these tasks and sub-tasks\nwould be very complex. So instead, we designed a multi-agent system with\nagents responsible for each top-level task, plus a \"concierge\" agent that can\ndirect the user to the correct agent.\n\n##  What we built\n\nWe built a system of agents to complete the above tasks. [ It's open-source\n](https://github.com/run-llama/multi-agent-concierge/) ! There are four basic\n\"task\" agents:\n\n  * A stock lookup agent (which takes care of sub-tasks like looking up symbols) \n  * An authentication agent (which asks for username and password) \n  * An account balance agent (which takes care of sub-tasks like selecting an account) \n  * A money transfer agent (which takes care of tasks like asking what account to transfer to, and how much) \n\nThere are also three \"meta\" agents:\n\n  1. A **concierge agent** : this agent is responsible for interacting with the user when they first arrive, letting them know what sort of tasks are available, and providing feedback when tasks are complete. \n  2. An **orchestration agent** : this agent never provides output directly to the user. Instead, it looks at what the user is currently trying to accomplish, and responds with the plain-text name of the agent that should handle the task. The code then routes to that agent. \n  3. A **continuation agent** : it's sometimes necessary to chain agents together to complete a task. For instance, to check your account balance, you need to be authenticated first. The authentication agent doesn't know if you were simply trying to authenticate yourself or if it's part of a chain, and it doesn't need to. When the authentication agent completes, the continuation agent checks chat history to see what the original task was, and if there's more to do, it formulates a new request to the orchestration agent to get you there without further user input. \n\nA **global state** keeps track of the user and their current state, shared\nbetween all the agents.\n\nThe flow of the the system looks something like this:\n\n##  The system in action\n\nTo get a sense of how this works in practice, here's sample output including\nhelpful debug statements. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3403, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89dc47e1-3765-42a2-aa08-9c3ba27702c7": {"__data__": {"id_": "89dc47e1-3765-42a2-aa08-9c3ba27702c7", "embedding": null, "metadata": {"filename": "building-a-multi-agent-concierge-system.md", "extension": ".md", "title": "Building a multi-agent concierge system", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/building-a-multi-agent-concierge-system"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c26ea0e7-6cd9-4a86-bcff-9ff279215e36", "node_type": "4", "metadata": {"filename": "building-a-multi-agent-concierge-system.md", "extension": ".md", "title": "Building a multi-agent concierge system", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/building-a-multi-agent-concierge-system"}, "hash": "1c1bcded2311b3bf3b83b277223a74fe78e34955fa9683f403b6960bc5a3828b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02305182-6743-4f03-8342-d6461b02c6c5", "node_type": "1", "metadata": {"filename": "building-a-multi-agent-concierge-system.md", "extension": ".md", "title": "Building a multi-agent concierge system", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/building-a-multi-agent-concierge-system"}, "hash": "bfd7e8a497c465f4abb8f26a8d0105e9ee20c8cb0809cf66716bf1f2ada67adf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ef8ae00-5ed7-4536-a6d3-63683048e637", "node_type": "1", "metadata": {}, "hash": "602c15bb77a6e8639bc5bbd92886d50b81a3d1a3c612dd06cfb3547e2e9bbbc8", "class_name": "RelatedNodeInfo"}}, "text": "Output that would be ordinarily shown to the user\nhas two ` >> ` , while user input has one ` > ` .\n\nAt the beginning of the conversation nothing's happened yet, so you get routed\nto the concierge:\n\n    \n    \n    No current speaker, asking orchestration agent to decide\n    \n    Concierge agent selected\n    \n    >> Hi there! How can I assist you today? Here are some things I can help you with:\n    \n    >> Looking up a stock price\n    \n    >> Authenticating you\n    \n    >> Checking an account balance (requires authentication first)\n    \n    >> Transferring money between accounts (requires authentication and checking an account balance first)\n    \n    >> What would you like to do?\n    \n    > Transfer money\n\nThe \"transfer money\" task requires authentication. The orchestration agent\nchecks if you're authenticated while deciding how to route you (it does this\ntwice for some reason, it's a demo!):\n\n    \n    \n    No current speaker, asking orchestration agent to decide\n    \n    Orchestrator is checking if authenticated\n    \n    Orchestrator is checking if authenticated\n    \n    Auth agent selected\n\nIt correctly determines you're not authenticated, so it routes you to the\nauthentication agent:\n\n    \n    \n    >> To transfer money, I need to authenticate you first. Could you please provide your username and password?\n    \n    > seldo\n\nThis is a fun part: you've provided input, but it's not sufficient to complete\nthe task (you didn't give a password). So when the flow goes back to the\norchestration agent, the global state indicates that the \"authenticate\" agent\nis already running and hasn't completed yet, so it routes back to the\nauthentication agent, and does that again for the password:\n\n    \n    \n    There's already a speaker: authenticate\n    \n    Auth agent selected\n    \n    Recording username\n    \n    >> Thank you! Now, could you please provide your password?\n    \n    > monkey\n    \n    There's already a speaker: authenticate\n    \n    Auth agent selected\n    \n    Logging in seldo\n    \n    Checking if authenticated\n    \n    Authentication is complete\n\nNow the auth agent has called a done() function that indicates to the global\nstate that it has completed its task. So the flow now goes to the continuation\nagent, which looks at the chat history and sees that the user was trying to\ntransfer money. So it generates a prompt, as if spoken by the user, and sends\nthat to the orchestration agent:\n\n    \n    \n    >> You have been successfully authenticated. Another agent will assist you with transferring money.\n    \n    Asking the continuation agent to decide what to do next\n    \n    Continuation agent said \"I would like to transfer money.\"\n    \n    No current speaker, asking orchestration agent to decide\n    \n    Orchestrator checking if account has a balance\n    \n    Orchestrator checking if account has a balance\n    \n    Account balance agent selected\n\nNow you're authenticated, but you haven't checked your balance yet, which the\norchestration agent knows is necessary for transferring money. So it routes\nyou to the account balance agent (after checking twice for some reason):\n\n    \n    \n    >> Before you can transfer money, you need to check your account balance. Let's start by looking up your account balance. Could you please provide the name of the account you're interested in?\n    \n    > Checking\n    \n    There's already a speaker: account_balance\n    \n    Account balance agent selected\n    \n    Looking up account ID for Checking\n    \n    Looking up account balance for 1234567890\n    \n    Account balance lookup is complete\n    \n    >> Your Checking account has a balance of $1000. Another agent will assist you with transferring money.\n    \n    ", "mimetype": "text/plain", "start_char_idx": 3403, "end_char_idx": 7100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ef8ae00-5ed7-4536-a6d3-63683048e637": {"__data__": {"id_": "8ef8ae00-5ed7-4536-a6d3-63683048e637", "embedding": null, "metadata": {"filename": "building-a-multi-agent-concierge-system.md", "extension": ".md", "title": "Building a multi-agent concierge system", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/building-a-multi-agent-concierge-system"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c26ea0e7-6cd9-4a86-bcff-9ff279215e36", "node_type": "4", "metadata": {"filename": "building-a-multi-agent-concierge-system.md", "extension": ".md", "title": "Building a multi-agent concierge system", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/building-a-multi-agent-concierge-system"}, "hash": "1c1bcded2311b3bf3b83b277223a74fe78e34955fa9683f403b6960bc5a3828b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89dc47e1-3765-42a2-aa08-9c3ba27702c7", "node_type": "1", "metadata": {"filename": "building-a-multi-agent-concierge-system.md", "extension": ".md", "title": "Building a multi-agent concierge system", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/building-a-multi-agent-concierge-system"}, "hash": "3803a039cafaa259f49e7072c7cd9559d1bb63969e7ee1d1420c5ced62af93f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99d0d58e-5992-42d3-ad15-fb8a337ac590", "node_type": "1", "metadata": {}, "hash": "2b437b0457b67ea206e1ca85a0033ec5a20b97884dfb29a8dc4a5fa36d022315", "class_name": "RelatedNodeInfo"}}, "text": "Asking the continuation agent to decide what to do next\n    \n    Continuation agent said \"I would like to transfer money.\"\n    \n    No current speaker, asking orchestration agent to decide\n    \n    Transfer money agent selected\n\nThe account balance agent asks you which account, uses a tool to get the ID\nfor that account, and then marks itself as done. The continuation agent kicks\nin again and sees that you still haven't completed your original task of\ntransferring money, so it prompts the orchestrator agent again. Unfortunately\nthe orchestrator gets a little confused, and loops twice before finally\nrouting you to the transfer money agent:\n\n    \n    \n    Money transfer is complete\n    \n    >> Another agent will assist you with transferring money.\n    \n    Asking the continuation agent to decide what to do next\n    \n    Continuation agent said \"I would like to transfer money.\"\n    \n    No current speaker, asking orchestration agent to decide\n    \n    Transfer money agent selected\n    \n    Money transfer is complete\n    \n    >> Another agent will assist you with transferring money.\n    \n    Asking the continuation agent to decide what to do next\n    \n    Continuation agent said \"I would like to transfer money.\"\n    \n    No current speaker, asking orchestration agent to decide\n    \n    Orchestrator checking if account has a balance\n    \n    Transfer money agent selected\n    \n    >> You have already checked your account balance. Please provide the following details to proceed with the money transfer:\n    \n    >> 1. The account ID to which you want to transfer the money.\n    \n    >> 2. The amount you want to transfer.\n    \n    > To account ID 1234324\n    \n    There's already a speaker: transfer_money\n    \n    Transfer money agent selected\n    \n    >> How much would you like to transfer to account ID 1234324?\n    \n    > 500\n    \n    There's already a speaker: transfer_money\n    \n    Transfer money agent selected\n    \n    Checking if balance is sufficient\n    \n    Transferring 500 from 1234567890 account 1234324\n    \n    Money transfer is complete\n    \n    >> The transfer of $500 to account ID 1234324 has been successfully completed. If you need any further assistance, feel free to ask!\n    \n    ", "mimetype": "text/plain", "start_char_idx": 7100, "end_char_idx": 9327, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99d0d58e-5992-42d3-ad15-fb8a337ac590": {"__data__": {"id_": "99d0d58e-5992-42d3-ad15-fb8a337ac590", "embedding": null, "metadata": {"filename": "building-a-multi-agent-concierge-system.md", "extension": ".md", "title": "Building a multi-agent concierge system", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/building-a-multi-agent-concierge-system"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c26ea0e7-6cd9-4a86-bcff-9ff279215e36", "node_type": "4", "metadata": {"filename": "building-a-multi-agent-concierge-system.md", "extension": ".md", "title": "Building a multi-agent concierge system", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/building-a-multi-agent-concierge-system"}, "hash": "1c1bcded2311b3bf3b83b277223a74fe78e34955fa9683f403b6960bc5a3828b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ef8ae00-5ed7-4536-a6d3-63683048e637", "node_type": "1", "metadata": {"filename": "building-a-multi-agent-concierge-system.md", "extension": ".md", "title": "Building a multi-agent concierge system", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/building-a-multi-agent-concierge-system"}, "hash": "c7ff91a0d02ad2476f1a51926d311e86f1a32726351271399559602e525dd75e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "404c7933-e8ce-48c4-9d44-d5e893380d1f", "node_type": "1", "metadata": {}, "hash": "cbb484c8cd3ed5899546dbfa5c21de94058f2eb576507a440f8ccd8ccbd9b429", "class_name": "RelatedNodeInfo"}}, "text": "Asking the continuation agent to decide what to do next\n    \n    Continuation agent said no_further_tasks\n\nWe've reached the end of the task! The continuation agent sees that there are\nno further tasks, and routes you back to the concierge.\n\n##  The code\n\nNow let's look at some highlights of the code that gets all of this done. The\ncore of the system is a central loop that runs forever. At the core of that is\na very simple block that simply asks the orchestration agent who should speak\nnext, and sets the ` next_speaker ` value which is contained in the state\nobject that is passed between all the agents. Note that if there's already a\nsub-agent speaking, that agent gets to keep speaking.\n\n    \n    \n    current_history = root_memory.get()\n    \n    # who should speak next?\n    if (state[\"current_speaker\"]):\n      print(f\"There's already a speaker: {state['current_speaker']}\")\n      next_speaker = state[\"current_speaker\"]\n    else:\n      print(\"No current speaker, asking orchestration agent to decide\")\n      orchestration_response = orchestration_agent_factory(state).chat(\n        user_msg_str, \n        chat_history=current_history\n      )\n      next_speaker = str(orchestration_response).strip()\n\nThe orchestration agent has a very strict prompt; its output only goes to\nother machines. It includes a natural-language summary of the dependencies\nbetween agents:\n\n    \n    \n        system_prompt = (f\"\"\"\n            You are on orchestration agent.\n            Your job is to decide which agent to run based on the current state of the user and what they've asked to do. Agents are identified by short strings.\n            What you do is return the name of the agent to run next. You do not do anything else.\n            \n            The current state of the user is:\n            {pprint.pformat(state, indent=4)}\n    \n            If a current_speaker is already selected in the state, simply output that value.\n    \n            If there is no current_speaker value, look at the chat history and the current state and you MUST return one of these strings identifying an agent to run:\n            * \"{Speaker.STOCK_LOOKUP.value}\" - if they user wants to look up a stock price (does not require authentication)\n            * \"{Speaker.AUTHENTICATE.value}\" - if the user needs to authenticate\n            * \"{Speaker.ACCOUNT_BALANCE.value}\" - if the user wants to look up an account balance\n                * If they want to look up an account balance, but they haven't authenticated yet, return \"{Speaker.AUTHENTICATE.value}\" instead\n            * \"{Speaker.TRANSFER_MONEY.value}\" - if the user wants to transfer money between accounts (requires authentication and checking an account balance first)\n                * If they want to transfer money, but is_authenticated returns false, return \"{Speaker.AUTHENTICATE.value}\" instead\n                * If they want to transfer money, but has_balance returns false, return \"{Speaker.ACCOUNT_BALANCE.value}\" instead\n            * \"{Speaker.CONCIERGE.value}\" - if the user wants to do something else, or hasn't said what they want to do, or you can't figure out what they want to do. Choose this by default.\n    \n            ", "mimetype": "text/plain", "start_char_idx": 9327, "end_char_idx": 12508, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "404c7933-e8ce-48c4-9d44-d5e893380d1f": {"__data__": {"id_": "404c7933-e8ce-48c4-9d44-d5e893380d1f", "embedding": null, "metadata": {"filename": "building-a-multi-agent-concierge-system.md", "extension": ".md", "title": "Building a multi-agent concierge system", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/building-a-multi-agent-concierge-system"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c26ea0e7-6cd9-4a86-bcff-9ff279215e36", "node_type": "4", "metadata": {"filename": "building-a-multi-agent-concierge-system.md", "extension": ".md", "title": "Building a multi-agent concierge system", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/building-a-multi-agent-concierge-system"}, "hash": "1c1bcded2311b3bf3b83b277223a74fe78e34955fa9683f403b6960bc5a3828b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99d0d58e-5992-42d3-ad15-fb8a337ac590", "node_type": "1", "metadata": {"filename": "building-a-multi-agent-concierge-system.md", "extension": ".md", "title": "Building a multi-agent concierge system", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/building-a-multi-agent-concierge-system"}, "hash": "47a118ed84a9c2bf671af6794c563b0203402d63eb5f39e7cfd988e5f57b2017", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78633de4-b8c4-47f8-b646-c3eb5d71e37a", "node_type": "1", "metadata": {}, "hash": "a9dc3dbf9e875e5127084b068e4bef9a9842ee61b9169046e307f0a91a22ba42", "class_name": "RelatedNodeInfo"}}, "text": "Output one of these strings and ONLY these strings, without quotes.\n            NEVER respond with anything other than one of the above five strings. ", "mimetype": "text/plain", "start_char_idx": 12508, "end_char_idx": 12658, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78633de4-b8c4-47f8-b646-c3eb5d71e37a": {"__data__": {"id_": "78633de4-b8c4-47f8-b646-c3eb5d71e37a", "embedding": null, "metadata": {"filename": "building-a-multi-agent-concierge-system.md", "extension": ".md", "title": "Building a multi-agent concierge system", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/building-a-multi-agent-concierge-system"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c26ea0e7-6cd9-4a86-bcff-9ff279215e36", "node_type": "4", "metadata": {"filename": "building-a-multi-agent-concierge-system.md", "extension": ".md", "title": "Building a multi-agent concierge system", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/building-a-multi-agent-concierge-system"}, "hash": "1c1bcded2311b3bf3b83b277223a74fe78e34955fa9683f403b6960bc5a3828b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "404c7933-e8ce-48c4-9d44-d5e893380d1f", "node_type": "1", "metadata": {"filename": "building-a-multi-agent-concierge-system.md", "extension": ".md", "title": "Building a multi-agent concierge system", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/building-a-multi-agent-concierge-system"}, "hash": "c5fbb53b2f599959863ce2854ab718d77d70bafcc9c44871bf5088030e89ee89", "class_name": "RelatedNodeInfo"}}, "text": "DO NOT be helpful or conversational.\n        \"\"\")\n\nA simple if-else block takes the output of the orchestration agent and uses it\nto instantiate the sub-agent to run next. This is when the state object gets\npassed to each sub-agent:\n\n    \n    \n            if next_speaker == Speaker.STOCK_LOOKUP:\n                print(\"Stock lookup agent selected\")\n                current_speaker = stock_lookup_agent_factory(state)\n                state[\"current_speaker\"] = next_speaker\n            elif next_speaker == Speaker.AUTHENTICATE:\n                print(\"Auth agent selected\")\n                current_speaker = auth_agent_factory(state)\n                state[\"current_speaker\"] = next_speaker\n            elif next_speaker == Speaker.ACCOUNT_BALANCE:\n                print(\"Account balance agent selected\")\n                current_speaker = account_balance_agent_factory(state)\n                state[\"current_speaker\"] = next_speaker\n            elif next_speaker == Speaker.TRANSFER_MONEY:\n                print(\"Transfer money agent selected\")\n                current_speaker = transfer_money_agent_factory(state)\n                state[\"current_speaker\"] = next_speaker\n            elif next_speaker == Speaker.CONCIERGE:\n                print(\"Concierge agent selected\")\n                current_speaker = concierge_agent_factory(state)\n            else:\n                print(\"Orchestration agent failed to return a valid speaker; ask it to try again\")\n                is_retry = True\n                continue\n\nAnd then the full chat history is passed as part of a regular chat message to\nthe newly-instantiated agent:\n\n    \n    \n    response = current_speaker.chat(user_msg_str, chat_history=current_history)\n\nThe agent reads its prompt and the user input and decides what to say. As we\nsaw in our very first block of code, if the speaker is already selected, then\nthe loop will keep talking to the current sub-agent. This continues until the\nsub-agent has completed its task, at which point its prompt instructs it to\ncall the ` done() ` function:\n\n    \n    \n        def done() -> None:\n            \"\"\"When you complete your task, call this tool.\"\"\"\n            print(\"Money transfer is complete\")\n            state[\"current_speaker\"] = None\n            state[\"just_finished\"] = True\n\nThis modifies the state, setting the current speaker to none. This triggers\nthe outer loop to run the continuation agent, to see if there's anything else\nto do:\n\n    \n    \n            elif state[\"just_finished\"] == True:\n                print(\"Asking the continuation agent to decide what to do next\")\n                user_msg_str = str(continuation_agent_factory(state).chat(\"\"\"\n                    Look at the chat history to date and figure out what the user was originally trying to do.\n                    They might have had to do some sub-tasks to complete that task, but what we want is the original thing they started out trying to do.                                                                      \n                    Formulate a sentence as if written by the user that asks to continue that task.\n                    If it seems like the user really completed their task, output \"no_further_task\" only.\n                \"\"\", chat_history=current_history))\n                print(f\"Continuation agent said {user_msg_str}\")\n                if user_msg_str == \"no_further_task\":\n                    user_msg_str = input(\">> \").strip()\n                state[\"just_finished\"] = False\n\nThe continuation agent's prompt instructs it to reply as if it were the user\nasking to perform a task, or to output ` no_further_task ` if there's no more\nto do. If there's a new task, the output of the continuation agent becomes the\ninput to the orchestrator, which selects a new speaker. If there's no further\ntask, the loop pauses for more user input.\n\nAnd that's the full system! The sub-agents can be arbitrarily complicated,\nmulti-turn systems in themselves, and the outer loop doesn't need to know how\nthey work, just how they depend on each other.\n\n##  What's next\n\nWe think there's some novel stuff in here: coordinating multiple agents\n\"speaking\" simultaneously, creating implicit \"chains\" of agents through\nnatural language instructions, using a \"continuation\" agent to manage those\nchains, and using a global state this way. We're excited to see what you do\nwith the patterns we've laid out here. Don't forget to [ check out the open-\nsource repo ](https://github.com/run-llama/multi-agent-concierge/) !\n\n", "mimetype": "text/plain", "start_char_idx": 12658, "end_char_idx": 17158, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c27d85e9-7b67-4b32-9e32-f6f031d1735b": {"__data__": {"id_": "c27d85e9-7b67-4b32-9e32-f6f031d1735b", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-07-16.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-16", "date": "Jul 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-16"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c745a3ea-f895-4b3f-aca6-71413beba7d6", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-07-16.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-16", "date": "Jul 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-16"}, "hash": "771926fe982537246eb97abda22976ad51d989fb68cddb698940ac0adeec75fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c505e468-4788-49f5-896a-c8f103c9ee30", "node_type": "1", "metadata": {}, "hash": "4f97555c271417d401097c3545ec2d925804d94914a5e3fc98421dea9e48fb33", "class_name": "RelatedNodeInfo"}}, "text": "Hello, Llama Family!\n\nWelcome to this week\u2019s edition of the LlamaIndex newsletter! We\u2019re thrilled to\nshare some exciting updates about our products, the implementation of\nGraphRAG, demos that have achieved over $1M in ARR, extensive guides, in-depth\ntutorials, and hackathons.\n\nBefore we get into the details of our newsletter, we\u2019re thrilled to share the\nbeta launch of LlamaCloud. This new data processing layer boosts RAG workflows\nwith sophisticated parsing, indexing, and retrieval functions. Alongside this,\nwe\u2019re also introducing LlamaTrace in partnership with Arize AI, which provides\nunmatched tracing, observability, and evaluation capabilities for LLM\napplication workflows.\n\nSignup here: [ cloud.llamaindex.ai ](https://t.co/yQGTiRSNvj)\n\n##  **The highlights:**\n\n  * **LlamaCloud Launch:** We\u2019ve launched the beta release of LlamaCloud, a data processing layer designed to enhance RAG workflows with state-of-the-art parsing, indexing, and retrieval capabilities. [ Blogpost ](https://www.llamaindex.ai/blog/llamacloud-built-for-enterprise-llm-app-builders) , [ Tweet ](https://x.com/llama_index/status/1810716602247348242) . \n  * **LlamaTrace Launch:** In collaboration with Arize AI, we\u2019ve introduced LlamaTrace, offering unmatched tracing, observability, and evaluation capabilities for LLM application workflows. It features detailed call stack tracing, one-click setup through LlamaIndex, and seamless integration with LlamaCloud. [ Blogpost ](https://www.llamaindex.ai/blog/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications) , [ Tweet ](https://x.com/llama_index/status/1811462543535464796) . \n  * **GraphRAG Implementation:** Implementation of GraphRAG with LlamaIndex, focusing on graph generation, community building, summaries, and community-based retrieval to improve answer aggregation. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/GraphRAG_v1.ipynb) , [ Tweet ](https://x.com/llama_index/status/1812517033445396754) . \n  * **Redis Queue Integration with Llama-Agents:** We have integrated Redis Queue with llama-agents to boost coordination and communication in multi-agent workflows, ensuring robust performance. [ Notebook ](https://github.com/run-llama/llama-agents/tree/main/examples/redis/simple-redis-app) , [ Tweet ](https://x.com/llama_index/status/1812202419025293784) . \n\n##  **Feature Releases and Enhancements:**\n\n  1. We have launched the beta release of LlamaCloud, a data processing layer that enhances RAG workflows with advanced parsing, indexing, and retrieval capabilities. [ Blogpost ](https://www.llamaindex.ai/blog/llamacloud-built-for-enterprise-llm-app-builders) , [ Tweet ](https://x.com/llama_index/status/1810716602247348242) . \n  2. We have launched an implementation[beta] of GraphRAG concepts with LlamaIndex focussing on graph generation, building communities and community summaries, and community-based retrieval to aggregate answers from summaries. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/GraphRAG_v1.ipynb) , [ Tweet ](https://x.com/llama_index/status/1812517033445396754) . \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3169, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c505e468-4788-49f5-896a-c8f103c9ee30": {"__data__": {"id_": "c505e468-4788-49f5-896a-c8f103c9ee30", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-07-16.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-16", "date": "Jul 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-16"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c745a3ea-f895-4b3f-aca6-71413beba7d6", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-07-16.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-16", "date": "Jul 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-16"}, "hash": "771926fe982537246eb97abda22976ad51d989fb68cddb698940ac0adeec75fd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c27d85e9-7b67-4b32-9e32-f6f031d1735b", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-07-16.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-16", "date": "Jul 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-16"}, "hash": "ab2c75a28f58b34a9c777cbe06b884fc744bb95af8460340c613313f7ed1f941", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0151546-4171-4e92-b18b-70e28bd7c4eb", "node_type": "1", "metadata": {}, "hash": "5c7bc60d4f1949d9af0986f7c46549961a03e92df083a9ac1277ae53b26febdd", "class_name": "RelatedNodeInfo"}}, "text": "3. We have integrated Redis Queue with llama-agents to enhance coordination in multi-agent workflows, allowing for robust communication. [ Notebook ](https://github.com/run-llama/llama-agents/tree/main/examples/redis/simple-redis-app) , [ Tweet ](https://x.com/llama_index/status/1812202419025293784) . \n  4. We have introduced LlamaTrace in collaboration with Arize AI, offering unparalleled tracing, observability, and evaluation capabilities for LLM application workflows. LlamaTrace stands out for its detailed tracing, which logs the entire call stack, one-click setup through LlamaIndex, and seamless integration with LlamaCloud for easy access and authentication. [ Blogpost ](https://www.llamaindex.ai/blog/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications) , [ Tweet ](https://x.com/llama_index/status/1811462543535464796) . \n  ", "mimetype": "text/plain", "start_char_idx": 3169, "end_char_idx": 4036, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0151546-4171-4e92-b18b-70e28bd7c4eb": {"__data__": {"id_": "c0151546-4171-4e92-b18b-70e28bd7c4eb", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-07-16.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-16", "date": "Jul 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-16"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c745a3ea-f895-4b3f-aca6-71413beba7d6", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-07-16.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-16", "date": "Jul 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-16"}, "hash": "771926fe982537246eb97abda22976ad51d989fb68cddb698940ac0adeec75fd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c505e468-4788-49f5-896a-c8f103c9ee30", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-07-16.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-16", "date": "Jul 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-16"}, "hash": "bc8772a4e19fe0df120b18ebe81876404e8e37587e2fffc68d1495a7e15cc5a6", "class_name": "RelatedNodeInfo"}}, "text": "5. We have integrated NebulaGraph with LlamaIndex, enhancing PropertyGraph capabilities with sophisticated extractors, customizable properties on nodes and edges, and advanced retrieval options. [ Docs ](https://docs.llamaindex.ai/en/stable/examples/property_graph/property_graph_nebula/) , [ Tweet ](https://x.com/llama_index/status/1811190191597773282) . \n\n##  **Demos:**\n\n  * [ Lyzrai ](https://www.llamaindex.ai/blog/case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex) has achieved over $1M ARR using LlamaIndex! This full-stack autonomous AI agent framework enhances AI sales and marketing functions with LlamaIndex\u2019s data connectors and RAG capabilities, boasting rapid revenue growth, high accuracy, and customer satisfaction. \n\n##  **Guides:**\n\n  * [ Guide ](https://github.com/run-llama/llama_parse/blob/main/examples/multimodal/multimodal_rag_slide_deck.ipynb) to Multi-Modal RAG for Document Processing that introduces a multi-modal RAG architecture using LlamaParse, LlamaIndex, and GPT-4o, designed to handle complex slide decks. [ Tweet ](https://x.com/llama_index/status/1812963306032013586) . \n  * [ Guide ](https://x.com/llama_index/status/1812157431788835094) to using LlamaParse and GPT-4o for Financial Report RAG to to effectively parse and synthesize complex financial documents, enhancing clarity and accuracy in data analysis. \n  * [ Guide ](https://github.com/meta-llama/llama-recipes/tree/main/recipes/3p_integrations/llamaindex/dlai_agentic_rag) to Building Agentic RAG with Llama3: Explore our comprehensive cookbooks, created in collaboration with AI at Meta, featuring advanced techniques from routing and tool use to constructing complex agent reasoning loops and multi-document agents using purely local models like Llama3. \n\n##  **Tutorials:**\n\n  * [ 1LittleCoder\u2019s ](https://x.com/1littlecoder) [ video tutorial ](https://www.youtube.com/watch?v=aiySmi5JocQ) demonstrates how to deploy self-hosted llama-agents using Arcee AI, MistralAI, and Ollama, including setup, local model integration, and tool development. \n  * [ kingzzm\u2019s ](https://x.com/kingzzm) [ tutorial ](https://ai.gopubby.com/advanced-rag-retrieval-strategies-flow-and-modular-672493acb4a7) on using LlamaIndex to build advanced RAG flows, detailing how to compose and visualize each step from basic retrieval and prompting to advanced techniques and evaluation with RAGAS. \n  * [ Mervin Praison\u2019s ](https://x.com/MervinPraison) [ tutorial ](https://www.youtube.com/watch?v=nEQCpSd5mx8) on using llama-agents, detailing the framework\u2019s purpose, a step-by-step setup guide for multi-agent services, and how it stands out from other frameworks. \n\n##  **Events:**\n\n  * [ Join our online hackathon ](https://lablab.ai/event/llama-3-ai-hackathon) this Friday, 19th, to build AI apps with Llama 3 from Meta and win cash, credits, and prizes from us and our co-hosts [ TogetherAI ](https://x.com/togethercompute) , [ Milvus ](https://x.com/milvusio) , and [ LablabAI ](https://x.com/lablabai) . \n\n", "mimetype": "text/plain", "start_char_idx": 4036, "end_char_idx": 7048, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "056e2fd5-2ef6-4f35-b05f-c955c1e653b2": {"__data__": {"id_": "056e2fd5-2ef6-4f35-b05f-c955c1e653b2", "embedding": null, "metadata": {"filename": "arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications.md", "extension": ".md", "title": "Arize AI and LlamaIndex Roll Out Joint Platform for Evaluating LLM Applications", "date": "Jul 11, 2024", "url": "https://www.llamaindex.ai/blog/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9f3cc26a-e7ce-4a76-9684-cbce575ad49b", "node_type": "4", "metadata": {"filename": "arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications.md", "extension": ".md", "title": "Arize AI and LlamaIndex Roll Out Joint Platform for Evaluating LLM Applications", "date": "Jul 11, 2024", "url": "https://www.llamaindex.ai/blog/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications"}, "hash": "0229287818ba01ce89a4b072f7989a839d6f507c5fecdb311c45b7371e076576", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f65a9f90-b9e1-4a3c-a1fa-167210874251", "node_type": "1", "metadata": {}, "hash": "9850664630d4b74543d6a7a55a6a99d6e133449d78fc83395a03d1a37a5812b0", "class_name": "RelatedNodeInfo"}}, "text": "###  Strategic alliance and joint product promises to broaden the adoption of\ngenerative AI across industries\n\nArize AI, a pioneer and leader in AI observability and LLM evaluation, and\nLlamaIndex, a leading data framework for LLM applications, debuted a new joint\noffering today called LlamaTrace, a hosted version of Arize OSS Phoenix.\n\nAccording to a soon-to-release survey, 47.7% of AI engineers and developers\nbuilding generative AI applications are leveraging retrieval today in their\nLLM Applications. By connecting data to generative AI, orchestration\nframeworks like LlamaIndex can be game-changers in accelerating generative AI\ndevelopment. However, for many teams and enterprises technical challenges\nremain in getting modern LLM systems \u2013 with layers of abstraction \u2013 ready for\nthe real world.\n\nTo help, Arize and LlamaIndex are debuting an LLM tracing and observability\nplatform that works natively with the LlamaIndex and Arize ecosystem. With a\nfoundation based on [ Arize Phoenix OSS ](https://phoenix.arize.com/) , the\nhosted version of Phoenix offers the ability to persist application telemetry\ndata generated during AI development in order to better experiment, iterate,\nand collaborate in development or production.\n\nThe solution has a foundation in open source and features a fully hosted,\nonline, persistent deployment option for teams that do not want to self host.\nAI engineers can instantly log traces, persist datasets, run experiments, run\nevaluations \u2013 and share those insights with colleagues.\n\nThe new offering is available today, and can be accessed through either a\nLlamaIndex or Arize account.\n\n> \u201cWe share a vision with LlamaIndex in enabling builders to reduce the time\n> it takes to deploy generative AI into production but in a way that is super\n> battle hardened for business-critical use cases,\u201d said Jason Lopatecki, CEO\n> and Co-Founder of Arize. \u201cAs leaders in our respective spaces with a common\n> philosophy in empowering AI engineers and developers, we\u2019re uniquely\n> positioned here to do something that can move modern LLMOps forward and\n> broaden adoption.\u201d\n\n> \u201cPrototyping a RAG pipeline or agent is easy, but every AI engineer needs\n> the right data processing layer, orchestration framework, and\n> experimentation/monitoring tool in order to take these applications to\n> production. LlamaTrace by Arize offers the richest toolkit we\u2019ve seen in\n> enabling developers to observe, debug, and evaluate every granular step of a\n> very complex LLM workflow, and it nicely complements the production-ready\n> data platform and orchestration framework that LlamaCloud and LlamaIndex\n> offer.\u201d - Jerry Liu, CEO of LlamaIndex\n\n**About Arize AI**\n\nArize AI is an AI observability and LLM evaluation platform that helps teams\ndeliver and maintain more successful AI in production. Arize\u2019s automated\nmonitoring and observability platform allows teams to quickly detect issues\nwhen they emerge, troubleshoot why they happened, and improve overall\nperformance across both traditional ML and generative use cases. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3048, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f65a9f90-b9e1-4a3c-a1fa-167210874251": {"__data__": {"id_": "f65a9f90-b9e1-4a3c-a1fa-167210874251", "embedding": null, "metadata": {"filename": "arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications.md", "extension": ".md", "title": "Arize AI and LlamaIndex Roll Out Joint Platform for Evaluating LLM Applications", "date": "Jul 11, 2024", "url": "https://www.llamaindex.ai/blog/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9f3cc26a-e7ce-4a76-9684-cbce575ad49b", "node_type": "4", "metadata": {"filename": "arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications.md", "extension": ".md", "title": "Arize AI and LlamaIndex Roll Out Joint Platform for Evaluating LLM Applications", "date": "Jul 11, 2024", "url": "https://www.llamaindex.ai/blog/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications"}, "hash": "0229287818ba01ce89a4b072f7989a839d6f507c5fecdb311c45b7371e076576", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "056e2fd5-2ef6-4f35-b05f-c955c1e653b2", "node_type": "1", "metadata": {"filename": "arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications.md", "extension": ".md", "title": "Arize AI and LlamaIndex Roll Out Joint Platform for Evaluating LLM Applications", "date": "Jul 11, 2024", "url": "https://www.llamaindex.ai/blog/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications"}, "hash": "4c3802106f14ab7487f3327ae9e3e607e0f42e31d5437f677e6260e7507240bf", "class_name": "RelatedNodeInfo"}}, "text": "Arize is\nheadquartered in Berkeley, CA.\n\n", "mimetype": "text/plain", "start_char_idx": 3048, "end_char_idx": 3089, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f52edd71-c7cd-4b8d-95c0-edadb1bff567": {"__data__": {"id_": "f52edd71-c7cd-4b8d-95c0-edadb1bff567", "embedding": null, "metadata": {"filename": "case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex.md", "extension": ".md", "title": "Case study: Lyzr: Taking autonomous AI agents to $1M+ ARR with LlamaIndex", "date": "Jul 10, 2024", "url": "https://www.llamaindex.ai/blog/case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f3d8b800-430b-447e-8711-5a07466261ac", "node_type": "4", "metadata": {"filename": "case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex.md", "extension": ".md", "title": "Case study: Lyzr: Taking autonomous AI agents to $1M+ ARR with LlamaIndex", "date": "Jul 10, 2024", "url": "https://www.llamaindex.ai/blog/case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex"}, "hash": "7f944df3b251fb2cf997461955665d4601fcff11247de437c258aca9e2c16ec8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "218276ac-b3d6-40c2-8457-bca9bdbf321c", "node_type": "1", "metadata": {}, "hash": "49cd7c28b73ca02fe35e54edcf9872a6c7dd62a59345949e826b58fb89b512aa", "class_name": "RelatedNodeInfo"}}, "text": "##  What is Lyzr?\n\n[ Lyzr ](https://www.lyzr.ai/) is a full-stack agent framework that\nspecializes in building fully autonomous AI agents for enterprises. Their\nfocus is on achieving Organizational General Intelligence (OGI) by harnessing\nagent data. Lyzr offers pre-built agents like [ Jazon\n](https://www.lyzr.ai/jazon/) , an AI sales development representative, and [\nSkott ](https://www.lyzr.ai/skott/) , an AI content marketing agent, as well\nas a no-code builder for custom agent creation. Their platform enables\norganizations to build, deploy, and manage AI agents that can handle complex\ntasks and workflows autonomously.\n\n##  How does LlamaIndex help?\n\nLlamaIndex plays a crucial role in Lyzr's technology stack:\n\n  1. **Context augmentation** : LlamaIndex components supply essential context to Lyzr's agents, creating Retrieval-Augmented Generation (RAG) systems that enable them to perform focused and effective work. \n  2. **Custom data access** : LlamaIndex\u2019s data connectors are the preferred way for Lyzr agents to access customer-specific information. LlamaIndex\u2019s huge library of connectors means they can connect no matter where the customer stores their data. \n  3. **Flexible retrieval** : Lyzr uses LlamaIndex's customizable retrieval methods to optimize performance for different use cases. Lyzr\u2019s AutoRAG system determines the optimal retrieval model, chunk size, and other parameters based on input data and use case and passes those to LlamaIndex. \n\n##  What have the results been like?\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1514, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "218276ac-b3d6-40c2-8457-bca9bdbf321c": {"__data__": {"id_": "218276ac-b3d6-40c2-8457-bca9bdbf321c", "embedding": null, "metadata": {"filename": "case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex.md", "extension": ".md", "title": "Case study: Lyzr: Taking autonomous AI agents to $1M+ ARR with LlamaIndex", "date": "Jul 10, 2024", "url": "https://www.llamaindex.ai/blog/case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f3d8b800-430b-447e-8711-5a07466261ac", "node_type": "4", "metadata": {"filename": "case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex.md", "extension": ".md", "title": "Case study: Lyzr: Taking autonomous AI agents to $1M+ ARR with LlamaIndex", "date": "Jul 10, 2024", "url": "https://www.llamaindex.ai/blog/case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex"}, "hash": "7f944df3b251fb2cf997461955665d4601fcff11247de437c258aca9e2c16ec8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f52edd71-c7cd-4b8d-95c0-edadb1bff567", "node_type": "1", "metadata": {"filename": "case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex.md", "extension": ".md", "title": "Case study: Lyzr: Taking autonomous AI agents to $1M+ ARR with LlamaIndex", "date": "Jul 10, 2024", "url": "https://www.llamaindex.ai/blog/case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex"}, "hash": "ee6ed2f2bc4429370652f6de67e1fe55fb4b525b120eb57f37444287d75f0100", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10e1a522-d29f-4053-9922-b16e5104465f", "node_type": "1", "metadata": {}, "hash": "c51fce3bae22876c28a6d5c6d5958e010d963cd661940717fa24b3a6d083be26", "class_name": "RelatedNodeInfo"}}, "text": "The integration of LlamaIndex into Lyzr's framework has contributed to\nsignificant growth and improved performance:\n\n**1\\. Rapid revenue growth** : Lyzr's annual recurring revenue jumped from\naround $100,000 to about $1.5 million in less than 60 days.\n\n**2\\. ", "mimetype": "text/plain", "start_char_idx": 1514, "end_char_idx": 1773, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10e1a522-d29f-4053-9922-b16e5104465f": {"__data__": {"id_": "10e1a522-d29f-4053-9922-b16e5104465f", "embedding": null, "metadata": {"filename": "case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex.md", "extension": ".md", "title": "Case study: Lyzr: Taking autonomous AI agents to $1M+ ARR with LlamaIndex", "date": "Jul 10, 2024", "url": "https://www.llamaindex.ai/blog/case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f3d8b800-430b-447e-8711-5a07466261ac", "node_type": "4", "metadata": {"filename": "case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex.md", "extension": ".md", "title": "Case study: Lyzr: Taking autonomous AI agents to $1M+ ARR with LlamaIndex", "date": "Jul 10, 2024", "url": "https://www.llamaindex.ai/blog/case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex"}, "hash": "7f944df3b251fb2cf997461955665d4601fcff11247de437c258aca9e2c16ec8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "218276ac-b3d6-40c2-8457-bca9bdbf321c", "node_type": "1", "metadata": {"filename": "case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex.md", "extension": ".md", "title": "Case study: Lyzr: Taking autonomous AI agents to $1M+ ARR with LlamaIndex", "date": "Jul 10, 2024", "url": "https://www.llamaindex.ai/blog/case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex"}, "hash": "fd438493268abb16f7c79a5559d496c722c5b2c66f7d38b21e10435c0478e1e6", "class_name": "RelatedNodeInfo"}}, "text": "Enhanced agent accuracy** : LlamaIndex's advanced capabilities and\naccurate RAG have led to highly accurate agents with very low error rates,\nproviding a great alternative to OpenAI\u2019s Assistant API.\n\n**3\\. Scalability** : The flexibility provided by LlamaIndex has allowed Lyzr\nto sustain its growth and expand its agent offerings.\n\n##  What do customers think?\n\nCustomer reception of Lyzr's LlamaIndex-powered agents has been overwhelmingly\npositive:\n\n**1\\. High adoption rate** : 75% of Lyzr's customers use two or more AI agents\nincluding custom workflow agents, indicating strong adoption of Lyzr Agent\nFramework.\n\n**2\\. Customer Persona** : Lyzr\u2019s \u2018fully autonomous\u2019 AI agents seem to have\ncaptured customer\u2019s imagination with SaaS CTOs being the primary adopter of\nLyzr AI Agents to automate their backend workflows in a more reliable, secure\nand predictable manner.\n\n**3\\. Positive testimonials** : Customers like SurePeople love Lyzr:\n\n> \"SurePeople is delighted to announce our partnership with Lyzr.ai, a key\n> player in fortifying the scalability, security, and future-readiness of our\n> AI infrastructure. Thanks to their versatile Agents, we're empowered to\n> operate at the forefront of innovation, underpinned by a robust framework\n> that bolsters our AI applications. In an ever-evolving landscape of\n> artificial intelligence, Lyzr.ai's Agents ensure we remain at the cutting\n> edge. Additionally, our collaboration has been enriched by their\n> exceptionally skilled and cooperative team.\" \u2013 Niko Drakoulis, CEO of\n> SurePeople\n\n##  What's next for Lyzr?\n\nLyzr has ambitious plans for the future, building on their success with\nLlamaIndex, including new agents such as [ Kathy ](https://www.lyzr.ai/kathy/)\n, an AI competitor analyst, and Diane, an AI HR agent, with several others in\nthe pipeline. They\u2019re also developing a framework called Lyzr AgentMesh to\nenable interaction between these different AI agents, creating a cohesive AI-\ndriven workforce.\n\n> \u201cWe are extremely thankful to Jerry and team for bringing LlamaIndex to the\n> AI community. You guys have saved countless hours of tackling data retrieval\n> challenges for us and many other builders in this space.\u201d - Siva Surendira,\n> Founder, Lyzr AI.\n\nBy continuing to use LlamaIndex as their RAG partner, Lyzr is well-positioned\nto expand its offerings and further establish itself as a leader in autonomous\nAI agent technology for enterprises.\n\n##  Want to see what LlamaCloud can do for you?\n\n[ Sign up for LlamaCloud ](https://cloud.llamaindex.ai) and [ get on the\nwaitlist\n](https://docs.google.com/forms/d/e/1FAIpQLSdehUJJB4NIYfrPIKoFdF4j8kyfnLhMSH_qYJI_WGQbDWD25A/viewform)\nfor full access!\n\n", "mimetype": "text/plain", "start_char_idx": 1773, "end_char_idx": 4450, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb177c3e-fa44-4dd5-bc55-fa99ff7a7799": {"__data__": {"id_": "eb177c3e-fa44-4dd5-bc55-fa99ff7a7799", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-07-09.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-09", "date": "Jul 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-09"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "70ccc5c6-8e72-4a65-a7b2-86bf59b26221", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-07-09.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-09", "date": "Jul 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-09"}, "hash": "718ceb5e633f38cfb80f2ad3b2b9eecd0dd5edb4ef61a2142b0e0789eded9c48", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4dfce439-df67-4e9b-ba22-a1512a12d200", "node_type": "1", "metadata": {}, "hash": "af184d9ce90678c8dbd56cc4b201c94ba25bbd5f82e0351a0cc6d272e247577f", "class_name": "RelatedNodeInfo"}}, "text": "Hello, Llama Lovers!\n\nWelcome to this week\u2019s edition of the LlamaIndex newsletter! We\u2019re thrilled to\nshare some exciting updates about ` llama-agents ` , along with demos,\nextensive guides, and in-depth tutorials to enhance your understanding of our\ntools.\n\nBefore we dive into our newsletter, we\u2019re excited to announce the return of\nCommunity Office Hours. If you have use-cases, in-depth questions, or feedback\nfor the team at LlamaIndex, join us during our community office hours! We\u2019ll\nset up a 15-30 minute Zoom call to discuss it.\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 538, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4dfce439-df67-4e9b-ba22-a1512a12d200": {"__data__": {"id_": "4dfce439-df67-4e9b-ba22-a1512a12d200", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-07-09.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-09", "date": "Jul 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-09"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "70ccc5c6-8e72-4a65-a7b2-86bf59b26221", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-07-09.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-09", "date": "Jul 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-09"}, "hash": "718ceb5e633f38cfb80f2ad3b2b9eecd0dd5edb4ef61a2142b0e0789eded9c48", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb177c3e-fa44-4dd5-bc55-fa99ff7a7799", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-07-09.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-09", "date": "Jul 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-09"}, "hash": "2c60f72121145aa7fc25311840740b9f77848408af59dbbf9d44eb3235725ea6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b25908b-73ad-4503-90c1-74d98f67a3f4", "node_type": "1", "metadata": {}, "hash": "cdd1024efa344da35b9960178d0c342fbbf88bab49e9e6db1ab4a0aadec0d6a3", "class_name": "RelatedNodeInfo"}}, "text": "[ **Sign up here**\n](https://docs.google.com/forms/d/e/1FAIpQLSefrnmxQWD-1OhSP51kUKtdbw9EGDjrMLefkZFACKD19TKsuQ/viewform?usp=sf_link)\nto participate.\n\n##  **The highlights:**\n\n  * **Multi-Agent Kubernetes Kit Launched:** Deploy multi-agent systems easily with our new Kubernetes Starter Kit featuring ready-to-use tools and configurations. [ Notebook ](https://github.com/run-llama/llama-agents/tree/main/examples/docker-kubernetes) , [ Tweet ](https://x.com/llama_index/status/1807801281324765469) . \n  * **Enhanced Communication with RabbitMQ:** Boost multi-agent system reliability and scalability in production with our new RabbitMQ integration. [ Notebook ](https://github.com/run-llama/llama-agents/tree/main/examples/rabbitmq) , [ Tweet ](https://x.com/llama_index/status/1810342085171855753) . \n  * **Reflection as a Service Guide:** Improve agent reliability with our guide on building Reflection as a Service, perfect for output validation and correction. [ Notebook ](https://github.com/run-llama/llama-agents/blob/main/examples/reflection/toxicity_reflection_service.ipynb) , [ Tweet ](https://x.com/llama_index/status/1808898730638389262) . \n  * **Corrective RAG as a Service Guide:** Create a self-correcting RAG that ensures context relevance and integrates search fallbacks before generation. [ Notebook ](https://github.com/run-llama/llama-agents/blob/main/examples/corrective_rag.ipynb) , [ Tweet ](https://x.com/llama_index/status/1809282069606068486) . \n  * **Tutorial series on Property Graphs:** 6-part video series on Property Graphs in LlamaIndex using MistralAI, Neo4j, and Ollama. [ Videos ](https://www.youtube.com/playlist?list=PLTZkGHtR085ZYstpcTFWqP27D-SPZe6EZ) , [ Tweet ](https://x.com/llama_index/status/1810410943215710510) . \n\n", "mimetype": "text/plain", "start_char_idx": 538, "end_char_idx": 2300, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b25908b-73ad-4503-90c1-74d98f67a3f4": {"__data__": {"id_": "5b25908b-73ad-4503-90c1-74d98f67a3f4", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-07-09.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-09", "date": "Jul 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-09"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "70ccc5c6-8e72-4a65-a7b2-86bf59b26221", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-07-09.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-09", "date": "Jul 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-09"}, "hash": "718ceb5e633f38cfb80f2ad3b2b9eecd0dd5edb4ef61a2142b0e0789eded9c48", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4dfce439-df67-4e9b-ba22-a1512a12d200", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-07-09.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-09", "date": "Jul 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-09"}, "hash": "50d9689198a746823c2ab2d5157a3c203b843b34a0d6082bde404f48d2e075df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c3b96c4b-e570-46bb-a032-da22dd9edc86", "node_type": "1", "metadata": {}, "hash": "d7aab5a6ebe6440295cf79bfd38e2819d2f002cbcb13d832c1e14e2531abb44c", "class_name": "RelatedNodeInfo"}}, "text": "##  **Feature Releases and Enhancements:**\n\n  1. We have launched a Multi-Agent on Kubernetes Starter Kit to build and deploy a multi-agent system using Docker Compose and Kubernetes using llama-agents. This kit includes prebuilt agent loops and tools, as well as Dockerfiles and Kubernetes manifests for easy production deployment. [ Notebook ](https://github.com/run-llama/llama-agents/tree/main/examples/docker-kubernetes) , [ Tweet ](https://x.com/llama_index/status/1807801281324765469) . \n  2. We have integrated RabbitMQ with llama-agents to enhance multi-agent communication, offering scalability and reliability for handling large request volumes in production. [ Notebook ](https://github.com/run-llama/llama-agents/tree/main/examples/rabbitmq) , [ Tweet ](https://x.com/llama_index/status/1810342085171855753) . \n  3. [ [ Yi-01.AI ](http://Yi-01.AI) ]( [ http://Yi-01.AIhttps ](http://Yi-01.AIhttps) ://x.com/01AI_Yi) is integrated with LlamaIndex for enhanced retrieval and indexing, streamlining the development of smarter, faster RAG applications. [ Docs ](https://docs.llamaindex.ai/en/latest/examples/llm/yi/) . \n  ", "mimetype": "text/plain", "start_char_idx": 2300, "end_char_idx": 3431, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c3b96c4b-e570-46bb-a032-da22dd9edc86": {"__data__": {"id_": "c3b96c4b-e570-46bb-a032-da22dd9edc86", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-07-09.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-09", "date": "Jul 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-09"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "70ccc5c6-8e72-4a65-a7b2-86bf59b26221", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-07-09.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-09", "date": "Jul 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-09"}, "hash": "718ceb5e633f38cfb80f2ad3b2b9eecd0dd5edb4ef61a2142b0e0789eded9c48", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b25908b-73ad-4503-90c1-74d98f67a3f4", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-07-09.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-09", "date": "Jul 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-09"}, "hash": "ad50876014d6bbbedd6e7c2bfea3a39a3ede8ad6085e869f8c217c16e588432d", "class_name": "RelatedNodeInfo"}}, "text": "4. We have launched a [ 6-part video series ](https://www.youtube.com/playlist?list=PLTZkGHtR085ZYstpcTFWqP27D-SPZe6EZ) on Property Graphs in LlamaIndex using MistralAI, Neo4j and Ollama. [ Tweet ](https://x.com/llama_index/status/1810410943215710510) . \n\n##  **Demos:**\n\n  * [ OpenContracts ](https://github.com/JSv4/OpenContracts) by [ John Scrudato ](https://x.com/johnscrudato) : A fully open-source, AI-powered Document Analytics Tool, integrates genAI capabilities and LlamaIndex for robust query handling and data extraction across documents. This tool is particularly valuable for legal analysis, enabling users to manage, process, and query vast arrays of contracts and legal documents. [ Docs ](https://jsv4.github.io/OpenContracts/) . \n\n##  **Guides:**\n\n  * Guide to build Reflection as a Service to enhance agent reliability with our new standalone service, ideal for validating and correcting outputs across multiple agents. [ Notebook ](https://github.com/run-llama/llama-agents/blob/main/examples/reflection/toxicity_reflection_service.ipynb) , [ Tweet ](https://x.com/llama_index/status/1808898730638389262) . \n  * Guide to build Corrective RAG as a Service, a self-correcting RAG that dynamically validates context relevance, seamlessly integrating web search fallbacks before generation. [ Notebook ](https://github.com/run-llama/llama-agents/blob/main/examples/corrective_rag.ipynb) , [ Tweet ](https://x.com/llama_index/status/1809282069606068486) . \n\n##  **Tutorials:**\n\n  * [ Pavan Kumar\u2019s ](https://x.com/pavan_mantha1) [ tutorial ](https://blog.gopenai.com/harnessing-ai-at-the-edge-building-a-rag-system-with-ollama-qdrant-and-raspberry-pi-45ac3212cf75) to build a RAG pipeline that lives on a Raspberry Pi device with docker, Ollama, Qdrant, and using LlamaIndex as the orchestration layer. \n  * [ Trade Mamba\u2019s ](https://x.com/AdiDror6) video [ tutorial ](https://www.youtube.com/watch?v=uOLhleiOM84) to build an AI-enabled trading assistant using LlamaIndex\u2019s agent/tool/RAG abstractions for tasks like tracking portfolio values, managing stock orders, and conducting vector searches for semantic information. \n  * [ Giskard\u2019s ](https://x.com/giskard_ai) [ toolkit ](https://docs.giskard.ai/en/stable/reference/notebooks/RAGET.html) enables diverse question generation featuring question types like simple, complex, distracting, situational, double, and conversational for RAG evaluation, as demonstrated in the tutorial on using a LlamaIndex pipeline with an IPCC Climate Report. \n  * [ Pavan Kumar\u2019s ](https://x.com/pavan_mantha1) [ tutorial ](https://blog.stackademic.com/building-a-multi-document-react-agent-for-financial-analysis-using-llamaindex-and-qdrant-72a535730ac3) demonstrates building a Multi-Document Financial Analyst Agent using LlamaIndex RAG and ReAct tools, analyzing categorized SEC documents with SnowflakeDB embeddings and MistralAI via Ollama. \n  * Ross A.\u2019s [ tutorial ](https://medium.com/@rossashman/the-art-of-rag-part-4-retrieval-evaluation-427bb5db0475) on retrieval evaluations for RAG delves into essential metrics like precision@K and NDCG, and demonstrates how to convert datasets to BEIR format for assessing LlamaIndex retrievers. \n\n##  **Webinar:**\n\n  * Join us for a [ webinar ](https://lu.ma/dywrdye5) on July 10th, featuring Jerry Liu (LlamaIndex) and Ayush Thakur (Weights & Biases) on **A Principled Approach to RAG Experimentation + Evaluation** to learn how to build, evaluate, and refine RAG pipelines. \n\n", "mimetype": "text/plain", "start_char_idx": 3431, "end_char_idx": 6910, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45a52a84-1325-405e-8b93-d8c99499409a": {"__data__": {"id_": "45a52a84-1325-405e-8b93-d8c99499409a", "embedding": null, "metadata": {"filename": "llamacloud-built-for-enterprise-llm-app-builders.md", "extension": ".md", "title": "LlamaCloud - Built for Enterprise LLM App Builders", "date": "Jul 9, 2024", "url": "https://www.llamaindex.ai/blog/llamacloud-built-for-enterprise-llm-app-builders"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "daf66bb2-a0d8-4f36-a6e5-40ecb0ad9745", "node_type": "4", "metadata": {"filename": "llamacloud-built-for-enterprise-llm-app-builders.md", "extension": ".md", "title": "LlamaCloud - Built for Enterprise LLM App Builders", "date": "Jul 9, 2024", "url": "https://www.llamaindex.ai/blog/llamacloud-built-for-enterprise-llm-app-builders"}, "hash": "6c0ceddf9c7d00d0c9d482c697f0b92bd1ef68edad142fee4498b691cbde00a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3b07c3c-9974-4085-81b0-7c52c23e1343", "node_type": "1", "metadata": {}, "hash": "972e9b4a2e16404c7cb7beaf506a975cca3c91b5438a930290807f1f54add1bf", "class_name": "RelatedNodeInfo"}}, "text": "##  RAG is only as Good as your Data\n\nBuilding production-ready LLM applications is hard. We've been chatting with\nhundreds of users, ranging from Fortune 500 enterprises to pre-seed startups\nand here's what they tell us they struggle with:\n\n  * **Data Quality Issues** : Most companies deal with large sets of complex, heterogeneous documents. Think PDFs with messy formatting, images, tables across multiple pages, different languages - the list goes on. Ensuring high-quality data input is crucial. \"Garbage in, garbage out\" holds especially true for LLM applications. \n  * **Scalability Hurdles** : Each new data source requires significant engineering hours for custom parsing and tuning. Keeping data sources in sync isn't easy either. \n  * **Accuracy Concerns** : Bad retrievals and hallucinations are common problems when LLMs interact with enterprise data, leading to unreliable outputs. \n  * **Configuration Overload:** Fine-tuning LLM applications involves numerous parameters and often requires deep technical expertise, making iterative improvement a daunting task. \n\nAs developers shift from prototypes towards building production applications -\ncomplex orchestration is needed and they want to centralize their abstractions\nfor managing their data. They want a unified interface for processing and\nretrieving over their diverse sources of data.\n\nTo address these difficulties, we soft-launched LlamaCloud and made LlamaParse\nwidely available a few months ago to bring production-grade context-\naugmentation to your LLM and RAG applications. LlamaParse can already support\n50+ languages and 100+ document formats. The adoption has been incredible - we\nhave grown to tens of thousands of active users for LlamaParse who have\nprocessed tens of million pages! Here\u2019s an example from Dean Barr, Applied AI\nLead at Carlyle:\n\n> As an AI Applied Data Scientist who was granted one of the first ML patents\n> in the U.S., and who is building cutting-edge AI capabilities at one of the\n> world's largest Private Equity Funds, I can confidently say that LlamaParse\n> from LlamaIndex is currently the best technology I have seen for parsing\n> complex document structures for Enterprise RAG pipelines. Its ability to\n> preserve nested tables, extract challenging spatial layouts, and images is\n> key to maintaining data integrity in advanced RAG and agentic model\n> building.\n\n##  The Rise of Centralized Knowledge Management\n\nWe have designed LlamaCloud to cater to the need of **production-grade**\n**context-augmentation** for your LLM and RAG applications. Let's take a tour\nof what LlamaCloud brings to the table:\n\n  1. **LlamaParse** : Our state-of-the-art parser that turns complex documents with tables and charts into LLM-friendly formats. You can learn more about [ LlamaParse here ](https://docs.cloud.llamaindex.ai/llamaparse/getting_started) . \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2860, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3b07c3c-9974-4085-81b0-7c52c23e1343": {"__data__": {"id_": "b3b07c3c-9974-4085-81b0-7c52c23e1343", "embedding": null, "metadata": {"filename": "llamacloud-built-for-enterprise-llm-app-builders.md", "extension": ".md", "title": "LlamaCloud - Built for Enterprise LLM App Builders", "date": "Jul 9, 2024", "url": "https://www.llamaindex.ai/blog/llamacloud-built-for-enterprise-llm-app-builders"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "daf66bb2-a0d8-4f36-a6e5-40ecb0ad9745", "node_type": "4", "metadata": {"filename": "llamacloud-built-for-enterprise-llm-app-builders.md", "extension": ".md", "title": "LlamaCloud - Built for Enterprise LLM App Builders", "date": "Jul 9, 2024", "url": "https://www.llamaindex.ai/blog/llamacloud-built-for-enterprise-llm-app-builders"}, "hash": "6c0ceddf9c7d00d0c9d482c697f0b92bd1ef68edad142fee4498b691cbde00a9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45a52a84-1325-405e-8b93-d8c99499409a", "node_type": "1", "metadata": {"filename": "llamacloud-built-for-enterprise-llm-app-builders.md", "extension": ".md", "title": "LlamaCloud - Built for Enterprise LLM App Builders", "date": "Jul 9, 2024", "url": "https://www.llamaindex.ai/blog/llamacloud-built-for-enterprise-llm-app-builders"}, "hash": "954fb6b80501f518b81cd3e952f00a9f66860aa340cc2b01c0726362aa55038d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33bf00bb-dc2b-4f24-9145-449e9a06f346", "node_type": "1", "metadata": {}, "hash": "03f827b08cbef58a24aca1458baf38877b49e5502f1c4ca93bbc9a0aa9a7e158", "class_name": "RelatedNodeInfo"}}, "text": "2. **Managed Ingestion** : Connect to enterprise data sources and your choice of data sinks with ease. We support [ multiple data sources ](https://docs.cloud.llamaindex.ai/llamacloud/data_sources) and are adding more. LlamaCloud provides default parsing configurations for generating vector embeddings, while also allowing deep customization for specific applications. \n  3. **Advanced Retrieval** : LlamaCloud allows basic semantic search retrieval as well as advanced techniques like hybrid search, reranking, and metadata filtering to improve the accuracy of the retrieval. This provides the necessary configurability to build end to end RAG over complex documents. \n  4. **LlamaCloud Playground** : An interactive UI to test and refine your ingestion and retrieval strategies before deployment. \n  5. ", "mimetype": "text/plain", "start_char_idx": 2860, "end_char_idx": 3666, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33bf00bb-dc2b-4f24-9145-449e9a06f346": {"__data__": {"id_": "33bf00bb-dc2b-4f24-9145-449e9a06f346", "embedding": null, "metadata": {"filename": "llamacloud-built-for-enterprise-llm-app-builders.md", "extension": ".md", "title": "LlamaCloud - Built for Enterprise LLM App Builders", "date": "Jul 9, 2024", "url": "https://www.llamaindex.ai/blog/llamacloud-built-for-enterprise-llm-app-builders"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "daf66bb2-a0d8-4f36-a6e5-40ecb0ad9745", "node_type": "4", "metadata": {"filename": "llamacloud-built-for-enterprise-llm-app-builders.md", "extension": ".md", "title": "LlamaCloud - Built for Enterprise LLM App Builders", "date": "Jul 9, 2024", "url": "https://www.llamaindex.ai/blog/llamacloud-built-for-enterprise-llm-app-builders"}, "hash": "6c0ceddf9c7d00d0c9d482c697f0b92bd1ef68edad142fee4498b691cbde00a9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b3b07c3c-9974-4085-81b0-7c52c23e1343", "node_type": "1", "metadata": {"filename": "llamacloud-built-for-enterprise-llm-app-builders.md", "extension": ".md", "title": "LlamaCloud - Built for Enterprise LLM App Builders", "date": "Jul 9, 2024", "url": "https://www.llamaindex.ai/blog/llamacloud-built-for-enterprise-llm-app-builders"}, "hash": "a35df7cbea436d041f935a05021e59f7449b57dad823c97024c2f6d12b29db91", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "917501e0-e0ed-4a8d-b7ea-34abf7ea6265", "node_type": "1", "metadata": {}, "hash": "c2213cb4b8860281c43c86d52ef4ae41d540032c8ca03f69fbd73a56ebe1b6d3", "class_name": "RelatedNodeInfo"}}, "text": "**Scalability and Security** : Handle large volumes of production data. Compliance certifications as well as deployment options are available based on your security needs. \n\nThis video gives a detailed walk through of LlamaCloud:\n\nOur customers tell us that LlamaCloud enables developers to spend less time\nsetting up and iterating on their data pipelines for LLM use cases, allowing\nthem to iterate through the LLM application development lifecycle much more\nquickly. Here\u2019s what Teemu Lahdenpera, CTO at [ Scaleport.ai\n](http://Scaleport.ai) had to say:\n\n> LlamaCloud has really sped up our development timelines. Getting to\n> technical prototypes quickly allows us to show tangible value instantly,\n> improving our sales outcomes. When needed, switching from the LlamaCloud UI\n> to code has been really seamless. The configurable parsing and retrieval\n> features have significantly improved our response accuracy.\n\n> We've also seen great results with LlamaParse and found it outperforming\n> GPT-4 vision on some OCR tasks!\n\n##  Try it yourself\n\nWe\u2019ve opened up an official waitlist for LlamaCloud. Here's how you can get\ninvolved:\n\n  1. **Join the LlamaCloud Waitlist** : [ Sign up here ](https://docs.google.com/forms/d/e/1FAIpQLSdehUJJB4NIYfrPIKoFdF4j8kyfnLhMSH_qYJI_WGQbDWD25A/viewform) . \n  2. ", "mimetype": "text/plain", "start_char_idx": 3666, "end_char_idx": 4968, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "917501e0-e0ed-4a8d-b7ea-34abf7ea6265": {"__data__": {"id_": "917501e0-e0ed-4a8d-b7ea-34abf7ea6265", "embedding": null, "metadata": {"filename": "llamacloud-built-for-enterprise-llm-app-builders.md", "extension": ".md", "title": "LlamaCloud - Built for Enterprise LLM App Builders", "date": "Jul 9, 2024", "url": "https://www.llamaindex.ai/blog/llamacloud-built-for-enterprise-llm-app-builders"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "daf66bb2-a0d8-4f36-a6e5-40ecb0ad9745", "node_type": "4", "metadata": {"filename": "llamacloud-built-for-enterprise-llm-app-builders.md", "extension": ".md", "title": "LlamaCloud - Built for Enterprise LLM App Builders", "date": "Jul 9, 2024", "url": "https://www.llamaindex.ai/blog/llamacloud-built-for-enterprise-llm-app-builders"}, "hash": "6c0ceddf9c7d00d0c9d482c697f0b92bd1ef68edad142fee4498b691cbde00a9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33bf00bb-dc2b-4f24-9145-449e9a06f346", "node_type": "1", "metadata": {"filename": "llamacloud-built-for-enterprise-llm-app-builders.md", "extension": ".md", "title": "LlamaCloud - Built for Enterprise LLM App Builders", "date": "Jul 9, 2024", "url": "https://www.llamaindex.ai/blog/llamacloud-built-for-enterprise-llm-app-builders"}, "hash": "1b9a05e67d1a0d9230e1e13459a898820763ca66893889fa914dfe8f4d24ba9b", "class_name": "RelatedNodeInfo"}}, "text": "**Get in Touch** : Have questions? Want to discuss unlimited commercial use? [ Contact us ](https://www.llamaindex.ai/contact) and let's chat! Note: we support private deployments for a select number of enterprises \n  3. **Stay Updated** : Follow us on [ Twitter ](https://twitter.com/llama_index) and join our [ Discord community ](https://discord.gg/dGcwcsnxhU) to stay in the loop. \n\nIn the meantime, anyone can create an account at [\nhttps://cloud.llamaindex.ai/ ](https://cloud.llamaindex.ai/) . While you\u2019re\nwaiting for official LlamaCloud access, anyone can immediately start using our\nLlamaParse APIs.\n\nWe\u2019re shipping a _lot_ of features in the next few weeks. We look forward to\nseeing the context-augmented LLM applications that you can build on top of\nLlamaCloud!\n\n**FAQ**\n\n**Have you got some examples of how to use LlamaCloud?**\n\nWe sure do! One of the strengths of LlamaCloud is how easily the endpoints\nintegrate into your existing code. Our [ llamacloud-demo repo\n](https://github.com/run-llama/llamacloud-demo) has lots of examples from [\ngetting started ](https://github.com/run-llama/llamacloud-\ndemo/blob/main/examples/getting_started.ipynb) to [ running evaluations\n](https://github.com/run-llama/llamacloud-\ndemo/blob/main/examples/batch_eval.ipynb) .\n\n**Is this competitive with vector databases?**\n\nNo. LlamaCloud is focused primarily on data parsing and ingestion, which is a\ncomplementary layer to any vector storage provider. The retrieval layer is\norchestration on top of an existing storage system. LlamaIndex open-source\nintegrates with 40+ of the most popular vector databases, and we are\nintegrating LlamaCloud with storage providers based on customer requests.\n\n", "mimetype": "text/plain", "start_char_idx": 4968, "end_char_idx": 6663, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fcd3e45d-e6f7-4965-840f-ddb8004dfebe": {"__data__": {"id_": "fcd3e45d-e6f7-4965-840f-ddb8004dfebe", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-07-02.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-02", "date": "Jul 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-02"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "71e4c768-21b6-44c5-9a87-672c07afb160", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-07-02.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-02", "date": "Jul 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-02"}, "hash": "b641d9448e8bf374b3d0b7cce27d038f1089d2e268131320ce79dbbd817e1af9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d08611c-16e1-4579-aa4f-8f55231f8cda", "node_type": "1", "metadata": {}, "hash": "ab56b046f39288c03a7b0afab836dddfd0200a5329a9a4976de4382ee5854231", "class_name": "RelatedNodeInfo"}}, "text": "Hello, Llama enthusiasts!\n\nWelcome to this week\u2019s edition of the LlamaIndex newsletter! In this issue,\nwe\u2019re excited to bring you exciting updates about ` llama-agents ` , live\ndemos, extensive guides, and in-depth tutorials to enhance your understanding\nof our tools.\n\nBefore moving into our newsletter, we have an exciting update on our\nenterprise offerings. We are thrilled to announce the waitlist release of\nLlamaCloud, our fully-managed ingestion service. [ Sign up\n](http://bit.ly/llamacloud) now if you\u2019re eager to collaborate and build LLM\napplications with LlamaCloud.\n\n##  **The highlights:**\n\n  * **Launched Llama-Agents Framework:** Our new alpha-release, llama-agents, enables multi-agent AI systems for production with a distributed architecture, HTTP API communication, and agentic orchestration. It\u2019s designed for easy deployment, scalability, and observability. [ Blogpost ](https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems) , [ Tweet ](https://x.com/llama_index/status/1806116419995844947) . \n  * **` create-llama ` Integrated with LlamaCloud: ** Streamline your LLM application data pipelines with create-llama, now integrated with LlamaCloud for faster setup and efficient system maintenance. [ Tweet ](https://x.com/MarcusSchiesser/status/1806960577299767767) . \n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1361, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d08611c-16e1-4579-aa4f-8f55231f8cda": {"__data__": {"id_": "6d08611c-16e1-4579-aa4f-8f55231f8cda", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-07-02.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-02", "date": "Jul 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-02"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "71e4c768-21b6-44c5-9a87-672c07afb160", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-07-02.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-02", "date": "Jul 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-02"}, "hash": "b641d9448e8bf374b3d0b7cce27d038f1089d2e268131320ce79dbbd817e1af9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fcd3e45d-e6f7-4965-840f-ddb8004dfebe", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-07-02.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-02", "date": "Jul 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-02"}, "hash": "6ba415d506aa49137d995780e8773d874eb5285617a2c44178942383c6a5dea7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28cfa0ba-2816-42ab-bf07-a970a381109a", "node_type": "1", "metadata": {}, "hash": "28b8529efa60ec4ae0512abd4e08371f095fa7166eafd2a57e8bf491bf89f1a6", "class_name": "RelatedNodeInfo"}}, "text": "##  **Feature Releases and Enhancements:**\n\n  1. We have launched llama-agents - new alpha-release framework that enables multi-agent AI systems to go into production. It features a distributed, service-oriented architecture, communication through standard HTTP APIs, agentic orchestration of flows, and is designed for easy deployment, scalability, and observability. [ Blogpost ](https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems) , [ Tweet ](https://x.com/llama_index/status/1806116419995844947) . \n  2. create-llama is now integrated with LlamaCloud to streamline the setup and management of data pipelines for LLM applications, providing a fast and efficient way to deploy and maintain these systems. [ Tweet ](https://x.com/MarcusSchiesser/status/1806960577299767767) . \n  ", "mimetype": "text/plain", "start_char_idx": 1361, "end_char_idx": 2214, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28cfa0ba-2816-42ab-bf07-a970a381109a": {"__data__": {"id_": "28cfa0ba-2816-42ab-bf07-a970a381109a", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-07-02.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-02", "date": "Jul 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-02"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "71e4c768-21b6-44c5-9a87-672c07afb160", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-07-02.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-02", "date": "Jul 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-02"}, "hash": "b641d9448e8bf374b3d0b7cce27d038f1089d2e268131320ce79dbbd817e1af9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d08611c-16e1-4579-aa4f-8f55231f8cda", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-07-02.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-02", "date": "Jul 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-02"}, "hash": "4296f2b4fc60a9bdff83b150687d5053b3993eb0ca84b7e1e98c0c6d82d8a8fc", "class_name": "RelatedNodeInfo"}}, "text": "3. We have integrated with DSPy for Optimized RAG by utilizing DSPy\u2019s optimization capabilities with LlamaIndex\u2019s data tools to enhance your query pipelines, optimize prompts, or repurpose DSPy predictors. [ Cookbook ](https://github.com/stanfordnlp/dspy/blob/main/examples/llamaindex/dspy_llamaindex_rag.ipynb) , [ Tweet ](https://x.com/llama_index/status/1805622494130586078) . \n\n##  **Demos:**\n\n  * Automating Code Reviews, [ project ](https://x.com/GanatraSoham/status/1807787558157320376) by [ Composio ](https://x.com/composiohq) with LlamaIndex automates code reviews using an AI agent in under 100 lines of code that monitors GitHub PRs, reviews them immediately upon creation, and posts feedback directly to your Slack channel. [ Codebase ](https://github.com/ComposioHQ/composio/tree/master/python/examples/pr_agent/pr_agent_llama_index) . \n\n##  **Guides:**\n\n  * [ Guide ](https://github.com/run-llama/llama-agents/blob/main/examples/agentic_rag_toolservice.ipynb) to Building an Agentic RAG Service with our comprehensive notebook that walks you through creating vector indexes, transforming them into query engines, turning each engine into a tool, providing these tools to agents, and launching the agents as services. \n  * Guide to AI Agents with LlamaIndex: Andrei\u2019s comprehensive workshop from Gen AI Philippines, showcasing LLM applications through LlamaIndex. This beginner-friendly session covers topics from RAG to multi-hop agents. [ Video ](https://drive.google.com/file/d/1kInT-szYWH71DvKvhhE5XAUVhTl8ZXJA/view) , [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/docs/presentations/materials/2024-06-22-genai-philippines.ipynb) . \n\n##  **Tutorials:**\n\n  * [ Kingzzm\u2019s ](https://x.com/kingzzm) [ tutorial ](https://generativeai.pub/advanced-rag-retrieval-strategies-hybrid-retrieval-997d39659720) on crafting a custom hybrid retriever using LlamaIndex\u2019s flexible abstractions. This tutorial teaches you how to integrate full text and dense search capabilities from Elastic, and how to write your own reciprocal rank fusion function for optimal retrieval strategy. \n  * [ Jeff\u2019s ](https://x.com/gswithai) [ tutorial ](https://www.youtube.com/watch?v=i8ldunneSW8) on which outlines the essential tools needed to construct a report generator using a ReAct agent. Learn how to integrate a RAG tool over guideline documents, a web search tool, and a report generation tool that converts markdown text into PDFs. \n  * [ 1littlecoder\u2019s ](https://x.com/1littlecoder) [ tutorial ](https://www.youtube.com/watch?v=_aTEI3ISkQA) on llama-agents provides a detailed introduction to transforming multi-agent systems into microservices for production, including setup examples and a walkthrough of the architecture involving the control plane, message queue, and agent services using LlamaIndex abstractions. \n  * [ Mervin Praison\u2019s ](https://x.com/MervinPraison) [ tutorial ](https://www.youtube.com/watch?v=nEQCpSd5mx8) on the llama-agents framework provides a concise guide to setting up agent services, from notebook synchronization to server-client interactions, complete with over 10 practical examples. \n\n", "mimetype": "text/plain", "start_char_idx": 2214, "end_char_idx": 5353, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc7557e3-de9e-49fe-b1a3-68d2c128bc4e": {"__data__": {"id_": "bc7557e3-de9e-49fe-b1a3-68d2c128bc4e", "embedding": null, "metadata": {"filename": "introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.md", "extension": ".md", "title": "Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems", "date": "Jun 26, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b6700bfd-653b-4d5f-9db9-6f04c642990b", "node_type": "4", "metadata": {"filename": "introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.md", "extension": ".md", "title": "Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems", "date": "Jun 26, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems"}, "hash": "ff4d278f758b012a8398b6484fc7080bec8fa57d827c6cd794afdb623384a11f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2e7561c-89f2-470b-9c3e-163a1014475d", "node_type": "1", "metadata": {}, "hash": "b52825c990a757c72fbd252ab4009f755448a9ce2ea6edae6044d6984ce457fe", "class_name": "RelatedNodeInfo"}}, "text": "We're excited to announce the alpha release of ` llama-agents ` , a new open-\nsource framework designed to simplify the process of building, iterating, and\ndeploying multi-agent AI systems and turn your agents into production\nmicroservices. Whether you're working on complex question-answering systems,\ncollaborative AI assistants, or distributed AI workflows, llama-agents\nprovides the tools and structure you need to bring your ideas to life.\n\n##  Key Features of llama-agents\n\n  1. **Distributed Service Oriented Architecture:** every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks. \n  2. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 713, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2e7561c-89f2-470b-9c3e-163a1014475d": {"__data__": {"id_": "f2e7561c-89f2-470b-9c3e-163a1014475d", "embedding": null, "metadata": {"filename": "introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.md", "extension": ".md", "title": "Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems", "date": "Jun 26, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b6700bfd-653b-4d5f-9db9-6f04c642990b", "node_type": "4", "metadata": {"filename": "introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.md", "extension": ".md", "title": "Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems", "date": "Jun 26, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems"}, "hash": "ff4d278f758b012a8398b6484fc7080bec8fa57d827c6cd794afdb623384a11f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc7557e3-de9e-49fe-b1a3-68d2c128bc4e", "node_type": "1", "metadata": {"filename": "introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.md", "extension": ".md", "title": "Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems", "date": "Jun 26, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems"}, "hash": "48ce886c3fe22d8170feb9b159256b56124b29c0b1ff6ce8cb626b958cd81758", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18739f06-e810-4316-aa13-cb984bbd1ea9", "node_type": "1", "metadata": {}, "hash": "6f3caf04d81881d94357c105e64870f6f4eb43e2a07ce4d8bcc01c4ca04a1b73", "class_name": "RelatedNodeInfo"}}, "text": "**Communication via standardized API interfaces:** interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue. \n  3. **Define agentic and explicit orchestration flows:** developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an \u201cagentic orchestrator\u201d that decides which agents are relevant to the task. \n  4. ", "mimetype": "text/plain", "start_char_idx": 713, "end_char_idx": 1142, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "18739f06-e810-4316-aa13-cb984bbd1ea9": {"__data__": {"id_": "18739f06-e810-4316-aa13-cb984bbd1ea9", "embedding": null, "metadata": {"filename": "introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.md", "extension": ".md", "title": "Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems", "date": "Jun 26, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b6700bfd-653b-4d5f-9db9-6f04c642990b", "node_type": "4", "metadata": {"filename": "introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.md", "extension": ".md", "title": "Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems", "date": "Jun 26, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems"}, "hash": "ff4d278f758b012a8398b6484fc7080bec8fa57d827c6cd794afdb623384a11f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2e7561c-89f2-470b-9c3e-163a1014475d", "node_type": "1", "metadata": {"filename": "introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.md", "extension": ".md", "title": "Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems", "date": "Jun 26, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems"}, "hash": "1f30327e5619b6d92f21a0413726a6b1b2aba296984c58317cc25bd710700b3b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c21b1199-0bff-4a84-8d9f-0a3d94e19682", "node_type": "1", "metadata": {}, "hash": "80e93788a359590b1f9684edc3f100f5e422714e02bf58404a941666b0ff7492", "class_name": "RelatedNodeInfo"}}, "text": "**Ease of deployment:** launch, scale and monitor each agent and your control plane independently. \n  5. **Scalability and resource management:** use our built-in observability tools to monitor the quality and performance of the system and each individual agent service \n\nLet's dive into how you can start using llama-agents to build your own multi-\nagent systems.\n\n##  Getting Started with llama-agents\n\nFirst, install the framework using pip:\n\n    \n    \n    pip install llama-agents llama-index-agent-openai\n\n###  Basic System Setup\n\nHere's a simple example of how to set up a basic multi-agent system using\nllama-agents. First we\u2019ll bring in our dependencies and set up our control\nplane, which contains our LLM-powered orchestrator\n\n    \n    \n    import dotenv\n    dotenv.load_dotenv() # our .env file defines OPENAI_API_KEY\n    from llama_agents import (\n        AgentService,\n        ControlPlaneServer,\n        SimpleMessageQueue,\n        AgentOrchestrator,\n    )\n    from llama_index.core.agent import FunctionCallingAgentWorker\n    from llama_index.core.tools import FunctionTool\n    from llama_index.llms.openai import OpenAI\n    import logging\n    \n    # turn on logging so we can see the system working\n    logging.getLogger(\"llama_agents\").setLevel(logging.INFO)\n    \n    # Set up the message queue and control plane\n    message_queue = SimpleMessageQueue()\n    control_plane = ControlPlaneServer(\n        message_queue=message_queue,\n        orchestrator=AgentOrchestrator(llm=OpenAI()),\n    )\n\nNext we create our tools using LlamaIndex\u2019s existing abstractions, provide\nthose tools to an agent, and turn that agent into an independent microservice:\n\n    \n    \n    # create a tool\n    def get_the_secret_fact() -> str:\n        \"\"\"Returns the secret fact.\"\"\"\n        return \"The secret fact is: A baby llama is called a 'Cria'.\"\n    \n    tool = FunctionTool.from_defaults(fn=get_the_secret_fact)\n    \n    # Define an agent\n    worker = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())\n    agent = worker.as_agent()\n    \n    # Create an agent service\n    agent_service = AgentService(\n        agent=agent,\n        message_queue=message_queue,\n        description=\"General purpose assistant\",\n        service_name=\"assistant\",\n    )\n\nFinally we launch the service and the control plane. Note that here we\u2019re\nusing a helper function to run a single query through the system and then\nexit; next we\u2019ll show how to deploy this to production.\n\n    \n    \n    # Set up the launcher for local testing\n    from llama_agents import LocalLauncher\n    \n    launcher = LocalLauncher(\n        [agent_service],\n        control_plane,\n        message_queue,\n    )\n    \n    # Run a single query through the system\n    result = launcher.launch_single(\"What's the secret fact?\")\n    print(result)\n\n##  Deploying Your Multi-Agent System\n\nOnce you've tested your system locally, you can deploy it as a set of services\nfor real production use. Here's how you might set that up. This is similar to\nthe previous example, but we\u2019ve added a second agent service and we\u2019re using a\ndifferent launcher. Let\u2019s bring in our dependencies and set up our control\nplane again:\n\n    \n    \n    import dotenv\n    dotenv.load_dotenv()\n    from llama_agents import (\n        AgentService,\n        AgentOrchestrator,\n        ControlPlaneServer,\n        SimpleMessageQueue,\n    )\n    \n    from llama_index.core.agent import FunctionCallingAgentWorker\n    from llama_index.core.tools import FunctionTool\n    from llama_index.llms.openai import OpenAI\n    import logging\n    \n    # change logging level to enable or disable more verbose logging\n    logging.getLogger(\"llama_agents\").setLevel(logging.INFO)\n    \n    # create our multi-agent framework components\n    message_queue = SimpleMessageQueue()\n    control_plane = ControlPlaneServer(\n        message_queue=message_queue,\n        orchestrator=AgentOrchestrator(llm=OpenAI()),\n    )\n\nThen as before we create a tool and an agent, though this time we\u2019ll add a\nsecond agent:\n\n    \n    \n    # create a tool\n    def get_the_secret_fact() -> str:\n        \"\"\"Returns the secret fact.\"\"\"\n        return \"The secret fact is: A baby llama is called a 'Cria'.\"\n    \n    tool = FunctionTool.from_defaults(fn=get_the_secret_fact)\n    \n    # create our agents\n    worker1 = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())\n    worker2 = FunctionCallingAgentWorker.from_tools([], llm=OpenAI())\n    agent1 = worker1.as_agent()\n    agent2 = worker2.as_agent()\n\nWe turn those agents into services:\n\n    \n    \n    agent_server_1 = AgentService(\n        agent=agent1,\n        message_queue=message_queue,\n        description=\"Useful for getting the secret fact.\",\n        service_name=\"secret_fact_agent\",\n        host=\"localhost\",\n        port=8003\n    )\n    agent_server_2 = AgentService(\n        agent=agent2,\n        message_queue=message_queue,\n        description=\"Useful for getting random dumb facts.\",\n        service_name=\"dumb_fact_agent\",\n        host=\"localhost\",\n        port=8004\n    )\n\nAnd finally we launch each service as an independent agent. Here we\u2019re doing\nthem all from a single script, but each of these could be a totally separate\nservice, launched and scaled independently:\n\n    \n    \n    from llama_agents import ServerLauncher, CallableMessageConsumer\n    \n    # Additional human consumer\n    def handle_result(message) -> None:\n        print(f\"Got result:\", message.data)\n    \n    \n    # the final result is published to a \"human\" consumer\n    # so we define one to handle it!\n    human_consumer = CallableMessageConsumer(\n        handler=handle_result, message_type=\"human\"\n    )\n    \n    # Define Launcher\n    launcher = ServerLauncher(\n        [agent_server_1, agent_server_2],\n        control_plane,\n        message_queue,\n        additional_consumers=[human_consumer]\n    )\n    \n    launcher.launch_servers()\n\n##  Real-time monitoring\n\nOne of the coolest debugging features of our multi-agent system is our agent\nmonitor, which is built right in. You launch it like this:\n\n    \n    \n    llama-agents monitor --control-plane-url http://127.0.0.1:8000\n\nOnce launched, you get an intuitive, point-and-click terminal application. You\ncan see both of the agents running, and at the bottom you can inject a task\nlike the query \u201cWhat is the secret fact?\u201d You\u2019ll get a job ID which you can\nthen click on to see your results:\n\n##  Building a Query Rewriting RAG System\n\nLet's look at a more complex example: a Query Rewriting RAG system. This\nsystem will rewrite user queries to improve retrieval, then use the rewritten\nquery to perform RAG over a document.\n\nThis example demonstrates how to create a more sophisticated system that\ncombines query rewriting with RAG to improve question-answering capabilities.\nSee [ this notebook ](https://github.com/run-llama/llama-\nagents/blob/main/examples/query_rewrite_rag.ipynb) for a fuller explanation of\nwhat\u2019s going on.\n\n    \n    \n    import dotenv\n    dotenv.load_dotenv() # our .env defines OPENAI_API_KEY\n    from llama_index.core import VectorStoreIndex, Document\n    from llama_index.core.agent import FnAgentWorker\n    from llama_index.core import PromptTemplate\n    from llama_index.core.query_pipeline import QueryPipeline\n    from llama_index.core.query_engine import RetrieverQueryEngine\n    from llama_agents import (\n        AgentService,\n        ControlPlaneServer,\n        SimpleMessageQueue,\n        PipelineOrchestrator,\n        ServiceComponent,\n    )\n    from llama_agents.launchers import LocalLauncher\n    from llama_index.llms.openai import OpenAI\n    import logging\n    \n    # change logging level to enable or disable more verbose logging\n    logging.getLogger(\"llama_agents\").setLevel(logging.INFO)\n    \n    # Load and index your document\n    docs = [Document(text=\"The rabbit is a small mammal with long ears and a fluffy tail. His name is Peter.\")]\n    index = VectorStoreIndex.from_documents(docs)\n    \n    # Define a query rewrite agent\n    HYDE_PROMPT_STR = (\n        \"Please rewrite the following query to include more detail:\\n{query_str}\\n\"\n    )\n    HYDE_PROMPT_TMPL = PromptTemplate(HYDE_PROMPT_STR)\n    \n    def run_hyde_fn(state):\n        prompt_tmpl, llm, input_str = (\n            state[\"prompt_tmpl\"],\n            state[\"llm\"],\n            state[\"__task__\"].input,\n        )\n        qp = QueryPipeline(chain=[prompt_tmpl, llm])\n        output = qp.run(query_str=input_str)\n        state[\"__output__\"] = str(output)\n        return state, True\n    \n    hyde_agent = FnAgentWorker(\n        fn=run_hyde_fn,\n        initial_state={\"prompt_tmpl\": HYDE_PROMPT_TMPL, \"llm\": OpenAI()}\n    ).as_agent()\n    \n    # Define a RAG agent\n    def run_rag_fn(state):\n        retriever, llm, input_str = (\n            state[\"retriever\"],\n            state[\"llm\"],\n            state[\"__task__\"].input,\n        )\n        query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm)\n        response = query_engine.query(input_str)\n        state[\"__output__\"] = str(response)\n        return state, True\n    \n    rag_agent = FnAgentWorker(\n        fn=run_rag_fn,\n        initial_state={\"retriever\": index.as_retriever(), \"llm\": OpenAI()}\n    ).as_agent()\n    \n    # Set up the multi-agent system\n    message_queue = SimpleMessageQueue()\n    \n    query_rewrite_service = AgentService(\n        agent=hyde_agent,\n        message_queue=message_queue,\n        description=\"Query rewriting service\",\n        service_name=\"query_rewrite\",\n    )\n    \n    rag_service = AgentService(\n        agent=rag_agent,\n        message_queue=message_queue,\n        description=\"RAG service\",\n        service_name=\"rag\",\n    )\n    \n    # Create the pipeline\n    pipeline = QueryPipeline(chain=[\n        ServiceComponent.from_service_definition(query_rewrite_service),\n        ServiceComponent.from_service_definition(rag_service),\n    ])\n    orchestrator = PipelineOrchestrator(pipeline)\n    \n    control_plane = ControlPlaneServer(\n        message_queue=message_queue,\n        orchestrator=orchestrator,\n    )\n    \n    # Set up the launcher\n    launcher = LocalLauncher(\n        [query_rewrite_service, rag_service],\n        control_plane,\n        message_queue,\n    )\n    \n    # Run a query\n    result = launcher.launch_single(\"Tell me about rabbits\")\n    print(result)\n\n##  Public roadmap\n\nThis is an alpha release, meaning that we\u2019d love your feedback on features to\nbetter help you build multi-agent systems in production! We\u2019ve created a [\npublic roadmap ](https://github.com/run-llama/llama-agents/discussions/49)\nshowing where we plan to go from here. ", "mimetype": "text/plain", "start_char_idx": 1142, "end_char_idx": 11698, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c21b1199-0bff-4a84-8d9f-0a3d94e19682": {"__data__": {"id_": "c21b1199-0bff-4a84-8d9f-0a3d94e19682", "embedding": null, "metadata": {"filename": "introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.md", "extension": ".md", "title": "Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems", "date": "Jun 26, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b6700bfd-653b-4d5f-9db9-6f04c642990b", "node_type": "4", "metadata": {"filename": "introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.md", "extension": ".md", "title": "Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems", "date": "Jun 26, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems"}, "hash": "ff4d278f758b012a8398b6484fc7080bec8fa57d827c6cd794afdb623384a11f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18739f06-e810-4316-aa13-cb984bbd1ea9", "node_type": "1", "metadata": {"filename": "introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.md", "extension": ".md", "title": "Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems", "date": "Jun 26, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems"}, "hash": "287df68bb68caac73797a906528c9306ad02125f9bd7a0181c129a77b2514c9a", "class_name": "RelatedNodeInfo"}}, "text": "We\u2019re actively seeking public feedback\non what works for you and what doesn\u2019t.\n\n##  Dive in!\n\n` llama-agents ` provides a powerful, flexible framework for building complex\nmulti-agent AI systems. Whether you're prototyping a new idea or scaling to\nproduction, ` llama-agents ` offers the tools you need to bring your AI vision\nto life. Check out [ the repo ](https://github.com/run-llama/llama-agents) to\nlearn more, especially our library of [ examples ](https://github.com/run-\nllama/llama-agents/tree/main/examples) .\n\nWe're excited to see what the community builds with ` llama-agents ` . Happy\ncoding!\n\n", "mimetype": "text/plain", "start_char_idx": 11698, "end_char_idx": 12306, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c9fc031-8fb4-4175-bba6-2676a382f25c": {"__data__": {"id_": "9c9fc031-8fb4-4175-bba6-2676a382f25c", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-06-25.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-25", "date": "Jun 25, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-25"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8d27ef78-ba49-4b72-a6a0-e9c3a62c3b14", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-06-25.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-25", "date": "Jun 25, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-25"}, "hash": "e53043bfefeeb97ed28f298e03c6e4ed31ec1afbc786c4dd9a8ae7ae792ee09f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "740931d1-e134-453b-9517-bd3ad041aee7", "node_type": "1", "metadata": {}, "hash": "25b8b5bd74d9733cf9b812e33beab66c1324d504c4afecc27322d1bf214e0e24", "class_name": "RelatedNodeInfo"}}, "text": "Hello to All Llama Lovers!\n\nWelcome to this week\u2019s issue of the LlamaIndex newsletter! This edition is\npacked with thrilling updates, comprehensive guides, and detailed tutorials to\nhelp you gain a deeper understanding of our tools.\n\n##  **The highlights:**\n\n  * **CrewAI Multi-Agent Integration:** Integrated with CrewAI to enhance task-solving with specialized agent crews and LlamaIndex integrations. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/crewai_llamaindex.ipynb) , [ Tweet ](https://x.com/llama_index/status/1803818106063925749) . \n  * **MistralAI Fine-Tuning API Integration:** Enhance model training and performance monitoring with our new integration of MistralAI\u2019s Fine-Tuning API. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/finetuning/mistralai_fine_tuning.ipynb) , [ Tweet ](https://x.com/llama_index/status/1803470522455380044) . \n\n##  **Feature Releases and Enhancements:**\n\n  1. We have launched a Multi-Agent integration with CrewAI to build a ` crew ` of specialized agents that collaboratively solve tasks. Enhance these agents with external knowledge and third-party tools through easy integrations with LlamaIndex, including advanced RAG query engines and tools from LlamaHub. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/crewai_llamaindex.ipynb) , [ Tweet ](https://x.com/llama_index/status/1803818106063925749) . \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1479, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "740931d1-e134-453b-9517-bd3ad041aee7": {"__data__": {"id_": "740931d1-e134-453b-9517-bd3ad041aee7", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-06-25.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-25", "date": "Jun 25, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-25"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8d27ef78-ba49-4b72-a6a0-e9c3a62c3b14", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-06-25.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-25", "date": "Jun 25, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-25"}, "hash": "e53043bfefeeb97ed28f298e03c6e4ed31ec1afbc786c4dd9a8ae7ae792ee09f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c9fc031-8fb4-4175-bba6-2676a382f25c", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-06-25.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-25", "date": "Jun 25, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-25"}, "hash": "3aff98b4f12f7e39a18bedace5083dabebfa75ea251d98a478b98dde1e2d2fed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97efe43a-7f79-4bd5-b561-95d363341bc4", "node_type": "1", "metadata": {}, "hash": "ad442d11e5e9c2564e28af03c6753c71f1a994ac1052030a3a53890ec80bf414", "class_name": "RelatedNodeInfo"}}, "text": "2. We have integrated MistralAI\u2019s Fine-Tuning API to create and synthesize training and evaluation datasets, assess model after fine-tuning, and monitor performance metrics with RAGAS and Weights & Biases. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/finetuning/mistralai_fine_tuning.ipynb) , [ Tweet ](https://x.com/llama_index/status/1803470522455380044) . \n\n##  **Demos:**\n\n  * [ RAGapp ](https://github.com/ragapp/ragapp) by [ **Marcus Schiesser ](https://x.com/MarcusSchiesser) ** simplifies Agentic RAG in enterprise settings with functionalities akin to using GPTs by OpenAI. The latest version includes a code interpreter and a tool to call any OpenAPI, all built using LlamaIndex. \n\n##  **Guides:**\n\n  * [ Guide ](https://lightning.ai/blochsphere/studios/multi-document-agentic-rag-for-quantum-computing) to Multi-Document Agentic RAG Using LightningAI: Jay Shah\u2019s template that enables you to set up a multi-document agent for search and summarization across research notebooks. This out-of-the-box solution, integrated with Streamlit, allows for full visualization and is part of LightningAI\u2019s suite of tools for developing and sharing ML and genAI native apps. \n  * [ Guide ](https://github.com/Azure-Samples/llama-index-python/tree/main) to Making a Serverless RAG Chatbot: Azure\u2019s quick start repository for creating a serverless RAG chatbot using LlamaIndex and AzureOpenAI. \n  * [ Guide ](https://docs.llamaindex.ai/en/stable/understanding/agent/basic_agent/) to Building an Agent in LlamaIndex: Our comprehensive guide which covers building a basic agent, using local models, adding RAG, enhancing retrieval with LlamaParse, and developing custom tools. \n\n##  **Tutorials:**\n\n  * [ JinoRohit\u2019s ](https://x.com/jino_rohit) tutorial on using a LlamaIndex pipeline with MLflow for systematic tracking and tuning of RAG parameters, enhancing answer accuracy through precise evaluation metrics and datasets. \n  * Hanane Dupouy\u2019s [ tutorial ](https://www.linkedin.com/posts/hanane-d-algo-trader_corrective-rag-crag-for-financial-analysis-activity-7203277247321886721-G9yj?utm_source=share&utm_medium=member_desktop) demonstrates how to apply CRAG (Corrective RAG) for financial analysis using LlamaIndex\u2019s CRAG LlamaPack. This technique assesses retrieval quality and supplements the knowledge base with web searches to ensure contextual accuracy and relevance. \n  ", "mimetype": "text/plain", "start_char_idx": 1479, "end_char_idx": 3893, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97efe43a-7f79-4bd5-b561-95d363341bc4": {"__data__": {"id_": "97efe43a-7f79-4bd5-b561-95d363341bc4", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-06-25.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-25", "date": "Jun 25, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-25"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8d27ef78-ba49-4b72-a6a0-e9c3a62c3b14", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-06-25.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-25", "date": "Jun 25, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-25"}, "hash": "e53043bfefeeb97ed28f298e03c6e4ed31ec1afbc786c4dd9a8ae7ae792ee09f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "740931d1-e134-453b-9517-bd3ad041aee7", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-06-25.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-25", "date": "Jun 25, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-25"}, "hash": "e15e13bd02606354c093d095b8799fec41a153d501b1fbb111a12e32b95489b3", "class_name": "RelatedNodeInfo"}}, "text": "* [ Soham\u2019s ](https://x.com/GanatraSoham) [ tutorial ](https://www.youtube.com/watch?v=5AnK0DXHuy8) to create an agent that automates GitHub commits using Composio and LlamaIndex Tools. \n  * [ Aruna Withanage\u2019s ](https://x.com/deltaaruna) [ tutorial ](https://medium.com/rahasak/llamaindex-query-pipelines-tutorial-text-to-sql-example-d859ed90b87c) on creating custom text-to-SQL pipelines using LlamaIndex\u2019s DAG capabilities. \n\n", "mimetype": "text/plain", "start_char_idx": 3893, "end_char_idx": 4322, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a3a43a8-6e12-4bcf-962a-3d7fc15f2f40": {"__data__": {"id_": "2a3a43a8-6e12-4bcf-962a-3d7fc15f2f40", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-06-18.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-18", "date": "Jun 18, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-18"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a5d297c-1c31-4730-89e5-e0b02aa9ef1e", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-06-18.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-18", "date": "Jun 18, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-18"}, "hash": "35001c12b711455f782254a93c5d151b1d05b4699d0b7cffb0af556f32618a94", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ca2d89d-db47-4bfd-b892-da19dded09da", "node_type": "1", "metadata": {}, "hash": "0e1b2beb3713ecfcf6ee8184af61ac9efd9aadd78af6040b549a1d9edcf37468", "class_name": "RelatedNodeInfo"}}, "text": "Hey Llama Followers\n\nWelcome to this week\u2019s edition of the LlamaIndex newsletter! We\u2019re bringing\nyou an exciting set of updates and valuable resources from Mixture-of-Agents\n(MoA) paper as LlamaPack to how AtomicWork\u2019s Atom AI assistant leverages\nLlamaIndex to boost productivity and manage data effectively. Be sure to check\nout our in-depth guides, educational tutorials, and webinars for deeper\ninsights into our tools.\n\n##  **The highlights:**\n\n  * **Mixture-of-Agents (MoA) LlamaPack:** We have integrated the Mixture-of-Agents (MoA) demonstrating that open-source LLMs can boost task capabilities. MoA outperforms GPT-4 Omni in the AlpacaEval 2.0 benchmarks. [ LlamaPack ](https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-mixture-of-agents/README.md) , [ Tweet ](https://x.com/llama_index/status/1801305617878937959) . \n  * **TiDB Integration with LlamaIndex:** PingCap has now integrated their TiDB database with our LlamaIndex\u2019s knowledge graph functionality, making it available as an open-source project. [ Docs ](https://docs.llamaindex.ai/en/latest/examples/vector_stores/TiDBVector/) , [ Tweet ](https://x.com/llama_index/status/1800987302837297387) . \n  * **RAG and Agents Cookbook:** We have released a detailed cookbook on building RAG and Agents. This guide features enhanced observability through our LlamaIndex instrumentation module and ArizeAI. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/docs/presentations/materials/2024-06-13-vector-ess-oss-tools.ipynb) , [ Tweet ](https://x.com/llama_index/status/1801726691813036214) . \n  * **AtomicWork\u2019s Enterprise AI Assistant:** AtomicWork\u2019s enterprise AI assistant, Atom, leverages LlamaIndex to handle diverse data formats, boosting productivity and improving the employee experience. Check out the details in their detailed [ blog ](https://www.atomicwork.com/blog/llamaindex-loaders-powering-atom) . \n  * [ Guide ](https://github.com/run-llama/llama_parse/blob/main/examples/excel/dcf_rag.ipynb) to RAG Over Excel Files: Guide to use LlamaParse to accurately represent Excel files in a spatial grid format, enhancing data interpretation and reducing errors in question-answering. \n\n##  **Feature Releases and Enhancements:**\n\n  1. We have integrated Mixture-of-Agents (MoA) [ paper ](https://arxiv.org/abs/2406.04692) from [ TogetherAI ](https://x.com/togethercompute) as LlamaPack from demonstrating that open-source large language models (LLMs) can enhance task capabilities. The paper shows that MoA outperforms GPT-4 Omni in the AlpacaEval 2.0 benchmarks. [ LlamaPack ](https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-mixture-of-agents/README.md) , [ Tweet ](https://x.com/llama_index/status/1801305617878937959) . \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2792, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ca2d89d-db47-4bfd-b892-da19dded09da": {"__data__": {"id_": "0ca2d89d-db47-4bfd-b892-da19dded09da", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-06-18.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-18", "date": "Jun 18, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-18"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a5d297c-1c31-4730-89e5-e0b02aa9ef1e", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-06-18.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-18", "date": "Jun 18, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-18"}, "hash": "35001c12b711455f782254a93c5d151b1d05b4699d0b7cffb0af556f32618a94", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a3a43a8-6e12-4bcf-962a-3d7fc15f2f40", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-06-18.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-18", "date": "Jun 18, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-18"}, "hash": "a3dab09f839ec6f705afd616391e00d454dc9b1352f652545394470b67051c92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60457367-3313-4aee-9caa-5970b843fa5a", "node_type": "1", "metadata": {}, "hash": "f5ea09e7b82b9bf3a4a8a60298b5b81454df27768d25a67d65f20c14848a2e1d", "class_name": "RelatedNodeInfo"}}, "text": "2. [ PingCap ](https://x.com/pingcap) has integrated their TiDB database with our LlamaIndex\u2019s knowledge graph functionality, now accessible as an open source project. [ Docs ](https://docs.llamaindex.ai/en/latest/examples/vector_stores/TiDBVector/) , [ Tweet ](https://x.com/llama_index/status/1800987302837297387) . \n  3. We have released a detailed cookbook on building RAG and Agents, featuring supercharged observability throughout the call stack, enabled by our LlamaIndex instrumentation module and ArizeAI. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/docs/presentations/materials/2024-06-13-vector-ess-oss-tools.ipynb) , [ Tweet ](https://x.com/llama_index/status/1801726691813036214) . \n  4. We have released the workshop slides and notebooks from our presentation on \u201cBuilding an Advanced Research Agent on Databricks\u201d at the Data AI Summit. This workshop focused on enhancing research assistants beyond the standard RAG setups. [ Slide deck ](https://docs.google.com/presentation/d/1yiuHEQEAhWEvVskbD9jwmfjopznVeZGwwWUzBIZ_P9U/) , [ Notebook1 ](https://colab.research.google.com/drive/18RUkf8IpHVSJF-rDh8cOj0QJ6UwQonfh?usp=sharing) , [ Notebook2 ](https://colab.research.google.com/drive/18RUkf8IpHVSJF-rDh8cOj0QJ6UwQonfh?usp=sharing) , [ Tweet ](https://x.com/llama_index/status/1802734801201623117) . \n\n##  **Real-World Use cases:**\n\n  * AtomicWork\u2019s enterprise AI assistant, Atom, utilizes LlamaIndex to handle various data formats, ensuring accurate and secure data retrieval. Atom enhances decision-making and manages unstructured data effectively, boosting productivity and improving the employee experience. Check out the details in their detailed [ blog ](https://www.atomicwork.com/blog/llamaindex-loaders-powering-atom) . \n\n##  **Guides:**\n\n  * [ Guide ](https://github.com/run-llama/llama_parse/blob/main/examples/excel/dcf_rag.ipynb) to RAG Over Excel Files using LlamaParse to accurately represent Excel files in a spatial grid format, enhancing data interpretation and reducing errors in question-answering. \n  * [ Guide ](https://www.singlestore.com/blog/claude-3-multimodal-with-llamaindex-and-singlestore/?utm_medium=referral&utm_source=pavan&utm_term=lnkdn&utm_content=multimod) to Building a Multimodal RAG Pipeline by [ Pavan Belagatti ](https://x.com/Pavan_Belagatti/status/1802955250795417790) using Claude-3 and SingleStoreDB. \n  * [ Guide ](https://github.com/mistralai/cookbook/blob/main/third_party/LlamaIndex/ollama_mistral_llamaindex.ipynb) to building fully local RAG application using MistralAI, Ollama and LlamaIndex. \n\n##  **Tutorials:**\n\n  * [ Tomaz Bratanic ](https://x.com/tb_tomaz) \u2019s [ tutorial ](https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamaindex) on constructing a knowledge graph, perform entity deduplication, design a custom graph retriever, and implement a question-answering flow. \n  * [ Mervin Praison ](https://x.com/MervinPraison) \u2019s [ tutorial ](https://www.youtube.com/watch?v=jnWaUtS2Fr8) on creating the core components of an agent defining tools, integrating them into an agent reasoning loop, and wrapping everything with a user interface. using local models and [ chainlit ](https://github.com/Chainlit/chainlit) . \n  * [ Arkiti ](https://x.com/AkritiUpadhya13) \u2019s [ tutorial ](https://medium.com/@akriti.upadhyay/text-to-sql-using-singlestore-helios-groq-and-llama-3-0ebe1150cbe2) on building a dynamic text-to-SQL solution using Llama 3 and GroqInc, highlighting the scalable and fast capabilities of SingleStoreDB Helios for multi-cloud deployments. \n  * [ Kingzzm ](https://x.com/kingzzm) \u2019s [ tutorial ](https://ai.gopubby.com/advanced-rag-retrieval-strategy-embedded-tables-fdb3e44003a5) on Advanced RAG Patterns detailing effective strategies for handling documents with embedded tables, utilizing tools like LlamaParse and Nougat for enhanced QA performance. \n\n##  **Webinar:**\n\n  * [ Webinar ](https://www.youtube.com/watch?v=9AqdqJdCNCw) on The Future of Web Agents with MultiOn. ", "mimetype": "text/plain", "start_char_idx": 2792, "end_char_idx": 6799, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60457367-3313-4aee-9caa-5970b843fa5a": {"__data__": {"id_": "60457367-3313-4aee-9caa-5970b843fa5a", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-06-18.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-18", "date": "Jun 18, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-18"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a5d297c-1c31-4730-89e5-e0b02aa9ef1e", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-06-18.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-18", "date": "Jun 18, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-18"}, "hash": "35001c12b711455f782254a93c5d151b1d05b4699d0b7cffb0af556f32618a94", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ca2d89d-db47-4bfd-b892-da19dded09da", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-06-18.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-18", "date": "Jun 18, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-18"}, "hash": "c5afb739e1783c8f5bea6735370e257153158ff844cc4341361e81bad491b738", "class_name": "RelatedNodeInfo"}}, "text": "[ Div Garg ](https://x.com/DivGarg9) provided a full demo walkthrough and discuss the agentification of the internet. \n\n", "mimetype": "text/plain", "start_char_idx": 6799, "end_char_idx": 6919, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "edc33c91-72a3-491a-926c-f5ceb1ca4ca6": {"__data__": {"id_": "edc33c91-72a3-491a-926c-f5ceb1ca4ca6", "embedding": null, "metadata": {"filename": "customizing-property-graph-index-in-llamaindex.md", "extension": ".md", "title": "Customizing property graph index in LlamaIndex", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "55a84224-699c-4b12-9c4b-12367806480c", "node_type": "4", "metadata": {"filename": "customizing-property-graph-index-in-llamaindex.md", "extension": ".md", "title": "Customizing property graph index in LlamaIndex", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamaindex"}, "hash": "acbe25d05327de8abfc6b12c09823aefbbf915a3e52ab410ca503b0b60695b5d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbd9d33d-6d02-462c-99e1-1bd7c177cab5", "node_type": "1", "metadata": {}, "hash": "e3714b3c9597d80185682064cf57aef61ff7a3f9569c44df8219d0d7e64fb27e", "class_name": "RelatedNodeInfo"}}, "text": "Learn how to implement entity deduplication and custom retrieval methods to\nincrease GraphRAG accuracy\n\n_This is a guest post by Neo4J_\n\nThe [ property graph index ](https://www.llamaindex.ai/blog/introducing-the-\nproperty-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms)\nis an excellent addition to LlamaIndex and an upgrade from the previous\nknowledge graph integration. First, the data representation is slightly\ndifferent. In the previous integration, the graph was represented with\ntriples, but now we have a proper property graph integration where nodes have\nlabels and optionally node properties.\n\nExample of a property graph model.\n\nEach node is assigned a label indicating its type, such as Person,\nOrganization, Project, or Department. Nodes and relationships may also store\nnode properties for other relevant details, such as the date of birth or\nproject start and end date, as shown in this example.\n\nSecond, the property graph index is designed to be modular, so you can use one\nor multiple (custom) knowledge graph constructors as well as retrievers,\nmaking it an incredible tool to build your first knowledge graph or customize\nthe implementation for your specific needs.\n\nProperty graph workflow\n\nThe image illustrates the property graph integration within the LlamaIndex ,\nbeginning with documents being passed to graph constructors. These\nconstructors are modular components responsible for extracting structured\ninformation, which is then stored in a knowledge graph. The graph can be built\nusing various or custom modules, highlighting the system\u2019s flexibility to\nadapt to different data sources or extraction needs.\n\nGraph retrievers then access the knowledge graph to retrieve data. This stage\nis also modular, allowing for the use of multiple retrievers or custom\nsolutions designed to query specific types of data or relationships within the\ngraph. Finally, the retrieved data is used by a LLM to generate an answer,\nrepresenting the output or the insight derived from the process. This flow\nemphasizes a highly adaptable and scalable system where each component can be\nindependently modified or replaced to enhance the overall functionality or to\ntailor it to specific requirements.\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2228, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbd9d33d-6d02-462c-99e1-1bd7c177cab5": {"__data__": {"id_": "bbd9d33d-6d02-462c-99e1-1bd7c177cab5", "embedding": null, "metadata": {"filename": "customizing-property-graph-index-in-llamaindex.md", "extension": ".md", "title": "Customizing property graph index in LlamaIndex", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "55a84224-699c-4b12-9c4b-12367806480c", "node_type": "4", "metadata": {"filename": "customizing-property-graph-index-in-llamaindex.md", "extension": ".md", "title": "Customizing property graph index in LlamaIndex", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamaindex"}, "hash": "acbe25d05327de8abfc6b12c09823aefbbf915a3e52ab410ca503b0b60695b5d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "edc33c91-72a3-491a-926c-f5ceb1ca4ca6", "node_type": "1", "metadata": {"filename": "customizing-property-graph-index-in-llamaindex.md", "extension": ".md", "title": "Customizing property graph index in LlamaIndex", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamaindex"}, "hash": "7f9c1c0fbe8e7d518f662b65be3e9efc1c61463d101e0e7250d08803ba737095", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b99dd4b-5607-4a9c-a4a1-51a5307b0977", "node_type": "1", "metadata": {}, "hash": "146eb1e2929f1baeb0a6b47b95eb7d3f52739a3cb864498d0b002b95a893326b", "class_name": "RelatedNodeInfo"}}, "text": "In this blog post you will learn how to:\n\n  1. Construct a knowledge graph using a schema-guided extraction \n  2. Perform entity deduplication using a combination of text embedding and word similarity techniques \n  3. Design a custom graph retriever \n  4. Finally, you will implement a question answering flow using the custom retriever \n\nThe code is available on [ GitHub\n](https://github.com/tomasonjo/blogs/blob/master/llm/llama_index_neo4j_custom_retriever.ipynb)\n.\n\n##  Environment setup\n\nIn this blog post, we will use Neo4j as the underlying graph store. The\neasiest way is to get started is to a free instance on [ Neo4j Aura\n](https://neo4j.com/cloud/platform/aura-graph-database/) , which offers cloud\ninstances of the Neo4j database. Alternatively, you can also set up a local\ninstance of the Neo4j database by downloading the [ Neo4j Desktop\n](https://neo4j.com/download/) application and creating a local database\ninstance.\n\n    \n    \n    from llama_index.graph_stores.neo4j import Neo4jPGStore\n    \n    username=\"neo4j\"\n    password=\"stump-inlet-student\"\n    url=\"bolt://52.201.215.224:7687\"\n    \n    graph_store = Neo4jPGStore(\n        username=username,\n        password=password,\n        url=url,\n    )\n\nAdditionally, you will require a working OpenAI API key.\n\n    \n    \n    import os\n    \n    os.environ[\"OPENAI_API_KEY\"] = \"sk-\"\n\n##  Dataset\n\nIn this blog post, we will use a [ sample news article dataset fetched from\nDiffbot ](https://www.diffbot.com/solutions/news-monitoring/) , which I\u2019ve\nmade available on [ GitHub for easier access\n](https://github.com/tomasonjo/blog-datasets/blob/main/news_articles.csv) .\n\nSample records from the dataset.\n\nSince the property graph index operates with documents, we have to wrap the\ntext from the news as LlamaIndex documents.\n\n    \n    \n    import pandas as pd\n    from llama_index.core import Document\n    \n    news = pd.read_csv(\n      \"https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/news_articles.csv\")\n    documents = [Document(text=f\"{row['title']}: {row['text']}\") for i, row in news.iterrows()]\n\n##  Graph construction\n\nAs mentioned, LlamaIndex provides multiple [ out-of-the-box graph constructors\n](https://docs.llamaindex.ai/en/latest/module_guides/indexing/lpg_index_guide/#construction)\n. In this example, we will use the [ SchemaLLMPathExtractor\n](https://docs.llamaindex.ai/en/latest/module_guides/indexing/lpg_index_guide/#schemallmpathextractor)\n, which allows us to define the schema of the graph structure we want to\nextract from documents.\n\nSchema-guided graph structure extraction.\n\nWe begin by defining the types of nodes and relationships we want the LLM to\nextract.\n\n    \n    \n    entities = Literal[\"PERSON\", \"LOCATION\", \"ORGANIZATION\", \"PRODUCT\", \"EVENT\"]\n    relations = Literal[\n        \"SUPPLIER_OF\",\n        \"COMPETITOR\",\n        \"PARTNERSHIP\",\n        \"ACQUISITION\",\n        \"WORKS_AT\",\n        \"SUBSIDIARY\",\n        \"BOARD_MEMBER\",\n        \"CEO\",\n        \"PROVIDES\",\n        \"HAS_EVENT\",\n        \"IN_LOCATION\",\n    ]\n\nAs you can see, we are focusing our graph extraction around people and\norganizations. Next, we will specify the relationships associated with each\nnode label.\n\n    \n    \n    # define which entities can have which relations\n    validation_schema = {\n        \"Person\": [\"WORKS_AT\", \"BOARD_MEMBER\", \"CEO\", \"HAS_EVENT\"],\n        \"Organization\": [\n            \"SUPPLIER_OF\",\n            \"COMPETITOR\",\n            \"PARTNERSHIP\",\n            \"ACQUISITION\",\n            \"WORKS_AT\",\n            \"SUBSIDIARY\",\n            \"BOARD_MEMBER\",\n            \"CEO\",\n            \"PROVIDES\",\n            \"HAS_EVENT\",\n            \"IN_LOCATION\",\n        ],\n        \"Product\": [\"PROVIDES\"],\n        \"Event\": [\"HAS_EVENT\", \"IN_LOCATION\"],\n        \"Location\": [\"HAPPENED_AT\", \"IN_LOCATION\"],\n    }\n\nFor example, a person can have the following relationships:\n\n  * WORKS_AT \n  * BOARD_MEMBER \n  * CEO \n  * HAS_EVENT \n\nThe schema is quite specific except for the EVENT node label, which is\nslightly more ambiguous and allows the LLM to capture various types of\ninformation.\n\nNow that we have defined the graph schema, we can input it into the `\nSchemaLLMPathExtractor ` and use it to construct a graph.\n\n    \n    \n    from llama_index.core import PropertyGraphIndex\n    \n    kg_extractor = SchemaLLMPathExtractor(\n        llm=llm,\n        possible_entities=entities,\n        possible_relations=relations,\n        kg_validation_schema=validation_schema,\n        # if false, allows for values outside of the schema\n        # useful for using the schema as a suggestion\n        strict=True,\n    )\n    \n    NUMBER_OF_ARTICLES = 250\n    \n    index = PropertyGraphIndex.from_documents(\n        documents[:NUMBER_OF_ARTICLES],\n        kg_extractors=[kg_extractor],\n        llm=llm,\n        embed_model=embed_model,\n        property_graph_store=graph_store,\n        show_progress=True,\n    )\n\nThis code extracts graph information from 250 news articles, but you can\nadjust the number how you see fit. There are 2500 articles in total.\n\n_Note that extracting 250 articles takes about 7 minutes with GPT-4o. However,\nyou can accelerate the process by employing parallelization through the_ _`\nnum_workers ` _ _parameter._\n\nWe can visualize a small subgraph to inspect what was stored.\n\nText chunks are blue, while entity nodes are all the rest.\n\nThe constructed graph contains both text chunks (blue), which contain text and\nembeddings. If an entity was mentioned in the text chunk, there is a `\nMENTIONS ` relationships between the text chunk and entity. Additionally,\nentities can have relationships to other entities.\n\n##  Entity deduplication\n\nEntity deduplication or disambiguation is an important but often overlooked\nstep in graph construction. Essentially, it is a cleaning step where you try\nto match multiple nodes that represent a single entity and merge them together\ninto a single node for better graph structural integrity.\n\nFor example, in our constructed graph I could find some examples that could be\nmerged.\n\nPotential entity duplicates.\n\nWe will use a combination of text embedding similarity and word distance to\nfind potential duplicates. We start by defining the vector index on our\nentities in the graph.\n\n    \n    \n    graph_store.structured_query(\"\"\"\n    CREATE VECTOR INDEX entity IF NOT EXISTS\n    FOR (m:`__Entity__`)\n    ON m.embedding\n    OPTIONS {indexConfig: {\n     `vector.dimensions`: 1536,\n     `vector.similarity_function`: 'cosine'\n    }}\n    \"\"\")\n\nThe next Cypher query finds duplicates and is quite involved and I took me,\nMichael Hunger, and Eric Monk a couple of hours to perfect it.\n\n    \n    \n    similarity_threshold = 0.9\n    word_edit_distance = 5\n    data = graph_store.structured_query(\"\"\"\n    MATCH (e:__Entity__)\n    CALL {\n      WITH e\n      CALL db.index.vector.queryNodes('entity', 10, e.embedding)\n      YIELD node, score\n      WITH node, score\n      WHERE score > toFLoat($cutoff)\n          AND (toLower(node.name) CONTAINS toLower(e.name) OR toLower(e.name) CONTAINS toLower(node.name)\n               OR apoc.text.distance(toLower(node.name), toLower(e.name)) < $distance)\n          AND labels(e) = labels(node)\n      WITH node, score\n      ORDER BY node.name\n      RETURN collect(node) AS nodes\n    }\n    WITH distinct nodes\n    WHERE size(nodes) > 1\n    WITH collect([n in nodes | n.name]) AS results\n    UNWIND range(0, size(results)-1, 1) as index\n    WITH results, index, results[index] as result\n    WITH apoc.coll.sort(reduce(acc = result, index2 IN range(0, size(results)-1, 1) |\n            CASE WHEN index <> index2 AND\n                size(apoc.coll.intersection(acc, results[index2])) > 0\n                THEN apoc.coll.union(acc, results[index2])\n                ELSE acc\n            END\n    )) as combinedResult\n    WITH distinct(combinedResult) as combinedResult\n    // extra filtering\n    WITH collect(combinedResult) as allCombinedResults\n    UNWIND range(0, size(allCombinedResults)-1, 1) as combinedResultIndex\n    WITH allCombinedResults[combinedResultIndex] as combinedResult, combinedResultIndex, allCombinedResults\n    WHERE NOT any(x IN range(0,size(allCombinedResults)-1,1) \n        WHERE x <> combinedResultIndex\n        AND apoc.coll.containsAll(allCombinedResults[x], combinedResult)\n    )\n    RETURN combinedResult  \n    \"\"\", param_map={'cutoff': similarity_threshold, 'distance': word_edit_distance})\n    for row in data:\n        print(row)\n\nWithout getting into too many details, we use a combination of text embeddings\nand word distance to find potential duplicates in our graph. You can tune `\nsimilarity_threshold ` and ` word_distance ` to find the best combination that\ndetects as many duplicates without too much false positives. ", "mimetype": "text/plain", "start_char_idx": 2228, "end_char_idx": 10942, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b99dd4b-5607-4a9c-a4a1-51a5307b0977": {"__data__": {"id_": "1b99dd4b-5607-4a9c-a4a1-51a5307b0977", "embedding": null, "metadata": {"filename": "customizing-property-graph-index-in-llamaindex.md", "extension": ".md", "title": "Customizing property graph index in LlamaIndex", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "55a84224-699c-4b12-9c4b-12367806480c", "node_type": "4", "metadata": {"filename": "customizing-property-graph-index-in-llamaindex.md", "extension": ".md", "title": "Customizing property graph index in LlamaIndex", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamaindex"}, "hash": "acbe25d05327de8abfc6b12c09823aefbbf915a3e52ab410ca503b0b60695b5d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbd9d33d-6d02-462c-99e1-1bd7c177cab5", "node_type": "1", "metadata": {"filename": "customizing-property-graph-index-in-llamaindex.md", "extension": ".md", "title": "Customizing property graph index in LlamaIndex", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamaindex"}, "hash": "9a233f143cb5d9150736ce0202b10ae24fe2ca0962e5be2788c9ef23669b1f03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "451d84f2-5030-499f-b2e2-0a2761039850", "node_type": "1", "metadata": {}, "hash": "d50b3e53c993fb7aab80d2c1eaf9249ce292e08c8dae2c6fe03204e7a0c7c4b4", "class_name": "RelatedNodeInfo"}}, "text": "Unfortunately,\nentity disambiguation is a hard problem and there are no perfect solutions.\nWith this approach, we get quite good results, but there are some false\npositives in there as well:\n\n    \n    \n    ['1963 AFL Draft', '1963 NFL Draft']\n    ['June 14, 2023', 'June 15 2023']\n    ['BTC Halving', 'BTC Halving 2016', 'BTC Halving 2020', 'BTC Halving 2024', 'Bitcoin Halving', 'Bitcoin Halving 2024']\n\nIt is up to you to tweak the dials, and maybe add some manual exceptions\nbefore merging duplicate nodes.\n\n##  Implementing a custom retriever\n\nGreat, we have constructed a knowledge graph based on the news dataset. ", "mimetype": "text/plain", "start_char_idx": 10942, "end_char_idx": 11562, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "451d84f2-5030-499f-b2e2-0a2761039850": {"__data__": {"id_": "451d84f2-5030-499f-b2e2-0a2761039850", "embedding": null, "metadata": {"filename": "customizing-property-graph-index-in-llamaindex.md", "extension": ".md", "title": "Customizing property graph index in LlamaIndex", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "55a84224-699c-4b12-9c4b-12367806480c", "node_type": "4", "metadata": {"filename": "customizing-property-graph-index-in-llamaindex.md", "extension": ".md", "title": "Customizing property graph index in LlamaIndex", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamaindex"}, "hash": "acbe25d05327de8abfc6b12c09823aefbbf915a3e52ab410ca503b0b60695b5d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b99dd4b-5607-4a9c-a4a1-51a5307b0977", "node_type": "1", "metadata": {"filename": "customizing-property-graph-index-in-llamaindex.md", "extension": ".md", "title": "Customizing property graph index in LlamaIndex", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamaindex"}, "hash": "971509748795ec2be0ee7ca1ae5a98243a67d705df5eb71c2d742d5a337cc2a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8427a98a-8271-40c6-a476-4bb7c73b2576", "node_type": "1", "metadata": {}, "hash": "fb2fd1c443b6618ba09ca4a42f7713f457003119d52497aac6c201a46fdba016", "class_name": "RelatedNodeInfo"}}, "text": "Now,\nlet\u2019s examine our retriever options. At the moment, there are [ four existing\nretrievers available\n](https://docs.llamaindex.ai/en/latest/module_guides/indexing/lpg_index_guide/#retrieval-\nand-querying) :\n\n  * [ LLMSynonymRetriever ](https://docs.llamaindex.ai/en/latest/module_guides/indexing/lpg_index_guide/#default-llmsynonymretriever) : takes the query, and tries to generate keywords and synonyms to retrieve nodes (and therefore the paths connected to those nodes). \n  * [ VectorContextRetriever ](https://docs.llamaindex.ai/en/latest/module_guides/indexing/lpg_index_guide/#default-if-supported-vectorcontextretriever) : retrieves nodes based on their vector similarity, and then fetches the paths connected to those nodes \n  * [ TextToCypherRetriever ](https://docs.llamaindex.ai/en/latest/module_guides/indexing/lpg_index_guide/#texttocypherretriever) : uses a graph store schema, your query, and a prompt template in order to generate and execute a cypher query \n  * [ CypherTemplateRetriever ](https://docs.llamaindex.ai/en/latest/module_guides/indexing/lpg_index_guide/#cyphertemplateretriever) : Rather than letting the LLM have free-range of generating any cypher statement, we can instead provide a cypher template and have the LLM fill in the parameters. \n\nAdditionally, implementing a custom retriever is straightforward, so that is\nexactly what we will do here. ", "mimetype": "text/plain", "start_char_idx": 11562, "end_char_idx": 12948, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8427a98a-8271-40c6-a476-4bb7c73b2576": {"__data__": {"id_": "8427a98a-8271-40c6-a476-4bb7c73b2576", "embedding": null, "metadata": {"filename": "customizing-property-graph-index-in-llamaindex.md", "extension": ".md", "title": "Customizing property graph index in LlamaIndex", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "55a84224-699c-4b12-9c4b-12367806480c", "node_type": "4", "metadata": {"filename": "customizing-property-graph-index-in-llamaindex.md", "extension": ".md", "title": "Customizing property graph index in LlamaIndex", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamaindex"}, "hash": "acbe25d05327de8abfc6b12c09823aefbbf915a3e52ab410ca503b0b60695b5d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "451d84f2-5030-499f-b2e2-0a2761039850", "node_type": "1", "metadata": {"filename": "customizing-property-graph-index-in-llamaindex.md", "extension": ".md", "title": "Customizing property graph index in LlamaIndex", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamaindex"}, "hash": "393ec61bda78d66fbda457fa9e8a5f22f3b00b969964075dd4f035c47e8006ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0197b0e5-d1e8-426d-8bb8-2dfe11693e3e", "node_type": "1", "metadata": {}, "hash": "dc53b2f9a627e93ee958e62e20dacabb349944dcd8ce8b385f7d5beb1e52bb11", "class_name": "RelatedNodeInfo"}}, "text": "Our custom retriever will first identify\nentities in the input query and then execute the VectorContextRetriever for\neach identified entity separately.\n\nFirst, we will define the entity extraction model and prompt.\n\n    \n    \n    from pydantic import BaseModel\n    from typing import Optional, List\n    \n    \n    class Entities(BaseModel):\n        \"\"\"List of named entities in the text such as names of people, organizations, concepts, and locations\"\"\"\n        names: Optional[List[str]]\n    \n    \n    prompt_template_entities = \"\"\"\n    Extract all named entities such as names of people, organizations, concepts, and locations\n    from the following text:\n    {text}\n    \"\"\"\n\nNow we can progress to the custom retriever implementation.\n\n    \n    \n    from typing import Any, Optional\n    \n    from llama_index.core.embeddings import BaseEmbedding\n    from llama_index.core.retrievers import CustomPGRetriever, VectorContextRetriever\n    from llama_index.core.vector_stores.types import VectorStore\n    from llama_index.program.openai import OpenAIPydanticProgram\n    \n    \n    class MyCustomRetriever(CustomPGRetriever):\n        \"\"\"Custom retriever with entity detection.\"\"\"\n        def init(\n            self,\n            ## vector context retriever params\n            embed_model: Optional[BaseEmbedding] = None,\n            vector_store: Optional[VectorStore] = None,\n            similarity_top_k: int = 4,\n            path_depth: int = 1,\n            include_text: bool = True,\n            **kwargs: Any,\n        ) -> None:\n            \"\"\"Uses any kwargs passed in from class constructor.\"\"\"\n            self.entity_extraction = OpenAIPydanticProgram.from_defaults(\n                output_cls=Entities, prompt_template_str=prompt_template_entities\n            )\n            self.vector_retriever = VectorContextRetriever(\n                self.graph_store,\n                include_text=self.include_text,\n                embed_model=embed_model,\n                similarity_top_k=similarity_top_k,\n                path_depth=path_depth,\n            )\n    \n        def custom_retrieve(self, query_str: str) -> str:\n            \"\"\"Define custom retriever with entity detection.\n    \n            Could return `str`, `TextNode`, `NodeWithScore`, or a list of those.\n            \"\"\"\n            entities = self.entity_extraction(text=query_str).names\n            result_nodes = []\n            if entities:\n                print(f\"Detected entities: {entities}\")\n                for entity in entities:\n                    result_nodes.extend(self.vector_retriever.retrieve(entity))\n            else:\n                result_nodes.extend(self.vector_retriever.retrieve(query_str))\n            final_text = \"\\n\\n\".join(\n                [n.get_content(metadata_mode=\"llm\") for n in result_nodes]\n            )\n            return final_text\n\nThe ` MyCustomRetriever ` class has only two methods. You can use the ` init `\nmethod to instantiate any functions or classes you will be using in the\nretriever. In this example, we instantiate the entity detection OpenAI program\nalong with the vector context retriever.\n\nThe ` custom_retrieve ` method is called during retrieval. In our custom\nretriever implementation, we first identify any relevant entities in the text.\nIf any entities are found, we iterate and execute the vector context retriever\nfor each entity. On the other hand, if no entities are identified we pass the\nentire input to the vector context retriever.\n\nAs you can observe, you can easily customize the retriever for your use-case\nby incorporating existing retrievers or starting from scratch as you can\neasily execute Cypher statements by using the ` structured_query ` method of\nthe graph store.\n\n##  Question-answering flow\n\nLet\u2019s wrap it up by using the custom retriever to answer an example question.\nWe need to pass the retriever to the ` RetrieverQueryEngine ` .\n\n    \n    \n    from llama_index.core.query_engine import RetrieverQueryEngine\n    \n    custom_sub_retriever = MyCustomRetriever(\n        index.property_graph_store,\n        include_text=True,\n        vector_store=index.vector_store,\n        embed_model=embed_model\n    )\n    \n    query_engine = RetrieverQueryEngine.from_args(\n        index.as_retriever(sub_retrievers=[custom_sub_retriever]), llm=llm\n    )\n\nLet\u2019s test it out!\n\n    \n    \n    response = query_engine.query(\n        \"What do you know about Maliek Collins or Darragh O\u2019Brien?\"\n    )\n    print(str(response))\n    # Detected entities: ['Maliek Collins', \"Darragh O'Brien\"]\n    # Maliek Collins is a defensive tackle who has played for the Dallas Cowboys, Las Vegas Raiders, and Houston Texans. Recently, he signed a two-year contract extension with the Houston Texans worth $23 million, including a $20 million guarantee. This new deal represents a raise from his previous contract, where he earned $17 million with $8.5 million guaranteed. Collins is expected to be a key piece in the Texans' defensive line and fit well into their 4-3 alignment.\n    # Darragh O\u2019Brien is the Minister for Housing and has been involved in the State\u2019s industrial relations process and the Government. He was recently involved in a debate in the D\u00e1il regarding the pay and working conditions of retained firefighters, which led to a heated exchange and almost resulted in the suspension of the session. ", "mimetype": "text/plain", "start_char_idx": 12948, "end_char_idx": 18275, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0197b0e5-d1e8-426d-8bb8-2dfe11693e3e": {"__data__": {"id_": "0197b0e5-d1e8-426d-8bb8-2dfe11693e3e", "embedding": null, "metadata": {"filename": "customizing-property-graph-index-in-llamaindex.md", "extension": ".md", "title": "Customizing property graph index in LlamaIndex", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "55a84224-699c-4b12-9c4b-12367806480c", "node_type": "4", "metadata": {"filename": "customizing-property-graph-index-in-llamaindex.md", "extension": ".md", "title": "Customizing property graph index in LlamaIndex", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamaindex"}, "hash": "acbe25d05327de8abfc6b12c09823aefbbf915a3e52ab410ca503b0b60695b5d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8427a98a-8271-40c6-a476-4bb7c73b2576", "node_type": "1", "metadata": {"filename": "customizing-property-graph-index-in-llamaindex.md", "extension": ".md", "title": "Customizing property graph index in LlamaIndex", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamaindex"}, "hash": "3c4fa5dbae0d0d05c32abe0fd7bb8b0975cd0b0b1e87594fcb5c8722dd0b3536", "class_name": "RelatedNodeInfo"}}, "text": "O\u2019Brien expressed confidence that the dispute could be resolved and encouraged unions to re-engage with the industrial relations process.\n\n##  Summary\n\nIn this blog post, we\u2019ve explored the intricacies of customizing the property\ngraph index within LlamaIndex, focusing on implementing entity deduplication\nand designing custom retrieval methods to enhance GraphRAG accuracy. The\nproperty graph index allows for a modular and flexible approach, utilizing\nvarious graph constructors and retrievers to tailor the implementation to your\nspecific needs. Whether you\u2019re building your first knowledge graph or\noptimizing for a unique dataset, these customizable components offer a\npowerful toolkit. We invite you to test out the property graph index\nintegration to see how they can elevate your knowledge graph projects.\n\nAs always, the code is available on [ GitHub\n](https://github.com/tomasonjo/blogs/blob/master/llm/llama_index_neo4j_custom_retriever.ipynb)\n.\n\n", "mimetype": "text/plain", "start_char_idx": 18275, "end_char_idx": 19234, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd99a727-efbb-473e-91a1-1d712d6dc12a": {"__data__": {"id_": "fd99a727-efbb-473e-91a1-1d712d6dc12a", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-06-11.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-11", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-11"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2337ae24-e176-4aca-95bb-15c5bc0857de", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-06-11.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-11", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-11"}, "hash": "d5c925198c3f457299d0d42ecf3b112d915042948ef59778ee4567a25803ef99", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25770b6f-adbf-4d62-b8fd-0efc198cf728", "node_type": "1", "metadata": {}, "hash": "bc085593094b65346aafcd8d830d98054ceaf46c666f33e16ee9fa1662ee15aa", "class_name": "RelatedNodeInfo"}}, "text": "Hello Llama Fans\n\nStep into this week's edition of the LlamaIndex newsletter, where we bring you\na slew of exciting updates, in-depth guides, demos, enriching educational\ntutorials, and webinars designed to enhance your experience and understanding\nof our platforms and tools.\n\n##  **The highlights:**\n\n  * **Enhanced Memory Modules:** New memory modules in LlamaIndex boost agentic RAG capabilities with Vector Memory for message storage and retrieval, and Simple Composable Memory for integrating multiple memory sources. [ Notebook1 ](https://docs.llamaindex.ai/en/stable/examples/agent/memory/vector_memory/) , [ Notebook2 ](https://docs.llamaindex.ai/en/stable/examples/agent/memory/composable_memory/) , [ Tweet ](https://x.com/llama_index/status/1799114410985988399) . \n  * **Create-llama and E2B Integration:** Launched integration turns agents into advanced data analysts, enabling Python coding for data analysis and generating detailed files like graph images. [ Tweet ](https://x.com/llama_index/status/1799176083381866757) . \n  * **LlamaParse and Knowledge Graphs:** [ Guide ](https://github.com/run-llama/llama_parse/blob/main/examples/knowledge_graphs/kg_agent.ipynb) on integrating LlamaParse with Knowledge Graphs to develop RAG pipelines and agents for complex query handling. \n  * **Prometheus-2 RAG Evaluation:** [ Guide ](https://docs.llamaindex.ai/en/latest/examples/cookbooks/prometheus2_cookbook/) on using Prometheus-2, an affordable, transparent LLM based on Mistral models for effective RAG application evaluation with customized criteria. \n  * **Agentic RAG :** [ Video tutorial ](https://www.youtube.com/watch?v=MXPYbjjyHXc) on Agentic RAG covering memory, planning, and reasoning, enhancing knowledge retrieval and agent capabilities. \n\n##  **Feature Releases and Enhancements:**\n\n  1. We have introduced new memory modules in LlamaIndex for enhancing agentic RAG capabilities. The Vector Memory module enables storage and retrieval of user messages using vector search, while the Simple Composable Memory module allows for integration of multiple memory sources. [ Notebook1 ](https://docs.llamaindex.ai/en/stable/examples/agent/memory/vector_memory/) , [ Notebook2 ](https://docs.llamaindex.ai/en/stable/examples/agent/memory/composable_memory/) , [ Tweet ](https://x.com/llama_index/status/1799114410985988399) . \n  2. We have launched an integration between Create-llama and E2B\u2019s sandbox, transforming agents into powerful data analysts. This new feature allows agents to write Python code for data analysis and return comprehensive files, like graph images, enhancing the scope of what agents can accomplish. [ Tweet ](https://x.com/llama_index/status/1799176083381866757) . \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2714, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25770b6f-adbf-4d62-b8fd-0efc198cf728": {"__data__": {"id_": "25770b6f-adbf-4d62-b8fd-0efc198cf728", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-06-11.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-11", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-11"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2337ae24-e176-4aca-95bb-15c5bc0857de", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-06-11.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-11", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-11"}, "hash": "d5c925198c3f457299d0d42ecf3b112d915042948ef59778ee4567a25803ef99", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd99a727-efbb-473e-91a1-1d712d6dc12a", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-06-11.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-11", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-11"}, "hash": "025e35b7388cb995890b898c59024324c73f0a9528f0e77575c25e69365fd4e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a53f81d-7e22-4b61-b8ce-3c46c1b2e8b3", "node_type": "1", "metadata": {}, "hash": "b1508860fb4fe6ab14976bdd815bde0ac378a1d6c80d28e5603319e1c062d88b", "class_name": "RelatedNodeInfo"}}, "text": "3. We have launched an integration with Nomic-Embed-Vision that transforms Nomic-Embed-Text into a multimodal embedding that excels in handling image, text, and combined tasks, outperforming OpenAI CLIP with open access for all. [ Notebook ](https://github.com/zanussbaum/llama_index/blob/aeaaec27a21064836e4dbf956333e4d66bb41e1d/docs/docs/examples/multi_modal/multi_modal_rag_nomic.ipynb) . \n\n##  **Guides:**\n\n  * [ Guide ](https://github.com/run-llama/llama_parse/blob/main/examples/knowledge_graphs/kg_agent.ipynb) to Integrating LlamaParse with Knowledge Graphs to develop a RAG pipeline for sophisticated query retrieval, and create an agent capable of answering complex queries effectively. \n  * [ Guide ](https://docs.llamaindex.ai/en/latest/examples/cookbooks/prometheus2_cookbook/) to Using Prometheus-2 for RAG Evaluation for assessing RAG applications, built on Mistral base models, it offers an affordable and transparent solution for evaluation, capable of direct assessments, pairwise rankings, and tailored criteria, ensuring alignment with human judgments. \n  * [ Guide ](https://generativeai.pub/advanced-rag-retrieval-strategy-query-rewriting-a1dd61815ff0) to Three Forms of Query Rewriting for RAG to enhance RAG pipelines with techniques like sub-question decomposition, HyDE for aligning questions with embedding semantics, and step-back prompting for tackling complex queries more effectively. \n\n##  **Demos:**\n\n  * [ Laurie Voss ](https://x.com/seldo) \u2019s [ LLM-powered file organizer project ](https://github.com/run-llama/file-organizer) that categorizes files into folders based on LLM-decided categories without renaming them, ensuring important filenames remain intact. ", "mimetype": "text/plain", "start_char_idx": 2714, "end_char_idx": 4411, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a53f81d-7e22-4b61-b8ce-3c46c1b2e8b3": {"__data__": {"id_": "6a53f81d-7e22-4b61-b8ce-3c46c1b2e8b3", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-06-11.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-11", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-11"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2337ae24-e176-4aca-95bb-15c5bc0857de", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-06-11.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-11", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-11"}, "hash": "d5c925198c3f457299d0d42ecf3b112d915042948ef59778ee4567a25803ef99", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25770b6f-adbf-4d62-b8fd-0efc198cf728", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-06-11.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-11", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-11"}, "hash": "431a9fc22d01507d863fd7466603475a96250503d4ed9d0d4301453f2d737568", "class_name": "RelatedNodeInfo"}}, "text": "It organizes your files in multiple passes to balance folder sizes, resulting in descriptive yet practical folder names to help you find files easily. \n\n##  **Tutorials:**\n\n  * [ Laurie Voss ](https://x.com/seldo) \u2019s [ video tutorial ](https://www.youtube.com/watch?v=MXPYbjjyHXc) on transitioning from basic RAG to fully agentic knowledge retrieval, featuring real-world code examples that cover routing, memory, planning, tool use, and advanced agentic reasoning methods like Chain of Thought and Tree of Thought, along with insights into observability, controllability, and customizability. \n  * [ Prince krampah ](https://x.com/Prince_krampah) 's [ tutorials ](https://x.com/llama_index/status/1799463683179098203) on Agentic RAG Systems, offering comprehensive insights into advanced system building with detailed explanations on router query engines, function calling, and multi-step reasoning across complex documents. \n  * [ kingzzm\u2019s ](https://x.com/kingzzm) [ tutorial ](https://generativeai.pub/advanced-rag-retrieval-strategy-query-rewriting-a1dd61815ff0) on Three Forms of Query Rewriting for RAG to enhance RAG pipelines with techniques like sub-question decomposition, HyDE for aligning questions with embedding semantics, and step-back prompting for tackling complex queries more effectively. \n  * [ Rajdeep Borgohain ](https://x.com/rborgohain4) 's [ tutorial ](https://docs.inferless.com/cookbook/serverless-customer-service-bot) to build a customer-support voicebot with advanced features like speech-to-text and text-to-speech, integrated into a RAG pipeline for efficient handling of customer support exchanges using Inferless, LlamaIndex, faster-whisper, Piper, and Pinecone. \n  * [ Pavan Mantha ](https://x.com/pavan_mantha1) 's [ tutorial ](https://towardsdev.com/production-ready-secure-and-powerful-ai-implementations-with-azure-services-671b68631212) on securing RAG apps using Azure for application security, including identity management, secure key storage, and managed Qdrant. \n\n##  **Webinar:**\n\n  * [ Join us ](https://lu.ma/kqxmbuou) for our webinar with [ Tomaz Bratanic ](https://x.com/tb_tomaz) from Neo4j on LlamaIndex property graph for insights into high-level and low-level graph construction, retrieval, and knowledge graph agents. \n\n", "mimetype": "text/plain", "start_char_idx": 4411, "end_char_idx": 6687, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d3e257e-727f-4ccf-9633-dc8fa0284ea8": {"__data__": {"id_": "6d3e257e-727f-4ccf-9633-dc8fa0284ea8", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-06-04.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-04", "date": "Jun 4, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-04"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8f6774da-c6df-4b05-86e6-d291f430d976", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-06-04.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-04", "date": "Jun 4, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-04"}, "hash": "5eb346b119eb940a946d9477b2aaf097768a771db1c2ca28ea3f2629bed32a6c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cfde95e4-5a2e-4d6b-9653-b14131bd52a2", "node_type": "1", "metadata": {}, "hash": "c699f9f3fdc4f957ab789facc28063ce91c67cb1d1d74b960ec762ca07004312", "class_name": "RelatedNodeInfo"}}, "text": "Hello, LlamaIndex Family!\n\nWe're thrilled to connect with you again and bring you the latest and greatest\nfrom the world of LlamaIndex. This week, we're excited to present an array of\nupdates and a diverse lineup of content designed to enhance your LlamaIndex\nexperience, particularly when working with Knowledge Graphs. From integrations\nand guides to demos and tutorials, we've got you covered with all the tools\nand insights you need.\n\n##  **The highlights:**\n\n  * **Elevating Knowledge Graphs:** The Property Graph Index, introduced in LlamaIndex, transforms how knowledge graphs (KGs) are built and queried. This powerful toolkit enhances graph searches with vector capabilities. [ Docs ](https://docs.llamaindex.ai/en/stable/module_guides/indexing/lpg_index_guide/) , [ Tweet ](https://x.com/llama_index/status/1795869279457546447) . \n  * **Spreadsheet Insights with LlamaParse:** LlamaParse now supports spreadsheet parsing, turning complex Excel files into LLM-friendly tables for improved performance and data handling. [ Notebook ](https://github.com/run-llama/llama_parse/blob/main/examples/demo_excel.ipynb) , [ Tweet ](https://x.com/llama_index/status/1796237002364613040) . \n  * **Code Generation with Codestral:** Codestral, a cutting-edge model from MistralAI, is now integrated into LlamaIndex. This code-generating tool supports over 80 programming languages. [ Docs ](https://docs.llamaindex.ai/en/latest/examples/cookbooks/codestral/) , [ Tweet ](https://x.com/llama_index/status/1795900182439276731) . \n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1525, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cfde95e4-5a2e-4d6b-9653-b14131bd52a2": {"__data__": {"id_": "cfde95e4-5a2e-4d6b-9653-b14131bd52a2", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-06-04.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-04", "date": "Jun 4, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-04"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8f6774da-c6df-4b05-86e6-d291f430d976", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-06-04.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-04", "date": "Jun 4, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-04"}, "hash": "5eb346b119eb940a946d9477b2aaf097768a771db1c2ca28ea3f2629bed32a6c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d3e257e-727f-4ccf-9633-dc8fa0284ea8", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-06-04.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-04", "date": "Jun 4, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-04"}, "hash": "3dd13b198f9cf2f906d49afbd4e4bdfee03fe3d58a67b7561aba85267abde987", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f24f694-3817-41bf-a19a-5a9c44e58d6c", "node_type": "1", "metadata": {}, "hash": "7b269760f396ae311bcd3e2cd6c5f5d9c340c131d08864e950b6ca4da16f3ec0", "class_name": "RelatedNodeInfo"}}, "text": "##  **Feature Releases and Enhancements:**\n\n  1. We have introduced the Property Graph Index, a major feature that establishes LlamaIndex as the premier framework for building knowledge graphs (KGs) with LLMs. This sophisticated toolkit enables the construction and querying of KGs, allowing for joint vector and graph searches even in graph stores that lack native vector support. [ Docs ](https://docs.llamaindex.ai/en/stable/module_guides/indexing/lpg_index_guide/) , [ Tweet ](https://x.com/llama_index/status/1795869279457546447) . \n  ", "mimetype": "text/plain", "start_char_idx": 1525, "end_char_idx": 2065, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f24f694-3817-41bf-a19a-5a9c44e58d6c": {"__data__": {"id_": "9f24f694-3817-41bf-a19a-5a9c44e58d6c", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-06-04.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-04", "date": "Jun 4, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-04"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8f6774da-c6df-4b05-86e6-d291f430d976", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-06-04.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-04", "date": "Jun 4, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-04"}, "hash": "5eb346b119eb940a946d9477b2aaf097768a771db1c2ca28ea3f2629bed32a6c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cfde95e4-5a2e-4d6b-9653-b14131bd52a2", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-06-04.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-04", "date": "Jun 4, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-04"}, "hash": "fbcd12a5256e5866430477c2469612d4d81a72b2ea834e46f3b039f972c467e4", "class_name": "RelatedNodeInfo"}}, "text": "2. We have launched support for parsing spreadsheets in LlamaParse, allowing you to convert complex Excel files and other spreadsheet formats into clean, LLM-friendly tables for improved RAG pipeline performance. [ Notebook ](https://github.com/run-llama/llama_parse/blob/main/examples/demo_excel.ipynb) , [ Tweet ](https://x.com/llama_index/status/1796237002364613040) . \n  3. We have integrated Codestral from MistralAI into LlamaIndex, providing day 0 support for this cutting-edge code-generating model trained on over 80 programming languages. [ Docs ](https://docs.llamaindex.ai/en/latest/examples/cookbooks/codestral/) , [ Tweet ](https://x.com/llama_index/status/1795900182439276731) . \n  4. We have integrated PostgresML into LlamaIndex, perfect for those who love Postgres and want to build AI applications. It serves open-source models locally, handles embeddings, and allows you to train or fine-tune models directly in Python and JavaScript. [ Blogpost ](https://www.llamaindex.ai/blog/simplify-your-rag-application-architecture-with-llamaindex-postgresml) , [ Tweet ](https://x.com/llama_index/status/1795561227319734360) . \n  5. We have integrated with Milvus Lite to provide an easy start to vector search, offering day-1 support with LlamaIndex. [ Docs ](https://docs.llamaindex.ai/en/stable/examples/vector_stores/MilvusIndexDemo/) , [ Tweet ](https://x.com/llama_index/status/1796305277073174654) . \n\n##  **Guides:**\n\n  * [ Guide ](https://docs.llamaindex.ai/en/stable/examples/property_graph/property_graph_custom_retriever/) to Building a Custom Graph Retriever to create a custom graph retriever for your specific needs by combining vector search and graph search with reranking for improved results. \n  * [ Guide ](https://docs.llamaindex.ai/en/stable/examples/llm/nvidia_nim/) to Building GenAI Applications in minutes with NVIDIA's NIM inference microservices, offering an easy and fast way to deploy GenAI applications. This step-by-step guide teaches you how to run models, generate embeddings, and re-rank data for optimal results. \n  * [ Guide ](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/property_graph/property_graph_advanced.ipynb) to Constructing Knowledge Graphs with LLMs**,** build knowledge graphs using local models and Neo4j, starting with defining entities and relationships, using SchemaLLMPathExtractor to create structured graphs, and querying to uncover insights. \n\n##  **Demos:**\n\n  * [ Omakase RAG Orchestrator ](https://github.com/ammirsm/llamaindex-omakase-rag) , a project developed by [ Amir Mehr ](https://x.com/thatisamir) , is a web app template designed to help you build scalable RAG applications using Django, LlamaIndex, and Google Drive. It features a full-featured RAG API, data source management, user access control, and an admin panel. \n  * [ gmail-extractor ](https://github.com/run-llama/gmail-extractor) , a project by Laurie project that trains a Python script with an LLM to extract structured data from Gmail. By iteratively improving the script based on email data, the LLM can effectively modify and enhance it to extract information with precision. \n\n##  **Tutorials:**\n\n  * Sherlock Xu\u2019s [ tutorial ](https://www.bentoml.com/blog/serving-a-llamaindex-rag-app-as-rest-apis) from BentoML on Serving A LlamaIndex RAG App as REST APIs. \n\n##  **Papers:**\n\n  * FinTextQA, a new benchmark dataset for long-form financial question answering, has been introduced by Jian Chen and their team. This benchmark was evaluated using LlamaIndex's Auto-Merging and Sentence Window Retrievers, along with various embeddings, rerankers, and LLMs, offering a comprehensive question-answering system for financial text. \n\n##  **Webinar:**\n\n  * [ Webinar ](https://www.youtube.com/watch?v=o0DPxvgML5c) with authors of memary - Julian Saks, Kevin Li, Seyeong Han. Memary is a fully open-source reference implementation for long-term memory in autonomous agents \n\n##  **Events:**\n\n  * [ Join ](https://www.meetup.com/nlp_london/events/301171675/) Pierre from LlamaIndex along with speakers from Weaviate, and Weights & Biases on June 12th at the London NLP meetup, focusing on the challenges and solutions for using LLMs with financial services data in production settings. \n\n", "mimetype": "text/plain", "start_char_idx": 2065, "end_char_idx": 6316, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "570cd9f5-8273-46a3-b34e-1f581f3c11c3": {"__data__": {"id_": "570cd9f5-8273-46a3-b34e-1f581f3c11c3", "embedding": null, "metadata": {"filename": "introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms.md", "extension": ".md", "title": "Introducing the Property Graph Index: A Powerful New Way to Build Knowledge Graphs with LLMs", "date": "May 29, 2024", "url": "https://www.llamaindex.ai/blog/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a80607e8-6bf5-42bf-9cbe-31a1388845ba", "node_type": "4", "metadata": {"filename": "introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms.md", "extension": ".md", "title": "Introducing the Property Graph Index: A Powerful New Way to Build Knowledge Graphs with LLMs", "date": "May 29, 2024", "url": "https://www.llamaindex.ai/blog/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms"}, "hash": "28133e960d2036f05278db90017e1f960d24eec068aa9460495a7fed74b28a4d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "153d06e5-e3cb-48f0-9544-c2640cfc237e", "node_type": "1", "metadata": {}, "hash": "2f05a7fba9774951b7fe1de099b61a0a67a6ee10fafda7da009dc56589c4dcf9", "class_name": "RelatedNodeInfo"}}, "text": "We're thrilled to announce a new feature in LlamaIndex that expands our\nknowledge graph capabilities to be more flexible, extendible, and robust.\nIntroducing the Property Graph Index!\n\n##  Why Property Graphs?\n\nTraditional knowledge graph representations like knowledge triples (subject,\npredicate, object) are limited in expressiveness. They lack the ability to:\n\n  * Assign labels and properties to nodes and relationships \n  * Represent text nodes as vector embeddings \n  * Perform both vector and symbolic retrieval \n\nOur existing ` KnowledgeGraphIndex ` was burdened with these limitations, as\nwell as general limitations on the architecture of the index itself.\n\nThe Property Graph Index solves these issues. By using a labeled property\ngraph representation, it enables far richer modeling, storage and querying of\nyour knowledge graph.\n\nWith Property Graphs, you can:\n\n  * Categorize nodes and relationships into types with associated metadata \n  * Treat your graph as a superset of a vector database for hybrid search \n  * Express complex queries using the Cypher graph query language \n\nThis makes Property Graphs a powerful and flexible choice for building\nknowledge graphs with LLMs.\n\n##  Constructing Your Graph\n\nThe Property Graph Index offers several ways to extract a knowledge graph from\nyour data, and you can combine as many as you want:\n\n**1\\. Schema-Guided Extraction** : Define allowed entity types, relationship\ntypes, and their connections in a schema. The LLM will only extract graph data\nthat conforms to this schema.\n\n    \n    \n    from llama_index.indices.property_graph import SchemaLLMPathExtractor\n    \n    entities = Literal[\"PERSON\", \"PLACE\", \"THING\"]\n    relations = Literal[\"PART_OF\", \"HAS\", \"IS_A\"]\n    schema = {\n        \"PERSON\": [\"PART_OF\", \"HAS\", \"IS_A\"],\n        \"PLACE\": [\"PART_OF\", \"HAS\"], \n        \"THING\": [\"IS_A\"],\n    }\n    \n    kg_extractor = SchemaLLMPathExtractor(\n      llm=llm, \n      possible_entities=entities, \n      possible_relations=relations, \n      kg_validation_schema=schema,\n      strict=True,  # if false, allows values outside of spec\n    )\n\n**2\\. Implicit Extraction** : Use LlamaIndex constructs to specify\nrelationships between nodes in your data. The graph will be built based on the\n` node.relationships ` attribute. For example, when running a document through\na node parser, the ` PREVIOUS ` , ` NEXT ` and ` SOURCE ` relationships will\nbe captured.\n\n    \n    \n    from llama_index.core.indices.property_graph import ImplicitPathExtractor\n    \n    kg_extractor = ImplicitPathExtractor()\n\n**3\\. Free-Form Extraction** : Let the LLM infer the entities, relationship\ntypes and schema directly from your data in a free-form manner. (This is\nsimilar to how the ` KnowledgeGraphIndex ` works today.)\n\n    \n    \n    from llama_index.core.indices.property_graph import SimpleLLMPathExtractor\n    \n    kg_extractor = SimpleLLMPathExtractor(llm=llm)\n\nMix and match these extraction approaches for fine-grained control over your\ngraph structure.\n\n    \n    \n    from llama_index.core import PropertyGraphIndex\n    \n    index = PropertyGraphIndex.from_documents(docs, kg_extractors=[...])\n\n###  Embeddings\n\nBy default, all graph nodes are embedded. While some graph databases support\nembeddings natively, you can also specify and use any vector store from\nLlamaIndex on top of your graph database.\n\n    \n    \n    index = PropertyGraphIndex(..., vector_store=vector_store)\n\n##  Querying Your Graph\n\nThe Property Graph Index supports a wide variety of querying techniques that\ncan be combined and run concurrently.\n\n**1\\. Keyword/Synonym-Based Retrieval** : Expand your query into relevant\nkeywords and synonyms and find matching nodes.\n\n    \n    \n    from llama_index.core.indices.property_graph import LLMSynonymRetriever\n    \n    sub_retriever = LLMSynonymRetriever(index.property_graph_store, llm=llm)\n\n**2\\. Vector Similarity** : Retrieve nodes based on the similarity of their\nvector representations to your query.\n\n    \n    \n    from llama_index.core.indices.property_graph import VectorContextRetriever\n    \n    sub_retriever = VectorContextRetriever(\n      index.property_graph_store, \n      vector_store=index.vector_store,\n      embed_model=embed_model,\n    )\n\n**3\\. Cypher Queries** : Use the expressive Cypher graph query language to\nspecify complex graph patterns and traverse multiple relationships.\n\n    \n    \n    from llama_index.core.indices.property_graph import CypherTemplateRetriever\n    from llama_index.core.bridge.pydantic import BaseModel, Field\n    \n    class Params(BaseModel):\n     \u201c\u201d\u201dParameters for a cypher query.\u201d\u201d\u201d\n     names: list[str] = Field(description=\u201dA list of possible entity names or keywords related to the query.\u201d)\n     \n    cypher_query = \"\"\"\n       MATCH (c:Chunk)-[:MENTIONS]->(o) \n       WHERE o.name IN $names\n       RETURN c.text, o.name, o.label;\n    \"\"\"\n       \n    sub_retriever = CypherTemplateRetriever(\n     index.property_graph_store, \n     Params, \n     cypher_query,\n     llm=llm,\n    )\n\nInstead of providing a template, you can also let the LLM write the entire\ncypher query based on context from the query and database:\n\n    \n    \n    from llama_index.core.indices.property_graph import TextToCypherRetriever\n    \n    sub_retriever = TextToCypherRetriever(index.property_graph_store, llm=llm)\n\n**4\\. Custom Graph Traversal** : Define your own graph traversal logic by\nsubclassing key retriever components.\n\nThese retrievers can be combined and composed for hybrid search that leverages\nboth the graph structure and vector representations of nodes.\n\n    \n    \n    from llama_index.indices.property_graph import VectorContextRetriever, LLMSynonymRetriever\n    \n    vector_retriever = VectorContextRetriever(index.property_graph_store, embed_model=embed_model)  \n    synonym_retriever = LLMSynonymRetriever(index.property_graph_store, llm=llm)\n    \n    retriever = index.as_retriever(sub_retrievers=[vector_retriever, synonym_retriever])\n\n##  Using the Property Graph Store\n\nUnder the hood, the Property Graph Index uses a ` PropertyGraphStore `\nabstraction to store and retrieve graph data. You can also use this store\ndirectly for lower-level control.\n\nThe store supports:\n\n  * Inserting and updating nodes, relationships and properties \n  * Querying nodes by ID or properties \n  * Retrieving relationship paths from a starting node \n  * Executing Cypher queries (if the backing store supports it) \n\n    \n    \n    from llama_index.graph_stores.neo4j import Neo4jPGStore\n    \n    graph_store = Neo4jPGStore(\n        username=\"neo4j\",\n        password=\"password\",\n        url=\"bolt://localhost:7687\",\n    )\n    \n    # insert nodes\n    nodes = [\n        EntityNode(name=\"llama\", label=\"ANIMAL\", properties={\"key\": \"value\"}),\n        EntityNode(name=\"index\", label=\"THING\", properties={\"key\": \"value\"}), \n    ]\n    graph_store.upsert_nodes(nodes)\n    \n    # insert relationships  \n    relations = [\n        Relation(\n            label=\"HAS\",\n            source_id=nodes[0].id, \n            target_id=nodes[1].id,\n        )\n    ]\n    graph_store.upsert_relations(relations)\n    \n    # query nodes\n    llama_node = graph_store.get(properties={\"name\": \"llama\"})[0]\n    \n    # get relationship paths  \n    paths = graph_store.get_rel_map([llama_node], depth=1)\n    \n    # run Cypher query\n    results = graph_store.structured_query(\"MATCH (n) RETURN n LIMIT 10\")  \n\nSeveral backing stores are supported, including in-memory, disk-based, and\nNeo4j.\n\n##  Learn More\n\n  * [ Property Graph Index Overview ](https://docs.llamaindex.ai/en/stable/module_guides/indexing/lpg_index_guide/)\n  * [ Basic Usage Notebook ](https://docs.llamaindex.ai/en/stable/examples/property_graph/property_graph_basic/)\n  * [ Advanced Usage with Neo4j ](https://docs.llamaindex.ai/en/stable/examples/property_graph/property_graph_advanced/)\n  * [ Using the Property Graph Store Directly ](https://docs.llamaindex.ai/en/stable/examples/property_graph/property_graph_neo4j/)\n\nA huge thanks to our partners at [ Neo4j ](https://neo4j.com/) for their\ncollaboration on this launch, especially [ Tomaz Bratanic\n](https://www.linkedin.com/in/tomazb/) for the detailed integration guide and\ndesign guidance.\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 8182, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "153d06e5-e3cb-48f0-9544-c2640cfc237e": {"__data__": {"id_": "153d06e5-e3cb-48f0-9544-c2640cfc237e", "embedding": null, "metadata": {"filename": "introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms.md", "extension": ".md", "title": "Introducing the Property Graph Index: A Powerful New Way to Build Knowledge Graphs with LLMs", "date": "May 29, 2024", "url": "https://www.llamaindex.ai/blog/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a80607e8-6bf5-42bf-9cbe-31a1388845ba", "node_type": "4", "metadata": {"filename": "introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms.md", "extension": ".md", "title": "Introducing the Property Graph Index: A Powerful New Way to Build Knowledge Graphs with LLMs", "date": "May 29, 2024", "url": "https://www.llamaindex.ai/blog/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms"}, "hash": "28133e960d2036f05278db90017e1f960d24eec068aa9460495a7fed74b28a4d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "570cd9f5-8273-46a3-b34e-1f581f3c11c3", "node_type": "1", "metadata": {"filename": "introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms.md", "extension": ".md", "title": "Introducing the Property Graph Index: A Powerful New Way to Build Knowledge Graphs with LLMs", "date": "May 29, 2024", "url": "https://www.llamaindex.ai/blog/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms"}, "hash": "c5b7b94b4d859ba9d7b3d1153e791a3defb1ae3e483c904d49583721244d148a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c419a347-46eb-4183-a3d4-7be3c5ec2d81", "node_type": "1", "metadata": {}, "hash": "a78c97437a30db6f3c8841edf459935b9dd07472796914d1014402286034f062", "class_name": "RelatedNodeInfo"}}, "text": "We can't wait to see what you build with the new Property Graph Index! As\nalways, feel free to join our [ Discord ](http://discord.gg/dGcwcsnxhU) to\nshare your projects, ask questions, and get support from the community.\n\n", "mimetype": "text/plain", "start_char_idx": 8182, "end_char_idx": 8404, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c419a347-46eb-4183-a3d4-7be3c5ec2d81": {"__data__": {"id_": "c419a347-46eb-4183-a3d4-7be3c5ec2d81", "embedding": null, "metadata": {"filename": "introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms.md", "extension": ".md", "title": "Introducing the Property Graph Index: A Powerful New Way to Build Knowledge Graphs with LLMs", "date": "May 29, 2024", "url": "https://www.llamaindex.ai/blog/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a80607e8-6bf5-42bf-9cbe-31a1388845ba", "node_type": "4", "metadata": {"filename": "introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms.md", "extension": ".md", "title": "Introducing the Property Graph Index: A Powerful New Way to Build Knowledge Graphs with LLMs", "date": "May 29, 2024", "url": "https://www.llamaindex.ai/blog/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms"}, "hash": "28133e960d2036f05278db90017e1f960d24eec068aa9460495a7fed74b28a4d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "153d06e5-e3cb-48f0-9544-c2640cfc237e", "node_type": "1", "metadata": {"filename": "introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms.md", "extension": ".md", "title": "Introducing the Property Graph Index: A Powerful New Way to Build Knowledge Graphs with LLMs", "date": "May 29, 2024", "url": "https://www.llamaindex.ai/blog/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms"}, "hash": "dec74983cdb082ca206d9d06b677a0dff5060a86c1162ea8da67f325afe93a53", "class_name": "RelatedNodeInfo"}}, "text": "Happy building!\n\nThe LlamaIndex Team\n\n", "mimetype": "text/plain", "start_char_idx": 8404, "end_char_idx": 8442, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4547f9cd-fc3a-4db5-864a-a62a63dcc668": {"__data__": {"id_": "4547f9cd-fc3a-4db5-864a-a62a63dcc668", "embedding": null, "metadata": {"filename": "simplify-your-rag-application-architecture-with-llamaindex-postgresml.md", "extension": ".md", "title": "Simplify your RAG application architecture with LlamaIndex + PostgresML", "date": "May 28, 2024", "url": "https://www.llamaindex.ai/blog/simplify-your-rag-application-architecture-with-llamaindex-postgresml"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "45edea59-d0cf-49c5-83a4-240649039ec3", "node_type": "4", "metadata": {"filename": "simplify-your-rag-application-architecture-with-llamaindex-postgresml.md", "extension": ".md", "title": "Simplify your RAG application architecture with LlamaIndex + PostgresML", "date": "May 28, 2024", "url": "https://www.llamaindex.ai/blog/simplify-your-rag-application-architecture-with-llamaindex-postgresml"}, "hash": "2224e0455fac6f885d92b9c08aecec8ceadac8a390451d094c71be1bddf71f20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fdbe389e-850d-41d4-9212-c783cdd76265", "node_type": "1", "metadata": {}, "hash": "50d35c3d74af882b285988105b8157602e6183d91995bd1eada5e5352cad99e8", "class_name": "RelatedNodeInfo"}}, "text": "We\u2019re happy to announce the recent integration of LlamaIndex with PostgresML \u2014\na comprehensive machine learning platform built on PostgreSQL. The PostgresML\nManaged Index allows LlamaIndex users to seamlessly manage document storage,\nsplitting, embedding, and retrieval. By using PostgresML as the backend, users\nbenefit from a streamlined and optimized process for Retrieval-Augmented\nGeneration (RAG). This integration unifies embedding, vector search, and text\ngeneration into a single network call, resulting in faster, more reliable, and\neasier-to-manage RAG workflows.\n\n**The problem with typical RAG workflows**\n\nTypical Retrieval-Augmented Generation (RAG) workflows come with significant\ndrawbacks, particularly for users.\n\nPoor performance is a major issue, as these workflows involve multiple network\ncalls to different services for embedding, vector storage, and text\ngeneration, leading to increased latency. Additionally, there are privacy\nconcerns when sensitive data is sent to various LLM providers. These user-\ncentric issues are compounded by other challenges:\n\n  * Increased dev time to master new technologies \n  * Complicated maintenance and scalability issues due to multiple points of failure \n  * Costly vendors required for multiple services \n\nThe diagram above illustrates the complexity, showing how each component\ninteracts across different services \u2014 exacerbating these problems.\n\n**Solution**\n\nThe PostgresML Managed Index offers a comprehensive solution to the challenges\nof typical RAG workflows.\n\nBy managing document storage, splitting, embedding generation, and retrieval\nall within a single system, PostgresML significantly reduces dev time, scaling\ncosts, and overall spend when you eliminate the need for multiple point\nsolutions. Most importantly, it enhances the user experience by consolidating\nembedding, vector search, and text generation into a single network call \u2014\nresulting in improved performance and reduced latency. Additionally, the use\nof open-source models ensures transparency and flexibility, while operating\nwithin the database addresses privacy concerns and provides users with a\nsecure and efficient RAG workflow.\n\n**About PostgresML**\n\nPostgresML [ [ github ](https://github.com/postgresml/postgresml) || [ website\n](https://postgresml.org/) || [ docs ](https://postgresml.org/docs) ] allows\nusers to take advantage of the fundamental relationship between data and\nmodels, by moving the models to your database rather than constantly moving\ndata to the models. This in-database approach to AI architecture results in\nmore scalable, reliable and efficient applications. On the PostgresML cloud,\nyou can perform vector operations, create embeddings, and generate real-time\noutputs in one process, directly where your data resides.\n\nKey highlights:\n\n  * Model Serving - GPU accelerated inference engine for interactive applications, with no additional networking latency or reliability costs \n  * Model Store - Access to open-source models including state of the art LLMs from Hugging Face, and track changes in performance between versions \n  * Model Training - Train models with your application data using more than 50 algorithms for regression, classification or clustering tasks; fine tune pre-trained models like Llama and BERT to improve performance \n  * Feature Store - Scalable access to model inputs, including vector, text, categorical, and numeric data: vector database, text search, knowledge graph and application data all in one low-latency system \n  * Python and JavaScript SDKs - SDK clients can perform advanced ML/AI tasks in a single SQL request without having to transfer additional data, models, hardware or dependencies to your application \n  * Serverless deployments - Enjoy instant autoscaling, so your applications can handle peak loads without overprovisioning \n\nPostgresML has a range of capabilities. In the following sections, we\u2019ll guide\nyou through just one use case \u2013 RAG \u2013 and how to use the PostgresML Managed\nIndex on LlamaIndex to build a better RAG app.\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4050, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fdbe389e-850d-41d4-9212-c783cdd76265": {"__data__": {"id_": "fdbe389e-850d-41d4-9212-c783cdd76265", "embedding": null, "metadata": {"filename": "simplify-your-rag-application-architecture-with-llamaindex-postgresml.md", "extension": ".md", "title": "Simplify your RAG application architecture with LlamaIndex + PostgresML", "date": "May 28, 2024", "url": "https://www.llamaindex.ai/blog/simplify-your-rag-application-architecture-with-llamaindex-postgresml"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "45edea59-d0cf-49c5-83a4-240649039ec3", "node_type": "4", "metadata": {"filename": "simplify-your-rag-application-architecture-with-llamaindex-postgresml.md", "extension": ".md", "title": "Simplify your RAG application architecture with LlamaIndex + PostgresML", "date": "May 28, 2024", "url": "https://www.llamaindex.ai/blog/simplify-your-rag-application-architecture-with-llamaindex-postgresml"}, "hash": "2224e0455fac6f885d92b9c08aecec8ceadac8a390451d094c71be1bddf71f20", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4547f9cd-fc3a-4db5-864a-a62a63dcc668", "node_type": "1", "metadata": {"filename": "simplify-your-rag-application-architecture-with-llamaindex-postgresml.md", "extension": ".md", "title": "Simplify your RAG application architecture with LlamaIndex + PostgresML", "date": "May 28, 2024", "url": "https://www.llamaindex.ai/blog/simplify-your-rag-application-architecture-with-llamaindex-postgresml"}, "hash": "65fff80b8eaa673b76c8422fda2f6ef52382d5b8aabbd71afd0ab32b6d9e66b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81ad8b41-396b-424c-8a2f-0f7a73df51a5", "node_type": "1", "metadata": {}, "hash": "b940b181f927d3e6f6c93932beb3aa6e1c7eb70e673a27038a1dc65a461dcc61", "class_name": "RelatedNodeInfo"}}, "text": "**How it works in LlamaIndex**\n\nLet\u2019s look at a simple question-answering example using the PostgresML Managed\nIndex. For this example, we will be using Paul Graham\u2019s essays.\n\n**Step 1: Get Your Database Connection String**\n\nIf you haven\u2019t already, [ create your PostgresML account\n](https://postgresml.org/signup) . You\u2019ll get $100 in free credits when you\ncomplete your profile.\n\nSet the PGML_DATABASE_URL environment variable:\n\n    \n    \n    export PGML_DATABASE_URL=\"{YOUR_CONNCECTION_STRING}\"\n\nAlternatively, you can pass the pgml_database_url argument when creating the\nindex.\n\n**Step 2: Create the PostgresML Managed Index**\n\nFirst install Llama_index and the PostgresML Managed Index component:\n\n    \n    \n    pip install llama_index llama-index-indices-managed-postgresml\n\nThen load in the data:\n\n    \n    \n    mkdir data\n    curl -o data/paul_graham_essay.txt https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n\nFinally create the PostgresML Managed Index:\n\n    \n    \n    from llama_index.core.readers import SimpleDirectoryReader\n    from llama_index.indices.managed.postgresml import PostgresMLIndex\n    \n    \n    documents = SimpleDirectoryReader(\"data\").load_data()\n    index = PostgresMLIndex.from_documents(\n        documents, collection_name=\"llama-index-example\"\n    )\n\nNote the collection_name is used to uniquely identify the index you are\nworking with.\n\nHere we are using the SimpleDirectoryReader to load in the documents and then\nwe construct the PostgresMLIndex from those documents.\n\n", "mimetype": "text/plain", "start_char_idx": 4050, "end_char_idx": 5627, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81ad8b41-396b-424c-8a2f-0f7a73df51a5": {"__data__": {"id_": "81ad8b41-396b-424c-8a2f-0f7a73df51a5", "embedding": null, "metadata": {"filename": "simplify-your-rag-application-architecture-with-llamaindex-postgresml.md", "extension": ".md", "title": "Simplify your RAG application architecture with LlamaIndex + PostgresML", "date": "May 28, 2024", "url": "https://www.llamaindex.ai/blog/simplify-your-rag-application-architecture-with-llamaindex-postgresml"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "45edea59-d0cf-49c5-83a4-240649039ec3", "node_type": "4", "metadata": {"filename": "simplify-your-rag-application-architecture-with-llamaindex-postgresml.md", "extension": ".md", "title": "Simplify your RAG application architecture with LlamaIndex + PostgresML", "date": "May 28, 2024", "url": "https://www.llamaindex.ai/blog/simplify-your-rag-application-architecture-with-llamaindex-postgresml"}, "hash": "2224e0455fac6f885d92b9c08aecec8ceadac8a390451d094c71be1bddf71f20", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fdbe389e-850d-41d4-9212-c783cdd76265", "node_type": "1", "metadata": {"filename": "simplify-your-rag-application-architecture-with-llamaindex-postgresml.md", "extension": ".md", "title": "Simplify your RAG application architecture with LlamaIndex + PostgresML", "date": "May 28, 2024", "url": "https://www.llamaindex.ai/blog/simplify-your-rag-application-architecture-with-llamaindex-postgresml"}, "hash": "03466ac6d95d22408a9f7ff24fe223fae63456d8d634d5378baf815ec0852ce2", "class_name": "RelatedNodeInfo"}}, "text": "This workflow does not require document preprocessing. Instead, the documents\nare sent directly to PostgresML where they are stored, split, and embedded per\nthe pipeline specification. For more information on pipelines see: [\nhttps://postgresml.org/docs/api/client-sdk/pipelines\n](https://postgresml.org/docs/api/client-sdk/pipelines) Custom Pipelines can\nbe passed into the PostgresML index at creation, but by default documents are\nsplit using the recursive_character splitter and embedded with\nintfloat/e5-small-v2 .\n\n**Step 3: Querying**\n\nNow that we have created our index we can use it for retrieval and querying:\n\n    \n    \n    retriever = index.as_retriever()\n    docs = retriever.retrieve(\"Was the author puzzled by the IBM 1401?\")\n    for doc in docs:\n        print(doc)\n\nPostgreML does embedding and retrieval in a single network call. Compare this\nquery against other common LlamaIndex embedding and vector storage\nconfigurations and you will notice a significant speed up.\n\nUsing the PostgresML Index as a query_engine is just as easy:\n\n    \n    \n    response = index.as_query_engine().query(\"Was the author puzzled by the IBM 1401?\")\n    print(response)\n\nOnce again, notice how fast the response was! The PostgresML Managed Index is\ndoing embedding, retrieval, and augmented generation in one network call. The\nspeed up becomes even more apparent when streaming:\n\n    \n    \n    query_engine = index.as_query_engine(streaming=True)\n    results = query_engine.query(\"Was the author puzzled by the IBM 1401?\")\n    for text in results.response_gen:\n        print(text, end=\"\", flush=True)\n\nNote that by default the query_engine uses meta-llama/Meta-Llama-3-8B-Instruct\nbut this is completely configurable.\n\n**Key takeaways**\n\nThe PostgresML Managed Index uniquely unifies embedding, vector search, and\ntext generation into a single network call. LlamaIndex users can expect\nfaster, more reliable, and easier-to-manage RAG workflows by using PostgresML\nas the backend.\n\nTo get started with PostgresML and LlamaIndex, you can follow the PostgresML\nintro [ guide ](https://postgresml.org/docs/introduction/getting-started/) to\nsetup your account, and the examples above with your own data.\n\n", "mimetype": "text/plain", "start_char_idx": 5627, "end_char_idx": 7825, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d3ea23a-df0e-451d-a2d8-0f426b3b3bc8": {"__data__": {"id_": "0d3ea23a-df0e-451d-a2d8-0f426b3b3bc8", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-05-28.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-28", "date": "May 28, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-28"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09ff30b1-4a55-41c9-8a24-0b49a7293cb1", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-05-28.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-28", "date": "May 28, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-28"}, "hash": "35d24265c9b05a316b57f92e6b5cb8d2d03f58ff100a2947612a344cf2ddfaf2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31722dad-a163-4722-baba-1bf8fe4db7f5", "node_type": "1", "metadata": {}, "hash": "390ecf4c7a4caf0b2225372134fc632b9143c40b950bdb5cbd75daafba50b140", "class_name": "RelatedNodeInfo"}}, "text": "Greetings, LlamaIndex Family!\n\nWelcome to your latest weekly update from LlamaIndex! We're excited to present\na variety of outstanding integration updates, detailed guides, demos,\neducational tutorials, and informative webinars this week.\n\n##  **The highlights:**\n\n  * **Secure Code Execution with AzureCodeInterpreterTool:** Securely run LLM-generated code with Azure Container Apps, integrated with LlamaIndex for safe code execution. \n  * **Build Automated Email Agents:** Create email agents with MultiOn and LlamaIndex that autonomously read, index, and respond to emails. \n  * **LlamaFS for Organized Files:** Alex Reibman's team developed LlamaFS to automatically structure messy file directories, enhanced by Llama 3 and Groq Inc.'s API. \n  * **RAGApp's No-Code Chatbots:** Deploy RAG chatbots easily with RAGApp's no-code interface, fully open-source and cloud-compatible. \n\n##  **Feature Releases and Enhancements:**\n\n  1. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 933, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "31722dad-a163-4722-baba-1bf8fe4db7f5": {"__data__": {"id_": "31722dad-a163-4722-baba-1bf8fe4db7f5", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-05-28.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-28", "date": "May 28, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-28"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09ff30b1-4a55-41c9-8a24-0b49a7293cb1", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-05-28.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-28", "date": "May 28, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-28"}, "hash": "35d24265c9b05a316b57f92e6b5cb8d2d03f58ff100a2947612a344cf2ddfaf2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d3ea23a-df0e-451d-a2d8-0f426b3b3bc8", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-05-28.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-28", "date": "May 28, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-28"}, "hash": "ef885341fcdaa6d2e91d15b9fc0f6cf6839259fdf85fa2e40b92cc8628f324c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9700408-1613-40df-ab35-c1ed67472ee7", "node_type": "1", "metadata": {}, "hash": "d9aa7ab28e900672758b60959ac856ab6816215bd28973539b312e3d4a0edbf4", "class_name": "RelatedNodeInfo"}}, "text": "We have launched Azure Container Apps dynamic sessions to securely run LLM-generated code in a sandbox. Integrated into LlamaIndex, this feature ensures safe execution of complex code tasks by your agents. Set up a session pool on Azure, add the AzureCodeInterpreterTool to your agent, and you\u2019re ready to go. [ Blogpost ](https://www.llamaindex.ai/blog/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions) , [ Tweet ](https://x.com/llama_index/status/1792958928357335115) . \n  2. ", "mimetype": "text/plain", "start_char_idx": 933, "end_char_idx": 1441, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9700408-1613-40df-ab35-c1ed67472ee7": {"__data__": {"id_": "b9700408-1613-40df-ab35-c1ed67472ee7", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-05-28.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-28", "date": "May 28, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-28"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09ff30b1-4a55-41c9-8a24-0b49a7293cb1", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-05-28.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-28", "date": "May 28, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-28"}, "hash": "35d24265c9b05a316b57f92e6b5cb8d2d03f58ff100a2947612a344cf2ddfaf2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31722dad-a163-4722-baba-1bf8fe4db7f5", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-05-28.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-28", "date": "May 28, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-28"}, "hash": "8be3696819ef5e80b2276d575721a697efd24bde914611d87128227f87cc0d8f", "class_name": "RelatedNodeInfo"}}, "text": "We have integrated with the open source Nomic embed, now fully operable locally. This integration allows for completely local embeddings and introduces a dynamic inference mode that optimizes embedding latency. The system automatically selects between local and remote embeddings based on speed, ensuring optimal performance. [ Docs ](https://docs.llamaindex.ai/en/stable/examples/embeddings/nomic/) , [ Tweet ](https://x.com/llama_index/status/1793677965978673598) . \n  3. We have integrated the Vespa vector store, supporting hybrid search with BM25. [ Docs ](https://docs.llamaindex.ai/en/stable/examples/vector_stores/VespaIndexDemo/) , [ Tweet ](https://x.com/llama_index/status/1794106979213869413) . \n  4. We have integrated with MyMagic AI to facilitate batch data processing for GenAI applications. This setup allows you to pre-process large datasets with an LLM, enabling advanced analysis and querying capabilities. [ Docs ](https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex) , [ Tweet ](https://x.com/llama_index/status/1793385512386150856) . \n\n##  **Guides:**\n\n  * [ Guide ](https://www.llamaindex.ai/blog/automate-online-tasks-with-multion-and-llamaindex) to building an automated Email Agent with MultiOn and LlamaIndex that can autonomously read and index emails for easy retrieval and draft responses using advanced browsing capabilities. \n  * [ Guide ](https://www.koyeb.com/tutorials/using-llamaindex-and-mongodb-to-build-a-job-search-assistant#create-a-job-indexing-api-endpoint) to building Full-Stack Job Search Assistant by Rishi Raj Jain using Gokoyeb, MongoDB, and LlamaIndex. This guide takes you through setting up MongoDB Atlas, crafting a Next.js application, developing UI components, and deploying your app on Koyeb, complete with real-time response streaming and continuous job updates. \n\n##  **Demos:**\n\n  * LlamaFS, a project developed by [ Alex Reibman ](https://x.com/AlexReibman) and his team, automatically organizes messy file directories into neatly structured folders with interpretable names. Enhanced by Llama 3 and supported by Groq Inc.'s API, Ollama's fully local mode and LlamaIndex, this tool significantly improves file management efficiency. [ Code ](https://github.com/iyaja/llama-fs) , [ Tweet ](https://x.com/llama_index/status/1794762651769430381) . \n  * RAGApp, a project developed by [ Marcus Schiesser ](https://x.com/MarcusSchiesser) , offers a no-code interface for configuring RAG chatbots as simply as GPTs by OpenAI. This fully open-source docker container can be deployed on any cloud platform, allowing users to set up the LLM, define system prompts, upload knowledge bases, and launch chatbots via UI or API. [ Code ](https://github.com/ragapp/ragapp) , [ Tweet ](https://x.com/llama_index/status/1794030544415818062) . \n\n##  **Tutorials:**\n\n  * [ Phil Chirchir\u2019s ](https://x.com/ronoh4) [ tutorial ](https://medium.com/@leighphil4/dspy-rag-with-llamaindex-programming-llms-over-prompting-1b12d12cbc43) on DSPy RAG with LlamaIndex. It demonstrates how to integrate DSPy bootstrapping models with a LlamaIndex RAG pipeline powered by LlamaParse. \n  * [ Pavan Kumar\u2019s ](https://x.com/pavan_mantha1) [ tutorial ](https://towardsdev.com/harnessing-gpt-4os-vision-for-advanced-search-building-image-embeddings-with-qdrant-5dd887cf40b5) on advanced image indexing for RAG demonstrates how to combine image embeddings with structured annotations using multimodal models. It details how to enhance image search with LlamaIndex and Qdrant Engine\u2019s capabilities. \n  * Jayita Bhattacharyya\u2019s [ tutorial ](https://itsjb13.medium.com/building-a-rag-chatbot-using-llamaindex-groq-with-llama3-chainlit-b1709f770f55) on Building a RAG Chatbot using Llamaindex, Groq with Llama3 & Chainlit. \n\n##  **Webinar:**\n\n  * [ Webinar ](https://www.youtube.com/watch?v=_1JZfv7r4mY) with OpenDevin team to learn how to build an Open-Source Coding Assistant using OpenDevin. \n\n", "mimetype": "text/plain", "start_char_idx": 1441, "end_char_idx": 5382, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1912b081-3cca-42be-9db5-0230d93f0f47": {"__data__": {"id_": "1912b081-3cca-42be-9db5-0230d93f0f47", "embedding": null, "metadata": {"filename": "automate-online-tasks-with-multion-and-llamaindex.md", "extension": ".md", "title": "Automate online tasks with MultiOn and LlamaIndex", "date": "May 23, 2024", "url": "https://www.llamaindex.ai/blog/automate-online-tasks-with-multion-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ee6537a5-76b4-467c-bdde-63d6310ac37c", "node_type": "4", "metadata": {"filename": "automate-online-tasks-with-multion-and-llamaindex.md", "extension": ".md", "title": "Automate online tasks with MultiOn and LlamaIndex", "date": "May 23, 2024", "url": "https://www.llamaindex.ai/blog/automate-online-tasks-with-multion-and-llamaindex"}, "hash": "71fc333b547ff340b7e6a778dba56912a60c8ea373f3daea35ffd6b7af822d9a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2577cf2-3cc3-404a-923b-d6bb5d2be0f8", "node_type": "1", "metadata": {}, "hash": "801947657a21bc557bdb63bf1a1cf7111335315c23782575ee45e955f21c7bbc", "class_name": "RelatedNodeInfo"}}, "text": "##  **Introduction**\n\nMultiOn is an AI agents platform designed to facilitate the autonomous\ncompletion of tasks in any web environment. It empowers developers to build AI\nagents that can manage online activities from start to finish, handling\neverything from simple data retrieval to complex interactions.\n\nLlamaIndex complements this by providing an orchestration framework that\nbridges the gap between private and public data essential for building\napplications with Large Language Models. It facilitates data ingestion,\nindexing, and querying, making it indispensable for developers looking to\nleverage generative AI.\n\nIn this article, we'll demonstrate how MultiOn's capabilities can be\nseamlessly integrated within the LlamaIndex framework, showcasing a practical\napplication that leverages both technologies to automate and streamline web\ninteractions.\n\n##  **Technical walkthrough: Integrating MultiOn with LlamaIndex**\n\nLet\u2019s explore a practical example where MultiOn and LlamaIndex work in tandem\nto manage email interactions and web browsing.\n\n**Step 1: Setting Up the Environment** We begin by setting up our AI agent\nwith the necessary configurations and API keys:\n\n    \n    \n    import openai\n    from llama_index.agent.openai import OpenAIAgent\n    openai.api_key = \"sk-your-key\"\n    \n    from llama_index.tools.multion import MultionToolSpec\n    multion_tool = MultionToolSpec(api_key=\"your-multion-key\")\n\n**Step 2: Integrating Gmail Search Tool** Next, we integrate a Gmail search\ntool to help our agent fetch and analyze emails, providing the necessary\ncontext for further actions:\n\n    \n    \n    from llama_index.tools.google import GmailToolSpec\n    from llama_index.core.tools.ondemand_loader_tool import OnDemandLoaderTool\n    \n    gmail_tool = GmailToolSpec()\n    gmail_loader_tool = OnDemandLoaderTool.from_tool(\n        gmail_tool.to_tool_list()[1],\n        name=\"gmail_search\",\n        description=\"\"\"\n             This tool allows you to search the users gmail inbox and give directions for how to summarize or process the emails\n    \n            You must always provide a query to filter the emails, as well as a query_str to process the retrieved emails.\n            All parameters are required\n            \n            If you need to reply to an email, ask this tool to build the reply directly\n            Examples:\n                query='from:adam subject:dinner', max_results=5, query_str='Where are adams favourite places to eat'\n                query='dentist appointment', max_results=1, query_str='When is the next dentist appointment'\n                query='to:jerry', max_results=1, query_str='summarize and then create a response email to jerrys latest email'\n                query='is:inbox', max_results=5, query_str='Summarize these emails'\n        \"\"\"\n    )\n\n**Step 3: Initialize agent**\n\nInitialise the agent with tools and a system prompt\n\n    \n    \n    agent = OpenAIAgent.from_tools(\n        [*multion_tool.to_tool_list(), gmail_loader_tool],\n        system_prompt=\"\"\"\n    \t    You are an AI agent that assists the user in crafting email responses based on previous conversations.\n    \t    \n    \t    The gmail_search tool connects directly to an API to search and retrieve emails, and answer questions based on the content.\n    \t    The browse tool allows you to control a web browser with natural language to complete arbitrary actions on the web.\n    \t    \n    \t    Use these two tools together to gain context on past emails and respond to conversations for the user.\n        \"\"\"\n    )\n\n**Step 4: Agent Execution Flow** With our tools integrated, the agent is now\nequipped to perform a series of tasks:\n\n**1\\. Search and Summarize Emails** : The agent uses LlamaIndex's Gmail tool\nto fetch relevant emails and summarize the content, providing a basis for\ndrafting a response.\n\n    \n    \n    print(agent.chat(\"browse to the latest email from Julian and open the email\"))\n    \n    \n    Added user message to memory: browse to the latest email from Julian and open the email\n    === Calling Function ===\n    Calling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Browse to the latest email from Julian and open the email\"}\n    Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=1054044249014.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.compose+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.readonly&state=JSdsfdsi990sddsd&access_type=offline\n    Got output: Open the email from Julian to view the latest communication.\n    ========================\n     \n    I have opened the latest email from Julian for you to view. If you need any specific information or action to be taken, please let me know.\n\n**2\\. Generate Response** : Based on the summarized information, the agent\ncrafts an appropriate response to the email chain.\n\n    \n    \n    print(agent.chat(\n    \t\"Summarize the email chain with julian and create a response to the last email that confirms all the details\"\n    ))\n    \n    \n    Added user message to memory: Summarize the email chain with julian and create a response to the last email that confirms all the details\n    === Calling Function ===\n    Calling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Summarize the email chain with Julian and create a response to the last email confirming all the details\"}\n    Got output: The email chain with Julian involved a change in an event scheduled for Friday, August 6, 2021, from 15:30 to 16:00 United Kingdom Time on Google Meet. The instructions for joining were provided in the description. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5742, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2577cf2-3cc3-404a-923b-d6bb5d2be0f8": {"__data__": {"id_": "a2577cf2-3cc3-404a-923b-d6bb5d2be0f8", "embedding": null, "metadata": {"filename": "automate-online-tasks-with-multion-and-llamaindex.md", "extension": ".md", "title": "Automate online tasks with MultiOn and LlamaIndex", "date": "May 23, 2024", "url": "https://www.llamaindex.ai/blog/automate-online-tasks-with-multion-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ee6537a5-76b4-467c-bdde-63d6310ac37c", "node_type": "4", "metadata": {"filename": "automate-online-tasks-with-multion-and-llamaindex.md", "extension": ".md", "title": "Automate online tasks with MultiOn and LlamaIndex", "date": "May 23, 2024", "url": "https://www.llamaindex.ai/blog/automate-online-tasks-with-multion-and-llamaindex"}, "hash": "71fc333b547ff340b7e6a778dba56912a60c8ea373f3daea35ffd6b7af822d9a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1912b081-3cca-42be-9db5-0230d93f0f47", "node_type": "1", "metadata": {"filename": "automate-online-tasks-with-multion-and-llamaindex.md", "extension": ".md", "title": "Automate online tasks with MultiOn and LlamaIndex", "date": "May 23, 2024", "url": "https://www.llamaindex.ai/blog/automate-online-tasks-with-multion-and-llamaindex"}, "hash": "feeee1c8733329b8d7271432cec117c720c49bdb93414f5bebc1318985f07b9e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0db569a2-bdf7-41f6-b7d1-0888824195d0", "node_type": "1", "metadata": {}, "hash": "1a11fdecf9d2cf491f80506fa2021f23e1b3927b83b89e4ddd802266f5bbfba8", "class_name": "RelatedNodeInfo"}}, "text": "The email also included contact information for joining the meeting. Julian and Nassar were listed as attendees, with Julian being the organizer. The email was authenticated and passed SPF and DKIM checks.\n    \n    In response to the last email, I would confirm all the details of the event change, reiterating the date, time, platform (Google Meet), and any specific instructions provided. I would express gratitude for the update and confirm attendance at the revised event timing.\n    ========================\n    \n    Based on the email chain with Julian, here is a summary:\n    - The event scheduled for Friday, August 6, 2021, has been changed from 15:30 to 16:00 United Kingdom Time on Google Meet.\n    - Instructions for joining the meeting were provided in the email.\n    - Attendees included Julian and Nassar, with Julian as the organizer.\n    - The email passed SPF and DKIM checks.\n    \n    To respond and confirm all the details, you can mention the revised event date and time, the platform (Google Meet), and express gratitude for the update. Confirm your attendance at the new timing. Let me know if you would like me to draft the response email for you.\n\n", "mimetype": "text/plain", "start_char_idx": 5742, "end_char_idx": 6915, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0db569a2-bdf7-41f6-b7d1-0888824195d0": {"__data__": {"id_": "0db569a2-bdf7-41f6-b7d1-0888824195d0", "embedding": null, "metadata": {"filename": "automate-online-tasks-with-multion-and-llamaindex.md", "extension": ".md", "title": "Automate online tasks with MultiOn and LlamaIndex", "date": "May 23, 2024", "url": "https://www.llamaindex.ai/blog/automate-online-tasks-with-multion-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ee6537a5-76b4-467c-bdde-63d6310ac37c", "node_type": "4", "metadata": {"filename": "automate-online-tasks-with-multion-and-llamaindex.md", "extension": ".md", "title": "Automate online tasks with MultiOn and LlamaIndex", "date": "May 23, 2024", "url": "https://www.llamaindex.ai/blog/automate-online-tasks-with-multion-and-llamaindex"}, "hash": "71fc333b547ff340b7e6a778dba56912a60c8ea373f3daea35ffd6b7af822d9a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2577cf2-3cc3-404a-923b-d6bb5d2be0f8", "node_type": "1", "metadata": {"filename": "automate-online-tasks-with-multion-and-llamaindex.md", "extension": ".md", "title": "Automate online tasks with MultiOn and LlamaIndex", "date": "May 23, 2024", "url": "https://www.llamaindex.ai/blog/automate-online-tasks-with-multion-and-llamaindex"}, "hash": "a2a929247328e7aa59021badb76b4950e2bb1afa4580b9c9f8ab6460a6383164", "class_name": "RelatedNodeInfo"}}, "text": "**3\\. Send Email through MultiOn** : Finally, the generated response is passed\nto the MultiOn agent, which manages the action of sending the email through\nthe web browser.\n\n    \n    \n    print(agent.chat(\n    \t\"pass the entire generated email to the browser and have it send the email as a reply to the chain\"\n    ))\n    \n    \n    Added user message to memory: pass the entire generated email to the browser and have it send the email as a reply to the chain\n    === Calling Function ===\n    Calling function: browse with args: {\"cmd\": \"Compose a reply email to Julian confirming the event change to Fri 6 Aug 2021 from 15:30 to 16:00 UK Time on Google Meet. Express readiness to attend and thank Julian for the details.\"}\n    Got output: Email response sent to Julian\n    ========================\n\n##  Next Steps\n\nMultiOn is an officially supported tool on LlamaHub, the central page for all\nLlamaIndex integrations (from tools to LLMs to vector stores). Check out the [\nLlamaHub page ](https://llamahub.ai/l/tools/llama-index-tools-multion?from=)\nhere.\n\nIf you\u2019re interested in running through this tutorial on building a browser +\nGmail-powered agent yourself, check out our [ notebook\n](https://github.com/run-llama/llama_index/blob/main/llama-index-\nintegrations/tools/llama-index-tools-multion/examples/multion.ipynb) .\n\nThe integration of MultiOn and LlamaIndex offers a powerful toolkit for\ndevelopers aiming to automate and streamline online tasks. As these\ntechnologies evolve, they will continue to unlock new potentials in AI\napplication, significantly impacting how developers interact with digital\nenvironments and manage data.\n\n", "mimetype": "text/plain", "start_char_idx": 6915, "end_char_idx": 8558, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "61a60dcd-a8f6-4800-81c1-5b399378ece9": {"__data__": {"id_": "61a60dcd-a8f6-4800-81c1-5b399378ece9", "embedding": null, "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb446d01-a356-4334-995f-9c48f9fd774a", "node_type": "4", "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "9fa6c05cb14bac9d0e24f5480a646a38f7ad05cdc4f0df72b5ae1195bf9f11a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9df290b-a6e4-4837-97c6-e002dc05b0fa", "node_type": "1", "metadata": {}, "hash": "ef46ae366c8c2282def63378d475442f554251b144914252674720dbc9c601b7", "class_name": "RelatedNodeInfo"}}, "text": "_This is a guest post from MyMagic AI._\n\n[ MyMagic AI ](https://mymagic.ai/) allows processing and analyzing large\ndatasets with AI. MyMagic AI offers a powerful API for _batch_ inference (also\nknown as _offline_ or _delayed_ inference) that brings various open-source\nLarge Language Models (LLMs) such as Llama 70B, Mistral 7B, Mixtral 8x7B,\nCodeLlama70b, and advanced Embedding models to its users. Our framework is\ndesigned to perform data extraction, summarization, categorization, sentiment\nanalysis, training data generation, and embedding, to name a few. And now it's\nintegrated directly into LlamaIndex!\n\n##  Part 1: batch inference\n\n###  How It Works:\n\n**1\\. Setup** :\n\n  1. Organize Your Data in an AWS S3 or GCS Bucket: \n    1. Create a folder using your user ID assigned to you upon registration. \n    2. Inside that folder, create another folder (called a \"session\") to store all the files you need for your tasks. \n  2. Purpose of the 'Session' Folder: \n    1. This \"Session\" folder keeps your files separate from others, making sure that your tasks run on the right set of files. You can name your session subfolder anything you like. \n  3. Granting Access to MyMagic AI: \n    1. To allow MyMagic AI to securely access your files in the cloud, follow the setup instructions provided in the [ MyMagic AI documentation ](https://docs.mymagic.ai/) . \n\n**2\\. Install** : Install both MyMagic AI\u2019s API integration and LlamaIndex\nlibrary:\n\n    \n    \n    pip install llama-index\n    pip install llama-index-llms-mymagic\n\n**3\\. API Request:** The llamaIndex library is a wrapper around MyMagic AI\u2019s\nAPI. What it does under the hood is simple: it sends a POST request to the\nMyMagic AI API while specifying the model, storage provider, bucket name,\nsession name, and other necessary details.\n\n    \n    \n    import asyncio\n    from llama_index.llms.mymagic import MyMagicAI\n    \n    llm = MyMagicAI(\n        api_key=\"user_...\", # provided by MyMagic AI upon sign-up\n        storage_provider=\"s3\",\n        bucket_name=\"batch-bucket\", # you may name anything\n        session=\"my-session\",\n        role_arn=\"arn:aws:iam::<your account id>:role/mymagic-role\",\n        system_prompt=\"You are an AI assistant that helps to summarize the documents without essential loss of information\", # default prompt at https://docs.mymagic.ai/api-reference/endpoint/create\n        region=\"eu-west-2\",\n    )\n\nWe have designed the integration to allow the user to set up the bucket and\ndata together with the system prompt when instantiating the llm object. Other\ninputs, e.g. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2562, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f9df290b-a6e4-4837-97c6-e002dc05b0fa": {"__data__": {"id_": "f9df290b-a6e4-4837-97c6-e002dc05b0fa", "embedding": null, "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb446d01-a356-4334-995f-9c48f9fd774a", "node_type": "4", "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "9fa6c05cb14bac9d0e24f5480a646a38f7ad05cdc4f0df72b5ae1195bf9f11a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61a60dcd-a8f6-4800-81c1-5b399378ece9", "node_type": "1", "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "83eee6c696a5e6f7fb334652558706110feacd789fb1019c710d2d0d9cef1575", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32ea91e0-0f1e-412b-9dec-f279408e985e", "node_type": "1", "metadata": {}, "hash": "8157b502d6e77360eb8efcd0deb0fea92985d57a1705b54fd198e3b0ee5d2745", "class_name": "RelatedNodeInfo"}}, "text": "question (i.e. your prompt), model and max_tokens are dynamic\nrequirements when submitting complete and acomplete requests.\n\n    \n    \n    resp = llm.complete(\n        question=\"Summarise this in one sentence.\",\n        model=\"mixtral8x7\", \n        max_tokens=20,  # default is 10\n    )\n    print(resp)\n    async def main():\n        aresp = await llm.acomplete(\n            question=\"Summarize this in one sentence.\",\n            model=\"llama7b\",\n            max_tokens=20,\n        )\n        print(aresp)\n    \n    asyncio.run(main())\n\nThis dynamic entry allows developers to experiment with different prompts and\nmodels in their workflow while also controlling for model output to cap their\nspending limit. MyMagic AI\u2019s backend supports both synchronous requests\n(complete) and asynchronous requests (acomplete). It is advisable, however, to\nuse our async endpoints as much as possible as batch jobs are inherently\nasynchronous with potentially long processing times (depending on the size of\nyour data).\n\nCurrently, we do not support chat or achat methods as our API is not designed\nfor real-time interactive experience. However, we are planning to add those\nmethods in the future that will function in a \u201cbatch way\u201d. The user queries\nwill be aggregated and appended as one prompt (to give the chat context) and\nsent to all files at once.\n\n###  Use Cases\n\nWhile there are myriads of use cases, here we provide a few to help motivate\nour users. Feel free to embed our API in your workflows that are good fit for\nbatch processing.\n\n####  1\\. Extraction\n\nImagine needing to extract specific information from millions of files stored\nin a bucket. Information from all files will be extracted with one API call\ninstead of a million sequential ones.\n\n####  2\\. Classification\n\nFor businesses looking to classify customer reviews such as positive, neutral,\nand negative. With one request you can start processing the requests over the\nweekend and get them ready by Monday morning.\n\n", "mimetype": "text/plain", "start_char_idx": 2562, "end_char_idx": 4538, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "32ea91e0-0f1e-412b-9dec-f279408e985e": {"__data__": {"id_": "32ea91e0-0f1e-412b-9dec-f279408e985e", "embedding": null, "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb446d01-a356-4334-995f-9c48f9fd774a", "node_type": "4", "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "9fa6c05cb14bac9d0e24f5480a646a38f7ad05cdc4f0df72b5ae1195bf9f11a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9df290b-a6e4-4837-97c6-e002dc05b0fa", "node_type": "1", "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "a2f78b8cb8c07293b148df3a25391c095db9dca50a4b7d4332712ad07ea6abfa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb3df1f1-5478-4f10-945b-2e064039911b", "node_type": "1", "metadata": {}, "hash": "92e83e043e6d8a9b0d8ec5afd44c84ef698f9b36f7e68cc2d128bccc227c5523", "class_name": "RelatedNodeInfo"}}, "text": "####  3\\. Embedding\n\nEmbedding text files for further machine learning applications is another\npowerful use case of MyMagic AI's API. You will be ready for your vector db in\na matter of days not weeks.\n\n####  4\\. Training (Fine-tuning) Data Generation\n\nImagine generating thousands of synthetic data for your fine-tuning tasks.\nWith MyMagic AI\u2019s API, you can reduce the generation time by a factor of 5-10x\ncompared to GPT-3.5.\n\n####  5\\. Transcription\n\nMyMagic AI\u2019s API supports different types of files, so it is also easy to\nbatch transcribe many mp3 or mp4 files in your bucket.\n\n##  Part 2: Integration with LlamaIndex\u2019s RAG Pipeline\n\nThe output from batch inference processes, often voluminous, can seamlessly\nintegrate into LlamaIndex's RAG pipeline for effective data storage and\nretrieval.\n\nThis section demonstrates how to use the Llama3 model from the Ollama library\ncoupled with BGE embedding to manage information storage and execute queries.\nPlease ensure the following prerequisites are installed and Llama3 model is\npulled:\n\n    \n    \n    pip install llama-index-embeddings-huggingface\n    curl -fsSL https://ollama.com/install.sh | sh\n    ollama pull llama3\n\nFor this demo, we have run a batch summarization job on 5 Amazon reviews (but\nthis might be millions in some real scenarios) and saved the results as\nreviews_1_5.json:\n\n    \n    \n    {\n      \"id_review1\": {\n        \"query\": \"Summarize the document!\",\n        \"output\": \"The document describes a family with a young boy who believes there is a zombie in his closet, while his parents are constantly fighting. ", "mimetype": "text/plain", "start_char_idx": 4538, "end_char_idx": 6122, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb3df1f1-5478-4f10-945b-2e064039911b": {"__data__": {"id_": "eb3df1f1-5478-4f10-945b-2e064039911b", "embedding": null, "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb446d01-a356-4334-995f-9c48f9fd774a", "node_type": "4", "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "9fa6c05cb14bac9d0e24f5480a646a38f7ad05cdc4f0df72b5ae1195bf9f11a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32ea91e0-0f1e-412b-9dec-f279408e985e", "node_type": "1", "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "35cab26d90bd5f70bd4544435aa056dde97779bac5cc7947fb9d1758be6859ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c59f89b2-7d31-428a-9bfd-d0d421809981", "node_type": "1", "metadata": {}, "hash": "2d2da2a922fc32cf6215ee11a7bf9d8beebd1da1fa8c03cfc989594c51be0b3c", "class_name": "RelatedNodeInfo"}}, "text": "The movie is criticized for its inconsistent genre, described as a slow-paced drama with occasional thriller elements. The review praises the well-playing parents and the decent dialogs but criticizes the lack of a boogeyman-like horror element. The overall rating is 3 out of 10.\"\n      },\n      \"id_review2\": {\n        \"query\": \"Summarize the document!\",\n        \"output\": \"The document is a positive review of a light-hearted Woody Allen comedy. The reviewer praises the witty dialogue, likable characters, and Woody Allen's control over his signature style. The film is noted for making the reviewer laugh more than any recent Woody Allen comedy and praises Scarlett Johansson's performance. It concludes by calling the film a great comedy to watch with friends.\"\n      },\n      \"id_review3\": {\n        \"query\": \"Summarize the document!\",\n        \"output\": \"The document describes a well-made film about one of the great masters of comedy, filmed in an old-time BBC fashion that adds realism. The actors, including Michael Sheen, are well-chosen and convincing. The production is masterful, showcasing realistic details like the fantasy of the guard and the meticulously crafted sets of Orton and Halliwell's flat. Overall, it is a terrific and well-written piece.\"\n      ", "mimetype": "text/plain", "start_char_idx": 6122, "end_char_idx": 7398, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c59f89b2-7d31-428a-9bfd-d0d421809981": {"__data__": {"id_": "c59f89b2-7d31-428a-9bfd-d0d421809981", "embedding": null, "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb446d01-a356-4334-995f-9c48f9fd774a", "node_type": "4", "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "9fa6c05cb14bac9d0e24f5480a646a38f7ad05cdc4f0df72b5ae1195bf9f11a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb3df1f1-5478-4f10-945b-2e064039911b", "node_type": "1", "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "4233322405de00d8a261eaaeeabfbf621ae3ce9269fcd7b1c8965fbc6381c118", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd379a1d-3770-4ef7-a1fe-dba9fdd1f28c", "node_type": "1", "metadata": {}, "hash": "69ecbe2120f8261017790d472a70fd4c4b2f778a5c3e7770956126fd93d8bdae", "class_name": "RelatedNodeInfo"}}, "text": "},\n      \"id_review4\": {\n        \"query\": \"Summarize the document!\",\n        \"output\": \"Petter Mattei's 'Love in the Time of Money' is a visually appealing film set in New York, exploring human relations in the context of money, power, and success. The characters, played by a talented cast including Steve Buscemi and Rosario Dawson, are connected in various ways but often unaware of their shared links. The film showcases the different stages of loneliness experienced by individuals in a big city. Mattei successfully portrays the world of these characters, creating a luxurious and sophisticated look. The film is a modern adaptation of Arthur Schnitzler's play on the same theme. Mattei's work is appreciated, and viewers look forward to his future projects.\"\n      },\n      \"id_review5\": {\n        \"query\": \"Summarize the document!\",\n        \"output\": \"The document describes the TV show 'Oz', set in the Oswald Maximum Security State Penitentiary. Known for its brutality, violence, and lack of privacy, it features an experimental section of the prison called Em City, where all the cells have glass fronts and face inwards. The show goes where others wouldn't dare, featuring graphic violence, injustice, and the harsh realities of prison life. The viewer may become comfortable with uncomfortable viewing if they can embrace their darker side.\"\n      },\n      \"token_count\": 3391\n    }\n    \n\nNow let\u2019s embed and store this document and ask questions using LlamaIndex\u2019s\nquery engine. Bring in our dependencies:\n\n    \n    \n    import os\n    \n    from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n    from llama_index.core.indices.vector_store import VectorStoreIndex\n    from llama_index.core.settings import Settings\n    from llama_index.core.readers import SimpleDirectoryReader\n    from llama_index.llms.ollama import Ollama\n\nConfigure the embedding model and Llama3 model\n\n    \n    \n    embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n    llm = Ollama(model=\"llama3\", request_timeout=300.0)\n\nUpdate settings for the indexing pipeline:\n\n    \n    \n    Settings.llm = llm\n    Settings.embed_model = embed_model\n    Settings.chunk_size = 512 # This parameter defines the size of text chunks for embedding\n    \n    documents = SimpleDirectoryReader(\"reviews_1_5.json\").load_data() #Modify path for your case\n\nNow create our index, our query engine and run a query:\n\n    \n    \n    index = VectorStoreIndex.from_documents(documents, show_progress=True)\n    \n    query_engine = index.as_query_engine(similarity_top_k=3)\n    \n    response = query_engine.query(\"What is the least favourite movie?\")\n    print(response)\n\nOutput:\n\n    \n    \n    Based on query results, the least favourite movie is: review 1 with a rating of 3 out of 10.\n\n", "mimetype": "text/plain", "start_char_idx": 7398, "end_char_idx": 10179, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd379a1d-3770-4ef7-a1fe-dba9fdd1f28c": {"__data__": {"id_": "fd379a1d-3770-4ef7-a1fe-dba9fdd1f28c", "embedding": null, "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb446d01-a356-4334-995f-9c48f9fd774a", "node_type": "4", "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "9fa6c05cb14bac9d0e24f5480a646a38f7ad05cdc4f0df72b5ae1195bf9f11a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c59f89b2-7d31-428a-9bfd-d0d421809981", "node_type": "1", "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}, "hash": "5b5186eb7d270d36bb02db6f7d29c84a817fd604ecf3d362052e9888ec2675b2", "class_name": "RelatedNodeInfo"}}, "text": "Now we know that the review 1 is the least favorite movie among these reviews.\n\n##  Next Steps\n\nThis shows how batch inference combined with real-time inference can be a\npowerful tool for analyzing, storing and retrieving information from massive\namounts of data. [ Get started with MyMagic AI\u2019s API ](https://vivacious-\nriver-18.authkit.app/sign-up?redirect_uri=https://api.mymagic.ai/workspace)\ntoday!\n\n", "mimetype": "text/plain", "start_char_idx": 10179, "end_char_idx": 10584, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "233c9a73-4a6b-4487-addb-f802e9facc2c": {"__data__": {"id_": "233c9a73-4a6b-4487-addb-f802e9facc2c", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-05-21.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-21", "date": "May 21, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "50909071-1542-4e51-b237-0fc7ae4c7343", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-05-21.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-21", "date": "May 21, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-21"}, "hash": "41bb36f03ff49fe800a3c9792e37310dba5884b1b88eb887f083cb3d14bb4efb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00b6f41d-74ae-4d2e-90ef-397129c43695", "node_type": "1", "metadata": {}, "hash": "38a480a284be022050919e4b585193508da2958a1b709f0cf039ae1ec80dc800", "class_name": "RelatedNodeInfo"}}, "text": "Hello LlamaIndex Community!\n\nWelcome to another exciting weekly update from LlamaIndex! Last week was a\nstandout in the AI world with significant updates on GPT-4o and Gemini models.\nWe're thrilled to bring you a host of exceptional integration updates, guides,\ntutorials, webinars, and events.\n\n##  **The highlights:**\n\n  * **LlamaIndex on Vertex AI (Google Cloud):** Introducing the new RAG API powered by advanced LlamaIndex modules on Vertex AI (Google Cloud). [ Docs ](https://cloud.google.com/vertex-ai/generative-ai/docs/llamaindex-on-vertexai) , [ Tweet ](https://x.com/llama_index/status/1790768330099580940) . \n  * **Enhanced Document Parsing with GPT-4o:** Integrated GPT-4o in LlamaParse for superior document parsing. [ Notebook ](https://github.com/run-llama/llama_parse/blob/main/examples/test_tesla_impact_report/test_gpt4o.ipynb) , [ Tweet ](https://x.com/llama_index/status/1791145604955152767) . \n  * **Cookbook on Structured Image Extraction with GPT-4o:** Check out our new cookbook on using GPT-4o for Structured Image Extraction. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/multi_modal/gpt4o_mm_structured_outputs.ipynb) , [ Tweet ](https://x.com/llama_index/status/1791258285993230786) . \n\n##  **Feature Releases and Enhancements:**\n\n  1. We have launched LlamaIndex on Vertex AI (Google Cloud) to introduce the new RAG API, enhanced by LlamaIndex's advanced modules. This integration simplifies setup and enhances user access for the developers with the flexibility to connect a variety of data sources and file types. It fully supports the latest LLMs, including Gemini 1.5 Flash, Gemini 1.5 Pro, and Gemini 1.0 models. [ Docs ](https://cloud.google.com/vertex-ai/generative-ai/docs/llamaindex-on-vertexai) , [ Tweet ](https://x.com/llama_index/status/1790768330099580940) . \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1843, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00b6f41d-74ae-4d2e-90ef-397129c43695": {"__data__": {"id_": "00b6f41d-74ae-4d2e-90ef-397129c43695", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-05-21.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-21", "date": "May 21, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "50909071-1542-4e51-b237-0fc7ae4c7343", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-05-21.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-21", "date": "May 21, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-21"}, "hash": "41bb36f03ff49fe800a3c9792e37310dba5884b1b88eb887f083cb3d14bb4efb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "233c9a73-4a6b-4487-addb-f802e9facc2c", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-05-21.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-21", "date": "May 21, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-21"}, "hash": "09a896ab6fa17e9919c5f9302becb873788035a0267c1953490417fa7addd588", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "849ee03e-dc51-47b6-9342-23475e2aebce", "node_type": "1", "metadata": {}, "hash": "cf0a6d46dc02e45ec89a26d1cc11b15e0e76481e6896e9d7c29c49d66e5cc08a", "class_name": "RelatedNodeInfo"}}, "text": "2. We have introduced GPT-4o with LlamaParse, offering enhanced document parsing into markdown for complex files, ensuring higher data quality for your RAG pipeline. Note the increased cost of $0.60 USD per page. Note the increased cost of $0.60 USD per page compared to the standard $0.003 per page. [ Notebook ](https://github.com/run-llama/llama_parse/blob/main/examples/test_tesla_impact_report/test_gpt4o.ipynb) , [ Tweet ](https://x.com/llama_index/status/1791145604955152767) . \n  ", "mimetype": "text/plain", "start_char_idx": 1843, "end_char_idx": 2331, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "849ee03e-dc51-47b6-9342-23475e2aebce": {"__data__": {"id_": "849ee03e-dc51-47b6-9342-23475e2aebce", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-05-21.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-21", "date": "May 21, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "50909071-1542-4e51-b237-0fc7ae4c7343", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-05-21.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-21", "date": "May 21, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-21"}, "hash": "41bb36f03ff49fe800a3c9792e37310dba5884b1b88eb887f083cb3d14bb4efb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00b6f41d-74ae-4d2e-90ef-397129c43695", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-05-21.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-21", "date": "May 21, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-21"}, "hash": "87cb64cbd0d7c8af2a2fed20cb851176598b8018d093b7c4f21eeee8dff50c21", "class_name": "RelatedNodeInfo"}}, "text": "3. We have released a cookbook on using GPT-4o for Structured Image Extraction, showing how to convert images into structured JSONs with a 0% failure rate and higher quality than GPT-4V. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/multi_modal/gpt4o_mm_structured_outputs.ipynb) , [ Tweet ](https://x.com/llama_index/status/1791258285993230786) . \n  4. LlamaParse integration with Quivr to enhance document parsing capabilities. Now, you can easily process complex documents like PDFs, PPTX, and Markdown files, ensuring clean data storage and accurate retrieval in your personalized AI assistants. [ Docs ](https://docs.quivr.app/configuring/llamaparse) , [ Tweet ](https://x.com/llama_index/status/1790880249049485313) . \n\n##  **Guides:**\n\n  * [ Guide ](https://github.com/run-llama/llama_parse/blob/main/examples/caltrain/caltrain_text_mode.ipynb) to Enhanced QA with LlamaParse on complex tables like train schedules. This approach uses spatial text layout and GPT-4o to preserve essential information, ensuring accurate and error-free data interpretation. \n  * [ Guide ](https://jina.ai/news/binary-embeddings-all-the-ai-3125-of-the-fat/) to Speeding Up Vector Search with Minimal Accuracy Loss using Jina Embeddings to achieve 32x faster vector search performance at just a 4% cost in accuracy. It involves encoding your data as binary digits, significantly reducing storage and compute requirements. \n\n##  **Tutorials:**\n\n  * Kate Silverstein [ tutorial ](https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant) on building local research assistant**,** set up a local, private research assistant on your laptop effortlessly with Mozilla's llamafile. Tutorial covers everything from downloading and activating the model, to connecting via LlamaIndex and managing your data. \n  * Plaban Nayak [ tutorial ](https://medium.com/the-ai-forum/multi-document-agentic-rag-using-llama-index-and-mistral-b334fa45d3ee) on Multi-document Agentic RAG using Llama-Index and Mistral. \n  * Diptiman Raichaudhuri [ tutorial ](https://diptimanrc.medium.com/text2sql-opensource-duckdb-nsql-7b-with-ollama-and-llamaindex-on-local-setup-6f266f78bc4f) on fully local Text-to-SQL using DuckDB as the database, Ollama + Mixtral-8x7B as the model, and LlamaIndex for text-to-SQL orchestration. \n  * Mandar Karhade [ tutorial ](https://pub.towardsai.net/how-to-optimize-chunk-sizes-for-rag-in-production-fae9019796b6) on showing an end-to-end experimentation pipeline for iterating on chunk sizes, generating a synthetic dataset, and measuring how it affects evaluation metrics. \n\n##  **Webinar:**\n\nJoin us for a [ webinar ](https://lu.ma/nzh3o83f) on \"Open-Source Longterm\nMemory for Autonomous Agents\" this Thursday at 9am PT, where we'll explore the\nmemary architecture with Julian Saks, Kevin Li, Seyeong Han and rest of memary\nteam, diving deep into the challenges and future of long-term memory for\nautonomous systems.\n\n##  **Events:**\n\nWe are having our first-ever [ meetup\n](https://partiful.com/e/IwbFDzAX0xPCPsC897SX) at our new office in San\nFrancisco! [ Join us ](https://partiful.com/e/IwbFDzAX0xPCPsC897SX) to connect\nwith our team and friends from Activeloop and Tryolabs, as we discuss the\nlatest developments in generative AI.\n\n", "mimetype": "text/plain", "start_char_idx": 2331, "end_char_idx": 5646, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f9df9c1-aafb-4235-9c2f-59534251ac72": {"__data__": {"id_": "1f9df9c1-aafb-4235-9c2f-59534251ac72", "embedding": null, "metadata": {"filename": "secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.md", "extension": ".md", "title": "Secure code execution in LlamaIndex with Azure Container Apps dynamic sessions", "date": "May 21, 2024", "url": "https://www.llamaindex.ai/blog/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2fd405cc-abb1-4276-a505-f0c54683c981", "node_type": "4", "metadata": {"filename": "secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.md", "extension": ".md", "title": "Secure code execution in LlamaIndex with Azure Container Apps dynamic sessions", "date": "May 21, 2024", "url": "https://www.llamaindex.ai/blog/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions"}, "hash": "1412866b83a2e3097569dc2bb3e861a08914ea118bd77a8e0685d98ba1241bc8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a452cb9-b9ec-4d21-844a-66f02127e785", "node_type": "1", "metadata": {}, "hash": "8934b1b54baf3d54b0589be0d8dfb4d96b8bb52866bed2c9341ad0c380ca50c8", "class_name": "RelatedNodeInfo"}}, "text": "One of the many amazing feats that LLMs are capable of is generating\nexecutable code. This can be used to solve a variety of complex problems that\nrequire calculations and fixed logic that traditional computing excels at but\nLLMs can struggle to perform directly. When building agents to perform complex\ntasks, equipping your agent with code execution as an available tool can be a\npowerful strategy.\n\nHowever, this strategy comes with a major drawback: executable code can be\nflawed or even dangerous to execute, and detecting whether code will be\nproblematic prior to executing it is arguably an expression of the [ Halting\nProblem ](https://en.wikipedia.org/wiki/Halting_problem) , making it\nimpossible to guarantee success at detection.\n\nThe solution is [ sandboxing\n](https://en.wikipedia.org/wiki/Sandbox_\\(computer_security\\)) , to isolate\npotentially problematic code from the host environment. Now, thanks to dynamic\nsessions in Azure Container Apps, the ability to execute sandboxed code\ngenerated by an LLM is simple directly from LlamaIndex. It\u2019s implemented as a\ntool that can be used by any LlamaIndex agent.\n\nIn this blog post we\u2019ll show you exactly how to use the new Azure Code\nInterpreter tool and walk you through a couple of examples of how to make the\nmost of it. You can see the full code in [ this notebook\n](https://docs.llamaindex.ai/en/latest/examples/tools/azure_code_interpreter/)\nand read more in the [ tool documentation ](https://llamahub.ai/l/tools/llama-\nindex-tools-azure-code-interpreter) on LlamaHub and on [ learn.microsoft.com\n](https://learn.microsoft.com/en-us/azure/container-apps/sessions-code-\ninterpreter?tabs=azure-cli) .\n\n##  Set up Azure Container Apps dynamic sessions\n\nFirst, install our python packages including the tool:\n\n    \n    \n    pip install llama-index\n    pip install llama-index-llms-azure\n    pip install llama-index-tools-azure-code-interpreter\n\nIn [ the notebook\n](https://docs.llamaindex.ai/en/latest/examples/tools/azure_code_interpreter/)\nwe\u2019re using GPT 3.5 Turbo hosted on Azure as the LLM, but you can use any LLM\ncapable of tool use:\n\n    \n    \n    from llama_index.llms.azure_openai import AzureOpenAI\n    llm = AzureOpenAI(\n        model=\"gpt-35-turbo\",\n        deployment_name=\"gpt-35-deploy\",\n        api_key=api_key,\n        azure_endpoint=azure_endpoint,\n        api_version=api_version,\n    )\n\nOnce you\u2019ve got your LLM set up, you\u2019ll need to [ create a session pool\n](https://learn.microsoft.com/en-us/azure/container-apps/sessions-code-\ninterpreter?tabs=azure-cli#code-interpreter-session-pool) to host your\nexecutions. Doing this will give you a pool management endpoint URL that you\ncan provide to LlamaIndex like this:\n\n    \n    \n    # Import the AzureCodeInterpreterToolSpec from llama_index\n    from llama_index.tools.azure_code_interpreter import (\n        AzureCodeInterpreterToolSpec,\n    )\n    \n    # Create the AzureCodeInterpreterToolSpec with the pool_managment_endpoint set to your session management endpoint\n    # It is optional to set the local_save_path, but it is recommended to set it to a path where the tool can automatically save any intermediate data generated from Python code's output.\n    azure_code_interpreter_spec = AzureCodeInterpreterToolSpec(\n        pool_managment_endpoint=\"your-pool-management-endpoint\",\n        local_save_path=\"local-file-path-to-save-intermediate-data\",\n    )\n\nThis sets up a tool ready to be used with LlamaIndex. You\u2019re now ready to set\nup your agent:\n\n    \n    \n    # Import the ReActAgent\n    from llama_index.core.agent import ReActAgent\n    \n    # Create the ReActAgent and inject the tools defined in the AzureDynamicSessionsToolSpec\n    agent = ReActAgent.from_tools(\n        azure_code_interpreter_spec.to_tool_list(), llm=llm, verbose=True\n    )\n\nIn this example we\u2019re providing only a single tool, but you could provide any\nother tools you like to your ReAct agent. Now you\u2019ve got an agent, you\u2019re\nready to ask it to perform tasks!\n\n##  Dynamic sessions code interpreter in action\n\nIn our first example, we\u2019re going to ask the agent the time in Seattle. This\nis usually a tricky task for LLMs, which don\u2019t know what time it is anywhere!\n\n    \n    \n    # Test the agent with simple answers that could leverage Python codes\n    print(agent.chat(\"Tell me the current time in Seattle.\"))\n\nThe agent generates python code to determine the time and convert it to the\ncorrect time zone. It passes this code to Azure Container Apps dynamic\nsessions, which execute the code and return the answer:\n\n    \n    \n    Thought: To provide the current time in Seattle, I need to calculate it based on the current UTC time and adjust for Seattle's time zone, which is Pacific Daylight Time (PDT) during daylight saving time and Pacific Standard Time (PST) outside of daylight saving time. PDT is UTC-7, and PST is UTC-8. I can use the code interpreter tool to get the current UTC time and adjust it accordingly.\n    Action: code_interpreter\n    Action Input: {'python_code': \"from datetime import datetime, timedelta; import pytz; utc_now = datetime.now(pytz.utc); seattle_time = utc_now.astimezone(pytz.timezone('America/Los_Angeles')); seattle_time.strftime('%Y-%m-%d %H:%M:%S %Z%z')\"}\n    Observation: {'$id': '1', 'status': 'Success', 'stdout': '', 'stderr': '', 'result': '2024-05-04 13:54:09 PDT-0700', 'executionTimeInMilliseconds': 120}\n    Thought: I can answer without using any more tools. I'll use the user's language to answer.\n    ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5472, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a452cb9-b9ec-4d21-844a-66f02127e785": {"__data__": {"id_": "2a452cb9-b9ec-4d21-844a-66f02127e785", "embedding": null, "metadata": {"filename": "secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.md", "extension": ".md", "title": "Secure code execution in LlamaIndex with Azure Container Apps dynamic sessions", "date": "May 21, 2024", "url": "https://www.llamaindex.ai/blog/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2fd405cc-abb1-4276-a505-f0c54683c981", "node_type": "4", "metadata": {"filename": "secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.md", "extension": ".md", "title": "Secure code execution in LlamaIndex with Azure Container Apps dynamic sessions", "date": "May 21, 2024", "url": "https://www.llamaindex.ai/blog/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions"}, "hash": "1412866b83a2e3097569dc2bb3e861a08914ea118bd77a8e0685d98ba1241bc8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f9df9c1-aafb-4235-9c2f-59534251ac72", "node_type": "1", "metadata": {"filename": "secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.md", "extension": ".md", "title": "Secure code execution in LlamaIndex with Azure Container Apps dynamic sessions", "date": "May 21, 2024", "url": "https://www.llamaindex.ai/blog/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions"}, "hash": "6118c9eafdd8438b0b441c355d6a2b42bcffbc342d05439343ca0ae7fc401dc8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11704085-188f-4d20-a04f-7af15c810a1b", "node_type": "1", "metadata": {}, "hash": "0f8137da8a4d5a0a5eb2a63aa4ef17f9b2ebad7ca68db421958729ff96edcdac", "class_name": "RelatedNodeInfo"}}, "text": "Answer: The current time in Seattle is 2024-05-04 13:54:09 PDT.\n    The current time in Seattle is 2024-05-04 13:54:09 PDT.\n    \n\nYou can also use the tool to safely inspect and manipulate data, as in this\nexample where we ask it to open a CSV file and answer questions about it:\n\n    \n    \n    # Upload a sample temperature file of a day in Redmond Washington and ask a question about it\n    res = azure_code_interpreter_spec.upload_file(\n        local_file_path=\"./TemperatureData.csv\"\n    )\n    if len(res) != 0:\n        print(\n            agent.chat(\"Find the highest temperature in the file that I uploaded.\")\n        )\n\nIt doesn\u2019t just read data from the CSV, it performs math on it to determine\nthe highest temperature:\n\n    \n    \n    Thought: I need to use the list_files tool to get the metadata for the uploaded file, and then use python to read the file and find the highest temperature.\n    Action: list_files\n    Action Input: {}\n    Observation: [RemoteFileMetadata(filename='TemperatureData.csv', size_in_bytes=514, file_full_path='/mnt/data/TemperatureData.csv')]\n    Thought: I have the metadata for the file. I need to use python to read the file and find the highest temperature.\n    Action: code_interpreter\n    Action Input: {'python_code': \"import csv\\n\\nwith open('/mnt/data/TemperatureData.csv', 'r') as f:\\n    reader = csv.reader(f)\\n    next(reader)\\n    highest_temp = float('-inf')\\n    for row in reader:\\n        temp = float(row[1])\\n        if temp > highest_temp:\\n            highest_temp = temp\\nprint(highest_temp)\"}\n    Observation: {'$id': '1', 'status': 'Success', 'stdout': '12.4\\n', 'stderr': '', 'result': '', 'executionTimeInMilliseconds': 26}\n    Thought: I have the highest temperature. I can answer the question.\n    ", "mimetype": "text/plain", "start_char_idx": 5472, "end_char_idx": 7236, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11704085-188f-4d20-a04f-7af15c810a1b": {"__data__": {"id_": "11704085-188f-4d20-a04f-7af15c810a1b", "embedding": null, "metadata": {"filename": "secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.md", "extension": ".md", "title": "Secure code execution in LlamaIndex with Azure Container Apps dynamic sessions", "date": "May 21, 2024", "url": "https://www.llamaindex.ai/blog/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2fd405cc-abb1-4276-a505-f0c54683c981", "node_type": "4", "metadata": {"filename": "secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.md", "extension": ".md", "title": "Secure code execution in LlamaIndex with Azure Container Apps dynamic sessions", "date": "May 21, 2024", "url": "https://www.llamaindex.ai/blog/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions"}, "hash": "1412866b83a2e3097569dc2bb3e861a08914ea118bd77a8e0685d98ba1241bc8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a452cb9-b9ec-4d21-844a-66f02127e785", "node_type": "1", "metadata": {"filename": "secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.md", "extension": ".md", "title": "Secure code execution in LlamaIndex with Azure Container Apps dynamic sessions", "date": "May 21, 2024", "url": "https://www.llamaindex.ai/blog/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions"}, "hash": "51c5d5af58069f4b89fbf43f331ade4476e378f94f0908376ac2cc49797749d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1dbd87e1-a3a3-4fc7-90b9-1facc33d8802", "node_type": "1", "metadata": {}, "hash": "8c7e13bc5a0e0bb899d3a16226b4c9ee4f005970dfbb17c9a0d3672d61a99b04", "class_name": "RelatedNodeInfo"}}, "text": "Answer: The highest temperature in the file is 12.4 degrees.\n    The highest temperature in the file is 12.4 degrees.\n\nThe tool can also safely modify your uploaded files, as in this example where\nwe get it to sort the data in the CSV:\n\n    \n    \n    # Ask the LLM to perform modifications on the sample temperature file\n    print(\n        agent.chat(\n            \"Rearrange the temperature data in a descending order and save it back to the original csv file.\"\n        )\n    )\n\nIn this example, you\u2019ll see that the LLM initially produces code with a syntax\nerror, but the agent is smart enough to figure out what went wrong and correct\nitself:\n\n    \n    \n    Thought: To proceed, I need to first list the files in the session pool to identify the uploaded temperature data file.\n    Action: list_files\n    Action Input: {}\n    Observation: [RemoteFileMetadata(filename='TemperatureCurve.png', size_in_bytes=35512, file_full_path='/mnt/data/TemperatureCurve.png'), RemoteFileMetadata(filename='TemperatureData.csv', size_in_bytes=514, file_full_path='/mnt/data/TemperatureData.csv')]\n    Thought: The temperature data file is identified as 'TemperatureData.csv'. I need to read its contents, rearrange the temperature data in descending order, and then save the changes back to the original CSV file.\n    Action: code_interpreter\n    Action Input: {'python_code': \"import pandas as pd\\n\\ndf = pd.read_csv('/mnt/data/TemperatureData.csv')\\ndf_sorted = df.sort_values(by='Temperature', ascending=False)\\ndf_sorted.to_csv('/mnt/data/TemperatureData.csv', index=False)\\n'filename': 'TemperatureData.csv'\"}\n    Observation: {'$id': '1', 'status': 'Failure', 'stdout': '', 'stderr': \"illegal target for annotation (3407605186.py, line 6)  Cell In[2363], line 6\\n    'filename': 'TemperatureData.csv'\\n    ^\\nSyntaxError: illegal target for annotation\\n\\n\", 'result': None, 'executionTimeInMilliseconds': 9}\n    Thought: There was a syntax error in the Python code due to an incorrect line. I need to correct the code and execute it again to sort the temperature data and save it back to the CSV file.\n    Action: code_interpreter\n    Action Input: {'python_code': \"import pandas as pd\\n\\ndf = pd.read_csv('/mnt/data/TemperatureData.csv')\\ndf_sorted = df.sort_values(by='Temperature', ascending=False)\\ndf_sorted.to_csv('/mnt/data/TemperatureData.csv', index=False)\"}\n    Observation: {'$id': '1', 'status': 'Success', 'stdout': '', 'stderr': '', 'result': '', 'executionTimeInMilliseconds': 91}\n    Thought: I can answer without using any more tools. I'll use the user's language to answer.\n    ", "mimetype": "text/plain", "start_char_idx": 7236, "end_char_idx": 9825, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1dbd87e1-a3a3-4fc7-90b9-1facc33d8802": {"__data__": {"id_": "1dbd87e1-a3a3-4fc7-90b9-1facc33d8802", "embedding": null, "metadata": {"filename": "secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.md", "extension": ".md", "title": "Secure code execution in LlamaIndex with Azure Container Apps dynamic sessions", "date": "May 21, 2024", "url": "https://www.llamaindex.ai/blog/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2fd405cc-abb1-4276-a505-f0c54683c981", "node_type": "4", "metadata": {"filename": "secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.md", "extension": ".md", "title": "Secure code execution in LlamaIndex with Azure Container Apps dynamic sessions", "date": "May 21, 2024", "url": "https://www.llamaindex.ai/blog/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions"}, "hash": "1412866b83a2e3097569dc2bb3e861a08914ea118bd77a8e0685d98ba1241bc8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11704085-188f-4d20-a04f-7af15c810a1b", "node_type": "1", "metadata": {"filename": "secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.md", "extension": ".md", "title": "Secure code execution in LlamaIndex with Azure Container Apps dynamic sessions", "date": "May 21, 2024", "url": "https://www.llamaindex.ai/blog/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions"}, "hash": "68693f91891fa192b56dbdedf4ab6d2e7290c709986b0f95b6bc702287d3154a", "class_name": "RelatedNodeInfo"}}, "text": "Answer: The temperature data has been successfully rearranged in descending order and saved back to the original CSV file, 'TemperatureData.csv'.\n    The temperature data has been successfully rearranged in descending order and saved back to the original CSV file, 'TemperatureData.csv'.\n\nModifying files would not be useful if you couldn\u2019t retrieve them after\nmodification, which is done like this:\n\n    \n    \n    # Download the modified file\n    azure_code_interpreter_spec.download_file_to_local(\n        remote_file_path=\"TemperatureData.csv\",\n        local_file_path=\"/.../SortedTemperatureData.csv\",\n    )\n\n##  Endless possibilities\n\nThe scope of tasks that you can achieve with sandboxed code execution is as\nbroad as programming itself, and having safe execution guaranteed allows you\nto confidently hand agents tasks that previously you might have been hesitant\nabout. We think this is an amazing addition to our LLM agent capabilities and\nwe\u2019re excited to see what you build with it.\n\n", "mimetype": "text/plain", "start_char_idx": 9825, "end_char_idx": 10820, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "39b61921-4798-46f0-adb7-964408b5ee30": {"__data__": {"id_": "39b61921-4798-46f0-adb7-964408b5ee30", "embedding": null, "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bafef8a1-f34a-4811-a591-36e13ee567c8", "node_type": "4", "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "b9758283ffec56a30a8f3ccef0d62673749b98c47cbbfb461b14b6b706bf8c4e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc6a8923-7d50-4c27-a26e-14118d8627eb", "node_type": "1", "metadata": {}, "hash": "5a005bda79f1b5ad90e47cbf84079edfb146d89d4c5fc3eca733d1409c04d05f", "class_name": "RelatedNodeInfo"}}, "text": "_This is a guest post from our friends at Mozilla about[ Llamafile\n](https://future.mozilla.org/news/llamafile-four-months-of-progress-towards-\ndemocratizing-ai/) _\n\n[ llamafile ](https://github.com/Mozilla-Ocho/llamafile) , an open source\nproject from Mozilla, is one of the simplest ways to run a large language\nmodel (LLM) on your laptop. All you have to do is download a llamafile from [\nHuggingFace ](https://huggingface.co/models?sort=trending&search=llamafile)\nthen run the file. That's it. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 498, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc6a8923-7d50-4c27-a26e-14118d8627eb": {"__data__": {"id_": "bc6a8923-7d50-4c27-a26e-14118d8627eb", "embedding": null, "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bafef8a1-f34a-4811-a591-36e13ee567c8", "node_type": "4", "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "b9758283ffec56a30a8f3ccef0d62673749b98c47cbbfb461b14b6b706bf8c4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "39b61921-4798-46f0-adb7-964408b5ee30", "node_type": "1", "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "3e9566e10d06992997c3675cb66bf622d9978c72bc601def2a2560a9ad7bf3a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9cc8dc14-3b6e-4da8-ad44-070655b5ac67", "node_type": "1", "metadata": {}, "hash": "e7256cb033069549fac23a025a70c6cf53df430aeca37c43b1cac28b96870721", "class_name": "RelatedNodeInfo"}}, "text": "**On most computers, you won't need to install\nanything.**\n\nThere are a few reasons why you might want to run an LLM on your laptop,\nincluding:\n\n1\\. Privacy: Running locally means you won't have to share your data with\nthird parties.\n\n2\\. High availability: Run your LLM-based app without an internet connection.\n\n3\\. Bring your own model: You can easily test many different open-source LLMs\n(anything available on HuggingFace) and see which one works best for your\ntask.\n\n4\\. Free debugging/testing: Local LLMs allow you to test many parts of an LLM-\nbased system without paying for API calls.\n\nIn this blog post, we'll show how to set up a llamafile and use it to run a\nlocal LLM on your computer. Then, we'll show how to use LlamaIndex with your\nllamafile as the LLM & embedding backend for a local RAG-based research\nassistant. You won't have to sign up for any cloud service or send your data\nto any third party--everything will just run on your laptop.\n\nNote: You can also get all of the example code below as a Jupyter notebook\nfrom our [ GitHub repo ](https://github.com/Mozilla-Ocho/llamafile-llamaindex-\nexamples) .\n\n##  Download and run a llamafile\n\nFirst, what is a llamafile? A llamafile is an executable LLM that you can run\non your own computer. It contains the weights for a given open source LLM, as\nwell as everything needed to actually run that model on your computer. There's\nnothing to install or configure (with a few caveats, discussed [ here\n](https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file#gotchas) ).\n\nEach llamafile bundles 1) model weights & metadata in gguf format + 2) a copy\nof [ `llama.cpp` ](https://github.com/ggerganov/llama.cpp) specially compiled\nusing [Cosmopolitan Libc](https://github.com/jart/cosmopolitan). This allows\nthe models to run on most computers without additional installation.\nllamafiles also come with a ChatGPT-like browser interface, a CLI, and an\nOpenAI-compatible REST API for chat models.\n\nThere are only 2 steps to setting up a llamafile:\n\n1\\. Download a llamafile\n\n2\\. Make the llamafile executable\n\nWe'll go through each step in detail below.\n\n###  Step 1: Download a llamafile\n\nThere are many llamafiles available on the [ HuggingFace model hub\n](https://huggingface.co/models?sort=trending&search=llamafile) (just search\nfor 'llamafile') but for the purpose of this walkthrough, we'll use [\nTinyLlama-1.1B\n](https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile)\n(0.67 GB, [ model info\n](https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile) ). To\ndownload the model, you can either click this download link: [ TinyLlama-1.1B\n](https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile?download=true)\nor open a terminal and use something like `wget`. The download should take\n5-10 minutes depending on the quality of your internet connection.\n\n    \n    \n    wget https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile \n\nThis model is small and won't be very good at actually answering questions\nbut, since it's a relatively quick download and its inference speed will allow\nyou to index your vector store in just a few minutes, it's good enough for the\nexamples below. For a higher-quality LLM, you may want to use a larger model\nlike [ Mistral-7B-Instruct\n](https://huggingface.co/Mozilla/Mistral-7B-Instruct-v0.2-llamafile/resolve/main/mistral-7b-instruct-v0.2.Q4_0.llamafile?download=true)\n(5.15 GB, [ model info\n](https://huggingface.co/Mozilla/Mistral-7B-Instruct-v0.2-llamafile) ).\n\n###  Step 2: Make the llamafile executable\n\nIf you didn't download the llamafile from the command line, figure out where\nyour browser stored your downloaded llamafile.\n\nNow, open your computer's terminal and, if necessary, go to the directory\nwhere your llamafile is stored: `cd path/to/downloaded/llamafile`\n\n**If you're using macOS, Linux, or BSD** , you'll need to grant permission for\nyour computer to execute this new file. (You only need to do this once.):\n\n**If you're on Windows, instead just rename the file by adding \".exe\" on the\nend** e.g. rename `TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile` to\n`TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile.exe`\n\n    \n    \n    chmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile\n\n###  Kick the tires\n\nNow, your llamafile should be ready to go. First, you can check which version\nof the llamafile library was used to build the llamafile binary you should\ndownloaded:\n\n    \n    \n    ./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --version\n    \n    llamafile v0.7.0\n\nThis post was written using a model built with `llamafile v0.7.0`. If your\nllamafile displays a different version and some of the steps below don't work\nas expected, please [ post an issue on the llamafile issue tracker\n](https://github.com/Mozilla-Ocho/llamafile/issues) .\n\nThe easiest way to use your llamafile is via its built-in chat interface. In a\nterminal, run\n\n    \n    \n    ./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile\n\nYour browser should open automatically and display a chat interface. (If it\ndoesn't, just open your browser and point it at http://localhost:8080). When\nyou're done chatting, return to your terminal and hit `Control-C` to shut down\nllamafile. If you're running these commands inside a notebook, just interrupt\nthe notebook kernel to stop the llamafile.\n\nIn the rest of this walkthrough, we'll be using the llamafile's built-in\ninference server instead of the browser interface. The llamafile's server\nprovides a REST API for interacting with the TinyLlama LLM via HTTP. Full\nserver API documentation is available [ here ](https://github.com/Mozilla-\nOcho/llamafile/blob/main/llama.cpp/server/README.md#api-endpoints) . To start\nthe llamafile in server mode, run:\n\n    \n    \n    ./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding\n\n###  Summary: Download and run a llamafile\n\n    \n    \n    # 1. Download the llamafile-ized model\n    wget https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile\n    \n    # 2. Make it executable (you only need to do this once)\n    chmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile\n    \n    # 3. Run in server mode\n    ./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding\n\n##  Build a research assistant using LlamaIndex and llamafile\n\nNow, we'll show how to use LlamaIndex with your llamafile to build a research\nassistant to help you learn about some topic of interest--for this post, we\nchose [ homing pigeons ](https://en.wikipedia.org/wiki/Homing_pigeon) . We'll\nshow how to prepare your data, index into a vector store, then query it.\n\n", "mimetype": "text/plain", "start_char_idx": 498, "end_char_idx": 7280, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9cc8dc14-3b6e-4da8-ad44-070655b5ac67": {"__data__": {"id_": "9cc8dc14-3b6e-4da8-ad44-070655b5ac67", "embedding": null, "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bafef8a1-f34a-4811-a591-36e13ee567c8", "node_type": "4", "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "b9758283ffec56a30a8f3ccef0d62673749b98c47cbbfb461b14b6b706bf8c4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc6a8923-7d50-4c27-a26e-14118d8627eb", "node_type": "1", "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "a41c41637a4fd5ff59c4853660941501cc3276faec8403eef79e8548b5f06fcb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d833caa7-9457-44ae-9773-11cd1c95d4a2", "node_type": "1", "metadata": {}, "hash": "f2cc5e33bf115179aa230c426b625f5f901f06884086bcd039e14c71710d05d9", "class_name": "RelatedNodeInfo"}}, "text": "One of the nice things about running an LLM locally is privacy. You can mix\nboth \"public data\" like Wikipedia pages and \"private data\" without worrying\nabout sharing your data with a third party. Private data could include e.g.\nyour private notes on a topic or PDFs of classified content. As long as you\nuse a local LLM (and a local vector store), you won't have to worry about\nleaking data. Below, we'll show how to combine both types of data. Our vector\nstore will include Wikipedia pages, an Army manual on caring for homing\npigeons, and some brief notes we took while we were reading about this topic.\n\nTo get started, download our example data:\n\n    \n    \n    mkdir data\n    \n    # Download 'The Homing Pigeon' manual from Project Gutenberg\n    wget https://www.gutenberg.org/cache/epub/55084/pg55084.txt -O data/The_Homing_Pigeon.txt\n    \n    # Download some notes on homing pigeons\n    wget https://gist.githubusercontent.com/k8si/edf5a7ca2cc3bef7dd3d3e2ca42812de/raw/24955ee9df819e21975b1dd817938c1bfe955634/homing_pigeon_notes.md -O data/homing_pigeon_notes.md\n\nNext, we'll need to install LlamaIndex and a few of its integrations:\n\n    \n    \n    # Install llama-index\n    pip install llama-index-core\n    # Install llamafile integrations and SimpleWebPageReader\n    pip install llama-index-embeddings-llamafile llama-index-llms-llamafile llama-index-readers-web\n\n###  Start your llamafile server and configure LlamaIndex\n\nIn this example, we'll use the same llamafile to both produce the embeddings\nthat will get indexed in our vector store and as the LLM that will answer\nqueries later on. (However, there is no reason you can't use one llamafile for\nthe embeddings and separate llamafile for the LLM functionality--you would\njust need to start the llamafile servers on different ports.)\n\nTo start the llamafile server, open a terminal and run:\n\n    \n    \n    ./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding --port 8080\n\nNow, we'll configure LlamaIndex to use this llamafile:\n\n    \n    \n    # Configure LlamaIndex\n    from llama_index.core import Settings\n    from llama_index.embeddings.llamafile import LlamafileEmbedding\n    from llama_index.llms.llamafile import Llamafile\n    from llama_index.core.node_parser import SentenceSplitter\n    \n    Settings.embed_model = LlamafileEmbedding(base_url=\"http://localhost:8080\")\n    \n    Settings.llm = Llamafile(\n    \tbase_url=\"http://localhost:8080\",\n    \ttemperature=0,\n    \tseed=0\n    )\n    \n    # Also set up a sentence splitter to ensure texts are broken into semantically-meaningful chunks (sentences) that don't take up the model's entire\n    # context window (2048 tokens). Since these chunks will be added to LLM prompts as part of the RAG process, we want to leave plenty of space for both\n    # the system prompt and the user's actual question.\n    ", "mimetype": "text/plain", "start_char_idx": 7280, "end_char_idx": 10126, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d833caa7-9457-44ae-9773-11cd1c95d4a2": {"__data__": {"id_": "d833caa7-9457-44ae-9773-11cd1c95d4a2", "embedding": null, "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bafef8a1-f34a-4811-a591-36e13ee567c8", "node_type": "4", "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "b9758283ffec56a30a8f3ccef0d62673749b98c47cbbfb461b14b6b706bf8c4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9cc8dc14-3b6e-4da8-ad44-070655b5ac67", "node_type": "1", "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "33bb345386d27c403a6c569ae887f3c92e3433825847b4af8b228537864641a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8be4bc96-a20c-44af-906b-110eedee19b2", "node_type": "1", "metadata": {}, "hash": "9c6b4d219a43847f8a82a01ecab8af804031441e1fdd53526ccac0329bcc0b84", "class_name": "RelatedNodeInfo"}}, "text": "Settings.transformations = [\n    \tSentenceSplitter(\n        \tchunk_size=256,\n        \tchunk_overlap=5\n    \t)\n    ]\n\n###  Prepare your data and build a vector store\n\nNow, we'll load our data and index it.\n\n    \n    \n    # Load local data\n    from llama_index.core import SimpleDirectoryReader\n    local_doc_reader = SimpleDirectoryReader(input_dir='./data')\n    docs = local_doc_reader.load_data(show_progress=True)\n    \n    # We'll load some Wikipedia pages as well\n    from llama_index.readers.web import SimpleWebPageReader\n    urls = [\n    \t'https://en.wikipedia.org/wiki/Homing_pigeon',\n    \t'https://en.wikipedia.org/wiki/Magnetoreception',\n    ]\n    web_reader = SimpleWebPageReader(html_to_text=True)\n    docs.extend(web_reader.load_data(urls))\n    \n    # Build the index\n    from llama_index.core import VectorStoreIndex\n    \n    index = VectorStoreIndex.from_documents(\n    \tdocs,\n    \tshow_progress=True,\n    )\n    \n    # Save the index\n    index.storage_context.persist(persist_dir=\"./storage\")\n\n###  Query your research assistant\n\nFinally, we're ready to ask some questions about homing pigeons.\n\n    \n    \n    query_engine = index.as_query_engine()\n    print(query_engine.query(\"What were homing pigeons used for?\"))\n    \n    \n    \t", "mimetype": "text/plain", "start_char_idx": 10126, "end_char_idx": 11371, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8be4bc96-a20c-44af-906b-110eedee19b2": {"__data__": {"id_": "8be4bc96-a20c-44af-906b-110eedee19b2", "embedding": null, "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bafef8a1-f34a-4811-a591-36e13ee567c8", "node_type": "4", "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "b9758283ffec56a30a8f3ccef0d62673749b98c47cbbfb461b14b6b706bf8c4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d833caa7-9457-44ae-9773-11cd1c95d4a2", "node_type": "1", "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}, "hash": "95a32472cd5b20a1e9d0c4fe7be7d68406deab5affc1b93a68bda8285d512b7b", "class_name": "RelatedNodeInfo"}}, "text": "Homing pigeons were used for a variety of purposes, including military reconnaissance, communication, and transportation. They were also used for scientific research, such as studying the behavior of birds in flight and their migration patterns. In addition, they were used for religious ceremonies and as a symbol of devotion and loyalty. Overall, homing pigeons played an important role in the history of aviation and were a symbol of the human desire for communication and connection.\n    \n    \n    print(query_engine.query(\"When were homing pigeons first used?\"))\n    \n    \n    The context information provided in the given context is that homing pigeons were first used in the 19th century. However, prior knowledge would suggest that homing pigeons have been used for navigation and communication for centuries.\n\n##  Conclusion\n\nIn this post, we've shown how to download and set up an LLM running locally\nvia llamafile. Then, we showed how to use this LLM with LlamaIndex to build a\nsimple RAG-based research assistant for learning about homing pigeons. Your\nassistant ran 100% locally: you didn't have to pay for API calls or send data\nto a third party.\n\nAs a next step, you could try running the examples above with a better model\nlike [ Mistral-7B-Instruct\n](https://huggingface.co/Mozilla/Mistral-7B-Instruct-v0.2-llamafile/resolve/main/mistral-7b-instruct-v0.2.Q4_0.llamafile?download=true)\n. You could also try building a research assistant for different topic like\n\"semiconductors\" or \"how to bake bread\".\n\nTo find out more about llamafile, check out the project on [ GitHub\n](https://github.com/Mozilla-Ocho/llamafile) , read this [ blog post\n](https://justine.lol/oneliners/) on bash one-liners using LLMs, or say hi to\nthe community on [ Discord ](https://discord.com/invite/teDuGYVTB2) .\n\n", "mimetype": "text/plain", "start_char_idx": 11371, "end_char_idx": 13177, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90e6a7d0-f6b4-4b3e-af5a-f0e00faed3fc": {"__data__": {"id_": "90e6a7d0-f6b4-4b3e-af5a-f0e00faed3fc", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-05-14.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-14", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-14"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac308aa5-05f0-4693-bec1-037bacdeb574", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-05-14.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-14", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-14"}, "hash": "b0af979e2b5cbba6c9e1df50f5ade34b31a9b3cda4748fe4b7bee50d66f212dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "166658dd-c3b2-4f95-8aa8-fa5514696daf", "node_type": "1", "metadata": {}, "hash": "39837291dc512e3af449031188b153f6740abb976c530233ab2bb04f329a408b", "class_name": "RelatedNodeInfo"}}, "text": "Hello LlamaIndex Family!\n\nWelcome to another thrilling weekly update from LlamaIndex! We're excited to\nshare a variety of outstanding updates, guides, and tutorials with you. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 175, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "166658dd-c3b2-4f95-8aa8-fa5514696daf": {"__data__": {"id_": "166658dd-c3b2-4f95-8aa8-fa5514696daf", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-05-14.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-14", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-14"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac308aa5-05f0-4693-bec1-037bacdeb574", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-05-14.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-14", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-14"}, "hash": "b0af979e2b5cbba6c9e1df50f5ade34b31a9b3cda4748fe4b7bee50d66f212dc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90e6a7d0-f6b4-4b3e-af5a-f0e00faed3fc", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-05-14.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-14", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-14"}, "hash": "67b5372d059479e04f9eb6466b94e79677ec81c157236caf8d75c661591f8592", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdc0bf98-af01-437b-bcb7-ed8ed05085c9", "node_type": "1", "metadata": {}, "hash": "0a06673a4682459b0403192ac48848944ce3eb0ccbcfbb46c885e1111f50151b", "class_name": "RelatedNodeInfo"}}, "text": "But\nfirst, we have an exciting announcement to make.\n\nWe are thrilled to announce a new [ course\n](https://www.deeplearning.ai/short-courses/building-agentic-rag-with-\nllamaindex/?utm_campaign=llamaindexC2-launch&utm_content=292515650&utm_medium=social&utm_source=twitter&hss_channel=tw-992153930095251456)\nin collaboration with DeepLearningAI\u2014Building Agentic RAG. In this course,\nyou\u2019ll learn how to build a research assistant that can reason over multiple\ndocuments and answer complex questions. You\u2019ll also learn how to step through\nthe execution of the agent and steer it with human feedback. [ Check it out\n](https://www.deeplearning.ai/short-courses/building-agentic-rag-with-\nllamaindex/?utm_campaign=llamaindexC2-launch&utm_content=292515650&utm_medium=social&utm_source=twitter&hss_channel=tw-992153930095251456)\nand take your RAG skills to the next level!\n\n##  **The highlights:**\n\n  1. Day 0 Support for GPT-4o **-** [ Tweet ](https://x.com/llama_index/status/1790081409039872070)\n  2. Llama3 Cookbook **-** [ Tweet ](https://x.com/llama_index/status/1790047097024348444)\n  3. TypeScript Agent [ Guide ](https://github.com/run-llama/ts-agents) **.**\n\n##  **Feature Releases and Enhancements:**\n\n  1. We have introduced day 0 support for GPT-4o in both Python and TypeScript. Additionally, we've created demo notebooks ( [ demo1 ](https://colab.research.google.com/drive/1CTIs9C6HLqKLqU_PpG1NXaZaH0Ja5pzg#scrollTo=FB55THvgVWXs) and [ demo2 ](https://colab.research.google.com/drive/1CTIs9C6HLqKLqU_PpG1NXaZaH0Ja5pzg#scrollTo=0ZTqYzIl2DU1) ) to help you easily experiment with GPT-4o using LlamaIndex. [ Tweet ](https://x.com/llama_index/status/1790081409039872070) . \n  ", "mimetype": "text/plain", "start_char_idx": 175, "end_char_idx": 1856, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bdc0bf98-af01-437b-bcb7-ed8ed05085c9": {"__data__": {"id_": "bdc0bf98-af01-437b-bcb7-ed8ed05085c9", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-05-14.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-14", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-14"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac308aa5-05f0-4693-bec1-037bacdeb574", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-05-14.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-14", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-14"}, "hash": "b0af979e2b5cbba6c9e1df50f5ade34b31a9b3cda4748fe4b7bee50d66f212dc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "166658dd-c3b2-4f95-8aa8-fa5514696daf", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-05-14.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-14", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-14"}, "hash": "e49f4237b9c43413d4d787bf72137af9f388d146393dbec80ad601a803a2d92f", "class_name": "RelatedNodeInfo"}}, "text": "2. We have launched Llama3 cookbooks showcasing interesting use cases for Llama 3, from basic chat functionalities to advanced agent development. Ideal for anyone building with local models, either on your laptop or through an API. [ Notebook, ](https://github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/llamaindex_cookbook.ipynb) [ Tweet ](https://x.com/llama_index/status/1790047097024348444) . \n\n##  **Guides:**\n\n  * [ Guide ](https://github.com/run-llama/ts-agents) to building agents in TypeScript: Dive into our comprehensive, open-source guide developed by Laurie that walks you through every step of agent development, from setting up with basic functions to integrating advanced features like local and remote LLMs, and data querying with vectorDB. \n  * [ Guide ](https://x.com/llama_index/status/1789694066382537079) to using RAG for content moderation: [ CloudRaft ](https://twitter.com/cloudraftio) shows how to set up a RAG pipeline to moderate user-generated images effectively, ensuring compliance with predefined rules through techniques like semantic search and efficient inferencing with small LLMs. \n\n##  **Tutorials:**\n\n  * [ Kxsystems ](https://twitter.com/kxsystems) advanced workshop on \"Building Advanced RAG over Complex PDFs with LlamaParse\" to demonstrate how LlamaParse can tackle the challenge of extracting diverse elements like text, tables, images, and graphs from complex research papers. [ Video Tutorial ](https://www.youtube.com/watch?v=EL9lCOgLR58) , [ BlogPost ](https://medium.com/kx-systems/rag-llamaparse-advanced-pdf-parsing-for-retrieval-c393ab29891b) , [ Notebook ](https://colab.research.google.com/github/KxSystems/kdbai-samples/blob/main/LlamaParse_pdf_RAG/llamaParse_demo.ipynb) . \n  * [ Arslan Shahid ](https://twitter.com/naivebaesian) [ tutorial ](https://medium.com/firebird-technologies/generate-powerpoints-using-llama-3-a-first-step-in-automating-slide-decks-536f5fcb6e0e) on Generating PowerPoints with Llama 3, using LlamaIndex to create a Llama3 RAG pipeline. The approach not only answers questions but also generates PowerPoint slide decks by utilizing the python-pptx library to write code programmatically for slide creation. \n  * [ Hanane Dupouy ](https://www.linkedin.com/in/hanane-d-algo-trader/) [ demonstrates ](https://www.linkedin.com/posts/hanane-d-algo-trader_introspective-agent-llamaindex-financial-activity-7193317247342243841-kqor/?utm_source=share&utm_medium=member_desktop) Building a Financial Agent that can Perform Reflection. The approach helps in analyzing stock prices by implementing two types of reflection: CRITIC (tool use) and self-reflection (no tools). \n  * [ zhaozhiming\u2019s ](https://twitter.com/kingzzm) [ tutorial ](https://generativeai.pub/llamaindex-and-rag-evaluation-tools-59bae2944bb3) on evaluating RAG systems, utilizing evaluation libraries like TruLens, Ragas, UpTrain, and DeepEval to assess RAG systems using metrics such as faithfulness, relevance, and answer correctness. \n\n", "mimetype": "text/plain", "start_char_idx": 1856, "end_char_idx": 4849, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d216f694-079a-4d62-9245-ae16d17a0451": {"__data__": {"id_": "d216f694-079a-4d62-9245-ae16d17a0451", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-05-07.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-07", "date": "May 7, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-07"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d2235e16-18fc-4f83-a5fa-e394cdf57e7d", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-05-07.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-07", "date": "May 7, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-07"}, "hash": "6625430fbeb97d5a1a327282cd8432e2b4da241e5ea8f066c26a18dfd4c6d645", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62e3d134-9d6a-466a-8be3-6dd61862701b", "node_type": "1", "metadata": {}, "hash": "8895092e10e0221bfa9c3e09710e2c9d1406c72e44fb837c3f93aa4eed5a62df", "class_name": "RelatedNodeInfo"}}, "text": "Hello LlamaIndex fam!\n\nIf you\u2019re in SF, join us for the first-ever [ Llama 3 Hackathon\n](https://twitter.com/cerebral_valley/status/1785366241030607209) ( [\ninvitation here ](https://partiful.com/e/p5bNF0WkDd1n7JYs3m0A) )! Shack15 is\nan amazing venue and it\u2019s sure to be a fun time. If you can\u2019t make it, stay\ntuned for the rundown on the cool projects that come out of the event. Now, on\nto the highlights:\n\n**The highlights:**\n\n  * [ LlamaIndex.TS hits v0.3! ](https://twitter.com/llama_index/status/1785722400480637291) Loads of new features inside! \n  * [ LlamaIndex Python hits v0.10.34 ](https://twitter.com/llama_index/status/1786426320731488692) ! A bumper release! \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 677, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62e3d134-9d6a-466a-8be3-6dd61862701b": {"__data__": {"id_": "62e3d134-9d6a-466a-8be3-6dd61862701b", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-05-07.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-07", "date": "May 7, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-07"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d2235e16-18fc-4f83-a5fa-e394cdf57e7d", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-05-07.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-07", "date": "May 7, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-07"}, "hash": "6625430fbeb97d5a1a327282cd8432e2b4da241e5ea8f066c26a18dfd4c6d645", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d216f694-079a-4d62-9245-ae16d17a0451", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-05-07.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-07", "date": "May 7, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-07"}, "hash": "9a5bf62bb3d454ec8aef29f03259d547dc4273f27dda6eb83a7aba9275c19bfd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1740996f-08ba-418e-bb10-932067e0f673", "node_type": "1", "metadata": {}, "hash": "e74de1967444f978a9e43c75820193d3b403c9473446508110451c7696ee2718", "class_name": "RelatedNodeInfo"}}, "text": "* That\u2019s a lot! \n\n", "mimetype": "text/plain", "start_char_idx": 677, "end_char_idx": 695, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1740996f-08ba-418e-bb10-932067e0f673": {"__data__": {"id_": "1740996f-08ba-418e-bb10-932067e0f673", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-05-07.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-07", "date": "May 7, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-07"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d2235e16-18fc-4f83-a5fa-e394cdf57e7d", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-05-07.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-07", "date": "May 7, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-07"}, "hash": "6625430fbeb97d5a1a327282cd8432e2b4da241e5ea8f066c26a18dfd4c6d645", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62e3d134-9d6a-466a-8be3-6dd61862701b", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-05-07.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-07", "date": "May 7, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-07"}, "hash": "9e0b9a79bd970ddb7f8194261df37e927053f8e5147b85b1b020f22ba89e9a7e", "class_name": "RelatedNodeInfo"}}, "text": "**Feature Releases and Enhancements:**\n\nTwo big releases this week!\n\n  * LlamaIndex.TS hit version 0.3! [ Tweet ](https://twitter.com/llama_index/status/1785722400480637291) , [ Blog post ](https://ts.llamaindex.ai/blog/welcome-llamaindexts-v0.3)\n    * Features: \n      * Agent support including ReAct, Anthropic and OpenAI agents, as well as a generic AgentRunner class \n      * Standardized Web Streams compatible with React 19, Deno, and Node 22 \n      * More comprehensive type system \n      * Enhanced support for deployment on Next.js, Deno, Cloudflare Workers and Waku \n  * LlamaIndex Python hit version 0.10.34! [ Tweet ](https://twitter.com/llama_index/status/1786426320731488692)\n    * Features: \n      * [ Introspective agents that work through reflection ](https://docs.llamaindex.ai/en/latest/examples/agent/introspective_agent_toxicity_reduction/)\n      * [ Support for huggingface text-generation-inference API ](https://docs.llamaindex.ai/en/stable/examples/llm/huggingface/)\n      * [ Structured planning agent ](https://docs.llamaindex.ai/en/stable/examples/agent/structured_planner/)\n      * [ A chat summary memory buffer ](https://docs.llamaindex.ai/en/stable/examples/memory/ChatSummaryMemoryBuffer/)\n      * [ Rust-based LLM support with Mistral.rs ](https://llamahub.ai/l/llms/llama-index-llms-mistral-rs)\n      * [ Milvus sparse hybrid search ](https://docs.llamaindex.ai/en/stable/examples/vector_stores/MilvusHybridIndexDemo/)\n      * [ Google Firestore vector store support ](https://docs.llamaindex.ai/en/stable/examples/vector_stores/FirestoreVectorStore/)\n  * A new LlamaPack for the Reflection Agentic Pattern. [ Tweet ](https://twitter.com/_nerdai_/status/1786486394996617341)\n\n**Demos:**\n\n  * Filter AirBnB listings using natural language with this open-source demo! It uses Mistral AI\u2019s Mixtral 8x7b and Qdrant engine, plus Streamlit to build UI. [ Tweet ](https://twitter.com/llama_index/status/1786138653271671257) , [ Blog post ](https://harshadsuryawanshi.medium.com/exploring-airbnb-listings-with-semantic-search-a-qdrant-and-llm-powered-approach-a65cd170f710)\n  * Fully local RAG with Llama 3, Ollama and LlamaIndex! A short, sweet guide. [ Tweet ](https://twitter.com/llama_index/status/1787164022565183849) , [ Blog post ](https://blog.gopenai.com/improved-rag-with-llama3-and-ollama-c17dc01f66f6)\n  * Fine-tune your embedding model using labels from a reranker. [ Tweet ](https://twitter.com/llama_index/status/1787296562504425903) , [ Blog post ](https://medium.com/rahasak/optimizing-rag-supervised-embeddings-reranking-with-your-data-with-llamaindex-88344ff89da7)\n\n**Guides:**\n\n  * Hanane Dupouy walks us through building an agent that can perform complex financial calculations. [ Tweet ](https://twitter.com/llama_index/status/1785325832317415641) , [ Slides ](https://www.linkedin.com/posts/hanane-d-algo-trader_anthropic-agent-rag-with-complex-financial-activity-7190389324343881728-TOXX/?utm_source=share&utm_medium=member_desktop)\n  * Plaban Nayak sets up a local, open-source RAG pipeline that uses Llama 3 and Qdrant to demonstrate how to improve the accuracy of your RAG with reranking. [ Tweet ](https://twitter.com/llama_index/status/1786093311658451337) , [ Blog post ](https://nayakpplaban.medium.com/build-an-advanced-reranking-rag-system-using-llama-index-llama-3-and-qdrant-a8b8654174bc)\n  * Jason Zhou talks about the components needed for agentic RAG. [ Tweet ](https://twitter.com/llama_index/status/1786830550441099524)\n  * Divyanshu Dixit walks us through agents dedicated to workflow automation. [ Tweet ](https://twitter.com/divyanshu_van/status/1786786672648110415) , [ Blog post ](https://div.beehiiv.com/p/need-talk-agents)\n\n**Tutorials:**\n\n  * Tyler Hutcherson of Redis and our own Laurie Voss walk you through building agentic RAG with semantic caching and other production-ready techniques. [ Video ](https://www.youtube.com/watch?v=mTNiGfYfdWY&t=5s) , [ Notebook ](https://github.com/redis-developer/agentic-rag/blob/main/Agentic_RAG_Redis_LlamaIndex.ipynb)\n  * Cleanlab has a tutorial on getting trustworthiness scores from your RAG pipeline to allow you to avoid hallucinations and course-correct. [ Tweet ](https://twitter.com/llama_index/status/1786914595342589978) , [ Notebook ](https://help.cleanlab.ai/tutorials/tlm_rag/)\n\n**Webinars:**\n\n  * On May 8 we\u2019ll be co-hosting a webinar with Pulumi on deploying AI applications to AWS. [ Tweet ](https://twitter.com/llama_index/status/1785456924852715907)\n  * Our own Andrei and friends walk you from basic RAG through handling long-context RAG all the way to evaluating your RAG pipeline. [ Tweet ](https://twitter.com/llama_index/status/1785802821604127205) , [ Video ](https://www.youtube.com/watch?v=bNqSRNMgwhQ) , [ Notebook ](https://github.com/nerdai/talks/blob/main/2024/mlops/mlops-rag-bootcamp.ipynb)\n\n", "mimetype": "text/plain", "start_char_idx": 695, "end_char_idx": 5536, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a50e86bb-7445-439d-b248-974e2f76f7eb": {"__data__": {"id_": "a50e86bb-7445-439d-b248-974e2f76f7eb", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-04-30.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-30", "date": "Apr 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-30"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5c4b727b-76da-42d5-bbb0-8aa2d6fecbef", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-04-30.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-30", "date": "Apr 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-30"}, "hash": "19aeb27a20e0bd0319c1daa3dbf9b3d466ba72cb09bc1c494c630b05fd6fe4d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7fd4c6c2-9798-4b54-8908-a83c8167553f", "node_type": "1", "metadata": {}, "hash": "23f93b1210d9986fdffa311406e878b5994caa887aa7e73baef3c88c816cb310", "class_name": "RelatedNodeInfo"}}, "text": "Greetings, LlamaIndex fans!\n\nIt\u2019s delightful springtime weather out here in San Francisco and we hope\nyou\u2019re having a good day! Check out this week\u2019s summary of news, guides and\ntutorials.\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 190, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7fd4c6c2-9798-4b54-8908-a83c8167553f": {"__data__": {"id_": "7fd4c6c2-9798-4b54-8908-a83c8167553f", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-04-30.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-30", "date": "Apr 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-30"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5c4b727b-76da-42d5-bbb0-8aa2d6fecbef", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-04-30.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-30", "date": "Apr 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-30"}, "hash": "19aeb27a20e0bd0319c1daa3dbf9b3d466ba72cb09bc1c494c630b05fd6fe4d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a50e86bb-7445-439d-b248-974e2f76f7eb", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-04-30.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-30", "date": "Apr 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-30"}, "hash": "3321acd9ee4efcfaf9f68b3411af419e3ad2b68888f30ccd2d91999f3911d297", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac1226b9-8b01-49c0-937d-a1b1511395b4", "node_type": "1", "metadata": {}, "hash": "10fad1466dbc5aeed431112e3dab4ef2d65724b6ff53dfe71c94fd66b16e9491", "class_name": "RelatedNodeInfo"}}, "text": "**The highlights:**\n\n  * Day 0 support for Microsoft\u2019s Phi-3 Mini! [ Tweet ](https://twitter.com/llama_index/status/1782893301214986593)\n  * create-llama now supports Llama 3 and Phi-3 and has lots of new features! [ Tweet ](https://twitter.com/MarcusSchiesser/status/1783049713601933487)\n  * Simon was on a security podcast! [ Tweet ](https://twitter.com/llama_index/status/1783963718256411126)\n\n**Feature Releases and Enhancements:**\n\n  1. Jina AI released powerful new open-source rerankers and we have day 0 support as usual! [ Tweet ](https://twitter.com/llama_index/status/1782531355970240955)\n  2. Phi-3 mini was released by Microsoft, a powerful new small model, and we [ put it through its paces ](https://twitter.com/llama_index/status/1782870458121282003) (spoiler: it\u2019s good!) and released day-0 support via Ollama! [ Tweet ](https://twitter.com/llama_index/status/1782893301214986593)\n  3. Our create-llama application generator was updated with many features including being able to show the sources it retrieved from, as well as Llama3 and Phi-3 support. Build an app from scratch in 30 seconds! ", "mimetype": "text/plain", "start_char_idx": 190, "end_char_idx": 1301, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac1226b9-8b01-49c0-937d-a1b1511395b4": {"__data__": {"id_": "ac1226b9-8b01-49c0-937d-a1b1511395b4", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-04-30.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-30", "date": "Apr 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-30"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5c4b727b-76da-42d5-bbb0-8aa2d6fecbef", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-04-30.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-30", "date": "Apr 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-30"}, "hash": "19aeb27a20e0bd0319c1daa3dbf9b3d466ba72cb09bc1c494c630b05fd6fe4d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7fd4c6c2-9798-4b54-8908-a83c8167553f", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-04-30.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-30", "date": "Apr 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-30"}, "hash": "5715bc9e2c0e7c12ec1ed50381ae70359e5e42fc58e7e07b170de9d250a2233f", "class_name": "RelatedNodeInfo"}}, "text": "[ Tweet ](https://twitter.com/MarcusSchiesser/status/1783049713601933487)\n  4. Language Agent Tree Search (LATS) is a powerful new technique that iteratively plans out an array of potential futures, interleaving tool use and reflection to solve problems. We released a Llama Pack implementation. [ Tweet ](https://twitter.com/llama_index/status/1783147291882443112)\n\n**Demos:**\n\n  * **Memary** is a reference implementation of using long-term memory in knowledge graph form for building agents. [ Tweet ](https://twitter.com/llama_index/status/1784604356224164186)\n  * Our hackathon winners wrote a blog post about their winning project, a knowledge-retrieval bot trained on documentation, including how they built it. [ Tweet ](https://twitter.com/llama_index/status/1785060253995851992)\n\n**Guides:**\n\n  * Co-founder Jerry shared his latest deck, a guide to building a context-augmented research assistant that enables multi-hop Q&A, reflection and more. [ Slides ](https://docs.google.com/presentation/d/1IWjo8bhoatWccCfGLYw_QhUI4zfF-MujN3ORIDCBIbc/edit#slide=id.g27749d82178_0_92) , [ tweet ](https://twitter.com/jerryjliu0/status/1782428884287578213)\n  * Corrective RAG or CRAG adds a retrieval evaluation module that determines whether the retrieved context is \u201ccorrect\u201d and improves retrieval. Check out this guide on how to build it step-by-step! [ Tweet ](https://twitter.com/llama_index/status/1782799757376963006)\n  * Jerry also went in-depth on the ingredients necessary for building a complex agent. [ Tweet ](https://twitter.com/jerryjliu0/status/1784279431739265439)\n  * Michael of KX Systems demonstrated making retrieval a multi-hop process for better results. [ Tweet ](https://twitter.com/llama_index/status/1784363604340576615)\n  * A reference architecture for advanced RAG with LlamaIndex and AWS Bedrock. [ Tweet ](https://twitter.com/llama_index/status/1784962053641478454)\n\n**Tutorials:**\n\n  * Build a best-in-class RAG application using Qdrant as a vector store, Jina AI embeddings, and Mixtral 8x7b as the LLM. [ Tweet ](https://twitter.com/llama_index/status/1783601807903863184)\n  * Learn 3+ patterns for building LLM apps on AWS with LlamaIndex. [ Tweet ](https://twitter.com/llama_index/status/1783877951278432733)\n  * A 9-part series on taking RAG from prototype to production. [ Tweet ](https://twitter.com/llama_index/status/1784257178758697272)\n\n**Webinars:**\n\n  * KX Systems are hosting a webinar on May 1 about getting the most out of LlamaParse! [ Tweet ](https://twitter.com/llama_index/status/1783622871614664990)\n  * Co-founder Simon appeared on the MLSecOps podcast talking about security in LLM applications. [ Tweet ](https://twitter.com/llama_index/status/1783963718256411126)\n\n\u200d **Community:**\n\nWe launched a [ LlamaIndex user group in Korea\n](https://twitter.com/llama_index/status/1783286375640613300) !\n\n", "mimetype": "text/plain", "start_char_idx": 1301, "end_char_idx": 4152, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "596e6bab-2f92-40ce-9d63-1cdaa6528faa": {"__data__": {"id_": "596e6bab-2f92-40ce-9d63-1cdaa6528faa", "embedding": null, "metadata": {"filename": "streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.md", "extension": ".md", "title": "Streamlining knowledge work with LlamaIndex, Fireworks and MongoDB", "date": "Apr 29, 2024", "url": "https://www.llamaindex.ai/blog/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "12f4f396-4852-44b5-a835-e401ce95bb35", "node_type": "4", "metadata": {"filename": "streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.md", "extension": ".md", "title": "Streamlining knowledge work with LlamaIndex, Fireworks and MongoDB", "date": "Apr 29, 2024", "url": "https://www.llamaindex.ai/blog/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb"}, "hash": "9499fc1f51d779b7dcf4f5e7f58f0136cfe4cb63e30f84c5d49cf6a176ea7c44", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b4b331a-d25b-4313-8bd8-b6e83df3ca96", "node_type": "1", "metadata": {}, "hash": "33e73e866b3c5fc50e9312461437ae26295aaf3876efe373e1813aa3d75653fa", "class_name": "RelatedNodeInfo"}}, "text": "_This is a guest post from Team CLAB, the winners of \"Best Use of LlamaIndex\"\nat our recent hackathon with MongoDB._\n\nImagine this: you\u2019re deep in a coding project, and a critical question pops up\nabout a specific tool or library. You start the dreaded documentation shuffle\n\u2014 searching through wikis, FAQs, maybe even firing up a separate chatbot for\nspecific tools (like those from LlamaIndex, FireworksAI or anyone else). It\u2019s\nfrustrating! ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 443, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b4b331a-d25b-4313-8bd8-b6e83df3ca96": {"__data__": {"id_": "2b4b331a-d25b-4313-8bd8-b6e83df3ca96", "embedding": null, "metadata": {"filename": "streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.md", "extension": ".md", "title": "Streamlining knowledge work with LlamaIndex, Fireworks and MongoDB", "date": "Apr 29, 2024", "url": "https://www.llamaindex.ai/blog/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "12f4f396-4852-44b5-a835-e401ce95bb35", "node_type": "4", "metadata": {"filename": "streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.md", "extension": ".md", "title": "Streamlining knowledge work with LlamaIndex, Fireworks and MongoDB", "date": "Apr 29, 2024", "url": "https://www.llamaindex.ai/blog/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb"}, "hash": "9499fc1f51d779b7dcf4f5e7f58f0136cfe4cb63e30f84c5d49cf6a176ea7c44", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "596e6bab-2f92-40ce-9d63-1cdaa6528faa", "node_type": "1", "metadata": {"filename": "streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.md", "extension": ".md", "title": "Streamlining knowledge work with LlamaIndex, Fireworks and MongoDB", "date": "Apr 29, 2024", "url": "https://www.llamaindex.ai/blog/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb"}, "hash": "9c484bb2d6cf025d5292d1a4b85fdda9524125aea55e30de06850d59a3047ea3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "138e5330-8019-4f2b-a3e1-1a9cffe23a9f", "node_type": "1", "metadata": {}, "hash": "25b94592312fb14fa2034c03982a491ea75a045cde903d979507ffa922a3900e", "class_name": "RelatedNodeInfo"}}, "text": "We wanted to change that.\n\nThat\u2019s why Team CLAB built [ LlamaWorksDB (try it out!) ](https://clab-\nui.vercel.app/) , your friendly AI-powered doc wizard . No more scattered\nsearches! It taps into the knowledge of multiple sponsors of our hackathon\nincluding LlamaIndex, Fireworks.ai, and MongoDB, all through a single chatbot\ninterface. Need something explained from MongoDB\u2019s docs? ", "mimetype": "text/plain", "start_char_idx": 443, "end_char_idx": 826, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "138e5330-8019-4f2b-a3e1-1a9cffe23a9f": {"__data__": {"id_": "138e5330-8019-4f2b-a3e1-1a9cffe23a9f", "embedding": null, "metadata": {"filename": "streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.md", "extension": ".md", "title": "Streamlining knowledge work with LlamaIndex, Fireworks and MongoDB", "date": "Apr 29, 2024", "url": "https://www.llamaindex.ai/blog/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "12f4f396-4852-44b5-a835-e401ce95bb35", "node_type": "4", "metadata": {"filename": "streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.md", "extension": ".md", "title": "Streamlining knowledge work with LlamaIndex, Fireworks and MongoDB", "date": "Apr 29, 2024", "url": "https://www.llamaindex.ai/blog/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb"}, "hash": "9499fc1f51d779b7dcf4f5e7f58f0136cfe4cb63e30f84c5d49cf6a176ea7c44", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b4b331a-d25b-4313-8bd8-b6e83df3ca96", "node_type": "1", "metadata": {"filename": "streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.md", "extension": ".md", "title": "Streamlining knowledge work with LlamaIndex, Fireworks and MongoDB", "date": "Apr 29, 2024", "url": "https://www.llamaindex.ai/blog/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb"}, "hash": "8219d4efd2a69f26bb3e3f32495928e899bf2c28e1b013f55209ae9a2d098803", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f5312e2-5743-4d78-ba89-e715c2eaff3a", "node_type": "1", "metadata": {}, "hash": "aaf8fd287dbe42816b4c2a5d4855e8a29dc7482ca709ea862881a2f63054f32b", "class_name": "RelatedNodeInfo"}}, "text": "Got it! Want a code\nexample from Fireworks.ai? ", "mimetype": "text/plain", "start_char_idx": 826, "end_char_idx": 873, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f5312e2-5743-4d78-ba89-e715c2eaff3a": {"__data__": {"id_": "0f5312e2-5743-4d78-ba89-e715c2eaff3a", "embedding": null, "metadata": {"filename": "streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.md", "extension": ".md", "title": "Streamlining knowledge work with LlamaIndex, Fireworks and MongoDB", "date": "Apr 29, 2024", "url": "https://www.llamaindex.ai/blog/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "12f4f396-4852-44b5-a835-e401ce95bb35", "node_type": "4", "metadata": {"filename": "streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.md", "extension": ".md", "title": "Streamlining knowledge work with LlamaIndex, Fireworks and MongoDB", "date": "Apr 29, 2024", "url": "https://www.llamaindex.ai/blog/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb"}, "hash": "9499fc1f51d779b7dcf4f5e7f58f0136cfe4cb63e30f84c5d49cf6a176ea7c44", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "138e5330-8019-4f2b-a3e1-1a9cffe23a9f", "node_type": "1", "metadata": {"filename": "streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.md", "extension": ".md", "title": "Streamlining knowledge work with LlamaIndex, Fireworks and MongoDB", "date": "Apr 29, 2024", "url": "https://www.llamaindex.ai/blog/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb"}, "hash": "7791c1915a2efb9beecbef371851041f34d55f8a6026d5be2f8ed47f572bbfec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11f4622f-0fac-47d5-ae35-2022bf4226d7", "node_type": "1", "metadata": {}, "hash": "681cf8a86646919dd4de67f55c4608e3ecd52fca67cc51e63d2c043f9d802a19", "class_name": "RelatedNodeInfo"}}, "text": "Easy!\n\n##  The foundation: LlamaIndex and data ingestion\n\nLlamaIndex was the heart and soul of LlamaWorksDB. It\u2019s like a super versatile\ntoolbox for handling all kinds of documentation! ", "mimetype": "text/plain", "start_char_idx": 873, "end_char_idx": 1059, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11f4622f-0fac-47d5-ae35-2022bf4226d7": {"__data__": {"id_": "11f4622f-0fac-47d5-ae35-2022bf4226d7", "embedding": null, "metadata": {"filename": "streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.md", "extension": ".md", "title": "Streamlining knowledge work with LlamaIndex, Fireworks and MongoDB", "date": "Apr 29, 2024", "url": "https://www.llamaindex.ai/blog/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "12f4f396-4852-44b5-a835-e401ce95bb35", "node_type": "4", "metadata": {"filename": "streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.md", "extension": ".md", "title": "Streamlining knowledge work with LlamaIndex, Fireworks and MongoDB", "date": "Apr 29, 2024", "url": "https://www.llamaindex.ai/blog/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb"}, "hash": "9499fc1f51d779b7dcf4f5e7f58f0136cfe4cb63e30f84c5d49cf6a176ea7c44", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f5312e2-5743-4d78-ba89-e715c2eaff3a", "node_type": "1", "metadata": {"filename": "streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.md", "extension": ".md", "title": "Streamlining knowledge work with LlamaIndex, Fireworks and MongoDB", "date": "Apr 29, 2024", "url": "https://www.llamaindex.ai/blog/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb"}, "hash": "de0bce2a2155562460ac06bc42746f4b23ff8b3b03351f4963208755b9f9a033", "class_name": "RelatedNodeInfo"}}, "text": "We primarily used their open-\nsource readers to grab info straight from websites. A cool hack we did was\ncustomize the ` SimpleWebPageReader ` . We taught it to ignore website\nnavigation bars, saving us a ton of precious tokens. While this worked great\nfor the documentation sites, we used LlamaIndex\u2019s ` GithubRepositoryReader `\nto easily read through each repo.\n\n    \n    \n    from llama_index.readers.web import SimpleWebPageReader\n    import re\n    \n    class LlamaDocsPageReader(SimpleWebPageReader):\n       def load_data(self, urls):\n           documents = super().load_data(urls)\n           processed_documents = []\n           for doc in documents:\n               processed_doc = self.process_document(doc)\n               processed_documents.append(processed_doc)\n           return processed_documents\n    \n       def process_document(self, document):\n           # Split the document text by \"Table of Contents\"\n           pattern = r'(?i)\\n\\n*table\\s*of\\s*contents\\n\\n*'\n           parts = re.split(pattern, document.text, maxsplit=1)\n           # If there is a part after \"Table of Contents\", use it as the document text\n           if len(parts) > 1:\n               document.text = \"Table of contents\".join(parts[1:])\n           return document\n\nChoosing how to split up the docs was interesting. LlamaIndex has options\nranging from the basic ` SentenceSplitter ` to their ` SemanticNodeParser ` ,\nwhich uses AI to group similar ideas. We went with the latter for those\nperfectly sized, meaningful chunks.\n\nFinally, we embedded each \u2018node\u2019 and sent each as a document to MongoDB. Talk\nabout streamlined! MongoDB stored the text, metadata, _and_ our embeddings \u2014\nideal for the kind of search we wanted to build. We used Nomic\u2019s flexible\nembedding model via Fireworks, which let us fine-tune the dimensions for\nmaximum efficiency.\n\n    \n    \n    # FireworksEmbedding defaults to using model\n    embed_model = FireworksEmbedding(api_key=os.getenv('FIREWORKS_API_KEY'),\n                                    model=\"nomic-ai/nomic-embed-text-v1.5\",\n                                    embed_batch_size=10,\n                                    dimensions=768 # can range from 64 to 768\n                                    )\n    \n    # the tried and true sentence splitter\n    text_splitter = SentenceSplitter(chunk_size=1000, chunk_overlap=200)\n    # the semantic splitter uses our embedding model to group semantically related sentences together\n    semantic_parser = SemanticSplitterNodeParser(embed_model=embed_model)\n    \n    # we set up MongoDB as our document and vector database\n    vector_store = MongoDBAtlasVectorSearch(\n       pymongo.MongoClient(os.getenv('MONGO_URI')),\n       db_name=\"fireParse\",\n       collection_name=\"llamaIndexDocs\",\n       index_name=\"llama_docs_index\"\n    )\n    \n    #finally we use LlamaIndex's pipeline to string this all together\n    pipeline = IngestionPipeline(\n       transformations=[\n           semantic_parser, #can replace with text_splitter\n           embed_model,\n       ],\n       vector_store=vector_store,\n    )\n\nOnce we have everything set up, we can create documents from URLs in MongoDB!\nBelow is an example of using three URLs but we used hundreds.\n\n    \n    \n    example_urls = [\n       \"https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook\",\n       \"https://docs.llamaindex.ai/en/stable/examples/cookbooks/anthropic_haiku/\",\n      \"https://docs.llamaindex.ai/en/stable/examples/vector_stores/MongoDBAtlasVectorSearch/\"\n    ]\n    \n    # read in the documents and pass them through our pipeline\n    documents = LlamaDocsPageReader(html_to_text=True).load_data(example_urls)\n    pipeline.run(documents=documents, show_progress=True)\n\nYou can see in MongoDB how our documents have text, embedding (with 768\ndimensions), and metadata.\n\nExample document in MongoDB Atlas that resulted from the pipeline\n\n##  MongoDB Atlas for vector search\n\nMongoDB Atlas was our go-to for storing both the documentation text and the\nembeddings themselves. It\u2019s incredibly versatile! Setting up vector search\nwithin Atlas is a breeze, allowing us to quickly find the most relevant\ndocument chunks. Plus, LlamaIndex\u2019s metadata parsing played perfectly with\nAtlas \u2014 we could easily filter results based on things like document source or\ntopic.\n\n**Setting up Vector Search:** It was remarkably simple! We just specified\nthese few things:\n\n  * Path to the embedding field within our documents. \n  * Embedding dimension size. \n  * Similarity metric (e.g., cosine similarity). \n  * That it\u2019s a vector index. \n\n**Filtering Power (Optional):** For even finer control, we could add paths to\nfields we wanted to filter our searches by (like the company\u2019s name).\n\nWhether you\u2019re building a complex web app or a quick Streamlit prototype,\nLlamaIndex ChatEngines have you covered. They effortlessly manage conversation\nhistory, let you perform lightning-fast vector searches, and unlock a whole\nsuite of powerful tools.\n\nWe built our ChatEngine directly from our trusty MongoDB index. This\nintegration was surprisingly simple:\n\n    \n    \n    def get_index():\n       logger.info(\"Connecting to index from MongoDB...\")\n       store = MongoDBAtlasVectorSearch(\n           db_name=os.environ[\"MONGODB_DATABASE\"],\n           collection_name=os.environ[\"MONGODB_VECTORS\"],\n           index_name=os.environ[\"MONGODB_VECTOR_INDEX\"],\n       )\n       index = VectorStoreIndex.from_vector_store(store)\n       logger.info(\"Finished connecting to index from MongoDB.\")\n       return index\n    \n    index = get_index()\n    index.as_chat_engine(\n        llm = Fireworks(\n                 api_key=env_vars['FIREWORKS_API_KEY'],\n                 model=\"accounts/fireworks/models/mixtral-8x22b-instruct\" #Can be changed out for Llama3\n                 )\n        chat_mode=\"best\", \n        context_prompt=(\n               \"\"\" You are a software developer bot that is an expert at reading over documentation to answer questions.\n               Use the relevant documents for context:\n               {context_str}\n               \\nInstruction: Use the previous chat history, or the context above, to interact and help the user.\n               \"\"\"\n               ),\n        verbose=True\n        )\n\n##  ` create-llama ` : from idea to app in record time\n\nWe were seriously impressed by Create-Llama. Usually, building a full-stack\napp takes time, but Create-Llama had us up and running in under 15 minutes!\nAll we did was point it towards our vector database and give a few basic\ndetails. Honestly, it made development a joy! [ This blog post goes into more\ndetail about how to use create-llama. ](https://www.llamaindex.ai/blog/create-\nllama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191)\n\nThe create-llama setup screen  The create-llama app, customized and ready to\ngo\n\n##  Deployment: Render and Vercel\n\nTo make LlamaWorksDB production-ready and easily accessible, we turned to [\nRender ](https://render.com/) and [ Vercel ](https://vercel.com/) . Render was\na perfect fit for our Python FastAPI backend, as it focuses on ease of\ndeployment and scalability. Vercel seamlessly handled our Next.js frontend \u2014\nwe loved its developer-centric approach and the effortless build process. Both\nplatforms made deployment a breeze, letting us focus on coding rather than\ncomplex infrastructure setup.\n\n##  Future directions\n\nOur hackathon success is just the beginning. We envision LlamaWorksDB evolving\ninto a powerhouse for developers seeking answers within technical\ndocumentation. Here\u2019s how we see it growing:\n\n  * **Enhanced Retrieval:** We\u2019re excited to experiment with LlamaIndex\u2019s powerful capabilities like MultiVectorSearch to further refine our results. Integrating different LLMs will open up new possibilities for how LlamaWorksDB understands and interacts with technical content. \n  * **A Focus on Documentation:** We want to double down on making LlamaWorksDB the ultimate tool for navigating documentation. This means exploring specialized techniques and tools designed specifically for understanding complex technical information. \n\nLlamaWorksDB is an open-source project in Beta, and we believe in the power of\ncollaboration! If you\u2019re passionate about AI-powered documentation tools, we\ninvite you to:\n\n  * **Try it out:** Explore our [ GitHub repo ](https://github.com/clab2024/clab/tree/main) and give [ LlamaWorksDB a spin ](https://clab-ui.vercel.app/) ! \n  * **Contribute:** Help us build new features, test integrations, and refine our search capabilities. \n  * **Share your feedback:** Let us know how we can make LlamaWorksDB even better. \n\nTogether, let\u2019s revolutionize the way developers interact with documentation!\n\nExplore our project and join the innovation: [\nhttps://github.com/clab2024/clab/ ](https://github.com/clab2024/clab/)\n\n[ https://clab-ui.vercel.app/ ](https://clab-ui.vercel.app/) (front-end)\n(leverages free credits responds late)\n\n[ https://clab.onrender.com/docs ](https://clab.onrender.com/docs) (back-end)\n\n##  Meet Team CLAB!\n\n  * [ **Chris Wood** ](https://www.linkedin.com/in/chris-wood-b9282a22a/) : Up-and-coming tech whiz, ready to graduate with valuable insights from his internship at Tutello. \n  * [ **Leo Walker** ](https://www.linkedin.com/in/leowalker/) : Data Scientist with the discipline and precision of a Military Veteran. \n  * [ **Andrew Townsend** ](https://www.linkedin.com/in/andrew-g-townsend/) : A Computer Science graduate from SJSU, bringing fresh academic perspectives. \n  * [ **Barath Subramaniam** ](https://www.linkedin.com/in/barathsubramaniam/) : The strategic mind behind Product Security AI and Data engineering at Adobe. Twitter: [ @baraths84 ](https://twitter.com/baraths84)\n\nTeam CLAB (plus Laurie)\n\n", "mimetype": "text/plain", "start_char_idx": 1059, "end_char_idx": 10772, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a037525-5661-45c9-8432-78b91891cd1b": {"__data__": {"id_": "3a037525-5661-45c9-8432-78b91891cd1b", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-04-23.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-23", "date": "Apr 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-23"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92099854-6b11-439b-a990-718a7476119a", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-04-23.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-23", "date": "Apr 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-23"}, "hash": "6c0dc8f521b07d23eb131721f0b246b77c37c8ecdfd31cac5944b95dd5c676f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d683169e-0629-495a-9182-d6a60c242eb7", "node_type": "1", "metadata": {}, "hash": "5b8d1900cf9bc9b689febf8a2ae836374ff64d500b9ca9d608b8a92130b2e673", "class_name": "RelatedNodeInfo"}}, "text": "Hello LlamaIndex Community!\n\nWelcome to another thrilling weekly update from LlamaWorld! We're excited to\nbring you a variety of outstanding updates, including Cookbooks, demos,\nguides, and tutorials.\n\n##  **The highlights:**\n\n  * **MistralAI's 8x22b Model Cookbook:** Released cookbook for MistralAI's 8x22b model with detailed guidance on RAG, query routing, and tool applications. [ Docs ](https://docs.llamaindex.ai/en/latest/examples/cookbooks/mistralai/) , [ Tweet ](https://x.com/llama_index/status/1780646484712788085) . \n  * **Llama 3 Model Cookbook:** A comprehensive cookbook for Meta's Llama 3 model from simple prompt runs to complex RAG pipeline, agents and tools, accessible directly from Hugging Face. [ Docs ](https://docs.llamaindex.ai/en/latest/examples/cookbooks/llama3_cookbook/) , [ Tweet ](https://x.com/llama_index/status/1781039161325293981) . \n  * **create-llama Llama 3 template** : create-llama template for Meta's Llama 3 to quickly start building full-stack LLM applications using the **` nextjs-llama3 ` ** template with a single CLI command. [ Tweet ](https://x.com/jerryjliu0/status/1781843300938666050) . \n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1141, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d683169e-0629-495a-9182-d6a60c242eb7": {"__data__": {"id_": "d683169e-0629-495a-9182-d6a60c242eb7", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-04-23.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-23", "date": "Apr 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-23"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92099854-6b11-439b-a990-718a7476119a", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-04-23.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-23", "date": "Apr 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-23"}, "hash": "6c0dc8f521b07d23eb131721f0b246b77c37c8ecdfd31cac5944b95dd5c676f7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a037525-5661-45c9-8432-78b91891cd1b", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-04-23.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-23", "date": "Apr 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-23"}, "hash": "9c3e1b51f026eb416ded701fac0c652a4915a99bb367d5faba8fafe87ea4e530", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b925097b-b92e-4a5f-911a-2c3ea81e6bd0", "node_type": "1", "metadata": {}, "hash": "a3adc65742331d0f4046d76ff247bc47b5faaf3123e74a35efade50f107f32f0", "class_name": "RelatedNodeInfo"}}, "text": "##  **Feature Releases and Enhancements:**\n\n  1. We have released a cookbook for the latest MistralAI model, the powerful 8x22b, which sets a new standard for open models. The cookbook covers RAG, query routing, and tool use cases. [ Docs ](https://docs.llamaindex.ai/en/latest/examples/cookbooks/mistralai/) , [ Tweet ](https://x.com/llama_index/status/1780646484712788085) . \n  2. We have released a cookbook for latest Meta's new Llama 3 model, available directly from Hugging Face. This guide covers everything from running basic prompts to setting up a full RAG pipeline, agents and tools. [ Docs ](https://docs.llamaindex.ai/en/latest/examples/cookbooks/llama3_cookbook/) , [ Tweet ](https://x.com/llama_index/status/1781039161325293981) . \n  3. We have introduced a template for integrating Meta's Llama 3 in create-llama. Simply run **` npx create-llama ` ** and select the **` nextjs-llama3 ` ** template to build full-stack LLM application with Llama 3 in one CLI command. [ Tweet ](https://x.com/jerryjliu0/status/1781843300938666050) . \n\n##  **Demos:**\n\n  * [ Open Source AI Diagram Generator ](https://github.com/rsrohan99/ai-diagram-generator) by [ Rohan ](https://twitter.com/clusteredbytes) using LlamaIndex's Pydantic program with partial JSON parsing and Vercel AI SDK to generate and stream diagrams dynamically for an enhanced user experience. \n  * [ DREAM ](https://github.com/aishwaryaprabhat/goku/tree/main/goku/dream) : A Distributed RAG Experimentation Framework by Aishwarya Prabhat, featuring a full-stack blueprint for optimizing RAG setups in a distributed environment. This setup includes Ray for computing, LlamaIndex for advanced techniques, Ragas for synthetic data, MinIO, MLflow, Project Jupyter, and ArgoCD. \n  ", "mimetype": "text/plain", "start_char_idx": 1141, "end_char_idx": 2888, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b925097b-b92e-4a5f-911a-2c3ea81e6bd0": {"__data__": {"id_": "b925097b-b92e-4a5f-911a-2c3ea81e6bd0", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-04-23.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-23", "date": "Apr 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-23"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92099854-6b11-439b-a990-718a7476119a", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-04-23.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-23", "date": "Apr 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-23"}, "hash": "6c0dc8f521b07d23eb131721f0b246b77c37c8ecdfd31cac5944b95dd5c676f7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d683169e-0629-495a-9182-d6a60c242eb7", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-04-23.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-23", "date": "Apr 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-23"}, "hash": "3235fed77e6421d07c70675ea1417e17bfc8ba91a5020a23b6fb31bc0005a0d9", "class_name": "RelatedNodeInfo"}}, "text": "* [ Firecrawl ](https://firecrawl.dev/?ref=github) from [ Mendable ](https://www.mendable.ai/) is an API service that crawls a given URL and converts its content, including all accessible subpages, into clean markdown format. It utilizes LlamaParse from LlamaIndex for PDF parsing. \n\n##  **Guides:**\n\n  * [ Guide ](https://qdrant.tech/documentation/examples/hybrid-search-llamaindex-jinaai/) to integrating Qdrant Hybrid Cloud with LlamaIndex, featuring JinaAI embeddings, MistralAI's Mixtral 8x7b, and our LlamaParse document parser. \n  * [ Guide ](https://www.elastic.co/search-labs/blog/rag-with-llamaIndex-and-elasticsearch) to building RAG using completely open and free components from Elastic, featuring Ollama and MistralAI, demonstrates how to assemble a RAG application with LlamaIndex using entirely free software. \n  * [ Guide ](https://www.youtube.com/watch?v=JLmI0GJuGlY) to Building a Code-Writing Agent: [ TechWithTimm ](https://twitter.com/TechWithTimm) demonstrated how to create an agent that writes code by reading your documentation. Learn how to set up local LLMs with Ollama, parse documentation using LlamaParse, build an agent, and teach it to write code. \n  * [ Guide ](https://medium.com/@diagnosta/lora-fine-tuning-of-embedding-models-using-llamaindex-a60b823a2c94) to Fine-tuning Embedding Models for RAG with LoRA by Mariboo demonstrates how to enhance Hugging Face models using LlamaIndex's finetuning techniques, including steps from quantization to fine-tuning with QLoRA. \n\n##  **Tutorials:**\n\n  * Khye Wei's [ tutorial ](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/advanced-rag-with-azure-ai-search-and-llamaindex/ba-p/4115007) from Microsoft demonstrates how to use LlamaIndex with Azure's AI Search to create powerful RAG applications, including Hybrid Search, Query Rewriting, and SubQuestionQuery Engine. \n  * [ Hanane Dupouy ](https://www.linkedin.com/in/hanane-d-algo-trader/) 's [ tutorial ](https://www.linkedin.com/posts/hanane-d-algo-trader_react-financial-agent-llamaindex-activity-7186333474256035840-jyQV/?utm_source=share&utm_medium=member_desktop) on Building a Finance Agent with LlamaIndex to query public companies with tools for looking up stock prices, summarizing financial news, and plotting stock data, all streamlined through LlamaIndex's ReAct agent and API abstractions. \n  * [ Andy Singal ](https://twitter.com/andysingal) 's [ tutorial ](https://ai.gopubby.com/enhancing-document-retrieval-with-memory-a-tutorial-for-llamaindex-with-colbert-based-agent-1c3c47461122) on Building a ColBERT-powered Retrieval Agent with Memory demonstrates how to enhance a RAG pipeline with \"state\" storage for a more personalized, conversational assistant using LlamaIndex's custom agent and query pipeline abstractions. \n  * Mariboo\u2019s [ tutorial ](https://medium.com/@diagnosta/lora-fine-tuning-of-embedding-models-using-llamaindex-a60b823a2c94) on Fine-tuning Embedding Models for RAG with LoRA using LlamaIndex's finetuning abstractions. \n\n", "mimetype": "text/plain", "start_char_idx": 2888, "end_char_idx": 5898, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e404499-d155-4e89-a838-240a6641b0da": {"__data__": {"id_": "5e404499-d155-4e89-a838-240a6641b0da", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-04-16.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-16", "date": "Apr 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-16"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4aed660-228a-454e-9d8b-f0402edd3d6b", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-04-16.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-16", "date": "Apr 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-16"}, "hash": "04f291380d28e084701074435528db1c91eab94e637a2c2b31dccf71ff4ff805", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5360d1d2-2df1-4e9b-8fa7-1bb19441bdeb", "node_type": "1", "metadata": {}, "hash": "b3cc3dd33f8d15b4d60017bf169eb1146d90e94869984b149fc07609dfa64ac8", "class_name": "RelatedNodeInfo"}}, "text": "Hello, LlamaIndex Family!\n\nWelcome to another thrilling weekly update from LlamaGalaxy! We're excited to\nbring you a variety of outstanding updates, including the Chain of Abstraction\nLlamaPack, create-tsi, demos, guides, tutorials, and much more.\n\nBefore we delve into these updates, we have an exciting tutorial series on\nAgents and Tools for you to check out. Perfect for beginners, this series\ncovers everything from advanced QA/RAG implementations to step-wise execution.\nBy the end, you\u2019ll have gained a deeper understanding of how to use agent\nreasoning with tool use to build simple applications. Check them out:\n\n  1. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 627, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5360d1d2-2df1-4e9b-8fa7-1bb19441bdeb": {"__data__": {"id_": "5360d1d2-2df1-4e9b-8fa7-1bb19441bdeb", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-04-16.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-16", "date": "Apr 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-16"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4aed660-228a-454e-9d8b-f0402edd3d6b", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-04-16.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-16", "date": "Apr 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-16"}, "hash": "04f291380d28e084701074435528db1c91eab94e637a2c2b31dccf71ff4ff805", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e404499-d155-4e89-a838-240a6641b0da", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-04-16.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-16", "date": "Apr 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-16"}, "hash": "6346cfb1b01ad26cebd924a7804345b5e7d13e0dcc348be2dcbf2cde0ca4ded5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1b4dbee-ecbb-4ee8-a407-844e22b629f5", "node_type": "1", "metadata": {}, "hash": "ed0550ad8690c8632bc303c8a02ed27a681da9925feab8aa658b05051a61f7f8", "class_name": "RelatedNodeInfo"}}, "text": "[ Overview ](https://www.youtube.com/watch?v=-AuHlVMyEA0)\n  2. [ ReAct agents ](https://youtu.be/pRUc6JPw6CY)\n  3. [ Function Calling agents ](https://youtu.be/6INvyrC4WrA)\n  4. [ Retrieval-Augmented agent ](https://youtu.be/K7h17Jjtbzg)\n  5. [ Controlling tool outputs ](https://youtu.be/gFRbkRtLGZQ)\n  6. [ Agents with step-by-step execution ](https://youtu.be/JGkSxdPFgyQ)\n\n##  **The highlights:**\n\n  * **Chain of Abstraction LlamaPack:** Chain of Abstraction technique as llamapack a method enabling multi-step reasoning for enhanced tool use introduced by Silin Gao's team. [ LlamaPack ](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/agent/coa_agent.ipynb) , [ Tweet ](https://x.com/llama_index/status/1778845258119524640) . \n  * **Create-tsi Toolkit:** Launched a toolkit for building full-stack RAG applications with customizable features like web crawling, local file indexing, and multilingual support, all hosted in EU data centers. [ Code ](https://github.com/telekom/create-tsi) , [ Tweet ](https://x.com/llama_index/status/1778812761893650551) . \n  * **Improved Agent Control** : **` return_direct ` ** feature in tools allows direct output returns, reducing costs and enhancing response efficiency. [ Docs ](https://docs.llamaindex.ai/en/latest/examples/agent/return_direct_agent/) , [ Tweet ](https://x.com/llama_index/status/1778072285003550932) . \n\n##  **Feature Releases and Enhancements:**\n\n  1. We have introduced the Chain of Abstraction Technique Developed by Silin Gao, and team as LlamaPack, this new method enables LLMs to generate multi-step reasoning chains for efficient sequence planning, enhancing tool use beyond single-shot functions. [ LlamaPack ](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/agent/coa_agent.ipynb) , [ Tweet ](https://x.com/llama_index/status/1778845258119524640) . \n  ", "mimetype": "text/plain", "start_char_idx": 627, "end_char_idx": 2496, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1b4dbee-ecbb-4ee8-a407-844e22b629f5": {"__data__": {"id_": "e1b4dbee-ecbb-4ee8-a407-844e22b629f5", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-04-16.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-16", "date": "Apr 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-16"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4aed660-228a-454e-9d8b-f0402edd3d6b", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-04-16.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-16", "date": "Apr 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-16"}, "hash": "04f291380d28e084701074435528db1c91eab94e637a2c2b31dccf71ff4ff805", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5360d1d2-2df1-4e9b-8fa7-1bb19441bdeb", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-04-16.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-16", "date": "Apr 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-16"}, "hash": "f84477e745abb3e6d83258aa8a47a8366f5777a43d951ae31fdcc0eb7efd3986", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6c71f0a-ce7f-4b32-88e0-c96ff56daf5b", "node_type": "1", "metadata": {}, "hash": "0ef797f5d37b9ff3c2154200b86bc06e6b7cccab9a999268eee45114bb797c3d", "class_name": "RelatedNodeInfo"}}, "text": "2. We have launched create-tsi: A toolkit in collaboration with T-Systems and Marcus Schiesser to generate GDPR-compliant, full-stack AI applications via a CLI interface. Build enterprise-grade RAG bots with customizable features like web crawling, local file indexing, and multilingual support, all hosted in EU data centers. [ Code ](https://github.com/telekom/create-tsi) , [ Tweet ](https://x.com/llama_index/status/1778812761893650551) . \n  ", "mimetype": "text/plain", "start_char_idx": 2496, "end_char_idx": 2942, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6c71f0a-ce7f-4b32-88e0-c96ff56daf5b": {"__data__": {"id_": "a6c71f0a-ce7f-4b32-88e0-c96ff56daf5b", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-04-16.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-16", "date": "Apr 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-16"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4aed660-228a-454e-9d8b-f0402edd3d6b", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-04-16.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-16", "date": "Apr 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-16"}, "hash": "04f291380d28e084701074435528db1c91eab94e637a2c2b31dccf71ff4ff805", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1b4dbee-ecbb-4ee8-a407-844e22b629f5", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-04-16.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-16", "date": "Apr 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-16"}, "hash": "65963947a363f2a3fd183d89c377f0334fc6ed3d6534d162bc9cfdd2c90eae13", "class_name": "RelatedNodeInfo"}}, "text": "3. We have introduced **` return_direct ` ** feature in tools that enhances agent controllability by allowing direct output returns as final responses. This optimizes for reduced latency and costs, and effectively halts the agent after crucial actions like booking confirmations or answering queries. [ Docs ](https://docs.llamaindex.ai/en/latest/examples/agent/return_direct_agent/) , [ Tweet ](https://x.com/llama_index/status/1778072285003550932) . \n\n##  **Demos:**\n\n  * [ RAG-enhanced MetaGPT ](https://x.com/llama_index/status/1777851305308102845) : A robust multi-agent framework that features structured team dynamics for problem-solving, now supercharged with domain-specific knowledge from LlamaIndex modules. This framework supports diverse data inputs, sophisticated retrieval options, and efficient data management for enhanced agent performance. \n\n##  **Guides:**\n\n  * [ Guide ](https://towardsdatascience.com/mastering-rag-systems-from-fundamentals-to-advanced-with-strategic-component-evaluation-3551be31858f) to Building and Evaluating Advanced RAG by Hamza Gharbi for setting up a basic RAG pipeline, defining custom evaluation functions, and optimizing retrieval techniques. \n  * [ Paper ](https://arxiv.org/abs/2403.11996) by Prof. [ Markus J. Buehler ](https://twitter.com/ProfBuehlerMIT) : Using LLM-Generated Knowledge Graphs to Accelerate Biomaterials Discovery - This study showcases how a comprehensive knowledge graph from over 1000 scientific papers reveals novel insights and connections, driving innovation in biomaterials through art as inspiration. The KG construction was done with the help of LlamaIndex modules. \n  * [ Guide ](https://aws.plainenglish.io/rag-implementation-using-aws-bedrock-and-llamaindex-62b346fd0156) to Full-Stack RAG Application with AWS Bedrock: Set up Bedrock embeddings, use LlamaIndex for PDF retrieval, and build an interactive Streamlit interface, an ideal resource for enterprises starting with AWS services. \n  * [ Guide ](https://docs.llamaindex.ai/en/latest/examples/pipeline/query_pipeline_memory/) to Building a Lightweight ColBERT Retrieval Agent: Learn how to create an agent capable of advanced document retrieval and maintaining conversation memory, without the complexity of heavyweight agent frameworks. \n  * [ Guide ](https://arxiv.org/pdf/2404.01037.pdf) to the Best RAG Techniques: 'ARAGOG' [ paper ](https://arxiv.org/pdf/2404.01037.pdf) by Matous Eibich is a comprehensive evaluation survey exploring various RAG methods from classic vector databases to LlamaIndex's advanced techniques. Key findings highlight the effectiveness of HyDE, LLM reranking, and sentence window retrieval for improving precision and answer similarity. \n\n##  **Tutorials:**\n\n  * [ Akash Mathur ](https://akash-mathur.medium.com/) \u2019s [ tutorial ](https://akash-mathur.medium.com/data-management-in-llamaindex-smart-tracking-and-debugging-of-document-changes-7b81c304382b) on Data Management in LlamaIndex: Featuring LlamaCloud and its open-source counterpart, this tutorial showcases efficient live data handling to minimize costs and latency in LLM applications. \n  * [ Leonie ](https://twitter.com/helloiamleonie) \u2019s interactive [ tutorial ](https://lightning.ai/weaviate/studios/chat-with-your-code-rag-with-weaviate-and-llamaindex) to create an app that lets you converse with code from a GitHub repository. \n  * [ kingzzm\u2019s ](https://twitter.com/kingzzm) [ tutorial ](https://generativeai.pub/advanced-rag-retrieval-strategies-auto-merging-retrieval-dc3f869654c4) on enhancing RAG Performance to overcome the issue of 'broken' context in RAG construction by dynamically creating contiguous chunks with auto-merging retrieval. \n  * [ Activeloop ](https://twitter.com/activeloop) \u2019s [ tutorial ](https://www.activeloop.ai/resources/ai-pill-identifier/) on Multimodal RAG for Pill Search teaches how to identify pills using images and text. This helps in identifying unknown pills, checking drug interactions and side effects, and confirming proper dosage amounts. \n  * Fanghua Yu's [ tutorial ](https://medium.com/@yu-joshua/using-llamaparse-for-knowledge-graph-creation-from-documents-3bd1e1849754) on using LlamaParse for Knowledge Graph Creation from Documents. \n\n##  **Webinars:**\n\n  * [ Webinar ](https://www.youtube.com/watch?v=pira_p6aRVA) with [ Tianjun Zhang ](https://twitter.com/tianjun_zhang) and [ Shishir Patil ](https://twitter.com/shishirpatil_) , the two lead co-authors of RAFT, where they presented RAFT and discussed fine-tuning and RAG. \n\n", "mimetype": "text/plain", "start_char_idx": 2942, "end_char_idx": 7460, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10c0cd5c-1f7b-4960-a65a-e7381e7468f0": {"__data__": {"id_": "10c0cd5c-1f7b-4960-a65a-e7381e7468f0", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-04-09.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-09", "date": "Apr 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-09"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8da54c0-1c6e-4205-bfa1-e94c535666ad", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-04-09.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-09", "date": "Apr 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-09"}, "hash": "4f3182c6be11ffc107dc8a2602c2ab7dea74b53483eb7474cbc79836bead6cb8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26947e47-2847-463c-b177-33acb517f78b", "node_type": "1", "metadata": {}, "hash": "d5a0b1a3f185b822ed29a0a9f8883ed119d9fe218de5c8649fb0590140bd0746", "class_name": "RelatedNodeInfo"}}, "text": "Hello, LlamaIndex members!\n\nWelcome to another thrilling weekly update from LlamaUniverse! We're excited\nto present a variety of outstanding updates, including Anthropic's Function\nCalling, Cookbooks, RankLLM, Guides, Tutorials, and much more.\n\n##  **The highlights:**\n\n  * **Anthropic's Claude Function Calling Agent:** Enhance QA/RAG and workflow automation with advanced tool calling in an agent framework. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/agent/anthropic_agent.ipynb) , [ Tweet ](https://x.com/llama_index/status/1776051869850476840) . \n  * **RankLLM Integration:** RankLLM is an open-source LLM collection for reranking, surpassing GPT-4 based alternatives is now integrated with LlamaIndex. [ Notebook ](https://docs.llamaindex.ai/en/latest/examples/node_postprocessor/rankLLM/) , [ Tweet ](https://x.com/llama_index/status/1775166279911186930) . \n  * **LlamaIndex + MistralAI Cookbook Series:** Launched a cookbook series with MistralAI for building diverse RAG applications, from basic to advanced, with distinctive methods and abstractions. [ Cookbooks ](https://github.com/mistralai/cookbook/tree/main/third_party/LlamaIndex) , [ Tweet ](https://x.com/llama_index/status/1775977013054259210)\n\n##  **Feature Releases and Enhancements:**\n\n  1. We have introduced the Anthropic\u2019s Claude Function Calling Agent, leveraging advanced tool calling capabilities within an agent framework for enhanced QA/RAG and workflow automation. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/agent/anthropic_agent.ipynb) , [ Tweet ](https://x.com/llama_index/status/1776051869850476840) . \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1668, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26947e47-2847-463c-b177-33acb517f78b": {"__data__": {"id_": "26947e47-2847-463c-b177-33acb517f78b", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-04-09.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-09", "date": "Apr 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-09"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8da54c0-1c6e-4205-bfa1-e94c535666ad", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-04-09.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-09", "date": "Apr 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-09"}, "hash": "4f3182c6be11ffc107dc8a2602c2ab7dea74b53483eb7474cbc79836bead6cb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10c0cd5c-1f7b-4960-a65a-e7381e7468f0", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-04-09.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-09", "date": "Apr 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-09"}, "hash": "5c890380fabb38612171a542792a76fbb4007bf67b982ae059b0b0d11f20f845", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a0c7415-674c-4615-adc9-30878caf52ca", "node_type": "1", "metadata": {}, "hash": "911274c791f4890298c84a3e3853aa3347f02ac27914c59d1d93076a4bf73835", "class_name": "RelatedNodeInfo"}}, "text": "2. RankLLM (by [ Ronak Pradeep ](https://twitter.com/rpradeep42) ) integration with LlamaIndex - an open-source LLM collection fine-tuned for reranking, offering top-notch results and outperforming GPT-4 based rerankers. [ Notebook ](https://docs.llamaindex.ai/en/latest/examples/node_postprocessor/rankLLM/) , [ Tweet ](https://x.com/llama_index/status/1775166279911186930) . \n  ", "mimetype": "text/plain", "start_char_idx": 1668, "end_char_idx": 2048, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a0c7415-674c-4615-adc9-30878caf52ca": {"__data__": {"id_": "3a0c7415-674c-4615-adc9-30878caf52ca", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-04-09.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-09", "date": "Apr 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-09"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8da54c0-1c6e-4205-bfa1-e94c535666ad", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-04-09.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-09", "date": "Apr 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-09"}, "hash": "4f3182c6be11ffc107dc8a2602c2ab7dea74b53483eb7474cbc79836bead6cb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26947e47-2847-463c-b177-33acb517f78b", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-04-09.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-09", "date": "Apr 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-09"}, "hash": "7cba123b10f7878b0563f317f55bd58d51565b43bafb4e3b17a674a1587b941e", "class_name": "RelatedNodeInfo"}}, "text": "3. We have launched the LlamaIndex + MistralAI Cookbook Series for creating a range of RAG applications, from simple setups to advanced agents, featuring unique abstractions and techniques. [ Cookbooks ](https://github.com/mistralai/cookbook/tree/main/third_party/LlamaIndex) , [ Tweet ](https://x.com/llama_index/status/1775977013054259210)\n  4. We launched create-llama for building full-stack RAG/agent applications with a single CLI command, akin to create-react-app, for a comprehensive chatbot setup including tool use. [ Tweet ](https://x.com/llama_index/status/1776615916102463775) . \n\n##  **Demos:**\n\n  * [ AutoRAG ](https://github.com/Marker-Inc-Korea/AutoRAG) by Marker-Inc-Korea: Streamline RAG pipeline optimization with an automated three-step process, from data preparation to evaluation and optimal pipeline adoption, enhancing the efficiency of the RAG pipeline using LlamaIndex. \n\n##  **Guides:**\n\n  * [ Guide ](https://docs.llamaindex.ai/en/stable/examples/vector_stores/KDBAI_Advanced_RAG_Demo/) to Building Advanced RAG with Temporal Filters: Learn how to enhance your RAG pipeline with time-based metadata for more effective financial report analysis using LlamaIndex and [ KDB.AI ](http://KDB.AI) vector store. \n  * [ Guide ](https://github.com/mistralai/cookbook/blob/main/third_party/LlamaIndex/Adaptive_RAG.ipynb) to Adaptive RAG for dynamically selecting RAG strategies based on query complexity, enhancing efficiency across varying question types. \n\n##  **Tutorials:**\n\n  * **[ (\u03bbx.x)eranga ](https://twitter.com/lambdaEranga) \u2019s ** [ tutorial ](https://medium.com/rahasak/build-rag-application-using-a-llm-running-on-local-computer-with-ollama-and-llamaindex-97703153db20) on the step-by-step process for building RAG with local models (LlamaIndex, Ollama, HuggingFace Embeddings, ChromaDB) and wrapping it all in a Flask server. \n  * [ Ivan Ilin ](https://twitter.com/ivanilin9) \u2019s [ video tutorial ](https://www.youtube.com/watch?v=9QqNVxlySGE) on [ iki.ai ](https://t.co/fHXUEcF7sZ) \\- an LLM-powered digital library, for organizing, and sharing information within teams or organizations. \n  * [ Tutorial ](https://www.koyeb.com/tutorials/use-llamaindex-to-build-a-retrieval-augmented-generation-rag-application#introduction) on scaling LLM Applications with Koyeb on deploying a full-stack RAG application globally without infrastructure setup, using Koyeb, LlamaIndex.TS, and TogetherAI. \n  * [ Ankush Singal ](https://twitter.com/andysingal) 's [ tutorial ](https://ai.gopubby.com/unlocking-the-power-of-multi-document-agents-with-llamaindex-d09e4d7dfe0e) on Building Multi-Document Agents with LlamaIndex covers advanced multi-document agent concepts, where documents serve as sub-agents enabling complex QA, semantic search, and summarization. \n  * [ Rohan ](https://twitter.com/clusteredbytes) \u2019s [ tutorial ](https://github.com/rsrohan99/rag-stream-intermediate-events-tutorial) on building a Full-Stack RAG application that streams intermediate results to visual UI components with event queues and server-side events. \n  * [ Hanane Dupouy ](https://www.linkedin.com/in/hanane-d-algo-trader/) 's [ tutorial ](https://www.linkedin.com/posts/hanane-d-algo-trader_financial-agent-with-llamaindex-activity-7181531880146513920-mf9e/?utm_source=share&utm_medium=member_desktop) on building a Finance Agent using an LLM with Yahoo Finance and LlamaIndex abstractions to analyze financial data for publicly traded companies, covering everything from balance sheets to stock recommendations. \n\n##  **Webinars:**\n\n  * [ Webinar ](https://www.youtube.com/watch?v=YY0VaSjPV1Y) with [ Daniel Huynh ](https://www.linkedin.com/in/dhuynh95/) ****featuring LaVague - an agent that can navigate the web in your Jupyter/Colab notebook. \n  * [ Webinar ](https://www.youtube.com/watch?v=t_xhaa-iTNw) with [ Logan Kelly ](https://www.linkedin.com/in/logankkelly/) featuring [ CallSine ](https://www.callsine.com/) that utilizes LlamaIndex abstractions and LLMs for personalized sales outreach. \n\n", "mimetype": "text/plain", "start_char_idx": 2048, "end_char_idx": 6062, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a529084c-9523-4372-8ff2-f6dd9f5b6978": {"__data__": {"id_": "a529084c-9523-4372-8ff2-f6dd9f5b6978", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-04-02.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-02", "date": "Apr 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-02"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "586dbf6c-0124-4c92-afea-b93b6368d319", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-04-02.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-02", "date": "Apr 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-02"}, "hash": "a41a086f02ff976c08c20befc2ddb51347f2a29c1879b428c49d3cc7dbd2971f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "128c4ac9-7ee6-4378-a57b-a3323fc3af70", "node_type": "1", "metadata": {}, "hash": "87c202288395dbd2c486c0584fabc425a38f4e9c5f753530d216c15ee896e352", "class_name": "RelatedNodeInfo"}}, "text": "Greetings, LlamaIndex community!\n\nWelcome to another exciting weekly update from LlamaGalaxy! We're thrilled to\nshare a range of fantastic updates with you, including the introduction of\nRAFT LlamaPack, enhanced memory and cost efficiency in RAG with Cohere's\nembeddings, and much more.\n\n###  **The highlights:**\n\n  1. **DeepLearningAI Course:** JavaScript RAG Web Apps with LlamaIndex collaborative course with DeepLearningAI. [ Course ](https://www.deeplearning.ai/short-courses/javascript-rag-web-apps-with-llamaindex/) , [ Tweet ](https://x.com/AndrewYNg/status/1773006786058219889?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 597, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "128c4ac9-7ee6-4378-a57b-a3323fc3af70": {"__data__": {"id_": "128c4ac9-7ee6-4378-a57b-a3323fc3af70", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-04-02.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-02", "date": "Apr 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-02"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "586dbf6c-0124-4c92-afea-b93b6368d319", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-04-02.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-02", "date": "Apr 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-02"}, "hash": "a41a086f02ff976c08c20befc2ddb51347f2a29c1879b428c49d3cc7dbd2971f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a529084c-9523-4372-8ff2-f6dd9f5b6978", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-04-02.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-02", "date": "Apr 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-02"}, "hash": "f91331300eeb32cc9d4eb5ab099af4070a76852a3b2acc2e2a427a0134c27601", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50877f86-fc75-4faf-abc9-6936bcd6b5fe", "node_type": "1", "metadata": {}, "hash": "45e7449446bb23d373a1df2b87b56f59abfd2d6dcb54004f5392399e3f7e6a24", "class_name": "RelatedNodeInfo"}}, "text": "2. **RAFTDatasetPack LlamaPack** : Introduced RAFTDatasetPack for dataset generation using RAFT - Retrieval Augmented Fine Tuning for training models to differentiate between relevant 'oracle' documents and 'distractor' documents. [ LlamaPack ](https://github.com/run-llama/llama_index/tree/main/llama-index-packs/llama-index-packs-raft-dataset) , [ Tweet ](https://x.com/llama_index/status/1772662480210198809?s=20) . \n  3. **Memory Efficiency with Cohere Embeddings:** Utilize Cohere's Int8 and binary embeddings for cost-effective and low-memory RAG operations. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/cohere_retriever_eval.ipynb) , [ Tweet ](https://x.com/llama_index/status/1773402379016138955?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 597, "end_char_idx": 1360, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50877f86-fc75-4faf-abc9-6936bcd6b5fe": {"__data__": {"id_": "50877f86-fc75-4faf-abc9-6936bcd6b5fe", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-04-02.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-02", "date": "Apr 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-02"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "586dbf6c-0124-4c92-afea-b93b6368d319", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-04-02.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-02", "date": "Apr 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-02"}, "hash": "a41a086f02ff976c08c20befc2ddb51347f2a29c1879b428c49d3cc7dbd2971f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "128c4ac9-7ee6-4378-a57b-a3323fc3af70", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-04-02.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-02", "date": "Apr 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-02"}, "hash": "5095c74d2c94ddc545d8f9539cdeb26366aa1657b9298c94dec16515ae0cdb6a", "class_name": "RelatedNodeInfo"}}, "text": "4. **Python Docs Makeover:** Revamped Python documentation with accessible example notebooks, advanced search, and comprehensive API details. [ API Ref ](https://docs.llamaindex.ai/en/stable/api_reference/) , [ Tweet ](https://x.com/llama_index/status/1772355240299520083?s=20) , [ Docs ](https://t.co/BS7oDqZ7qW)\n\n###  **Feature Releases and Enhancements:**\n\n  1. We introduced RAFT - Retrieval Augmented Fine Tuning, a method from [ Tianjun Zhang ](https://www.linkedin.com/in/tianjun-zhang-333bb2126/overlay/about-this-profile/?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base%3B1dQCZFffT4aXk6ePSYdUYg%3D%3D) and [ Shishir Patil ](https://www.linkedin.com/in/shishir-patil/overlay/about-this-profile/?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base%3BNG6wPCQHTaWKxcdltRvvjw%3D%3D) to enhance domain-specific RAG performance in LLMs. By training models to differentiate between relevant 'oracle' documents and 'distractor' documents, RAFT improves context understanding. Try it out with our new RAFTDatasetPack LlamaPack for dataset generation. [ LlamaPack ](https://github.com/run-llama/llama_index/tree/main/llama-index-packs/llama-index-packs-raft-dataset) , [ Tweet ](https://x.com/llama_index/status/1772662480210198809?s=20) . \n  2. We collaborated with DeepLearningAI for a course that goes beyond teaching RAG techniques; it guides you on integrating RAG into a full-stack application. Learn to construct a backend API, develop an interactive React component, and tackle the unique challenges of deploying RAG on a server rather than just in a notebook. [ Course ](https://www.deeplearning.ai/short-courses/javascript-rag-web-apps-with-llamaindex/) , [ Tweet ](https://x.com/AndrewYNg/status/1773006786058219889?s=20) . \n  3. We integrated with Cohere's Int8 and Binary Embeddings for a memory-efficient solution for your RAG pipeline. This addresses the high memory usage and costs associated with large dataset operations in RAG. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/cohere_retriever_eval.ipynb) , [ Tweet ](https://x.com/llama_index/status/1773402379016138955?s=20)\n  4. We launched revamped Python docs with top-level example notebooks, improved search with previews, and overhauled API documentation. [ API Ref ](https://docs.llamaindex.ai/en/stable/api_reference/) , [ Tweet ](https://x.com/llama_index/status/1772355240299520083?s=20) , [ Docs ](https://t.co/BS7oDqZ7qW)\n\n###  **Demos:**\n\n  * [ RestAI ](https://x.com/llama_index/status/1774159755675898010?s=20) , a project by [ Pedro Dias ](https://twitter.com/pedromdias) is a nifty platform that offers RAG, advanced text-to-SQL, and multimodal inference as a service with a nifty UI. \n  * [ Ragdoll ](https://github.com/bennyschmidt/ragdoll) and [ Ragdoll Studio ](https://github.com/bennyschmidt/ragdoll-studio) by bennyschmidt: Create AI Personas for characters, web assistants, or game NPCs using LlamaIndex TS, local LLMs, and image generation with Ollama and StabilityAI. \n\n###  **Guides:**\n\n  * [ Guide ](https://towardsdatascience.com/designing-rags-dbb9a7c1d729) to Designing RAG Systems by [ Micha\u0142 Oleszak ](https://michaloleszak.medium.com/) for an in-depth look at crucial design decisions in building efficient RAG systems, spanning five key areas: Indexing, Storing, Retrieval, Synthesis, and Evaluation. \n\n###  **Tutorials:**\n\n  * [ Sujit Patil ](https://twitter.com/palsujit) [ tutorial ](https://sujitpal.blogspot.com/2024/03/hierarchical-and-other-indexes-using.html) on combining semantic chunking with hierarchical clustering and indexing for RAG content enrichment. \n  * Florian June's [ tutorial ](https://ai.gopubby.com/advanced-rag-08-self-rag-c0c5b5952e0e) on crafting a dynamic RAG system with integrated reflection, a guide to building Self-RAG from scratch. \n  * Laurie's [ video tutorial ](https://x.com/llama_index/status/1773783011785585141?s=20) on using LlamaParse's LLM-powered parsing turns complex insurance policies into clear yes-or-no statements, improving LLM responses on coverage queries. \n  * [ Akriti\u2019s ](https://twitter.com/AkritiUpadhya13) [ tutorial ](https://medium.com/@akriti.upadhyay/building-real-time-financial-news-rag-chatbot-with-gemini-and-qdrant-64c0a3fbe45b) on Building Real-Time Financial News RAG Chatbot with Gemini, and Qdrant. \n  * Marco Bertelli's [ tutorial ](https://python.plainenglish.io/deploying-a-production-ready-rag-server-a-comprehensive-guide-with-llamaindex-dbe57cc960df) on deploying a RAG server for real-time use, and covering efficient embedding serving, concurrent request handling, and failure resilience. \n  * [ Sudarshan Koirala\u2019s ](https://twitter.com/mesudarshan) [ tutorial ](https://www.youtube.com/watch?v=wCFXae8hiYA) on building advanced PDF RAG with LlamaParse and purely local models for embedding, LLMs, and reranking. \n\n###  **Webinars:**\n\n  * [ Register for a webinar ](https://lu.ma/v1bdat63) with [ Tianjun Zhang ](https://www.linkedin.com/in/tianjun-zhang-333bb2126/overlay/about-this-profile/?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base%3B1dQCZFffT4aXk6ePSYdUYg%3D%3D) and [ Shishir Patil ](https://www.linkedin.com/in/shishir-patil/overlay/about-this-profile/?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base%3BNG6wPCQHTaWKxcdltRvvjw%3D%3D) on how to do retrieval-augmented fine-tuning (RAFT). \n  * [ Webinar ](https://www.youtube.com/watch?v=TeEX7CoHT9k) with [ Daniel ](https://twitter.com/dani_avila7) on [ CodeGPT ](https://codegpt.co/) \\- a platform for AI Copilots that help your coding workflows, with components built on top of LlamaIndex components. \n  * [ Vectara\u2019s ](https://twitter.com/vectara?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor) [ Panel Discussion ](https://www.youtube.com/watch?v=R5pddHfUThQ&t=351s) on 'Why RAG will Never Die?\u2019. \n\n", "mimetype": "text/plain", "start_char_idx": 1360, "end_char_idx": 7180, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89097361-fe0d-4ed9-9e41-9a65c664b4a5": {"__data__": {"id_": "89097361-fe0d-4ed9-9e41-9a65c664b4a5", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-03-26.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-26", "date": "Mar 26, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-26"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ae3c90e6-c392-4877-8f3a-cd210ee29352", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-03-26.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-26", "date": "Mar 26, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-26"}, "hash": "0705828bdd355770acdff0cabb6234a8f0a860827cceb08d078763979f366c2a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3e6ec6d-644d-48b2-aaf4-4d06e2661f5d", "node_type": "1", "metadata": {}, "hash": "e86555db6596c76e1640a8d67e34781bedc13972884a185846d99dede2d980ef", "class_name": "RelatedNodeInfo"}}, "text": "Hi there, LlamaIndex followers!\n\nWelcome to another thrilling weekly update from the LlamaUniverse. We're\nexcited to bring you a fantastic array of updates, including Privacy-\nPreserving In-Context Learning with LlamaPacks and RAG Networks. Dive into our\nguides on MistralAI, explore Gemma LLMs, and enjoy a plethora of engaging\ntutorials using LlamaIndex, alongside upcoming webinars and events.\n\n####  **The highlights:**\n\n  1. **Privacy-Preserving In-Context Learning:** Leveraging **[ Xinyu Tang ](https://twitter.com/XinyuTang7) \u2019s ** [ paper ](https://arxiv.org/abs/2309.11765) , we've introduced LlamaPack for LLM/RAG apps, enabling the creation of few-shot demonstrations that maintain privacy and data integrity. [ LlamaPack ](https://llamahub.ai/l/llama-packs/llama-index-packs-diff-private-simple-dataset?from=llama-packs) , [ Tweet ](https://x.com/llama_index/status/1770837291855991085?s=20) . \n  2. **Privacy-Preserving RAG Network:** We present Privacy-Preserving RAG Network which facilitates the use of confidential datasets in healthcare and online platforms while safeguarding user privacy. [ Blogpost ](https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network) , [ Tweet ](https://x.com/llama_index/status/1770966231769854076?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1280, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3e6ec6d-644d-48b2-aaf4-4d06e2661f5d": {"__data__": {"id_": "a3e6ec6d-644d-48b2-aaf4-4d06e2661f5d", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-03-26.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-26", "date": "Mar 26, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-26"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ae3c90e6-c392-4877-8f3a-cd210ee29352", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-03-26.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-26", "date": "Mar 26, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-26"}, "hash": "0705828bdd355770acdff0cabb6234a8f0a860827cceb08d078763979f366c2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89097361-fe0d-4ed9-9e41-9a65c664b4a5", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-03-26.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-26", "date": "Mar 26, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-26"}, "hash": "659b4a564b9af814dcb825a41ada5c934025ab25c6051f9448cdacf1cb149499", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f07df39-1b58-4c24-a551-2cf41b6e3f4c", "node_type": "1", "metadata": {}, "hash": "c6c8c7abbdd04edd14ce385b6470ff2bfdc83295e6dcc37df7300f942121b33b", "class_name": "RelatedNodeInfo"}}, "text": "3. **Advanced RAG and Agents with MistralAI:** [ Guide ](https://docs.google.com/presentation/d/1dbfoxzNcoI-D45RKZfO1UfBJIr4v0YtHhj1cwuCj020/edit#slide=id.p) on using MistralAI with LlamaIndex and LlamaParse, advancing RAG capabilities and agent development through custom pipelines and sophisticated parsing. \n\n####  **Feature Releases and Enhancements:**\n\n  * We launched a LlamaPack based on **[ Xinyu Tang ](https://twitter.com/XinyuTang7) \u2019s ** [ paper ](https://arxiv.org/abs/2309.11765) ****for secure in-context learning in LLM/RAG apps, focusing on generating few-shot demonstrations from private datasets with differential privacy, ensuring the synthetic examples reflect the data distribution without exposing sensitive details. [ LlamaPack ](https://llamahub.ai/l/llama-packs/llama-index-packs-diff-private-simple-dataset?from=llama-packs) , [ Tweet ](https://x.com/llama_index/status/1770837291855991085?s=20) . \n  * We introduced a privacy-preserving RAG network by [ Andrei ](https://www.linkedin.com/in/nerdai/) in LlamaIndex, enabling the use of sensitive datasets like healthcare and online user data without compromising individual privacy. This approach allows data providers to synthetically generate and share data for RAG queries securely. [ Blogpost ](https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network) , [ Tweet ](https://x.com/llama_index/status/1770966231769854076?s=20) . \n  * We introduce a template by [ **Sasha** ](https://twitter.com/hackgoofer) for agent-human interaction in RAG implementations, focusing on minimal human input. It triggers human intervention only for vague or malformed queries, enhancing clarity and precision in the response process. ", "mimetype": "text/plain", "start_char_idx": 1280, "end_char_idx": 2998, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f07df39-1b58-4c24-a551-2cf41b6e3f4c": {"__data__": {"id_": "8f07df39-1b58-4c24-a551-2cf41b6e3f4c", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-03-26.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-26", "date": "Mar 26, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-26"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ae3c90e6-c392-4877-8f3a-cd210ee29352", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-03-26.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-26", "date": "Mar 26, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-26"}, "hash": "0705828bdd355770acdff0cabb6234a8f0a860827cceb08d078763979f366c2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3e6ec6d-644d-48b2-aaf4-4d06e2661f5d", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-03-26.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-26", "date": "Mar 26, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-26"}, "hash": "97f33af17b21b9398edd9b56c4d0e148b16019fa3dad334c68ad80298829d127", "class_name": "RelatedNodeInfo"}}, "text": "[ LlamaPack ](https://llamahub.ai/l/llama-packs/llama-index-packs-query-understanding-agent?from=llama-packs) , [ Tweet ](https://x.com/llama_index/status/1771207903439159404?s=20) . \n  * [ BAM Elevate ](https://twitter.com/BAMelevate) integrated Databricks Vector Search into LlamaIndex, enabling vector search capabilities within the Databricks ecosystem. [ Blogpost ](https://www.bamelevate.com/news/llamaindex-and-databricks-integration-announcement) , [ Tweet ](https://x.com/llama_index/status/1770585400840699974?s=20) . \n  * We launched LlamaParse integration with LlamaIndex TypeScript, an industry-leading parser for PDFs and various document types accessible directly from JS/TS. Utilize the create-llama command-line tool or integrate LlamaParse directly into your app for enhanced document processing. [ Example ](https://github.com/run-llama/LlamaIndexTS/blob/main/examples/readers/src/llamaparse.ts) , [ Tweet ](https://x.com/llama_index/status/1770496142020895159?s=20) . \n\n####  **Guides:**\n\n  * [ Guide ](https://docs.google.com/presentation/d/1dbfoxzNcoI-D45RKZfO1UfBJIr4v0YtHhj1cwuCj020/edit#slide=id.p) to Advanced RAG and Agents with MistralAI using LlamaIndex and LlamaParse to construct sophisticated RAG and agents, including custom query pipelines, document parsing, and reference applications. \n  * [ Guide ](https://www.kaggle.com/code/iamleonie/advanced-rag-with-gemma-weaviate-and-llamaindex) to Integrating Custom Models with LlamaIndex: [ Leonie Monigatti ](https://twitter.com/helloiamleonie) demonstrates the process of incorporating your custom model, like Gemma, into LlamaIndex \n  * [ Guide ](https://www.llamaindex.ai/blog/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai) to combat prompt injection attacks, like the \"white text\" attack, by rigorously screening data during ingestion and retrieval, ensuring the integrity of LLM-powered systems against deceptive manipulations by [ Oleksandr Yaremchuk ](https://twitter.com/alex_yaremchuk) from [ Protect AI ](https://twitter.com/ProtectAICorp) . \n\n####  **Tutorials:**\n\n  * [ Akriti Upadhyay ](https://twitter.com/AkritiUpadhya13) \u2019s [ tutorial ](https://medium.com/@akriti.upadhyay/integrating-llamaindex-and-qdrant-similarity-search-for-patient-record-retrieval-7090e77b971e) to prototype on patient data safely, featuring synthetic dataset generation, storage in Qdrant Vector DB, and querying with llama.cpp LLM using LlamaIndex. \n  * [ Frank Baele ](https://twitter.com/BaeleFrank) \u2019s [ tutorial ](https://franz.be/blogpost/bringing-a-naive-rag-to-production) on developing a production-grade RAG pipeline with LlamaParse, detailing document parsing, advanced ingestion techniques, Vector DB selection, and insights on evaluation, deployment, and budget management. \n  * [ Video tutorial ](https://www.youtube.com/watch?v=xwfR8fC_Azs) by Ashish on creating an advanced PDF RAG agent, utilizing LlamaParse for text and tables extraction, defining retrievers and routers, and adding a sub-question layer, all integrated with LlamaIndex and MistralAI. \n  * [ UpTrain ](https://twitter.com/UpTrainAI) [ tutorial ](https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations) on Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations. \n  * [ Ravi Theja ](https://twitter.com/ravithejads) [ tutorial ](https://ravidesetty.medium.com/introducing-navarasa-2-0-indic-gemma-7b-2b-instruction-tuned-model-on-15-indian-languages-31f6565b2750) on showcasing RAG with LlamaIndex on 15 Indian languages using Navarasa-2.0 - a Gemma finetuned model on 15 Indian languages. \n\n####  **Webinars:**\n\n[ Register for a webinar ](https://lu.ma/z2vhi06e) with [ **Daniel Huynh**\n](https://twitter.com/dhuynh95) featuring LaVague, an agent that can navigate\nthe web in your Jupyter/Colab notebook.\n\n####  Events:\n\n  * [ Join us ](https://x.com/lablabai/status/1770847744313241637?s=20) for a Panel discussion on 'Why RAG Will Never Die - The Context Window Myth\u2019 with panelists from LlamaIndex, Vectara, Nvidia, and TogetherAI. \n  * We are hosting a RAG [ meetup ](https://www.meetup.com/paris-retrieval-augmented-generation-group/events/299374545/) in Paris on March 27th featuring talks on advanced RAG strategies, building a RAG CLI, and the significance of open-source RAG in business. \n\n", "mimetype": "text/plain", "start_char_idx": 2998, "end_char_idx": 7314, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2343ae5-961d-41ce-8767-9ce5d4df3134": {"__data__": {"id_": "a2343ae5-961d-41ce-8767-9ce5d4df3134", "embedding": null, "metadata": {"filename": "secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.md", "extension": ".md", "title": "Secure RAG with LlamaIndex and LLM Guard by Protect AI", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "285ce5de-c5d9-41ee-b830-567ecace871a", "node_type": "4", "metadata": {"filename": "secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.md", "extension": ".md", "title": "Secure RAG with LlamaIndex and LLM Guard by Protect AI", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai"}, "hash": "2f45e06923ed901a30c0fa57d3ce6ee85fc35e12f61ece228b8945d8cdb75a7b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a7ae56f-18c7-41c0-aefb-20b649267ee9", "node_type": "1", "metadata": {}, "hash": "5d5ce8c2c9f5dc622f412efc6b9808e0fcd6d6fda4fedf6d91211d718381f932", "class_name": "RelatedNodeInfo"}}, "text": "_This is a guest post by Protect AI._\n\nWe believe that RAG will be one of the preferred approaches for enterprises\nwhen developing LLM applications to generate prompt responses that are more\nrelevant, and accurate, tailored to and based on company-specific content.\nHowever, while analyzing web pages with ChatGPT may leave the LLM vulnerable\nto injections embedded within the webpage, it is crucial to recognize that\ninjections may also be concealed within the vector database or knowledge graph\nwhere data is retrieved and injected into the LLM.\n\nThat is why we\u2019re thrilled to describe how LLM Guard by Protect AI can secure\nyour data sources accessed for context in your LLM application, built with\nLlamaIndex.\n\n[ LLM Guard ](https://llm-guard.com/) is an open source solution by [ Protect\nAI ](https://protectai.com/llm-guard) designed to fortify the security of\nLarge Language Models ( [ LLMs\n](https://www.helpnetsecurity.com/2023/05/10/security-privacy-risks-large-\nlanguage-models-video/) ). It is designed for easy integration and deployment\nin production environments. It provides extensive security scanners for both\nprompts and responses of LLMs to detect, redact, and sanitize against\nadversarial prompt attacks, data leakage, and integrity breaches (e.g.\noffensive content, hallucination).\n\nLLM Guard was built for a straightforward purpose: despite the potential of\nLLMs, corporate adoption has been hesitant. This reluctance stems from the\nsignificant security risks and a lack of control and observability of\nimplementing these technologies. With over 2.5M downloads of its models, and a\nGoogle Patch Reward, LLM Guard is the open source standard and market leader\nin LLM security at inference.\n\n**Secure RAG with LlamaIndex**\n\nIn the following [ example ](https://llm-\nguard.com/usage/notebooks/llama_index_rag/) , we showcase a practical approach\nto improve the security of your RAG application. Specifically, we will explore\na RAG application designed to facilitate the automated screening of candidate\nCVs by HR teams. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2040, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a7ae56f-18c7-41c0-aefb-20b649267ee9": {"__data__": {"id_": "0a7ae56f-18c7-41c0-aefb-20b649267ee9", "embedding": null, "metadata": {"filename": "secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.md", "extension": ".md", "title": "Secure RAG with LlamaIndex and LLM Guard by Protect AI", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "285ce5de-c5d9-41ee-b830-567ecace871a", "node_type": "4", "metadata": {"filename": "secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.md", "extension": ".md", "title": "Secure RAG with LlamaIndex and LLM Guard by Protect AI", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai"}, "hash": "2f45e06923ed901a30c0fa57d3ce6ee85fc35e12f61ece228b8945d8cdb75a7b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2343ae5-961d-41ce-8767-9ce5d4df3134", "node_type": "1", "metadata": {"filename": "secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.md", "extension": ".md", "title": "Secure RAG with LlamaIndex and LLM Guard by Protect AI", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai"}, "hash": "cf26e42cb3a947e96fb7bb97793d815f5c59586c0b505b9255f2dfcb7a53ce0d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bda59a28-dc2a-4ac8-9c0e-89079449c61c", "node_type": "1", "metadata": {}, "hash": "dae8761758586f552272b47b0b0a684f96bab31dafa87b07d03275eee73a11b5", "class_name": "RelatedNodeInfo"}}, "text": "Within the batch of CVs, there exists a diverse pool of\ncandidates, including one who lacks experience and consequently is not the\nmost suitable candidate. The nature of the attack manifests as an embedded\nprompt injection within the CV of this particular candidate, concealed in\nwhite text, rendering it challenging to detect with the naked eye.\n\n[ In the notebook example ](https://llm-\nguard.com/usage/notebooks/llama_index_rag/) , we conducted the attack\ninitially and then repeated the process subsequent to fortifying the\napplication with LLM Guard. With this example, we show how you can use LLM\nGuard and LlamaIndex for both input and output scanning of documents to detect\nany malicious content. Although ideally, we should scan documents before the [\ningestion\n](https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/transformations.html)\n, for simplicity in the example, we chose to do scanning during [ retrieval\n](https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/root.html)\n. In real-use cases, it's critical to do the scanning both during retrieval of\nreal-time data from APIs (not vector stores) which we still need to verify as\nit can contain poisoned sources of information. For output scanning, it can\nsimply be done by taking the results generated by LlamaIndex and running them\nthrough LLM Guard.\n\n    \n    \n    llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1, output_parser=output_parser)\n    \n    service_context = ServiceContext.from_defaults(\n        llm=llm, \n        transformations=transformations,\n        callback_manager=callback_manager,\n    )\n    index = VectorStoreIndex.from_documents(\n        documents, service_context=service_context\n    )\n    \n    input_scanners = [\n        Anonymize(vault, entity_types=[\"PERSON\", \"EMAIL_ADDRESS\", \"EMAIL_ADDRESS_RE\", \"PHONE_NUMBER\"]), \n        Toxicity(), \n        PromptInjection(),\n        Secrets()\n    ]\n    \n    llm_guard_postprocessor = LLMGuardNodePostProcessor(\n        scanners=input_scanners,\n        fail_fast=False,\n        skip_scanners=[\"Anonymize\"],\n    )\n    \n    query_engine = index.as_query_engine(\n        similarity_top_k=3,\n        node_postprocessors=[llm_guard_postprocessor]\n    )\n    response = query_engine.query(\"I am screening candidates for adult caregiving opportunity. Please recommend me an experienced person. Return just a name\")\n    print(str(response))\n    \n\n**LLM Guard protects your LLM applications**\n\nAs demonstrated in the practical example of securing an HR screening\napplication with [ LLM Guard ](https://llm-guard.com/) , the significance of\nmitigating potential attacks, cannot be overstated. Besides that, as LLMs\nevolve rapidly and embed advanced capabilities like agency and multi-modality,\nthe complexity and impact of potential breaches escalate significantly. Thus,\nprioritizing RAG security becomes not just a necessity but rather fundamental\nin safeguarding against increasingly sophisticated threats and ensuring the\nintegrity of critical enterprise LLM applications.\n\nTry out LLM Guard by going to our [ library\n](https://github.com/protectai/llm-guard) or [ documentation ](https://llm-\nguard.com/) . ", "mimetype": "text/plain", "start_char_idx": 2040, "end_char_idx": 5232, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bda59a28-dc2a-4ac8-9c0e-89079449c61c": {"__data__": {"id_": "bda59a28-dc2a-4ac8-9c0e-89079449c61c", "embedding": null, "metadata": {"filename": "secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.md", "extension": ".md", "title": "Secure RAG with LlamaIndex and LLM Guard by Protect AI", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "285ce5de-c5d9-41ee-b830-567ecace871a", "node_type": "4", "metadata": {"filename": "secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.md", "extension": ".md", "title": "Secure RAG with LlamaIndex and LLM Guard by Protect AI", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai"}, "hash": "2f45e06923ed901a30c0fa57d3ce6ee85fc35e12f61ece228b8945d8cdb75a7b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a7ae56f-18c7-41c0-aefb-20b649267ee9", "node_type": "1", "metadata": {"filename": "secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.md", "extension": ".md", "title": "Secure RAG with LlamaIndex and LLM Guard by Protect AI", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai"}, "hash": "e5484cca65077544035d185a21391cc80638049777bf141c0cf6df38318f0227", "class_name": "RelatedNodeInfo"}}, "text": "Also, [ join our Slack\n](https://join.slack.com/t/laiyerai/shared_invite/zt-28jv3ci39-sVxXrLs3rQdaN3mIl9IT~w)\nchannel for any questions!\n\n", "mimetype": "text/plain", "start_char_idx": 5232, "end_char_idx": 5370, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67aab263-dfeb-4c2f-8b99-b98488533e09": {"__data__": {"id_": "67aab263-dfeb-4c2f-8b99-b98488533e09", "embedding": null, "metadata": {"filename": "retrieving-privacy-safe-documents-over-a-network.md", "extension": ".md", "title": "Retrieving Privacy-Safe Documents Over A Network", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a862e693-ee31-4336-8f5d-fee03192c4d0", "node_type": "4", "metadata": {"filename": "retrieving-privacy-safe-documents-over-a-network.md", "extension": ".md", "title": "Retrieving Privacy-Safe Documents Over A Network", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network"}, "hash": "d5d8487e5d253849b5641b7fa079148c99e372d6c5dd34474d391e213c5f0bd4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b296474e-5d28-4b9a-a11c-953298ebc5f3", "node_type": "1", "metadata": {}, "hash": "2d6c89c213b13ac34590eb999d0d722860e09938b8c366a310deef87953ef6dc", "class_name": "RelatedNodeInfo"}}, "text": "In a [ recent blog post ](https://www.llamaindex.ai/blog/querying-a-network-\nof-knowledge-with-llama-index-networks-d784b4c3006f) , we introduced our `\nllama-index-networks ` library extension that makes it possible to build a\nnetwork of RAG systems, which users can query. The benefits of such a network\nare clear: connecting to a diverse set of knowledge stores\u2014that one may not\notherwise have access to\u2014means more accurate responses to an even wider\nbreadth of queries.\n\nA main caveat to these networks though is that the data being shared across\nthe network ought to be privacy safe. In this blog post, we demonstrate how to\nturn private, sensitive data into privacy-safe versions that can be\nsubsequently and safely shared across a network. To do so, we\u2019ll be relying on\nsome recent developments in the area of Privacy-Enhancing Techniques.\n\n##  The story of Alex, Bob and Beth continues\n\nTo illustrate all of this, we will again make use of our three made-up\ncharacters Alex, Bob and Beth. As a quick reminder, Alex is a data consumer\nwho wants to access the data sources that Bob and Beth possess and are willing\nto supply.\n\nWe showed then how such data a collaboration could be permitted through `\nllama-index-networks ` by taking the following steps:\n\n  1. Bob and Beth both build their respective QueryEngine\u2019s (RAG in llama-index lingo) \n  2. Bob and Beth both expose their QueryEngine behind a ContributorService \n  3. Alex builds a NetworkQueryEngine that connects to Bob and Beth\u2019s ContributorService\u2019s \n\nIn part two of this story, we add the wrinkle that Bob and Beth possess\nprivate, sensitive data that must be carefully protected before to sharing to\nAlex. Or, put in another way, we need to add a step 0. to the above steps\nwhich applies protective measures to the private datasets.\n\nMeasures for protecting data (or more specifically the data subjects) depends\non the use-case factors such as what the data involves and how its intended to\nbe shared and ultimately processed. De-anonymizing techniques such as wiping\nPII (i.e., personal identifiable indicators) are often applied. However, in\nthis blog post we highlight another privacy-enhancing technique called\nDifferential Privacy.\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2207, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b296474e-5d28-4b9a-a11c-953298ebc5f3": {"__data__": {"id_": "b296474e-5d28-4b9a-a11c-953298ebc5f3", "embedding": null, "metadata": {"filename": "retrieving-privacy-safe-documents-over-a-network.md", "extension": ".md", "title": "Retrieving Privacy-Safe Documents Over A Network", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a862e693-ee31-4336-8f5d-fee03192c4d0", "node_type": "4", "metadata": {"filename": "retrieving-privacy-safe-documents-over-a-network.md", "extension": ".md", "title": "Retrieving Privacy-Safe Documents Over A Network", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network"}, "hash": "d5d8487e5d253849b5641b7fa079148c99e372d6c5dd34474d391e213c5f0bd4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67aab263-dfeb-4c2f-8b99-b98488533e09", "node_type": "1", "metadata": {"filename": "retrieving-privacy-safe-documents-over-a-network.md", "extension": ".md", "title": "Retrieving Privacy-Safe Documents Over A Network", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network"}, "hash": "28ca5e6129a721f540a64eddea95d088fcfd021e63c2fa61e5e70470545ed890", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c9e8377-b523-4ab7-b60b-44f3f01c1032", "node_type": "1", "metadata": {}, "hash": "a69ffa5c165501c6af97bcdb68b10c6a602936d2fae7e2f0c7d27d57d9236ad6", "class_name": "RelatedNodeInfo"}}, "text": "Part 2: of Alex, Bob and Beth. This time Bob and Beth have sensitive data that\nthey want to share, but can\u2019t unless protective measures are applied before\nsharing across the network.\n\nPart 2: of Alex, Bob and Beth. ", "mimetype": "text/plain", "start_char_idx": 2207, "end_char_idx": 2422, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c9e8377-b523-4ab7-b60b-44f3f01c1032": {"__data__": {"id_": "6c9e8377-b523-4ab7-b60b-44f3f01c1032", "embedding": null, "metadata": {"filename": "retrieving-privacy-safe-documents-over-a-network.md", "extension": ".md", "title": "Retrieving Privacy-Safe Documents Over A Network", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a862e693-ee31-4336-8f5d-fee03192c4d0", "node_type": "4", "metadata": {"filename": "retrieving-privacy-safe-documents-over-a-network.md", "extension": ".md", "title": "Retrieving Privacy-Safe Documents Over A Network", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network"}, "hash": "d5d8487e5d253849b5641b7fa079148c99e372d6c5dd34474d391e213c5f0bd4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b296474e-5d28-4b9a-a11c-953298ebc5f3", "node_type": "1", "metadata": {"filename": "retrieving-privacy-safe-documents-over-a-network.md", "extension": ".md", "title": "Retrieving Privacy-Safe Documents Over A Network", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network"}, "hash": "772a27c3c38265065ba4200355c3d9a42142e965e8c5ce03d241b400c4a34f0b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbacc335-cd04-4b6e-b3e1-526672cf5bda", "node_type": "1", "metadata": {}, "hash": "70664e18b70596a2140499ce0b0e050fa08988702b3068da7b12929071bc02eb", "class_name": "RelatedNodeInfo"}}, "text": "This time Bob and Beth have sensitive data that\nthey want to share, but can\u2019t unless protective measures are applied before\nsharing across the network.\n\n###  Sidebar: differential privacy primer\n\nIn short, differential privacy is a method that provides mathematical\nguarantees (up to a certain level of chance) that an adversary would not be\nable to learn that a specific individual belonged to a private dataset after\nonly seeing the output of running this private dataset through a protected\ndata processing step. In other words, an individual\u2019s inclusion in the private\ndataset cannot be learned from the output of a differentially-private\nalgorithm.\n\nBy protecting against the threat of dataset inclusion, we mitigate the risk\nthat an adversary is able to link the private data with their external sources\nto learn more about the data subject and potentially cause more privacy harms\n(such as distortion).\n\nA light introduction to differential privacy.\n\n", "mimetype": "text/plain", "start_char_idx": 2422, "end_char_idx": 3380, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fbacc335-cd04-4b6e-b3e1-526672cf5bda": {"__data__": {"id_": "fbacc335-cd04-4b6e-b3e1-526672cf5bda", "embedding": null, "metadata": {"filename": "retrieving-privacy-safe-documents-over-a-network.md", "extension": ".md", "title": "Retrieving Privacy-Safe Documents Over A Network", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a862e693-ee31-4336-8f5d-fee03192c4d0", "node_type": "4", "metadata": {"filename": "retrieving-privacy-safe-documents-over-a-network.md", "extension": ".md", "title": "Retrieving Privacy-Safe Documents Over A Network", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network"}, "hash": "d5d8487e5d253849b5641b7fa079148c99e372d6c5dd34474d391e213c5f0bd4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c9e8377-b523-4ab7-b60b-44f3f01c1032", "node_type": "1", "metadata": {"filename": "retrieving-privacy-safe-documents-over-a-network.md", "extension": ".md", "title": "Retrieving Privacy-Safe Documents Over A Network", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network"}, "hash": "bb5cc59de96d411cf23b86b0d45d0e54b2e0e65b758115054d88f1f9e4192e1b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af8f5c03-5619-46e8-9b44-5086fcd51851", "node_type": "1", "metadata": {}, "hash": "e288f7ec726012cfd4ee015e2d42c017963a32fdb61646907bccc8effbd51b50", "class_name": "RelatedNodeInfo"}}, "text": "A light introduction to differential privacy.\n\nComing back to the story of Alex, Bob and Beth, in order to protect Bob and\nBeth\u2019s data, we will make use of an algorithm that uses a pre-trained LLM to\ncreate synthetic copies of private data that satisfies the differential\nprivate mathematical guarantees. This algorithm was introduced in the paper\nentitled \u201cPrivacy-preserving in-context learning with differentially private\nfew-shot generation\u201d by Xinyu Tang et al., which appeared in ICLR 2024. It is\nthe synthetic copies that we can use to share across the network!\n\nThere we have it, the added privacy wrinkle and our differentially privacy\napproach means that we have to take the following steps to facilitate this\ndata collaboration.\n\n  1. Bob and Beth create privacy-safe synthetic copies of their private datasets \n  2. Bob and Beth both build their respective QueryEngine\u2019s over their synthetic datasets \n  3. Bob and Beth both expose their QueryEngine behind a ContributorService \n  4. Alex builds a NetworkQueryEngine that connects to Bob and Beth\u2019s ContributorService\u2019s \n\n###  Creating differentially private synthetic copies of a private dataset\n\nFortunately, for step 0., we can make use of the ` DiffPrivateSimpleDataset `\npack.\n\n    \n    \n    from llama_index.core.llama_datasets.simple import LabelledSimpleDataset\n    from llama_index.packs.diff_private_simple_dataset.base import PromptBundle\n    from llama_index.packs.diff_private_simple_dataset import DiffPrivateSimpleDatasetPack\n    from llama_index.llms.openai import OpenAI\n    import tiktoken\n    \n    # Beth uses `DiffPrivateSimpleDatasetPack` to generate synthetic copies\n    \n    llm = OpenAI(\n        model=\"gpt-3.5-turbo-instruct\",\n        max_tokens=1,\n        logprobs=True,\n        top_logprobs=5,  # OpenAI only allows for top 5 next token\n    )                    # as opposed to entire vocabulary\n    tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo-instruct\")\n    \n    beth_private_dataset: LabelledSimpleDataset = ... # a dataset that contains\n    \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# examples with two attributes\n    \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# `text` and `reference_label`\n    \n    beth_synthetic_generator = DiffPrivateSimpleDatasetPack(\n        llm=llm,\n        tokenizer=tokenizer,\n        prompt_bundle=prompt_bundle,    # params for preparing required prompts\n        simple_dataset=simple_dataset,  # to generate the synthetic examples \n    )\n    \n    beth_synthetic_dataset = await beth_synthetic_generator.arun(\n    \t\tsize=3,  # number of synthetic observations to create\n    \t\tsigma=0.5  # param that determines the level of privacy\n    )\n\nWith the synthetic dataset in hand, Bob and Beth can apply the steps\nintroduced in our previous post to build their privacy-safe QueryEngine. It\u2019s\nworthwhile to mention here that as mentioned by the authors of the paper, the\nsynthetic copies can be used as many times as one would like in a downstream\ntask and it would incur no additional privacy cost! ", "mimetype": "text/plain", "start_char_idx": 3380, "end_char_idx": 6370, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "af8f5c03-5619-46e8-9b44-5086fcd51851": {"__data__": {"id_": "af8f5c03-5619-46e8-9b44-5086fcd51851", "embedding": null, "metadata": {"filename": "retrieving-privacy-safe-documents-over-a-network.md", "extension": ".md", "title": "Retrieving Privacy-Safe Documents Over A Network", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a862e693-ee31-4336-8f5d-fee03192c4d0", "node_type": "4", "metadata": {"filename": "retrieving-privacy-safe-documents-over-a-network.md", "extension": ".md", "title": "Retrieving Privacy-Safe Documents Over A Network", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network"}, "hash": "d5d8487e5d253849b5641b7fa079148c99e372d6c5dd34474d391e213c5f0bd4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fbacc335-cd04-4b6e-b3e1-526672cf5bda", "node_type": "1", "metadata": {"filename": "retrieving-privacy-safe-documents-over-a-network.md", "extension": ".md", "title": "Retrieving Privacy-Safe Documents Over A Network", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network"}, "hash": "4aa51d132cc8c3757ebf5cb2ba8d2d9fe3cf622b1f7cd3223a6529f478a41e67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a422e3c5-9810-47e9-ba7f-c6ba9a1c72b0", "node_type": "1", "metadata": {}, "hash": "355ecaf93391e035e16ff72dd39243c32f1ff411357040728173de090f0f76d4", "class_name": "RelatedNodeInfo"}}, "text": "(This is due to the post-\nprocessing property of differential privacy.)\n\n##  Example: Symptom2Disease\n\nIn this section of the blog post, we go over an actual example application of\nthe privacy-safe networks over the [ Symptom2Disease\n](https://www.kaggle.com/datasets/niyarrbarman/symptom2disease/data) dataset.\nThis dataset consists of 1,200 examples each containing a \u201csymptoms\u201d\ndescription as well as the associated \u201cdisease\u201d label \u2014 the dataset contains\nobservations for 24 distinct disease labels. We split the dataset into two\ndisjoint subsets, one for training and the other for testing. Moreover, we\nconsider this original dataset to be private, requiring protective measures\nbefore being shared across a network.\n\n###  Generate privacy-safe synthetic observations of Symptom2Disease\n\nWe use the training subset and apply the ` DiffPrivateSimpleDatasetPack ` on\nit in order to generate privacy-safe, synthetic observations. But in order to\ndo so, we first need to turn the raw Symptom2Disease dataset into a `\nLabelledSimpleDataset ` object.\n\n    \n    \n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    from llama_index.core.llama_dataset.simple import (\n        LabelledSimpleDataExample,\n        LabelledSimpleDataset,\n    )\n    from llama_index.core.llama_dataset.base import CreatedBy, CreatedByType\n    \n    # load the Symptom2Disease.csv file\n    df = pd.read_csv(\"Symptom2Disease.csv\")\n    train, test = train_test_split(df, test_size=0.2)\n    \n    # create a LabelledSimpleDataset (which is what the pack works with)\n    examples = []\n    for index, row in df.iterrows():\n        example = LabelledSimpleDataExample(\n            reference_label=row[\"label\"],\n            text=row[\"text\"],\n            text_by=CreatedBy(type=CreatedByType.HUMAN),\n        )\n        examples.append(example)\n    \n    simple_dataset = LabelledSimpleDataset(examples=examples)\n\nNow we can use the llama-pack to create our synthetic observations.\n\n    \n    \n    import llama_index.core.instrumentation as instrument\n    from llama_index.core.llama_dataset.simple import LabelledSimpleDataset\n    from llama_index.packs.diff_private_simple_dataset.base import PromptBundle\n    from llama_index.packs.diff_private_simple_dataset import DiffPrivateSimpleDatasetPack\n    from llama_index.llms.openai import OpenAI\n    import tiktoken\n    from .event_handler import DiffPrivacyEventHandler\n    import asyncio\n    import os\n    \n    NUM_SPLITS = 3\n    T_MAX = 150\n    \n    llm = OpenAI(\n        model=\"gpt-3.5-turbo-instruct\",\n        max_tokens=1,\n        logprobs=True,\n        top_logprobs=5,\n    )\n    tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo-instruct\")\n    \n    prompt_bundle = PromptBundle(\n        instruction=(\n            \"You are a patient experiencing symptoms of a specific disease. \"\n            \"Given a label of disease type, generate the chosen type of symptoms accordingly.\\n\"\n            \"Start your answer directly after 'Symptoms: '. Begin your answer with [RESULT].\\n\"\n        ),\n        label_heading=\"Disease\",\n        text_heading=\"Symptoms\",\n    )\n    \n    dp_simple_dataset_pack = DiffPrivateSimpleDatasetPack(\n        llm=llm,\n        tokenizer=tokenizer,\n        prompt_bundle=prompt_bundle,\n        simple_dataset=simple_dataset,\n    )\n    \n    synthetic_dataset = await dp_simple_dataset_pack.arun(\n        sizes=3,\n        t_max=T_MAX,\n        sigma=1.5,\n        num_splits=NUM_SPLITS,\n        num_samples_per_split=8,  # number of private observations to create a\n    )                             # synthetic obsevation\n    synthetic_dataset.save_json(\"synthetic_dataset.json\")\n\n###  Create a network with two contributors\n\nNext, we imagine that there are two contributors that each have their own set\nof Symptom2Disease datasets. In particular, we split the 24 categories of\ndiseases into two disjoint sets and consider each Contributor to possess only\none of the two sets. Note that we created the synthetic observations on the\nfull training set, though we could have easily done this on the split datasets\nas well.\n\nNow that we have the synthetic observations, we can follow a slightly modified\nversion of steps 1. through 3. defined in the story of Alex, Bob and Beth. The\nmodification here is that we\u2019re using Retrievers instead of QueryEngine (the\nchoice of Retriever or QueryEngine is completely up to the user).\n\n**Step 1:** Contributor\u2019s build their Retriever over their synthetic datasets.\n\n    \n    \n    import os\n    from llama_index.core import VectorStoreIndex\n    from llama_index.core.llama_dataset.simple import LabelledSimpleDataset\n    from llama_index.core.schema import TextNode\n    \n    \n    # load the synthetic dataset\n    synthetic_dataset = LabelledSimpleDataset.from_json(\n        \"./data/contributor1_synthetic_dataset.json\"\n    )\n    \n    \n    nodes = [\n        TextNode(text=el.text, metadata={\"reference_label\": el.reference_label})\n        for el in synthetic_dataset[:]\n    ]\n    \n    index = VectorStoreIndex(nodes=nodes)\n    similarity_top_k = int(os.environ.get(\"SIMILARITY_TOP_K\"))\n    retriever = index.as_retriever(similarity_top_k=similarity_top_k)\n\n**Step 2:** Contributor\u2019s expose their Retrievers behind a\nContributorRetrieverService\n\n    \n    \n    from llama_index.networks.contributor.retriever.service import (\n        ContributorRetrieverService,\n        ContributorRetrieverServiceSettings,\n    )\n    \n    settings = ContributorRetrieverServiceSettings() # loads from .env file\n    service = ContributorRetrieverService(config=settings, retriever=retriever)\n    app = service.app\n\n**Step 3:** Define the NetworkRetriever that connects to the\nContributorRetrieverServices\n\n    \n    \n    from llama_index.networks.network.retriever import NetworkRetriever\n    from llama_index.networks.contributor.retriever import ContributorRetrieverClient\n    from llama_index.postprocessor.cohere_rerank import CohereRerank\n    \n    # ContributorRetrieverClient's connect to the ContributorRetrieverService\n    contributors = [\n        ContributorRetrieverClient.from_config_file(\n            env_file=f\"./client-env-files/.env.contributor_{ix}.client\"\n        )\n        for ix in range(1, 3)\n    ]\n    reranker = CohereRerank(top_n=5)\n    network_retriever = NetworkRetriever(\n        contributors=contributors, node_postprocessors=[reranker]\n    )\n\nWith the ` NetworkRetriever ` established, we can retrieve synthetic\nobservations from the two contributors data against a query.\n\n    \n    \n    related_records = network_retriever.aretrieve(\"Vomitting and nausea\")\n    print(related_records) # contain symptoms/disease records that are similar to\n    \t\t\t\t\t\t\t\t\t\t\t # to the queried symptoms.\n\n", "mimetype": "text/plain", "start_char_idx": 6370, "end_char_idx": 13065, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a422e3c5-9810-47e9-ba7f-c6ba9a1c72b0": {"__data__": {"id_": "a422e3c5-9810-47e9-ba7f-c6ba9a1c72b0", "embedding": null, "metadata": {"filename": "retrieving-privacy-safe-documents-over-a-network.md", "extension": ".md", "title": "Retrieving Privacy-Safe Documents Over A Network", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a862e693-ee31-4336-8f5d-fee03192c4d0", "node_type": "4", "metadata": {"filename": "retrieving-privacy-safe-documents-over-a-network.md", "extension": ".md", "title": "Retrieving Privacy-Safe Documents Over A Network", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network"}, "hash": "d5d8487e5d253849b5641b7fa079148c99e372d6c5dd34474d391e213c5f0bd4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af8f5c03-5619-46e8-9b44-5086fcd51851", "node_type": "1", "metadata": {"filename": "retrieving-privacy-safe-documents-over-a-network.md", "extension": ".md", "title": "Retrieving Privacy-Safe Documents Over A Network", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network"}, "hash": "5eced6bfce2139ce74d978135114a0db5b6572d707dff91712eb80d5c575bcea", "class_name": "RelatedNodeInfo"}}, "text": "###  Evaluating the ` NetworkRetriever `\n\nTo evaluate the efficacy of the ` NetworkRetriever ` we make use of our test\nset in order to compute two traditional retrieval metrics, namely: hit rate\nand mean reciprocal rank.\n\n  * **hit rate:** a hit occurs if any of the retrieved nodes share the same disease label as the test query (symptoms). The hit rate then is the total number of hits divided by the size of the test set. \n  * **mean reciprocal rank:** similar to hit rate, but now we take into account the position of the first retrieved node that shares the same disease label as the test query. If there is no such retrieved node, then the reciprocal rank of the test is equal to 0. The mean reciprocal rank is then merely the average of all reciprocal ranks across the test set. \n\nIn addition to evaluating the ` NetworkRetriever ` we consider the two\nbaselines that represent Retrieving only over the individual Contributor\u2019s\nsynthetic datasets.\n\nRetriever evaluations, with sigma equal to 1.5.\n\nIn the image above, we observe that the NetworkRetriever outperforms both the\nindividual contributor Retriever\u2019s in the test set. This shouldn\u2019t be hard to\ngrasp however since the network retriever has access to more data since it has\naccess to both the Contributor\u2019s synthetic observations\u2014this is the point\nafter all of a network!\n\nAnother important observation can be made upon inspection of these results.\nThat is, the privacy-safe synthetic observations do indeed do the job of\nprotecting privacy while still maintaining utility in the original dataset.\nThis is often the concern when applying privacy measures such as differential\nprivacy, where noise is incorporated to protect the data. Too much noise will\nprovide high levels of privacy, but at the same time, may render the data\nuseless in downstream tasks. From the table above, we see that at least for\nthis example (though it does corroborate the results of the paper) that the\nsynthetic observations still do match well with the test set, which are indeed\nreal observations (i.e. not synthetically generated).\n\nFinally, this level of privacy can be controlled via the noise parameter `\nsigma ` . In the example above we used a ` sigma ` of 1.5, which for this\ndataset amounts to an ` epsilon ` (i.e., privacy-loss measure) value of 1.3.\n(Privacy loss levels between 0 and 1 are [ generally considered to be quite\nprivate\n](https://www.notion.so/Standups-8d1e478a8ce54ffa9788eef1fd416042?pvs=21) .)\nBelow, we share the evaluations that result from using a ` sigma ` of 0.5,\nwhich amounts to an ` epsilon ` of 15.9\u2014higher values of ` epsilon ` or\nprivacy-loss means less privacy.\n\n    \n    \n    # use the `DiffPrivacySimpleDatasetPack` to get the value of epsilon\n    epsilon = dp_simple_dataset_pack.sigma_to_eps(\n    \t\tsigma=0.5,\n    \t\tmechanism=\"gaussian\",\n    \t\tsize=3*24,\n    \t\tmax_token_cnt=150  # number of max tokens to generate per synthetic example\n    )\n\nRetriever evaluations with less noise and thus less privacy i.e., sigma equal\nto 0.5.\n\nSo we see after comparing the evaluation metrics with different levels of\nprivacy that when we use the synthetic observations that have higher levels of\nprivacy, we take a bit of a hit in the performance as seen in the decrease in\nboth the hit rate and mean reciprocal rank. This indeed is an illustration of\nthe privacy tradeoff. If we take a look at some of the examples from the\nsynthetic datasets, we can perhaps gain insight as to why this may be\nhappening.\n\n    \n    \n    # synthetic example epsilon = 1.3\n    {\n        \"reference_label\": \"Psoriasis\",\n        \"text\": \"[RESULTS] red, scalloped patches on skin; itching and burning sensation; thick, pitted nails on fingers and toes; joint discomfort; swollen and stiff joints; cracked and painful skin on palms and feet\",\n        \"text_by\": {\n            \"model_name\": \"gpt-3.5-turbo-instruct\",\n            \"type\": \"ai\"\n        }\n    },\n    \n    # synthetic example epsilon = 15.9\n    {\n      \"reference_label\": \"Migraine\",\n      \"text\": \"Intense headache, sensitivity to light and sound, nausea, vomiting, vision changes, and fatigue.\",\n      \"text_by\": {\n        \"model_name\": \"gpt-3.5-turbo-instruct\",\n        \"type\": \"ai\"\n      }\n    },\n\nWe can see that synthetic datasets with higher level of privacy are not as\nclean in terms of punctuation symbols in the text when compared to those with\nlower levels of privacy. This makes sense because the differential privacy\nalgorithm adds noise to the mechanics of next-token generation. Thus,\nperturbing this process greatly has affect on the instruction-following\ncapabilities of the LLM.\n\n###  In summary\n\n  * We used differential privacy to create privacy-safe, synthetic observations in order to permit the data collaboration of private data that may not be otherwise possible. \n  * We demonstrated the benefits of the NetworkRetriever that has access to more data than what the individual Contributor Retriever may have access to. \n  * We demonstrated the affects of varying degrees of privacy on the synthetic observations, and by extension, the NetworkRetriever. \n\n##  Learn more!\n\nTo delve deeper into the materials of this blog post, we share a few links\nbelow:\n\n  * Source code for the privacy-safe networks retriever demo. With this, you can try the above all out yourself! ( [ link ](https://github.com/run-llama/llama_index/tree/main/llama-index-networks/examples/privacy_safe_retrieval) ) \n  * Demo notebooks for the ` DiffPrivateSimpleDataset ` ( [ link ](https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-diff-private-simple-dataset/examples/basic_demo/demo_usage.ipynb) ) \n  * The source code for creating the synthetic Symptom2Disease observations using the ` DiffPrivateSimpleDataset ` ( [ link ](https://github.com/run-llama/llama_index/tree/main/llama-index-packs/llama-index-packs-diff-private-simple-dataset/examples/symptom_2_disease) ) \n\n", "mimetype": "text/plain", "start_char_idx": 13065, "end_char_idx": 18987, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fa1b58eb-7d4a-49b4-8338-72f14fbb8aab": {"__data__": {"id_": "fa1b58eb-7d4a-49b4-8338-72f14fbb8aab", "embedding": null, "metadata": {"filename": "supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.md", "extension": ".md", "title": "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations", "date": "Mar 19, 2024", "url": "https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9cae519d-8301-4f1a-aeaa-acfb3292c99b", "node_type": "4", "metadata": {"filename": "supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.md", "extension": ".md", "title": "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations", "date": "Mar 19, 2024", "url": "https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations"}, "hash": "f8100da6206ccae117431f581f3298b2dd95bb453c0f28929d830d1e83b389bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de098a77-ad7f-42c9-a3db-ab3d8db35c36", "node_type": "1", "metadata": {}, "hash": "679e955d94cfd4943a8e015fe50e972a824d2b172a97fa6f50f82690ec08bbd3", "class_name": "RelatedNodeInfo"}}, "text": "_This is a guest post from Uptrain._\n\nWe are excited to announce the recent integration of LlamaIndex with UpTrain -\nan open-source LLM evaluation framework to evaluate your RAG pipelines and\nexperiment with different configurations. As an increasing number of companies\nare graduating their LLM prototypes to production-ready systems, robust\nevaluations provide a systematic framework to make decisions rather than going\nwith the \u2018vibes\u2019. By combining LlamaIndex's flexibility and UpTrain's\nevaluation framework, developers can experiment with different configurations,\nfine-tuning their LLM-based applications for optimal performance.\n\n##  **About UpTrain**\n\n**UpTrain** [ [ github ](https://github.com/uptrain-ai/uptrain) || [ website\n](https://uptrain.ai/) || [ docs ](https://docs.uptrain.ai/getting-\nstarted/introduction) ] is an open-source platform to evaluate and improve LLM\napplications. It provides grades for 20+ preconfigured checks (covering\nlanguage, code, embedding use cases), performs root cause analyses on\ninstances of failure cases and provides guidance for resolving them.\n\n**Key Highlights:**\n\n  * **Data Security:** As an open-source solution, UpTrain conducts all evaluations and analyses locally, ensuring that your data remains within your secure environment (except for the LLM calls). \n  * **Custom Evaluator LLMs:** UpTrain allows for [ customisation of your evaluator LLM ](https://github.com/uptrain-ai/uptrain/blob/main/examples/open_source_evaluator_tutorial.ipynb) , offering options among several endpoints, including OpenAI, Anthropic, Llama, Mistral, or Azure. \n  * **Insights that help with model improvement:** Beyond mere evaluation, UpTrain performs [ root cause analysis ](https://github.com/uptrain-ai/uptrain/blob/main/examples/root_cause_analysis/rag_with_citation.ipynb) to pinpoint the specific components of your LLM pipeline, that are underperforming, as well as identifying common patterns among failure cases, thereby helping in their resolution. \n  * **Diverse Experimentations:** The platform enables [ experimentation ](https://github.com/uptrain-ai/uptrain/tree/main/examples/experiments) with different prompts, LLM models, RAG modules, embedding models, etc. and helps you find the best fit for your specific use case. \n  * **Compare open-source LLMs:** With UpTrain, you can compare your fine-tuned open-source LLMs against proprietary ones (such as GPT-4), helping you to find the most cost-effective model without compromising quality. \n\nIn the following sections, we will illustrate how you can use UpTrain to\nevaluate your LlamaIndex pipeline. The evaluations demonstrated here will help\nyou quickly find what\u2019s affecting the quality of your responses, allowing you\nto take appropriate corrective actions.\n\n##  **LlamaIndex x UpTrain Callback Handler**\n\nWe introduce an UpTrain Callback Handler which makes evaluating your existing\nLlamaIndex Pipeline seamless. By adding just a few lines of code, UpTrain will\nautomatically perform a series of checks - evaluating the quality of generated\nresponses, the quality of contextual data retrieved by the RAG pipeline as\nwell as the performance of all the interim steps.\n\nIf you wish to skip right ahead to the tutorial, check it out [ here.\n](https://colab.research.google.com/github/run-\nllama/llama_index/blob/main/docs/examples/callbacks/UpTrainCallback.ipynb)\n\n##  **Evals across the board: From Vanilla to Advanced RAG**\n\nVanilla RAG involves a few steps. You need to embed the documents and store\nthem in a vector database. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3539, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de098a77-ad7f-42c9-a3db-ab3d8db35c36": {"__data__": {"id_": "de098a77-ad7f-42c9-a3db-ab3d8db35c36", "embedding": null, "metadata": {"filename": "supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.md", "extension": ".md", "title": "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations", "date": "Mar 19, 2024", "url": "https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9cae519d-8301-4f1a-aeaa-acfb3292c99b", "node_type": "4", "metadata": {"filename": "supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.md", "extension": ".md", "title": "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations", "date": "Mar 19, 2024", "url": "https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations"}, "hash": "f8100da6206ccae117431f581f3298b2dd95bb453c0f28929d830d1e83b389bb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa1b58eb-7d4a-49b4-8338-72f14fbb8aab", "node_type": "1", "metadata": {"filename": "supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.md", "extension": ".md", "title": "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations", "date": "Mar 19, 2024", "url": "https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations"}, "hash": "1d52e19d27457396235b117d28f7c8257f60cf3521fad925706143d1386b74c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a50644c-df14-4eae-ba9b-6d575af2d5cd", "node_type": "1", "metadata": {}, "hash": "a877e3f61ddc936045dd8c4b1cadd82c3fc21174a8ffa3c21674cd1a2b12811b", "class_name": "RelatedNodeInfo"}}, "text": "When the user asks questions, the framework embeds\nthem and uses similarity search to find the most relevant documents. The\ncontent of these retrieved documents, and the original query, are then passed\non to the LLM to generate the final response.\n\nWhile the above is a great starting point, there have been a lot of\nimprovements to achieve better results. Advanced RAG applications have many\nadditional steps that improve the quality of the retrieved documents, which in\nturn improve the quality of your responses.\n\nBut as Uncle Ben famously said to Peter Parker in the GenAI universe:\n\n_\u201cWith increased complexity comes more points of failure.\u201d._\n\nMost of the LLM evaluation tools only evaluate the final context-response pair\nand fail to take into consideration the intermediary steps of an advanced RAG\npipeline. Let\u2019s look at all the evaluations provided by UpTrain.\n\n##  **Addressing Points of Failure in RAG Pipelines**\n\n###  **1\\. RAG Query Engine Evaluation**\n\nLet's first take a Vanilla RAG Pipeline and see how you can test its\nperformance. UpTrain provides three operators curated for testing both the\nretrieved context as well as the LLM's response.\n\n  * [ **Context Relevance** ](https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-relevance) : However informative the documents retrieved might be, if they are not relevant to your query, you will likely not get a response that answers your query. The Context Relevance operator determines if the documents fetched from the vector store contain information that can be used to answer your query. \n  * [ **Factual Accuracy** ](https://docs.uptrain.ai/predefined-evaluations/context-awareness/factual-accuracy) : Now that we have checked if the context contains information to answer our query, we will check if the response provided by the LLM is backed by the information present in the context. The Factual Accuracy operator assesses if the LLM is hallucinating or providing information that is not present in the context. \n  * [ **Response Completeness** ](https://docs.uptrain.ai/predefined-evaluations/response-quality/response-completeness) : Not all queries are straightforward. ", "mimetype": "text/plain", "start_char_idx": 3539, "end_char_idx": 5710, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a50644c-df14-4eae-ba9b-6d575af2d5cd": {"__data__": {"id_": "7a50644c-df14-4eae-ba9b-6d575af2d5cd", "embedding": null, "metadata": {"filename": "supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.md", "extension": ".md", "title": "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations", "date": "Mar 19, 2024", "url": "https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9cae519d-8301-4f1a-aeaa-acfb3292c99b", "node_type": "4", "metadata": {"filename": "supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.md", "extension": ".md", "title": "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations", "date": "Mar 19, 2024", "url": "https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations"}, "hash": "f8100da6206ccae117431f581f3298b2dd95bb453c0f28929d830d1e83b389bb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de098a77-ad7f-42c9-a3db-ab3d8db35c36", "node_type": "1", "metadata": {"filename": "supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.md", "extension": ".md", "title": "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations", "date": "Mar 19, 2024", "url": "https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations"}, "hash": "2a4d7dc64af861ad6d30e2459bd18cdbe62e037e05235f0089792916d110c093", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7033d9be-effe-4e62-9074-76037665321c", "node_type": "1", "metadata": {}, "hash": "495a141e2119b489a7a5f1b5c68ab5c40a13dc5d3045675fa78c4b5817972bbf", "class_name": "RelatedNodeInfo"}}, "text": "Some of them have multiple parts to them. A good response should be able to answer all the aspects of the query. The Response Completeness operator checks if the response contains all the information requested by the query. \n\n###  **2\\. Sub-Question Query Engine Evaluation**\n\nLet's say you tried out a Vanilla RAG pipeline and got consistently low\nResponse Completeness scores. This means that the LLM is not answering all\naspects of your query. One of the ways to solve this is by splitting the query\ninto multiple smaller sub-queries that the LLM can answer more easily. To do\nthis, you can use the SubQuestionQueryGeneration operator provided by\nLlamaIndex. This operator decomposes a question into sub-questions, generating\nresponses for each using an RAG query engine.\n\nIf you include this SubQuery module in your RAG pipeline, it introduces\nanother point of failure, e.g. what if the sub-questions that we split our\noriginal question aren't good representations of it? UpTrain automatically\nadds new evaluations to check how well the module performs:\n\n  * [ **Sub Query Completeness** ](https://docs.uptrain.ai/predefined-evaluations/sub-query/sub-query-completeness) : It evaluates whether the sub-questions accurately and comprehensively cover the original query. \n  * Context Relevance, Factual Accuracy and Response Completeness for each of the sub-queries. \n\n", "mimetype": "text/plain", "start_char_idx": 5710, "end_char_idx": 7081, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7033d9be-effe-4e62-9074-76037665321c": {"__data__": {"id_": "7033d9be-effe-4e62-9074-76037665321c", "embedding": null, "metadata": {"filename": "supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.md", "extension": ".md", "title": "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations", "date": "Mar 19, 2024", "url": "https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9cae519d-8301-4f1a-aeaa-acfb3292c99b", "node_type": "4", "metadata": {"filename": "supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.md", "extension": ".md", "title": "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations", "date": "Mar 19, 2024", "url": "https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations"}, "hash": "f8100da6206ccae117431f581f3298b2dd95bb453c0f28929d830d1e83b389bb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a50644c-df14-4eae-ba9b-6d575af2d5cd", "node_type": "1", "metadata": {"filename": "supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.md", "extension": ".md", "title": "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations", "date": "Mar 19, 2024", "url": "https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations"}, "hash": "38506b1deaaa8718c9cf84489a1129f172e61b9f91fe7a8252b1b2f4fad21817", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "175ba084-a296-4e9f-b71d-b501d5a22ad4", "node_type": "1", "metadata": {}, "hash": "0a50ccf9c3fbaf641f746560dd17ada077c2baea265bf00aec0eb512d0c8f1cc", "class_name": "RelatedNodeInfo"}}, "text": "###  **3\\. Reranking Evaluations**\n\nWe looked at a way of dealing with low Response Completeness scores. Now,\nlet's look at a way of dealing with low Context Relevance scores.\n\nRAG pipelines retrieve documents based on semantic similarity. These documents\nare ordered based on how similar they are to the query asked. However, recent\nresearch [ [ Lost in the Middle: How Language Model Uses Long Contexts\n](https://arxiv.org/pdf/2307.03172.pdf) ] has shown that the LLMs are\nsensitive to the placement of the most critical information within the\nretrieved context. To solve this, you might want to add a reranking block.\n\nReranking involves using a semantic search model (specially tuned for the\nreranking task) that breaks down the retrieved context into smaller chunks,\nfinds the semantic similarity between them and the query and rewrites the\ncontext by ranking them in order of their similarity.\n\nWe observed that when using the reranking operators in LlamaIndex, two\nscenarios can occur. These scenarios differ based on the number of nodes\nbefore and after the reranking process:\n\n**a. Same Number of Nodes Before and After Reranking:**\n\nIf the number of nodes after the reranking remains the same, then we need to\ncheck if the new order is such that nodes higher in rank are more relevant to\nthe query as compared to the older order. To check for this, UpTrain provides\na Context Reranking operator.\n\n  * [ **Context Reranking** ](https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-reranking) : Checks if the order of reranked nodes is more relevant to the query than the original order. \n\n**b. Fewer Number of Nodes After Reranking:**\n\nReducing the number of nodes can help the LLM give better responses. This is\nbecause the LLMs process smaller context lengths better. However, we need to\nmake sure that we don't lose information that would have been useful in\nanswering the question. Therefore, during the process of reranking, if the\nnumber of nodes in the output is reduced, we provide a Context Conciseness\noperator.\n\n  * [ **Context Conciseness** ](https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-conciseness) : Examines whether the reduced number of nodes still provides all the required information. \n\n##  **Key Takeaways: Enhancing RAG Pipelines Through Advanced Techniques and\nEvaluation**\n\nLet's do a quick recap here. We started off with a Vanilla RAG pipeline and\nevaluated the quality of the generated response and retrieved context. Then,\nwe moved to advanced RAG concepts like the SubQuery technique (used to combat\ncases with low Response Completeness scores) and the Reranking technique (used\nto improve the quality of retrieved context) and looked at advanced\nevaluations to quantify their performance.\n\nThis essentially provides a framework to systematically test the performance\nof different modules as well as evaluate if they actually lead to better\nquality responses by making data-driven decisions.\n\nMuch of the success in the field of Artificial intelligence can be attributed\nto experimentation with different architectures, hyperparameters, datasets,\netc., and our integration with UpTrain allows you to import those best\npractices while building RAG pipelines. Get started with uptrain with this [\nquickstart tutorial ](https://docs.uptrain.ai/getting-started/quickstart) .\n\n", "mimetype": "text/plain", "start_char_idx": 7081, "end_char_idx": 10439, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "175ba084-a296-4e9f-b71d-b501d5a22ad4": {"__data__": {"id_": "175ba084-a296-4e9f-b71d-b501d5a22ad4", "embedding": null, "metadata": {"filename": "supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.md", "extension": ".md", "title": "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations", "date": "Mar 19, 2024", "url": "https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9cae519d-8301-4f1a-aeaa-acfb3292c99b", "node_type": "4", "metadata": {"filename": "supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.md", "extension": ".md", "title": "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations", "date": "Mar 19, 2024", "url": "https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations"}, "hash": "f8100da6206ccae117431f581f3298b2dd95bb453c0f28929d830d1e83b389bb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7033d9be-effe-4e62-9074-76037665321c", "node_type": "1", "metadata": {"filename": "supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.md", "extension": ".md", "title": "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations", "date": "Mar 19, 2024", "url": "https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations"}, "hash": "9edf11d93b5ac83db74329ee37a84d6ac4e7b87deaed3f5ae0908b2b71bb2a32", "class_name": "RelatedNodeInfo"}}, "text": "##  **References**\n\n  1. [ UpTrain Callback Handler Tutorial ](https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/callbacks/UpTrainCallback.ipynb)\n  2. [ UpTrain GitHub Repository ](https://github.com/uptrain-ai/uptrain)\n  3. [ Advanced RAG Techniques: an Illustrated Overview ](https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6)\n  4. [ Lost in the Middle: How Language Models Use Long Contexts ](https://arxiv.org/pdf/2307.03172.pdf)\n  5. [ UpTrainCallbackHandler documentation ](https://docs.llamaindex.ai/en/stable/community/integrations/uptrain.html)\n  6. [ UpTrain Website ](https://uptrain.ai/)\n\n", "mimetype": "text/plain", "start_char_idx": 10439, "end_char_idx": 11114, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d8b3c20-1778-4967-a7af-f110282a989b": {"__data__": {"id_": "4d8b3c20-1778-4967-a7af-f110282a989b", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-03-19.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-19", "date": "Mar 19, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-19"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fd63239a-c906-41ee-b807-30467082eeda", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-03-19.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-19", "date": "Mar 19, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-19"}, "hash": "15d4f87bd953b2a6233ce2257c3e8a92a647e3ffdd4d0ff27fbf67576fd46435", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b73e9c3f-7df7-4cb1-af7e-8ef34a8d24a4", "node_type": "1", "metadata": {}, "hash": "dfa8a9bce2014a4032de7964005b91fb221e9ee725936ed6b2494cbd304916ff", "class_name": "RelatedNodeInfo"}}, "text": "Greetings, LlamaIndex enthusiasts!\n\nWelcome to another exciting weekly update from the world of LlamaVerse!\n\nWe have an amazing news for you from LlamaIndex. We've officially launched\nLlamaParse, a GenAI-native document parsing solution. With state-of-the-art\ntable and chart extraction, natural language steerable instructions, and\ncompatibility with over a dozen document types, LlamaParse excels in creating\naccurate RAG applications from complex documents. After a successful private\npreview with 2k users and 1M pages parsed, it's now ready to transform your\ndocument handling. Check out our [ launch post\n](https://www.llamaindex.ai/blog/launching-the-first-genai-native-document-\nparsing-platform) for all the details!\n\n**The highlights:**\n\n  1. **New observability with Instrumentation:** Enhanced developer workflow with a new Instrumentation module for improved observability. [ Docs ](https://docs.llamaindex.ai/en/stable/module_guides/observability/instrumentation.html) , [ Tweet ](https://x.com/llama_index/status/1768730443921396220?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1059, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b73e9c3f-7df7-4cb1-af7e-8ef34a8d24a4": {"__data__": {"id_": "b73e9c3f-7df7-4cb1-af7e-8ef34a8d24a4", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-03-19.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-19", "date": "Mar 19, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-19"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fd63239a-c906-41ee-b807-30467082eeda", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-03-19.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-19", "date": "Mar 19, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-19"}, "hash": "15d4f87bd953b2a6233ce2257c3e8a92a647e3ffdd4d0ff27fbf67576fd46435", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d8b3c20-1778-4967-a7af-f110282a989b", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-03-19.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-19", "date": "Mar 19, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-19"}, "hash": "e4f4b7951185fe06e919a156e968efa38a1b3680e7ce6fbf2824f258c3f72a58", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a056ff1b-5f59-4101-9172-35ad569a254a", "node_type": "1", "metadata": {}, "hash": "0679349bbf41e0c8a08e1a1f39b6ac2a38217d25cc86cd59ca331dff094daaf3", "class_name": "RelatedNodeInfo"}}, "text": "2. **LlamaParse accepts natural language parsing instructions** : Easily extract math snippets from PDFs into LaTeX with LlamaParse. [ Blogpost ](https://www.llamaindex.ai/blog/launching-the-first-genai-native-document-parsing-platform) , [ Tweet ](https://x.com/llama_index/status/1768443551267049492?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 1059, "end_char_idx": 1372, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a056ff1b-5f59-4101-9172-35ad569a254a": {"__data__": {"id_": "a056ff1b-5f59-4101-9172-35ad569a254a", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-03-19.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-19", "date": "Mar 19, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-19"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fd63239a-c906-41ee-b807-30467082eeda", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-03-19.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-19", "date": "Mar 19, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-19"}, "hash": "15d4f87bd953b2a6233ce2257c3e8a92a647e3ffdd4d0ff27fbf67576fd46435", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b73e9c3f-7df7-4cb1-af7e-8ef34a8d24a4", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-03-19.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-19", "date": "Mar 19, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-19"}, "hash": "950f3ace972bc3a3c5deed39d2b1de931c0c897e39eb6afbe5e9d765ecaacca4", "class_name": "RelatedNodeInfo"}}, "text": "3. **Financial Data Parsing:** Transform PowerPoint parsing, utilizing LlamaParse to extract and interpret complex financial data from .pptx files, enabling detailed and accurate financial analysis. [ Notebook ](https://github.com/run-llama/llama_parse/blob/main/examples/other_files/demo_ppt_financial.ipynb) , [ Tweet ](https://x.com/llama_index/status/1768303288381030408?s=20) . \n\n**Feature Releases and Enhancements:**\n\n  * We introduced LlamaIndex v0.10.20, featuring our new Instrumentation module, a leap in observability that simplifies developer workflows by providing a module-level dispatcher, reducing the need for individual callback managers and facilitating comprehensive handler sets across your application. [ Docs ](https://docs.llamaindex.ai/en/stable/module_guides/observability/instrumentation.html) , [ Tweet ](https://x.com/llama_index/status/1768730443921396220?s=20) . \n  * We have launched parsing by prompting feature in LlamaParse to properly extract out any math snippets from PDFs into LaTex which helps you to plug easily into your RAG pipeline. [ Blogpost ](https://www.llamaindex.ai/blog/launching-the-first-genai-native-document-parsing-platform) , [ Tweet ](https://x.com/llama_index/status/1768443551267049492?s=20) . \n  * We have launched an advanced RAG pipeline for Financial PowerPoints, using LlamaParse to tackle the challenge of parsing .pptx files. Our solution accurately extracts slides, including text, tables, and charts, enabling precise question-answering over complex financial data. [ Notebook ](https://github.com/run-llama/llama_parse/blob/main/examples/other_files/demo_ppt_financial.ipynb) , [ Tweet ](https://x.com/llama_index/status/1768303288381030408?s=20) . \n  * We collaborated with langfuse to launch open-source observability for your RAG pipeline, enhancing your application with integrated tracing, prompt management, and evaluation in just two lines of code. [ Blogpost ](https://www.llamaindex.ai/blog/one-click-open-source-rag-observability-with-langfuse) , [ Docs ](https://docs.llamaindex.ai/en/stable/examples/callbacks/LangfuseCallbackHandler.html) , [ Tweet ](https://x.com/llama_index/status/1769790083564208218?s=20) . \n  * Search-in-the-Chain: a method by Shicheng Xu et al., is now integrated into LlamaIndex, enhancing question-answering with an advanced system that interleaves retrieval and planning. This approach verifies each reasoning step in a chain, allowing for dynamic replanning and application in various agent reasoning contexts. [ LlamaPack ](https://llamahub.ai/l/llama-packs/llama-index-packs-searchain?from=) , [ Tweet ](https://x.com/llama_index/status/1769035278063399208?s=20)\n\n**Demos:**\n\n  * Home AI, a tool created with create-llama, to help home searches by using LLMs to automate the parsing of complex property disclosures, enabling users to filter searches with unprecedented detail and efficiency. [ Blogpost ](https://devpost.com/software/home-ai) , [ Code ](https://github.com/2sunflower33/homeai) , [ Tweet ](https://x.com/llama_index/status/1767289805719978288?s=20) . \n\n**Guides:**\n\n  * [ Guide ](https://github.com/tensorsense/Retrieval-Framework/blob/main/hierarchical_retrieval.ipynb) to using LlamaIndex and Mathpix to parse, index, and query complex mathematics within scientific papers, detailing steps from parsing tables and extracting images to indexing in a RAG app and answering questions with precise LaTeX outputs, to showcase hierarchical retrieval technique. \n\n**Tutorials:**\n\n  * [ Thomas Reid ](https://twitter.com/taupirho) \u2019s [ tutorial ](https://ai.gopubby.com/llamaparse-rag-beats-all-comers-60948c6cc0e4) on using LlamaParse can help properly extract text from a Tesla quarterly filings. \n  * [ Sudarshan Koirala ](https://twitter.com/mesudarshan) [ video tutorial ](https://www.youtube.com/watch?v=w7Ap6gZFXl0) on RAG with LlamaParse, Qdrant, and Groq. \n  * Kyosuke Morita [ tutorial ](https://pub.towardsai.net/rag-based-job-search-assistant-98dd72c98fbd) showing how to match a candidate to jobs based on their CV with LlamaParse + LlamaIndex. \n  * [ Cobus Greyling ](https://twitter.com/CobusGreylingZA) [ tutorial ](https://cobusgreyling.medium.com/agentic-rag-context-augmented-openai-agents-578e96212bc0) on Agentic RAG: Context-Augmented OpenAI Agents. \n  * [ Roey Ben Chaim ](https://twitter.com/RoeyBC) \u2019s [ tutorial ](https://www.llamaindex.ai/blog/pii-detector-hacking-privacy-in-rag) on PII Detector: hacking privacy in RAG. \n\n**Webinars:**\n\n  * [ Webinar ](https://www.youtube.com/watch?v=Bhnq8grQm5Y) with Charles Packer, lead author of MemGPT on Long-Term, Self-Editing Memory with MemGPT \n\nEvents:\n\n  * We are hosting a RAG [ meetup ](https://www.meetup.com/paris-retrieval-augmented-generation-group/events/299374545/) in Paris on March 27th featuring talks on advanced RAG strategies, building a RAG CLI, and the significance of open-source RAG in business. \n\n", "mimetype": "text/plain", "start_char_idx": 1372, "end_char_idx": 6286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "260ad09d-bcef-42c4-91bb-558ee9762842": {"__data__": {"id_": "260ad09d-bcef-42c4-91bb-558ee9762842", "embedding": null, "metadata": {"filename": "one-click-open-source-rag-observability-with-langfuse.md", "extension": ".md", "title": "One-click Open Source RAG Observability with Langfuse", "date": "Mar 18, 2024", "url": "https://www.llamaindex.ai/blog/one-click-open-source-rag-observability-with-langfuse"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4fa009e1-cf5d-4a11-9279-7d0fc2a8888c", "node_type": "4", "metadata": {"filename": "one-click-open-source-rag-observability-with-langfuse.md", "extension": ".md", "title": "One-click Open Source RAG Observability with Langfuse", "date": "Mar 18, 2024", "url": "https://www.llamaindex.ai/blog/one-click-open-source-rag-observability-with-langfuse"}, "hash": "b2448a6d51f64d46aaf01da26cdbbbfbca0f16cb4dc6d3b9cc74802795c36aab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33ee8c71-de46-496a-87c4-3be3d580c4c4", "node_type": "1", "metadata": {}, "hash": "d70a4360044d6803db342f492a86a4aeb6e3e8ff7f7564d3647e4d8bf2ca9f67", "class_name": "RelatedNodeInfo"}}, "text": "_This is a guest post from the team at Langfuse_\n\nThere are so many different ways to make RAG work for a use case. What vector\nstore to use? ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 142, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33ee8c71-de46-496a-87c4-3be3d580c4c4": {"__data__": {"id_": "33ee8c71-de46-496a-87c4-3be3d580c4c4", "embedding": null, "metadata": {"filename": "one-click-open-source-rag-observability-with-langfuse.md", "extension": ".md", "title": "One-click Open Source RAG Observability with Langfuse", "date": "Mar 18, 2024", "url": "https://www.llamaindex.ai/blog/one-click-open-source-rag-observability-with-langfuse"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4fa009e1-cf5d-4a11-9279-7d0fc2a8888c", "node_type": "4", "metadata": {"filename": "one-click-open-source-rag-observability-with-langfuse.md", "extension": ".md", "title": "One-click Open Source RAG Observability with Langfuse", "date": "Mar 18, 2024", "url": "https://www.llamaindex.ai/blog/one-click-open-source-rag-observability-with-langfuse"}, "hash": "b2448a6d51f64d46aaf01da26cdbbbfbca0f16cb4dc6d3b9cc74802795c36aab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "260ad09d-bcef-42c4-91bb-558ee9762842", "node_type": "1", "metadata": {"filename": "one-click-open-source-rag-observability-with-langfuse.md", "extension": ".md", "title": "One-click Open Source RAG Observability with Langfuse", "date": "Mar 18, 2024", "url": "https://www.llamaindex.ai/blog/one-click-open-source-rag-observability-with-langfuse"}, "hash": "dc6180b3cb3eebee347f3704b36dae0efc40751e8198cd47543026393242d9a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed5f6a20-a67e-403a-840a-065155aa89a1", "node_type": "1", "metadata": {}, "hash": "d16d45b8af8fb9f3180af3e39cafdee3e8772863d0ccae657920da5448239dbb", "class_name": "RelatedNodeInfo"}}, "text": "What retrieval strategy to use? LlamaIndex makes it easy to try\nmany of them without having to deal with the complexity of integrations,\nprompts and memory all at once.\n\nInitially, we at Langfuse worked on complex RAG/agent applications and quickly\nrealized that there is a new need for observability and experimentation to\ntweak and iterate on the details. In the end, these details matter to get from\nsomething cool to an actually reliable RAG application that is safe for users\nand customers. Think of this: if there is a user session of interest in your\n_production_ RAG application, how can you quickly see whether the retrieved\ncontext for that session was actually relevant or the LLM response was on\npoint?\n\nThus, we started working on [ Langfuse.com ](http://langfuse.com) ( [ GitHub\n](https://github.com/langfuse/langfuse) ) to establish an open source LLM\nengineering platform with tightly integrated features for tracing, prompt\nmanagement, and evaluation. In the beginning we just solved our own and our\nfriends\u2019 problems. Today we are at over 1000 projects which rely on Langfuse,\nand 2.3k stars on GitHub. You can either [ self-host\n](https://langfuse.com/docs/deployment/self-host) Langfuse or use the [ cloud\ninstance ](https://cloud.langfuse.com) maintained by us.\n\nWe are thrilled to announce our new integration with LlamaIndex today. This\nfeature was [ highly requested\n](https://github.com/orgs/langfuse/discussions/828) by our community and\naligns with our project's focus on native integration with major application\nframeworks. Thank you to everyone who contributed and tested it during the\nbeta phase!\n\n##  The challenge\n\nWe love LlamaIndex, since the clean and standardized interface abstracts a lot\nof complexity away. Let\u2019s take this simple example of a VectorStoreIndex and a\nChatEngine.\n\n    \n    \n    from llama_index.core import SimpleDirectoryReader\n    from llama_index.core import VectorStoreIndex\n    \n    documents = SimpleDirectoryReader(\"./data\").load_data()\n    \n    index = VectorStoreIndex.from_documents(documents)\n    \n    chat_engine = index.as_chat_engine()\n    \n    print(chat_engine.chat(\"What problems can I solve with RAG?\"))\n    print(chat_engine.chat(\"How do I optimize my RAG application?\"))\n\nIn just 3 lines we loaded our local documents, added them to an index and\ninitialized a ChatEngine with memory. Subsequently we had a stateful\nconversation with the chat_engine.\n\nThis is awesome to get started, but we quickly run into questions like:\n\n  * _\u201cWhat context is actually retrieved from the index to answer the questions?\u201d_\n  * _\u201cHow is chat memory managed?\u201d_\n  * _\u201cWhich steps add the most latency to the overall execution? How to optimize it?\u201d_\n\n##  One-click OSS observability to the rescue\n\nWe integrated Langfuse to be a one-click integration with LlamaIndex using the\nglobal callback manager.\n\nPreparation\n\n  1. Install the community package (pip install llama-index-callbacks-langfuse) \n  2. Copy/paste the environment variables from the Langfuse project settings to your Python project: 'LANGFUSE_SECRET_KEY', 'LANGFUSE_PUBLIC_KEY' and 'LANGFUSE_HOST' \n\nNow, you only need to set the global langfuse handler:\n\n    \n    \n    from llama_index.core import set_global_handler\n    \n    set_global_handler(\"langfuse\")\n\nAnd voil\u00e1, with just two lines of code you get detailed traces for all aspects\nof your RAG application in Langfuse. They automatically include latency and\nusage/cost breakdowns.\n\n", "mimetype": "text/plain", "start_char_idx": 142, "end_char_idx": 3600, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed5f6a20-a67e-403a-840a-065155aa89a1": {"__data__": {"id_": "ed5f6a20-a67e-403a-840a-065155aa89a1", "embedding": null, "metadata": {"filename": "one-click-open-source-rag-observability-with-langfuse.md", "extension": ".md", "title": "One-click Open Source RAG Observability with Langfuse", "date": "Mar 18, 2024", "url": "https://www.llamaindex.ai/blog/one-click-open-source-rag-observability-with-langfuse"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4fa009e1-cf5d-4a11-9279-7d0fc2a8888c", "node_type": "4", "metadata": {"filename": "one-click-open-source-rag-observability-with-langfuse.md", "extension": ".md", "title": "One-click Open Source RAG Observability with Langfuse", "date": "Mar 18, 2024", "url": "https://www.llamaindex.ai/blog/one-click-open-source-rag-observability-with-langfuse"}, "hash": "b2448a6d51f64d46aaf01da26cdbbbfbca0f16cb4dc6d3b9cc74802795c36aab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33ee8c71-de46-496a-87c4-3be3d580c4c4", "node_type": "1", "metadata": {"filename": "one-click-open-source-rag-observability-with-langfuse.md", "extension": ".md", "title": "One-click Open Source RAG Observability with Langfuse", "date": "Mar 18, 2024", "url": "https://www.llamaindex.ai/blog/one-click-open-source-rag-observability-with-langfuse"}, "hash": "5ddc00f2a9900ed026f25a0cba05acbb280e4bebacd28cf81c4647573052e685", "class_name": "RelatedNodeInfo"}}, "text": "##  Group multiple chat threads into a session\n\nWorking with lots of teams building GenAI/LLM/RAG applications, we\u2019ve\ncontinuously added more features that are useful to debug and improve these\napplications. One example is [ session tracking\n](https://langfuse.com/docs/tracing/sessions) for conversational applications\nto see the traces in context of a full message thread.\n\nTo activate it, just add an id that identifies the session as a trace param\nbefore calling the chat_engine.\n\n    \n    \n    from llama_index.core import global_handler\n    \n    global_handler.set_trace_params(\n      session_id=\"your-session-id\"\n    )\n    \n    chat_engine.chat(\"What did he do growing up?\")\n    chat_engine.chat(\"What did he do at USC?\")\n    chat_engine.chat(\"How old is he?\")\n\nThereby you can see all these chat invocations grouped into a session view in\nLangfuse Tracing:\n\nNext to sessions, you can also track individual users or add tags and metadata\nto your Langfuse traces.\n\n##  Trace more complex applications and use other Langfuse features for prompt\nmanagement and evaluation\n\nThis integration makes it easy to get started with Tracing. If your\napplication ends up growing into using custom logic or other\nframeworks/packages, all Langfuse integrations are fully interoperable.\n\nWe have also built additional features to version control and collaborate on\nprompts (langfuse [ prompt management ](https://langfuse.com/docs/prompts/get-\nstarted) ), track [ experiments ](https://langfuse.com/docs/experimentation) ,\nand [ evaluate ](https://langfuse.com/docs/scores/overview) production traces.\nFor RAG specifically, we collaborated with the RAGAS team and it\u2019s easy to run\ntheir popular eval suite on traces captured with Langfuse (see [ cookbook\n](https://langfuse.com/docs/scores/model-based-evals/ragas) ).\n\n##  Get started\n\nThe easiest way to get started is to follow the [ cookbook\n](https://docs.llamaindex.ai/en/stable/examples/callbacks/LangfuseCallbackHandler.html)\nand check out the [ docs ](https://langfuse.com/docs/integrations/llama-\nindex/get-started) .\n\n##  Feedback? Ping us\n\nWe\u2019d love to hear any feedback. Come join us on our [ community discord\n](https://langfuse.com/discord) or add your thoughts to this [ GitHub thread\n](https://github.com/orgs/langfuse/discussions/828) .\n\n", "mimetype": "text/plain", "start_char_idx": 3600, "end_char_idx": 5896, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6efb13b-fa6b-49da-ae6f-b3ab71a2c906": {"__data__": {"id_": "d6efb13b-fa6b-49da-ae6f-b3ab71a2c906", "embedding": null, "metadata": {"filename": "llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim.md", "extension": ".md", "title": "LlamaIndex Accelerates Enterprise Generative AI with NVIDIA NIM", "date": "Mar 18, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "133fdeaf-3f1e-4672-8e84-db5a8958ecf7", "node_type": "4", "metadata": {"filename": "llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim.md", "extension": ".md", "title": "LlamaIndex Accelerates Enterprise Generative AI with NVIDIA NIM", "date": "Mar 18, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim"}, "hash": "ea3cf49d736185e50d814fe2fb37ee93c89d2ffb2a96a77b1dff1240e18ee943", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "471455da-d5ed-4ccc-b356-469a805e8bec", "node_type": "1", "metadata": {}, "hash": "5aebe60878e0a70afbadf4e7d400dc40d2df0bdcd6abc3cb83a4c4824ddcffb0", "class_name": "RelatedNodeInfo"}}, "text": "Generative AI is rapidly transforming the global economy. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 58, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "471455da-d5ed-4ccc-b356-469a805e8bec": {"__data__": {"id_": "471455da-d5ed-4ccc-b356-469a805e8bec", "embedding": null, "metadata": {"filename": "llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim.md", "extension": ".md", "title": "LlamaIndex Accelerates Enterprise Generative AI with NVIDIA NIM", "date": "Mar 18, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "133fdeaf-3f1e-4672-8e84-db5a8958ecf7", "node_type": "4", "metadata": {"filename": "llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim.md", "extension": ".md", "title": "LlamaIndex Accelerates Enterprise Generative AI with NVIDIA NIM", "date": "Mar 18, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim"}, "hash": "ea3cf49d736185e50d814fe2fb37ee93c89d2ffb2a96a77b1dff1240e18ee943", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6efb13b-fa6b-49da-ae6f-b3ab71a2c906", "node_type": "1", "metadata": {"filename": "llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim.md", "extension": ".md", "title": "LlamaIndex Accelerates Enterprise Generative AI with NVIDIA NIM", "date": "Mar 18, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim"}, "hash": "54f96a396c852b82545688bf9880480bfdaf2461d639948b7f457ab39dda31ce", "class_name": "RelatedNodeInfo"}}, "text": "Enterprises are\nincreasingly looking to adopt generative AI to drive business transformation,\nbut face challenges around protecting IP, ensuring security and compliance,\nand moving smoothly from proof of concept to production.\n\nThat\u2019s why LlamaIndex is excited to announce that it is integrated with [\nNVIDIA NIM inference microservices\n](https://nvidianews.nvidia.com/news/generative-ai-microservices-for-\ndevelopers) to help enterprises seamlessly deploy generative AI at scale.\nNVIDIA NIM, part of the [ NVIDIA AI Enterprise ](https://www.nvidia.com/en-\nus/data-center/products/ai-enterprise/) software platform, optimizes inference\non more than two dozen popular AI models from NVIDIA and its partner\necosystem.\n\nLlamaIndex is an open-source tool for connecting your data to LLMs and\nextracting valuable insights. By integrating NVIDIA NIM runtimes with\nLlamaIndex\u2019s data connection capabilities, enterprises will be able to:\n\n  * Connect generative AI models hosted using NVIDIA NIM to their own proprietary data sources, allowing them to generate accurate model outputs while keeping sensitive data secure \n  * Search across and extract insights from both structured and unstructured enterprise data to enhance the knowledge and accuracy of AI models \n  * Build data-enriched generative AI applications for use cases like enterprise search, question answering, analytics, and more \n\nFor developers, using NVIDIA NIM with LlamaIndex provides a seamless path from\nexperimentation to production. They can access the NIM microservices from the\nnewly launched NVIDIA API catalog to quickly build AI applications using\nindustry-standard protocols, and then easily transition those applications to\nwork on their self-hosted NVIDIA NIM instance for enhanced security,\ncustomization, and cost effectiveness at scale.\n\nThe combination of NVIDIA NIM for optimized model inference and LlamaIndex\u2019s\ndata connection helps unlock the full potential of enterprise-level generative\nAI. We look forward to seeing the innovative applications that emerge from\nthis integration as more organizations embrace AI to transform their business.\n\nYou can expect an in-depth technical blog post about how to use NVIDIA NIM and\nLlamaIndex very soon!\n\n", "mimetype": "text/plain", "start_char_idx": 58, "end_char_idx": 2286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56e6e5dc-9928-4491-9bdf-2ef14b86a35a": {"__data__": {"id_": "56e6e5dc-9928-4491-9bdf-2ef14b86a35a", "embedding": null, "metadata": {"filename": "pii-detector-hacking-privacy-in-rag.md", "extension": ".md", "title": "PII Detector: hacking privacy in RAG", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/pii-detector-hacking-privacy-in-rag"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "34dce242-97ab-4f72-a5e0-97a98365e7be", "node_type": "4", "metadata": {"filename": "pii-detector-hacking-privacy-in-rag.md", "extension": ".md", "title": "PII Detector: hacking privacy in RAG", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/pii-detector-hacking-privacy-in-rag"}, "hash": "b6e6bc68e1f136b1dde8a1d0fd8ae026b5eb3cbcfa282fd1f2686fdf7340ab82", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66f83256-8a5d-4cfe-bd52-dccd52fb7b7f", "node_type": "1", "metadata": {}, "hash": "8b7927e5eaf493ffba69493f5877a775054ffd13e0d75b3f458dd08b8fef1fa0", "class_name": "RelatedNodeInfo"}}, "text": "A couple of days ago at the DataStax HQ, I had the chance to participate at\nthe LlamaIndex RAG-A-THON. Over the span of the weekend, we had to implement a\nsolution that leverages Retrieval Augmented Generation (RAG) technique.\n\nBecause of my background in cybersecurity, I was leaning towards the security\npitfalls and obstacles of the RAG technique. One of the first things that came\nto mind was the fact that a lot of the unstructured data used is unsanitized\nand can contain sensitive data.\n\n##  PII: What? ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 510, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66f83256-8a5d-4cfe-bd52-dccd52fb7b7f": {"__data__": {"id_": "66f83256-8a5d-4cfe-bd52-dccd52fb7b7f", "embedding": null, "metadata": {"filename": "pii-detector-hacking-privacy-in-rag.md", "extension": ".md", "title": "PII Detector: hacking privacy in RAG", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/pii-detector-hacking-privacy-in-rag"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "34dce242-97ab-4f72-a5e0-97a98365e7be", "node_type": "4", "metadata": {"filename": "pii-detector-hacking-privacy-in-rag.md", "extension": ".md", "title": "PII Detector: hacking privacy in RAG", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/pii-detector-hacking-privacy-in-rag"}, "hash": "b6e6bc68e1f136b1dde8a1d0fd8ae026b5eb3cbcfa282fd1f2686fdf7340ab82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56e6e5dc-9928-4491-9bdf-2ef14b86a35a", "node_type": "1", "metadata": {"filename": "pii-detector-hacking-privacy-in-rag.md", "extension": ".md", "title": "PII Detector: hacking privacy in RAG", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/pii-detector-hacking-privacy-in-rag"}, "hash": "61a9c7f47548c394217b098ac33a995a13ed43c0d2fa55ff953cf566778901b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e08eb439-8c47-42a9-b61d-9e0ae87e9d49", "node_type": "1", "metadata": {}, "hash": "2463c0a8aacac26218ddd7792307519d662efc3cbad346f799564f9a8d8a3a42", "class_name": "RelatedNodeInfo"}}, "text": "Why?\n\nPII stands for Personally Identifiable Information. It refers to any\ninformation that can be used to identify a specific individual. This can be\nnames, addresses, phone numbers, email addresses, social security numbers, and\nfinancial information.\n\nThere are a couple of reasons why handling PIIs is important:\n\n  1. **Privacy:** PII often includes sensitive and private details (like addresses), so protecting it preserves customers\u2019 privacy. \n  2. **Identity Theft:** PII can also lead to identity theft (e.g. one\u2019s social security number gets compromised). \n  3. **Legal Compliance:** Protecting PII is also the law. Many countries and regions have enacted laws and regulations that require organizations to protect PII. GDPR (General Data Protection Regulation) in the EU or HIPAA (Health Insurance Portability and Accountability Act) in the United States govern the way we handle PII. \n  4. **Trust and Reputation:** A data breach or mishandling of PII will severely damage one\u2019s reputation and trust. \n  5. **Financial Security:** PII may include financial information, such as credit card numbers and banking details. Compromised PII can lead to fraudulent transactions. \n  6. **National Security Concerns:** All of the above are crucial in sovereign environments. \n\n##  PII in RAG\n\nEverything listed is applicable to almost all applications leveraging RAG.\nRemember that the RAG technique contains two components \u2014 the model and the\nvector database. For this reason, each of these components need to address\nPII.\n\n###  Model\n\nLanguage models, are trained on large datasets that may contain real-world\ndata, potentially including PII and customer data. When the models generate\ntext, there is a risk that they\u2019ll produce content that includes PII. This is\neven more crucial if you\u2019re creating a multi-tenant application, and you want\nto prevent data leak. This risk can be mitigated by either filtering or\nanonymizing the response. Training the models on anonymized data that is\nstripped of any sensitive information is the better approach to prevent leaks\nof PII.\n\n###  Vector Database\n\nVector databases, just like regular databases should not persist sensitive\ninformation plainly. This kind of information should only be persisted using\nencryption, hashing, salt and access controls. Having said that, one should\nalso make sure that the similarity search returned by the Database won\u2019t\nretrieve personal data.\n\nOn top of that, various regulations such as GDPR and HIPAA still apply here.\nSo, if the original data contain PII, you might need to add another instance\nin Europe or any additional region in accordance with regulations. Persisted\ndata should be encrypted or hashed (and additionally salted).\n\n##  Introducing: Presidio\n\n[ Presidio ](https://microsoft.github.io/presidio/) is an open-source library\nmaintained by Microsoft (see our [ GitHub\n](https://github.com/microsoft/presidio) repo). It\u2019s derived from the Latin\nword praesidium which means \u201cprotection\u201d or garrison.\n\n  * It enables organizations to preserve privacy using a unified SDK. \n  * It provides fast **_identification_ ** and **_anonymization_ ** modules for private entities in text and images such as credit card numbers, names, locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more. \n\n_Disclaimer: Nothing is bulletproof. It\u2019s your responsibility to make sure\nsensitive data is anonymized._\n\n###  How Does Presidio Work?\n\n  1. **Predefined** or **custom PII recognizers** leverage _Named Entity Recognition (NER)_ , _regular expressions_ , _rule-based logic_ and _checksum_ (e.g. bitcoin address validation). \n  2. It\u2019s extensible, so you can add your own entities and your own detection mechanisms. \n  3. It\u2019s customizable, so you can create your own anonymizers, and exclude/include certain entities (e.g. exclude anonymization of geographical locations). \n\n###  LlamaIndex Post Processors\n\nThere was already some PII integration using NER models and LLMs! These were\nimplemented as post processors that run in the end of the pipeline:\n\n    \n    \n    from llama_index.postprocessor import NERPIINodePostprocessor\n    from llama_index import ServiceContext\n    from llama_index.schema import TextNode\n    \n    text = \"\"\"\n    My name is Roey Ben Chaim and my credit card number is 4095-2609-9393-4932. \n    My email is robo@presidio.site and I live in Amsterdam.\n    Have you been to a P\u00e1lmi Einarsson concert before?\n    What is the limit for card 4158112277712? My IBAN is GB90YNTU67299444055881. \n    What's your last name? ", "mimetype": "text/plain", "start_char_idx": 510, "end_char_idx": 5071, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e08eb439-8c47-42a9-b61d-9e0ae87e9d49": {"__data__": {"id_": "e08eb439-8c47-42a9-b61d-9e0ae87e9d49", "embedding": null, "metadata": {"filename": "pii-detector-hacking-privacy-in-rag.md", "extension": ".md", "title": "PII Detector: hacking privacy in RAG", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/pii-detector-hacking-privacy-in-rag"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "34dce242-97ab-4f72-a5e0-97a98365e7be", "node_type": "4", "metadata": {"filename": "pii-detector-hacking-privacy-in-rag.md", "extension": ".md", "title": "PII Detector: hacking privacy in RAG", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/pii-detector-hacking-privacy-in-rag"}, "hash": "b6e6bc68e1f136b1dde8a1d0fd8ae026b5eb3cbcfa282fd1f2686fdf7340ab82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66f83256-8a5d-4cfe-bd52-dccd52fb7b7f", "node_type": "1", "metadata": {"filename": "pii-detector-hacking-privacy-in-rag.md", "extension": ".md", "title": "PII Detector: hacking privacy in RAG", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/pii-detector-hacking-privacy-in-rag"}, "hash": "93c96ece897a1daf80a4cb30e236a510141130d348a3395cb445249da5ca42eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53887dbf-7135-4ccd-b275-8dfe44c4643e", "node_type": "1", "metadata": {}, "hash": "3ce6985d850fb73d6824ae08269fa56a51c5111b2f203420e55a67e88baa085f", "class_name": "RelatedNodeInfo"}}, "text": "Bob, it's Bob.\n    My great great grandfather was called Yulan Peres, \n    and my great great grandmother was called Jennifer Holst\n    I can't browse to your site, keep getting address 179.177.214.91 blocked error\n    Just posted a photo https://www.FilmFranchise.dk/\n    \"\"\"\n    \n    node = TextNode(text=text)\n    \n    service_context = ServiceContext.from_defaults()\n    processor = NERPIINodePostprocessor(service_context=service_context)\n    \n    from llama_index.schema import NodeWithScore\n    \n    new_nodes = processor.postprocess_nodes([NodeWithScore(node=node)])\n    print(new_nodes[0].node.get_text())\n\nRunning the above code resulted in the following:\n\n    \n    \n    My name is [PER_12] and my credit card number is 4095-2609-9393-4932. \n    My email is robo@presidio.site and I live in [LOC_123].\n    Have you been to a [PER_153] concert before?\n    What is the limit for card 4158112277712? My IBAN is GB90YNTU67299444055881. \n    What's your last name? [PER_286], it's [PER_286].\n    My great great grandfather was called [PER_339], \n    and my great great grandmother was called [PER_395]\n    I can't browse to your site, keep getting address 179.177.214.91 blocked error\n    Just posted a photo https://www.[ORG_521].dk/\n\nAs can be seen in this example, while NER models do a decent job in detecting\nPII, they might miss some entities such as IBAN code, credit card numbers,\nemails, medical license and more.\n\nPresidio detects more out of the box entities than traditional models. This is\npossible because Presidio leverages a couple of methods in detecting PII \u2014\nfrom NER models to regular expressions and rule-based logic.\n\n##  Integrating Presidio with LlamaIndex\n\nI ended up integrating **_PresidioPIINodePostprocessor_ ** that got the text\nas an input and masked it. Doing this was possible using Presidio\u2019s analyzer\nand anonymizer:\n\n    \n    \n    from presidio_analyzer import AnalyzerEngine\n    from presidio_anonymizer import AnonymizerEngine\n    \n    analyzer = AnalyzerEngine(supported_languages=[\"en\"])\n    results = analyzer.analyze(text=text, language='en')\n    engine = AnonymizerEngine()\n    new_text = engine.anonymize(text=text, analyzer_results=results)\n\nThis was pretty fun and simple. However, given the input text \u201cAlice and Bob\nare friends\u201d, the output would be: \u201c<PERSON> and <PERSON> are friends\u201d. I\ncould not have that.\n\nSo, I added a counter and mapped the original values with the masked values,\nmaking sure that whenever an entity was seen again, the previously asked value\nwas used:\n\n    \n    \n    def anonymize_function(origin, entity_type):\n        nonlocal pii_counter\n        nonlocal inverted_mapping\n        nonlocal mapping\n        if entity_type not in inverted_mapping:\n            inverted_mapping[entity_type] = {}\n        typed_mapping = inverted_mapping[entity_type]\n        if origin in typed_mapping:\n            return typed_mapping[origin]\n        new_value = f\"<{entity_type}_{pii_counter}>\"\n        typed_mapping[origin] = new_value\n        mapping[new_value]=origin\n        pii_counter+=1\n        return typed_mapping[origin]\n    \n    from presidio_analyzer import AnalyzerEngine\n    from presidio_anonymizer import AnonymizerEngine\n    from presidio_anonymizer.entities import OperatorConfig\n    \n    analyzer = AnalyzerEngine(supported_languages=[\"en\"])\n    results = analyzer.analyze(text=text, language='en')\n    engine = AnonymizerEngine()\n    new_text = engine.anonymize(text=text, analyzer_results=results, \n                                operators={\"DEFAULT\": OperatorConfig(\"custom\", \n                                params={\"lambda\": anonymize_function})})\n\n_Note: Currently presidio doesn\u2019t contain the entity type as an input\nparameter in the lambda function, so I had to add this functionality._\n\n##  Test and Benchmark\n\nOnce this was all up and running, I was able to call the newly added presidio\npost processor with the text from the previous run:\n\n    \n    \n    from llama_index.postprocessor import PresidioPIINodePostprocessor\n    from llama_index import ServiceContext\n    from llama_index.schema import TextNode\n    \n    text = \"\"\"\n    My name is Roey Ben Chaim and my credit card number is 4095-2609-9393-4932. \n    My email is robo@presidio.site and I live in Amsterdam.\n    ", "mimetype": "text/plain", "start_char_idx": 5071, "end_char_idx": 9338, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "53887dbf-7135-4ccd-b275-8dfe44c4643e": {"__data__": {"id_": "53887dbf-7135-4ccd-b275-8dfe44c4643e", "embedding": null, "metadata": {"filename": "pii-detector-hacking-privacy-in-rag.md", "extension": ".md", "title": "PII Detector: hacking privacy in RAG", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/pii-detector-hacking-privacy-in-rag"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "34dce242-97ab-4f72-a5e0-97a98365e7be", "node_type": "4", "metadata": {"filename": "pii-detector-hacking-privacy-in-rag.md", "extension": ".md", "title": "PII Detector: hacking privacy in RAG", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/pii-detector-hacking-privacy-in-rag"}, "hash": "b6e6bc68e1f136b1dde8a1d0fd8ae026b5eb3cbcfa282fd1f2686fdf7340ab82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e08eb439-8c47-42a9-b61d-9e0ae87e9d49", "node_type": "1", "metadata": {"filename": "pii-detector-hacking-privacy-in-rag.md", "extension": ".md", "title": "PII Detector: hacking privacy in RAG", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/pii-detector-hacking-privacy-in-rag"}, "hash": "4339e94cdc3927157c69de3579a131822cd405aafd46177b27c508b004816ebd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d9051207-6887-4310-84ec-d38f342dbb1e", "node_type": "1", "metadata": {}, "hash": "67bb801b8e2b197107d9aefa0fa148ad0cd198e3103701687ad82bf404010f54", "class_name": "RelatedNodeInfo"}}, "text": "Have you been to a P\u00e1lmi Einarsson concert before?\n    What is the limit for card 4158112277712? My IBAN is GB90YNTU67299444055881. \n    What's your last name? ", "mimetype": "text/plain", "start_char_idx": 4911, "end_char_idx": 5071, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d9051207-6887-4310-84ec-d38f342dbb1e": {"__data__": {"id_": "d9051207-6887-4310-84ec-d38f342dbb1e", "embedding": null, "metadata": {"filename": "pii-detector-hacking-privacy-in-rag.md", "extension": ".md", "title": "PII Detector: hacking privacy in RAG", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/pii-detector-hacking-privacy-in-rag"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "34dce242-97ab-4f72-a5e0-97a98365e7be", "node_type": "4", "metadata": {"filename": "pii-detector-hacking-privacy-in-rag.md", "extension": ".md", "title": "PII Detector: hacking privacy in RAG", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/pii-detector-hacking-privacy-in-rag"}, "hash": "b6e6bc68e1f136b1dde8a1d0fd8ae026b5eb3cbcfa282fd1f2686fdf7340ab82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "53887dbf-7135-4ccd-b275-8dfe44c4643e", "node_type": "1", "metadata": {"filename": "pii-detector-hacking-privacy-in-rag.md", "extension": ".md", "title": "PII Detector: hacking privacy in RAG", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/pii-detector-hacking-privacy-in-rag"}, "hash": "e949ec328c29be547af2165cb224282a49a00d2642db4c56ec8f6c011e958016", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "905219a8-1a01-4c15-9ddb-aeb8a4a2b17c", "node_type": "1", "metadata": {}, "hash": "77b8fa2cd85a251a39322a0881f17d67f2c1c4f431ac7002b828b2d67c52821b", "class_name": "RelatedNodeInfo"}}, "text": "Bob, it's Bob.\n    My great great grandfather was called Yulan Peres, \n    and my great great grandmother was called Jennifer Holst\n    I can't browse to your site, keep getting address 179.177.214.91 blocked error\n    Just posted a photo https://www.FilmFranchise.dk/\n    \"\"\"\n    \n    node = TextNode(text=text)\n    \n    service_context = ServiceContext.from_defaults()\n    processor = PresidioPIINodePostprocessor(service_context=service_context)\n    \n    from llama_index.schema import NodeWithScore\n    \n    new_nodes = processor.postprocess_nodes([NodeWithScore(node=node)])\n    print(new_nodes[0].node.get_text())\n\nRunning the above code resulted in the following:\n\n    \n    \n    My name is <PERSON_12> and my credit card number is <CREDIT_CARD_11>. \n    My email is <EMAIL_ADDRESS_10> and I live in <LOCATION_9>.\n    ", "mimetype": "text/plain", "start_char_idx": 9498, "end_char_idx": 10322, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "905219a8-1a01-4c15-9ddb-aeb8a4a2b17c": {"__data__": {"id_": "905219a8-1a01-4c15-9ddb-aeb8a4a2b17c", "embedding": null, "metadata": {"filename": "pii-detector-hacking-privacy-in-rag.md", "extension": ".md", "title": "PII Detector: hacking privacy in RAG", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/pii-detector-hacking-privacy-in-rag"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "34dce242-97ab-4f72-a5e0-97a98365e7be", "node_type": "4", "metadata": {"filename": "pii-detector-hacking-privacy-in-rag.md", "extension": ".md", "title": "PII Detector: hacking privacy in RAG", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/pii-detector-hacking-privacy-in-rag"}, "hash": "b6e6bc68e1f136b1dde8a1d0fd8ae026b5eb3cbcfa282fd1f2686fdf7340ab82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d9051207-6887-4310-84ec-d38f342dbb1e", "node_type": "1", "metadata": {"filename": "pii-detector-hacking-privacy-in-rag.md", "extension": ".md", "title": "PII Detector: hacking privacy in RAG", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/pii-detector-hacking-privacy-in-rag"}, "hash": "c399fb73e96db80d0af9b3e6d3029036d580018d82731d854c5b8a29a4329786", "class_name": "RelatedNodeInfo"}}, "text": "Have you been to a <PERSON_8> concert before?\n    What is the limit for card <CREDIT_CARD_7>? My IBAN is <IBAN_CODE_6>. \n    What's your last name? <PERSON_5>, it's <PERSON_5>.\n    My great great grandfather was called <PERSON_4>, \n    and my great great grandmother was called <PERSON_3>\n    I can't browse to your site, keep getting address <IP_ADDRESS_2> blocked error\n    Just posted a photo <URL_1>\n\nOverall Presidio detected 12 entities while the other NER solution detected 8.\nNotice that credit card numbers, email address, IBAN, IP address and the URL\n(at least some of it) weren\u2019t detected.\n\nI was curious to see how the parsing of these strings would work on the LLM,\nso I populated the index and queried the following:\n\n    \n    \n    from llama_index import VectorStoreIndex\n    \n    index = VectorStoreIndex([n.node for n in new_nodes])\n    response = index.as_query_engine().query(\n        \"What is my name?\"\n    )\n    print(response)\n\nWhich resulted in:\n\n    \n    \n    Your name is <PERSON_12>.\n\n##  How It Ended\n\nAnyway, this project won the 3rd place (in the continuous track) in the RAG-A-\nTHON.\n\nNote: this picture doesn\u2019t contain PII\n\n###  Update\n\nPresidio is now fully integrated into LlamaIndex as a post processor, follow\nthis [ notebook ](https://github.com/run-\nllama/llama_index/blob/ff51e2cbf9815a44ebade9a1382216435d7f4bc8/docs/examples/node_postprocessor/PII.ipynb#L289)\nto learn how to use Presidio for PII masking. The next steps would be to add\nmore customization and anonymization options.\n\n", "mimetype": "text/plain", "start_char_idx": 10322, "end_char_idx": 11846, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc135e87-5378-4b9a-9760-d4c8bd1f81e8": {"__data__": {"id_": "fc135e87-5378-4b9a-9760-d4c8bd1f81e8", "embedding": null, "metadata": {"filename": "launching-the-first-genai-native-document-parsing-platform.md", "extension": ".md", "title": "Launching the first GenAI-native document parsing platform", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/launching-the-first-genai-native-document-parsing-platform"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f6a5ddab-07c3-418f-87a9-4d00c7c5da26", "node_type": "4", "metadata": {"filename": "launching-the-first-genai-native-document-parsing-platform.md", "extension": ".md", "title": "Launching the first GenAI-native document parsing platform", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/launching-the-first-genai-native-document-parsing-platform"}, "hash": "18d344b00ad6906d9834bd6798559327ee3188e44559a4bd64674f814f481874", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d12785df-5e5a-46c7-b887-60400c3257c1", "node_type": "1", "metadata": {}, "hash": "f81185cde2a2bfe25cb43fa2902ff8931af37e9f9d03565839f33402d6c50324", "class_name": "RelatedNodeInfo"}}, "text": "Our mission at LlamaIndex is to connect the world\u2019s data to the power of LLMs,\nand today we\u2019re pleased to announce our latest big step towards that goal with\nthe world\u2019s first GenAI-native document parsing platform, LlamaParse.\n\nWe launched the first public version of LlamaParse [ 3 weeks ago\n](https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-\naf8cedf9006b) and the response has been huge with well over 2,000 users\nparsing over 1 million pages! We\u2019ve been hard at work releasing hundreds of\nbug fixes and new features since then, and today we\u2019re releasing a game-\nchanging new feature, **GenAI-powered parsing instructions** .\n\n##  Using LLMs for world-class parsing\n\nThe key insight behind parsing instructions is that **you know what kind of\ndocuments you have** , so you already know what kind of output you want. Why\nmake the parser guess when an LLM-enabled parser can take simple, natural-\nlanguage instructions from you and provide radically better parsing results?\n\n###  Example 1: rich table support\n\nSince we first released LlamaParse it has featured [ industry-leading table\nextraction ](https://github.com/run-\nllama/llama_parse/blob/main/examples/demo_advanced.ipynb) capabilities. Under\nthe hood, this has been using LLM intelligence since the start. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1288, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d12785df-5e5a-46c7-b887-60400c3257c1": {"__data__": {"id_": "d12785df-5e5a-46c7-b887-60400c3257c1", "embedding": null, "metadata": {"filename": "launching-the-first-genai-native-document-parsing-platform.md", "extension": ".md", "title": "Launching the first GenAI-native document parsing platform", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/launching-the-first-genai-native-document-parsing-platform"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f6a5ddab-07c3-418f-87a9-4d00c7c5da26", "node_type": "4", "metadata": {"filename": "launching-the-first-genai-native-document-parsing-platform.md", "extension": ".md", "title": "Launching the first GenAI-native document parsing platform", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/launching-the-first-genai-native-document-parsing-platform"}, "hash": "18d344b00ad6906d9834bd6798559327ee3188e44559a4bd64674f814f481874", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc135e87-5378-4b9a-9760-d4c8bd1f81e8", "node_type": "1", "metadata": {"filename": "launching-the-first-genai-native-document-parsing-platform.md", "extension": ".md", "title": "Launching the first GenAI-native document parsing platform", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/launching-the-first-genai-native-document-parsing-platform"}, "hash": "b63c01c1ef4e39f479a7ad1e647afc0ba92670038d07bd19b31375a5ee3da5a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9143fa24-704e-4121-8f67-9c7884d86800", "node_type": "1", "metadata": {}, "hash": "f7b86e07b88c73867ed9c466652c8c5a5891c21ca071b64af2cb4a595ab0f3ef", "class_name": "RelatedNodeInfo"}}, "text": "It seamlessly\nintegrates with the advanced indexing/retrieval capabilities that the open-\nsource framework offers, enabling users to build state-of-the-art document\nRAG. Now with JSON mode (see below) and parsing instructions, you can take\nthis even further.\n\n###  Example 2: parsing comic books\n\nParsing translated manga presents a particular challenge for a parser since a\nregular parser interprets the panels as cells in a table, and the reading\norder is right-to-left even though the book is in English, as shown in this\nextract from \"The manga guide to calculus\", by Hiroyuki Kojima:\n\nUsing LlamaParse, you can give the parser plain, English-language instructions\non what to do:\n\n    \n    \n    The provided document is a manga comic book. \n    Most pages do NOT have title. ", "mimetype": "text/plain", "start_char_idx": 1288, "end_char_idx": 2067, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9143fa24-704e-4121-8f67-9c7884d86800": {"__data__": {"id_": "9143fa24-704e-4121-8f67-9c7884d86800", "embedding": null, "metadata": {"filename": "launching-the-first-genai-native-document-parsing-platform.md", "extension": ".md", "title": "Launching the first GenAI-native document parsing platform", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/launching-the-first-genai-native-document-parsing-platform"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f6a5ddab-07c3-418f-87a9-4d00c7c5da26", "node_type": "4", "metadata": {"filename": "launching-the-first-genai-native-document-parsing-platform.md", "extension": ".md", "title": "Launching the first GenAI-native document parsing platform", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/launching-the-first-genai-native-document-parsing-platform"}, "hash": "18d344b00ad6906d9834bd6798559327ee3188e44559a4bd64674f814f481874", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d12785df-5e5a-46c7-b887-60400c3257c1", "node_type": "1", "metadata": {"filename": "launching-the-first-genai-native-document-parsing-platform.md", "extension": ".md", "title": "Launching the first GenAI-native document parsing platform", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/launching-the-first-genai-native-document-parsing-platform"}, "hash": "c2f62806f3977dff726aea82927c6c46a9bd7c0f3991e28a658e9b3e275cc109", "class_name": "RelatedNodeInfo"}}, "text": "It does not contain tables. \n    Try to reconstruct the dialogue happening in a cohesive way.\n\n(You can see the full code in our [ demonstration notebook\n](https://colab.research.google.com/drive/1dO2cwDCXjj9pS9yQDZ2vjg-0b5sRXQYo) ,\nincluding what it looks like to parse this without the instructions)\n\nThe result is a perfect parse!\n\n    \n    \n    # The Asagake Times\n    \n    Sanda-Cho Distributor\n    \n    A newspaper distributor?\n    \n    Do I have the wrong map?\n\n###  Example 3: mathematical equations\n\nAnother challenging format for parsing is complex mathematical equations (by\ncoincidence, the manga we picked as an example is all about how to do\nmathematics):\n\nTo parse this, we take the same instructions as before and add one sentence: `\nOutput any math equation in LATEX markdown (between $$) ` . The result of\nparsing is clear LaTeX instructions, which render the equations perfectly:\n\n###  Anything an LLM can do, our parser can do\n\nYou can use this kind of natural-language instruction to do all sorts of\nadvanced pre-processing on your documents \u2014 simplify language, include\nsentiment analysis, translate them to another language! We can\u2019t wait to see\nwhat you do with the power of LlamaParse.\n\n##  JSON mode\n\nParsing instructions are definitely the headline feature, but we have dozens\nof other features new to LlamaParse since launch. A standout is JSON mode, a\nrich programmatic format perfect for when you want more precision about\nexactly what you want to parse out. JSON mode\u2019s output includes\n\n  * the full structure of the document that was parsed \n  * tables, text and headings marked \n  * tables are available as CSV and JSON \n  * images are marked and available for extraction (see below) \n  * a wealth of metadata about each node \n\nIf you are building a custom RAG strategy JSON mode gives you everything you\nneed to build it. Check out our [ JSON mode examples!\n](https://github.com/run-llama/llama_parse/blob/main/examples/demo_json.ipynb)\n\n##  Image extraction\n\nOne of the best features of JSON mode is image extraction: every page that\ncontains images comes with a list of images, marked up with metadata including\ntheir size and position on the page, and you can [ retrieve these images\ndirectly\n](https://api.cloud.llamaindex.ai/docs#/parsing/get_job_image_result_api_parsing_job__job_id__result_image__name__get)\nand [ include them in your indexing ](https://github.com/run-\nllama/llama_parse/blob/main/examples/demo_json.ipynb) to extract even more\ninformation from your complex, image-heavy documents.\n\n##  Expanded document types\n\nWe launched LlamaParse with exceptional support for PDFs, and we have\ncontinued to expand its capability every day. We\u2019ve also added support for a\nlarge array of document types:\n\n  * Microsoft Word (.doc, .docx) \n  * Microsoft PowerPoint (.pptx) \n  * Rich Text Format (.rtf) \n  * Apple Pages (.pages) \n  * Apple Keynote (.key) \n  * ePub books (.epub) \n  * And dozens more! \n\nAll of these document types \u201cjust work\u201d without any additional work on your\npart, and we are constantly expanding the list of supported file types. Check\nout this [ demo notebook\n](https://colab.research.google.com/drive/1B5OlhHU8ewppuWf_d4dhZJW2vdYiqZ95?usp=sharing)\nwhere we demonstrate parsing a PowerPoint file.\n\n##  And one more thing\u2026 unlimited parsing!\n\nThe huge demand for LlamaParse has included many people asking to go beyond\nour free daily limits via paid plans, and we\u2019re happy to answer those\nrequests. Our pricing is simple:\n\n  * 7000 pages/week are free \n  * Additional pages are $0.003/page, or $3 per 1000 pages \n  * Maximum size for one document is 750 pages \n\nAnd of course we retain our generous free tier of 1000 pages/day.\n\nThe public version of LlamaParse is a hosted service. If you want to extend\nLlamaParse capabilities to build advanced document RAG, or wish to deploy\nLlamaParse in a private cloud, [ get in touch.\n](https://www.llamaindex.ai/contact)\n\n", "mimetype": "text/plain", "start_char_idx": 2067, "end_char_idx": 5994, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fa83e3ed-d876-4370-9e1f-55f8cd65502f": {"__data__": {"id_": "fa83e3ed-d876-4370-9e1f-55f8cd65502f", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-03-12.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-12", "date": "Mar 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-12"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac98b60c-713a-42c1-a0ad-aac8ad68c12a", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-03-12.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-12", "date": "Mar 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-12"}, "hash": "eff966067ef6777e95e01b6009b8fcb813fce40524364c91394499f253352c9a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dab6a365-d236-4462-b42a-b9a8b93b2523", "node_type": "1", "metadata": {}, "hash": "716819e02ef9ef93b0a944ffe36e37631eee840cd0ad0d9e746c3d9b4f530410", "class_name": "RelatedNodeInfo"}}, "text": "Salutations, LlamaIndex fans!\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 31, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dab6a365-d236-4462-b42a-b9a8b93b2523": {"__data__": {"id_": "dab6a365-d236-4462-b42a-b9a8b93b2523", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-03-12.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-12", "date": "Mar 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-12"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac98b60c-713a-42c1-a0ad-aac8ad68c12a", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-03-12.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-12", "date": "Mar 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-12"}, "hash": "eff966067ef6777e95e01b6009b8fcb813fce40524364c91394499f253352c9a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa83e3ed-d876-4370-9e1f-55f8cd65502f", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-03-12.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-12", "date": "Mar 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-12"}, "hash": "82bddfbef788c8754354c6a92427efff1cbaa4a44018b9ded2a940874149c44e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "381a8915-892c-4638-bdcb-eaff7a0295eb", "node_type": "1", "metadata": {}, "hash": "1a84da35e86b14241be96494c275ce8e2d490c53c879e72133460b72a39a11b9", "class_name": "RelatedNodeInfo"}}, "text": "It's been another thrilling week in LlamaLand! With the release of Anthropic\u2019s\nnew models Claude-3 Opus, Sonnet, and Haiku, we have numerous tutorials,\ncookbooks, and updates to share with you.\n\nA quick reminder: we are running our [ first ever user survey\n](https://www.surveymonkey.com/r/9W8VX2H) . It takes only 3 minutes and it\nhelps us out a lot! Now let\u2019s dive in.\n\n", "mimetype": "text/plain", "start_char_idx": 31, "end_char_idx": 403, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "381a8915-892c-4638-bdcb-eaff7a0295eb": {"__data__": {"id_": "381a8915-892c-4638-bdcb-eaff7a0295eb", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-03-12.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-12", "date": "Mar 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-12"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac98b60c-713a-42c1-a0ad-aac8ad68c12a", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-03-12.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-12", "date": "Mar 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-12"}, "hash": "eff966067ef6777e95e01b6009b8fcb813fce40524364c91394499f253352c9a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dab6a365-d236-4462-b42a-b9a8b93b2523", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-03-12.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-12", "date": "Mar 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-12"}, "hash": "c801e974a466be35c41e4d2d8dc6c156e8cac8f0455f5961ecf1ba6209e9093b", "class_name": "RelatedNodeInfo"}}, "text": "**The highlights:**\n\n  1. **LlamaParse JSON Mode** : A new feature that transforms PDF content into structured data, simplifying RAG pipeline development for complex documents containing images, text and tables. [ Notebook ](https://github.com/run-llama/llama_parse/blob/main/examples/demo_json.ipynb) , [ Tweet ](https://x.com/llama_index/status/1765439865351766135?s=20) . \n  2. **Hierarchical Code Splitting** : Enhance code understanding with a novel technique that organizes large codebases into a hierarchical structure for improved navigation and task-solving. [ LlamaPack ](https://llamahub.ai/l/llama-packs/llama-index-packs-code-hierarchy?from=llama-packs) , [ Tweet ](https://x.com/llama_index/status/1766152269874266170?s=20) . \n  3. **Anthropic Cookbook Series** : Learn to build various LLM applications with Claude 3, ranging from simple to complex, through detailed guides and tutorials. [ Cookbooks ](https://github.com/anthropics/anthropic-cookbook/tree/main/third_party/LlamaIndex) , [ Tweet ](https://x.com/llama_index/status/1767218890856358115?s=20) . \n\n**Feature Releases and Enhancements:**\n\n  * We launched LlamaParse JSON Mode, a new feature that structures text and images from PDFs into a dict format. With the integration of multimodal models like claude-3 opus, it's now simpler to develop RAG pipelines for complex PDFs containing text, images, and tables. [ Notebook ](https://github.com/run-llama/llama_parse/blob/main/examples/demo_json.ipynb) , [ Tweet ](https://x.com/llama_index/status/1765439865351766135?s=20) . \n  * We launched a novel hierarchical code splitting technique to enhance RAG/agents for code comprehension, featuring ` CodeHierarchyNodeParser ` by ryanpeach. This method breaks down large code files into a hierarchical structure, enabling a knowledge graph-like approach for efficient code navigation and task-solving. [ LlamaPack ](https://llamahub.ai/l/llama-packs/llama-index-packs-code-hierarchy?from=llama-packs) , [ Tweet ](https://x.com/llama_index/status/1766152269874266170?s=20) . \n  * We integrated with Videodb to run RAG over video streams using LlamaIndex. This tool allows you to upload, search, and stream videos based on spoken words or visual scenes, now available as a built-in retriever in LlamaIndex. [ Tweet ](https://x.com/llama_index/status/1765481657765912599?s=20) . \n\n**Demos:**\n\n  * [ **Build an AI Browser Copilot** ](https://github.com/mithril-security/LaVague?tab=readme-ov-file) : a project by [ **Daniel Huynh** ](https://twitter.com/dhuynh95) that demonstrates how to create a browser agent using RAG, local embeddings, and Mixtral to execute browser tasks from a Colab notebook, showcased with a video on navigating HuggingFace datasets. \n  * [ RAG over your code ](https://lightning.ai/lightning-ai/studios/chat-with-your-code-using-rag) : a project by [ **Akshay** ](https://twitter.com/akshay_pachaar) on creating a local code assistant using LlamaIndex, MistralAI, and Streamlit to index and query GitHub repositories, offering a foundational guide for advanced code QA. \n  * [ https://nething.xyz/: ](https://nething.xyz/:) a project by Raymond Weitekamp on generating production-ready 3D CAD models from text prompts. It uses LLM code generation to create commands to generate printable 3D objects from text prompts using LlamaIndex. [ Blog ](https://www.llamaindex.ai/blog/unlocking-the-3rd-dimension-for-generative-ai-part-1) , [ Tweet ](https://x.com/llama_index/status/1764771352077320517?s=20) . \n\n**Guides:**\n\n  * [ Guide ](https://x.com/llama_index/status/1767218890856358115?s=20) to the Anthropic Cookbook Series: Create context-augmented LLM apps using Claude 3, from basic RAG to advanced agents, through six notebooks and four videos. \n  * Video [ guide ](https://www.youtube.com/watch?v=VRDNyFj-xeE) exploring diverse applications of Claude-3 with LlamaIndex. tooling, covering Vanilla RAG, routing, sub-question query planning, structured data extraction, text-to-SQL, and agents\u2014a perfect starter kit for Claude enthusiasts. \n\n**Tutorials:**\n\n  * [ Tutorial ](https://ai.gopubby.com/leveraging-llamaindex-step-wise-react-agent-for-efficient-document-handling-3a0f92b9ca22) by [ Ankush k Singal ](https://medium.com/@andysingal?source=post_page-----3a0f92b9ca22--------------------------------) on building local LLM agents with Llama.cpp for step-wise execution and incorporating human feedback during execution. \n  * [ \u201cRAG over Complex PDFs V2\" ](https://www.youtube.com/watch?v=7qsxz2rURG4) : a comprehensive tutorial by [ **AI Makerspace** ](https://twitter.com/AIMakerspace) on crafting advanced RAG pipelines for handling messy PDFs with LlamaParse and LlamaIndex, where naive RAG falls short. \n  * [ Tutorial ](https://blog.streamlit.io/build-a-real-time-rag-chatbot-google-drive-sharepoint/) on building a real-time RAG chatbot using Google Drive and Sharepoint by [ Anup Surendran ](https://blog.streamlit.io/author/anup/) and [ Berke Can Rizai ](https://blog.streamlit.io/author/berke/) . \n  * Step-by-step [ tutorial ](https://jina.ai/news/precise-rag-with-jina-reranker-and-llamaindex/) to improve the quality of your RAG application using JinaAI reranker, LlamaIndex, and MistralAI. \n\n**Webinars:**\n\n  * [ Webinar ](https://www.youtube.com/watch?v=Bhnq8grQm5Y) with [ Parth Sarthi ](https://twitter.com/parthsarthi03) , lead author of RAPTOR - Tree-Structured Indexing and Retrieval. \n\nEvents:\n\n  * We are hosting a RAG [ meetup ](https://www.meetup.com/paris-retrieval-augmented-generation-group/events/299374545/) in Paris on March 27th featuring talks on advanced RAG strategies, building a RAG CLI, and the significance of open-source RAG in business. \n\n", "mimetype": "text/plain", "start_char_idx": 403, "end_char_idx": 6084, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "588475ff-3808-4e3d-b7e3-83860dcfab8a": {"__data__": {"id_": "588475ff-3808-4e3d-b7e3-83860dcfab8a", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-03-05.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-05", "date": "Mar 5, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-05"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc3fc09a-4c0f-4c9d-af9b-9f3313592874", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-03-05.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-05", "date": "Mar 5, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-05"}, "hash": "f343dd7079f73bf6962dbf5ebe9997b51bb881a96c8a03234699806fc4ec38ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16b414d6-12a7-4c30-b8d2-7aee4d0dabc3", "node_type": "1", "metadata": {}, "hash": "6d18eca785fe5d06f28e3061575b6d18e8d0ed65d3fcfc5ea60c4ca054771080", "class_name": "RelatedNodeInfo"}}, "text": "Greetings, LlamaIndex devotees!\n\nIt was another fun week to be at the center of the LLM universe, and we have\ntons to share!\n\n**The highlights:**\n\n  * We shared our thoughts on the future of **long-context RAG.** As LLMs with context windows over 1M tokens begin to appear, what changes about RAG, and how will LlamaIndex evolve? [ Tweet ](https://twitter.com/llama_index/status/1763620476847632744) , [ Blog post ](https://www.llamaindex.ai/blog/towards-long-context-rag)\n  * **llama-index-networks** lets you build a super-RAG application by combining answers from independent RAG apps over the network. [ Tweet ](https://twitter.com/llama_index/status/1762552542981230769) , [ Blog post ](https://www.llamaindex.ai/blog/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f) , [ repo ](https://github.com/run-llama/llama_index/tree/main/llama-index-networks)\n  * People loved our release of LlamaParse, a world-beating PDF parsing service, so we made it even better! [ Tweet ](https://twitter.com/llama_index/status/1763008808216060186) , [ blog post ](https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b)\n\n**Feature Releases and Enhancements:**\n\n  * We released a new llama-index-networks feature that lets you combine multiple independent RAG applications over the network, allowing you to run a single query across all the applications and get a single, combined answer. [ Tweet ](https://twitter.com/llama_index/status/1762552542981230769) , [ Blog post ](https://www.llamaindex.ai/blog/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f) , [ repo ](https://github.com/run-llama/llama_index/tree/main/llama-index-networks)\n  * Inference engine [ Groq ](https://groq.com/) wowed us and the world with their incredibly fast query times and we were delighted to introduce first-class support for their LLM APIs. [ Tweet ](https://twitter.com/llama_index/status/1762869201222639927) , [ notebook ](https://docs.llamaindex.ai/en/stable/examples/llm/groq.html)\n  * Users love LlamaParse, the world-beating PDF parsing service we released last week. We pushed improved parsing and OCR support for 81+ languages! We also increased the usage cap from 1k to 10k pages per day. [ Tweet ](https://twitter.com/llama_index/status/1763008808216060186) , [ blog post ](https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b)\n  * We migrated our [ blog off of Medium ](https://www.llamaindex.ai/blog) , we hope you like the new look and the absence of nag screens! \n  * RAPTOR is a new tree-structured technique for advanced RAG; we turned the paper into a LlamaPack, allowing you to use the new technique in one line of code. [ Tweet ](https://twitter.com/llama_index/status/1763972097628684607) , [ package ](https://llamahub.ai/l/llama-packs/llama-index-packs-raptor?from=) , [ notebook ](https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-raptor/examples/raptor.ipynb) , [ original paper ](https://arxiv.org/pdf/2401.18059.pdf)\n\n**Demos:**\n\n  * The Koda Retriever is a new retrieval concept: hybrid search where the alpha parameter controlling the importance of vector search vs. keyword search is tuned on a per-query basis by the LLM itself, based on a few-shot examples. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3298, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16b414d6-12a7-4c30-b8d2-7aee4d0dabc3": {"__data__": {"id_": "16b414d6-12a7-4c30-b8d2-7aee4d0dabc3", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-03-05.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-05", "date": "Mar 5, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-05"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc3fc09a-4c0f-4c9d-af9b-9f3313592874", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-03-05.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-05", "date": "Mar 5, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-05"}, "hash": "f343dd7079f73bf6962dbf5ebe9997b51bb881a96c8a03234699806fc4ec38ac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "588475ff-3808-4e3d-b7e3-83860dcfab8a", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-03-05.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-05", "date": "Mar 5, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-05"}, "hash": "02380b95e7caa4cccc7d84ec007ed2322f792b66456ecfbef54ed5baa7afd1a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa2e2706-accd-4073-a6a4-6d2f4745cb40", "node_type": "1", "metadata": {}, "hash": "9ebc958f3af3a1ca12e768b8ed64e6408f355a4a7d0d39c70e73f34e8d743218", "class_name": "RelatedNodeInfo"}}, "text": "[ Tweet ](https://twitter.com/llama_index/status/1763252392639042024) , [ notebook ](https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-koda-retriever/examples/alpha_tuning.ipynb) , [ package ](https://llamahub.ai/l/llama-packs/llama-index-packs-koda-retriever?from=) , [ blog post ](https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00)\n  * [ Mixedbread.ai ](http://Mixedbread.ai) released some state-of-the-art rerankers that perform better than anything seen before; we whipped up a quick cookbook to show you how to use them directly in LlamaIndex. ", "mimetype": "text/plain", "start_char_idx": 3298, "end_char_idx": 3963, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa2e2706-accd-4073-a6a4-6d2f4745cb40": {"__data__": {"id_": "aa2e2706-accd-4073-a6a4-6d2f4745cb40", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-03-05.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-05", "date": "Mar 5, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-05"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc3fc09a-4c0f-4c9d-af9b-9f3313592874", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-03-05.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-05", "date": "Mar 5, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-05"}, "hash": "f343dd7079f73bf6962dbf5ebe9997b51bb881a96c8a03234699806fc4ec38ac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16b414d6-12a7-4c30-b8d2-7aee4d0dabc3", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-03-05.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-05", "date": "Mar 5, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-05"}, "hash": "f0af358bd9e918bcb5f8ac49ad120264446b624ddb0cc1164134066220efbf2f", "class_name": "RelatedNodeInfo"}}, "text": "[ Tweet ](https://twitter.com/llama_index/status/1763654691886674134) , [ Notebook ](https://docs.llamaindex.ai/en/latest/cookbooks/mixedbread_reranker.html) , [ blog post ](https://www.mixedbread.ai/blog/mxbai-rerank-v1)\n\n**Guides:**\n\n  * **Function-calling cookbook with open source models** shows you how to use Fireworks AI\u2019s OpenAI-compatible API to use all native LlamaIndex support for function calling. [ Notebook ](https://docs.llamaindex.ai/en/stable/examples/llm/fireworks_cookbook.html) , [ Tweet ](https://twitter.com/llama_index/status/1762532341795487815) . \n  * We released a best practices cookbook showing how to use LlamaParse, our amazing PDF parser. [ Tweet ](https://twitter.com/llama_index/status/1763729200106840548) , [ notebook ](https://github.com/run-llama/llama_parse/blob/main/examples/demo_table_comparisons.ipynb)\n  * A **comprehensive guide to semantic chunking for RAG** by Florian June covers embedding-based chunking, BERT-based chunking techniques, and LLM-based chunking for everything you need to know about this highly effective technique to improve retrieval quality. [ Tweet ](https://twitter.com/llama_index/status/1764335221141631471) , [ Blog post ](https://pub.towardsai.net/advanced-rag-05-exploring-semantic-chunking-97c12af20a4d)\n\n**Tutorials:**\n\n  * Our own [ Andrei ](https://twitter.com/_nerdai_) presented a notebook on building Basic RAG with LlamaIndex at [ Vector Institute ](https://vectorinstitute.ai/) \u2019s RAG bootcamp. [ Tweet ](https://twitter.com/_nerdai_/status/1762924582183350782) , [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/presentations/materials/2024-02-28-rag-bootcamp-vector-institute.ipynb)\n  * ClickHouse presented an in-depth tutorial using LlamaIndex to query both structured and unstructured data, and built a bot that queries Hacker News to find what people are saying about the most popular technologies. [ Tweet ](https://twitter.com/llama_index/status/1763282902358585445) , [ blog post ](https://clickhouse.com/blog/building-hackernews-stackoverflow-chatbot-with-llamaindex-and-clickhouse)\n  * POLM (Python, OpenAI, LlamaIndex, MongoDB) is a new reference architecture for building RAG applications and MongoDB has a beautiful, step-by-step tutorial for building it out. [ Tweet ](https://twitter.com/llama_index/status/1764078471276642469) , [ blog post ](https://www.mongodb.com/developer/products/atlas/rag-with-polm-stack-llamaindex-openai-mongodb/)\n\n**Webinar:**\n\n  * Our CEO Jerry Liu will do a joint webinar with Adam Kamor of [ Tonic.ai ](http://Tonic.ai) about building fully-local RAG applications with Ollama and Tonic. People love local models! [ Tweet ](https://twitter.com/llama_index/status/1763304038442192978) , [ Registration page ](https://www.tonic.ai/webinars/preserve-privacy-using-local-rag-with-tonic-ai-llamaindex)\n  * Jerry also did a webinar with [ Traceloop ](https://www.traceloop.com/) on leveling up your LLM application with observability. [ Tweet ](https://twitter.com/llama_index/status/1763364010676900080) , [ YouTube ](https://www.youtube.com/watch?v=if-2elFrfgM)\n  * Our hackathon at the beginning of February was a huge success! Check out this webinar in which we invited the winners to come and talk about their projects. [ Tweet ](https://twitter.com/llama_index/status/1764020728427667605) , [ YouTube ](https://www.youtube.com/watch?v=THkeXnwwSw4) . \n\n", "mimetype": "text/plain", "start_char_idx": 3963, "end_char_idx": 7364, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "acac801a-11e9-459b-8301-46c67e452175": {"__data__": {"id_": "acac801a-11e9-459b-8301-46c67e452175", "embedding": null, "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cd0d369d-13e9-421e-aa70-c388b567acdb", "node_type": "4", "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "8dbc3145c7b909a6f9a4bd4ea8ea7b7ac8679e6d4b729522cd4258a0aa5323e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02602e9d-2105-4940-8370-03865c493134", "node_type": "1", "metadata": {}, "hash": "93c034cef567f6e1caea01c8b6e88f5c59a0ea50b583872ea0f3115c06d99a94", "class_name": "RelatedNodeInfo"}}, "text": "Google recently released [ Gemini 1.5 Pro with a 1M context window\n](https://blog.google/technology/ai/google-gemini-next-generation-model-\nfebruary-2024/#gemini-15) , available to a limited set of developers and\nenterprise customers. Its performance has caught the imagination of [ AI\nTwitter ](https://x.com/alliekmiller/status/1760522046251962459?s=20) . It [\nachieves 99.7% recall in the \u201cNeedle in a Haystack\u201d\n](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf)\nexperiment popularized by [ Greg Kamradt\n](https://x.com/GregKamradt/status/1722386725635580292?s=20) . Early users\nhave shared results feeding dozens of research papers, financial reports at\nonce and report impressive results in terms of its ability to synthesize\nacross vast troves of information.\n\nNaturally, this begs the question - is RAG dead? Some [ folks think so\n](https://twitter.com/francis_yao_/status/1759962812229800012?s=46&t=pfae6EnnrBq2o8ok0KpVqw)\n, while others [ disagree\n](https://x.com/ptsi/status/1758511307433947625?s=20) . ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1041, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02602e9d-2105-4940-8370-03865c493134": {"__data__": {"id_": "02602e9d-2105-4940-8370-03865c493134", "embedding": null, "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cd0d369d-13e9-421e-aa70-c388b567acdb", "node_type": "4", "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "8dbc3145c7b909a6f9a4bd4ea8ea7b7ac8679e6d4b729522cd4258a0aa5323e0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "acac801a-11e9-459b-8301-46c67e452175", "node_type": "1", "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "2d449826b81ebb58266d3a14c2da11ffbeb65ce68dc617bc1279766cf5852e5b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "718c3d47-5861-4b73-9869-ce3c66aa7553", "node_type": "1", "metadata": {}, "hash": "42c89f63d51cbe3b20cb37dac5e9207b011411693de9f0a569877599c5812ce8", "class_name": "RelatedNodeInfo"}}, "text": "Those in the first\ncamp make valid points. Most small data use cases can fit within a 1-10M\ncontext window. Tokens will get cheaper and faster to process over time.\nHaving an LLM natively interleave retrieval/generation via attention layers\nleads to a higher response quality compared to the one-shot retrieval present\nin naive RAG.\n\nWe were fortunate to have a preview of Gemini 1.5 Pro\u2019s capabilities, and\nthrough playing around with it developed a thesis for how context-augmented\nLLM applications will evolve. This blog post clarifies **our mission as a data\nframework** along with **our view of what long-context LLM architectures will\nlook like.** Our view is that while long-context LLMs will simplify certain\nparts of the RAG pipeline (e.g. chunking), there will need to be evolved RAG\narchitectures to handle the new use cases that long-context LLMs bring along.\nNo matter what new paradigms emerge, our mission at LlamaIndex is to build\ntooling towards that future.\n\n##  Our Mission Goes Beyond RAG\n\nThe goal of LlamaIndex is very simple: **enable developers to build LLM\napplications over their data.** This mission goes beyond just RAG. To date we\nhave invested a considerable amount of effort in advancing RAG techniques for\nexisting LLMs, and we\u2019ve done so because it\u2019s enabled developers to unlock\ndozens of new use cases such as QA over semi-structured data, over complex\ndocuments, and agentic reasoning in a multi-doc setting.\n\nBut we\u2019re also excited about Gemini Pro, and we will continue to advance\nLlamaIndex as a production data framework in a long-context LLM future.\n\n**An LLM framework is intrinsically valuable.** As an open-source data\nframework, LlamaIndex paves the cowpaths towards building any LLM use case\nfrom prototype to production. A framework makes it easier to build these use\ncases versus building from scratch. We enable _all_ developers to build for\nthese use cases, whether it\u2019s setting up the proper architecture using our\ncore abstractions or leveraging the hundreds of integrations in our ecosystem.\nNo matter what the underlying LLM advancements are and whether RAG continues\nto exist in its current form, we continue to make the framework production-\nready, including watertight abstractions, first-class documentation, and\nconsistency.\n\n[ We also launched LlamaCloud last week\n](https://blog.llamaindex.ai/introducing-llamacloud-and-llamaparse-\naf8cedf9006b) . Our mission for LlamaCloud remains building the data infra\nenabling any enterprise to make their vast unstructured, semi-structured, and\nstructured data sources production-ready for use with LLMs.\n\n##  Initial Gemini 1.5 Pro Observations\n\nDuring our initial testing we played around with some PDFs: SEC 10K Filings,\nArXiv papers, this monster [ Schematic Design Binder\n](https://www.lowellhsproject.com/DocumentCenter/View/236/LHS-Schematic-\nDesign-Binder) , and more. We will do a lot more deeper analyses once the APIs\nare available, but in the meantime we share observations below.\n\nGemini results are impressive and consistent with what we\u2019ve seen in the\ntechnical report and on socials:\n\n  * **Gemini has impressive recall of specific details:** We threw in 100k-1M tokens of context, and asked questions over very specific details in these documents (unstructured text and tabular data), and in all cases Gemini was able to recall the details. See above for Gemini comparing table results in the 2019 Uber 10K Filing. \n  * **Gemini has impressive summarization capabilities.** The model can analyze large swaths of information across multiple documents and synthesize answers. \n\nThis figure shows a question-response pair from Gemini over the 2019 Uber 10K\nfiling. The question and answer is shown at the top and the source table is\nshown at the bottom. Gemini is able to return the correct answer.\n\nThere are some parts where we noticed Gemini struggles a bit.\n\n  * **Gemini doesn\u2019t read all tables and charts correctly.** Gemini Pro still has a hard time being able to read figures and complex tables. \n  * **Gemini can take a long time.** Returning an answer over the Uber 10K Filing (~160k) takes ~20 seconds. Returning an answer over the LHS Schematic Design Binder (~890k) takes ~60+ seconds. \n  * **Gemini can hallucinate page numbers.** When asked to give a summary but also with page number citations, Gemini hallucinated the sources. \n\nAn example where Gemini 1.5 Pro still hallucinates. The model hallucinates a\nnumber when asked about the total number of gross bookings across all segments\n- the number is visible in the chart and can also be pieced together from the\ntable.\n\nDirectionally though it\u2019s an exciting glimpse of the future and warrants a\nbigger discussion on which RAG paradigms will fade and new architectures that\nwill emerge. See below!\n\n##  Long Contexts Resolve Some Pain Points, but some Challenges Remain\n\nGemini 1.5 Pro is just the first of many long-context LLMs to emerge, which\nwill inevitably change how users are building RAG.\n\nHere are some existing RAG pain points that we believe long-context LLMs will\nsolve:\n\n  1. **Developers will worry less about how to precisely tune chunking algorithms.** We honestly think this will be a huge blessing to LLM developers. Long-context LLMs enable native chunk sizes to be bigger. Assuming per-token cost and latency also go down, developers will no longer have to split hairs deciding how to split their chunks into granular strips through tuning chunking separators, chunk sizes, and careful metadata injection. Long-context LLMs enable chunks to be at the level of entire documents, or at the very least groups of pages. \n  2. **Developers will need to spend less time tuning retrieval and chain-of-thought over single documents** . An issue with small-chunk top-k RAG is that while certain questions may be answered over a specific snippet of the document, other questions require deep analysis between sections or between two documents (for instance comparison queries). For these use cases, developers will no longer have to rely on a chain-of-thought agent to do two retrievals against a weak retriever; instead, they can just one-shot prompt the LLM to obtain the answer. \n  3. ", "mimetype": "text/plain", "start_char_idx": 1041, "end_char_idx": 7222, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "718c3d47-5861-4b73-9869-ce3c66aa7553": {"__data__": {"id_": "718c3d47-5861-4b73-9869-ce3c66aa7553", "embedding": null, "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cd0d369d-13e9-421e-aa70-c388b567acdb", "node_type": "4", "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "8dbc3145c7b909a6f9a4bd4ea8ea7b7ac8679e6d4b729522cd4258a0aa5323e0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02602e9d-2105-4940-8370-03865c493134", "node_type": "1", "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "242027ff275163e715761b29913665e0892baa8d16f918e06a7bf118598fca3e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b94dc85-06a4-4523-813a-452dea9f54a7", "node_type": "1", "metadata": {}, "hash": "47bbabcc0561d534a6124a15c75352052b5e19eddc639c13be4be5deb3781f45", "class_name": "RelatedNodeInfo"}}, "text": "**Summarization will be easier.** This is related to the above statement. A lot of summarization strategies over big documents involve \u201chacks\u201d such as sequential refinement or hierarchical summarization (see our [ response synthesis modules ](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html) as a reference guide). This can now be alleviated with a single LLM call. \n  ", "mimetype": "text/plain", "start_char_idx": 7222, "end_char_idx": 7635, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b94dc85-06a4-4523-813a-452dea9f54a7": {"__data__": {"id_": "7b94dc85-06a4-4523-813a-452dea9f54a7", "embedding": null, "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cd0d369d-13e9-421e-aa70-c388b567acdb", "node_type": "4", "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "8dbc3145c7b909a6f9a4bd4ea8ea7b7ac8679e6d4b729522cd4258a0aa5323e0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "718c3d47-5861-4b73-9869-ce3c66aa7553", "node_type": "1", "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "ac5335d2f6bc1f60ce4972afb6c0255f19b0c1fb49763563539ebe7ad45dd701", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dcc7c189-997d-4c59-b90b-7a960855455d", "node_type": "1", "metadata": {}, "hash": "6a100c2224f57b34c792557c86b9bb1bd93fb0fe319437f30111bcd94eb6ef52", "class_name": "RelatedNodeInfo"}}, "text": "4. **Personalized memory will be better and easier to build:** A key issue for building conversational assistants is figuring out how to load sufficient conversational context into the prompt window. 4k tokens easily overflows this window for very basic web search agents - if it decides to load in a Wikipedia page for instance, that text will easily overflow the context. 1M-10M context windows will let developers more easily implement conversational memory with fewer compression hacks (e.g. vector search or automatic KG construction). \n\n", "mimetype": "text/plain", "start_char_idx": 7635, "end_char_idx": 8178, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dcc7c189-997d-4c59-b90b-7a960855455d": {"__data__": {"id_": "dcc7c189-997d-4c59-b90b-7a960855455d", "embedding": null, "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cd0d369d-13e9-421e-aa70-c388b567acdb", "node_type": "4", "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "8dbc3145c7b909a6f9a4bd4ea8ea7b7ac8679e6d4b729522cd4258a0aa5323e0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b94dc85-06a4-4523-813a-452dea9f54a7", "node_type": "1", "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "668cd176e67cb4dcbc1703047525f60f5c0c2d4e3c9bd753a33a2f74d19ea642", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a5d2c4e-6d13-4767-9ccb-3a6afc90294f", "node_type": "1", "metadata": {}, "hash": "1dcfac9e0b003f94807d09d9b930fe5953fdd61f2db78dd2d2cd5ca2c93692f9", "class_name": "RelatedNodeInfo"}}, "text": "There are, however, some lingering challenges:\n\n  1. **10M tokens is not enough for large document corpuses - kilodoc retrieval is still a challenge.** 1M tokens is around ~7 Uber SEC 10K filings. 10M tokens would be around ~70 filings. 10M tokens is roughly bounded by 40MB of data. While this is enough for many \u201csmall\u201d document corpuses, many knowledge corpuses in the enterprise are in the gigabytes or terabytes. To build LLM-powered systems over these knowledge corpuses, developers will still need to build in some way of retrieving this data to augment language models with context. \n  2. **Embedding models are lagging behind in context length.** So far the largest context window we\u2019ve seen for embeddings are [ 32k from together.ai ](https://hazyresearch.stanford.edu/blog/2024-01-11-m2-bert-retrieval) . This means that even if the chunks used for synthesis with long-context LLMs can be big, any text chunks used for retrieval still need to be a lot smaller. \n  3. ", "mimetype": "text/plain", "start_char_idx": 8178, "end_char_idx": 9156, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a5d2c4e-6d13-4767-9ccb-3a6afc90294f": {"__data__": {"id_": "2a5d2c4e-6d13-4767-9ccb-3a6afc90294f", "embedding": null, "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cd0d369d-13e9-421e-aa70-c388b567acdb", "node_type": "4", "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "8dbc3145c7b909a6f9a4bd4ea8ea7b7ac8679e6d4b729522cd4258a0aa5323e0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dcc7c189-997d-4c59-b90b-7a960855455d", "node_type": "1", "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "7a69ef0137d27ecbbd11294b76d4bb8d2aaf93c5f478a6df46a449a73622e062", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ae21d20-8814-4335-acfb-06cff996debd", "node_type": "1", "metadata": {}, "hash": "e745a288c062a07a57e589aacccfeabe2c8cc486c9a8a798d8350818b2be3a38", "class_name": "RelatedNodeInfo"}}, "text": "**Cost and Latency.", "mimetype": "text/plain", "start_char_idx": 9156, "end_char_idx": 9175, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ae21d20-8814-4335-acfb-06cff996debd": {"__data__": {"id_": "5ae21d20-8814-4335-acfb-06cff996debd", "embedding": null, "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cd0d369d-13e9-421e-aa70-c388b567acdb", "node_type": "4", "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "8dbc3145c7b909a6f9a4bd4ea8ea7b7ac8679e6d4b729522cd4258a0aa5323e0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a5d2c4e-6d13-4767-9ccb-3a6afc90294f", "node_type": "1", "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "f127ba741f8efa9be06e0bd4b68966d423500d3d321e145c80b6c4b86a1db38b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75f9bc1d-d52d-4d68-9944-5ee41193f402", "node_type": "1", "metadata": {}, "hash": "3993c40991e4b010c3bb895e377ea5f84fb37df0ab6ca285b745edc278f65743", "class_name": "RelatedNodeInfo"}}, "text": "** Yes, all cost and latency concerns are alleviated with time. Nevertheless, stuffing a 1M context window takes ~60 seconds and can cost anywhere from $0.50 to $20 with current pricing. An solution to this that [ Yao Fu ](https://twitter.com/Francis_YAO_) brought up is that a [ KV Cache ](https://x.com/Francis_YAO_/status/1759962812229800012?s=20) can cache the document activations, so that any subsequent generations can reuse the same cache. Which leads to our next point below. \n  ", "mimetype": "text/plain", "start_char_idx": 9175, "end_char_idx": 9663, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75f9bc1d-d52d-4d68-9944-5ee41193f402": {"__data__": {"id_": "75f9bc1d-d52d-4d68-9944-5ee41193f402", "embedding": null, "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cd0d369d-13e9-421e-aa70-c388b567acdb", "node_type": "4", "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "8dbc3145c7b909a6f9a4bd4ea8ea7b7ac8679e6d4b729522cd4258a0aa5323e0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ae21d20-8814-4335-acfb-06cff996debd", "node_type": "1", "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}, "hash": "ad948b74989ce4e65a8e0be934c5ce17a7ee588da4563c6ef3bfa527e3c6d49f", "class_name": "RelatedNodeInfo"}}, "text": "4. **A KV Cache takes up a significant amount of GPU memory, and has sequential dependencies** . We chatted with Yao and he mentioned that at the moment, caching 1M tokens worth of activations would use up approximately 100GB of GPU memory, or 2 H100s. There are also interesting challenges on how to best manage the cache especially when the underlying corpus is big - since each activation is a function of all tokens leading up to it, replacing any document in the KV cache would affect all activations following the document. \n\n##  Towards New RAG Architectures\n\nProper usage of long-context LLMs will necessitate new architectures to best\ntake advantage of their capabilities, while working around their remaining\nconstraints. We outline some proposals below.\n\n###  1\\. Small to Big Retrieval over Documents\n\nTo the extent that long-context LLMs need retrieval augmentation over big\nknowledge bases (e.g. in the gigabytes), we will need **small-to-big\nretrieval:** index and retrieve small chunks, but have each chunk link to big\nchunks that will ultimately be fed to LLMs during synthesis.\n\nThis architecture already exists in LlamaIndex in different forms ( [ sentence\nwindow retriever\n](https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/MetadataReplacementDemo.html)\nand [ recursive retrieval over chunk sizes\n](https://docs.llamaindex.ai/en/stable/examples/retrievers/recursive_retriever_nodes.html)\n), but can be scaled up even more for long-context LLMs - embed document\nsummaries, but link to entire documents.\n\nOne reason we want to embed and index smaller chunks is due to the fact that\ncurrent embedding models are not keeping up with LLMs in terms of context\nlength. Another reason is that there can actually be retrieval benefits in\nhaving multiple granular embedding representations compared to a single\ndocument-level embedding for a document. If there is a single embedding for a\ndocument, then that embedding has the burden of encoding all information\nthroughout the entire document. On the other hand, we\u2019ve found that embedding\nmany smaller chunks and having each small chunk link to a bigger chunk, will\nlead to better retrieval of the relevant information.\n\nCheck out the diagram above for an illustration of two flavors of small-to-big\nretrieval. One is indexing document summaries and linking them to documents,\nand the other is indexing smaller chunks within a document and linking them to\nthe document. Of course, you could also do both - a general best practice for\nimproving retrieval is to just try out multiple techniques at once and fuse\nthe results later.\n\n###  2\\. Intelligent Routing for Latency/Cost Tradeoffs\n\nThe arrival of long-context LLMs will inevitably raise questions on the amount\nof context that is suitable for each use case. Injecting LLMs with long\ncontext comes with real cost and latency tradeoffs and isn\u2019t suitable for\nevery use case or even every question. Although cost and latency will decrease\nin the future, we anticipate users will need to think carefully about this\ntradeoff for the next year or two.\n\nCertain questions that are asking about specific details are well suited for\nexisting RAG techniques of top-k retrieval and synthesis.\n\nMore complex questions require more context from disparate pieces of different\ndocuments, and in those settings it is less clear how to correctly answer\nthese questions while optimizing for latency and cost:\n\n  * Summarization questions require going over entire documents. \n  * Multi-part questions can be solved by doing chain-of-thought and interleaving retrieval and reasoning; they can also be solved by shoving all context into the prompt. \n\nWe imagine an intelligent routing layer that operates on top of multiple RAG\nand LLM synthesis pipelines over a knowledge base. Given a question, the\nrouter can ideally choose an optimal strategy in terms of cost and latency in\nterms of retrieving context to answer the question. This ensures that a single\ninterface can handle different types of questions while not becoming\nprohibitively expensive.\n\n###  3\\. Retrieval Augmented KV Caching\n\nAn optimization that Google and other companies are certainly working on is\nresolving latency and cost concerns through a **KV Cache.** At a high-level, a\nKV cache stores activations from pre-existing key and query vectors in an\nattention layer, preventing the need to recompute activations across the\nentire text sequence during LLM generation (we found [ this\n](https://medium.com/@plienhar/llm-inference-series-3-kv-caching-\nunveiled-048152e461c8) to be a nice intro reference to how a KV Cache works).\n\nUsing a KV Cache to cache all document tokens within the context window\nprevents the need to recompute activations for these tokens on subsequent\nconversations, bringing down latency and cost significantly.\n\nBut this leads to interesting retrieval strategies on how to best use the\ncache, particularly for knowledge corpuses that exceed the context length. We\nimagine a \u201c **retrieval augmented caching** \u201d paradigm emerging, where we want\nto retrieve the most relevant documents that the user would want to answer,\nwith the expectation that they will continue to use the documents that are in\nthe cache.\n\nThis could involve interleaving retrieval strategies with [ traditional\ncaching algorithms ](https://www.notion.so/Long-Context-Window-Blog-\nPost-e2e3faaac23140eabc0e066ce2783890?pvs=21) such as LRU caching. But a\ndifference with existing KV cache architectures is that the position matters,\nsince the cached vector is a function of all tokens leading up to that\nposition, not just the tokens in the document itself. This means that you\ncan\u2019t just swap out a chunk from the KV cache without affecting all cached\ntokens that occur after it positionally.\n\nIn general the API interface for using a KV Cache is up in the air. It\u2019s also\nup in the air as to whether the nature of the cache itself will evolve or\nalgorithms will evolve to best leverage the cache.\n\n###  What\u2019s Next\n\nWe believe the future of LLM applications is bright, and we are excited to be\nat the forefront of this rapidly evolving field. We invite developers and\nresearchers to join us in exploring the possibilities of long-context LLMs and\nbuilding the next generation of intelligent applications.\n\n", "mimetype": "text/plain", "start_char_idx": 9663, "end_char_idx": 15947, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45264645-55de-4c64-9052-ddec4ded2279": {"__data__": {"id_": "45264645-55de-4c64-9052-ddec4ded2279", "embedding": null, "metadata": {"filename": "unlocking-the-3rd-dimension-for-generative-ai-part-1.md", "extension": ".md", "title": "Unlocking the 3rd Dimension for Generative AI (Part 1)", "date": "Feb 29, 2024", "url": "https://www.llamaindex.ai/blog/unlocking-the-3rd-dimension-for-generative-ai-part-1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "337943d5-5cbd-4bed-b913-4521e0538d90", "node_type": "4", "metadata": {"filename": "unlocking-the-3rd-dimension-for-generative-ai-part-1.md", "extension": ".md", "title": "Unlocking the 3rd Dimension for Generative AI (Part 1)", "date": "Feb 29, 2024", "url": "https://www.llamaindex.ai/blog/unlocking-the-3rd-dimension-for-generative-ai-part-1"}, "hash": "430d5d9aec44390623d6d04d0fe99bc5f0a2ac450a65a826251fbe00492419e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6759d47-370c-4801-afbe-de24b1b6b902", "node_type": "1", "metadata": {}, "hash": "fd6c1ed255fa9ec69cd1a68400b7209d936a67a8ea840f13e0b0b0caa2aabdb7", "class_name": "RelatedNodeInfo"}}, "text": "It would be an understatement to say that generative AI has been taking the\nworld by storm over the past couple of years. While text (1D) and image (2D)\nmodels are reaching a level of quality that is truly transforming the way we\ncreate digital content, the same cannot be said for 3D models.\n\n3D generative AI has made significant strides in creating 3D digital assets,\nwith techniques such as Neural Radiance Fields (NeRFs) showing promising\nresults for applications such as video game development. However, the\nlimitations of current generative AI technologies become apparent when it\ncomes to applications beyond the digital realm, especially in the context of\nengineering and manufacturing. After spending more than 2 years actively\nengaged with the state-of-the-art tools for 3D generative AI, I personally\nhave not been able to generate a single model that I would actually want to\nhave manufactured into a physical object.\n\nTo put it simply: the current state of 3D generative AI is not very useful for\nengineers.\n\n##  The Motivation Behind neThing.xyz\n\nOur mission at [ polySpectra ](https://polyspectra.com/) is to help engineers\nmake their ideas real. The key insight that led to the invention of [\nneThing.xyz ](http://nething.xyz/) (pronounced \u201canything dot x,y,z\u201d) was that\nAI is actually quite good at writing code, and by training a \u201ccodegen\u201d AI on\ndomain specific languages for \u201ccode CAD\u201d, our AI can produce code that can be\nrapidly converted into 3D CAD models.\n\nFor some quick context, AI code generation tools are now achieving ~95%\nevaluation benchmarks against human programmers. At the current pace, I\nwouldn\u2019t be surprised if an AI hits 100% in the next three weeks. ( [ See this\nleaderboard ](https://paperswithcode.com/sota/code-generation-on-humaneval)\nfor more details.)\n\nIn 3D modeling, there is a growing buzz around \u201ccode CAD\u201d \u2014 a term that\nsignifies a paradigm shift in how we approach computer-aided design. Unlike\ntraditional graphical CAD interfaces, which rely heavily on visual tools and\nmanual user input, code CAD leverages programming to create and manipulate 3D\nmodels. This approach offers a more direct and potentially more powerful\nmethod for generating complex designs, as it allows for precision and\nautomation that can be difficult to achieve with mouse-driven interfaces.\n\nA testament to the rising prominence of code CAD was its debut this month in\nthe Too Tall Toby speed CAD competition. In the second match of the video\nbelow, [ Jern ](https://github.com/jdegenstein/) competes using the code CAD\npackage [ Build123d ](https://build123d.readthedocs.io/en/latest/) , against\n\u201cMr. Alex\u201d who is using the traditional CAD tool SolidWorks.\n\n[ (Jump to 1:18 for competition-grade Code CAD!)\n](https://www.youtube.com/watch?v=-coWKJhwQbM&t=4666s)\n\n_So my idea was simple: if AI can code, and code can CAD, why can\u2019t AI CAD?_\n\n##  Why RAG?\n\nTrying to create a 3D generative AI with this code generation approach led us\nto confront a significant challenge: the need for incredibly long prompts to\nprovide the AI with enough context about our code CAD domain-specific\nlanguages. This was essential for it to stand any chance of producing working\ncode, let alone something useful.\n\nI first studied Retrieval-Augmented Generation (RAG) under the tutelage of\nMayo Oshin. ( [ Check out his RAG course! ](https://maven.com/ai-chat-with-\ndata/chatgpt-your-data) ). I knew that RAG was going to be a necessary part of\nthe strategy for making neThing.xyz \u201csmarter\u201d, and through Mayo\u2019s course I had\nthe opportunity to meet Jerry Liu and ask him some questions about the more\nnuanced elements of retrieval.\n\nThe initial version of neThing.xyz was ok, but I really had to wrangle the\nLLM: each query involved about 10,000 tokens. So if a user asks for \u201ca box\u201d,\nit is not 2 input tokens I\u2019m paying for, it is 10,002 tokens. I knew I needed\na more scalable approach\u2026\n\n##  Enter the RAG-a-thon\n\nThe announcement of the \u201c [ RAG-a-thon ](https://rag-a-thon.devpost.com/) \u201d\npresented the perfect opportunity to quickly integrate RAG into neThing.xyz.\nFor those familiar with the whirlwind nature of hackathons, you\u2019ll understand\nwhen I say that time always seems to be in shorter supply than anticipated,\noften leading one to overestimate what can be accomplished. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4291, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6759d47-370c-4801-afbe-de24b1b6b902": {"__data__": {"id_": "a6759d47-370c-4801-afbe-de24b1b6b902", "embedding": null, "metadata": {"filename": "unlocking-the-3rd-dimension-for-generative-ai-part-1.md", "extension": ".md", "title": "Unlocking the 3rd Dimension for Generative AI (Part 1)", "date": "Feb 29, 2024", "url": "https://www.llamaindex.ai/blog/unlocking-the-3rd-dimension-for-generative-ai-part-1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "337943d5-5cbd-4bed-b913-4521e0538d90", "node_type": "4", "metadata": {"filename": "unlocking-the-3rd-dimension-for-generative-ai-part-1.md", "extension": ".md", "title": "Unlocking the 3rd Dimension for Generative AI (Part 1)", "date": "Feb 29, 2024", "url": "https://www.llamaindex.ai/blog/unlocking-the-3rd-dimension-for-generative-ai-part-1"}, "hash": "430d5d9aec44390623d6d04d0fe99bc5f0a2ac450a65a826251fbe00492419e8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45264645-55de-4c64-9052-ddec4ded2279", "node_type": "1", "metadata": {"filename": "unlocking-the-3rd-dimension-for-generative-ai-part-1.md", "extension": ".md", "title": "Unlocking the 3rd Dimension for Generative AI (Part 1)", "date": "Feb 29, 2024", "url": "https://www.llamaindex.ai/blog/unlocking-the-3rd-dimension-for-generative-ai-part-1"}, "hash": "b7ea459236c86da446a4b437dba3b52d7ade172b6d94a7e24c9b3be9f94dbb92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf2563e5-cac9-4f32-8926-560c46584cca", "node_type": "1", "metadata": {}, "hash": "253304117842c6fd373da9ffdb5005dadd9598e01b799047aa05f1435f3e7b9d", "class_name": "RelatedNodeInfo"}}, "text": "(For me, I usually\noverestimate what I can achieve by 3\u201310x!) With this in mind, I tried to set a\nmodest goal for the weekend: add LlamaIndex to neThing.xyz.\n\nMy ultimate aim was to leverage LlamaIndex to dramatically expand the corpus\nof documentation available to neThing.xyz. But in the theme of setting a low\nbar, I started by just breaking down my very large system prompt into a set of\ndocuments that LlamaIndex could retrieve from AstraDB, bringing back only the\nmost relevant example code for a given user\u2019s query.\n\nWith significant assistance from Logan Markewich from the LlamaIndex team, I\nmanaged to implement RAG via LlamaIndex and AstraDB in a single day. This\nimmediately reduced the average number of tokens per user query from about\n10,000 to roughly 2,000. The impact of this was huge, resulting in an 80% cost\nreduction in our OpenAI bill in just one day \u2014 a change that was incredibly\nmeaningful for us as a small business.\n\nWhile I am personally passionate about the myriad ways in which RAG can make\nLLMs smarter, I want to emphasize the immediate ROI that RAG provided. By\nsimply reorganizing the same set of information for retrieval through\nLlamaIndex, we achieved significant cost savings with minimal effort. As an\nentrepreneur, an 80% reduction in costs with just eight hours of work is a\ndeal I\u2019d take any day of the week.\n\nOn the final Sunday of the RAG-a-thon, I dedicated most of my time to ensuring\nthat my demo would function correctly. The \u201cdemo gods\u201d blessed me that day: I\nwon first place in the \u201ccontinuous innovation\u201d track!\n\n##  Examples of neThing.xyz in Action:\n\nText:\n\nCurves:\n\nThreads:\n\nPipes:\n\nLattices:\n\n##  What\u2019s Next?\n\nMy goal with neThing.xyz is to make the best 3D generative AI for engineers,\nwith a focus on \u201ctext-to-CAD\u201d. This is a really hard problem, and I shared\nsome of these challenges with LlamaIndex in our recent webinar, and in more\ndetail on [ Wevolver ](https://www.wevolver.com/article/teaching-ai-cad-is-\nhard) .\n\nOur key objectives are to make neThing.xyz faster, smarter, and cheaper. We\nare leveraging LlamaIndex to orchestrate the entire RAG pipeline, and we are\nreally excited about the amazing pace of developments in this open source\ncommunity.\n\n_What would it take to get a part like this from a natural language prompt?_\n\nHonestly, I don\u2019t know how to do it.\n\nWe are just getting things off the ground and I would be tremendously excited\nto have you join our community. Our AI will only ever be as smart as the sum\nof the community that trained it, and we are excited to see what you will\ncreate!\n\nPlease give [ neThing.xyz ](https://nething.xyz/) a try today and share your\nhonest feedback with us via our [ new community forum\n](https://forum.nething.xyz/) . Your input will play a crucial role in our\nongoing development efforts, helping us to refine and improve the tool.\n\n", "mimetype": "text/plain", "start_char_idx": 4291, "end_char_idx": 7144, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf2563e5-cac9-4f32-8926-560c46584cca": {"__data__": {"id_": "cf2563e5-cac9-4f32-8926-560c46584cca", "embedding": null, "metadata": {"filename": "unlocking-the-3rd-dimension-for-generative-ai-part-1.md", "extension": ".md", "title": "Unlocking the 3rd Dimension for Generative AI (Part 1)", "date": "Feb 29, 2024", "url": "https://www.llamaindex.ai/blog/unlocking-the-3rd-dimension-for-generative-ai-part-1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "337943d5-5cbd-4bed-b913-4521e0538d90", "node_type": "4", "metadata": {"filename": "unlocking-the-3rd-dimension-for-generative-ai-part-1.md", "extension": ".md", "title": "Unlocking the 3rd Dimension for Generative AI (Part 1)", "date": "Feb 29, 2024", "url": "https://www.llamaindex.ai/blog/unlocking-the-3rd-dimension-for-generative-ai-part-1"}, "hash": "430d5d9aec44390623d6d04d0fe99bc5f0a2ac450a65a826251fbe00492419e8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6759d47-370c-4801-afbe-de24b1b6b902", "node_type": "1", "metadata": {"filename": "unlocking-the-3rd-dimension-for-generative-ai-part-1.md", "extension": ".md", "title": "Unlocking the 3rd Dimension for Generative AI (Part 1)", "date": "Feb 29, 2024", "url": "https://www.llamaindex.ai/blog/unlocking-the-3rd-dimension-for-generative-ai-part-1"}, "hash": "981844027b1753e2fbf2844b2cec25e0568bd5dc6a636a4d61bf8ed0a90b43b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01a98d61-b97f-464c-a64a-e0bc922e24db", "node_type": "1", "metadata": {}, "hash": "9b94324968e80e706c66b0f498d6c299ff369455d861306ecb5edf522cb3fbe3", "class_name": "RelatedNodeInfo"}}, "text": "Make it real.\n\n", "mimetype": "text/plain", "start_char_idx": 7144, "end_char_idx": 7159, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01a98d61-b97f-464c-a64a-e0bc922e24db": {"__data__": {"id_": "01a98d61-b97f-464c-a64a-e0bc922e24db", "embedding": null, "metadata": {"filename": "unlocking-the-3rd-dimension-for-generative-ai-part-1.md", "extension": ".md", "title": "Unlocking the 3rd Dimension for Generative AI (Part 1)", "date": "Feb 29, 2024", "url": "https://www.llamaindex.ai/blog/unlocking-the-3rd-dimension-for-generative-ai-part-1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "337943d5-5cbd-4bed-b913-4521e0538d90", "node_type": "4", "metadata": {"filename": "unlocking-the-3rd-dimension-for-generative-ai-part-1.md", "extension": ".md", "title": "Unlocking the 3rd Dimension for Generative AI (Part 1)", "date": "Feb 29, 2024", "url": "https://www.llamaindex.ai/blog/unlocking-the-3rd-dimension-for-generative-ai-part-1"}, "hash": "430d5d9aec44390623d6d04d0fe99bc5f0a2ac450a65a826251fbe00492419e8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf2563e5-cac9-4f32-8926-560c46584cca", "node_type": "1", "metadata": {"filename": "unlocking-the-3rd-dimension-for-generative-ai-part-1.md", "extension": ".md", "title": "Unlocking the 3rd Dimension for Generative AI (Part 1)", "date": "Feb 29, 2024", "url": "https://www.llamaindex.ai/blog/unlocking-the-3rd-dimension-for-generative-ai-part-1"}, "hash": "3591af96536e70d0f84e60f46d0f2854604ca6aa7b38ffe2e51f410db9b549bb", "class_name": "RelatedNodeInfo"}}, "text": "Raymond\n\nP.S. \u2014 _This article is titled Part 1 for a reason. What do you want to see in\nPart 2? Tell me below!_\n\n", "mimetype": "text/plain", "start_char_idx": 7159, "end_char_idx": 7272, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "beecd20f-0b6c-4d09-b1a4-9a117447a6db": {"__data__": {"id_": "beecd20f-0b6c-4d09-b1a4-9a117447a6db", "embedding": null, "metadata": {"filename": "querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.md", "extension": ".md", "title": "Querying a network of knowledge with llama-index-networks", "date": "Feb 27, 2024", "url": "https://www.llamaindex.ai/blog/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ed7eb9b0-c368-4e08-b1bd-46a0ad5c3a26", "node_type": "4", "metadata": {"filename": "querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.md", "extension": ".md", "title": "Querying a network of knowledge with llama-index-networks", "date": "Feb 27, 2024", "url": "https://www.llamaindex.ai/blog/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f"}, "hash": "0baaf0c486c979505e0109d6c70e33a294c0c3ca29499fa8793e596f9fd0fc23", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44a8f9aa-f002-405a-8a4d-bb65d640ede8", "node_type": "1", "metadata": {}, "hash": "1846178d5c120cc623540bf303011a8592dba377f9aab23670bc0b0fa631e581", "class_name": "RelatedNodeInfo"}}, "text": "The main premise behind RAG is the injection of context (or knowledge) to the\nLLM in order to yield more accurate responses from it. As such, a crucial\ncomponent to a RAG system is the data source from which it gets its knowledge.\nIt\u2019s intuitive then to reason that the more knowledge that the RAG system can\ntap into, the better it would ultimately become (in terms of answering queries\nof potentially both depth and breadth). The spirit of this concept is not so\ndifferent from that found in basically every other data-driven discipline \u2014\naccess to more (good) data, that is subsequently and effectively used, usually\nleads to better outcomes.\n\nIt is with that backdrop, that we\u2019re thrilled to announce the release of our\nnewest latest ` llama-index ` library extension, called ` llama-index-networks\n` . This library extension makes it possible to build a network of RAGs built\non top of external data sources and supplied by external actors. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 946, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44a8f9aa-f002-405a-8a4d-bb65d640ede8": {"__data__": {"id_": "44a8f9aa-f002-405a-8a4d-bb65d640ede8", "embedding": null, "metadata": {"filename": "querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.md", "extension": ".md", "title": "Querying a network of knowledge with llama-index-networks", "date": "Feb 27, 2024", "url": "https://www.llamaindex.ai/blog/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ed7eb9b0-c368-4e08-b1bd-46a0ad5c3a26", "node_type": "4", "metadata": {"filename": "querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.md", "extension": ".md", "title": "Querying a network of knowledge with llama-index-networks", "date": "Feb 27, 2024", "url": "https://www.llamaindex.ai/blog/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f"}, "hash": "0baaf0c486c979505e0109d6c70e33a294c0c3ca29499fa8793e596f9fd0fc23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "beecd20f-0b6c-4d09-b1a4-9a117447a6db", "node_type": "1", "metadata": {"filename": "querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.md", "extension": ".md", "title": "Querying a network of knowledge with llama-index-networks", "date": "Feb 27, 2024", "url": "https://www.llamaindex.ai/blog/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f"}, "hash": "7c2efc0d539fe009b5d7cb1f386004711f2603c37206840fc81047bf8c2ba8df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa07156d-ccb3-4eb9-a7f5-11b73c0f94ed", "node_type": "1", "metadata": {}, "hash": "037f690a1fd7fda418a191d2e6ccb8acdd7822b4633fe1067c1358802fbfd864", "class_name": "RelatedNodeInfo"}}, "text": "This new\nnetwork paradigm allows for a new way for data suppliers to provide their data\nto those who want it in order to build more knowledgeable systems!\n\nIn this blog post, we\u2019ll introduce the main classes of the new extension\nlibrary and show you how in just a couple lines of code, you can make your\nQueryEngine ready to contribute as part of a network of RAGs. We\u2019ll also share\nideas of what this can mean for how data suppliers actually supply their data\nto consumers within this new era of LLMs.\n\nA note on terminology: in this post, we use ` llama-index-networks ` to refer\nto the actual extension, whereas ` llama-index[networks] ` refers to an\ninstallation of ` llama-index ` that comes with the ` llama-index-networks `\nextension.\n\n##  The story of Alex, Beth, and Bob\n\nAn illustrative example of actors in a network and their problem statements.\n\nTo illustrate how the llama-index-networks package can be used, we consider\nthree made-up characters, Alex, Bob, and Beth, and the following scenario:\n\n  * Both Bob and Beth each have their own set of documents and both have already built quite the outstanding RAG systems over them (using llama-index of course!) \n  * Alex has heard about these insightful documents that both Bob and Beth have and would like to be able to query the individual RAGs built over the both of them. \n  * Bob and Beth, being as kind as they are (or, perhaps they were paid some undisclosed dollar amount), agree to give access to Alex to their RAG systems. \n\nTo facilitate this new fashion of knowledge exchange, they agree to setup a\nnetwork of RAGs that Alex can query.\n\n##  Bob and Beth build a web service over their RAGs\n\nContributorService is built around a QueryEngine.\n\nWith the ` llama-index-networks ` package, Bob and Beth can make their\nrespective ` QueryEngine ` \u2019s ready to participate in the network with only a\nfew lines of code.\n\n    \n    \n    \"\"\"Beth's contributor service file.\n    \n    Beth builds her QueryEngine and exposes it behind the standard\n    LlamaIndex Network Contributor Service. \n    \n    NOTE: Bob would probably make use of Docker and cloud \n    compute services to make this production grade.\n    \"\"\"\n    \n    from llama_index.networks.contributor import ContributorService\n    import uvicorn\n    \n    beth_query_engine = ...\n    beth_contributor_service = ContributorService.from_config_file(\n        \".env.contributor\",  # settings/secrets for the service\n        beth_query_engine\n    )\n    \n    \n    if __name__ == \"__main__:\n        uvicorn.run(beth_contributor_service.app, port=8000)\n\nBob would use similar lines of code to make his ` QueryEngine ` ready to\ncontribute to any LlamaIndex network. Note, that the dotenv file `\n.env.contributor ` contains settings for the service as well as any necessary\napi keys (e.g., ` OPENAI_API_KEY ` or ` ANTHROPIC_API_KEY ` ), which under the\nhood is implemented as REST service using FastAPI.\n\n##  Alex builds a NetworkQueryEngine\n\nAlex builds a NetworkQueryEngine that connects to Beth and Bob\u2019s individual\nContributorService\u2019s.\n\n", "mimetype": "text/plain", "start_char_idx": 946, "end_char_idx": 3999, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fa07156d-ccb3-4eb9-a7f5-11b73c0f94ed": {"__data__": {"id_": "fa07156d-ccb3-4eb9-a7f5-11b73c0f94ed", "embedding": null, "metadata": {"filename": "querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.md", "extension": ".md", "title": "Querying a network of knowledge with llama-index-networks", "date": "Feb 27, 2024", "url": "https://www.llamaindex.ai/blog/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ed7eb9b0-c368-4e08-b1bd-46a0ad5c3a26", "node_type": "4", "metadata": {"filename": "querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.md", "extension": ".md", "title": "Querying a network of knowledge with llama-index-networks", "date": "Feb 27, 2024", "url": "https://www.llamaindex.ai/blog/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f"}, "hash": "0baaf0c486c979505e0109d6c70e33a294c0c3ca29499fa8793e596f9fd0fc23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44a8f9aa-f002-405a-8a4d-bb65d640ede8", "node_type": "1", "metadata": {"filename": "querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.md", "extension": ".md", "title": "Querying a network of knowledge with llama-index-networks", "date": "Feb 27, 2024", "url": "https://www.llamaindex.ai/blog/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f"}, "hash": "b0ae385eb1d3e2c84ab9beb6e32392626d81319e7208d5c612abcb093711c18c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8939bf29-d781-47e4-9f71-a48ed04e3bdd", "node_type": "1", "metadata": {}, "hash": "bed739e7045b956253568713be641ecb7e62d953bd58ad55eec39bbe2f851034", "class_name": "RelatedNodeInfo"}}, "text": "For Alex\u2019s part, he uses the ` NetworkQueryEngine ` class of the ` llama-\nindex-networks ` extension to be able to connect to both Beth and Bob\u2019s `\nContributorService ` \u2019s.\n\n    \n    \n    \"\"\"Alex's network query engine.\n    \n    Alex builds a NetworkQueryEngine to connect to a \n    list of ContributorService\u2019s.\n    \"\"\"\n    \n    from llama_index.networks.contributor import ContributorClient\n    from llama_index.networks.query_engine import NetworkQueryEngine\n    from llama_index.llms.groq import Groq\n    \n    # Use ContributorClient to connect to a ContributorService\n    beth_client = ContributorClient.from_config_file(\n        env_file=\".env.beth_contributor.client\"\n    )\n    bob_client = ContributorClient.from_config_file(\n        env_file=\".env.bob_contributor.client\"\n    )\n    contributors = [beth_client, bob_client]\n    \n    # NetworkQueryEngine\n    llm = Groq()\n    network_query_engine = NetworkQueryEngine.from_args(\n        contributors=contributors,\n        llm=llm\n    )\n    \n    # Can query it like any other query engine\n    network_query_engine.query(\"Why is the sky blue?\")\n\nHere the dotenv files store the service parameters such as an ` api_url `\nrequired to connect to the ` ContributorService ` .\n\n", "mimetype": "text/plain", "start_char_idx": 3999, "end_char_idx": 5227, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8939bf29-d781-47e4-9f71-a48ed04e3bdd": {"__data__": {"id_": "8939bf29-d781-47e4-9f71-a48ed04e3bdd", "embedding": null, "metadata": {"filename": "querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.md", "extension": ".md", "title": "Querying a network of knowledge with llama-index-networks", "date": "Feb 27, 2024", "url": "https://www.llamaindex.ai/blog/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ed7eb9b0-c368-4e08-b1bd-46a0ad5c3a26", "node_type": "4", "metadata": {"filename": "querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.md", "extension": ".md", "title": "Querying a network of knowledge with llama-index-networks", "date": "Feb 27, 2024", "url": "https://www.llamaindex.ai/blog/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f"}, "hash": "0baaf0c486c979505e0109d6c70e33a294c0c3ca29499fa8793e596f9fd0fc23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa07156d-ccb3-4eb9-a7f5-11b73c0f94ed", "node_type": "1", "metadata": {"filename": "querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.md", "extension": ".md", "title": "Querying a network of knowledge with llama-index-networks", "date": "Feb 27, 2024", "url": "https://www.llamaindex.ai/blog/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f"}, "hash": "2fb3a36ec387349a271bc3adbbac3ddb94991fec776a1a4352dcb5e5bf6d369f", "class_name": "RelatedNodeInfo"}}, "text": "Before moving on to the next section of this blog post, we\u2019ll take the next\nfew lines to explain a bit of what\u2019s involved under the hood when Alex query\u2019s\nhis ` NetworkQueryEngine ` . When the ` query() ` method is invoked (Async is\nalso supported!), the query is sent to all contributors. Their responses are\nstored as new ` Nodes ` and a response is synthesized (with the associated `\nResponseSynthesizer ` ). After reading that, some may notice that this is the\nusual pattern when working with our ` QueryEngine ` abstractions; and, that\nindeed was the point. Using a ` NetworkQueryEngine ` should be very similar to\nhow you would use any other ` QueryEngine ` in our library!\n\nThis marks the end of our little story about Alex, Bob and Beth. Before\nwrapping up this blog post, we first provide a few potential opportunities\nthat may arise through the usage of ` llama-index-networks ` .\n\n##  A new world of data supply and consumption\n\nRAG marketplaces is a use case that can be made possible with llama-\nindex[networks].\n\nOne possible world that could easily be powered by ` llama-index[networks] `\nare marketplaces for RAG. A place where data suppliers package their data in\nthe form of RAGs to data consumers look to expand their own query system\u2019s\nknowledge. Potential RAG (data) suppliers could be your local newspaper, book\npublishing companies, etc.\n\nIn this new data supply and consumption model, there is more onus on the data\nsupplier to prepare the data in a fashion that makes it easier to consume \u2014\nspecifically, the suppliers own the building of the query engine. This should\nbenefit the data consumer greatly since, with a standard interface that is\nprovided by the likes of a ` ContributorService ` (that encapsulates a `\nQueryEngine ` ), they can get access to the knowledge they seek from the data\neasier than ever before (i.e., in relation to traditional data marketplaces\nthat exchange raw data).\n\nIt is with this kind of vision that we\u2019ve built ` llama-index[networks] ` to\nmake it: (i) easier for data suppliers to supply the knowledge contained in\ntheir data in new and arguably more effective ways, and (ii) easier for data\nconsumers to connect to these new forms of external knowledge.\n\n##  Internal networks: another potential use case\n\nIn addition to powering RAG marketplaces, we foresee the need of connecting\nRAGs that an overarching company may own, but not necessarily manage. More\nconcretely, a franchise may have the rights to the data across all of its\noperations. And, while they could build a \u201ccentral\u201d, monolithic RAG over the\nentire database, it may still be more efficient and effective to build RAGs\nover the individual operators and query these instead.\n\nThe idea of exchanging information to build better, more knowledgeable systems\nis not new. However, the idea of using RAGs to facilitate that exchange may be\n(to our knowledge, it is), and we believe that both existing and new use cases\nrequiring such collaboration can benefit from concept.\n\n##  A quick note on privacy\n\nAn important consideration in the collaboration of data is privacy and\nsecurity. It bears mentioning that the examples above assume that the data\nthat is being shared across the network is in compliance with data privacy and\nsecurity laws and regulations. It is our belief that as this technology grows,\nthat the necessary features and capabilities will be developed and\nincorporated to facilitate in-compliance RAG networks.\n\nCheck out the demo to learn more!\n\nTo see an actual demo of a network connecting to a set of contributors, check\nout the Github repository [ code ](https://github.com/run-\nllama/llama_index/tree/main/llama-index-networks) for ` llama-index-networks `\nand navigate to the ` examples/demo ` subfolder.\n\n", "mimetype": "text/plain", "start_char_idx": 5227, "end_char_idx": 8978, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a74719b-11be-43ed-b862-c3ca8b1eabff": {"__data__": {"id_": "9a74719b-11be-43ed-b862-c3ca8b1eabff", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-02-27-4b9102a0f824.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201327", "date": "Feb 27, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-27-4b9102a0f824"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4532b31d-298d-4194-8b14-6f6600bd0257", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-02-27-4b9102a0f824.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201327", "date": "Feb 27, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-27-4b9102a0f824"}, "hash": "e44d2997f908f250a92a0aae7031718416882a516273f40f321cd4b6cdc8c59d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57ef073f-1ac3-4a8d-b96a-de078e1543e0", "node_type": "1", "metadata": {}, "hash": "3751503ae8b22425d1403ae6daa1faf249bc99fdf5eb1b2ff6375c4d29b86364", "class_name": "RelatedNodeInfo"}}, "text": "Yo, LlamaIndex Fans ,\n\nDive into a week brimming with thrilling developments at LlamaIndex! The\ndynamic input from our community and our rich selection of learning materials\nare all set to enhance your journey with LlamaIndex.\n\nLast week, the LlamaIndex ecosystem took a significant leap forward with the\nlaunch of LlamaCloud, a suite of advanced services designed for **production-\nlevel** **context enhancement** in LLM and RAG applications:\n\n  * **LlamaParse:** Offers sophisticated parsing for complex documents, making it possible to answer detailed queries. \n  * **Managed Ingestion and Retrieval API:** Facilitates easier data management, connecting with over 150 sources and 40+ storage solutions. \n\nLlamaParse is now available for a public preview, primarily focusing on PDFs\nwith a user cap, while the API is in a private preview for select enterprise\npartners. If you haven\u2019t explored these new features yet, we invite you to [\ncheck them out ](https://blog.llamaindex.ai/introducing-llamacloud-and-\nllamaparse-af8cedf9006b) for more details or to discuss commercial terms.\n\nYour innovation inspires us! We\u2019re eager to see the projects, articles, or\nvideos that inspire you. Share your remarkable works with us at [\nnews@llamaindex.ai ](mailto:news@llamaindex.ai) . And if you haven\u2019t already,\nsubscribe to our newsletter on our website to receive the latest LlamaIndex\nupdates straight to your inbox.\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1414, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "57ef073f-1ac3-4a8d-b96a-de078e1543e0": {"__data__": {"id_": "57ef073f-1ac3-4a8d-b96a-de078e1543e0", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-02-27-4b9102a0f824.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201327", "date": "Feb 27, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-27-4b9102a0f824"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4532b31d-298d-4194-8b14-6f6600bd0257", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-02-27-4b9102a0f824.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201327", "date": "Feb 27, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-27-4b9102a0f824"}, "hash": "e44d2997f908f250a92a0aae7031718416882a516273f40f321cd4b6cdc8c59d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a74719b-11be-43ed-b862-c3ca8b1eabff", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-02-27-4b9102a0f824.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201327", "date": "Feb 27, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-27-4b9102a0f824"}, "hash": "2def2f3f0fccacb2b6d403af32c6a19e9c31b881b64494e11317c69b5f82af4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af9e2a35-b430-4ac7-a010-7257d9e834bb", "node_type": "1", "metadata": {}, "hash": "1ab1f152019a4c176d61e190f9dffc27ac3ec122963267e2cfbf207ef80316f7", "class_name": "RelatedNodeInfo"}}, "text": "**The highlights:**\n\n  1. **Enhanced RAG Retrieval with Sub-Document Summaries:** Introducing a novel chunking method that improves RAG performance by incorporating hierarchical metadata into chunks, ensuring precise and context-aware information retrieval. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-subdoc-summary/examples/subdoc-summary.ipynb) , [ Tweet ](https://x.com/llama_index/status/1761793821422264757?s=20) . \n  2. **MistralAI Cookbook:** A comprehensive guide to leveraging the Mistral-Large model from MistralAI, featuring near-GPT-4 reasoning, function calling, and JSON output for cutting-edge applications. [ Docs ](https://docs.llamaindex.ai/en/latest/cookbooks/mistralai.html) , [ Tweet ](https://x.com/llama_index/status/1762231085243719748?s=20) . \n  3. **Gemma Cookbook:** A comprehensive guide to using Gemma, GoogleDeepMind\u2019s latest LLM offering, with options for 2B and 7B parameters, facilitating the development of local RAG systems on your laptop. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/ollama_gemma.ipynb) , [ Tweet ](https://x.com/jerryjliu0/status/1760471196402069771?s=20) . \n  4. **ColBERT Integration:** Document reranking with ColBERT via LlamaIndex, delivering a solution that is about 100x faster than BERT-based models for more efficient data processing. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/node_postprocessor/ColbertRerank.ipynb) , [ Tweet ](https://x.com/llama_index/status/1760830777179471933?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 1414, "end_char_idx": 3000, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "af9e2a35-b430-4ac7-a010-7257d9e834bb": {"__data__": {"id_": "af9e2a35-b430-4ac7-a010-7257d9e834bb", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-02-27-4b9102a0f824.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201327", "date": "Feb 27, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-27-4b9102a0f824"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4532b31d-298d-4194-8b14-6f6600bd0257", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-02-27-4b9102a0f824.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201327", "date": "Feb 27, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-27-4b9102a0f824"}, "hash": "e44d2997f908f250a92a0aae7031718416882a516273f40f321cd4b6cdc8c59d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "57ef073f-1ac3-4a8d-b96a-de078e1543e0", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-02-27-4b9102a0f824.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201327", "date": "Feb 27, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-27-4b9102a0f824"}, "hash": "92c05e2dd5f9836704ab4502d989c7000d425221ce702b95620b652229fca439", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "618aa816-a97d-48b5-a5f6-65027c369d59", "node_type": "1", "metadata": {}, "hash": "5891e61fa83f28c345eafeb28e896f6e1287af3dafb4502ba319d79b01448926", "class_name": "RelatedNodeInfo"}}, "text": "5. **Counselor Copilot \u2014 Social Impact Through RAG:** Spotlight on Counselor Copilot, an innovative RAG project supporting the Trevor Project\u2019s crisis counselors, providing real-time assistance with context, suggestions, and actions to aid LGBTQ+ youth effectively. [ BlogPost ](https://blog.llamaindex.ai/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3) , [ Tweet ](https://x.com/llama_index/status/1761433854458614075?s=20) . \n\n", "mimetype": "text/plain", "start_char_idx": 3000, "end_char_idx": 3463, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "618aa816-a97d-48b5-a5f6-65027c369d59": {"__data__": {"id_": "618aa816-a97d-48b5-a5f6-65027c369d59", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-02-27-4b9102a0f824.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201327", "date": "Feb 27, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-27-4b9102a0f824"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4532b31d-298d-4194-8b14-6f6600bd0257", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-02-27-4b9102a0f824.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201327", "date": "Feb 27, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-27-4b9102a0f824"}, "hash": "e44d2997f908f250a92a0aae7031718416882a516273f40f321cd4b6cdc8c59d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af9e2a35-b430-4ac7-a010-7257d9e834bb", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-02-27-4b9102a0f824.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201327", "date": "Feb 27, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-27-4b9102a0f824"}, "hash": "14010e398be426d0644c4b7a2389eb12158be56e445c62188d01f543603a70b3", "class_name": "RelatedNodeInfo"}}, "text": "**Feature Releases and Enhancements:**\n\n  * We have launched a new chunking strategy to enhance RAG retrieval: Sub-Document Summaries. This approach overcomes the limitations of naive chunking by injecting hierarchical metadata, offering a nuanced balance of global context awareness and precision through subdocument summaries for improved performance. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-subdoc-summary/examples/subdoc-summary.ipynb) , [ Tweet ](https://x.com/llama_index/status/1761793821422264757?s=20) . \n  * We have launched a cookbook for the latest ` mistral-large ` model from MistralAI offering advanced features like near GPT-4 level reasoning, Function calling, JSON Output, and more. [ Docs ](https://docs.llamaindex.ai/en/latest/cookbooks/mistralai.html) , [ Tweet ](https://x.com/llama_index/status/1762231085243719748?s=20) . \n  * We have launched a cookbook on Gemma, a new family of state-of-the-art LLMs by GoogleDeepMind, with 2B and 7B parameter options using Ollama to build local RAG on your laptop. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/ollama_gemma.ipynb) , [ Tweet ](https://x.com/jerryjliu0/status/1760471196402069771?s=20) . \n  * We have introduced ColBERT through LlamaIndex, offering a one-line integration for a reranking model that\u2019s ~100x faster than traditional BERT-based models, ensuring efficient document handling with superior performance. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/node_postprocessor/ColbertRerank.ipynb) , [ Tweet ](https://x.com/llama_index/status/1760830777179471933?s=20) . \n  * We have introduced a way to integrate advanced RAG into full-stack web apps with create-llama, using LlamaPacks, in just two lines of code. [ create-llama ](https://github.com/run-llama/LlamaIndexTS/tree/main/packages/create-llama) , [ Tweet ](https://x.com/llama_index/status/1761159412629336404?s=20) . \n\n**Demos:**\n\n[ Counselor Copilot ](https://github.com/zrizvi93/trevorhack) : An interesting\nRAG project by [ Riya Jagetia ](https://twitter.com/FintechRiya) and team,\ndesigned to assist crisis counselors at the Trevor Project in supporting\nLGBTQ+ youth. This tool acts as a real-time copilot, offering context,\nsuggested replies, and various actions to enhance counselor effectiveness,\nshowcasing a unique and socially impactful application of advanced RAG\ntechniques. [ BlogPost ](https://blog.llamaindex.ai/bridging-the-gap-in-\ncrisis-counseling-introducing-counselor-copilot-db42e26ab4f3) , [ Tweet\n](https://x.com/llama_index/status/1761433854458614075?s=20) .\n\n**Guides:**\n\n  * [ Guide ](https://docs.google.com/presentation/d/1NAAI6DoLEIw7RDvx4vXvFG2jz3WUKVpL4j39-E9MouQ/edit#slide=id.g2b99c281f78_0_0) to simplifying advanced RAG development: Our latest insights pinpoint solutions for key challenges, including our innovative LlamaParse for complex PDF QA, shared in our AI in Production presentation. \n\n**Tutorials:**\n\n  * [ Marco Bertelli ](https://medium.com/@marco.bertelli) [ tutorial ](https://medium.com/@marco.bertelli/unveiling-the-power-of-rag-building-an-interactive-chatbot-with-react-a-comprehensive-guide-99c409a5f69a) on Building an Interactive Chatbot with React. \n  * [ Wenqi Glantz ](https://twitter.com/wenqi_glantz) [ tutorial ](https://towardsdatascience.com/the-journey-of-rag-development-from-notebook-to-microservices-cc065d0210ef) on The Journey of RAG Development: From Notebook to Microservices. \n  * [ Wenqi Glantz ](https://twitter.com/wenqi_glantz) [ tutorial video ](https://www.youtube.com/watch?v=EBpT_cscTis) on 12 RAG Pain Points and Solutions in the RAG pipeline. \n\n**Webinar:**\n\n  * [ Webinar ](https://www.youtube.com/watch?v=ZP1F9z-S7T0) with Sisil from JasperAI on Practical Tips and Tricks for Productionizing RAG. \n\n", "mimetype": "text/plain", "start_char_idx": 3463, "end_char_idx": 7318, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e0276ad8-d3d1-4e63-823e-7ccf677cf833": {"__data__": {"id_": "e0276ad8-d3d1-4e63-823e-7ccf677cf833", "embedding": null, "metadata": {"filename": "bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.md", "extension": ".md", "title": "Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot", "date": "Feb 24, 2024", "url": "https://www.llamaindex.ai/blog/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e013f95-3d19-452b-b708-42053700a82e", "node_type": "4", "metadata": {"filename": "bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.md", "extension": ".md", "title": "Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot", "date": "Feb 24, 2024", "url": "https://www.llamaindex.ai/blog/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3"}, "hash": "844f62d3e81117a3f700e0f4bd6accda22381bdf363630386667a46d731f7020", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9357d29e-c5e6-4598-8647-5b915b6751b7", "node_type": "1", "metadata": {}, "hash": "b9bb17276a2e192d1d84f0f8f246f46d335ca0d44c3de0362ee1afafd711348a", "class_name": "RelatedNodeInfo"}}, "text": "_Co-authored by:[ Riya Jagetia\n](https://medium.com/u/e84f937083e3?source=post_page-----\ndb42e26ab4f3--------------------------------) , _ _[ Tarun Malik\n](https://medium.com/u/7d279643046e?source=post_page-----\ndb42e26ab4f3--------------------------------) , _ _[ Divija N\n](https://medium.com/u/6fe4060f2a53?source=post_page-----\ndb42e26ab4f3--------------------------------) , _ _[ Sharon Tan\n](https://medium.com/u/361137e928c3?source=post_page-----\ndb42e26ab4f3--------------------------------) , _ [ _Zehra Rizvi_\n](https://medium.com/u/7bdf7b817eec?source=post_page-----\ndb42e26ab4f3--------------------------------) , [ _Amanda Piyapanee_\n](https://medium.com/u/b75312a598d6?source=post_page-----\ndb42e26ab4f3--------------------------------)\n\nAt the recent LlamaIndex RAG-a-thon [1], our team\u2019s **\u201cCounselor Copilot\u201d**\nwon 2nd prize in the Traditional track and 1st prize in the Datastax/AstraDB\ncategory. More details can be found on our [ DevPost\n](https://devpost.com/software/counselor-copilot) [2] writeup.\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1022, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9357d29e-c5e6-4598-8647-5b915b6751b7": {"__data__": {"id_": "9357d29e-c5e6-4598-8647-5b915b6751b7", "embedding": null, "metadata": {"filename": "bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.md", "extension": ".md", "title": "Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot", "date": "Feb 24, 2024", "url": "https://www.llamaindex.ai/blog/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e013f95-3d19-452b-b708-42053700a82e", "node_type": "4", "metadata": {"filename": "bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.md", "extension": ".md", "title": "Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot", "date": "Feb 24, 2024", "url": "https://www.llamaindex.ai/blog/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3"}, "hash": "844f62d3e81117a3f700e0f4bd6accda22381bdf363630386667a46d731f7020", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e0276ad8-d3d1-4e63-823e-7ccf677cf833", "node_type": "1", "metadata": {"filename": "bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.md", "extension": ".md", "title": "Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot", "date": "Feb 24, 2024", "url": "https://www.llamaindex.ai/blog/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3"}, "hash": "21137fb7223393970c5d97718292585961f41298be8927028c0e63610378537f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c8b172c-a586-4077-a307-52d5e88e7293", "node_type": "1", "metadata": {}, "hash": "45a612f58086000be6ac7f018b593a7788c8c751b4b5644edf88f034a8e54225", "class_name": "RelatedNodeInfo"}}, "text": "##  Introduction\n\nAgainst the backdrop of growing strain on mental health services [3, 4], non-\nprofit organizations like The Trevor Project [5] are a critical part of the\ncare ecosystem. Focusing on helping LGBTQ+ youth who are contemplating\nsuicide, The Trevor Project provides accessible crisis services including via\nTrevorText, an online chat service with trained volunteer counselors.\n\n##  Problem: The Dual Challenge Faced by Crisis Counselors\n\nHowever, TrevorText counselors face significant challenges. Not only is there\nhigh demand for counselors during busy times like holidays and night shifts,\nbut also, counselors have to juggle a number of administrative tasks such as\nsifting through forms, responding to messages across multiple chats, and\nlocating relevant local resources. This not only increases the risk of\ncounselors burning out but also hampers their ability to provide timely and\neffective care.\n\nIn light of these challenges, there\u2019s a pressing need for innovative solutions\nto bridge the gap between the demand and supply of crisis services.\n\nWhile our hackathon project focused on augmenting TrevorText, our product can\nbe easily extended to general crisis chat alternatives as well.\n\n##  The Winning Solution: An AI Copilot for Crisis Counselors\n\nCounselor Copilot is a real-time assistant for crisis counselors that takes\ninto account contact context and chat history to suggest replies so that\ncounselors can focus on what they do best: providing care. **There is no\nprompting that is needed from counselors; the copilot works seamlessly in the\nbackground.**\n\nFurther, the copilot never directly replies to contacts; instead, replies are\nsuggested and can be edited.\n\nCounselor copilot takes into account contact context and chat history to\nprovide real-time reply suggestions to the counselors\n\nSpecifically, the copilot automates counselor tasks that include but are not\nlimited to:\n\n  1. Retrieving and synthesizing contact data from complex PDFs in real-time. This also provides counselors context on their contacts when conversations are initiated. \n  ", "mimetype": "text/plain", "start_char_idx": 1022, "end_char_idx": 3109, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c8b172c-a586-4077-a307-52d5e88e7293": {"__data__": {"id_": "3c8b172c-a586-4077-a307-52d5e88e7293", "embedding": null, "metadata": {"filename": "bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.md", "extension": ".md", "title": "Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot", "date": "Feb 24, 2024", "url": "https://www.llamaindex.ai/blog/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e013f95-3d19-452b-b708-42053700a82e", "node_type": "4", "metadata": {"filename": "bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.md", "extension": ".md", "title": "Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot", "date": "Feb 24, 2024", "url": "https://www.llamaindex.ai/blog/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3"}, "hash": "844f62d3e81117a3f700e0f4bd6accda22381bdf363630386667a46d731f7020", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9357d29e-c5e6-4598-8647-5b915b6751b7", "node_type": "1", "metadata": {"filename": "bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.md", "extension": ".md", "title": "Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot", "date": "Feb 24, 2024", "url": "https://www.llamaindex.ai/blog/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3"}, "hash": "3b253b5efb0518b9c0617a108b516c52d938815a93905d6d614c85c4fb35e738", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd52511f-8cb2-4daf-9542-3e278bc8069e", "node_type": "1", "metadata": {}, "hash": "0b01e70f6787c26ebaa1e7520b1101e24d7df044654bf933269516dd6f3d0eaf", "class_name": "RelatedNodeInfo"}}, "text": "2. Assessing from the chat context if emergency intervention is required. If so, suggesting escalation to a supervisor. \n  3. Using existing resources and guidelines from the organization to suggest appropriate replies. \n\n4\\. Searching for location-specific resources for contacts, and quickly\nsharing those resources via email.\n\n5\\. Completing case forms in a CRM for contacts, including summarizing the\ninteraction.\n\nWhile these tasks are important and necessary, they pull attention away from\nconversations with youth in crisis and take up precious time.\n\nWith Counselor copilot, these tasks are completed when they are required and\nwithout any prompting from counselors, providing more bandwidth for counselors\nand ultimately leading to higher-quality conversations with patients.\n\nBelow is a demo of our solution:\n\n##  How we built it\n\nWhen the chat is initiated, the Counselor Copilot gets the contact\u2019s data from\nthe CRM, which is stored in complex PDFs. We used LlamaParse to extract\nrelevant contact data in real-time and then provide a summary of that data to\ncounselors as context at the beginning of each conversation.\n\nFurther, we used a LlamaIndex ReAct Agent to monitor the conversation and \u2014\nbased on the chat history and contact context \u2014 deploy the right tool. Tools\nat the ReAct Agent\u2019s disposal include:\n\n  1. Escalating the conversation to a supervisor \n  2. ", "mimetype": "text/plain", "start_char_idx": 3109, "end_char_idx": 4489, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd52511f-8cb2-4daf-9542-3e278bc8069e": {"__data__": {"id_": "cd52511f-8cb2-4daf-9542-3e278bc8069e", "embedding": null, "metadata": {"filename": "bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.md", "extension": ".md", "title": "Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot", "date": "Feb 24, 2024", "url": "https://www.llamaindex.ai/blog/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e013f95-3d19-452b-b708-42053700a82e", "node_type": "4", "metadata": {"filename": "bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.md", "extension": ".md", "title": "Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot", "date": "Feb 24, 2024", "url": "https://www.llamaindex.ai/blog/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3"}, "hash": "844f62d3e81117a3f700e0f4bd6accda22381bdf363630386667a46d731f7020", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c8b172c-a586-4077-a307-52d5e88e7293", "node_type": "1", "metadata": {"filename": "bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.md", "extension": ".md", "title": "Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot", "date": "Feb 24, 2024", "url": "https://www.llamaindex.ai/blog/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3"}, "hash": "86c2df61a2946b4ba0faaa873d2910158f7dd51f78217ac50a678e0918064fac", "class_name": "RelatedNodeInfo"}}, "text": "Suggesting a response and related resources based on The Trevor Project\u2019s guidelines \n  3. Searching the web for location-specific resources and sending the resources to the contact \n\nFor tool #2, we created a vector database that contains The Trevor Project\u2019s\ndocuments, which highlight key guidelines for counselors based on different\nscenarios and situations that they may face. We used RAG to retrieve resources\nrelevant to the conversation, and GPT4 to draft a response for the counselor\nbased on those resources, both of which are essential due to the sensitive\nnature of the conversation.\n\nLastly, we used the conversation content to fill out a form with key\nSalesforce fields (e.g. name, age, city, state), as well as to summarize the\nconversation.\n\n##  Possible Extensions\n\nWe\u2019re excited by the potential for others to build on our work [6] and extend\nCounselor Copilot further. Some ideas include:\n\n  1. Reduce costs and improve quality of suggested responses: Fine-tune a state-of-the-art open-source LLM on extracts of chat conversations conducted by counselors \n  2. More targeted conversation management: Add a tool for the agent to assess the stage of the conversation, given that there are recommended styles and questions for each stage (e.g. establishing rapport, risk assessment) \n  3. Closed-loop feedback cycle: Allow counselors to thumbs-up or thumbs-down selected responses, as a natural way to collect human feedback which can be used for further model or agent training \n\n##  Conclusion: A Step Toward Efficient and Effective Crisis Care\n\nOur AI copilot for crisis counselors represents a significant step toward more\nefficient and effective crisis care. By automating administrative tasks, it\nfrees up counselors to focus on their core mission of providing youth in\ncrisis a safe place to talk. This not only enhances the quality of care\nprovided but also addresses the pressing issue of counselor shortage by\nmaximizing the impact of existing resources. As we continue to refine and\nexpand this technology, we envision a future where crisis counseling is more\naccessible, responsive, and impactful for all those in need\u200b\u200b.\n\n##  References\n\n  1. [ https://rag-a-thon.devpost.com/ ](https://rag-a-thon.devpost.com/)\n  2. [ https://devpost.com/software/counselor-copilot ](https://devpost.com/software/counselor-copilot)\n  3. [ https://www.mhanational.org/issues/state-mental-health-america ](https://www.mhanational.org/issues/state-mental-health-america)\n  4. [ https://www.aamc.org/news/growing-psychiatrist-shortage-enormous-demand-mental-health-services ](https://www.aamc.org/news/growing-psychiatrist-shortage-enormous-demand-mental-health-services)\n  5. [ https://www.thetrevorproject.org/ ](https://www.thetrevorproject.org/)\n  6. [ https://github.com/zrizvi93/trevorhack ](https://github.com/zrizvi93/trevorhack)\n\n", "mimetype": "text/plain", "start_char_idx": 4489, "end_char_idx": 7337, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae339b9a-8def-4927-89b4-16068e692e80": {"__data__": {"id_": "ae339b9a-8def-4927-89b4-16068e692e80", "embedding": null, "metadata": {"filename": "introducing-llamacloud-and-llamaparse-af8cedf9006b.md", "extension": ".md", "title": "Introducing LlamaCloud and LlamaParse", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fc37359-c51e-48bb-804a-ef7d247412bd", "node_type": "4", "metadata": {"filename": "introducing-llamacloud-and-llamaparse-af8cedf9006b.md", "extension": ".md", "title": "Introducing LlamaCloud and LlamaParse", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b"}, "hash": "dd36f5832f90e7453706f7ef2c229023e283222702b5381a97fe030b8c5ae902", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e012de1-bfcd-4851-b9b2-f29350ba5c39", "node_type": "1", "metadata": {}, "hash": "fd3085641d592917b978771656563acc6600d7ebe06b6ddbe491383ba66e5d93", "class_name": "RelatedNodeInfo"}}, "text": "Today is a big day for the LlamaIndex ecosystem: we are announcing LlamaCloud,\na new generation of managed parsing, ingestion, and retrieval services,\ndesigned to bring **production-grade** **context-augmentation** to your LLM\nand RAG applications.\n\nUsing LlamaCloud as an enterprise AI engineer, you can focus on writing the\nbusiness logic and not on data wrangling. Process large volumes of production\ndata, immediately leading to better response quality. LlamaCloud launches with\nthe following key components:\n\n  1. **LlamaParse:** Proprietary parsing for complex documents with embedded objects such as tables and figures. LlamaParse directly integrates with LlamaIndex ingestion and retrieval to let you build retrieval over complex, semi-structured documents. You\u2019ll be able to answer complex questions that simply weren\u2019t possible previously. \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 853, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e012de1-bfcd-4851-b9b2-f29350ba5c39": {"__data__": {"id_": "3e012de1-bfcd-4851-b9b2-f29350ba5c39", "embedding": null, "metadata": {"filename": "introducing-llamacloud-and-llamaparse-af8cedf9006b.md", "extension": ".md", "title": "Introducing LlamaCloud and LlamaParse", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fc37359-c51e-48bb-804a-ef7d247412bd", "node_type": "4", "metadata": {"filename": "introducing-llamacloud-and-llamaparse-af8cedf9006b.md", "extension": ".md", "title": "Introducing LlamaCloud and LlamaParse", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b"}, "hash": "dd36f5832f90e7453706f7ef2c229023e283222702b5381a97fe030b8c5ae902", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae339b9a-8def-4927-89b4-16068e692e80", "node_type": "1", "metadata": {"filename": "introducing-llamacloud-and-llamaparse-af8cedf9006b.md", "extension": ".md", "title": "Introducing LlamaCloud and LlamaParse", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b"}, "hash": "6f38adbf7e95294a22a21e9a5e2749a4e6f3029e74dac8563f2052e8d39303ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1a44eb5-8121-4e86-804b-72dc2cf5484b", "node_type": "1", "metadata": {}, "hash": "24916966c39a0f6dd142c240ace70c5d1ff2e4d89cfc40e5cbcc17e7ad8ded0e", "class_name": "RelatedNodeInfo"}}, "text": "2. **Managed Ingestion and Retrieval API:** An API which allows you to easily load, process, and store data for your RAG app and consume it in any language. Backed by data sources in [ LlamaHub ](https://llamahub.ai/) , including LlamaParse, and our data storage integrations. \n\n", "mimetype": "text/plain", "start_char_idx": 853, "end_char_idx": 1132, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1a44eb5-8121-4e86-804b-72dc2cf5484b": {"__data__": {"id_": "c1a44eb5-8121-4e86-804b-72dc2cf5484b", "embedding": null, "metadata": {"filename": "introducing-llamacloud-and-llamaparse-af8cedf9006b.md", "extension": ".md", "title": "Introducing LlamaCloud and LlamaParse", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fc37359-c51e-48bb-804a-ef7d247412bd", "node_type": "4", "metadata": {"filename": "introducing-llamacloud-and-llamaparse-af8cedf9006b.md", "extension": ".md", "title": "Introducing LlamaCloud and LlamaParse", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b"}, "hash": "dd36f5832f90e7453706f7ef2c229023e283222702b5381a97fe030b8c5ae902", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e012de1-bfcd-4851-b9b2-f29350ba5c39", "node_type": "1", "metadata": {"filename": "introducing-llamacloud-and-llamaparse-af8cedf9006b.md", "extension": ".md", "title": "Introducing LlamaCloud and LlamaParse", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b"}, "hash": "e855ee7fad3b741a324d18c6c3086cf761870abff7fb7d7de6be4ea1808330ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb34f723-a4f1-4ec6-9e13-4fca08352897", "node_type": "1", "metadata": {}, "hash": "d8e197fb2d462f394e0215bc073c9105210773d3d154ae4f1af3c19883f077d4", "class_name": "RelatedNodeInfo"}}, "text": "LlamaParse is available in a public preview setting starting today. It can\ncurrently handle PDFs and usage is capped for public use; [ contact us\n](https://llamaindex.ai/contact) for commercial terms. The managed ingestion\nand retrieval API is available as a private preview; we are offering access to\na limited set of enterprise design partners. If you\u2019re interested, [ get in\ntouch ](https://llamaindex.ai/contact) . (We\u2019ve also launched a [ new version\nof our website ](https://www.llamaindex.ai/) !)\n\n#  RAG is Only as Good as your Data\n\nA core promise of LLMs is the ability to automate knowledge search, synthesis,\nextraction, and planning over any source of unstructured data. Over the past\nyear a new data stack has emerged to power these context-augmented LLM\napplications, popularly referred to as Retrieval-Augmented Generation (RAG).\nThis stack includes loading data, processing it, embedding it, and loading\ninto a vector database. This enables downstream orchestration of retrieval and\nprompting to provide context within an LLM app.\n\nThis stack is different from any ETL stack before it, because unlike\ntraditional software, every decision in the data stack directly **affects the\naccuracy** of the full LLM-powered system. Every decision like chunk size and\nembedding model affects LLM outputs, and since LLMs are black boxes, you can\u2019t\nunit test your way to correct behavior.\n\nWe\u2019ve spent the past year hard at work at the forefront of providing tooling\nand educating users on how to build high-performing, advanced RAG for various\nuse cases. We crossed the 2M monthly download mark, and are used by large\nenterprises to startups, including Adyen, T-Systems, Jasper.ai, Weights and\nBiases, DataStax, and many more.\n\nBut while getting started with our famous 5-line starter example is easy,\nbuilding production-grade RAG remains a complex and subtle problem. In our\nhundreds of user conversations, we learned the biggest pain points:\n\n  * **Results aren\u2019t accurate enough:** The application was not able to produce satisfactory results for a long-tail of input tasks/queries. \n  * **The number of parameters to tune is overwhelming:** It\u2019s not clear which parameters across the data parsing, ingestion, retrieval. \n  * **PDFs are specifically a problem:** I have complex docs with lots of messy formatting. How do I represent this in the right way so the LLM can understand it? \n  * **Data syncing is a challenge:** Production data often updates regularly, and continuously syncing new data brings a new set of challenges. \n\nThese are the problems we set out to solve with LlamaCloud.\n\n#  Data Pipelines to Bring you to Production\n\nWe built LlamaCloud and LlamaParse as the data pipelines to get your RAG\napplication to production more quickly.\n\n#  LlamaParse\n\nLlamaParse is a state-of-the-art parser designed to specifically unlock RAG\nover complex PDFs with embedded tables and charts. This simply wasn\u2019t possible\nbefore with other approaches, and we\u2019re incredibly excited about this\ntechnology.\n\nLlamaParse Demo. Given a PDF file, returns a parsed markdown file that\nmaintains semantic structure within the document.\n\nFor the past few months we\u2019ve been obsessed with this problem. This is a\nsurprisingly prevalent use case across a variety of data types and verticals,\nfrom ArXiv papers to 10K filings to medical reports.\n\nNaive chunking and retrieval algorithms do terribly. We were the first to\npropose a [ novel recursive retrieval RAG technique\n](https://docs.llamaindex.ai/en/stable/examples/query_engine/pdf_tables/recursive_retriever.html)\nfor being able to hierarchically index and query over tables and text in a\ndocument. The only challenge that remained was how to properly parse out\ntables and text in the first place.\n\nComparison of LlamaParse vs. PyPDF over the Apple 10K filing. [ Full\ncomparisons are here\n](https://drive.google.com/file/d/1fyQAg7nOtChQzhF2Ai7HEeKYYqdeWsdt/view?usp=sharing)\n. A green highlight in a cell means that the RAG pipeline correctly returned\nthe cell value as the answer to a question over that cell. A red highlight\nmeans that the question was answered incorrectly.\n\nThis is where LlamaParse comes in. We\u2019ve developed a proprietary parsing\nservice that is incredibly good at parsing PDFs with complex tables into a\nwell-structured markdown format. This representation directly plugs into the\nadvanced Markdown parsing and recursive retrieval algorithms available in the\nopen-source library. The end result is that you are able to build RAG over\ncomplex documents that can answer questions over both tabular and unstructured\ndata. Check out the results below for a comparison:\n\nComparison of baseline PDF approach (top) vs. LlamaParse + advanced retrieval\n(bottom)  Results over the [ Uber 10K Dataset ](https://github.com/run-\nllama/llama-hub/tree/main/llama_hub/llama_datasets/10k/uber_2021) . For more\ninformation on our evaluation metrics check out our [ evaluation page\n](https://docs.llamaindex.ai/en/stable/module_guides/evaluating/root.html)\nhere.\n\nThis service is available in a **public preview mode:** available to everyone,\nbut with a usage limit (1k pages per day). It operates as a standalone service\nthat also plugs into our managed ingestion and retrieval API (see below).\nCheck out our [ LlamaParse onboarding here\n](https://docs.cloud.llamaindex.ai/llamaparse/) for more details.\n\n    \n    \n    from llama_parse import LlamaParse\n    \n    parser = LlamaParse(\n        api_key=\"llx-...\",  # can also be set in your env as LLAMA_CLOUD_API_KEY\n        result_type=\"markdown\",  # \"markdown\" and \"text\" are available\n        verbose=True\n    )\n\nFor unlimited commercial use of LlamaParse, [ get in touch\n](https://llamaindex.ai/contact) with us.\n\n**Next Steps**\n\nOur early users have already given us important feedback on what they\u2019d like\nto see next. Currently we primarily support PDFs with tables, but we are also\nbuilding out better support for figures, and and an expanded set of the most\npopular document types: .docx, .pptx, .html.\n\n#  Managed Ingestion and Retrieval\n\nOur other main offering in LlamaCloud is a managed ingestion and retrieval API\nwhich allows you to easily declare performant data pipelines for any context-\naugmented LLM application.\n\nGet clean data for your LLM application, so you can spend less time wrangling\ndata and more time writing core application logic. LlamaCloud empowers\nenterprise developers with the following benefits:\n\n  * **Engineering Time Savings:** Instead of having to write custom connectors and parsing logic in Python, our APIs allow you to directly connect to different data sources. \n  * **Performance:** we provide good out-of-the-box performance for different data types, while offering an intuitive path for experimentation, evaluation, and improvement. \n  * **Ease Systems Complexity:** Handle a large number of data sources with incremental updates. \n\nLet\u2019s do a brief tour through the core components!\n\n  ", "mimetype": "text/plain", "start_char_idx": 1132, "end_char_idx": 8072, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb34f723-a4f1-4ec6-9e13-4fca08352897": {"__data__": {"id_": "eb34f723-a4f1-4ec6-9e13-4fca08352897", "embedding": null, "metadata": {"filename": "introducing-llamacloud-and-llamaparse-af8cedf9006b.md", "extension": ".md", "title": "Introducing LlamaCloud and LlamaParse", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fc37359-c51e-48bb-804a-ef7d247412bd", "node_type": "4", "metadata": {"filename": "introducing-llamacloud-and-llamaparse-af8cedf9006b.md", "extension": ".md", "title": "Introducing LlamaCloud and LlamaParse", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b"}, "hash": "dd36f5832f90e7453706f7ef2c229023e283222702b5381a97fe030b8c5ae902", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1a44eb5-8121-4e86-804b-72dc2cf5484b", "node_type": "1", "metadata": {"filename": "introducing-llamacloud-and-llamaparse-af8cedf9006b.md", "extension": ".md", "title": "Introducing LlamaCloud and LlamaParse", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b"}, "hash": "482c7988066044a6b984d765911e6e4c311a4bb1f869175a1a1e7818b9f024a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "abf4768d-cbeb-42c4-bebe-9921ec8a4e09", "node_type": "1", "metadata": {}, "hash": "8c3d403e6cb7548b400171627c1f261dffc32dfcb03b34b5aece18d0ab94d618", "class_name": "RelatedNodeInfo"}}, "text": "1. **Ingestion:** Declare a managed pipeline to process and transform/chunk/embed data backed by our 150+ data sources in LlamaHub and our 40+ storage integrations as destinations. Automatically handle syncing and load balancing. ", "mimetype": "text/plain", "start_char_idx": 8072, "end_char_idx": 8302, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abf4768d-cbeb-42c4-bebe-9921ec8a4e09": {"__data__": {"id_": "abf4768d-cbeb-42c4-bebe-9921ec8a4e09", "embedding": null, "metadata": {"filename": "introducing-llamacloud-and-llamaparse-af8cedf9006b.md", "extension": ".md", "title": "Introducing LlamaCloud and LlamaParse", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fc37359-c51e-48bb-804a-ef7d247412bd", "node_type": "4", "metadata": {"filename": "introducing-llamacloud-and-llamaparse-af8cedf9006b.md", "extension": ".md", "title": "Introducing LlamaCloud and LlamaParse", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b"}, "hash": "dd36f5832f90e7453706f7ef2c229023e283222702b5381a97fe030b8c5ae902", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb34f723-a4f1-4ec6-9e13-4fca08352897", "node_type": "1", "metadata": {"filename": "introducing-llamacloud-and-llamaparse-af8cedf9006b.md", "extension": ".md", "title": "Introducing LlamaCloud and LlamaParse", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b"}, "hash": "140fe73059bd3c7862cef1e2f74ef0ff1740236d4c40ec5aaafc8fc3ab3d22d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "191eee4f-1cb3-453f-9882-5a023ef3b322", "node_type": "1", "metadata": {}, "hash": "e4a12469a2c76745dd6886f73e675fc27d35330b6a6fa6825abd0a4e184d380f", "class_name": "RelatedNodeInfo"}}, "text": "Define through the UI or our open-source library. \n  ", "mimetype": "text/plain", "start_char_idx": 8302, "end_char_idx": 8355, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "191eee4f-1cb3-453f-9882-5a023ef3b322": {"__data__": {"id_": "191eee4f-1cb3-453f-9882-5a023ef3b322", "embedding": null, "metadata": {"filename": "introducing-llamacloud-and-llamaparse-af8cedf9006b.md", "extension": ".md", "title": "Introducing LlamaCloud and LlamaParse", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fc37359-c51e-48bb-804a-ef7d247412bd", "node_type": "4", "metadata": {"filename": "introducing-llamacloud-and-llamaparse-af8cedf9006b.md", "extension": ".md", "title": "Introducing LlamaCloud and LlamaParse", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b"}, "hash": "dd36f5832f90e7453706f7ef2c229023e283222702b5381a97fe030b8c5ae902", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "abf4768d-cbeb-42c4-bebe-9921ec8a4e09", "node_type": "1", "metadata": {"filename": "introducing-llamacloud-and-llamaparse-af8cedf9006b.md", "extension": ".md", "title": "Introducing LlamaCloud and LlamaParse", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b"}, "hash": "f757fe9f68d21c50b307528c0fde2e0593b5b19627ab95af2ef93102f8ac8217", "class_name": "RelatedNodeInfo"}}, "text": "2. **Retrieval:** Get access to state-of-the-art, advanced retrieval backed by our open-source library and your data storage. Wrap it in an easy-to-use REST API that you can consume from any language. \n  3. **Playground:** Interactive UI to test and refine your ingestion/retrieval strategies pre-deployment, with evaluations in the loop. \n\nLlamaCloud Playground: configure, evaluate, and optimize your\ningestion/retrieval pipeline before deployment.  LlamaCloud Retrieval: Access\nadvanced retrieval over your storage system via an API.\n\nWe are opening up a private beta to a limited set of enterprise partners for\nthe managed ingestion and retrieval API. If you\u2019re interested in centralizing\nyour data pipelines and spending more time working on your actual RAG use\ncases, come [ talk to us ](https://llamaindex.ai/contact) .\n\n#  Launch Partners and Collaborators\n\nWe opened up access to LlamaParse at [ our hackathon ](https://rag-a-\nthon.devpost.com/) we co-hosted with [ Futureproof Labs\n](https://www.futureproofsv.com/) and [ Datastax ](https://www.datastax.com/)\nat the beginning of February. We saw some incredible applications of\nLlamaParse in action, [ including parsing building codes for Accessory\nDwelling Unit (ADU) planning ](/pioneering-the-future-of-housing-introducing-\ngenai-driven-adu-planning-ea950be71e2f) , [ parsing real-estate disclosures\nfor home buying ](https://devpost.com/software/home-ai) , and dozens more.\n\nEric Ciarla, co-founder at Mendable AI, incorporated LlamaParse into\nMendable\u2019s data stack: \u201cWe integrated LlamaParse into our [ open source data\nconnector repo ](https://github.com/mendableai/data-connectors) which powers\nour production ingestion suite. It was easy to integrate and more powerful\nthan any of the alternatives we tried.\u201d\n\nWe\u2019re also excited to be joined by initial launch partners and collaborators\nin the LLM and AI ecosystem, from storage to compute.\n\n**DataStax**\n\nDatastax has incorporated LlamaParse into their RAGStack to bring a privacy-\npreserving out-of-the-box RAG solution for enterprises: \"Last week one of our\ncustomers Imprompt has launched a pioneering 'Chat-to-Everything' platform\nleveraging RAGStack powered by LlamaIndex to enhance their enterprise\nofferings while prioritizing privacy.\" said Davor Bonaci, CTO and executive\nvice president, DataStax. \"We're thrilled to partner with LlamaIndex to bring\na streamlined solution to market. By incorporating LlamaIndex into RAGStack,\nwe are providing enterprise developers with a comprehensive Gen AI stack that\nsimplifies the complexities of RAG implementation, while offering long-term\nsupport and compatibility assurance.\u201d\n\n**MongoDB**\n\n\u201cMongoDB\u2019s partnership with LlamaIndex allows for the ingestion of data into\nthe MongoDB Atlas Vector database, and the retrieval of the index from Atlas\nvia LlamaParse and LlamaCloud, enabling the development of RAG systems and\nother AI applications,\u201d said Greg Maxson, Global Lead, AI Ecosystems at\nMongoDB. \u201cNow, developers can abstract complexities associated with data\ningestion, simplify RAG pipeline implementations, and more cost effectively\ndevelop large language model applications, ultimately accelerating generative\nAI app development and more quickly bringing apps to market.\u201d\n\n**Qdrant**\n\nAndr\u00e9 Zayarni, CEO of Qdrant, says \u201cThe Qdrant team is excited to partner with\nLlamaIndex to combine the power of optimal data preprocessing, vectorization,\nand ingestion with Qdrant for a powerful fullstack RAG solution.\u201d\n\n**NVIDIA**\n\nWe are also excited to collaborate with NVIDIA to integrate LlamaIndex with\nthe [ NVIDIA AI Enterprise ](https://www.nvidia.com/en-us/data-\ncenter/products/ai-enterprise/) software platform for production AI:\n\u201cLlamaCloud will help enterprises get generative AI applications from\ndevelopment into production with connectors that link proprietary data to the\npower of large language models,\u201d said Justin Boitano, vice president,\nEnterprise and Edge Computing, NVIDIA. \u201cPairing LlamaCloud with NVIDIA AI\nEnterprise can accelerate the end-to-end LLM pipeline \u2014 including data\nprocessing, embedding creation, indexing, and model inference on accelerated\ncomputing across clouds, data centers and out to the edge.\u201d\n\n#  FAQ\n\n**Is this competitive with vector databases?**\n\nNo. LlamaCloud is focused primarily on data parsing and ingestion, which is a\ncomplementary layer to any vector storage provider. The retrieval layer is\norchestration on top of an existing storage system. LlamaIndex open-source\nintegrates with 40+ of the most popular vector databases, and we are working\nhard to do the following:\n\n  1. Integrate LlamaCloud with storage providers of existing design partners \n  2. Make LlamaCloud available in a more \u201cself-serve\u201d manner. \n\n#  Next Steps\n\nAs mentioned in the above sections, LlamaParse is available for public preview\nstarting today with a usage cap. LlamaCloud is in a private preview mode; we\nare offering access to a limited set of enterprise design partners. If you\u2019re\ninterested come talk to us!\n\nLlamaParse: [ Repo ](https://github.com/run-llama/llama_parse) , [ Cookbook\n](https://github.com/run-\nllama/llama_parse/blob/main/examples/demo_advanced.ipynb) , [ Contact Us\n](https://www.llamaindex.ai/contact)\n\nLlamaCloud: [ Contact Us ](https://www.llamaindex.ai/contact)\n\n", "mimetype": "text/plain", "start_char_idx": 8355, "end_char_idx": 13650, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b3ce324-70a4-4d36-971f-7e42bd1d34a0": {"__data__": {"id_": "2b3ce324-70a4-4d36-971f-7e42bd1d34a0", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201320: introducing LlamaCloud", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ea1967e3-2009-45c0-a71c-01bad66e14eb", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201320: introducing LlamaCloud", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4"}, "hash": "ff5680cedac5a3c59a07c7fb60e37e12d028035caed5fca2e26b84eab7bdbcdf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e29cb32-defe-4a40-bd86-f22b5af0e2ce", "node_type": "1", "metadata": {}, "hash": "deab98d1d98cf0fa1a300b799a6bbccce5a7e60604e7157c91c13e74ac98ed6b", "class_name": "RelatedNodeInfo"}}, "text": "Hi there, LlamaIndex Enthusiasts ,\n\nToday marks a milestone for the LlamaIndex ecosystem with the [ introduction\nof LlamaCloud ](/introducing-llamacloud-and-llamaparse-af8cedf9006b) , a next-\ngeneration suite of managed parsing, ingestion, and retrieval services\ntailored for **production-grade** **context augmentation** in your LLM and RAG\napplications.\n\nAs an enterprise AI engineer using LlamaCloud, you can concentrate on crafting\nthe business logic, leaving the heavy lifting of data management to us.\nProcess vast amounts of production data effortlessly, enhancing response\nquality instantly. LlamaCloud debuts with:\n\n  * **LlamaParse:** A specialized parsing service for complex documents, including tables and figures, seamlessly integrated with LlamaIndex for handling semi-structured documents. This enables answering intricate queries previously out of reach. \n  * **Managed Ingestion and Retrieval API:** Simplify data loading, processing, and storage for your RAG applications, supported by over 150 data sources via [ LlamaHub ](https://llamahub.ai/) , including LlamaParse, and more than 40 data storage solutions. \n\nLlamaParse is now in public preview, with a current focus on PDFs and a usage\ncap for public users; [ contact us ](https://llamaindex.ai/contact) for\ncommercial terms. The managed API is in private preview, and available to a\nselect group of enterprise partners. Interested? ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1408, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e29cb32-defe-4a40-bd86-f22b5af0e2ce": {"__data__": {"id_": "1e29cb32-defe-4a40-bd86-f22b5af0e2ce", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201320: introducing LlamaCloud", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ea1967e3-2009-45c0-a71c-01bad66e14eb", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201320: introducing LlamaCloud", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4"}, "hash": "ff5680cedac5a3c59a07c7fb60e37e12d028035caed5fca2e26b84eab7bdbcdf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b3ce324-70a4-4d36-971f-7e42bd1d34a0", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201320: introducing LlamaCloud", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4"}, "hash": "00cad552893f9ba49d9fc38d69af2da5b0a434d630aa9c7a65b4376eba6abe21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "180f436d-aca5-4c60-9ee3-84f0c1f62231", "node_type": "1", "metadata": {}, "hash": "a47bea6138314ebcb530c23b83a71a0d0064388cc979e1134d16821411e25c1e", "class_name": "RelatedNodeInfo"}}, "text": "[ Reach out\n](https://llamaindex.ai/contact) for more details.\n\nYour inventive spirit is our driving force! We look forward to discovering the\nprojects, articles, or videos that excite you. Send your outstanding\ncontributions to [ news@llamaindex.ai\n](https://www.notion.so/15c61b205d114f0ebbe0be547a645628?pvs=21) . If you\nhaven\u2019t yet, join our newsletter via our [ website\n](https://www.llamaindex.ai/) to get all the newest LlamaIndex news directly\nin your inbox.\n\n**The highlights:**\n\n  1. **Corrective RAG LlamaPack:** We have launched LlamaPack with CRAG insights, refining information retrieval for enhanced accuracy and relevance. [ Tweet ](https://x.com/llama_index/status/1758530939276378255?s=20) , [ LlamaPack ](https://llamahub.ai/l/llama-packs/llama-index-packs-corrective-rag?from=llama-packs) . \n  2. **SELF-DISCOVER LlamaPack** : We have launched SELF-DISCOVER paper implementation as LlamaPack, leveraging meta-reasoning in LLMs for adaptive, complex problem-solving. [ Tweet ](https://x.com/llama_index/status/1759620529982755324?s=20) , [ LlamaPack ](https://llamahub.ai/l/llama-packs/llama-index-packs-self-discover?from=) . \n  ", "mimetype": "text/plain", "start_char_idx": 1408, "end_char_idx": 2557, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "180f436d-aca5-4c60-9ee3-84f0c1f62231": {"__data__": {"id_": "180f436d-aca5-4c60-9ee3-84f0c1f62231", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201320: introducing LlamaCloud", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ea1967e3-2009-45c0-a71c-01bad66e14eb", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201320: introducing LlamaCloud", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4"}, "hash": "ff5680cedac5a3c59a07c7fb60e37e12d028035caed5fca2e26b84eab7bdbcdf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e29cb32-defe-4a40-bd86-f22b5af0e2ce", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201320: introducing LlamaCloud", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4"}, "hash": "a38ec2079b8e90aca874a8a64ccda65481aae0707e7036423a0961aec7096823", "class_name": "RelatedNodeInfo"}}, "text": "3. **RAG Production** [ **Guide** ](https://www.youtube.com/watch?v=ZP1F9z-S7T0) **:** A comprehensive guide to production-ready RAG, featuring insights and strategies from Sisil Mehta at JasperAI. \n\n**Feature Releases and Enhancements:**\n\n  * We have integrated insights from the Corrective Retrieval Augmented Generation (CRAG) paper as LlamaPack, enhancing our system\u2019s ability to evaluate and refine retrieved information for more accurate and relevant responses. [ Tweet ](https://x.com/llama_index/status/1758530939276378255?s=20) , [ LlamaPack ](https://llamahub.ai/l/llama-packs/llama-index-packs-corrective-rag?from=llama-packs) . \n  * We have integrated SELF-DISCOVER a novel approach as LlamaPack to empower LLMs with meta-reasoning, allowing them to self-discover and adapt the most suitable reasoning modules from a selection, enabling more versatile and complex problem-solving capabilities. [ Tweet ](https://x.com/llama_index/status/1759620529982755324?s=20) , [ LlamaPack ](https://llamahub.ai/l/llama-packs/llama-index-packs-self-discover?from=) . \n\n**Demos:**\n\n  * [ DanswerAI ](https://x.com/ycombinator/status/1757434457382916571?s=20) , an out-of-the-box ChatGPT integration for enterprise knowledge, enhances efficiency across sales, IT, engineering, and customer support teams by connecting to common workplace tools like GDrive, Slack, and Jira, powered by Llama Index. \n  * [ GenAI for ADU Planning: ](https://x.com/llama_index/status/1758207209140601315?s=20) A comprehensive [ app ](https://x.com/llama_index/status/1758207209140601315?s=20) by [ **Rujun Gao** ](https://twitter.com/rujun_gao) that navigates local ADU regulations, analyzes buildable space via satellite imagery, offers floor plan suggestions, and connects users to local contractors, showcasing the power of multi-modal AI automation in enhancing productivity \n\n**Guides:**\n\n  * [ Guide ](https://www.youtube.com/watch?v=ZP1F9z-S7T0) to building production-ready RAG covering practical tips and tricks inspired by [ Sisil Mehta ](https://twitter.com/sisilmehta) from JasperAI. \n  * [ Guide ](https://docs.llamaindex.ai/en/latest/examples/embeddings/nomic.html#) to Nomic-embed-text-v1.5 with LlamaIndex: Get embeddings of any dimension from 64 to 768, inspired by Matryoshka Representation Learning. \n  * [ Guide ](https://github.com/ohdearquant/lionagi/blob/main/notebooks/RAG/AutoResearch2_ReAct.ipynb) to creating a RAG-powered research agent, notebook on building a workflow for scientific investigation, leveraging ArXiv, Wikipedia, textbooks, and more, with capabilities for fetching abstracts, generating ideas, and comprehensive information lookup, powered by Llama Index and LionAGI. \n\n**Tutorials:**\n\n  * [ Jerry Liu ](https://twitter.com/jerryjliu0) [ tutorial ](https://www.youtube.com/watch?v=swJsw9Jvgoc) on Introduction to LlamaIndex v0.10. \n  * [ Jerry Li ](https://twitter.com/jerryjliu0) [ tutorial ](https://www.youtube.com/watch?v=T0bgevj0vto) on Building Agents from scratch using Query Pipelines. \n  * [ Ravi Theja ](https://twitter.com/ravithejads) [ tutorial ](https://www.youtube.com/watch?v=tBA6yRmMF74) video on Building Multi-Modal applications with Ollama and LlamaIndex. \n  * Brett Young [ tutorial ](https://wandb.ai/byyoung3/ml-news/reports/Building-a-RAG-Based-Digital-Restaurant-Menu-with-LlamaIndex-and-W-B-Weave--Vmlldzo2NjE5Njkw) on Building a RAG-Based Digital Restaurant Menu with LlamaIndex and W&B Weave. \n  * [ Raghav ](https://www.linkedin.com/in/raghav-dixit/) [ tutorial ](/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e) on MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB. \n  * [ Tech With Tim ](https://twitter.com/TechWithTimm) [ tutorial ](https://www.youtube.com/watch?v=ul0QsodYct4) on querying unstructured data, analyzing tabular data with Pandas, and actioning results in a concise, step-by-step approach. \n  * [ Florian June ](https://medium.com/@florian_algo) \u2019s [ tutorial ](https://ai.plainenglish.io/advanced-rag-03-using-ragas-llamaindex-for-rag-evaluation-84756b82dca7) on using RAGAs and LlamaIndex for RAG evaluation. \n  * [ Davide Gallitelli ](https://dgallitelli95.medium.com/) \u2019s [ tutorial ](https://dgallitelli95.medium.com/deploying-an-huggingface-embedding-model-to-amazon-sagemaker-and-consuming-it-with-llama-index-4d4f6dcd2fbc) on Deploying a HuggingFace embedding model to Amazon SageMaker and consuming it with Llama-Index. \n\n**Webinar:**\n\n  * [ Webinar ](https://www.youtube.com/watch?v=k5Txq5C_AWA) with Henry Heng, co-founder of Flowise on building advanced no-code RAG apps over your data. \n\n", "mimetype": "text/plain", "start_char_idx": 2557, "end_char_idx": 7182, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "061febb8-3e45-4f38-abb4-3ccefaeedd23": {"__data__": {"id_": "061febb8-3e45-4f38-abb4-3ccefaeedd23", "embedding": null, "metadata": {"filename": "multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.md", "extension": ".md", "title": "MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB", "date": "Feb 17, 2024", "url": "https://www.llamaindex.ai/blog/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8cd3127-a3c7-459c-a783-369cf6a33b74", "node_type": "4", "metadata": {"filename": "multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.md", "extension": ".md", "title": "MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB", "date": "Feb 17, 2024", "url": "https://www.llamaindex.ai/blog/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e"}, "hash": "1d0cb5e3c9e00cae96958510d2fb59f2efee2da8e89212d08150050c675aea97", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "290c23c6-727d-49b9-a85c-f7036a47028d", "node_type": "1", "metadata": {}, "hash": "ed022eaa6f0b7734d12c97a32bcfa6b610447c1b77877a9f23d5c75744593da9", "class_name": "RelatedNodeInfo"}}, "text": "The widespread consumption of videos on platforms like YouTube, Instagram, and\nothers highlights the importance of efficiently processing and analyzing video\ncontent. This capability unlocks vast opportunities across various sectors,\nincluding media and entertainment, security, and education. However, the main\nchallenge is effectively extracting meaningful information from videos, which\nare inherently complex and multimodal data streams.\n\nThis blog post introduces a solution that leverages the LlamaIndex [ Python\nAPI ](https://docs.llamaindex.ai/en/latest/index.html#) for using the advanced\ncapabilities of OpenAI\u2019s [ GPT4V\n](https://help.openai.com/en/articles/8555496-gpt-4v-api) , combined with the\nefficient data management by [ LanceDB ](https://lancedb.github.io/lancedb/)\nacross all data formats, to process videos.\n\n\u2026But what does \u2018RAG\u2019 even mean?\n\nRetrieval-augmented generation (RAG) is a technique that merges information\nretrieval with generative AI to produce systems capable of generating precise\nand contextually relevant responses by tapping into large data repositories.\n\n#  Core Concept of RAG\n\nRAG operates in two stages:\n\n  1. **Retrieval** : Utilizes semantic search to find documents related to a query, leveraging the context and meaning beyond mere keywords. \n  2. **Generation** : Integrates retrieved information to produce coherent responses, allowing the AI to \u201clearn\u201d from a wide range of content dynamically. \n\n#  RAG Architecture\n\nThe architecture typically involves a dense vector search engine for retrieval\nand a transformer model for generation. The process:\n\n  * Performs a semantic search to fetch relevant documents. \n  * Processes these documents with the query to create a comprehensive context. \n  * The generative model then crafts a detailed response based on this enriched context. \n\n#  Extending to Multimodality\n\nMultimodal RAG integrates various data types (text, images, audio, video) in\nboth retrieval and generation phases, enabling richer information sourcing.\nFor example, responding to queries about \u201cclimate change impacts on polar\nbears\u201d might involve retrieving scientific texts, images, and videos to\nproduce an enriched, multi-format response.\n\nLet\u2019s return to our use case and dive into how it\u2019s all done. Moving forward,\nyou can access the full code on [ Google Colab ](https://github.com/run-\nllama/llama_index/blob/main/docs/examples/multi_modal/multi_modal_video_RAG.ipynb)\n.\n\nThe solution is divided into the following sections. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2500, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "290c23c6-727d-49b9-a85c-f7036a47028d": {"__data__": {"id_": "290c23c6-727d-49b9-a85c-f7036a47028d", "embedding": null, "metadata": {"filename": "multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.md", "extension": ".md", "title": "MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB", "date": "Feb 17, 2024", "url": "https://www.llamaindex.ai/blog/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8cd3127-a3c7-459c-a783-369cf6a33b74", "node_type": "4", "metadata": {"filename": "multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.md", "extension": ".md", "title": "MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB", "date": "Feb 17, 2024", "url": "https://www.llamaindex.ai/blog/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e"}, "hash": "1d0cb5e3c9e00cae96958510d2fb59f2efee2da8e89212d08150050c675aea97", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "061febb8-3e45-4f38-abb4-3ccefaeedd23", "node_type": "1", "metadata": {"filename": "multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.md", "extension": ".md", "title": "MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB", "date": "Feb 17, 2024", "url": "https://www.llamaindex.ai/blog/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e"}, "hash": "99bc4587e52c0f960deb29e646690316b90eca01358c8da38a15072a62a4502e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a71302d-5033-48a7-8790-4617a480a82d", "node_type": "1", "metadata": {}, "hash": "40e6dd9ac33be86cf5b0695be393c3421b982b68c8153f4ccfd4da82952756e9", "class_name": "RelatedNodeInfo"}}, "text": "Click on the topic to\nskip to a specific part:\n\n  1. Video Downloading \n  2. Video Processing \n  3. Building the Multi-Modal Index and Vector Store \n  4. Retrieving Relevant Images and Context \n  5. ", "mimetype": "text/plain", "start_char_idx": 2500, "end_char_idx": 2699, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a71302d-5033-48a7-8790-4617a480a82d": {"__data__": {"id_": "2a71302d-5033-48a7-8790-4617a480a82d", "embedding": null, "metadata": {"filename": "multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.md", "extension": ".md", "title": "MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB", "date": "Feb 17, 2024", "url": "https://www.llamaindex.ai/blog/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8cd3127-a3c7-459c-a783-369cf6a33b74", "node_type": "4", "metadata": {"filename": "multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.md", "extension": ".md", "title": "MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB", "date": "Feb 17, 2024", "url": "https://www.llamaindex.ai/blog/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e"}, "hash": "1d0cb5e3c9e00cae96958510d2fb59f2efee2da8e89212d08150050c675aea97", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "290c23c6-727d-49b9-a85c-f7036a47028d", "node_type": "1", "metadata": {"filename": "multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.md", "extension": ".md", "title": "MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB", "date": "Feb 17, 2024", "url": "https://www.llamaindex.ai/blog/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e"}, "hash": "9b7a86624d5abb6e69ac81a6ccfb4bccf73041a1e1ed7159b1648651e554dd09", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8ff0b29-eb62-41c6-b659-37665ce9ee1b", "node_type": "1", "metadata": {}, "hash": "d262e27e3b3bd5445ece2d7f0b88674b0a7f6c1e2904c53ee3d8ec80dc4ff32c", "class_name": "RelatedNodeInfo"}}, "text": "Reasoning and Response Generation \n\n#  1\\. Video Downloading\n\nTo begin, we need to locally download multimodal content from a publicly\navailable source, I used pytube to download a YouTube video by 3Blue1Brown on\nthe Gaussian function.\n\n    \n    \n    # SET CONFIG\n    video_url = \"https://www.youtube.com/watch?v=d_qvLDhkg00\"\n    output_video_path = \"./video_data/\"\n    output_folder = \"./mixed_data/\"\n    output_audio_path = \"./mixed_data/output_audio.wav\"\n    \n    filepath = output_video_path + \"input_vid.mp4\"\n    Path(output_folder).mkdir(parents=True, exist_ok=True)\n    \n    \n    def download_video(url, output_path):\n        \"\"\"\n        Download a video from a given url and save it to the output path.\n    \n        Parameters:\n        url (str): The url of the video to download.\n        output_path (str): The path to save the video to.\n    \n        Returns:\n        dict: A dictionary containing the metadata of the video.\n        \"\"\"\n      from pytube import YouTube\n    \n        yt = YouTube(url)\n        metadata = {\"Author\": yt.author, \"Title\": yt.title, \"Views\": yt.views}\n        yt.streams.get_highest_resolution().download(\n            output_path=output_path, filename=\"input_vid.mp4\"\n        )\n        return metadata\n    \n\nRun ` **metadata_vid = download_video(video_url, output_video_path)** ` to\ninvoke the function and store the video locally.\n\n#  _2\\. ", "mimetype": "text/plain", "start_char_idx": 2699, "end_char_idx": 4077, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8ff0b29-eb62-41c6-b659-37665ce9ee1b": {"__data__": {"id_": "f8ff0b29-eb62-41c6-b659-37665ce9ee1b", "embedding": null, "metadata": {"filename": "multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.md", "extension": ".md", "title": "MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB", "date": "Feb 17, 2024", "url": "https://www.llamaindex.ai/blog/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8cd3127-a3c7-459c-a783-369cf6a33b74", "node_type": "4", "metadata": {"filename": "multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.md", "extension": ".md", "title": "MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB", "date": "Feb 17, 2024", "url": "https://www.llamaindex.ai/blog/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e"}, "hash": "1d0cb5e3c9e00cae96958510d2fb59f2efee2da8e89212d08150050c675aea97", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a71302d-5033-48a7-8790-4617a480a82d", "node_type": "1", "metadata": {"filename": "multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.md", "extension": ".md", "title": "MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB", "date": "Feb 17, 2024", "url": "https://www.llamaindex.ai/blog/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e"}, "hash": "7f7e8bf86356e18bcbd054a232ed71cacaac3bb26696753a55f71ddd3c8bf387", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7d4ed71-33d9-4a5d-b36b-6d02aef48179", "node_type": "1", "metadata": {}, "hash": "d9a0413e7d17bc18c6a0e132c40d5734ea24bc96a97eee26a5afa8ea42baeca8", "class_name": "RelatedNodeInfo"}}, "text": "Video Processing_\n\nWe need to now extract multimodal content \u2014 Images, Text(via Audio). I\nextracted 1 frame every 5 seconds of the video (~160 frames) using ` moviepy `\n.\n\n    \n    \n    def video_to_images(video_path, output_folder):\n        \"\"\"\n        Convert a video to a sequence of images and save them to the output folder.\n    \n        Parameters:\n        video_path (str): The path to the video file.\n        output_folder (str): The path to the folder to save the images to.\n    \n        \"\"\"\n        clip = VideoFileClip(video_path)\n        clip.write_images_sequence(\n            os.path.join(output_folder, \"frame%04d.png\"), fps=0.2 #configure this for controlling frame rate.\n        )\n\nFollowing this, we extract the audio component:\n\n    \n    \n    def video_to_audio(video_path, output_audio_path):\n        \"\"\"\n        Convert a video to audio and save it to the output path.\n    \n        Parameters:\n        video_path (str): The path to the video file.\n        output_audio_path (str): The path to save the audio to.\n    \n        \"\"\"\n        clip = VideoFileClip(video_path)\n        audio = clip.audio\n        audio.write_audiofile(output_audio_path)\n\nNext, let\u2019s extract text from the audio using the SpeechRecognition library:\n\n    \n    \n    def audio_to_text(audio_path):\n        \"\"\"\n        Convert an audio file to text.\n    \n        Parameters:\n        audio_path (str): The path to the audio file.\n    \n        Returns:\n        test (str): The text recognized from the audio.\n    \n        \"\"\"\n        recognizer = sr.Recognizer()\n        audio = sr.AudioFile(audio_path)\n    \n        with audio as source:\n            # Record the audio data\n            audio_data = recognizer.record(source)\n    \n            try:\n                # Recognize the speech\n                text = recognizer.recognize_whisper(audio_data)\n            except sr.UnknownValueError:\n                print(\"Speech recognition could not understand the audio.\")\n            except sr.RequestError as e:\n                print(f\"Could not request results from service; {e}\")\n    \n        return text\n\nRun the below chunk to complete the extraction and storage process:\n\n    \n    \n    video_to_images(filepath, output_folder)\n    video_to_audio(filepath, output_audio_path)\n    text_data = audio_to_text(output_audio_path)\n    \n    with open(output_folder + \"output_text.txt\", \"w\") as file:\n        file.write(text_data)\n    print(\"Text data saved to file\")\n    file.close()\n    os.remove(output_audio_path)\n    print(\"Audio file removed\")\n\n#  3\\. Building the Multi-Modal Index and Vector Store\n\nAfter processing the video, we proceed to construct a multi-modal index and\nvector store. This entails generating embeddings for both textual and visual\ndata using OpenAI\u2019s CLIP model, subsequently storing and managing these\nembeddings in LanceDB VectorStore via the ` **LanceDBVectorStore** ` class.\n\n    \n    \n    from llama_index.indices.multi_modal.base import MultiModalVectorStoreIndex\n    from llama_index import SimpleDirectoryReader, StorageContext\n    \n    from llama_index import SimpleDirectoryReader, StorageContext\n    from llama_index.vector_stores import LanceDBVectorStore\n    \n    \n    from llama_index import (\n        SimpleDirectoryReader,\n    )\n    \n    text_store = LanceDBVectorStore(uri=\"lancedb\", table_name=\"text_collection\")\n    image_store = LanceDBVectorStore(uri=\"lancedb\", table_name=\"image_collection\")\n    storage_context = StorageContext.from_defaults(\n        vector_store=text_store, image_store=image_store\n    )\n    \n    # Create the MultiModal index\n    documents = SimpleDirectoryReader(output_folder).load_data()\n    \n    index = MultiModalVectorStoreIndex.from_documents(\n        documents,\n        storage_context=storage_context,\n    )\n\n#  4\\. Retrieving Relevant Images and Context\n\nWith the index in place, the system can then retrieve pertinent images and\ncontextual information based on input queries. This enhances the prompt with\nprecise and relevant multimodal data, anchoring the analysis in the video\u2019s\ncontent.\n\nLets set up the engine for retrieving, I am fetching top 5 most relevant `\nNodes ` from the vectordb based on the similarity score:\n\n    \n    \n    retriever_engine = index.as_retriever(\n        similarity_top_k=5, image_similarity_top_k=5\n    )\n\n> _A_ ` _Node_ ` _object is a \u201cchunk\u201d of any source Document, whether it\u2019s\n> text, an image, or other. It contains embeddings as well as meta information\n> of the chunk of data._\n>\n> By default, LanceDB uses ` l2 ` as metric type for evaluating similarity.\n> You can specify the metric type as ` cosine ` or ` dot ` if required.\n\nNext, we create a helper function for executing the retrieval logic:\n\n    \n    \n    from llama_index.response.notebook_utils import display_source_node\n    from llama_index.schema import ImageNode\n    \n    \n    def retrieve(retriever_engine, query_str):\n        retrieval_results = retriever_engine.retrieve(query_str)\n    \n        retrieved_image = []\n        retrieved_text = []\n        for res_node in retrieval_results:\n            if isinstance(res_node.node, ImageNode):\n                retrieved_image.append(res_node.node.metadata[\"file_path\"])\n            else:\n                display_source_node(res_node, source_length=200)\n                retrieved_text.append(res_node.text)\n    \n        return retrieved_image, retrieved_textdef retrieve(retriever_engine, query_str):\n        retrieval_results = retriever_engine.retrieve(query_str)\n\nLets input the query now and then move on to complete the process by\nretrieving and visualizing the data :\n\n    \n    \n    query_str = \"\"\"\n    Using examples from the video, explain all things covered regarding\n    the Gaussian function\n    \"\"\"\n    \n    \n    img, txt = retrieve(retriever_engine=retriever_engine, query_str=query_str)\n    image_documents = SimpleDirectoryReader(\n        input_dir=output_folder, input_files=img\n    ).load_data()\n    context_str = \"\".join(txt)\n    plot_images(img)\n\nYou should see something similar to the example below (note that the output\nwill vary depending on your query):\n\nDisplaying the similar Text objects (nodes)  Retrieved Images\n\nObserve that the ` node ` object displayed shows the ` Id ` of the data chunk\n, its similarity score and the source text of the chunk that was matched (for\nimages we get the filepath instead of text).\n\n#  5\\. Reasoning and Response Generation\n\nThe final step leverages GPT4V to reason about the correlations between the\ninput query and the augmented data. Below is the prompt template :\n\n    \n    \n    qa_tmpl_str = (\n        \"\"\"\n     Given the provided information, including relevant images and retrieved context from the video, \\\n     accurately and precisely answer the query without any additional prior knowledge.\\n\"\n        \"Please ensure honesty and responsibility, refraining from any racist or sexist remarks.\\n\"\n        \"---------------------\\n\"\n        \"Context: {context_str}\\n\"\n        \"Metadata for video: {metadata_str} \\n\"\n        \"---------------------\\n\"\n        \"Query: {query_str}\\n\"\n        \"Answer: \"\n    \"\"\"\n    )\n\nThe ` OpenAIMultiModal ` class from LlamaIndex enables us to incorporate image\ndata directly into our prompt object. Thus, in the final step, we enhance the\nquery and contextual elements within the template to produce the response as\nfollows:\n\n    \n    \n    from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n    \n    openai_mm_llm = OpenAIMultiModal(\n        model=\"gpt-4-vision-preview\", api_key=OPENAI_API_TOKEN, max_new_tokens=1500\n    )\n    \n    \n    response_1 = openai_mm_llm.complete(\n        prompt=qa_tmpl_str.format(\n            context_str=context_str, query_str=query_str, metadata_str=metadata_str\n        ),\n        image_documents=image_documents,\n    )\n    \n    pprint(response_1.text)\n\n> _The generated response captures the context pretty well and structures the\n> answer correctly :_\n>\n> The video \u201cA pretty reason why Gaussian + Gaussian = Gaussian\u201d by\n> 3Blue1Brown delves into the Gaussian function or normal distribution,\n> highlighting several critical aspects:\n>\n> **Central Limit Theorem:** It starts with the central limit theorem,\n> illustrating how the sum of multiple random variable copies tends toward a\n> normal distribution, improving with more variables.\n>\n> **Convolution of Random Variables:** Explains the addition of two random\n> variables as their distributions\u2019 convolution, focusing on visualizing this\n> through diagonal slices.\n", "mimetype": "text/plain", "start_char_idx": 4077, "end_char_idx": 12560, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7d4ed71-33d9-4a5d-b36b-6d02aef48179": {"__data__": {"id_": "c7d4ed71-33d9-4a5d-b36b-6d02aef48179", "embedding": null, "metadata": {"filename": "multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.md", "extension": ".md", "title": "MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB", "date": "Feb 17, 2024", "url": "https://www.llamaindex.ai/blog/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8cd3127-a3c7-459c-a783-369cf6a33b74", "node_type": "4", "metadata": {"filename": "multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.md", "extension": ".md", "title": "MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB", "date": "Feb 17, 2024", "url": "https://www.llamaindex.ai/blog/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e"}, "hash": "1d0cb5e3c9e00cae96958510d2fb59f2efee2da8e89212d08150050c675aea97", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8ff0b29-eb62-41c6-b659-37665ce9ee1b", "node_type": "1", "metadata": {"filename": "multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.md", "extension": ".md", "title": "MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB", "date": "Feb 17, 2024", "url": "https://www.llamaindex.ai/blog/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e"}, "hash": "612bb9c9de94d24dd43b0e46d0e69c147366dc1d973f24c8c776b638ecc482d6", "class_name": "RelatedNodeInfo"}}, "text": ">\n> **Gaussian Function:** Details the Gaussian function, emphasizing the\n> normalization factor for a valid probability distribution, and describes the\n> distribution\u2019s spread and center with standard deviation (\u03c3) and mean (\u03bc).\n>\n> **Convolution of Two Gaussians:** Discusses adding two normally distributed\n> variables, equivalent to convolving two Gaussian functions, and visualizes\n> this using the graph\u2019s rotational symmetry.\n>\n> **Rotational Symmetry and Slices:** Shows the rotational symmetry of e^(-x\u00b2)\n> * e^(-y\u00b2) around the origin, a unique Gaussian function property. It\n> explains computing the area under diagonal slices, equating to the\n> functions\u2019 convolution.\n>\n> **Resulting Distribution:** Demonstrates the convolution of two Gaussian\n> functions yielding another Gaussian, a notable exception in convolutions\n> usually resulting in a different function type.\n>\n> **Standard Deviation of the Result:** Concludes that convolving two normal\n> distributions with mean 0 and standard deviation (\u03c3) produces a normal\n> distribution with a standard deviation of sqrt(2) * \u03c3.\n>\n> **Implications for the Central Limit Theorem:** Highlights the convolution\n> of two Gaussians\u2019 role in the central limit theorem, positioning the\n> Gaussian distribution as a distribution space fixed point.\n>\n> The author uses visual examples and explanations throughout to clarify the\n> mathematical concepts related to the Gaussian function and its significance\n> in probability and statistics.\n\n#  Conclusion\n\nThe Multimodal RAG architecture offers a powerful and efficient solution for\nprocessing and analyzing video content. By leveraging the capabilities of\nOpenAI\u2019s GPT4V and LanceDB, this approach not only simplifies the video\nanalysis process but also enhances its accuracy and relevance. Whether for\ncontent creation, security surveillance, or educational purposes, the\npotential applications of this technology are vast and varied. As we continue\nto explore and refine these tools, the future of video analysis looks\npromising, with AI-driven solutions leading the way towards more insightful\nand actionable interpretations of video data.\n\nStay tuned for upcoming projects !\n\n", "mimetype": "text/plain", "start_char_idx": 12560, "end_char_idx": 14743, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67bd89c7-52bc-4c77-a198-80080f13d8c6": {"__data__": {"id_": "67bd89c7-52bc-4c77-a198-80080f13d8c6", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-02-13-26fa79601ba5.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201302\u201313", "date": "Feb 13, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-02-13-26fa79601ba5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "15ff97d4-bffb-4581-9df0-92c7b7f7734d", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-02-13-26fa79601ba5.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201302\u201313", "date": "Feb 13, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-02-13-26fa79601ba5"}, "hash": "365284f8b4ff38d601579c76e4abb8a310333957bac7b78a6f68230921fab646", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "251060f2-d29c-4482-9377-29d742582bc7", "node_type": "1", "metadata": {}, "hash": "4a46c5b292f832ec240654464cc280bb0a33b097f0385a4c5f1a6fcb71e3e644", "class_name": "RelatedNodeInfo"}}, "text": "Greetings, LlamaIndex Adventurers ,\n\nWelcome to an exhilarating week of discoveries at LlamaIndex! Our community\u2019s\nlively input and the abundance of learning tools await to supercharge your\njourney through LlamaIndex.\n\nBefore we dive into the updates, we have two major announcements:\n\n  * **LlamaIndex v0.10** : Our latest open-source release marks a monumental step towards production readiness. With a new core package and hundreds of integrations and LlamaPacks now available as separate PyPi packages, we\u2019ve massively improved organization and version tracking. Major updates include the refactoring of LlamaHub into a central hub for all integrations and the deprecation of ServiceContext for an enhanced development experience. [ Blog ](/llamaindex-v0-10-838e735948f8) , [ Migration Guide ](https://www.notion.so/6ede431dcb8841b09ea171e7f133bd77?pvs=21) . \n  * Introducing Short Courses on Advanced RAG Development: Master complex RAG systems with our series, covering everything from unstructured data to agent integration. Learn through LlamaIndex Query Pipelines, from basic text-to-SQL to advanced query techniques, and build scalable RAG applications with hands-on guidance. [ Video1 ](https://www.youtube.com/watch?v=CeDS1yvw9E4) , [ Video2 ](https://www.youtube.com/watch?v=L1o1VPVfbb0) . \n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1305, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "251060f2-d29c-4482-9377-29d742582bc7": {"__data__": {"id_": "251060f2-d29c-4482-9377-29d742582bc7", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-02-13-26fa79601ba5.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201302\u201313", "date": "Feb 13, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-02-13-26fa79601ba5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "15ff97d4-bffb-4581-9df0-92c7b7f7734d", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-02-13-26fa79601ba5.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201302\u201313", "date": "Feb 13, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-02-13-26fa79601ba5"}, "hash": "365284f8b4ff38d601579c76e4abb8a310333957bac7b78a6f68230921fab646", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67bd89c7-52bc-4c77-a198-80080f13d8c6", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-02-13-26fa79601ba5.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201302\u201313", "date": "Feb 13, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-02-13-26fa79601ba5"}, "hash": "bc54d0f60b5890c2730375ad7da95d3da8b23d4dee4490e014b912d8e5572a34", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6838fbe2-4f7b-46b0-9185-9c3c83708142", "node_type": "1", "metadata": {}, "hash": "79b9b46830dd38bd1077c2104e37dfc4746bf48d5d74bc02b16fc713fc6ba73a", "class_name": "RelatedNodeInfo"}}, "text": "Your creativity fuels our inspiration! We\u2019re excited to see any projects,\narticles, or videos you\u2019re passionate about. Share your incredible creations\nwith us at [ news@llamaindex.ai ](mailto:news@llamaindex.ai) . Haven\u2019t joined\nour newsletter yet? Make sure to subscribe on our [ website\n](https://www.llamaindex.ai/) for the latest LlamaIndex updates delivered\ndirectly to your inbox.\n\n**The highlights:**\n\n  1. **Self-RAG** : Introducing Self-RAG, now part of LlamaIndex as a LlamaPack. Boosts LLM training and RAG workflows with dynamic capabilities. [ Notebook ](https://github.com/run-llama/llama-hub/blob/main/llama_hub/llama_packs/self_rag/self_rag.ipynb) , [ Tweet ](https://x.com/llama_index/status/1754909796594221187?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 1305, "end_char_idx": 2045, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6838fbe2-4f7b-46b0-9185-9c3c83708142": {"__data__": {"id_": "6838fbe2-4f7b-46b0-9185-9c3c83708142", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-02-13-26fa79601ba5.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201302\u201313", "date": "Feb 13, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-02-13-26fa79601ba5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "15ff97d4-bffb-4581-9df0-92c7b7f7734d", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-02-13-26fa79601ba5.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201302\u201313", "date": "Feb 13, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-02-13-26fa79601ba5"}, "hash": "365284f8b4ff38d601579c76e4abb8a310333957bac7b78a6f68230921fab646", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "251060f2-d29c-4482-9377-29d742582bc7", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-02-13-26fa79601ba5.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201302\u201313", "date": "Feb 13, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-02-13-26fa79601ba5"}, "hash": "410d584e24fa4b68d133966d63ccbcee12451aeb03bfe288078600c92fde048e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7bd5afe-8e98-4156-a2e9-6b323ba618c1", "node_type": "1", "metadata": {}, "hash": "dd7134309aa3741ee672ae0c66591c80b1f46bfe480c6bd21b82ef4b7d1dde76", "class_name": "RelatedNodeInfo"}}, "text": "2. **LlamaIndex + FlowiseAI Integration** : Seamlessly merge LlamaIndex with FlowiseAI for effortless, no-code RAG app development. [ Docs ](https://docs.flowiseai.com/integrations/llamaindex) , [ Tweet ](https://x.com/llama_index/status/1755641567174684953?s=20) . \n  3. **RAG Guide with MistralAI** : MistralAI\u2019s new doc includes a RAG guide with LlamaIndex. Utilize Mistral-medium for enhanced RAG functions. [ Docs ](https://docs.mistral.ai/guides/basic-RAG/#rag-with-llamaindex) . \n\n**Feature Releases and Enhancements:**\n\n  * We have introduced a seamless integration between LlamaIndex and FlowiseAI, enabling easy, no-code development of advanced RAG applications with a drag-and-drop interface for quick chatbot or agent integration. [ Docs ](https://docs.flowiseai.com/integrations/llamaindex) , [ Tweet ](https://x.com/llama_index/status/1755641567174684953?s=20) . \n  * We have introduced Self-RAG, a dynamic retrieval tool by [ Akari Asai ](https://twitter.com/AkariAsai) \u2019s team, now available as a LlamaPack for easy integration, enhancing LLM training and RAG workflows with dynamic, iterative capabilities. [ Notebook ](https://github.com/run-llama/llama-hub/blob/main/llama_hub/llama_packs/self_rag/self_rag.ipynb) , [ Tweet ](https://x.com/llama_index/status/1754909796594221187?s=20) . \n  * We have introduced the RAG CLI tool that allows you to search any file on your filesystem using on-device language model embeddings, featuring the power of Mistral-7B and bge-m3 for an advanced, customizable experience. [ Docs ](https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html) , [ Tweet ](https://x.com/llama_index/status/1754678983881621595?s=20) . \n  * We have launched full-stack agent servers with a single CLI command using ` **create-llama** ` from LlamaIndex, offering instant access to 50+ tools for any agent project. [ Tweet ](https://x.com/jerryjliu0/status/1755289964517167184?s=20) . \n  * We have introduced agents in LlamaIndex.TS, enabling advanced AI software development in TypeScript with features like function calling and multi-document handling. [ Blog ](/how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa) , [ Docs ](https://ts.llamaindex.ai/modules/agent/) , [ Tweet ](https://x.com/llama_index/status/1755688725106114818?s=20) . \n  * DeepEval is integrated with LlamaIndex, significantly enhancing RAG evaluation capabilities and introducing unit testing for LlamaIndex apps in CI/CD environments. [ Docs ](https://docs.confident-ai.com/docs/integrations-llamaindex) . \n\n**Guides:**\n\n  * [ Guide ](https://docs.mistral.ai/guides/basic-RAG/#rag-with-llamaindex) to RAG with LlamaIndex in MistralAI\u2019s new documentation with Mistral-medium and Mistral embedding models. \n  * [ Guide ](https://docs.llamaindex.ai/en/stable/examples/agent/custom_agent.html#step-wise-queries) to Building Agentic RAG to incorporate user feedback in real-time enhancing complex searches with a human-in-the-loop approach. \n  * [ Guide ](https://huggingface.co/blog/tgi-messages-api) to Integrating Huggingface\u2019s New Messages API with OpenAI compatibility, simplifying the integration process for Inference Endpoints and Text Generation Inference. \n\n**Tutorials:**\n\n  * [ Plaban Nayak ](https://nayakpplaban.medium.com/) [ tutorial ](https://www.notion.so/LlamaIndex-Newsletter-2024-02-06-86c1d1db060249f2ab8032357f3df323?pvs=21) on Setting up Query Pipeline For Advanced RAG Workflow using LlamaIndex. \n  * [ Krish Naik ](https://www.youtube.com/@krishnaik06) [ tutorial ](https://www.youtube.com/watch?v=f-AXdiCyiT8) on Step-by-Step Guide to Building a RAG LLM App with Llama2 and LlamaIndex. \n  * HelixML [ tutorial ](https://helixml.substack.com/p/how-we-got-fine-tuning-mistral-7b) to Knowledge Memorization by fine-tuning Mistral-7B for enhanced knowledge memorization, offering a new way to reason across contexts without RAG\u2019s limitations. \n  * [ Wenqi Glantz ](https://medium.com/@wenqiglantz) [ tutorial ](https://towardsdatascience.com/nemo-guardrails-the-ultimate-open-source-llm-security-toolkit-0a34648713ef) on NeMo Guardrails, the Ultimate Open-Source LLM Security Toolkit. \n\n**Webinar:**\n\n  * [ Webinar ](https://www.youtube.com/watch?v=96mRmQD4RnE) of Laurie with Ankit Khare(Rockset) delves into the essentials of RAG \u2014 its purpose, methodology, how LlamaIndex facilitates it, and exciting developments for 2024. \n  * [ Webinar ](https://www.youtube.com/watch?v=Ya1DhVW9gTo) with Zilong Wang, and Tianyang Liu on Advanced Tabular Data Understanding with LLMs. \n\n**Calling all enterprises:**\n\nAre you building with LlamaIndex? We are working hard to make LlamaIndex, even\nmore, Enterprise-ready and have sneak peeks at our upcoming products available\nfor partners. Interested? ", "mimetype": "text/plain", "start_char_idx": 2045, "end_char_idx": 6792, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7bd5afe-8e98-4156-a2e9-6b323ba618c1": {"__data__": {"id_": "d7bd5afe-8e98-4156-a2e9-6b323ba618c1", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-02-13-26fa79601ba5.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201302\u201313", "date": "Feb 13, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-02-13-26fa79601ba5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "15ff97d4-bffb-4581-9df0-92c7b7f7734d", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-02-13-26fa79601ba5.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201302\u201313", "date": "Feb 13, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-02-13-26fa79601ba5"}, "hash": "365284f8b4ff38d601579c76e4abb8a310333957bac7b78a6f68230921fab646", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6838fbe2-4f7b-46b0-9185-9c3c83708142", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-02-13-26fa79601ba5.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201302\u201313", "date": "Feb 13, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-02-13-26fa79601ba5"}, "hash": "2bb5b71a46c76f4c868b58e2ec971571f363498a17c1bb83b57fa713d38fb227", "class_name": "RelatedNodeInfo"}}, "text": "[ Get in touch.\n](https://docs.google.com/forms/d/e/1FAIpQLScBNdM2a_fn8UZOKmFQt6lBsrd1o6FflvsdPH-\nPn3JkdlN_Rg/viewform)\n\n", "mimetype": "text/plain", "start_char_idx": 6792, "end_char_idx": 6913, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f74226d-914a-41c0-bbf4-f839ea8a49d4": {"__data__": {"id_": "3f74226d-914a-41c0-bbf4-f839ea8a49d4", "embedding": null, "metadata": {"filename": "pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.md", "extension": ".md", "title": "Pioneering the Future of Housing: Introducing GenAI-Driven ADU Planning", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7711566b-e14a-4779-ab42-616803ecce59", "node_type": "4", "metadata": {"filename": "pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.md", "extension": ".md", "title": "Pioneering the Future of Housing: Introducing GenAI-Driven ADU Planning", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f"}, "hash": "9c481a3fe372e40c3979baac652975aae040403537836cc567c51e00eae4a85f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "619eb7c5-19b5-4c78-81cf-93def5e91d85", "node_type": "1", "metadata": {}, "hash": "fc58eddb6e4c7e38b405ba97be4f6d51c67f8f6fc32d0a62c7cba3911882cbf1", "class_name": "RelatedNodeInfo"}}, "text": "ADU Planner in action\n\nIn the midst of a pressing housing shortage, the American dream of\nhomeownership is being reimagined through the innovative concept of Accessory\nDwelling Units (ADUs). These compact, efficient homes, nestled in the\nbackyards of existing properties, are more than just a trend \u2014 they\u2019re a\nrevolution in modular and prefabricated living solutions. In 2022, the ADU\nmarket, combined with modular and prefabricated houses, soared to an\nimpressive $150 billion globally, with projections indicating a leap to $300\nbillion by 2032 ( [ link ](https://www.factmr.com/report/4227/prefabricated-\nhomes-market) ).\n\nYet, the journey to erecting an ADU has been far from simple. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 689, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "619eb7c5-19b5-4c78-81cf-93def5e91d85": {"__data__": {"id_": "619eb7c5-19b5-4c78-81cf-93def5e91d85", "embedding": null, "metadata": {"filename": "pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.md", "extension": ".md", "title": "Pioneering the Future of Housing: Introducing GenAI-Driven ADU Planning", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7711566b-e14a-4779-ab42-616803ecce59", "node_type": "4", "metadata": {"filename": "pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.md", "extension": ".md", "title": "Pioneering the Future of Housing: Introducing GenAI-Driven ADU Planning", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f"}, "hash": "9c481a3fe372e40c3979baac652975aae040403537836cc567c51e00eae4a85f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f74226d-914a-41c0-bbf4-f839ea8a49d4", "node_type": "1", "metadata": {"filename": "pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.md", "extension": ".md", "title": "Pioneering the Future of Housing: Introducing GenAI-Driven ADU Planning", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f"}, "hash": "23b466a4fb4a946d5956c749f204ca262596b0e946e9f3cda61b39a977062082", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f66a14f9-fe30-48b0-ba2d-605149fcc8d9", "node_type": "1", "metadata": {}, "hash": "d668a96fb8873205f26538c894e2743719485e18aa45acebe947efda7c076c8c", "class_name": "RelatedNodeInfo"}}, "text": "Traditionally,\nit involves extensive land surveys and numerous consultations with vendors to\nsift through an array of floor plans. Recognizing the need for a more\nefficient pathway, our team has crafted an innovative solution: a GenAI-\npowered ADU planning system. This cutting-edge tool deftly navigates through\nthe maze of city building codes, effortlessly connecting users with local\nvendors and presenting viable floor plan options \u2014 all within our user-\nfriendly app.\n\nOur mission is to streamline your ADU creation process, making it less\ndaunting and more accessible. Imagine the potential when the complexities of\nplanning are reduced to a few clicks \u2014 this is the power of GenAI at your\nfingertips.\n\n#  **Award-Winning Innovation: ADU Planner Takes the Lead**\n\nWe\u2019re thrilled to announce a milestone achievement: Our \u201cADU Planner\u201d project\nclinched the **top honor** at the [ **LlamaIndex RAG Hackathon**\n](https://rag-a-thon.devpost.com/) , held in the first weekend of February\n2024. This victory underscores our commitment to transforming the ADU space\nwith our pioneering technology.\n\nDiscover the features that set our project apart and explore the full scope of\nour innovative planner through the following resources:\n\n\u00b7 **Dive into the Details** : Visit our [ Devpost project page\n](https://devpost.com/software/adu-planner?ref_content=my-projects-\ntab&ref_feature=my_projects) for an in-depth look.\n\n\u00b7 **Explore Our Code** : For the tech enthusiasts, our codebase is available\non [ GitHub ](https://github.com/RJ-Heisenberg/ADU-Planner_rag-a-thon2024) .\n\nWe\u2019re just getting started, and this recognition only fuels our drive to\ninnovate and deliver solutions that matter.\n\n#  Watch Our Demo\n\n#  **Technologies**\n\nAt the heart of our \u201cADU Planner\u201d lies a synergy of cutting-edge technology\nand user-friendly design. The seamless interface is crafted with React,\nproviding an intuitive frontend experience, while our robust Flask backend is\nthe powerhouse of functionality, energized by the latest AI from GPT-3.5/4V\nand augmented by the precision of the LlamaIndex PDF parser and the\nversatility of Google Maps.\n\nHere\u2019s how our workflow unfolds:\n\n1\\. ", "mimetype": "text/plain", "start_char_idx": 689, "end_char_idx": 2855, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f66a14f9-fe30-48b0-ba2d-605149fcc8d9": {"__data__": {"id_": "f66a14f9-fe30-48b0-ba2d-605149fcc8d9", "embedding": null, "metadata": {"filename": "pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.md", "extension": ".md", "title": "Pioneering the Future of Housing: Introducing GenAI-Driven ADU Planning", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7711566b-e14a-4779-ab42-616803ecce59", "node_type": "4", "metadata": {"filename": "pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.md", "extension": ".md", "title": "Pioneering the Future of Housing: Introducing GenAI-Driven ADU Planning", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f"}, "hash": "9c481a3fe372e40c3979baac652975aae040403537836cc567c51e00eae4a85f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "619eb7c5-19b5-4c78-81cf-93def5e91d85", "node_type": "1", "metadata": {"filename": "pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.md", "extension": ".md", "title": "Pioneering the Future of Housing: Introducing GenAI-Driven ADU Planning", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f"}, "hash": "5ce6544d1102f731cbe6db2e6032e8057b28d98162abe5ca523b923e6367af9c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca181d59-c6b9-4bfd-8f26-7fae74584e12", "node_type": "1", "metadata": {}, "hash": "8c336ed26fe4ae129bffceeacd7d29eb8e4abb8aa57f89197eacccac9eb06f34", "class_name": "RelatedNodeInfo"}}, "text": "**Start with Simplicity:** Users begin by entering their address into our\nfrontend. This action triggers our Google geocoding-powered backend to spring\ninto action, capturing a detailed satellite image of the property in question.\n\n2\\. ", "mimetype": "text/plain", "start_char_idx": 2855, "end_char_idx": 3091, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca181d59-c6b9-4bfd-8f26-7fae74584e12": {"__data__": {"id_": "ca181d59-c6b9-4bfd-8f26-7fae74584e12", "embedding": null, "metadata": {"filename": "pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.md", "extension": ".md", "title": "Pioneering the Future of Housing: Introducing GenAI-Driven ADU Planning", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7711566b-e14a-4779-ab42-616803ecce59", "node_type": "4", "metadata": {"filename": "pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.md", "extension": ".md", "title": "Pioneering the Future of Housing: Introducing GenAI-Driven ADU Planning", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f"}, "hash": "9c481a3fe372e40c3979baac652975aae040403537836cc567c51e00eae4a85f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f66a14f9-fe30-48b0-ba2d-605149fcc8d9", "node_type": "1", "metadata": {"filename": "pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.md", "extension": ".md", "title": "Pioneering the Future of Housing: Introducing GenAI-Driven ADU Planning", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f"}, "hash": "1223153dd5ca3f0b6d989fd040ddb76622fc4ec1ea15b530de75b6edf92f87a9", "class_name": "RelatedNodeInfo"}}, "text": "**Deciphering the Details:** Next, the journey bifurcates into a dual\nanalysis mode. One path involves parsing through local building codes, a task\nadeptly handled by GPT-3.5, to unearth specific ADU regulations such as\nminimum and maximum floor area constraints. Concurrently, GPT-4V meticulously\nscans the satellite imagery to pinpoint any potential obstructions and\ndelineate the areas ripe for ADU development.\n\n3\\. **Bringing Plans to Life:** With the viable regions marked, our system\nembarks on a virtual quest, scouring local builders\u2019 websites for ADU floor\nplans that not only fit the legal criteria but also the physical realities of\nyour property. These plans are then vividly rendered within the identified\nbuildable zones.\n\n4\\. **A Click to the Future:** Engaging with a chosen floor plan is as simple\nas a click, transporting users to the builder\u2019s domain where immersive 3D\nrenderings, pricing details, and more await. To cap it off, our tool\nthoughtfully prepares data exports to facilitate those all-important initial\ndiscussions with ADU builders.\n\nThis is more than a tool \u2014 it\u2019s a gateway to turning your backyard into a\nspace of possibility.\n\n#  **Flowchart**\n\nFlowchart\n\n#  **Code Samples**\n\n**Querying PDFs**\n\nIn the following example, we\u2019ll query local ADU building codes for Saratoga,\nCA. Once you\u2019ve setup your LlamaIndex and OpenAI API keys, you\u2019ll be able to\nparse the PDFs as follows:\n\n    \n    \n    from llama_parse import LlamaParse\n    parser = LlamaParse(result_type='markdown')\n    docs = sum([\n      parser.load_data(file) for file in [\n        'Documents/ADU_FAQ.pdf',\n        'Documents/ADU_Handbook.pdf',\n        'Documents/SCC-ADU-Guidebook-FINAL-9.8.23.pdf']], [])\n\nThen, we can index the documents into an ephemeral, or in-memory, [ Chroma DB\n](https://www.trychroma.com/) embeddings store.\n\n    \n    \n    chroma_client = chromadb.EphemeralClient()\n    chroma_collection = chroma_client.get_or_create_collection('embeddings')\n    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n    index = VectorStoreIndex.from_documents(docs, storage_context=storage_context)\n\nNext, we can integrate the documents into our agentic flow by creating a tool\nfor our agent to query them.\n\n    \n    \n    tools = [\n      QueryEngineTool(\n           query_engine=index.as_query_engine(),\n           metadata=ToolMetadata(\n               name='saratoga_adu_codes',\n               description=('Provides information about ADU building codes for the city of Saratoga, CA.'),\n           ),\n       )\n    ]\n\nAll together, our agent will be able to answer questions about the documents\nas follows:\n\n    \n    \n    from llama_index.agent import OpenAIAgent\n    \n    agent = OpenAIAgent.from_tools(tools, verbose=True)\n    agent.chat(\"\"\"\\\n    Answer the following questions regarding Accessory Dwelling Unit (ADU) construction planning in Saratoga, California (CA):\n    - What are the typical side and rear setbacks for a detached ADU?\n    \"\"\")\n    \n    \n    &gt;&gt;&gt; 'For detached ADUs in Saratoga, California, the typical side and rear setbacks are a minimum of four feet from the lot lines.'\n\n**Image analysis**\n\nFirstly, we\u2019ll prepare an image and plot it for visual recognization.\n\n    \n    \n    import os\n    from pathlib import Path\n    from PIL import Image\n    import matplotlib.pyplot as plt\n    \n    input_image_path = Path(\"input_images\")\n    if not input_image_path.exists():\n        Path.mkdir(input_image_path)\n    \n    image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp']\n    \n    # Filter out non-image files\n    image_paths = [str(input_image_path / img_path) for img_path in os.listdir(input_image_path) \n                   if any(img_path.lower().endswith(ext) for ext in image_extensions)]\n    \n    def plot_images(image_paths):\n        images_shown = 0\n        plt.figure(figsize=(16, 9))\n        for img_path in image_paths:\n            if os.path.isfile(img_path):\n                image = Image.open(img_path)\n    \n                plt.subplot(2, 3, images_shown + 1)\n                plt.imshow(image)\n                plt.xticks([])\n                plt.yticks([])\n    \n                images_shown += 1\n                if images_shown &gt;= 6:  # Adjusted to match the subplot dimensions (2,3)\n                    break\n    \n    plot_images(image_paths)\n\nSecondly, we can do a image analysis and recognize the property layout based\non GPT-4V, using the following prompt sample as an example.\n\n    \n    \n    instruction = '''\n    You are an intelligent Accessory Dwelling Units (ADU) architect designer and contractor.\n    Please analyze the image, describle the layout, and then describe the pool, property, and driveways in the format of:\n    1. property: xxx\n    2. pool: xxx\n    ...\n    '''\n    \n    \n    from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n    from llama_index import SimpleDirectoryReader\n    \n    # put your local directore here\n    image_documents = SimpleDirectoryReader(\"./input_images\").load_data()\n    \n    openai_mm_llm = OpenAIMultiModal(\n        model=\"gpt-4-vision-preview\", api_key=OPENAI_API_TOKEN, max_new_tokens=1500, temperature = 0.0\n    )\n    \n    response = openai_mm_llm.complete(\n        prompt = instruction,\n        image_documents=image_documents,\n    )\n    \n    print(response)\n    \n    \n    Based on the aerial image provided, here is an analysis of the layout:\n    \n    1. Property: The property appears to be a residential lot with a single-family home. The house has a hipped roof with multiple sections, indicating a complex floor plan with possibly several rooms or wings. There is a landscaped area surrounding the house with various trees and shrubs, and the terrain seems to be sloped, as indicated by the terracing on the land.\n    \n    2. Pool: There is a kidney-shaped pool located to the northwest of the main house. It is surrounded by a paved area, likely for lounging and poolside activities, and is accessible via a curved pathway that leads from the house to the pool area.\n    \n    3. Driveways: It is not entirely clear from the image where the driveway is located, as the specific access point to the property is not visible. However, there seems to be a paved path leading from the bottom right of the image towards the house, which could be the driveway or a walkway. If it is the driveway, it would be located on the southeast side of the property, leading up to the house.\n    \n    Please note that without a broader view or additional context, some details about the property, such as the exact location of the driveway or additional structures, may not be accurately determined.\n\n#  **Conclusions**\n\nOur application markedly simplifies the ADU construction process for\nhomeowners. It adeptly navigates complex tasks such as land surveying and\nidentifying viable floor plans. While ADU vendors need to validate the chosen\nfloor plan and cities must grant construction approval, our tool propels this\nprogression by facilitating comprehensive at-home analysis. Looking ahead, we\nare poised to officially launch this application and forge partnerships with\nreputable ADU vendors, enhancing oversight in modular and prefabricated home\nconstruction. We are committed to personalizing our service further by\nadapting our recommendations to align with individual budget constraints and\nspecific layout preferences, ensuring that our users\u2019 visions for their homes\nare realized with precision and care.\n\n", "mimetype": "text/plain", "start_char_idx": 3091, "end_char_idx": 10617, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4863a3df-c1d9-463e-b44b-5d867c6d456b": {"__data__": {"id_": "4863a3df-c1d9-463e-b44b-5d867c6d456b", "embedding": null, "metadata": {"filename": "llamaindex-v0-10-838e735948f8.md", "extension": ".md", "title": "LlamaIndex v0.10", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-v0-10-838e735948f8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4081ae4b-e041-4da7-b180-b7dd2925f42f", "node_type": "4", "metadata": {"filename": "llamaindex-v0-10-838e735948f8.md", "extension": ".md", "title": "LlamaIndex v0.10", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-v0-10-838e735948f8"}, "hash": "489581cb05238524b666845547d45888c170af4cbbcaf9ee9e6693ca7c35cf96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9c25d5b-9356-401b-ad9e-45cad86c4c8a", "node_type": "1", "metadata": {}, "hash": "12210779d3ad8cbfd99dc752c06807d75ac9edb7d05321269d06f605746c993e", "class_name": "RelatedNodeInfo"}}, "text": "Today we\u2019re excited to launch LlamaIndex v0.10.0. It is by far the biggest\nupdate to our Python package to date ( [ see this gargantuan PR\n](https://github.com/run-llama/llama_index/pull/10537) ), and it takes a\nmassive step towards making LlamaIndex a next-generation, **production-ready\ndata framework** for your LLM applications.\n\nLlamaIndex v0.10 contains some major updates:\n\n  1. **We have created a** ` **llama-index-core** ` **package, and split all integrations and templates into separate packages:** Hundreds of integrations (LLMs, embeddings, vector stores, data loaders, callbacks, agent tools, and more) are now versioned and packaged as a separate PyPI packages, while preserving namespace imports: for example, you can still use ` from llama_index.llms.openai import OpenAI ` for a LLM. \n  2. [ **LlamaHub** ](https://llamahub.ai/) **will be the central hub for all integrations:** the former [ llama-hub ](https://github.com/run-llama/llama-hub) repo itself is consolidated into the main [ llama_index ](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations) repo. Instead of integrations being split between the core library and LlamaHub, every integration will be listed on LlamaHub. We are actively working on updating the site, stay tuned! \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f9c25d5b-9356-401b-ad9e-45cad86c4c8a": {"__data__": {"id_": "f9c25d5b-9356-401b-ad9e-45cad86c4c8a", "embedding": null, "metadata": {"filename": "llamaindex-v0-10-838e735948f8.md", "extension": ".md", "title": "LlamaIndex v0.10", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-v0-10-838e735948f8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4081ae4b-e041-4da7-b180-b7dd2925f42f", "node_type": "4", "metadata": {"filename": "llamaindex-v0-10-838e735948f8.md", "extension": ".md", "title": "LlamaIndex v0.10", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-v0-10-838e735948f8"}, "hash": "489581cb05238524b666845547d45888c170af4cbbcaf9ee9e6693ca7c35cf96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4863a3df-c1d9-463e-b44b-5d867c6d456b", "node_type": "1", "metadata": {"filename": "llamaindex-v0-10-838e735948f8.md", "extension": ".md", "title": "LlamaIndex v0.10", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-v0-10-838e735948f8"}, "hash": "e9680a79b500cc4e47ea4e40e6b27ac280cec6ee50e665a61cfab1d0b398177f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a335b17-4458-4884-ae09-14196a762a0a", "node_type": "1", "metadata": {}, "hash": "2b85fb9479fa71bc0b8dca7a5f7553ccfabb9b7ed40891ddc5755e5fd1539479", "class_name": "RelatedNodeInfo"}}, "text": "3. **ServiceContext is deprecated:** Every LlamaIndex user is familiar with ServiceContext, which over time has become a clunky, unneeded abstraction for managing LLMs, embeddings, chunk sizes, callbacks, and more. As a result we are completely deprecating it; you can now either directly specify arguments or set a default. \n\nUpgrading your codebase to LlamaIndex v0.10 may lead to some breakages,\nprimarily around our integrations/packaging changes, but fortunately we\u2019ve\nincluded some scripts to make it as easy as possible to migrate your codebase\nto use LlamaIndex v0.10.\n\nCheck out the below sections for more details, and go to the very last section\nfor resource links to everything.\n\n#  Splitting into `llama-index-core` and integration packages\n\nThe first and biggest change we\u2019ve made is a massive packaging refactor.\n\nLlamaIndex has evolved into a broad toolkit containing hundreds of\nintegrations:\n\n  * 150+ data loaders \n  * 35+ agent tools \n  * 50+ LlamaPack templates \n  * 50+ LLMs \n  * 25+ embeddings \n  * 40+ vector stores \n\nand more across the ` llama_index ` and ` llama-hub ` repos. The rapid growth\nof our ecosystem has been awesome to see, but it\u2019s also come with growing\npains:\n\n  * Many of the integrations lack proper tests \n  * Users are responsible for figuring out dependencies \n  * If an integration updates, users will have to update their entire ` llama-index ` Python package. \n\nIn response to this, we\u2019ve done the following.\n\n  * **Created** ` **llama-index-core** ` **:** This is a slimmed-down package that contains the core LlamaIndex abstractions and components, without any integrations. \n  * **Created separate packages for all integrations/templates:** Every integration is now available as a separate package. This includes _all_ integrations, including those on LlamaHub! See [ our Notion registry ](https://pretty-sodium-5e0.notion.site/ce81b247649a44e4b6b35dfb24af28a6?v=53b3c2ced7bb4c9996b81b83c9f01139) page for a full list of all packages. \n\nThe ` llama-index ` package still exists, and it imports ` llama-index-core `\nand a minimal set of integrations. Since we use OpenAI by default, this\nincludes OpenAI packages ( ` llama-index-llms-openai ` , ` llama-index-\nembeddings-openai ` , and OpenAI programs/question generation/multimodal), as\nwell as our beloved SimpleDirectoryReader (which is in ` llama-index-readers-\nfile ` ).\n\n**NOTE:** if you don\u2019t want to migrate to v0.10 yet and want to continue using\nthe current LlamaIndex abstractions, we are maintaining ` llama-index-legacy `\n(pinned to the latest release 0.9.48) for the foreseeable future.\n\n##  Revamped Folder Structure\n\nWe\u2019ve completely revamped the folder structure in the ` llama_index ` repo.\nThe most important folders you should care about are:\n\n  * ` **llama-index-core** ` : This folder contains all core LlamaIndex abstractions. \n  * ` **llama-index-integrations** ` : This folder contains third-party integrations for 19 LlamaIndex abstractions. This includes data loaders, LLMs, embedding models, vector stores, and more. See below for more details. \n  * ` **llama-index-packs** ` : This folder contains our 50+ LlamaPacks, which are templates designed to kickstart a user\u2019s application. \n\nOther folders:\n\n  * ` llama-index-legacy ` : contains the legacy LlamaIndex code. \n  * ` llama-index-experimental ` : contains experimental features. Largely unused right now (outside parameter tuning). \n  * ` llama-index-finetuning ` : contains LlamaIndex fine-tuning abstractions. These are still relatively experimental. \n\n", "mimetype": "text/plain", "start_char_idx": 1286, "end_char_idx": 4826, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a335b17-4458-4884-ae09-14196a762a0a": {"__data__": {"id_": "8a335b17-4458-4884-ae09-14196a762a0a", "embedding": null, "metadata": {"filename": "llamaindex-v0-10-838e735948f8.md", "extension": ".md", "title": "LlamaIndex v0.10", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-v0-10-838e735948f8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4081ae4b-e041-4da7-b180-b7dd2925f42f", "node_type": "4", "metadata": {"filename": "llamaindex-v0-10-838e735948f8.md", "extension": ".md", "title": "LlamaIndex v0.10", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-v0-10-838e735948f8"}, "hash": "489581cb05238524b666845547d45888c170af4cbbcaf9ee9e6693ca7c35cf96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9c25d5b-9356-401b-ad9e-45cad86c4c8a", "node_type": "1", "metadata": {"filename": "llamaindex-v0-10-838e735948f8.md", "extension": ".md", "title": "LlamaIndex v0.10", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-v0-10-838e735948f8"}, "hash": "4e0a1b152a1d4f9af0214ccb7114df6feb0d4a2bc6feeb557cbaf7c80adc7118", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "016945a6-5455-46f4-929e-cdc5b4a7263c", "node_type": "1", "metadata": {}, "hash": "abd781bc9419bb524950b4bd82663dc8d022a065b6d2646c42c03aa8af05be67", "class_name": "RelatedNodeInfo"}}, "text": "The sub-directories in ` integrations ` and ` packs ` represent individual\npackages. The name of the folder corresponds to the package name. For\ninstance, ` llama-index-integrations/llms/llama-index-llms-gemini `\ncorresponds to the ` llama-index-llms-gemini ` PyPI package.\n\nWithin each package folder, the source files are arranged in the same paths\nthat you use to import them. For example, in the Gemini LLM package, you\u2019ll\nsee a folder called ` llama_index/llms/gemini ` containing the source files.\nThis folder structure is what allows you to **preserve the top-level** `\n**llama_index** ` **namespace during importing.** In the case of Gemini LLM,\nyou would pip install ` llama-index-llms-gemini ` and then import using ` from\nllama_index.llms.gemini import Gemini ` .\n\nEvery one of these subfolders also has the resources needed to packagify it: `\npyproject.toml ` , ` poetry.lock ` , and a ` Makefile ` , along with a script\nto automatically create a package.\n\nIf you\u2019re looking to contribute an integration or pack, don\u2019t worry! We have a\n[ full contributing guide\n](https://docs.llamaindex.ai/en/stable/contributing/contributing.html)\ndesigned to make this as seamless as possible, make sure to check it out.\n\n#  Integrations\n\n_All_ third-party integrations are now under ` llama-index-integrations ` .\nThere are 19 folders in here. The main integration categories are:\n\n  * ` llms `\n  * ` embeddings `\n  * ` multi_modal_llms `\n  * ` readers `\n  * ` tools `\n  * ` vector_stores `\n\nFor completeness here are all the other categories: ` agent ` , ` callbacks `\n, ` evaluation ` , ` extractors ` , ` graph_stores ` , ` indices ` , `\noutput_parsers ` , ` postprocessor ` , ` program ` , ` question_gen ` , `\nresponse_synthesizers ` , ` retrievers ` , ` storage ` , ` tools ` .\n\nThe integrations in the most common categories can be found in our temporary [\n**Notion package registry page** ](https://pretty-\nsodium-5e0.notion.site/ce81b247649a44e4b6b35dfb24af28a6?v=53b3c2ced7bb4c9996b81b83c9f01139)\n. All integrations can be found in [ our Github repo ](https://github.com/run-\nllama/llama_index/tree/main/llama-index-integrations) . The folder name of\neach integration package corresponds to the name of the package \u2014 so if you\nfind an integration you like, you now know how to pip install it!\n\nWe are actively working to make all integrations viewable on LlamaHub. Our\nvision for LlamaHub is to be _the_ hub for all third-party integrations.\n\nIf you\u2019re interested in contributing a package, see our ` contributing `\nsection below!\n\n#  Usage Example\n\nHere is a simple example of installing and using an Anthropic LLM.\n\n` pip install llama-index-llms-anthropic `\n\n    \n    \n    from llama_index.llms.anthropic import Anthropic\n    llm = Anthropic(api_key=\"&lt;api_key&gt;\")\n\nHere is an example of using a data loader.\n\n` pip install llama-index-readers-notion `\n\n    \n    \n    from llama_index.readers.notion import NotionPageReader\n    integration_token = os.getenv(\"NOTION_INTEGRATION_TOKEN\")\n    page_ids = [\"&lt;page_id&gt;\"]\n    reader = NotionPageReader(integration_token=integration_token)\n    documents = reader.load_data(page_ids=page_ids)\n\nHere is an example of using a LlamaPack:\n\n` pip install llama-index-packs-sentence-window-retriever `\n\n    \n    \n    from llama_index.packs.sentence_window_retriever import SentenceWindowRetrieverPack\n    # create the pack\n    # get documents from any data loader\n    sentence_window_retriever_pack = SentenceWindowRetrieverPack(\n      documents\n    )\n    response = sentence_window_retriever_pack.run(\"Tell me a bout a Music celebritiy.\")\n\n#  Dealing with Breaking Changes\n\nThis update comes with breaking changes, mostly around imports. For all\nintegrations, you can no longer do any of these:\n\n    \n    \n    # no more using `llama_index.llms` as a top-level package\n    from llama_index.llms import OpenAI\n    # no more using `llama_index.vector_stores` as a top-level package\n    from llama_index.vector_stores import PineconeVectorStore\n    # llama_hub imports are now no longer supported.\n    from llama_hub.slack.base import SlackReader\n\nInstead you can do these:\n\n    \n    \n    from llama_index.llms.openai import OpenAI\n    from llama_index.vector_stores.pinecone import PineconeVectorStore\n    # NOTE: no longer import a separate llama_hub package\n    from llama_index.readers.slack import SlackReader\n\nSee our [ migration guide ](https://pretty-\nsodium-5e0.notion.site/v0-10-0-Migration-\nGuide-6ede431dcb8841b09ea171e7f133bd77) (also described below) for more\ndetails.\n\n#  LlamaHub as a Central Hub for Integrations\n\nWith these packaging updates, we\u2019re expanding the concept of LlamaHub to\nbecome a central hub of _all_ LlamaIndex integrations to fulfill its vision of\nbecoming an integration site at the center of the LLM ecosystem. This expands\nbeyond its existing domain of loaders, tools, packs, and datasets, to include\nLLMs, embeddings, vector stores, callbacks, and more.\n\n**This effort is still a WIP.** If you go to [ llamahub.ai\n](http://llamahub.ai) today, you\u2019ll see that the site has not been updated\nyet, and it still contains the current set of integrations (data loaders,\ntools, LlamaPacks, datasets). Rest assured we\u2019ll be updating the site in a few\nweeks; in the meantime check out our [ Notion package registry\n](https://pretty-\nsodium-5e0.notion.site/ce81b247649a44e4b6b35dfb24af28a6?v=53b3c2ced7bb4c9996b81b83c9f01139)\n/ [ repo ](https://github.com/run-llama/llama_index/tree/main/llama-index-\nintegrations) for a list of all integrations/packages.\n\n##  Sunsetting llama-hub repo\n\nSince all integrations have been moved to the llama_index repo, we are\nsunsetting the llama-hub repo (but [ LlamaHub ](https://llamahub.ai/) itself\nlives on!). We did the painstaking work of migrating and packaging all\nexisting llama-hub integrations. For all future contributions please submit\ndirectly to the llama_index repo!\n\n##  `download` syntax\n\nA popular UX for fetching integrations through LlamaHub has been the `\ndownload ` syntax: ` download_loader ` , ` download_llama_pack ` , and more.\n\n**This will still work, but have different behavior. Check out the details\nbelow:**\n\n  * ` download_llama_pack ` : Will download a pack under ` llama-index-packs ` to a local file on your disk. This allows you to directly use and modify the source code from the template. \n  * Every other download function ` download_loader ` , ` download_tool ` : This will directly run pip install on the relevant integration package. \n\n#  Deprecating ServiceContext\n\nLast but not least, we are deprecating our ` ServiceContext ` construct and as\na result improving the developer experience of LlamaIndex.\n\nOur ` ServiceContext ` object existed as a general configuration container\ncontaining an LLM, embedding model, callback, and more; it was created before\nwe had proper LLM, embedding, prompt abstractions and was meant to be an\nintermediate user-facing layer to let users define these parameters.\n\nOver time however, this object became increasingly difficult to use. Passing\nin an entire ` service_context ` container to any module (index, retriever,\npost-processor, etc.) made it hard to reason about which component was\nactually getting used. Since all modules use OpenAI by default, users were\ngetting asked to unnecessarily specify their OpenAI key even in cases where\nthey\u2019d want to use a local model (because the embedding model default was\nstill OpenAI). It was also laborious to import and type out.\n\nAnother related pain point was that if you had a custom model or especially a\ncustom callback, you had to manually pass in the ` service_context ` to _all_\nmodules. This was laborious and it was easy for users to forget, resulting in\nmissed callbacks or inconsistent model usage.\n\nTherefore we\u2019ve made the following changes:\n\n  1. **ServiceContext is now deprecated:** You should now directly pass in relevant parameters to modules, such as the embedding model for indexing and the LLM for querying/response synthesis. \n  2. ", "mimetype": "text/plain", "start_char_idx": 4826, "end_char_idx": 12823, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "016945a6-5455-46f4-929e-cdc5b4a7263c": {"__data__": {"id_": "016945a6-5455-46f4-929e-cdc5b4a7263c", "embedding": null, "metadata": {"filename": "llamaindex-v0-10-838e735948f8.md", "extension": ".md", "title": "LlamaIndex v0.10", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-v0-10-838e735948f8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4081ae4b-e041-4da7-b180-b7dd2925f42f", "node_type": "4", "metadata": {"filename": "llamaindex-v0-10-838e735948f8.md", "extension": ".md", "title": "LlamaIndex v0.10", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-v0-10-838e735948f8"}, "hash": "489581cb05238524b666845547d45888c170af4cbbcaf9ee9e6693ca7c35cf96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a335b17-4458-4884-ae09-14196a762a0a", "node_type": "1", "metadata": {"filename": "llamaindex-v0-10-838e735948f8.md", "extension": ".md", "title": "LlamaIndex v0.10", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-v0-10-838e735948f8"}, "hash": "489211c3bda8ae011694b57f04da706cd485734eef6f7caa92ad61cbd6afcbb5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7324830-a2e6-4098-ad67-434c76fdfa7d", "node_type": "1", "metadata": {}, "hash": "af70e037390fb22be988553e7edb5c336f43624c736990e34aa19ab3df4a4fd5", "class_name": "RelatedNodeInfo"}}, "text": "**You can now define global settings:** Define this once, and don\u2019t worry about specifying any custom parameters at all in your downstream code. This is especially useful for callbacks. \n\nAll references to ServiceContext in our docs/notebooks have been removed and\nchanged to use either direct modules or the global settings object. See our\nusage example below as well.\n\n#  Usage Example\n\nTo build a ` VectorStoreIndex ` and then query it, you can now pass in the\nembedding model and LLM directly\n\n    \n    \n    from llama_index.embeddings.openai import OpenAIEmbedding\n    from llama_index.llms.openai import OpenAI\n    from llama_index.core.callbacks import CallbackManager\n    \n    embed_model = OpenAIEmbedding()\n    llm = OpenAI()\n    callback_manager = CallbackManager()\n    index = VectorStoreIndex.from_documents(\n     documents, embed_model=embed_model, callback_manager=callback_manager\n    )\n    query_engine = index.as_query_engine(llm=llm)\n\nOr you can define a global settings object\n\n    \n    \n    from llama_index.core.settings import Settings\n    Settings.llm = llm\n    Settings.embed_model = embed_model\n    Settings.callback_manager = callback_manager\n    index = VectorStoreIndex.from_documents(documents)\n    query_engine = index.as_query_engine()\n\n#  Contributing to LlamaIndex v0.10\n\nv0.10 makes the ` llama_index ` repo the central place for all community\ncontributions, whether you are interested in contributing core refactors, or\nintegrations/packs!\n\nIf you\u2019re contributing an integration/pack, v0.10 makes it way easier for you\nto contribute something that can be independently versioned, tested, and\npackaged.\n\n", "mimetype": "text/plain", "start_char_idx": 12823, "end_char_idx": 14462, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a7324830-a2e6-4098-ad67-434c76fdfa7d": {"__data__": {"id_": "a7324830-a2e6-4098-ad67-434c76fdfa7d", "embedding": null, "metadata": {"filename": "llamaindex-v0-10-838e735948f8.md", "extension": ".md", "title": "LlamaIndex v0.10", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-v0-10-838e735948f8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4081ae4b-e041-4da7-b180-b7dd2925f42f", "node_type": "4", "metadata": {"filename": "llamaindex-v0-10-838e735948f8.md", "extension": ".md", "title": "LlamaIndex v0.10", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-v0-10-838e735948f8"}, "hash": "489581cb05238524b666845547d45888c170af4cbbcaf9ee9e6693ca7c35cf96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "016945a6-5455-46f4-929e-cdc5b4a7263c", "node_type": "1", "metadata": {"filename": "llamaindex-v0-10-838e735948f8.md", "extension": ".md", "title": "LlamaIndex v0.10", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-v0-10-838e735948f8"}, "hash": "03057f5f5054a37e02d16b1a5b1081c89f5ff32b7e20bca081bec6c16de68808", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5963f23-cee8-44e4-9829-f0e3b21a2915", "node_type": "1", "metadata": {}, "hash": "285f65860d90ef0074a102177daa5b0be8255337287ad8854ec57814712c4f4e", "class_name": "RelatedNodeInfo"}}, "text": "We have utility scripts to make the package creation process for an\nintegration or pack effortless:\n\n    \n    \n    # create a new pack\n    cd ./llama-index-packs\n    llamaindex-cli new-package --kind \"packs\" --name \"my new pack\"\n    \n    # create a new integration\n    cd ./llama-index-integrations/readers\n    llamaindex-cli new-pacakge --kind \"readers\" --name \"new reader\"\n\nTake a look at our updated [ contributing guide here\n](https://docs.llamaindex.ai/en/latest/contributing/contributing.html#contribute-\na-pack-reader-tool-or-dataset-formerly-from-llama-hub) for more details.\n\n#  Migration to v0.10\n\nIf you want to use LlamaIndex v0.10, you will need to do two main things:\n\n  1. ", "mimetype": "text/plain", "start_char_idx": 14462, "end_char_idx": 15150, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5963f23-cee8-44e4-9829-f0e3b21a2915": {"__data__": {"id_": "d5963f23-cee8-44e4-9829-f0e3b21a2915", "embedding": null, "metadata": {"filename": "llamaindex-v0-10-838e735948f8.md", "extension": ".md", "title": "LlamaIndex v0.10", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-v0-10-838e735948f8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4081ae4b-e041-4da7-b180-b7dd2925f42f", "node_type": "4", "metadata": {"filename": "llamaindex-v0-10-838e735948f8.md", "extension": ".md", "title": "LlamaIndex v0.10", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-v0-10-838e735948f8"}, "hash": "489581cb05238524b666845547d45888c170af4cbbcaf9ee9e6693ca7c35cf96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a7324830-a2e6-4098-ad67-434c76fdfa7d", "node_type": "1", "metadata": {"filename": "llamaindex-v0-10-838e735948f8.md", "extension": ".md", "title": "LlamaIndex v0.10", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-v0-10-838e735948f8"}, "hash": "28068fa383ce3f054dff408e60cb1b0c8dfeb0ba8ae856801986ec136d8bf753", "class_name": "RelatedNodeInfo"}}, "text": "Adjust imports to fit the new package structure for core modules/integrations \n  2. Deprecate ServiceContext \n\nLuckily, we\u2019ve created a comprehensive migration guide that also contains a\nCLI tool to _automatically_ upgrade your existing code and notebooks to v0.10!\n\nJust do\n\n    \n    \n    llamaindex-cli upgrade <source-dir>\n\n[ Check out the full migration guide here. ](https://pretty-\nsodium-5e0.notion.site/v0-10-0-Migration-\nGuide-6ede431dcb8841b09ea171e7f133bd77)\n\n#  Next Steps\n\nWe\u2019ve painstakingly revamped all of our README, documentation and notebooks to\nreflect these v0.10 changes. Check out the below section for a compiled list\nof all resources.\n\n##  Documentation\n\n[ v0.10 Documentation ](https://docs.llamaindex.ai/en/stable/)\n\n[ v0.10 Installation Guide\n](https://docs.llamaindex.ai/en/stable/getting_started/installation.html)\n\n[ v0.10 Quickstart\n](https://docs.llamaindex.ai/en/stable/getting_started/starter_example.html)\n\n[ Updated Contribution Guide\n](https://docs.llamaindex.ai/en/stable/contributing/contributing.html)\n\nTemporary [ v0.10 Package Registry ](https://pretty-\nsodium-5e0.notion.site/ce81b247649a44e4b6b35dfb24af28a6?v=53b3c2ced7bb4c9996b81b83c9f01139)\n\n[ v0.10 Migration Guide ](https://pretty-\nsodium-5e0.notion.site/v0-10-0-Migration-\nGuide-6ede431dcb8841b09ea171e7f133bd77)\n\n##  Repo\n\n[ Repo README ](https://github.com/run-llama/llama_index/tree/main)\n\n[ llama-index-integrations ](https://github.com/run-\nllama/llama_index/tree/main/llama-index-integrations)\n\n[ llama-index-packs ](https://github.com/run-\nllama/llama_index/tree/main/llama-index-packs)\n\n##  Example Notebooks\n\nThese are mostly to show our updated import syntax.\n\n  * [ Sub-Question Query Engine ](https://docs.llamaindex.ai/en/stable/examples/query_engine/sub_question_query_engine.html) (primarily uses core) \n  * [ Weaviate Vector Store Demo ](https://docs.llamaindex.ai/en/stable/examples/vector_stores/WeaviateIndexDemo.html)\n  * [ OpenAI Agent over RAG Pipelines ](https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent.html)\n\n#  Bug reports\n\nWe\u2019ll be actively monitoring our [ Github Issues ](https://github.com/run-\nllama/llama_index/issues) and [ Discord ](https://discord.gg/dGcwcsnxhU) . If\nyou run into any issues don\u2019t hesitate to hop into either of these channels!\n\n", "mimetype": "text/plain", "start_char_idx": 15150, "end_char_idx": 17448, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "38fb43a3-a889-41cc-bd0c-6fb0e50ce780": {"__data__": {"id_": "38fb43a3-a889-41cc-bd0c-6fb0e50ce780", "embedding": null, "metadata": {"filename": "how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa.md", "extension": ".md", "title": "How to build LLM Agents in TypeScript with LlamaIndex.TS", "date": "Feb 8, 2024", "url": "https://www.llamaindex.ai/blog/how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4fe84a89-4090-49b9-8cab-400b4b1abe3b", "node_type": "4", "metadata": {"filename": "how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa.md", "extension": ".md", "title": "How to build LLM Agents in TypeScript with LlamaIndex.TS", "date": "Feb 8, 2024", "url": "https://www.llamaindex.ai/blog/how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa"}, "hash": "985bb8d525c97e2d57d035b3676b4cf77004bbe66e0609b66727570ed0ae68d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be2d7434-f8da-4e82-8653-b5044604e165", "node_type": "1", "metadata": {}, "hash": "b5f70c3515d8c41c3efe99bd312f56b6964f8f5be2a82db56e17bf7c47baf679", "class_name": "RelatedNodeInfo"}}, "text": "Agents are autonomous systems that can execute end-to-end tasks without much\nor fewer instructions. These agents are capable of solving tasks related to\nquestions and answering, using tools to achieve a desired behavior, or even\nplanning tasks.\n\nIn this article, we will explore a few capabilities of LlamaIndex.TS\u2019s built-\nin agents to achieve a set of goals. We have two parts:\n\n  1. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 386, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be2d7434-f8da-4e82-8653-b5044604e165": {"__data__": {"id_": "be2d7434-f8da-4e82-8653-b5044604e165", "embedding": null, "metadata": {"filename": "how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa.md", "extension": ".md", "title": "How to build LLM Agents in TypeScript with LlamaIndex.TS", "date": "Feb 8, 2024", "url": "https://www.llamaindex.ai/blog/how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4fe84a89-4090-49b9-8cab-400b4b1abe3b", "node_type": "4", "metadata": {"filename": "how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa.md", "extension": ".md", "title": "How to build LLM Agents in TypeScript with LlamaIndex.TS", "date": "Feb 8, 2024", "url": "https://www.llamaindex.ai/blog/how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa"}, "hash": "985bb8d525c97e2d57d035b3676b4cf77004bbe66e0609b66727570ed0ae68d0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38fb43a3-a889-41cc-bd0c-6fb0e50ce780", "node_type": "1", "metadata": {"filename": "how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa.md", "extension": ".md", "title": "How to build LLM Agents in TypeScript with LlamaIndex.TS", "date": "Feb 8, 2024", "url": "https://www.llamaindex.ai/blog/how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa"}, "hash": "6637887a84f29692ba6ca58b3cbf25240e12d3be0a04a6bdeaa77be8c10c7c45", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd0fc390-b5c8-43c5-89a7-2a3ddbb1ef78", "node_type": "1", "metadata": {}, "hash": "73addf9c81c6d73bd24509c85a092595fe9de4e8c10a1f13b198400882386b4a", "class_name": "RelatedNodeInfo"}}, "text": "Building a simple agent for calculation. \n  2. Using agents with your personal/private data to answer questions. \n\nAll the full code examples will be at the end of the post.\n\n#  Setup\n\nTo start you need to have ` llamaindex ` package installed on your node\nenvironment setup and an OpenAI key.\n\nTo install the package:\n\n    \n    \n    npm install llamaindex \n\nTo set up the OpenAI key you can set your environment variable:\n\n    \n    \n    export OPENAI_API_KEY=sk-***************\n\n#  Agent for calculations\n\nThe first agent will be responsible for getting a user input of numbers and\nbased on this selecting the right tools to achieve the task\n\n##  Import the classes:\n\n    \n    \n    import { FunctionTool, OpenAIAgent } from \"llamaindex\";\n\n##  Function Tool\n\nThe first step will be creating tools that the agent will have access to, we\nwill be creating an ` add ` function and a ` multiply ` function.\n\nOpenAI provides us a function calling API which we can send our function\narguments and get back a response\n\nWe start by creating two functions:\n\n    \n    \n    // Define a function to sum two numbers\n    function sum({ a, b }: { a: number; b: number }): number {\n      return a + b;\n    }\n    \n    // Define a function to multiply two numbers\n    function multiply({ a, b }: { a: number; b: number }): number {\n      return a * b;\n    }\n\nNow we can set up the ` FunctionTool ` class which will be given to the agent,\nthis class requires the properties of the function and metadata for the tool,\nhelping the Large Language Models (LLMs) to identify which tool the LLM should\nuse and the parameters.\n\n    \n    \n    // Sum properties to give to the LLM\n    const sumJSON = {\n      type: \"object\",\n      properties: {\n        a: {\n          type: \"number\",\n          description: \"The first number\",\n        },\n        b: {\n          type: \"number\",\n          description: \"The second number\",\n        },\n      },\n      required: [\"a\", \"b\"],\n    };\n    \n    // Multiply properties to give to the LLM\n    const multiplyJSON = {\n      type: \"object\",\n      properties: {\n        a: {\n          type: \"number\",\n          description: \"The number to multiply\",\n        },\n        b: {\n          type: \"number\",\n          description: \"The multiplier\",\n        },\n      },\n      required: [\"a\", \"b\"],\n    };\n    \n    // Create sum function tool\n    const sumFunctionTool = new FunctionTool(sum, {\n      name: \"sum\",\n      description: \"Use this function to sum two numbers\",\n      parameters: sumJSON,\n    });\n    \n    // Creat multiply function tool\n    const multiplyFunctionTool = new FunctionTool(multiply, {\n      name: \"multiply\",\n      description: \"Use this function to multiply two numbers\",\n      parameters: multiplyJSON,\n    });\n\n##  Chat with Agent\n\nNow we have the tools to give to the agent, we can set up the agent:\n\n    \n    \n    // Setup the agent with the respective tools\n    const agent = new OpenAIAgent({\n      tools: [sumFunctionTool, multiplyFunctionTool],\n      verbose: true,\n    });\n\nAnd then ask a question:\n\n    \n    \n    // Chat with LLM\n    const response = await agent.chat({\n      message: \"How much is 5 + 5? then multiply by 2\",\n    });\n    \n    // Agent output\n    console.log(String(response));\n\nNow the agent will choose the right tools to achieve the desired task using\nthe functions provided by you. Then you should see an output as:\n\n    \n    \n    === Calling Function ===\n    Calling function: sum with args: {\n      \"a\": 5,\n      \"b\": 5\n    }\n    Got output 10\n    ==========================\n    === Calling Function ===\n    Calling function: multiply with args: {\n      \"a\": 10,\n      \"b\": 2\n    }\n    Got output 20\n    ==========================\n    The result of adding 5 and 5 is 10. When you multiply 10 by 2, the result is 20.\n\n#  Using Agents with your documents\n\nThe second agent will be responsible for going through a set of Dan Abramov\nessays and answering questions based on the available data.\n\nFirstly, we will import the necessary classes and functions\n\n##  Import the classes and functions\n\n    \n    \n    import {\n      OpenAIAgent,\n      SimpleDirectoryReader,\n      VectorStoreIndex,\n      SummaryIndex,\n      QueryEngineTool,\n    } from \"llamaindex\";\n\n##  Loading Documents\n\nNow we will load the documents and insert them into a local vector store index\nwhich will be responsible for storing the documents and allowing the agent to\nquery the most relevant data for the task and a summarize vector index which\ncan better help on tasks summary related.\n\n    \n    \n    // Load the documents\n    const documents = await new SimpleDirectoryReader().loadData({\n      directoryPath: \"node_modules/llamaindex/examples\",\n    });\n    \n    // Create a vector index from the documents\n    const vectorIndex = await VectorStoreIndex.fromDocuments(documents);\n    const summaryIndex = await SummaryIndex.fromDocuments(documents)\n\n##  Creating the Query Engine Tool\n\nNow we will create the tooling that allows the Agents to access the Vector\nIndex:\n\n    \n    \n    // Create a query engine from the vector index\n    const abramovQueryEngine = vectorIndex.asQueryEngine();\n    const abramovSummaryEngine = summaryIndex.asQueryEngine();\n    \n    // Create a QueryEngineTool with the vector engine\n    const vectorEngineTool = new QueryEngineTool({\n      queryEngine: abramovQueryEngine,\n      metadata: {\n        name: \"abramov_query_engine\",\n        description: \"Use this engine to answer specific questions about Abramov\",\n      },\n    });\n    \n    // Create a QueryEngineTool with the summary engine\n    const summaryEngineTool = new QueryEngineTool({\n      queryEngine: abramovSummaryEngine,\n      metadata: {\n        name: \"abramov_summary_engine\",\n        description: \"Use this engine to generate summaries about Abramov\",\n      },\n    });\n\n##  Creating the OpenAI Agent\n\nNow we can create and provide the necessary tools for the agent\n\n    \n    \n    // Setup the agent \n    const agent = new OpenAIAgent({\n      tools: [vectorEngineTool, summaryEngineTool],\n      verbose: true,\n    });\n\n##  Chat with Agent\n\nNow you can chat with the agent about Dan Abramov and it will select the right\ntools to achieve the goal.\n\n    \n    \n    // Chat with Agent\n    const response = await agent.chat({\n      message: \"Where he worked in his 20s?", "mimetype": "text/plain", "start_char_idx": 386, "end_char_idx": 6665, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd0fc390-b5c8-43c5-89a7-2a3ddbb1ef78": {"__data__": {"id_": "dd0fc390-b5c8-43c5-89a7-2a3ddbb1ef78", "embedding": null, "metadata": {"filename": "how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa.md", "extension": ".md", "title": "How to build LLM Agents in TypeScript with LlamaIndex.TS", "date": "Feb 8, 2024", "url": "https://www.llamaindex.ai/blog/how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4fe84a89-4090-49b9-8cab-400b4b1abe3b", "node_type": "4", "metadata": {"filename": "how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa.md", "extension": ".md", "title": "How to build LLM Agents in TypeScript with LlamaIndex.TS", "date": "Feb 8, 2024", "url": "https://www.llamaindex.ai/blog/how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa"}, "hash": "985bb8d525c97e2d57d035b3676b4cf77004bbe66e0609b66727570ed0ae68d0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be2d7434-f8da-4e82-8653-b5044604e165", "node_type": "1", "metadata": {"filename": "how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa.md", "extension": ".md", "title": "How to build LLM Agents in TypeScript with LlamaIndex.TS", "date": "Feb 8, 2024", "url": "https://www.llamaindex.ai/blog/how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa"}, "hash": "2931f34e65f1b2f16375c336f66b0ac19d896f393e744ef811ff230a1d61c3cd", "class_name": "RelatedNodeInfo"}}, "text": "\",\n    });\n    \n    // Log output\n    console.log(String(response));\n\nThe tool output\n\n    \n    \n    === Calling Function ===\n    Calling function: abramov_query_engine with args: {\n      \"query\": \"work experience in 20s\"\n    }\n    Got output The individual had their first job as a software developer in their 20s. They worked for a Russian-American outsourcing company and their salary was $18k/year.\n    ==========================\n    In his 20s, Abramov worked as a software developer for a Russian-American outsourcing company. His salary during that time was $18,000 per year.\n\n#  Conclusion\n\nAutonomous agents are powerful when talking about creating workflows and\nautomation that can take your business to the next level or make your life\neasier.\n\nThere is still a long way to go before they are fully autonomous but the\narrival of LLMs is allowing the first steps of these reasoning and decision-\nmaking engines\n\nDo you already use agents in your day-to-day? and wanna discuss agents or help\nwith agents? Reach me on [ Twitter ](https://twitter.com/manelferreira_)\n\n#  References\n\nCode example: [ https://github.com/EmanuelCampos/agents-typescript-example\n](https://github.com/EmanuelCampos/agents-typescript-example)\n\nLLamaIndexTS Documentation: [ https://ts.llamaindex.ai/modules/agent/openai\n](https://ts.llamaindex.ai/modules/agent/openai)\n\n[ https://www.llamaindex.ai/ ](https://www.llamaindex.ai/)\n\n", "mimetype": "text/plain", "start_char_idx": 6665, "end_char_idx": 8079, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8135804-ab63-4597-96a7-159057558f35": {"__data__": {"id_": "a8135804-ab63-4597-96a7-159057558f35", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-02-06-9a303130ad9f.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201306", "date": "Feb 6, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-06-9a303130ad9f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7d0ec380-908e-4cfe-a9cc-2294cd62ec76", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-02-06-9a303130ad9f.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201306", "date": "Feb 6, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-06-9a303130ad9f"}, "hash": "191713767c1cc860a72794f75141053b28fa56f48d09f5e0ff7a7baf3ec4cef9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb1954cb-075e-451a-bc4a-228bf923b8b0", "node_type": "1", "metadata": {}, "hash": "1c21eb45828ffe947e6cba4fa251ee273f9445cbcadb4353527a0560a0971174", "class_name": "RelatedNodeInfo"}}, "text": "Hello, LlamaIndex Explorers ,\n\nStep into a week full of exciting updates at LlamaIndex! Our community\u2019s\nvibrant contributions and extensive educational resources are here to amplify\nyour LlamaIndex exploration.\n\nBefore diving into the updates, we have an exciting announcement: We\u2019ve\nlaunched a [ $2,000 bounty program with Replit\n](https://replit.com/bounties?search=llamaindex) . This initiative invites\nopen-source contributors to create projects and templates focused on advanced\nRAG with LlamaIndex, from building RAG across thousands of documents to\nimplementing cutting-edge RAG research and crafting advanced templates.\n\nWe\u2019re inspired by your creativity! If you have a project, article, or video\nyou\u2019re excited about, we\u2019re eager to see it. Send your amazing work to [\nnews@llamaindex.ai ](https://www.notion.so/LlamaIndex-\nNewsletter-2024-02-06-86c1d1db060249f2ab8032357f3df323?pvs=21) . If you\nhaven\u2019t subscribed to our newsletter yet, don\u2019t miss out. Visit our [ website\n](https://www.llamaindex.ai/) and subscribe today to get all the newest\nupdates from LlamaIndex straight to your inbox.\n\n**The highlights:**\n\n  1. **Ollama Multimodal Integration Launch:** Introduced day-1 integration with Ollama Multi-Modal for developing local multimodal applications, including image extraction, multimodal RAG, and captioning. [ Notebook, ](https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/ollama_multi_modal.ipynb) [ Tweet ](https://x.com/llama_index/status/1753875735776018786?s=20)\n  2. ` create-llama ` **Enhanced RAG** : Updated create-llama for improved website content crawling and the creation of comprehensive RAG applications. [ Tweet ](https://x.com/MarcusSchiesser/status/1753373534708175263?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1747, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb1954cb-075e-451a-bc4a-228bf923b8b0": {"__data__": {"id_": "eb1954cb-075e-451a-bc4a-228bf923b8b0", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-02-06-9a303130ad9f.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201306", "date": "Feb 6, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-06-9a303130ad9f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7d0ec380-908e-4cfe-a9cc-2294cd62ec76", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-02-06-9a303130ad9f.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201306", "date": "Feb 6, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-06-9a303130ad9f"}, "hash": "191713767c1cc860a72794f75141053b28fa56f48d09f5e0ff7a7baf3ec4cef9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8135804-ab63-4597-96a7-159057558f35", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-02-06-9a303130ad9f.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201306", "date": "Feb 6, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-06-9a303130ad9f"}, "hash": "684269def8ed4257c00374bfb285ddb4bd9a9135c098f4c24a1dcd3daa1b157b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "340b6f5d-ab5c-490b-bb80-b8ce6ec92b33", "node_type": "1", "metadata": {}, "hash": "dd7134309aa3741ee672ae0c66591c80b1f46bfe480c6bd21b82ef4b7d1dde76", "class_name": "RelatedNodeInfo"}}, "text": "3. **Nomic Embedding** : [ Guide ](/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4) to Building a Fully Open Source Retriever with Nomic Embed and LlamaIndex. \n\n**Feature Releases and Enhancements:**\n\n  * We introduced day-1 integration with the Ollama Multi-Modal release enabling the creation of local multimodal applications on MacBook, including structured image extraction, multimodal RAG, and image captioning. [ Notebook, ](https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/ollama_multi_modal.ipynb) [ Tweet ](https://x.com/llama_index/status/1753875735776018786?s=20)\n  * We have updated create-llama on crawling a website\u2019s content, and create a full-stack RAG application based on the data. [ Tweet ](https://x.com/MarcusSchiesser/status/1753373534708175263?s=20) . \n\n**Guides:**\n\n  * [ Guide ](/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4) to Building a Fully Open Source Retriever with Nomic Embed and LlamaIndex. \n\nDemo:\n\n  * LlamaBot: [ Rohan ](https://twitter.com/clusteredbytes) developed an open-source Discord bot that listens to, remembers, and answers questions across servers, was inspired by a similar bot for Slack and developed using LlamaIndex, Gemini Pro, and Qdrant Engine. [ GitHub Repository ](https://github.com/rsrohan99/llamabot) , [ Tweet ](https://x.com/clusteredbytes/status/1754220009885163957?s=20) . \n\n**Tutorials:**\n\n  * [ Wenqi Glantz ](https://medium.com/@wenqiglantz) [ tutorial ](https://towardsdatascience.com/jump-start-your-rag-pipelines-with-advanced-retrieval-llamapacks-and-benchmark-with-lighthouz-ai-80a09b7c7d9d) on Jump-start Your RAG Pipelines with Advanced Retrieval LlamaPacks and Benchmark with Lighthouz AI. \n  * [ Ravi Theja ](https://www.linkedin.com/in/ravidesetty/) [ tutorial ](/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00) on Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG. \n  * [ Wenqi Glantz ](https://medium.com/@wenqiglantz) [ tutorial ](https://towardsdatascience.com/12-rag-pain-points-and-proposed-solutions-43709939a28c) on 12 RAG Pain Points and Proposed Solutions. \n  * [ Cobus Reyling ](https://twitter.com/CobusGreylingZA) [ tutorial ](/agentic-rag-with-llamaindex-2721b8a49ff6) on Agentic RAG With LlamaIndex. \n  * [ ChristopherGS ](https://twitter.com/ChrisSamiullah) [ tutorial ](https://christophergs.com/blog/ai-engineering-retrieval-augmented-generation-rag-llama-index) on Retrieval Augmented Generation (RAG) with Llama Index and Open-Source Models. \n  * [ Andrei ](https://twitter.com/_nerdai_) workshop tutorial on Evaluation of Multimodal RAG Systems using the LlamaIndex. \n  * [ Tutorial ](https://docs.pinecone.io/docs/llamaindex) on Building RAG application with Pinecone and LlamaIndex. \n  * [ Sudalai Rajkumar ](https://www.linkedin.com/in/sudalairajkumar/) [ tutorial ](https://srk.ai/blog/004-ai-llm-retrieval-eval-llamaindex) on RAG \u2014 Encoder and Reranker evaluation. \n  * [ Harshad Suryawanshi ](https://twitter.com/HarshadSurya1c) [ tutorial ](/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089) on RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex. \n  * [ Otmane Boughaba\u2019s ](https://twitter.com/Otmane404) [ tutorial ](https://otmaneboughaba.com/posts/local-rag-api/) on Building a Local RAG API with LlamaIndex, Qdrant, Ollama, and FastAPI. \n  * [ Iulia Brezeanu ](https://medium.com/@brezeanu.iulia) [ tutorial ](https://towardsdatascience.com/how-to-find-the-best-multilingual-embedding-model-for-your-rag-40325c308ebb) on How to Find the Best Multilingual Embedding Model for Your RAG. \n\nEvents\n\n  * [ Jerry Liu ](https://twitter.com/jerryjliu0) [ keynote ](https://t.co/hrUMF8bq8Q) on Beyond Naive Rag: Adding Agentic Layers. \n\n**Calling all enterprises:**\n\nAre you building with LlamaIndex? We are working hard to make LlamaIndex, even\nmore, Enterprise-ready and have sneak peeks at our upcoming products available\nfor partners. Interested? ", "mimetype": "text/plain", "start_char_idx": 1747, "end_char_idx": 5902, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "340b6f5d-ab5c-490b-bb80-b8ce6ec92b33": {"__data__": {"id_": "340b6f5d-ab5c-490b-bb80-b8ce6ec92b33", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-02-06-9a303130ad9f.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201306", "date": "Feb 6, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-06-9a303130ad9f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7d0ec380-908e-4cfe-a9cc-2294cd62ec76", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-02-06-9a303130ad9f.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201306", "date": "Feb 6, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-06-9a303130ad9f"}, "hash": "191713767c1cc860a72794f75141053b28fa56f48d09f5e0ff7a7baf3ec4cef9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb1954cb-075e-451a-bc4a-228bf923b8b0", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-02-06-9a303130ad9f.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201306", "date": "Feb 6, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-06-9a303130ad9f"}, "hash": "e229efe6f4f317264ea69312d02ef94a169e55ff0e24df5a51d02d26ace6c4c5", "class_name": "RelatedNodeInfo"}}, "text": "[ Get in touch.\n](https://docs.google.com/forms/d/e/1FAIpQLScBNdM2a_fn8UZOKmFQt6lBsrd1o6FflvsdPH-\nPn3JkdlN_Rg/viewform)\n\n", "mimetype": "text/plain", "start_char_idx": 5902, "end_char_idx": 6023, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c3ba5ae9-9611-46eb-9843-3a8af6ad93fa": {"__data__": {"id_": "c3ba5ae9-9611-46eb-9843-3a8af6ad93fa", "embedding": null, "metadata": {"filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce6327bc-87c8-422f-9e8f-e30fa9222be5", "node_type": "4", "metadata": {"filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "0870c90d0dc438893105a56b3911fad5184dfe184befa6d5854a1b23c2fdc79f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f32db4e-c89d-4a96-9e35-77acabfc5ef1", "node_type": "1", "metadata": {}, "hash": "6441268051c1a5cc9c908d6c107eb02089471b5469f5649aa2dbd3843826ddc9", "class_name": "RelatedNodeInfo"}}, "text": "Unlocking the power of AI should be as intuitive as using your favorite apps.\nThat\u2019s the philosophy behind RAGArch, my latest creation designed to demystify\nand streamline the process of setting up Retrieval-Augmented Generation (RAG)\npipelines. This tool is born from a simple vision: to provide a\nstraightforward, no-code platform that empowers both seasoned developers and\ncurious explorers in the world of AI to craft, test, and implement RAG\npipelines with confidence and ease.\n\n#  Features\n\nRAGArch leverages LlamaIndex\u2019s powerful LLM orchestration capabilities, to\nprovide a seamless experience and granular control over your RAG pipeline.\n\n  * **Intuitive Interface:** RAGArch\u2019s user-friendly interface, built with Streamlit, allows you to test different RAG pipeline components interactively. \n  * **Custom Configuration** : The app provides a wide range of options to configure Language Models, Embedding Models, Node Parsers, Response Synthesis Methods, and Vector Stores to suit your project\u2019s needs. \n  * **Live Testing:** Instantly test your RAG pipeline with your own data and see how different configurations affect the outcome. \n  * **One-Click Code Generation** : Once you\u2019re satisfied with the configuration, the app can generate the Python code for your custom RAG pipeline, ready to be integrated into your application. \n\n#  Tools and Technologies\n\nThe creation of RAGArch was made possible by integrating a variety of powerful\ntools and technologies:\n\n  * **UI:** Streamlit \n  * **Hosting:** Hugging Face Spaces \n  * **LLMs:** OpenAI GPT 3.5 and 4, Cohere API, Gemini Pro \n  * **LLM Orchestration:** Llamaindex \n  * **Embedding Models:** \u201cBAAI/bge-small-en-v1.5\u201d, \u201cWhereIsAI/UAE-Large-V1\u201d, \u201cBAAI/bge-large-en-v1.5\u201d, \u201ckhoa-klaytn/bge-small-en-v1.5-angle\u201d, \u201cBAAI/bge-base-en-v1.5\u201d, \u201cllmrails/ember-v1\u201d, \u201cjamesgpt1/sf_model_e5\u201d, \u201cthenlper/gte-large\u201d, \u201cinfgrad/stella-base-en-v2\u201d and \u201cthenlper/gte-base\u201d \n  * **Vector Stores:** Simple (Llamaindex default), Pinecone and Qdrant \n\n#  Deep Dive into the Code\n\nThe ` app.py ` script is the backbone of RAGArch, integrating various\ncomponents to provide a cohesive experience. The following are the key\nfunctions of app.py\n\n##  ` **upload_file** `\n\nThis function manages file uploads and uses Llamaindex's `\nSimpleDirectoryReader ` to load documents into the system. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2330, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f32db4e-c89d-4a96-9e35-77acabfc5ef1": {"__data__": {"id_": "3f32db4e-c89d-4a96-9e35-77acabfc5ef1", "embedding": null, "metadata": {"filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce6327bc-87c8-422f-9e8f-e30fa9222be5", "node_type": "4", "metadata": {"filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "0870c90d0dc438893105a56b3911fad5184dfe184befa6d5854a1b23c2fdc79f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c3ba5ae9-9611-46eb-9843-3a8af6ad93fa", "node_type": "1", "metadata": {"filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "28f413b40224ce308ac7da0a6bab046906e02294b21b86a420807b1ee49d22d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "baab64ef-8c30-4272-a2db-4316cf1e4ca5", "node_type": "1", "metadata": {}, "hash": "323412b3400cbf5af6289c041e91e2b4af425264e22c4a6c28114db58da57524", "class_name": "RelatedNodeInfo"}}, "text": "It supports a wide\narray of document types, including PDFs, text files, HTML, JSON files, and\nmore, making it versatile for processing diverse data sources.\n\n    \n    \n    def upload_file():\n        file = st.file_uploader(\"Upload a file\", on_change=reset_pipeline_generated)\n        if file is not None:\n            file_path = save_uploaded_file(file)\n            \n            if file_path:\n                loaded_file = SimpleDirectoryReader(input_files=[file_path]).load_data()\n                print(f\"Total documents: {len(loaded_file)}\")\n    \n                st.success(f\"File uploaded successfully. Total documents loaded: {len(loaded_file)}\")\n            return loaded_file\n        return None\n\n##  save_uploaded_file\n\nThis utility function saves the uploaded file to a temporary location on the\nserver, making it accessible for further processing. It\u2019s a crucial part of\nthe file handling process, ensuring data integrity and availability.\n\n    \n    \n    def save_uploaded_file(uploaded_file):\n        try:\n            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp_file:\n                tmp_file.write(uploaded_file.getvalue())\n                return tmp_file.name\n        except Exception as e:\n            st.error(f\"Error saving file: {e}\")\n            return None\n\n##  ` **select_llm** `\n\nAllows users to select a Large Language Model and initializes it for use. You\ncan choose from Google\u2019s Gemini Pro, Cohere, OpenAI\u2019s GPT 3.5 and GPT 4.\n\n    \n    \n    def select_llm():\n        st.header(\"Choose LLM\")\n        llm_choice = st.selectbox(\"Select LLM\", [\"Gemini\", \"Cohere\", \"GPT-3.5\", \"GPT-4\"], on_change=reset_pipeline_generated)\n        \n        if llm_choice == \"GPT-3.5\":\n            llm = OpenAI(temperature=0.1, model=\"gpt-3.5-turbo-1106\")\n            st.write(f\"{llm_choice} selected\")\n        elif llm_choice == \"GPT-4\":\n            llm = OpenAI(temperature=0.1, model=\"gpt-4-1106-preview\")\n            st.write(f\"{llm_choice} selected\")\n        elif llm_choice == \"Gemini\":\n            llm = Gemini(model=\"models/gemini-pro\")\n            st.write(f\"{llm_choice} selected\")\n        elif llm_choice == \"Cohere\":\n            llm = Cohere(model=\"command\", api_key=os.environ['COHERE_API_TOKEN'])\n            st.write(f\"{llm_choice} selected\")\n        return llm, llm_choice\n\n##  ` select_embedding_model `\n\nOffers a dropdown for users to select the embedding model of their choice from\na predefined list. I have included some of the top embedding models from\nHugging Face\u2019s MTEB leaderboard. Near the dropdown I have also included a\nhandy link to the leaderboard where users can get more information about the\nembedding models.\n\n    \n    \n    def select_embedding_model():\n        st.header(\"Choose Embedding Model\")\n        col1, col2 = st.columns([2,1])\n        with col2:\n            st.markdown(\"\"\"\n                        [Embedding Models Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n                        \"\"\")\n        model_names = [\n            \"BAAI/bge-small-en-v1.5\",\n            \"WhereIsAI/UAE-Large-V1\",\n            \"BAAI/bge-large-en-v1.5\",\n            \"khoa-klaytn/bge-small-en-v1.5-angle\",\n            \"BAAI/bge-base-en-v1.5\",\n            \"llmrails/ember-v1\",\n            \"jamesgpt1/sf_model_e5\",\n            \"thenlper/gte-large\",\n            \"infgrad/stella-base-en-v2\",\n            \"thenlper/gte-base\"\n        ]\n        selected_model = st.selectbox(\"Select Embedding Model\", model_names,  on_change=reset_pipeline_generated)\n        with st.spinner(\"Please wait\") as status:\n            embed_model = HuggingFaceEmbedding(model_name=selected_model)\n            st.session_state['embed_model'] = embed_model\n            st.markdown(F\"Embedding Model: {embed_model.model_name}\")\n            st.markdown(F\"Embed Batch Size: {embed_model.embed_batch_size}\")\n            st.markdown(F\"Embed Batch Size: {embed_model.max_length}\")\n    \n    \n        return embed_model, selected_model\n\n##  select_node_parser Function\n\nThis function allows users to choose a node parser, which is instrumental in\nbreaking down documents into manageable chunks or nodes, facilitating better\nhandling and processing. I have included some of the most commonly used node\nparsers supported by Llamaindex, which include SentenceSplitter, CodeSplitter,\nSemanticSplitterNodeParser, TokenTextSplitter, HTMLNodeParser, JSONNodeParser\nand MarkdownNodeParser.\n\n    \n    \n    def select_node_parser():\n        st.header(\"Choose Node Parser\")\n        col1, col2 = st.columns([4,1])\n        with col2:\n            st.markdown(\"\"\"\n                        [More Information](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/root.html)\n                        \"\"\")\n        parser_types = [\"SentenceSplitter\", \"CodeSplitter\", \"SemanticSplitterNodeParser\",\n                        \"TokenTextSplitter\", \"HTMLNodeParser\", \"JSONNodeParser\", \"MarkdownNodeParser\"]\n        parser_type = st.selectbox(\"Select Node Parser\", parser_types, on_change=reset_pipeline_generated)\n        \n        parser_params = {}\n        if parser_type == \"HTMLNodeParser\":\n            tags = st.text_input(\"Enter tags separated by commas\", \"p, h1\")\n            tag_list = tags.split(',')\n            parser = HTMLNodeParser(tags=tag_list)\n            parser_params = {'tags': tag_list}\n            \n        elif parser_type == \"JSONNodeParser\":\n            parser = JSONNodeParser()\n            \n        elif parser_type == \"MarkdownNodeParser\":\n            parser = MarkdownNodeParser()\n            \n        elif parser_type == \"CodeSplitter\":\n            language = st.text_input(\"Language\", \"python\")\n            chunk_lines = st.number_input(\"Chunk Lines\", min_value=1, value=40)\n            chunk_lines_overlap = st.number_input(\"Chunk Lines Overlap\", min_value=0, value=15)\n            max_chars = st.number_input(\"Max Chars\", min_value=1, value=1500)\n            parser = CodeSplitter(language=language, chunk_lines=chunk_lines, chunk_lines_overlap=chunk_lines_overlap, max_chars=max_chars)\n            parser_params = {'language': language, 'chunk_lines': chunk_lines, 'chunk_lines_overlap': chunk_lines_overlap, 'max_chars': max_chars}\n            \n        elif parser_type == \"SentenceSplitter\":\n            chunk_size = st.number_input(\"Chunk Size\", min_value=1, value=1024)\n            chunk_overlap = st.number_input(\"Chunk Overlap\", min_value=0, value=20)\n            parser = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n            parser_params = {'chunk_size': chunk_size, 'chunk_overlap': chunk_overlap}\n            \n        elif parser_type == \"SemanticSplitterNodeParser\":\n            if 'embed_model' not in st.session_state:\n                st.warning(\"Please select an embedding model first.\")\n                return None, None\n            \n            embed_model = st.session_state['embed_model']\n            buffer_size = st.number_input(\"Buffer Size\", min_value=1, value=1)\n            breakpoint_percentile_threshold = st.number_input(\"Breakpoint Percentile Threshold\", min_value=0, max_value=100, value=95)\n            parser = SemanticSplitterNodeParser(buffer_size=buffer_size, breakpoint_percentile_threshold=breakpoint_percentile_threshold, embed_model=embed_model)\n            parser_params = {'buffer_size': buffer_size, 'breakpoint_percentile_threshold': breakpoint_percentile_threshold}\n            \n        elif parser_type == \"TokenTextSplitter\":\n            chunk_size = st.number_input(\"Chunk Size\", min_value=1, value=1024)\n            chunk_overlap = st.number_input(\"Chunk Overlap\", min_value=0, value=20)\n            parser = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n            parser_params = {'chunk_size': chunk_size, 'chunk_overlap': chunk_overlap}\n    \n        # Save the parser type and parameters to the session state\n        st.session_state['node_parser_type'] = parser_type\n        st.session_state['node_parser_params'] = parser_params\n        \n        return parser, parser_type\n\nBelow the node parser selection, I have also included a preview of the first\nnode of the text after splitting/parsing, just to give the users an idea of\nhow the chunking is actually happening based the selected node parser and the\nrelevant parameters.\n\n##  ` select_response_synthesis_method `\n\nThis function allows users to choose how the RAG pipeline synthesizes\nresponses. I have included varioud response synthesis methods supported by\nLlamaindex including _refine_ , _tree_summarize_ , _compact_ ,\n_simple_summarize_ , _accumulate_ and _compact_accumulate._\n\nUsers can click on the more information link to get more details about\nresponse synthesis and the different types.\n\n    \n    \n    def select_response_synthesis_method():\n        st.header(\"Choose Response Synthesis Method\")\n        col1, col2 = st.columns([4,1])\n        with col2:\n            st.markdown(\"\"\"\n                        [More Information](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/response_synthesizers.html)\n                        \"\"\")\n        response_modes = [\n            \"refine\",\n            \"tree_summarize\",  \n            \"compact\", \n            \"simple_summarize\", \n            \"accumulate\", \n            \"compact_accumulate\"\n        ]\n        selected_mode = st.selectbox(\"Select Response Mode\", response_modes, on_change=reset_pipeline_generated)\n        response_mode = selected_mode\n        return response_mode, selected_mode\n\n##  ` select_vector_store `\n\nEnables users to choose a vector store, which is a critical component for\nstoring and retrieving embeddings in the RAG pipeline. This function supports\nthe selection from multiple vector store options including Simple (Llamaindex\ndefault), Pinecone and Qdrant.\n\n    \n    \n    def select_vector_store():\n        st.header(\"Choose Vector Store\")\n        vector_stores = [\"Simple\", \"Pinecone\", \"Qdrant\"]\n        selected_store = st.selectbox(\"Select Vector Store\", vector_stores, on_change=reset_pipeline_generated)\n    \n        vector_store = None\n        if selected_store == \"Pinecone\":\n            pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\n            index = pc.Index(\"test\")\n            vector_store = PineconeVectorStore(pinecone_index=index)\n        elif selected_store == \"Qdrant\":\n            client = qdrant_client.QdrantClient(location=\":memory:\")\n            vector_store = QdrantVectorStore(client=client, collection_name=\"sampledata\")\n        st.write(selected_store)\n        return vector_store, selected_store\n\n##  generate_rag_pipeline Function\n\nThis core function ties together the selected components to generate a RAG\npipeline. It initializes the pipeline with the chosen LLM, embedding model,\nnode parser, response synthesis method, and vector store. It is triggered by\npressing the \u2018Generate RAG Pipeline\u2019 button.\n\n    \n    \n    def generate_rag_pipeline(file, llm, embed_model, node_parser, response_mode, vector_store):\n        if vector_store is not None:\n            # Set storage context if vector_store is not None\n            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n        else:\n            storage_context = None\n    \n        # Create the service context\n        service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, node_parser=node_parser)\n    \n        # Create the vector index\n        vector_index = VectorStoreIndex.from_documents(documents=file, storage_context=storage_context, service_context=service_context, show_progress=True)\n        if storage_context:\n            vector_index.storage_context.persist(persist_dir=\"persist_dir\")\n    \n        # Create the query engine\n        query_engine = vector_index.as_query_engine(\n            response_mode=response_mode,\n            verbose=True,\n        )\n    \n        return query_engine\n\n##  generate_code_snippet Function\n\nThis function is the culmination of the user\u2019s selections, generating the\nPython code necessary to implement the configured RAG pipeline. It dynamically\nconstructs the code snippet based on the chosen LLM, embedding model, node\nparser, response synthesis method, and vector store, including the parameters\nset for the node parser.\n\n    \n    \n    def generate_code_snippet(llm_choice, embed_model_choice, node_parser_choice, response_mode, vector_store_choice):\n        node_parser_params = st.session_state.get('node_parser_params', {})\n        print(node_parser_params)\n        code_snippet = \"from llama_index.llms import OpenAI, Gemini, Cohere\\n\"\n        code_snippet += \"from llama_index.embeddings import HuggingFaceEmbedding\\n\"\n        code_snippet += \"from llama_index import ServiceContext, VectorStoreIndex, StorageContext\\n\"\n        code_snippet += \"from llama_index.node_parser import SentenceSplitter, CodeSplitter, SemanticSplitterNodeParser, TokenTextSplitter\\n\"\n        code_snippet += \"from llama_index.node_parser.file import HTMLNodeParser, JSONNodeParser, MarkdownNodeParser\\n\"\n        code_snippet += \"from llama_index.vector_stores import MilvusVectorStore, QdrantVectorStore\\n\"\n        code_snippet += \"import qdrant_client\\n\\n\"\n    \n        # LLM initialization\n        if llm_choice == \"GPT-3.5\":\n            code_snippet += \"llm = OpenAI(temperature=0.1, model='gpt-3.5-turbo-1106')\\n\"\n        elif llm_choice == \"GPT-4\":\n            code_snippet += \"llm = OpenAI(temperature=0.1, model='gpt-4-1106-preview')\\n\"\n        elif llm_choice == \"Gemini\":\n            code_snippet += \"llm = Gemini(model='models/gemini-pro')\\n\"\n        elif llm_choice == \"Cohere\":\n            code_snippet += \"llm = Cohere(model='command', api_key='&lt;YOUR_API_KEY&gt;')  # Replace &lt;YOUR_API_KEY&gt; with your actual API key\\n\"\n    \n        # Embedding model initialization\n        code_snippet += f\"embed_model = HuggingFaceEmbedding(model_name='{embed_model_choice}')\\n\\n\"\n    \n        # Node parser initialization\n        node_parsers = {\n            \"SentenceSplitter\": f\"SentenceSplitter(chunk_size={node_parser_params.get('chunk_size', 1024)}, chunk_overlap={node_parser_params.get('chunk_overlap', 20)})\",\n            \"CodeSplitter\": f\"CodeSplitter(language={node_parser_params.get('language', 'python')}, chunk_lines={node_parser_params.get('chunk_lines', 40)}, chunk_lines_overlap={node_parser_params.get('chunk_lines_overlap', 15)}, max_chars={node_parser_params.get('max_chars', 1500)})\",\n            \"SemanticSplitterNodeParser\": f\"SemanticSplitterNodeParser(buffer_size={node_parser_params.get('buffer_size', 1)}, breakpoint_percentile_threshold={node_parser_params.get('breakpoint_percentile_threshold', 95)}, embed_model=embed_model)\",\n            \"TokenTextSplitter\": f\"TokenTextSplitter(chunk_size={node_parser_params.get('chunk_size', 1024)}, chunk_overlap={node_parser_params.get('chunk_overlap', 20)})\",\n            \"HTMLNodeParser\": f\"HTMLNodeParser(tags={node_parser_params.get('tags', ['p', 'h1'])})\",  \n            \"JSONNodeParser\": \"JSONNodeParser()\",\n            \"MarkdownNodeParser\": \"MarkdownNodeParser()\"\n        }\n        code_snippet += f\"node_parser = {node_parsers[node_parser_choice]}\\n\\n\"\n    \n        # Response mode\n        code_snippet += f\"response_mode = '{response_mode}'\\n\\n\"\n    \n        # Vector store initialization\n        if vector_store_choice == \"Pinecone\":\n            code_snippet += \"pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\\n\"\n            code_snippet += \"index = pc.Index('test')\\n\"\n            code_snippet += \"vector_store = PineconeVectorStore(pinecone_index=index)\\n\"\n        elif vector_store_choice == \"Qdrant\":\n            code_snippet += \"client = qdrant_client.QdrantClient(location=':memory:')\\n\"\n            code_snippet += \"vector_store = QdrantVectorStore(client=client, collection_name='sampledata')\\n\"\n        elif vector_store_choice == \"Simple\":\n            code_snippet += \"vector_store = None  # Simple in-memory vector store selected\\n\"\n    \n        code_snippet += \"\\n# Finalizing the RAG pipeline setup\\n\"\n        code_snippet += \"if vector_store is not None:\\n\"\n        code_snippet += \"    storage_context = StorageContext.from_defaults(vector_store=vector_store)\\n\"\n        code_snippet += \"else:\\n\"\n        code_snippet += \"    storage_context = None\\n\\n\"\n    \n        code_snippet += \"service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, node_parser=node_parser)\\n\\n\"\n    \n        code_snippet += \"_file = 'path_to_your_file'  # Replace with the path to your file\\n\"\n        code_snippet += \"vector_index = VectorStoreIndex.from_documents(documents=_file, storage_context=storage_context, service_context=service_context, show_progress=True)\\n\"\n        code_snippet += \"if storage_context:\\n\"\n        code_snippet += \"    vector_index.storage_context.persist(persist_dir='persist_dir')\\n\\n\"\n    \n        code_snippet += \"query_engine = vector_index.as_query_engine(response_mode=response_mode, verbose=True)\\n\"\n    \n        return code_snippet\n\n#  Conclusion\n\nRAGArch stands at the intersection of innovation and practicality, offering a\nstreamlined no-code approach to RAG pipeline development. It\u2019s designed to\ndemystify the complexities of AI configurations. ", "mimetype": "text/plain", "start_char_idx": 2330, "end_char_idx": 19548, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "baab64ef-8c30-4272-a2db-4316cf1e4ca5": {"__data__": {"id_": "baab64ef-8c30-4272-a2db-4316cf1e4ca5", "embedding": null, "metadata": {"filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce6327bc-87c8-422f-9e8f-e30fa9222be5", "node_type": "4", "metadata": {"filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "0870c90d0dc438893105a56b3911fad5184dfe184befa6d5854a1b23c2fdc79f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f32db4e-c89d-4a96-9e35-77acabfc5ef1", "node_type": "1", "metadata": {"filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}, "hash": "8ca75f07e5203053ca88b9c79ac847d5cacdb5b7db129758a96b9e67cc411684", "class_name": "RelatedNodeInfo"}}, "text": "With RAGArch, both seasoned\ndevelopers and AI enthusiasts can craft custom pipelines with ease,\naccelerating the journey from idea to implementation.\n\nYour insights and contributions are invaluable as I continue to evolve this\ntool. Check out RAGArch on Github and let\u2019s start a conversation on Linkedin.\nI\u2019m always eager to collaborate and share knowledge with fellow tech\nadventurers.\n\n[ GitHub Repo ](https://github.com/AI-ANK/RAGArch)\n\n[ Connect with Me on LinkedIn\n](https://www.linkedin.com/in/harshadsuryawanshi/)\n\n[ Live Demo ](https://huggingface.co/spaces/AI-ANK/RAGArch)\n\n", "mimetype": "text/plain", "start_char_idx": 19548, "end_char_idx": 20131, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bad81132-00e1-4006-b70d-77e115ca1ee4": {"__data__": {"id_": "bad81132-00e1-4006-b70d-77e115ca1ee4", "embedding": null, "metadata": {"filename": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.md", "extension": ".md", "title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG", "date": "Jan 31, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e5972247-4238-4dc0-8884-fe424808f9b0", "node_type": "4", "metadata": {"filename": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.md", "extension": ".md", "title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG", "date": "Jan 31, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00"}, "hash": "cd09245b82f1009f2fcb3b7b6492460558920c9047bc0c854d3f6b5d6873230b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35e17dcf-4d2f-4abf-a255-60db9414fc65", "node_type": "1", "metadata": {}, "hash": "a0a7a62f3ae23d6606a5e74b425de36487f5f9df3417c1bc2bd051a1d022ec3c", "class_name": "RelatedNodeInfo"}}, "text": "#  Introduction\n\nRetrieving the appropriate chunks, nodes, or context is a critical aspect of\nbuilding an efficient Retrieval-Augmented Generation (RAG) application.\nHowever, a vector or embedding-based search may not be effective for all types\nof user queries.\n\nTo address this, [ Hybrid search ](https://weaviate.io/blog/hybrid-search-\nexplained) combines both keyword-based methods (BM25) and vector (embedding)\nsearch techniques. Hybrid search has a specific parameter, ` Alpha ` to\nbalance the weightage between keyword (BM25) and vector search in retrieving\nthe right context for your RAG application. (alpha=0.0 - keyword search (BM25)\nand alpha=1.0 - vector search)\n\nBut here\u2019s where it gets interesting: fine-tuning Alpha isn\u2019t just a task;\nit\u2019s an art form. Achieving the ideal balance is crucial for unlocking the\nfull potential of hybrid search. This involves adjusting different Alpha\nvalues for various types of user queries in your RAG system.\n\nIn this blog post, we will look into tuning Alpha within the Weaviate vector\ndatabase using the ` [ **Retrieval Evaluation**\n](https://docs.llamaindex.ai/en/stable/examples/evaluation/retrieval/retriever_eval.html)\n` module of LlamaIndex with and without rerankers with the help of Hit Rate\nand MRR metrics.\n\nBefore diving into the implementation, let\u2019s first understand the different\nquery types and metrics we will be using in this article.\n\n#  Different User Query Types:\n\nUser queries in an RAG application vary based on individual intent. For these\ndiverse query types, it\u2019s essential to fine-tune the ` **Alpha** ` parameter.\nThis process involves routing each user query to a specific ` **Alpha** `\nvalue for effective retrieval and response synthesis. [ Microsoft\n](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-ai-\nsearch-outperforming-vector-search-with-hybrid/ba-p/3929167) has identified\nvarious user query categories, and we have selected a few for tuning our\nhybrid search. Below are the different user query types we considered:\n\n  1. **Web Search Queries:** Brief queries similar to those typically inputted into search engines. \n  2. **Concept Seeking Queries:** Abstract questions that necessitate detailed, multi-sentence answers. \n  3. **Fact Seeking Queries:** Queries that have a single, definitive answer. \n  4. **Keyword Queries:** Concise queries composed solely of crucial identifier words. \n  5. **Queries With Misspellings:** Queries containing typos, transpositions, and common misspellings. \n  6. **Exact Sub-string Searches:** Queries that exactly match sub-strings from the original context. \n\nLet\u2019s look at sample examples in each of these different user query types:\n\n  1. **Web Search Queries**\n\n> ` _Transfer capabilities of LLaMA language model to non-English languages_ `\n\n**2\\. Concept Seeking Queries**\n\n> ` _What is the dual-encoder architecture used in recent works on dense\n> retrievers?_ `\n\n**3\\. Fact Seeking Queries**\n\n> ` _What is the total number of propositions the English Wikipedia dump is\n> segmented into in FACTOID WIKI?_ `\n\n**4\\. Keyword Queries**\n\n> ` _GTR retriever recall rate_ `\n\n**5\\. Queries With Misspellings**\n\n> ` _What is the advntage of prposition retrieval over sentnce or passage\n> retrieval?_ `\n\n**6\\. Exact Sub-string Searches**\n\n> ` _first kwords for the GTR retriever. Finer-grained_ `\n\n#  Retrieval Evaluation Metrics:\n\nWe will utilize Hit Rate and MRR metrics for retrieval evaluation. Let\u2019s get\ninto understanding these metrics.\n\n**Hit Rate:**\n\nHit Rate measures the proportion of queries for which the correct chunk/\ncontext appears within the top-k results chunks/ contexts. Put simply, it\nevaluates how frequently our system correctly identifies the chunk within its\ntop-k chunks.\n\n**Mean Reciprocal Rank (MRR):**\n\nMRR assesses a system\u2019s accuracy by taking into account the position of the\nhighest-ranking relevant chunk/ context for each query. It calculates the\naverage of the inverse of these positions across all queries. For instance, if\nthe first relevant chunk/ context is at the top of the list, its reciprocal\nrank is 1. If it\u2019s the second item, the reciprocal rank becomes 1/2, and this\npattern continues accordingly.\n\nThe remainder of this blog post is divided into two main sections:\n\n  1. Implementing ` **Alpha** ` Tuning in Hybrid Search for Various Query Types. \n  2. Analyzing the results of two different document datasets: \n\n  * **Indexing a Single Document:** The [ LLM Compiler Paper ](https://arxiv.org/pdf/2312.04511.pdf) . \n  * **Indexing Three Documents:** The [ LLM Compiler ](https://arxiv.org/pdf/2312.04511.pdf) , [ Llama Beyond English ](https://arxiv.org/abs/2401.01055) , and [ Dense X Retrieval ](https://arxiv.org/abs/2312.06648) Papers. \n\nYou can also continue following along in the [ Google Colab Notebook\n](https://colab.research.google.com/drive/1aiXqofZp7hSXuUdv2UGt_QoJa_liJDZ6?usp=sharing)\nfrom this point forward.\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4922, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35e17dcf-4d2f-4abf-a255-60db9414fc65": {"__data__": {"id_": "35e17dcf-4d2f-4abf-a255-60db9414fc65", "embedding": null, "metadata": {"filename": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.md", "extension": ".md", "title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG", "date": "Jan 31, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e5972247-4238-4dc0-8884-fe424808f9b0", "node_type": "4", "metadata": {"filename": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.md", "extension": ".md", "title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG", "date": "Jan 31, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00"}, "hash": "cd09245b82f1009f2fcb3b7b6492460558920c9047bc0c854d3f6b5d6873230b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bad81132-00e1-4006-b70d-77e115ca1ee4", "node_type": "1", "metadata": {"filename": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.md", "extension": ".md", "title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG", "date": "Jan 31, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00"}, "hash": "5b3646f9146a4cc98b2efeaca326a650a8bd700fc8073e0db77623c6bdb40bb6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29d66008-36de-4c9b-86d7-37809a635dbb", "node_type": "1", "metadata": {}, "hash": "867ec11fc3bcf8fe1efbd6521da407bb9e379ce47a5ae55178f900b25767bfde", "class_name": "RelatedNodeInfo"}}, "text": "#  Implementation\n\nWe will adopt a systematic approach to implement the experimental workflow,\nwhich involves the following steps:\n\n  1. Data Download. \n  ", "mimetype": "text/plain", "start_char_idx": 4922, "end_char_idx": 5077, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29d66008-36de-4c9b-86d7-37809a635dbb": {"__data__": {"id_": "29d66008-36de-4c9b-86d7-37809a635dbb", "embedding": null, "metadata": {"filename": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.md", "extension": ".md", "title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG", "date": "Jan 31, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e5972247-4238-4dc0-8884-fe424808f9b0", "node_type": "4", "metadata": {"filename": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.md", "extension": ".md", "title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG", "date": "Jan 31, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00"}, "hash": "cd09245b82f1009f2fcb3b7b6492460558920c9047bc0c854d3f6b5d6873230b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35e17dcf-4d2f-4abf-a255-60db9414fc65", "node_type": "1", "metadata": {"filename": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.md", "extension": ".md", "title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG", "date": "Jan 31, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00"}, "hash": "27f86b18cea6368dc5f0493243f7e696139e68281c4c7e88214bce2451fc051b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d47f71f-43ac-403d-b5ca-8e76f41bda22", "node_type": "1", "metadata": {}, "hash": "9eb7d715306f6b51aab18f067f879823c0809dcfc04fbbde7d1da893c324335a", "class_name": "RelatedNodeInfo"}}, "text": "2. Data Loading. \n  3. Weaviate Client Setup. \n  4. Index Creation and Node Insertion. \n  5. ", "mimetype": "text/plain", "start_char_idx": 5077, "end_char_idx": 5170, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d47f71f-43ac-403d-b5ca-8e76f41bda22": {"__data__": {"id_": "0d47f71f-43ac-403d-b5ca-8e76f41bda22", "embedding": null, "metadata": {"filename": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.md", "extension": ".md", "title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG", "date": "Jan 31, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e5972247-4238-4dc0-8884-fe424808f9b0", "node_type": "4", "metadata": {"filename": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.md", "extension": ".md", "title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG", "date": "Jan 31, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00"}, "hash": "cd09245b82f1009f2fcb3b7b6492460558920c9047bc0c854d3f6b5d6873230b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29d66008-36de-4c9b-86d7-37809a635dbb", "node_type": "1", "metadata": {"filename": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.md", "extension": ".md", "title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG", "date": "Jan 31, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00"}, "hash": "2054ddc48a19916962ae508a6802516a525862d3c7483f4baef6a0a4a7701abb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d475a7e-8fba-4477-b219-f339c222f706", "node_type": "1", "metadata": {}, "hash": "f465bd79c48890ffb0e7ff5e8103f378767a9d4a3550efc1a231e24b3a0699e7", "class_name": "RelatedNodeInfo"}}, "text": "Define LLM (GPT-4) \n  6. Define CohereAI Reranker. \n  7. Generation of Synthetic Queries for Various Query Types. \n  8. Define CustomRetriever. \n  9. Functions for Retrieval Evaluation and Metrics Calculation. \n  10. Conducting Retrieval Evaluation for Different Query Types and Alpha Values. \n\nLet\u2019s begin by defining some essential functions for our implementation.\n\n  ", "mimetype": "text/plain", "start_char_idx": 5170, "end_char_idx": 5541, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d475a7e-8fba-4477-b219-f339c222f706": {"__data__": {"id_": "0d475a7e-8fba-4477-b219-f339c222f706", "embedding": null, "metadata": {"filename": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.md", "extension": ".md", "title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG", "date": "Jan 31, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e5972247-4238-4dc0-8884-fe424808f9b0", "node_type": "4", "metadata": {"filename": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.md", "extension": ".md", "title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG", "date": "Jan 31, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00"}, "hash": "cd09245b82f1009f2fcb3b7b6492460558920c9047bc0c854d3f6b5d6873230b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d47f71f-43ac-403d-b5ca-8e76f41bda22", "node_type": "1", "metadata": {"filename": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.md", "extension": ".md", "title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG", "date": "Jan 31, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00"}, "hash": "5dc2ce983c00e1ce3845475e17a8571ebd36a1e17fd6196651ee7736979d3d80", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae0233f8-3d39-46d5-a534-ebb7eeb5470e", "node_type": "1", "metadata": {}, "hash": "17dafce1ddd403cdd75718c354c056c2574bded18caa0ee300aeeb3efb0e6275", "class_name": "RelatedNodeInfo"}}, "text": "1. ` get_weaviate_client ` \\- sets up weaviate client. \n  2. ` load_documents ` \\- load the documents from the file path. \n  3. ` create_nodes ` \\- create nodes by chunking the documents using a text splitter. \n  4. ", "mimetype": "text/plain", "start_char_idx": 5541, "end_char_idx": 5757, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae0233f8-3d39-46d5-a534-ebb7eeb5470e": {"__data__": {"id_": "ae0233f8-3d39-46d5-a534-ebb7eeb5470e", "embedding": null, "metadata": {"filename": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.md", "extension": ".md", "title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG", "date": "Jan 31, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e5972247-4238-4dc0-8884-fe424808f9b0", "node_type": "4", "metadata": {"filename": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.md", "extension": ".md", "title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG", "date": "Jan 31, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00"}, "hash": "cd09245b82f1009f2fcb3b7b6492460558920c9047bc0c854d3f6b5d6873230b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d475a7e-8fba-4477-b219-f339c222f706", "node_type": "1", "metadata": {"filename": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.md", "extension": ".md", "title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG", "date": "Jan 31, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00"}, "hash": "0d3f391439df87c5b89c32413dbc0c839cd76aed90a5e627d23521e5e372af18", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2529ebf-87af-40d5-b115-067c0a51316c", "node_type": "1", "metadata": {}, "hash": "aa8f3d32e085509b9746368efeedf677b631e491a480e25a7861d1c19829cc89", "class_name": "RelatedNodeInfo"}}, "text": "` connect_index ` \\- connect to weaviate index. \n  5. ` insert_nodes_index ` \\- insert nodes into the index. \n\n    \n    \n    def get_weaviate_client(api_key, url):\n      auth_config = weaviate.AuthApiKey(api_key=api_key)\n    \n      client = weaviate.Client(\n        url=url,\n        auth_client_secret=auth_config\n      )\n      return client\n    \n    def load_documents(file_path, num_pages=None):\n      if num_pages:\n        documents = SimpleDirectoryReader(input_files=[file_path]).load_data()[:num_pages]\n      else:\n        documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n      return documents\n    \n    def create_nodes(documents, chunk_size=512, chunk_overlap=0):\n      node_parser = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n      nodes = node_parser.get_nodes_from_documents(documents)\n      return nodes\n    \n    def connect_index(weaviate_client):\n      vector_store = WeaviateVectorStore(weaviate_client=weaviate_client)\n      storage_context = StorageContext.from_defaults(vector_store=vector_store)\n      index = VectorStoreIndex([], storage_context=storage_context)\n      return index\n    \n    def insert_nodes_index(index, nodes):\n      index.insert_nodes(nodes)\n\n  1. **Download Data**\n\n    \n    \n    !wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2312.04511.pdf\" -O \"llm_compiler.pdf\"\n    !wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2401.01055.pdf\" -O \"llama_beyond_english.pdf\"\n    !wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2312.06648.pdf\" -O \"dense_x_retrieval.pdf\"\n\n2\\. **Load Data**\n\n    \n    \n    # load documents, we will skip references and appendices from the papers.\n    documents1 = load_documents(\"llm_compiler.pdf\", 12)\n    documents2 = load_documents(\"dense_x_retrieval.pdf\", 9)\n    documents3 = load_documents(\"llama_beyond_english.pdf\", 7)\n    \n    # create nodes\n    nodes1 = create_nodes(documents1)\n    nodes2 = create_nodes(documents2)\n    nodes3 = create_nodes(documents3)\n\n3\\. **Setup Weaviate Client**\n\n    \n    \n    url = 'cluster URL'\n    api_key = 'your api key'\n    \n    client = get_weaviate_client(api_key, url)\n\n4\\. **Create an Index and Insert Nodes.**\n\n    \n    \n    index = connect_index(client)\n    \n    insert_nodes_index(index, nodes1)\n\n5\\. **Define LLM**\n\n    \n    \n    # Deing LLM for query generation\n    llm = OpenAI(model='gpt-4', temperature=0.1)\n\n6\\. **Create Synthetic Queries**\n\nWe will create queries as discussed earlier, check prompts for each of the\nquery types in the notebook, and code for each type of query. Showing code\nsnippet for reference.\n\n    \n    \n    queries = generate_question_context_pairs(\n        nodes, \n      llm=llm, \n      num_questions_per_chunk=2, \n      qa_generate_prompt_tmpl = qa_template\n    )\n\n7\\. **Define reranker**\n\n    \n    \n    reranker = CohereRerank(api_key=os.environ['COHERE_API_KEY'], top_n=4)\n\n8\\. **Define CustomRetriever**\n\nWe will define ` CustomRetriever ` class to perform retrieval operations with\nand without a reranker.\n\n    \n    \n    class CustomRetriever(BaseRetriever):\n        \"\"\"Custom retriever that performs hybrid search with and without reranker\"\"\"\n    \n        def __init__(\n            self,\n            vector_retriever: VectorIndexRetriever,\n            reranker: CohereRerank\n        ) -&gt; None:\n            \"\"\"Init params.\"\"\"\n    \n            self._vector_retriever = vector_retriever\n            self._reranker = reranker\n    \n        def _retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:\n            \"\"\"Retrieve nodes given query.\"\"\"\n    \n            retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n    \n            if self._reranker != None:\n                retrieved_nodes = self._reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n            else:\n                retrieved_nodes = retrieved_nodes[:4]\n    \n            return retrieved_nodes\n    \n        async def _aretrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:\n            \"\"\"Asynchronously retrieve nodes given query.\n    \n            Implemented by the user.\n    \n            \"\"\"\n            return self._retrieve(query_bundle)\n    \n        async def aretrieve(self, str_or_query_bundle: QueryType) -&gt; List[NodeWithScore]:\n            if isinstance(str_or_query_bundle, str):\n                str_or_query_bundle = QueryBundle(str_or_query_bundle)\n            return await self._aretrieve(str_or_query_bundle)\n\n9\\. **Define functions for retriever evaluation and metrics computation**\n\nWe will look into retriever performance for different ` alpha ` values with\nand without reranker.\n\n    \n    \n    # Alpha values and datasets to test\n    alpha_values = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n    \n    # Function to evaluate retriever and return results\n    async def evaluate_retriever(alpha, dataset, reranker=None):\n        retriever = VectorIndexRetriever(index,\n                                         vector_store_query_mode=\"hybrid\",\n                                         similarity_top_k=10,\n                                         alpha=alpha)\n        custom_retriever = CustomRetriever(retriever,\n                                           reranker)\n    \n        retriever_evaluator = RetrieverEvaluator.from_metric_names([\"mrr\", \"hit_rate\"], retriever=custom_retriever)\n        eval_results = await retriever_evaluator.aevaluate_dataset(dataset)\n        return eval_results\n    \n    # Function to calculate and store metrics\n    def calculate_metrics(eval_results):\n        metric_dicts = []\n        for eval_result in eval_results:\n            metric_dict = eval_result.metric_vals_dict\n            metric_dicts.append(metric_dict)\n    \n        full_df = pd.DataFrame(metric_dicts)\n    \n        hit_rate = full_df[\"hit_rate\"].mean()\n        mrr = full_df[\"mrr\"].mean()\n        return hit_rate, mrr\n\n**10\\. Retrieval Evaluation**\n\nHere we do retrieval evaluation on different query types (datasets) and alpha\nvalues to understand which alpha will be suitable for which query type. You\nneed to plug in the reranker accordingly to compute the retrieval evaluation\nwith and without the reranker.\n\n    \n    \n    # Asynchronous function to loop over datasets and alpha values and evaluate\n    async def main():\n        results_df = pd.DataFrame(columns=['Dataset', 'Alpha', 'Hit Rate', 'MRR'])\n    \n        for dataset in datasets_single_document.keys():\n            for alpha in alpha_values:\n                eval_results = await evaluate_retriever(alpha, datasets_single_document[dataset])\n                hit_rate, mrr = calculate_metrics(eval_results)\n                new_row = pd.DataFrame({'Dataset': [dataset], 'Alpha': [alpha], 'Hit Rate': [hit_rate], 'MRR': [mrr]})\n                results_df = pd.concat([results_df, new_row], ignore_index=True)\n    \n        # Determine the grid size for subplots\n        num_rows = len(datasets_single_document) // 2 + len(datasets_single_document) % 2\n        num_cols = 2\n    \n        # Plotting the results in a grid\n        fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, num_rows * 4), squeeze=False)  # Ensure axes is always 2D\n    \n        for i, dataset in enumerate(datasets_single_document):\n            ax = axes[i // num_cols, i % num_cols]\n            dataset_df = results_df[results_df['Dataset'] == dataset]\n            ax.plot(dataset_df['Alpha'], dataset_df['Hit Rate'], marker='o', label='Hit Rate')\n            ax.plot(dataset_df['Alpha'], dataset_df['MRR'], marker='o', linestyle='--', label='MRR')\n            ax.set_xlabel('Alpha')\n            ax.set_ylabel('Metric Value')\n            ax.set_title(f'{dataset}')\n            ax.legend()\n            ax.grid(True)\n    \n        # If the number of datasets is odd, remove the last (empty) subplot\n        if len(datasets_single_document) % num_cols != 0:\n            fig.delaxes(axes[-1, -1])  # Remove the last subplot if not needed\n    \n        # Adjust layout to prevent overlap\n        plt.tight_layout()\n        plt.show()\n    \n    # Run the main function\n    asyncio.run(main())\n\n#  Analyze the results:\n\nHaving completed the implementation phase, we now turn our attention to\nanalyzing the outcomes. We conducted two sets of experiments: one on a single\ndocument and another on multiple documents. ", "mimetype": "text/plain", "start_char_idx": 5757, "end_char_idx": 14021, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2529ebf-87af-40d5-b115-067c0a51316c": {"__data__": {"id_": "c2529ebf-87af-40d5-b115-067c0a51316c", "embedding": null, "metadata": {"filename": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.md", "extension": ".md", "title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG", "date": "Jan 31, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e5972247-4238-4dc0-8884-fe424808f9b0", "node_type": "4", "metadata": {"filename": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.md", "extension": ".md", "title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG", "date": "Jan 31, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00"}, "hash": "cd09245b82f1009f2fcb3b7b6492460558920c9047bc0c854d3f6b5d6873230b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae0233f8-3d39-46d5-a534-ebb7eeb5470e", "node_type": "1", "metadata": {"filename": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.md", "extension": ".md", "title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG", "date": "Jan 31, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00"}, "hash": "ec6734c6a18401b751fd18f98169c1623012b57667c95952f7a64b9ef6d2105f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f3ea832d-5230-4909-a5d0-32377f3c32ca", "node_type": "1", "metadata": {}, "hash": "467e1ed1c4c6010b447eb7f6b655d600f652e1d6d7b766fe3048d04e1bc366d1", "class_name": "RelatedNodeInfo"}}, "text": "These experiments varied in alpha\nvalues, types of user queries, and the inclusion or exclusion of a reranker.\nThe accompanying graphs display the results, focusing on the Hit Rate and MRR\n(Mean Reciprocal Rank) as retrieval evaluation metrics.\n\n> P  lease keep in mind that following observations are specific to the\n> datasets used in our study. We encourage you to conduct the experiment with\n> your own documents and draw your relevant observations and conclusions.\n\n##  **With Single Document:**\n\n**Without Reranker:**\n\n**With Reranker:**\n\n##  With Multiple Documents:\n\n**Without Reranker:**\n\n**With Reranker:**\n\n#  Observations:\n\n  1. There is a boost in Hit Rate and MRR in single and multiple documents indexing with the help of a reranker. Time and again it proves using reranker is pretty useful in your RAG application. \n  2. Though most of the time hybrid search wins over keyword/ vector search, it should be carefully evaluated for different query types based on user queries in the RAG application. \n  3. The behavior is different when you index a single document and multiple documents, which suggests it\u2019s always better to tune alpha as you add documents into the index. \n  4. Let\u2019s look at a deeper analysis of different query types: \n\n  * **Web Search Queries:**\n\n\u2014 MRR is higher with hybrid search with alpha=0.2/0.6 based on with/ without\nrerankers irrespective of single/ multiple documents indexing.\n\n\u2014 The Hit rate is higher with alpha=1.0 for both single/ multiple documents\nindexing and with/ without rerankers.\n\n  * **Concept Seeking Queries:**\n\n\u2014 MRR and Hit Rate are higher with hybrid search (with different alpha values)\nin Multiple documents indexing.\n\n\u2014 MRR and Hit Rate are higher at Alpha=0.0 indicating keyword search works\nbetter in Single document indexing. Should be noted that MRR has different\nbehavior with and without reranking.\n\n  * **Fact Seeking Queries**\n\n\u2014 MRR and Hit Rate are higher with Hybrid search with/ without reranker in\nMultiple documents indexing.\n\n\u2014 MRR and Hit Rate are higher with hybrid search with reranker and keyword\nsearch (alpha=0.0) is better without reranker in single documents indexing.\n\n  * **Keyword Queries**\n\n\u2014 MRR and Hit Rate are higher with Hybrid search with/ without reranker in\nMultiple documents indexing.\n\n\u2014 MRR and Hit Rate are higher with hybrid search with reranker and keyword\nsearch is better without reranker in single documents indexing. (though MRR is\nslightly higher with alpha=0.2)\n\n  * **Queries With Misspellings**\n\n\u2014 MRR and Hit Rate are higher with Hybrid search with/ without reranker in\nsingle and multiple documents indexing. (Though in some cases hybrid search\nwith alpha=1.0 wins).\n\n\u2014 This also demonstrates that vector search performs better with misspelled\nqueries, as keyword searches lose effectiveness in such cases.\n\n  * **Exact Sub-string Searches**\n\n\u2014 MRR and Hit Rate are higher with Keyword search with/ without reranker in\nSingle documents indexing and without reranker in multiple documents indexing.\n\n\u2014 MRR and Hit Rate are higher with Hybrid search (alpha=0.4) with reranker in\nmultiple documents indexing.\n\n#  What\u2019s Next?\n\nIn this blog post, we looked into the tuning of Alpha in a hybrid search\nsystem for a range of query types. It was interesting to see how the results\nvaried when indexing either a single document or multiple documents. Going\nforward, you might consider experimenting with documents from diverse domains,\nemploying different query lengths for various query types. Should you come\nacross any noteworthy observations, we encourage you to share them with us in\nthe comments. ", "mimetype": "text/plain", "start_char_idx": 14021, "end_char_idx": 17634, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f3ea832d-5230-4909-a5d0-32377f3c32ca": {"__data__": {"id_": "f3ea832d-5230-4909-a5d0-32377f3c32ca", "embedding": null, "metadata": {"filename": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.md", "extension": ".md", "title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG", "date": "Jan 31, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e5972247-4238-4dc0-8884-fe424808f9b0", "node_type": "4", "metadata": {"filename": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.md", "extension": ".md", "title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG", "date": "Jan 31, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00"}, "hash": "cd09245b82f1009f2fcb3b7b6492460558920c9047bc0c854d3f6b5d6873230b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2529ebf-87af-40d5-b115-067c0a51316c", "node_type": "1", "metadata": {"filename": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.md", "extension": ".md", "title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG", "date": "Jan 31, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00"}, "hash": "2ca564b161550b070530433b223d0ff24878de0e38a3a2aea5d7d1afe7a20481", "class_name": "RelatedNodeInfo"}}, "text": "It would certainly be interesting to discuss these findings with\nthe wider community.\n\n#  References:\n\n  1. [ Hybrid Search Explained ](https://weaviate.io/blog/hybrid-search-explained)\n  2. [ Azure AI Search: Outperforming vector search with hybrid retrieval and ranking capabilities ](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-ai-search-outperforming-vector-search-with-hybrid/ba-p/3929167)\n\n", "mimetype": "text/plain", "start_char_idx": 17634, "end_char_idx": 18057, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3ce4930-8c30-4584-adbc-c0edfd3e4d1e": {"__data__": {"id_": "b3ce4930-8c30-4584-adbc-c0edfd3e4d1e", "embedding": null, "metadata": {"filename": "building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.md", "extension": ".md", "title": "Building a Fully Open Source Retriever with Nomic Embed and LlamaIndex", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8cc35d72-043c-479f-b0bd-499aac454c6e", "node_type": "4", "metadata": {"filename": "building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.md", "extension": ".md", "title": "Building a Fully Open Source Retriever with Nomic Embed and LlamaIndex", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4"}, "hash": "85b9c524f0c9eaf62c053d85f1c689792f370e84d3c0cfdc090387f41379f2e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83d83596-7bba-4912-ac7f-c2c8ecf3b71c", "node_type": "1", "metadata": {}, "hash": "dcd1ecce37f5a7835edca7cd8f7945beffe634f309c27402a0b8d517efc5ad28", "class_name": "RelatedNodeInfo"}}, "text": "#  What is a Retriever?\n\nRecently, retrieval augmented generation (RAG) has enabled language models to\nreduce hallucinations, improve response quality, and maintain up-to-date\nknowledge of the world _without_ requiring retraining of the model itself.\nThis is done by equipping a language model with a _retriever_ and a\n_database._ At inference time, a RAG system uses the retriever to select\nrelevant documents from the database, and passes them to the language model\ncontext window.\n\nToday, the most popular type of retriever is based on an embedding model. This\nembedding model converts all of the documents in the database to a vector\nrepresentation. Then, at inference time, it converts the query to a vector\nrepresentation, and retrieves the most similar documents to the query vector\nfrom the database.\n\nIn this post, we are going to show you how to build a fully open source\nretriever using LlamaIndex and Nomic Embed, the first fully open source\nembedding model to exceed OpenAI Ada performance on both short and long\ncontext benchmarks.\n\n#  Why Open Source?\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1068, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83d83596-7bba-4912-ac7f-c2c8ecf3b71c": {"__data__": {"id_": "83d83596-7bba-4912-ac7f-c2c8ecf3b71c", "embedding": null, "metadata": {"filename": "building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.md", "extension": ".md", "title": "Building a Fully Open Source Retriever with Nomic Embed and LlamaIndex", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8cc35d72-043c-479f-b0bd-499aac454c6e", "node_type": "4", "metadata": {"filename": "building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.md", "extension": ".md", "title": "Building a Fully Open Source Retriever with Nomic Embed and LlamaIndex", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4"}, "hash": "85b9c524f0c9eaf62c053d85f1c689792f370e84d3c0cfdc090387f41379f2e2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b3ce4930-8c30-4584-adbc-c0edfd3e4d1e", "node_type": "1", "metadata": {"filename": "building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.md", "extension": ".md", "title": "Building a Fully Open Source Retriever with Nomic Embed and LlamaIndex", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4"}, "hash": "51990b5a49bce866121d47c78c47b5dc4075c3e479143a14aa21575fd3ee1d6d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6c9c97f-004c-4959-a13d-af0a2566ddf4", "node_type": "1", "metadata": {}, "hash": "bd335a834baa18444cdcef796518547f2c0c25e5f0f496bec0e2e313e5d1fa34", "class_name": "RelatedNodeInfo"}}, "text": "As AI becomes deployed in increasingly high impact domains, such as defense,\nmedicine, and finance, end-to-end auditability of the entire system becomes a\nkey component of safe AI deployment. Unfortunately, the closed source\nembedding models used in most RAG systems today have deliberately obfuscated\ntraining protocols and cannot be audited.\n\nFurther, as organizations adopting AI begin to mature, reliance on closed\nsource embedding models will result in vendor lock-in and a limited ability to\nmodify the embedding model to suit the needs of the business.\n\nLuckily, fully open source embedding models like Nomic Embed offer end-to-end\nauditability of the training process as well as a strong basis for further\nimprovements and modifications of the model.\n\n#  How To\n\nTo build an open source retriever with LlamaIndex and Nomic Embed, we will\nstart by importing the relevant libraries\n\n    \n    \n    from llama_index.embeddings import NomicEmbedding\n    from llama_index import (\n        VectorStoreIndex,\n        SimpleDirectoryReader,\n        ServiceContext,\n    )\n\nNext, we need to download some data for our database. For this example, we are\ngoing to use an essay by Paul Graham, which we download from [ here\n](https://raw.githubusercontent.com/run-\nllama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt)\nand place into a directory named ./data/paul_graham.\n\nNow, it\u2019s time to get the vectors for the documents in our database. To do\nthis, we are going to use the LlamaIndex SimpleDirectoryReader and Nomic\u2019s\nhosted inference service. You\u2019ll have to replace <NOMIC_API_KEY> with your\nNomic API key, which you can get after signing up for Nomic Atlas [ here\n](https://atlas.nomic.ai/) .\n\n    \n    \n    documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n    nomic_api_key = \"&lt;NOMIC_API_KEY&gt;\"\n    embed_model = NomicEmbedding(\n        api_key=nomic_api_key,\n        model_name=\"nomic-embed-text-v1\",\n        task_type=\"search_document\"\n    )\n    service_context = ServiceContext.from_defaults(\n        embed_model=embed_model, chunk_size=1024,\n    )\n    index = VectorStoreIndex.from_documents(\n        documents=documents, service_context=service_context, show_progress=True\n    )\n\nNotice that we set task_type to search_document in NomicEmbedding. Nomic Embed\nsupports many different types of tasks, and search_document is optimized for\nbuilding representations of documents for RAG databases.\n\nOnce our database is set up, we are ready to build our retriever. Using\nLlamaIndex, this is as simple as a few lines of python:\n\n    \n    \n    embed_model = NomicEmbedding(\n        api_key=nomic_api_key,\n        model_name=\"nomic-embed-text-v1\",\n        task_type=\"search_query\"\n    )\n    \n    service_context = ServiceContext.from_defaults(\n        embed_model=embed_model\n    )\n    \n    search_query_retriever = index.as_retriever(service_context=service_context, similarity_top_k=1)\n\nAgain, notice that we are using a new NomicEmbedding model with task_type set\nto search_query. This task type is optimized for embedding queries for search\nover a retrieval database.\n\nFinally, we can use our retriever to surface relevant documents given user\nqueries! As an example:\n\n    \n    \n    retrieved_nodes_nomic = retriever_nomic.retrieve(\n        \"What software did Paul write?\"\n    )\n\nreturns a document that describes Paul\u2019s first programs:\n\n    \n    \n    Node ID: 380fbb0e-6fc1-41de-a4f6-3f22cd508df3\n    Similarity: 0.6087318771843091\n    Text: What I Worked On\n    \n    February 2021\n    \n    Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. ", "mimetype": "text/plain", "start_char_idx": 1068, "end_char_idx": 4731, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c6c9c97f-004c-4959-a13d-af0a2566ddf4": {"__data__": {"id_": "c6c9c97f-004c-4959-a13d-af0a2566ddf4", "embedding": null, "metadata": {"filename": "building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.md", "extension": ".md", "title": "Building a Fully Open Source Retriever with Nomic Embed and LlamaIndex", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8cc35d72-043c-479f-b0bd-499aac454c6e", "node_type": "4", "metadata": {"filename": "building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.md", "extension": ".md", "title": "Building a Fully Open Source Retriever with Nomic Embed and LlamaIndex", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4"}, "hash": "85b9c524f0c9eaf62c053d85f1c689792f370e84d3c0cfdc090387f41379f2e2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83d83596-7bba-4912-ac7f-c2c8ecf3b71c", "node_type": "1", "metadata": {"filename": "building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.md", "extension": ".md", "title": "Building a Fully Open Source Retriever with Nomic Embed and LlamaIndex", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4"}, "hash": "55ad72f6040de4078169c24fb9b1b8c4ddab6476b274f54516ce13851789bc99", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "adb0e3fa-300e-41fe-9515-dae0c8fd5bc5", "node_type": "1", "metadata": {}, "hash": "dc7901f26f48c958bcc2c56b29ddcb34b464a3d3bb722a5639bad930fe36384c", "class_name": "RelatedNodeInfo"}}, "text": "I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n    \n    The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines \u2014 CPU, disk drives, printer, card reader \u2014 sitting up on a raised floor under bright fluorescent lights.\n    \n    The language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\n    \n    I was puzzled by the 1401. I couldn't figure out what to do with it. ", "mimetype": "text/plain", "start_char_idx": 4731, "end_char_idx": 5833, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "adb0e3fa-300e-41fe-9515-dae0c8fd5bc5": {"__data__": {"id_": "adb0e3fa-300e-41fe-9515-dae0c8fd5bc5", "embedding": null, "metadata": {"filename": "building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.md", "extension": ".md", "title": "Building a Fully Open Source Retriever with Nomic Embed and LlamaIndex", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8cc35d72-043c-479f-b0bd-499aac454c6e", "node_type": "4", "metadata": {"filename": "building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.md", "extension": ".md", "title": "Building a Fully Open Source Retriever with Nomic Embed and LlamaIndex", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4"}, "hash": "85b9c524f0c9eaf62c053d85f1c689792f370e84d3c0cfdc090387f41379f2e2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6c9c97f-004c-4959-a13d-af0a2566ddf4", "node_type": "1", "metadata": {"filename": "building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.md", "extension": ".md", "title": "Building a Fully Open Source Retriever with Nomic Embed and LlamaIndex", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4"}, "hash": "1493b916da36207f747e84bb205dcd283c00e46ddbb98abc27a1eac152239610", "class_name": "RelatedNodeInfo"}}, "text": "And in retrospect there's not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn't have any data stored on punched cards. The only other option was to do things that didn't rely on any input, like calculate approximations of pi, but I didn't know enough math to do anything interesting of that type. So I'm not surprised I can't remember any programs I wrote, because they can't have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn't. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager's expression made clear.\n    \n    With microcomputers, everything changed. \n\n#  Conclusion & Next Steps\n\nIn this post, we showed you how to build a fully open source retriever using\nNomic Embed and LlamaIndex. If you want to dive deeper, you can find the\nsource code for Nomic Embed [ here ](https://github.com/nomic-ai/contrastors)\n. You can also use Nomic [ Atlas ](https://atlas.nomic.ai/) to visualize your\nretrieval database, and [ LlamaIndex ](https://www.llamaindex.ai/) to connect\nit to a generative model for full RAG.\n\n", "mimetype": "text/plain", "start_char_idx": 5833, "end_char_idx": 7046, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5735661-0bd1-4550-8402-80137bc4c456": {"__data__": {"id_": "f5735661-0bd1-4550-8402-80137bc4c456", "embedding": null, "metadata": {"filename": "agentic-rag-with-llamaindex-2721b8a49ff6.md", "extension": ".md", "title": "Agentic RAG With LlamaIndex", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/agentic-rag-with-llamaindex-2721b8a49ff6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56b78b37-dd0a-4961-83bb-e841cd578c6b", "node_type": "4", "metadata": {"filename": "agentic-rag-with-llamaindex-2721b8a49ff6.md", "extension": ".md", "title": "Agentic RAG With LlamaIndex", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/agentic-rag-with-llamaindex-2721b8a49ff6"}, "hash": "3c445276a04abf6cef217dd54c4a0db392679264c9b10609755ec1c007ee4ac4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b30a738e-75d1-4a93-8f76-26a7fc4d836e", "node_type": "1", "metadata": {}, "hash": "69776ea3c178d1d4de82a1eec6db82948d213bb76c9771fadb1179cca9dd2a71", "class_name": "RelatedNodeInfo"}}, "text": "The topic of Agentic RAG explores how agents can be incorporated into existing\nRAG pipelines for enhanced, conversational search and retrieval.\n\n#  Introduction\n\n**Considering the architecture below, it is evident how Agentic RAG creates an\nimplementation which easily scales. New documents can be added with each new\nset being managed by a sub-agent.**\n\nThe basic structure of LlamaIndex\u2019s approach called Agentic RAG is shown in\nthe diagram below where a large set of documents are ingested, in this case it\nwas limited to 100.\n\nThe large corpus of data is broken up into smaller documents. An agent is\ncreated for each document, and each of the numerous document agents have the\npower of search via embeddings and to summarise the response.\n\nA top-level agent is created over the set of document agents. The meta-agent /\ntop-level agent performs tool retrieval and then uses Chain-of-Thought to\nanswer the user\u2019s question.\n\nThe Rerank endpoint computes a relevance score for the query and each\ndocument, and returns a sorted list from the most to the least relevant\ndocument.\n\n#  Notebook Example\n\nHere you will find a Colab [ notebook\n](https://github.com/cobusgreyling/LlamaIndex/blob/d8902482a247c76c7902ded143a875d5580f072a/Agentic_RAG_Multi_Document_Agents-v1.ipynb)\nwith a fully working and executed example of this [ implementation\n](https://github.com/cobusgreyling/LlamaIndex/blob/d8902482a247c76c7902ded143a875d5580f072a/Agentic_RAG_Multi_Document_Agents-v1.ipynb)\n.\n\nTo run the notebook, you will need an OpenAI and Cohere API key\u2026\n\n    \n    \n    import osimport openaiimport cohereos.environ[\"OPENAI_API_KEY\"] = \"Your API Key goes here\"os.environ[\"COHERE_API_KEY\"] = \"Your API Key goes here\"\n\nFor each document agent, the system prompt is:\n\n    \n    \n    You are a specialized agent designed to answer queries about the `{file_base}.html` part of the LlamaIndex docs.You must ALWAYS use at least one of the tools provided when answering a question; do NOT rely on prior knowledge.\n\nThe system prompt for the meta-agent /top-agent:\n\n    \n    \n     You are an agent designed to answer queries about the documentation. Please always use the tools provided to answer a question. Do not rely on prior knowledge\n\n##  Working Example\n\nI asked the agent the following question:\n\n` Tell me about LlamaIndex connectors `\n\nBelow is the detailed answer generated by the agent:\n\n**_Thought: I need to use a tool to help me answer the question._ **\n\n**_Action: tool_latest_search Action Input: {\u2018input\u2019: \u2018LlamaIndex\nconnectors\u2019}_ **\n\nAdded user message to memory: LlamaIndex connectors\n\n=== Calling Function ===  \nCalling function: vector_tool_latest_search with args: {  \n\u201cinput\u201d: \u201cLlamaIndex connectors\u201d  \n}  \nGot output: The LlamaIndex connectors can be found on the\n/content/docs.llamaindex.ai/en/latest/search.html page.\n========================\n\n**_Observation: The LlamaIndex connectors are not specifically mentioned in\nthe `latest_search.html` part of the LlamaIndex docs. You may need to check\nother sections of the documentation for detailed information about LlamaIndex\nconnectors._ **\n\n**_Thought: I need to search for information about LlamaIndex connectors in a\ndifferent tool._ **\n\n**_Action: tool_latest_index Action Input: {\u2018input\u2019: \u2018LlamaIndex connectors\u2019}_\n**\n\nAdded user message to memory: LlamaIndex connectors\n\n=== Calling Function ===  \nCalling function: vector_tool_latest_index with args: {  \n\u201cinput\u201d: \u201cLlamaIndex connectors\u201d  \n}\n\nGot output: LlamaIndex provides data connectors that allow you to ingest your\nexisting data from various sources and formats. These connectors can work with\nAPIs, PDFs, SQL, and more. By using LlamaIndex connectors, you can easily\nbring your data into the LlamaIndex ecosystem and make it accessible for\nnatural language access and retrieval. ========================\n\n**_Observation: LlamaIndex offers data connectors that enable you to import\nyour existing data from a variety of sources and formats. These connectors are\ncompatible with APIs, PDFs, SQL, and more. Utilizing LlamaIndex connectors\nallows you to seamlessly integrate your data into the LlamaIndex ecosystem,\nmaking it readily available for natural language access and retrieval._ **\n\n**_Thought: I can answer without using any more tools._ **\n\n**_Answer: LlamaIndex connectors are used to import existing data from various\nsources and formats into the LlamaIndex ecosystem. These connectors are\ncompatible with APIs, PDFs, SQL, and more, allowing seamless integration of\ndata for natural language access and retrieval._ **\n\nBelow is a snipped from the Colab notebook:\n\n[ Source\n](https://github.com/cobusgreyling/LlamaIndex/blob/d8902482a247c76c7902ded143a875d5580f072a/Agentic_RAG_Multi_Document_Agents-v1.ipynb)\n\nThis complex implementation from LlamaIndex is an example of multi-document\nagents which can:\n\n  1. Select documents relevant to a user query \n  2. Execute an agentic loop over the documents relevant to the query; including chain-of-thought, summarisation and reranking. \n\n#  In Conclusion\n\nThis implementation by LlamaIndex illustrates a few key principles\u2026\n\n  1. Agentic RAG, where an agent approach is followed for a RAG implementation adds resilience and intelligence to the RAG implementation. \n  2. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5245, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b30a738e-75d1-4a93-8f76-26a7fc4d836e": {"__data__": {"id_": "b30a738e-75d1-4a93-8f76-26a7fc4d836e", "embedding": null, "metadata": {"filename": "agentic-rag-with-llamaindex-2721b8a49ff6.md", "extension": ".md", "title": "Agentic RAG With LlamaIndex", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/agentic-rag-with-llamaindex-2721b8a49ff6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56b78b37-dd0a-4961-83bb-e841cd578c6b", "node_type": "4", "metadata": {"filename": "agentic-rag-with-llamaindex-2721b8a49ff6.md", "extension": ".md", "title": "Agentic RAG With LlamaIndex", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/agentic-rag-with-llamaindex-2721b8a49ff6"}, "hash": "3c445276a04abf6cef217dd54c4a0db392679264c9b10609755ec1c007ee4ac4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5735661-0bd1-4550-8402-80137bc4c456", "node_type": "1", "metadata": {"filename": "agentic-rag-with-llamaindex-2721b8a49ff6.md", "extension": ".md", "title": "Agentic RAG With LlamaIndex", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/agentic-rag-with-llamaindex-2721b8a49ff6"}, "hash": "83d24277fdcc53fa5507092fdd49ebcad91c8ed36d10cf8d2df38023932bed58", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1613584-22e3-4529-9e3c-c824a8eeefe5", "node_type": "1", "metadata": {}, "hash": "a1ab0801503ccf665b12658dedf68b18d6dfd8da661df6daef00eb555dd71b58", "class_name": "RelatedNodeInfo"}}, "text": "It is a good illustration of multi-agent orchestration. \n  3. This architecture serves as a good reference framework of how scaling an agent can be optimised with a second tier of smaller worker-agents. \n  4. Agentic RAG is an example of a controlled and well defined **_autonomous_ ** **_agent_ ** implementation. \n  5. One of the most sought-after enterprise LLM implementation types are RAG, Agentic RAG is a natural progression of this. \n  6. ", "mimetype": "text/plain", "start_char_idx": 5245, "end_char_idx": 5692, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1613584-22e3-4529-9e3c-c824a8eeefe5": {"__data__": {"id_": "d1613584-22e3-4529-9e3c-c824a8eeefe5", "embedding": null, "metadata": {"filename": "agentic-rag-with-llamaindex-2721b8a49ff6.md", "extension": ".md", "title": "Agentic RAG With LlamaIndex", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/agentic-rag-with-llamaindex-2721b8a49ff6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56b78b37-dd0a-4961-83bb-e841cd578c6b", "node_type": "4", "metadata": {"filename": "agentic-rag-with-llamaindex-2721b8a49ff6.md", "extension": ".md", "title": "Agentic RAG With LlamaIndex", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/agentic-rag-with-llamaindex-2721b8a49ff6"}, "hash": "3c445276a04abf6cef217dd54c4a0db392679264c9b10609755ec1c007ee4ac4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b30a738e-75d1-4a93-8f76-26a7fc4d836e", "node_type": "1", "metadata": {"filename": "agentic-rag-with-llamaindex-2721b8a49ff6.md", "extension": ".md", "title": "Agentic RAG With LlamaIndex", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/agentic-rag-with-llamaindex-2721b8a49ff6"}, "hash": "d4df2898b749385aa7bc207b0c08c26739b27e4f24243b3be259d46d6cc6c774", "class_name": "RelatedNodeInfo"}}, "text": "It is easy to envisage how this architecture can grow and expand over an organisation with more sub bots being added. \n\n**_Follow me on_ ** [ **_LinkedIn_ **\n](https://www.linkedin.com/in/cobusgreyling/) **_for updates on Large Language\nModels_ **\n\n", "mimetype": "text/plain", "start_char_idx": 5692, "end_char_idx": 5941, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1cc00555-83ad-4f3e-a9d8-7b022044e05f": {"__data__": {"id_": "1cc00555-83ad-4f3e-a9d8-7b022044e05f", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-01-30-0d01eb0d8cef.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201330", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-30-0d01eb0d8cef"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "512d7ecd-13a0-4eec-a049-a41fbd99357e", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-01-30-0d01eb0d8cef.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201330", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-30-0d01eb0d8cef"}, "hash": "09337dab8e5fbe81f2bf3b36b02fc7cdb333d080c9425a6376336389d4e78af6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe3d7723-1c52-4fd9-8ea8-49bd23c63f75", "node_type": "1", "metadata": {}, "hash": "7c412cbf02e88ef0f9bf820f3af62cce3739bf211e41f80231383b349cb28c20", "class_name": "RelatedNodeInfo"}}, "text": "Hello LlamaIndex Adventurers ,\n\nWelcome to another thrilling week at LlamaIndex! It\u2019s brimming with community\ncontributions and a wealth of educational content that will take your\nLlamaIndex experience to new heights. Dive into our latest features,\ncomprehensive tutorials, insightful guides, and interactive demos, all\ndesigned to supercharge your journey with LlamaIndex.\n\nBut first, let\u2019s ignite your excitement with a reminder about our upcoming [\nfirst-ever in-person hackathon ](https://rag-a-thon.devpost.com/) , happening\nFebruary 2nd-4th. Don\u2019t miss this incredible chance to mingle with fellow RAG\naficionados, collaborate on exciting projects, and vie for a share of over\n$16,000 in prizes!\n\nYour creations inspire us! ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 730, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe3d7723-1c52-4fd9-8ea8-49bd23c63f75": {"__data__": {"id_": "fe3d7723-1c52-4fd9-8ea8-49bd23c63f75", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-01-30-0d01eb0d8cef.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201330", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-30-0d01eb0d8cef"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "512d7ecd-13a0-4eec-a049-a41fbd99357e", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-01-30-0d01eb0d8cef.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201330", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-30-0d01eb0d8cef"}, "hash": "09337dab8e5fbe81f2bf3b36b02fc7cdb333d080c9425a6376336389d4e78af6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1cc00555-83ad-4f3e-a9d8-7b022044e05f", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-01-30-0d01eb0d8cef.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201330", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-30-0d01eb0d8cef"}, "hash": "7335f017915cbf07c6748b7b1bdca9f79f6d93eb704f5b113eb3dec26dedf05e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc219ed7-fb2c-4e74-807e-5d875061ae96", "node_type": "1", "metadata": {}, "hash": "baf23276a879578574d25fd5fdbc4d49f466515a0678c6747be70fcf13edc9f7", "class_name": "RelatedNodeInfo"}}, "text": "Whether it\u2019s a project, article, or video that\nyou\u2019re proud of, we\u2019d love to see it. Share your brilliance with us at [\nnews@llamaindex.ai ](mailto:news@llamaindex.ai) . And for those who haven\u2019t\nyet, make sure to subscribe to our newsletter on our [ website\n](https://www.llamaindex.ai/) \u2014 it\u2019s your gateway to all the latest and\ngreatest from LlamaIndex, delivered directly to your inbox.\n\n", "mimetype": "text/plain", "start_char_idx": 730, "end_char_idx": 1122, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc219ed7-fb2c-4e74-807e-5d875061ae96": {"__data__": {"id_": "bc219ed7-fb2c-4e74-807e-5d875061ae96", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-01-30-0d01eb0d8cef.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201330", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-30-0d01eb0d8cef"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "512d7ecd-13a0-4eec-a049-a41fbd99357e", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-01-30-0d01eb0d8cef.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201330", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-30-0d01eb0d8cef"}, "hash": "09337dab8e5fbe81f2bf3b36b02fc7cdb333d080c9425a6376336389d4e78af6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe3d7723-1c52-4fd9-8ea8-49bd23c63f75", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-01-30-0d01eb0d8cef.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201330", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-30-0d01eb0d8cef"}, "hash": "0524f66790c21a67100d313e677ee8f9d8b3f9facdb55bc6dbedc9332e58788f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e178418f-8218-4de5-94f6-e0fa2c2ae364", "node_type": "1", "metadata": {}, "hash": "dd7134309aa3741ee672ae0c66591c80b1f46bfe480c6bd21b82ef4b7d1dde76", "class_name": "RelatedNodeInfo"}}, "text": "**The highlights:**\n\n  1. **RAG CLI** : Easy-to-use tool for local file indexing and search, with advanced integration and customization features. [ Docs ](https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html) , [ Tweet ](https://x.com/llama_index/status/1750950516925079777?s=20) . \n  2. **JSONalyze** : Efficiently summarizes large JSON datasets, transforming them into SQLite for detailed SQL queries. [ Docs ](https://docs.llamaindex.ai/en/latest/examples/query_engine/JSONalyze_query_engine.html) , [ Tweet ](https://x.com/llama_index/status/1749541492191039873?s=20) . \n  3. **OpenAI Embeddings** : We now support the latest OpenAI ` **text-embedding-3-small** ` **and** ` **text-embedding-3-large** ` embeddings for improved accuracy and cost-effectiveness in data retrieval. [ Docs ](https://docs.llamaindex.ai/en/latest/examples/embeddings/OpenAI.html) , [ Tweet ](https://x.com/llama_index/status/1750640685894783068?s=20) . \n  4. **ReAct Agent** [ **Guide** ](https://github.com/run-llama/llama_index/blob/main/docs/examples/agent/agent_runner/query_pipeline_agent.ipynb) : From scratch guide for building ReAct agents, covering all key aspects from setup to memory management. \n  5. **Slack Bot** : Step-by-step [ guide ](/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840) for developing a learning Slack bot, integrated with advanced data engines and deployment tools. \n\n**Feature Releases and Enhancements:**\n\n  * We have launched RAG CLI: A straightforward command-line tool for indexing and searching any local file, featuring integration with IngestionPipeline, QueryPipeline, and ChromaDB, with support for local models and customizable logic. [ Docs ](https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html) , [ Tweet ](https://x.com/llama_index/status/1750950516925079777?s=20) . \n  * We have introduced JSONalyze, a query engine that swiftly summarizes large JSON datasets. It transforms JSON data into an SQLite table, enabling precise SQL queries for efficient data analysis, combining LlamaIndex\u2019s capabilities with text-to-SQL technology. [ Docs ](https://docs.llamaindex.ai/en/latest/examples/query_engine/JSONalyze_query_engine.html) , [ Tweet ](https://x.com/llama_index/status/1749541492191039873?s=20) . \n  * We have launched day 0 support for OpenAI\u2019s latest embedding models featuring cost-effective ` **text-embedding-3-small** ` and high-performance ` **text-embedding-3-large** ` , both with customizable dimensions for enhanced retrieval accuracy in Python and TypeScript versions of LlamaIndex. [ Docs ](https://docs.llamaindex.ai/en/latest/examples/embeddings/OpenAI.html) , [ Tweet ](https://x.com/llama_index/status/1750640685894783068?s=20) . \n  * We have launched Infer-Retrieve-Rerank as a LlamaPack, a technique developed by Karel Doostrlnck, as a simple yet effective LLM-based approach for tackling complex classification challenges with numerous categories, applicable in areas like medical diagnosis and job skill assessment. [ LlamaPack ](https://llamahub.ai/l/llama_packs-research-infer_retrieve_rerank?from=all) , [ Tweet ](https://x.com/llama_index/status/1752008109835559123?s=20) . \n  * We have launched LlamaPack with Vanna AI: An advanced text-to-SQL tool using RAG for storing, indexing, and generating SQL queries. [ LlamaPack ](https://llamahub.ai/l/llama_packs-vanna?from=all) . \n  * We have integrated with Zilliz Cloud Pipeline in partnership with Zilliz Universe. This fully managed, scalable retrieval service supports multi-tenancy. [ Blog ](/building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf) , [ Tweet ](https://x.com/llama_index/status/1750621271250096558?s=20) . \n  * We have partnered with Exa which created an advanced RAG-powered web search, designed for LLMs and now integrated with Llama Index agents, enhancing workflow automation and data source combination. [ Notebook ](https://github.com/run-llama/llama-hub/blob/main/llama_hub/tools/notebooks/exa.ipynb) , [ Tweet ](https://x.com/llama_index/status/1751011851952152710?s=20) . \n  * We have integrated with Neutrino, offering GPT-4 level performance at significantly reduced costs by smartly allocating queries to the most suitable model from a diverse range. [ Docs ](https://docs.llamaindex.ai/en/stable/examples/llm/neutrino.html) , [ Twitter ](https://x.com/llama_index/status/1749504764172493161?s=20) . \n\n**Guides:**\n\n  * [ Guide ](https://github.com/run-llama/llama_index/blob/main/docs/examples/agent/agent_runner/query_pipeline_agent.ipynb) to Building a ReAct Agent from Scratch and cookbook detailing the essential components for creating your agents, including reasoning prompts, output parsing, tool selection, and memory management. \n  * [ Guide ](/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840) to Building Slack Bot: Create and deploy an intelligent Slack bot that learns from conversations and accurately answers organizational queries, featuring integration with Qdrant Engine and Render. \n\n**Tutorials:**\n\n  * [ Marco Bertelli ](https://medium.com/@marco.bertelli) [ tutorial ](https://medium.com/@marco.bertelli/empowering-your-chatbot-unveiling-dynamic-knowledge-sources-with-advanced-integration-e8353e85099c) on Empowering Your Chatbot: Unveiling Dynamic Knowledge Sources with Advanced Integration. \n  * [ Tonic Validate ](/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9) tutorial on Implementing integration tests for LlamaIndex. \n  * [ Chia Jeng Yang ](https://chiajy.medium.com/) [ tutorial ](https://medium.com/enterprise-rag/injecting-knowledge-graphs-in-different-rag-stages-a3cd1221f57b) on Injecting Knowledge Graphs in different RAG stages. \n  * [ Wenqi Glantz ](https://medium.com/@wenqiglantz) [ tutorial ](https://towardsdatascience.com/jump-start-your-rag-pipelines-with-advanced-retrieval-llamapacks-and-benchmark-with-lighthouz-ai-80a09b7c7d9d) on Jump-start Your RAG Pipelines with Advanced Retrieval LlamaPacks and Benchmark with Lighthouz AI. \n\n**Webinar**\n\n  * LlamaIndex [ Webinar ](https://www.youtube.com/watch?v=aoLtTIYAafY) on Efficient Parallel Function Calling Agents with LLMCompiler with Sehoon Kim and Amir Gholami. \n\n**Calling all enterprises:**\n\nAre you building with LlamaIndex? We are working hard to make LlamaIndex, even\nmore, Enterprise-ready and have sneak peeks at our upcoming products available\nfor partners. Interested? ", "mimetype": "text/plain", "start_char_idx": 1122, "end_char_idx": 7610, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e178418f-8218-4de5-94f6-e0fa2c2ae364": {"__data__": {"id_": "e178418f-8218-4de5-94f6-e0fa2c2ae364", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-01-30-0d01eb0d8cef.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201330", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-30-0d01eb0d8cef"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "512d7ecd-13a0-4eec-a049-a41fbd99357e", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-01-30-0d01eb0d8cef.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201330", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-30-0d01eb0d8cef"}, "hash": "09337dab8e5fbe81f2bf3b36b02fc7cdb333d080c9425a6376336389d4e78af6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc219ed7-fb2c-4e74-807e-5d875061ae96", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-01-30-0d01eb0d8cef.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201330", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-30-0d01eb0d8cef"}, "hash": "b22ec561dece328bb356fb107cb3ead77ed815152012c410957f1c230d2d66a9", "class_name": "RelatedNodeInfo"}}, "text": "[ Get in touch.\n](https://docs.google.com/forms/d/e/1FAIpQLScBNdM2a_fn8UZOKmFQt6lBsrd1o6FflvsdPH-\nPn3JkdlN_Rg/viewform)\n\n", "mimetype": "text/plain", "start_char_idx": 7610, "end_char_idx": 7731, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2883a768-6223-482c-8bda-bcf82f594110": {"__data__": {"id_": "2883a768-6223-482c-8bda-bcf82f594110", "embedding": null, "metadata": {"filename": "tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.md", "extension": ".md", "title": "Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex", "date": "Jan 26, 2024", "url": "https://www.llamaindex.ai/blog/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1660f5f-af05-40c0-aeb6-ae58303ad8bf", "node_type": "4", "metadata": {"filename": "tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.md", "extension": ".md", "title": "Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex", "date": "Jan 26, 2024", "url": "https://www.llamaindex.ai/blog/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9"}, "hash": "833bde702cdf6187927ff5bebb67291677f77dc2e3e2adf4665d7c7ccc8d6713", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7935ff8b-b1fe-4d26-9eac-b423de5155fd", "node_type": "1", "metadata": {}, "hash": "7ab91d2aee1e7c3b28ac0595d658cc635ab1aef08a856354e3424de754bc19c0", "class_name": "RelatedNodeInfo"}}, "text": "_In this technical walkthrough, we\u2019ll highlight the functionality of Tonic\nValidate and its integration with LlamaIndex. Sign up for a free account_ [\n_here_ ](https://www.tonic.ai/validate) _before you start._\n\n#  Introduction\n\nAs enterprise adoption of generative AI technologies continues, companies are\nturning to Retrieval Augmented Generation (RAG) systems to extend the\napplication of large-language models (LLMs) to their private data (e.g., a\nchatbot that can answer questions based on internal technical documentation).\nTraditionally in software engineering, companies have placed a high emphasis\non implementing continuous integration tests to ensure systems remain\nperformant when updates are made. More recently, these same principles have\nbeen applied to machine learning models in production.\n\nHowever, as a young technology, RAG currently lacks best practices for\nintegration tests to ensure breaking changes aren\u2019t introduced to the\nproduction system. In this article, we will demonstrate how you can use Tonic\nValidate\u2019s RAG performance monitoring capabilities, LlamaIndex, and GitHub\nActions to create novel integration tests that alert you to changes in RAG\nsystem performance. To make things easy, Tonic Validate is available natively\nwithin LlamaIndex\u2019s core library \u2014 you can read more about that [ here\n](https://www.tonic.ai/blog/tonic-ai-and-llamaindex-join-forces-to-help-\ndevelopers-build-more-performant-rag-systems) .\n\n#  What is Tonic Validate?\n\nTonic Validate is a RAG benchmarking and evaluation platform that monitors\nperformance of RAG systems in production. It provides comprehensive metrics\nfor measuring the performance of each component in your RAG system,\nvisualizations for comparing performance across time as the system changes,\nand workflows for creating benchmark question-answer sets and reviewing LLM\nresponses. Tonic Validate shines a light on how your RAG system is truly\nperforming, enabling continuous performance monitoring of your production RAG\nsystems. You can [ learn more ](https://www.tonic.ai/validate) and [ sign up\nfor a free account ](https://validate.tonic.ai/) .\n\n#  Setting up LlamaIndex\n\nTo get started, let\u2019s create an example RAG system for us to test. In this\ncase, LlamaIndex provides a tool called ` create-llama ` which can generate a\nfull-stack RAG application for us. To install it, we need to make sure we have\nNode.JS installed and run the following command:\n\n    \n    \n    npx create-llama@latest\n\nThis command will take you through a series of prompts. Here are the options\nto select for each prompt:\n\n    \n    \n    What is your project named? \u00bb llama-validate-demo\n    Which template would you like to use? \u00bb Chat without streaming\n    Which framework would you like to use? \u00bb FastAPI (Python)\n    Would you like to install dependencies automatically? \u00bb No\n    Which model would you like to use? \u00bb gpt-4\u20131106-preview\n    Which data source would you like to use? \u00bb Use an example PDF\n    Would you like to use a vector database? \u00bb No, just store the data in the file system\n\nOnce these options are selected, your project should be created in a folder\ncalled ` llama-validate-demo ` . For this demo, we are going to replace the\nexample PDF ` create-llama ` provides with our own larger dataset. The dataset\nconsists of a collection of essays from Paul Graham\u2019s blog. This should more\nclosely replicate a real world scenario where a company runs RAG on a larger\ninternal dataset. To add the essays, download them from [ our Github\n](https://github.com/TonicAI/llama-validate-demo/blob/main/data.zip) and unzip\nthem inside the root folder of your created project. Make sure the unzipped\nfolder is named ` data ` . Be sure to delete any existing files in the folder\nbefore copying the new dataset.\n\nAfter you have the essays in the right directory, you can set up your OpenAI\nAPI key by setting it as an environment variable called ` OPENAI_API_KEY ` .\nYou can do this either via setting the environment variable system wide or by\ncreating a ` .env ` file in the root folder of your ` create-llama ` project.\nThen you can run the following commands in the root folder for your ` create-\nllama ` project:\n\n    \n    \n    poetry install\n    poetry shell\n    python app/engine/generate.py\n\nThis will install the dependencies and generate the RAG embeddings for the\nPaul Graham essays. After this, you can run the chatbot with:\n\n    \n    \n    python main.py\n\nTo test the chatbot, you can send a request via curl:\n\n    \n    \n    curl - location 'localhost:8000/api/chat' \\\n     - header 'Content-Type: application/json' \\\n     - data '{ \"messages\": [{ \"role\": \"user\", \"content\": \"In the early days, how were the Airbnb founders financing their startup?\" }] }'\n\nLlamaIndex will then return a response:\n\n    \n    \n    {\n        \"result\": {\n            \"role\": \"assistant\",\n            \"content\": \"In the early days, the Airbnb founders financed their startup by creating and selling themed breakfast cereals. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4975, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7935ff8b-b1fe-4d26-9eac-b423de5155fd": {"__data__": {"id_": "7935ff8b-b1fe-4d26-9eac-b423de5155fd", "embedding": null, "metadata": {"filename": "tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.md", "extension": ".md", "title": "Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex", "date": "Jan 26, 2024", "url": "https://www.llamaindex.ai/blog/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1660f5f-af05-40c0-aeb6-ae58303ad8bf", "node_type": "4", "metadata": {"filename": "tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.md", "extension": ".md", "title": "Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex", "date": "Jan 26, 2024", "url": "https://www.llamaindex.ai/blog/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9"}, "hash": "833bde702cdf6187927ff5bebb67291677f77dc2e3e2adf4665d7c7ccc8d6713", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2883a768-6223-482c-8bda-bcf82f594110", "node_type": "1", "metadata": {"filename": "tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.md", "extension": ".md", "title": "Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex", "date": "Jan 26, 2024", "url": "https://www.llamaindex.ai/blog/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9"}, "hash": "925e5e833a4d43ad47e476a08642828277d5e10743b92bf8f1e35b23ed59adcc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9521efa1-e6f9-47ea-bec2-bd9caea748e4", "node_type": "1", "metadata": {}, "hash": "2e32377767403af6be0f3fa2034bfc4670f73bd852c235efe7fbadb652b41fa5", "class_name": "RelatedNodeInfo"}}, "text": "They created limited-edition cereal boxes, such as \\\"Obama O's\\\" and \\\"Cap'n McCain's,\\\" during the 2008 U.S. presidential election, which became a collectible and helped them raise funds for their company. ", "mimetype": "text/plain", "start_char_idx": 4975, "end_char_idx": 5182, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9521efa1-e6f9-47ea-bec2-bd9caea748e4": {"__data__": {"id_": "9521efa1-e6f9-47ea-bec2-bd9caea748e4", "embedding": null, "metadata": {"filename": "tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.md", "extension": ".md", "title": "Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex", "date": "Jan 26, 2024", "url": "https://www.llamaindex.ai/blog/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1660f5f-af05-40c0-aeb6-ae58303ad8bf", "node_type": "4", "metadata": {"filename": "tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.md", "extension": ".md", "title": "Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex", "date": "Jan 26, 2024", "url": "https://www.llamaindex.ai/blog/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9"}, "hash": "833bde702cdf6187927ff5bebb67291677f77dc2e3e2adf4665d7c7ccc8d6713", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7935ff8b-b1fe-4d26-9eac-b423de5155fd", "node_type": "1", "metadata": {"filename": "tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.md", "extension": ".md", "title": "Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex", "date": "Jan 26, 2024", "url": "https://www.llamaindex.ai/blog/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9"}, "hash": "087f1be63199be39d8a59dfb211a92e5c415508425d276d9c1bc9f2044147c92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "281043d1-aed9-4ac1-8b37-a032571ecf6f", "node_type": "1", "metadata": {}, "hash": "dc4eeaa55056bb9c7b073c84c516f30d54527b00ae8f0c920cb1859a7707174c", "class_name": "RelatedNodeInfo"}}, "text": "This creative approach to funding allowed them to sustain the business in its initial phase before securing more traditional forms of investment.\"\n        }\n    }\n\nFinally, in ` llama-validate-demo/app/api/routers/chat.py ` we want to replace\nthe ` return _Result ` line at the end of the chat function with the\nfollowing.\n\n    \n    \n    return _Result(\n        result=_Message(\n            role=MessageRole.ASSISTANT,\n            content=response.response,\n            context=[x.text for x in response.source_nodes]\n        )\n    )\n\nThis allows the LlamaIndex API to return the RAG context that was used to\nanswer the question asked. Now, we can move on to setting up Tonic Validate!\n\n", "mimetype": "text/plain", "start_char_idx": 5182, "end_char_idx": 5869, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "281043d1-aed9-4ac1-8b37-a032571ecf6f": {"__data__": {"id_": "281043d1-aed9-4ac1-8b37-a032571ecf6f", "embedding": null, "metadata": {"filename": "tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.md", "extension": ".md", "title": "Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex", "date": "Jan 26, 2024", "url": "https://www.llamaindex.ai/blog/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1660f5f-af05-40c0-aeb6-ae58303ad8bf", "node_type": "4", "metadata": {"filename": "tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.md", "extension": ".md", "title": "Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex", "date": "Jan 26, 2024", "url": "https://www.llamaindex.ai/blog/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9"}, "hash": "833bde702cdf6187927ff5bebb67291677f77dc2e3e2adf4665d7c7ccc8d6713", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9521efa1-e6f9-47ea-bec2-bd9caea748e4", "node_type": "1", "metadata": {"filename": "tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.md", "extension": ".md", "title": "Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex", "date": "Jan 26, 2024", "url": "https://www.llamaindex.ai/blog/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9"}, "hash": "ff235426da4d278ec26eb175855f0090ebbe1cecfe12b96760f136b2e68608a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22bd3cc4-87fd-4c88-ac9e-6fe2c6de8670", "node_type": "1", "metadata": {}, "hash": "ca6c1e0f4af373166681060163463bd99cd41b7cad90643c5d6a7e8f489935b5", "class_name": "RelatedNodeInfo"}}, "text": "#  Setting up Tonic Validate\n\nTo set up Tonic Validate, first install it via poetry:\n\n    \n    \n    poetry add tonic-validate\n\nNow, we can create our tests for Tonic Validate. To get started, create a file\ninside ` llama-validate-demo/tests ` called ` validate_test.py ` . We will\nalso need to create a list of test questions and answers which you can find [\nhere ](https://github.com/TonicAI/llama-validate-\ndemo/blob/main/tests/qa_pairs.json) . Alternatively, you can also use the\nTonic Validate UI to create the test set and call it via the SDK \u2014 we\u2019ll be\nadding a feature to help generate these benchmarks using synthetic data to\nmake this process even easier. Download the ` qa_pairs.json ` file from the\nlink and paste it into ` llama-validate-demo/tests ` . Once we have both of\nthese files, we can add the following code into ` validate_test.py ` .\n\n    \n    \n    import json\n    import os\n    from tonic_validate import ValidateApi\n    from tonic_validate.metrics import AnswerSimilarityMetric, RetrievalPrecisionMetric, AugmentationPrecisionMetric, AnswerConsistencyMetric\n    from llama_index.evaluation import TonicValidateEvaluator\n    import requests\n    \n    from dotenv import load_dotenv\n    \n    load_dotenv()\n    \n    def get_llm_response(prompt):\n        url = \"http://localhost:8000/api/chat\"\n    \n        payload = json.dumps({\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": prompt\n                }\n            ]\n        })\n        headers = { 'Content-Type': 'application/json' }\n        response = requests.request(\"POST\", url, headers=headers, data=payload).json()\n        result = response['result']\n        return result['content'], result['context']\n\nThis code sets up the dependency imports and also specifies a `\nget_llm_response ` function which sends a request to the LlamaIndex API server\nwe set up earlier to get a response. Now, let\u2019s create a function that gets\nthe list of questions to ask LlamaIndex for our testing.\n\n    \n    \n    def get_q_and_a():\n        # Load qa_pairs.json\n        qa_pairs = json.load(open('./tests/qa_pairs.json'))\n        return ([x['question'] for x in qa_pairs], [x['answer'] for x in qa_pairs])\n\nThis function gets the question-answer pairs from our json file. The questions\nare what we will ask the RAG system and the answers are the correct answers\nfor those questions. For instance, if the question was \u201cWhat is the capital of\nFrance?\u201d then the answer would be \u201cParis\u201d.\n\nNext, we can add the code that queries LlamaIndex:\n\n    \n    \n    def get_responses(questions):\n        llm_answers = []\n        context_lists = []\n        for item in questions:\n            llm_answer, llm_context_list = get_llm_response(item)\n            llm_answers.append(llm_answer)\n            context_lists.append(llm_context_list)\n        return (llm_answers, context_lists)\n\nThis code iterates over the questions, queries LlamaIndex, and logs each\nresponse into an array. We have two arrays. One is the actual answer from\nLlamaIndex. The other is a list of the snippets of text (called the context\nlist) that LlamaIndex provided to help the LLM answer the question.\n\nNow we have a list of LLM responses generated from a list of test questions.\nLet\u2019s score them:\n\n    \n    \n    def score_run(questions, context_lists, reference_answers, llm_answers):\n        metrics = [\n            AnswerSimilarityMetric(),\n            RetrievalPrecisionMetric(),\n            AugmentationPrecisionMetric(),\n            AnswerConsistencyMetric()\n        ]\n        scorer = TonicValidateEvaluator(metrics, model_evaluator=\"gpt-4-1106-preview\")\n        run = scorer.evaluate_run(\n            questions, llm_answers, context_lists, reference_answers\n        )\n        return run, metrics\n\nWe first need to define the metrics in Tonic Validate that we want to use. You\ncan find a list of available metrics and their definitions [ here\n](https://github.com/TonicAI/tonic_validate?tab=readme-ov-file#tonic-validate-\nmetrics) . After we create the metrics, we can take advantage of Tonic\nValidate\u2019s integration with LlamaIndex. Since Tonic Validate is built into\nLlamaIndex\u2019s evaluation framework as an evaluator, all we need to do is create\na ` TonicValidateEvaluator ` , which scores the LlamaIndex responses across\nthe chosen metrics. Then we return the results along with the metrics.\n\nFinally, we can create our test function for pytest which evaluates\nLlamaIndex.\n\n    \n    \n    def test_llama_index():\n        questions, reference_answers = get_q_and_a()\n        llm_answers, context_lists = get_responses(questions)\n        run, metrics = score_run(questions, context_lists, reference_answers, llm_answers)\n        # Upload results to web ui\n        validate_api = ValidateApi()\n        # Get project id from env\n        project_id = os.getenv(\"PROJECT_ID\")\n        validate_api.upload_run(project_id, run)\n\nThis runs all the code we\u2019ve written to get the scores and then sends them to\nTonic Validate\u2019s API to visualize in the UI. In order to send the metrics for\neach run to the UI, you need to sign up for a free account, which you can do [\nhere ](https://validate.tonic.ai/) . I highly recommend utilizing the UI to\nmake visualizing and monitoring performance changes a breeze. Once you sign\nup, you will be taken through a short onboarding process where you create an\nAPI key and a project. The API key should be stored in an environment variable\ncalled ` TONIC_VALIDATE_API_KEY ` and the project ID in an environment\nvariable called ` PROJECT_ID ` .\n\nOnce you have set up your account and configured your environment variables,\nyou can run the test via the following commands:\n\n    \n    \n    poetry shell\n    pytest\n\nYou can also make the test fail if the metrics score too low. This would be a\npertinent step to add in if you want to avoid introducing breaking changes to\na production RAG system; for example, if you update the model version and the\nanswer similarity score suddenly drop below a certain threshold, you could\nhave the test fail and issue a warning to debug the issue.\n\n    \n    \n    # Check none of the metrics scored too low    \n    for metric in metrics:\n        if metric.name == AnswerSimilarityMetric.name:\n            assert run.overall_scores[metric.name] &gt;= 3.5\n        else:\n            assert run.overall_scores[metric.name] &gt;= 0.7\n\n#  Setting up GitHub Actions\n\nWith LlamaIndex and Tonic Validate configured, we have the ability to connect\ndata to an LLM and measure the accuracy of LLM responses. You can push this\nsetup into production and have a functional chatbot. ", "mimetype": "text/plain", "start_char_idx": 5869, "end_char_idx": 12476, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22bd3cc4-87fd-4c88-ac9e-6fe2c6de8670": {"__data__": {"id_": "22bd3cc4-87fd-4c88-ac9e-6fe2c6de8670", "embedding": null, "metadata": {"filename": "tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.md", "extension": ".md", "title": "Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex", "date": "Jan 26, 2024", "url": "https://www.llamaindex.ai/blog/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1660f5f-af05-40c0-aeb6-ae58303ad8bf", "node_type": "4", "metadata": {"filename": "tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.md", "extension": ".md", "title": "Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex", "date": "Jan 26, 2024", "url": "https://www.llamaindex.ai/blog/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9"}, "hash": "833bde702cdf6187927ff5bebb67291677f77dc2e3e2adf4665d7c7ccc8d6713", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "281043d1-aed9-4ac1-8b37-a032571ecf6f", "node_type": "1", "metadata": {"filename": "tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.md", "extension": ".md", "title": "Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex", "date": "Jan 26, 2024", "url": "https://www.llamaindex.ai/blog/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9"}, "hash": "6f7fe12b7a018ee0e0f6bd5f347a8bcd03d172b775cb183a31f6cb302a5285b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0a1f9d6-3dc6-4eb4-a6a3-3d9631fdd279", "node_type": "1", "metadata": {}, "hash": "7d8076e68e822de5ba811551bafb51f9a82f141f412c1e14b2ae81d0fce4ef6e", "class_name": "RelatedNodeInfo"}}, "text": "As is common in modern\nsoftware development practices, you will likely continue to fix bugs, make\nimprovements, and add new data or features to your RAG system. Before pushing\nto production, QA testing is in place to catch any changes to your code that\nmay introduce unintended effects. For example, adding a new dataset or\nupdating an LLM to a new version could lead to changes in the quality of\nresponses. One approach, the one that we recommend, for adding QA testing for\nyour RAG system is to use GitHub Actions to establish an integration test\nusing Tonic Validate that checks the LLM response quality of your RAG system,\nallowing you to catch and rectify any performance degradation before it is\npushed into production.\n\nTo set up Tonic Validate to run in GitHub Actions, we can create a folder `\nllama-validate-demo/.github/workflows ` with a file called ` python-app.yml `\n. In this file, we will include the following code configuration that defines\nthe integration test workflow:\n\n    \n    \n    # This workflow will install Python dependencies and run tests with LlamaIndex\n    \n    name: Python application\n    \n    on:\n      push:\n        branches: [ \"main\" ]\n      pull_request:\n        branches: [ \"main\" ]\n    \n    permissions:\n      contents: read\n    \n    jobs:\n      build:\n    \n        runs-on: ubuntu-latest\n        environment: Actions\n    \n        steps:\n        - uses: actions/checkout@v3\n        - name: Set up Python 3.11\n          uses: actions/setup-python@v3\n          with:\n            python-version: \"3.11\"\n        - name: Install dependencies\n          run: |\n            python -m pip install --upgrade pip\n            pip install poetry\n            poetry config virtualenvs.create false\n            poetry install --no-root --no-dev --no-directory\n        - name: Set PYTHONPATH\n          run: echo \"PYTHONPATH=$GITHUB_WORKSPACE\" &gt;&gt; $GITHUB_ENV\n        - name: Set up vector index\n          env:\n            OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n          run: |\n            python app/engine/generate.py\n        - name: Start up test server\n          env:\n            OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n            MODEL: gpt-4-1106-preview\n          run: |\n            python main.py &amp;\n            sleep 10\n        - name: Test with pytest\n          env:\n            OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n            TONIC_VALIDATE_API_KEY: ${{ secrets.TONIC_VALIDATE_API_KEY }}\n            PROJECT_ID: ${{ secrets.PROJECT_ID }}\n          run: |\n            pytest\n\nThis configures GitHub to run the tests defined with Tonic Validate upon every\ncommit. The GitHub Actions configuration downloads the repo, sets up the\ndependencies, generates the embeddings, and then starts up the test server and\nruns the test.\n\nAfter this file is set up, we just need to set our secrets in GitHub. In\nGitHub, go to ` Settings > Secrets and variables > Actions ` for your repo and\ncreate a secret called ` OPENAI_API_KEY ` , ` TONIC_VALIDATE_API_KEY ` , and `\nPROJECT_ID ` . These values will all be the same as the values you set\nearlier. Now your GitHub actions set up is complete and you can proactively\nmonitor changes to your RAG system during development and before going into\nproduction.\n\nTry pushing some commits to it and watch it run! To view the results, go to [\nTonic Validate\u2019s web app ](https://validate.tonic.ai/) and navigate to your\nproject. You should see a view like this that shows recent metrics and their\nevolution over time:\n\nNow you and your team can track your RAG system\u2019s performance over time to\nmake sure there aren\u2019t any dips in performance! Thank you for reading and make\nsure to check out Tonic Validate!\n\n", "mimetype": "text/plain", "start_char_idx": 12476, "end_char_idx": 16173, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0a1f9d6-3dc6-4eb4-a6a3-3d9631fdd279": {"__data__": {"id_": "d0a1f9d6-3dc6-4eb4-a6a3-3d9631fdd279", "embedding": null, "metadata": {"filename": "tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.md", "extension": ".md", "title": "Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex", "date": "Jan 26, 2024", "url": "https://www.llamaindex.ai/blog/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1660f5f-af05-40c0-aeb6-ae58303ad8bf", "node_type": "4", "metadata": {"filename": "tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.md", "extension": ".md", "title": "Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex", "date": "Jan 26, 2024", "url": "https://www.llamaindex.ai/blog/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9"}, "hash": "833bde702cdf6187927ff5bebb67291677f77dc2e3e2adf4665d7c7ccc8d6713", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22bd3cc4-87fd-4c88-ac9e-6fe2c6de8670", "node_type": "1", "metadata": {"filename": "tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.md", "extension": ".md", "title": "Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex", "date": "Jan 26, 2024", "url": "https://www.llamaindex.ai/blog/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9"}, "hash": "f25d6fa34daf6bc8c9baa5269981e7b4e02459c0e8e8b05e39c53e92d91f6948", "class_name": "RelatedNodeInfo"}}, "text": "_For more information on Tonic Validate, visit our_ [ _website_\n](https://www.tonic.ai/validate) _and sign up for a_ [ _free account_\n](https://validate.tonic.ai/signup) _today. You can also visit our GitHub_ [\n_page_ ](https://github.com/TonicAI/llama-validate-demo) _to view all of the\ncode used in this post and the rest of our SDK. Our LlamaIndex integration is\navailable_ [ _here_ ](https://github.com/run-\nllama/llama_index/blob/main/docs/examples/evaluation/TonicValidateEvaluators.ipynb)\n_._\n\n", "mimetype": "text/plain", "start_char_idx": 16173, "end_char_idx": 16674, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "acdafbb2-b8bd-4cb6-8651-cc92a56ee9f3": {"__data__": {"id_": "acdafbb2-b8bd-4cb6-8651-cc92a56ee9f3", "embedding": null, "metadata": {"filename": "introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41.md", "extension": ".md", "title": "Introducing the LlamaIndex retrieval-augmented generation command-line tool", "date": "Jan 26, 2024", "url": "https://www.llamaindex.ai/blog/introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68c2afb4-da61-42bd-aab7-4000cc2aa8be", "node_type": "4", "metadata": {"filename": "introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41.md", "extension": ".md", "title": "Introducing the LlamaIndex retrieval-augmented generation command-line tool", "date": "Jan 26, 2024", "url": "https://www.llamaindex.ai/blog/introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41"}, "hash": "14fa8586240045f59dfee822bc348c0750efb37d4965a8689141989f95025394", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b27bbcbd-6512-412d-98ee-ac97197315cc", "node_type": "1", "metadata": {}, "hash": "e4647031df317cdbc65cc1a0b18ffd07f3060457b54761402d0711589c08b53a", "class_name": "RelatedNodeInfo"}}, "text": "Want to try out retrieval-augmented generation (RAG) without writing a line of\ncode? We got you covered! Introducing the new ` llamaindex-cli ` tool,\ninstalled when you ` pip install llama-index ` ! It uses [ Chroma\n](https://docs.llamaindex.ai/en/stable/examples/vector_stores/ChromaIndexDemo.html)\nunder the hood, so you\u2019ll need to ` pip install chromadb ` as well.\n\n#  How to use it\n\n  1. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 392, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b27bbcbd-6512-412d-98ee-ac97197315cc": {"__data__": {"id_": "b27bbcbd-6512-412d-98ee-ac97197315cc", "embedding": null, "metadata": {"filename": "introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41.md", "extension": ".md", "title": "Introducing the LlamaIndex retrieval-augmented generation command-line tool", "date": "Jan 26, 2024", "url": "https://www.llamaindex.ai/blog/introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68c2afb4-da61-42bd-aab7-4000cc2aa8be", "node_type": "4", "metadata": {"filename": "introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41.md", "extension": ".md", "title": "Introducing the LlamaIndex retrieval-augmented generation command-line tool", "date": "Jan 26, 2024", "url": "https://www.llamaindex.ai/blog/introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41"}, "hash": "14fa8586240045f59dfee822bc348c0750efb37d4965a8689141989f95025394", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "acdafbb2-b8bd-4cb6-8651-cc92a56ee9f3", "node_type": "1", "metadata": {"filename": "introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41.md", "extension": ".md", "title": "Introducing the LlamaIndex retrieval-augmented generation command-line tool", "date": "Jan 26, 2024", "url": "https://www.llamaindex.ai/blog/introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41"}, "hash": "8d4188b9dc5e8e729304d0cd50de9587fb6416435fe6d56a212c4158a03c533e", "class_name": "RelatedNodeInfo"}}, "text": "**Set the** ` **OPENAI_API_KEY** ` **environment variable:** By default, this tool uses OpenAI\u2019s API. As such, you\u2019ll need to ensure the OpenAI API Key is set under the ` OPENAI_API_KEY ` environment variable whenever you use the tool. \n\n    \n    \n    $ export OPENAI_API_KEY=&lt;api_key&gt;\n\n**2\\. Ingest some files:** Now, you need to point the tool at some local files\nthat it can ingest into the local vector database. For this example, we\u2019ll\ningest the LlamaIndex ` README.md ` file:\n\n    \n    \n    $ llamaindex-cli rag --files \"./README.md\"\n\nYou can only specify a file glob pattern such as\n\n    \n    \n    $ llamaindex-cli rag --files \"./docs/**/*.rst\"\n\n**3\\. Ask a Question** : You can now start asking questions about any of the\ndocuments you\u2019d ingested in the prior step:\n\n    \n    \n    $ llamaindex-cli rag --question \"What is LlamaIndex?\" \n    LlamaIndex is a data framework that helps in ingesting, structuring, and accessing private or domain-specific data for LLM-based applications. It provides tools such as data connectors to ingest data from various sources, data indexes to structure the data, and engines for natural language access to the data. LlamaIndex follows a Retrieval-Augmented Generation (RAG) approach, where it retrieves information from data sources, adds it to the question as context, and then asks the LLM to generate an answer based on the enriched prompt. This approach overcomes the limitations of fine-tuning LLMs and provides a more cost-effective, up-to-date, and trustworthy solution for data augmentation. LlamaIndex is designed for both beginner and advanced users, with a high-level API for easy usage and lower-level APIs for customization and extension.\n\n**4\\. Open a Chat REPL** : You can even open a chat interface within your\nterminal! Just run ` llamaindex-cli rag --chat ` and start asking questions\nabout the files you\u2019ve ingested.\n\n#  Customize it to your heart\u2019s content!\n\nYou can customize ` llamaindex-cli ` to use any LLM model, even local models\nlike Mixtral 8x7b through [ Ollama\n](https://docs.llamaindex.ai/en/stable/examples/llm/ollama.html) , and you can\nbuild more advanced query and retrieval techniques. [ Check the documentation\n](https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html) for\ndetails on how to get started.\n\n", "mimetype": "text/plain", "start_char_idx": 392, "end_char_idx": 2697, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc47a609-ed74-452a-a5ed-fe1c0e804eef": {"__data__": {"id_": "dc47a609-ed74-452a-a5ed-fe1c0e804eef", "embedding": null, "metadata": {"filename": "building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf.md", "extension": ".md", "title": "Building Scalable RAG Applications with LlamaIndex and Zilliz Cloud Pipelines", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0e19263-36ae-4334-93ff-95f882469e66", "node_type": "4", "metadata": {"filename": "building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf.md", "extension": ".md", "title": "Building Scalable RAG Applications with LlamaIndex and Zilliz Cloud Pipelines", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf"}, "hash": "7a2fbbd8b49be60e4f4d69b2b010168ddc957b06828b11a02ac22adfbfd31e27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00693a2e-e6d3-4649-acc1-0e04ae6c575b", "node_type": "1", "metadata": {}, "hash": "66992ac124f63507fe75eca41889831af5a4833e1a678ea2107b85a500517316", "class_name": "RelatedNodeInfo"}}, "text": "#  Introduction\n\nWe are seeing a huge wave of developers building Retrieval Augmented\nGeneration (RAG) applications. The RAG tech stack generally contains a\nretrieval pipeline, LLM and prompt, among which LLM is accessible and\ndevelopers are comfortable with prompt customization. However, developers new\nto search and index often need extensive help to build an effective\n**retrieval** pipeline. A production-ready retrieval pipeline typically\nconsists of the following components:\n\n  * Document loader that parses and splits the long text \n  * Embedding model serving as core indexing component \n  * A vector database that stores the vector embeddings \n  * Advanced components to future optimize retrieval quality, such as re-ranker model to judge semantic similarity better \n\nIt\u2019s challenging to operate this complex tech stack. It involves managing\nsoftware package dependencies, hosting services in Kubernetes clusters, and\nmonitoring the performance of ML models. The high DevOps cost distracts\ndevelopers from the most critical part of the user experience of RAG\napplications: prompt engineering, answer generation, and user interface.\n\nWhile experienced search infrastructure engineers may still manage a\ncomplicated tech stack for its flexibility, Zilliz believes that most RAG\ndevelopers could benefit from a retrieval API service that is user-friendly\nand allows for lighter customization.\n\nIntegrating **Zilliz Cloud Pipelines** and **LlamaIndex** brings a new\napproach to solving this problem. Zilliz Cloud Pipelines is a fully managed,\nscalable retrieval service. LlamaIndex is a flexible RAG framework that\nprovides libraries and tools for organizing business logics such as retrieval\nand prompt engineering. The API service of Zilliz Cloud Pipelines is\nabstracted as a ManagedIndex in LlamaIndex. RAG developers using\n**_ZillizCloudPipelineIndex_ ** can easily scale the app from one user to\nmillions of users without the hassle of setting up and maintaining the complex\nretrieval tech stack. It hides the technical complexity behind a few function\ncalls, so that developers can focus on the core user experience of their RAG\napps.\n\nIn this blog, we show how to use _ZillizCloudPipelineIndex_ to build a high\nquality RAG chatbot. The chatbot is scalable and supports multi-tenancy\nthrough metadata filtering.\n\n#  Set up\n\nSince Zilliz Cloud Pipelines is an API service, first you need to set up a [\nZilliz Cloud\n](https://cloud.zilliz.com/signup?utm_source=referral&utm_medium=partner&utm_campaign=2024-01-18_blog_zcp-\nllamaindex_llamaindex&utm_content=llamaindex) account and create a free\nserverless cluster.\n\nNow you can construct **_ZillizCloudPipelineIndex_ ** and get the handler to\nindex docs and query later.\n\n    \n    \n    from llama_index.indices import ZillizCloudPipelineIndex\n    \n    zcp_index = ZillizCloudPipelineIndex(\n        project_id=\"&lt;YOUR_ZILLIZ_PROJECT_ID&gt;\",\n        cluster_id=\"&lt;YOUR_ZILLIZ_CLUSTER_ID&gt;\",\n        token=\"&lt;YOUR_ZILLIZ_API_KEY&gt;\",\n    )\n    zcp_index.create_pipelines(metadata_schema={\"user_id\": \"VarChar\", \"version\": \"VarChar\"})\n\nYou can copy the Project ID, Cluster ID and API Key from your Zilliz account\nas follows:\n\n#  Ingest Documents\n\nSuppose your application has multiple users, and you would like to tag each\nuser\u2019s document to provide isolation. Your application logic can be\nimplemented as follows. For simplicity, here we demo ingesting public\ndocuments. Currently, Zilliz Cloud Pipelines [ supports\n](https://docs.zilliz.com/docs/run-pipelines#run-an-ingestion-\npipeline?utm_source=referral&utm_medium=partner&utm_campaign=2024-01-18_blog_zcp-\nllamaindex_llamaindex) documents stored and managed in AWS S3 and Google Cloud\nStorage. Local document upload will also be supported soon.\n\n    \n    \n    # user1 ingests a document, it is technical documentation for v2.3 version. \n    zcp_index.insert_doc_url(\n        url=\"https://publicdataset.zillizcloud.com/milvus_doc.md\",\n        metadata={\"user_id\": \"user1\", \"version\": \"2.3\"},\n    )\n    # user2 ingests a document, it is technical documentation for v2.2 version. \n    zcp_index.insert_doc_url(\n        url=\"https://publicdataset.zillizcloud.com/milvus_doc_22.md\",\n        metadata={\"user_id\": \"user2\", \"version\": \"2.2\"},\n    )\n\n#  Query\n\nTo conduct semantic search with _ZillizCloudPipelineIndex_ , you can use it\n_as_query_engine()_ by specifying a few parameters:\n\n  * search_top_k: How many text nodes/chunks to retrieve. Optional, defaults to DEFAULT_SIMILARITY_TOP_K (2). \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4516, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00693a2e-e6d3-4649-acc1-0e04ae6c575b": {"__data__": {"id_": "00693a2e-e6d3-4649-acc1-0e04ae6c575b", "embedding": null, "metadata": {"filename": "building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf.md", "extension": ".md", "title": "Building Scalable RAG Applications with LlamaIndex and Zilliz Cloud Pipelines", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0e19263-36ae-4334-93ff-95f882469e66", "node_type": "4", "metadata": {"filename": "building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf.md", "extension": ".md", "title": "Building Scalable RAG Applications with LlamaIndex and Zilliz Cloud Pipelines", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf"}, "hash": "7a2fbbd8b49be60e4f4d69b2b010168ddc957b06828b11a02ac22adfbfd31e27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc47a609-ed74-452a-a5ed-fe1c0e804eef", "node_type": "1", "metadata": {"filename": "building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf.md", "extension": ".md", "title": "Building Scalable RAG Applications with LlamaIndex and Zilliz Cloud Pipelines", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf"}, "hash": "3e03d23f560ae29b3dc304b575ab05bf31456a170314a1e4dd44d0927a80cc3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97f4c66a-1315-459f-9ae9-0eb0ffbf9e76", "node_type": "1", "metadata": {}, "hash": "5e04c7b92d92df601d2b2700ef91ec52b434c2e479736394d89759186d67341c", "class_name": "RelatedNodeInfo"}}, "text": "* filters: Metadata filters. ", "mimetype": "text/plain", "start_char_idx": 4516, "end_char_idx": 4545, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97f4c66a-1315-459f-9ae9-0eb0ffbf9e76": {"__data__": {"id_": "97f4c66a-1315-459f-9ae9-0eb0ffbf9e76", "embedding": null, "metadata": {"filename": "building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf.md", "extension": ".md", "title": "Building Scalable RAG Applications with LlamaIndex and Zilliz Cloud Pipelines", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0e19263-36ae-4334-93ff-95f882469e66", "node_type": "4", "metadata": {"filename": "building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf.md", "extension": ".md", "title": "Building Scalable RAG Applications with LlamaIndex and Zilliz Cloud Pipelines", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf"}, "hash": "7a2fbbd8b49be60e4f4d69b2b010168ddc957b06828b11a02ac22adfbfd31e27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00693a2e-e6d3-4649-acc1-0e04ae6c575b", "node_type": "1", "metadata": {"filename": "building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf.md", "extension": ".md", "title": "Building Scalable RAG Applications with LlamaIndex and Zilliz Cloud Pipelines", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf"}, "hash": "a6bcdd0a45cb16a8193f77617e25775e475be5e48c67acba7d3cb7f601ffba83", "class_name": "RelatedNodeInfo"}}, "text": "Optional, defaults to None. In this example, we set the filter to only retrieve docs of a specific user, to provide user-level data isolation. \n  * output_metadata: What metadata fields to return with the retrieved text node. Optional, defaults to []. \n\n    \n    \n    # Query the documents in ZillizCloudPipelineIndex\n    from llama_index.vector_stores.types import ExactMatchFilter, MetadataFilters\n    \n    query_engine_for_user1 = zcp_index.as_query_engine(\n        search_top_k=3,\n        filters=MetadataFilters(\n            filters=[\n                ExactMatchFilter(key=\"user_id\", value=\"user1\")\n            ]  # The query would only search from documents of user1.\n        ),\n        output_metadata=[\"user_id\", \"version\"], # output these tags together with document text\n    )\n    \n    question = \"Can users delete entities by complex boolean expressions?\"\n    # The chatbot will only answer with the retrieved information from user1's documents\n    answer = query_engine_for_user1.query(question)\n\nThanks to the abstraction of LlamaIndex and Zilliz Cloud Pipelines, with just\n30 lines of code, we can demo a RAG service that supports multi-tenancy. Most\nimportantly, this simple RAG app could easily scale to serving millions of\nusers without changing any code.\n\n#  What to do next?\n\nYou can check out the official [ LlamaIndex documentation\n](https://docs.llamaindex.ai/en/stable/examples/managed/zcpDemo.html#managed-\nindex-with-zilliz-cloud-pipelines) to learn about advanced customization of\n_ZillizCloudPipelineIndex_ . Please ask questions at [ Zilliz user group\n](https://discord.gg/8uyFbECzPX) or [ LlamaIndex discord\n](https://discord.com/invite/eN6D2HQ4aX) if you have any questions. Zilliz\nCloud Pipelines will soon support local file upload and more choices of\nembedding and re-ranker models. Get a free [ Zilliz Cloud\n](https://cloud.zilliz.com/signup?utm_source=referral&utm_medium=partner&utm_campaign=2024-01-18_blog_zcp-\nllamaindex_llamaindex&utm_content=llamaindex) account and stay tuned for more\nupdates!\n\n", "mimetype": "text/plain", "start_char_idx": 4545, "end_char_idx": 6581, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22912f4c-8db2-4d26-a771-96e895fdf90f": {"__data__": {"id_": "22912f4c-8db2-4d26-a771-96e895fdf90f", "embedding": null, "metadata": {"filename": "building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.md", "extension": ".md", "title": "Building a Slack bot that learns with LlamaIndex, Qdrant and Render", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0f766867-73f3-4e06-bd1e-3b879e6bd02a", "node_type": "4", "metadata": {"filename": "building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.md", "extension": ".md", "title": "Building a Slack bot that learns with LlamaIndex, Qdrant and Render", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840"}, "hash": "bc23faa80294e842959fd6eba4822a0e3594aa3a7660dd15a40f9cee9af7da80", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "abbe847f-9f06-47ed-9148-aa6eebe08901", "node_type": "1", "metadata": {}, "hash": "12034156fb7b55fad9f98be33dd42bf750f9ba7d6447fbed92ff7293f8173dd0", "class_name": "RelatedNodeInfo"}}, "text": "In this post we\u2019re going to walk you through the process of building and\ndeploying a Slackbot that listens to your conversations, learns from them, and\nuses that knowledge to answer questions about what\u2019s going on in your Slack\nworkspace. We\u2019ll also deploy it to production on Render!\n\n#  Things you\u2019ll need to start\n\n  * Rudimentary understanding of LlamaIndex. If you haven\u2019t got that, the [ starter tutorial ](https://docs.llamaindex.ai/en/stable/getting_started/starter_example.html) in our documentation will give you as much as you need to understand this tutorial and takes only a few minutes. \n  * A working knowledge of Python, and Python 3.11 or higher installed \n  * A Slack workspace you can install apps to (so you\u2019ll need to be an admin) \n  * A clone of [ our Slackbot repo ](https://github.com/run-llama/llamabot) on your local machine. We\u2019ll be referring to files in this repo throughout the post. \n\n#  Step 1: Create a Slack app, and install it to your workspace\n\nThis is the most complicated step, because Slack is very picky about\npermissions.\n\nThe very first version of your Slackbot is going to be only about 20 lines of\ncode. All it does is provide a \u201cchallenge\u201d endpoint that Slack needs to verify\nyour app is available. You can see this code as the file ` 1_flask.py ` in the\nrepo. Let's walk through it.\n\nFirst we bring in your dependencies. You\u2019ll need to install these with pip or\npoetry if you don\u2019t have them already.\n\n    \n    \n    from flask import Flask, request, jsonify\n\nNow we\u2019ll create your flask app and set it up so it can run in development.\n\n    \n    \n    flask_app = Flask(__name__)\n    \n    if __name__ == \"__main__\":\n        flask_app.run(port=3000)\n\nBetween those lines we\u2019ll add our basic route: if a POST request is received\nthat contains a JSON object with a ` challenge ` key, we'll return the value\nof that key. Otherwise we'll do nothing.\n\n    \n    \n    @flask_app.route(\"/\", methods=[\"POST\"])\n    def slack_challenge():\n        if request.json and \"challenge\" in request.json:\n            print(\"Received challenge\")\n            return jsonify({\"challenge\": request.json[\"challenge\"]})\n        else:\n            print(\"Got unknown request incoming\")\n            print(request.json)\n        return\n\n#  Make your app available to Slack\n\nTo configure a Slack app, it needs to be running somewhere Slack can see it.\nSo let\u2019s run our Slack app:\n\n    \n    \n    python 1_flask.py\n\nAnd we\u2019ll set it up so the world can see it using [ ngrok\n](https://ngrok.com/) . You\u2019ll need to download and install ngrok for this\nstep. Once you have it installed, run the following command so it can find our\napp running on port 3000:\n\n    \n    \n    ngrok http 3000\n\nngrok will give you an HTTPS url like ` https://1bf6-64-38-189-168.ngrok-\nfree.app ` . Make a note of it, because we need to give that to Slack. Also\nkeep in mind that if you stop ngrok and start it again, this URL will change\nand you'll need to tell Slack about that. You'll only need this during\ndevelopment.\n\n#  Register your app with Slack\n\nGo to the [ Slack API site ](https://api.slack.com/apps) and click \u201cCreate New\nApp\u201d. You\u2019ll see a screen like this, you\u2019ll want to pick \u201cfrom scratch\u201d:\n\nPick a nice friendly name and the workspace you want to install it to. You\u2019ll\nsee a screen like this:\n\nNext you\u2019ll want to set up what permissions your app needs. Click the\n\u201cPermissions\u201d link in the bottom right:\n\nThis will bring you to the \u201cscopes\u201d screen where you\u2019ll need to add all the\nscopes you see in this picture, namely:\n\n  * channels:read \u2014 the lets your app see what channels are avaialble \n  * channels:join \u2014 this lets your app join channels \n  * channels:history \u2014 this lets your app see previous messages in channels \n  * chat:write \u2014 this lets your app send messages \n  * users:read \u2014 this lets your app see people\u2019s names \n\nOnce you\u2019ve saved those scopes, scroll up to \u201cInstall to workspace\u201d to install\nyour app.\n\nYou now need to tell Slack where your app is so you can receive messages from\nit. Click the \u201cEvent Subscriptions\u201d link in the left nav and fill it out so it\nlooks something like this, specifically:\n\n  * Set your Request URL to that URL that ngrok gave you earlier \n  * Subscribe to the ` message.channels ` event \n\nIf your app is running and ngrok is correctly tunneling, your Request URL\nshould be Verified.\n\nPhew! ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4340, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abbe847f-9f06-47ed-9148-aa6eebe08901": {"__data__": {"id_": "abbe847f-9f06-47ed-9148-aa6eebe08901", "embedding": null, "metadata": {"filename": "building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.md", "extension": ".md", "title": "Building a Slack bot that learns with LlamaIndex, Qdrant and Render", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0f766867-73f3-4e06-bd1e-3b879e6bd02a", "node_type": "4", "metadata": {"filename": "building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.md", "extension": ".md", "title": "Building a Slack bot that learns with LlamaIndex, Qdrant and Render", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840"}, "hash": "bc23faa80294e842959fd6eba4822a0e3594aa3a7660dd15a40f9cee9af7da80", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22912f4c-8db2-4d26-a771-96e895fdf90f", "node_type": "1", "metadata": {"filename": "building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.md", "extension": ".md", "title": "Building a Slack bot that learns with LlamaIndex, Qdrant and Render", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840"}, "hash": "d4f6764fc7cc25f89fcba6669b2eaf7ba0efb237765353fc2b4013d224f98773", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "96e3351b-7ca0-4d92-96b3-eb0e4e243611", "node_type": "1", "metadata": {}, "hash": "c4a1af49276b55e6d8ce50f746f049720cbec59fbd151988a44a4bca612bfe06", "class_name": "RelatedNodeInfo"}}, "text": "That was a lot. Your Slack app is now registered and Slack will send it\nmessages. But to get those messages, you have to tell it to join a channel.\n\n#  Step 2: Join a channel, and reply to messages\n\nTo do this we\u2019ll need to extend our app. You can see the final result of this\nstep in ` 2_join_and_reply.py ` . Let's walk through what we've added:\n\n    \n    \n    import dotenv, os\n    dotenv.load_dotenv()\n\nWe need some environment variables, so you\u2019ll need to add these lines and\ninstall ` python-dotenv ` . You'll also need to create a ` .env ` file in the\nroot of your project with three values:\n\n  * ` OPENAI_API_KEY ` : your OpenAI API key. You don't need this quite yet but you may as well [ get it now ](https://platform.openai.com/) . \n  * ` SLACK_BOT_TOKEN ` : you can find this in the \"OAuth and Permissions\" section of your Slack app. \n  * ` SLACK_SIGNING_SECRET ` : you can find this in the \"Basic Information\" section of your Slack app. \n\nWe\u2019re going to use Slack\u2019s handy Python SDK to build our app, so pip install `\nslack-bolt ` and then update all our imports:\n\n    \n    \n    from slack_bolt import App\n    from flask import Flask, request, jsonify\n    from slack_bolt.adapter.flask import SlackRequestHandler\n\nNow initialize a Slack Bolt app using those secrets we set just now:\n\n    \n    \n    app = App(\n        token=os.environ.get(\"SLACK_BOT_TOKEN\"),\n        signing_secret=os.environ.get(\"SLACK_SIGNING_SECRET\")\n    )\n    handler = SlackRequestHandler(app)\n\nTo listen to messages, the bot has to be in a channel. You can get it to join\nany and all public channels, but for the purposes of testing I\u2019ve created a\nchannel called ` #bot-testing ` and that's the one it's joining here:\n\n    \n    \n    channel_list = app.client.conversations_list().data\n    channel = next((channel for channel in channel_list.get('channels') if channel.get(\"name\") == \"bot-testing\"), None)\n    channel_id = channel.get('id')\n    app.client.conversations_join(channel=channel_id)\n\n` app.client ` is the Bolt framework's Slack WebClient, so you can do anything\na WebClient can do directly from within the framework. The final addition here\nis a very simple message listener:\n\n    \n    \n    @app.message()\n    def reply(message, say):\n        print(message)\n        say(\"Yes?\")\n\nIn the Bolt framework, the ` @app.message ` decorator tells the framework to\ntrigger this method when it receives a message event. The ` say ` parameter is\na function that will send a message back to the channel the message came from.\nSo this code will send a message back to the channel saying \"Yes?\" every time\nit receives a message.\n\n", "mimetype": "text/plain", "start_char_idx": 4340, "end_char_idx": 6953, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96e3351b-7ca0-4d92-96b3-eb0e4e243611": {"__data__": {"id_": "96e3351b-7ca0-4d92-96b3-eb0e4e243611", "embedding": null, "metadata": {"filename": "building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.md", "extension": ".md", "title": "Building a Slack bot that learns with LlamaIndex, Qdrant and Render", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0f766867-73f3-4e06-bd1e-3b879e6bd02a", "node_type": "4", "metadata": {"filename": "building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.md", "extension": ".md", "title": "Building a Slack bot that learns with LlamaIndex, Qdrant and Render", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840"}, "hash": "bc23faa80294e842959fd6eba4822a0e3594aa3a7660dd15a40f9cee9af7da80", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "abbe847f-9f06-47ed-9148-aa6eebe08901", "node_type": "1", "metadata": {"filename": "building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.md", "extension": ".md", "title": "Building a Slack bot that learns with LlamaIndex, Qdrant and Render", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840"}, "hash": "c76186094cc0cc7ef261ea93ad9592eb1154ad5074b2e5659607deedab06a199", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6cf405f-c94a-457e-8995-f94b75d9db5e", "node_type": "1", "metadata": {}, "hash": "4c65d20bccf03d14e7a374bf428ca132e2d460feedc9707a9f9deef558ee379c", "class_name": "RelatedNodeInfo"}}, "text": "Let\u2019s try it out! Stop running ` 1_flask.py ` and run ` python\n2_join_and_reply.py ` instead. You don't need to restart ` ngrok ` , it will\ncontinue to send messages to port 3000 as before. Here's me trying it out:\n\nSuccess! ", "mimetype": "text/plain", "start_char_idx": 6953, "end_char_idx": 7178, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6cf405f-c94a-457e-8995-f94b75d9db5e": {"__data__": {"id_": "b6cf405f-c94a-457e-8995-f94b75d9db5e", "embedding": null, "metadata": {"filename": "building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.md", "extension": ".md", "title": "Building a Slack bot that learns with LlamaIndex, Qdrant and Render", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0f766867-73f3-4e06-bd1e-3b879e6bd02a", "node_type": "4", "metadata": {"filename": "building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.md", "extension": ".md", "title": "Building a Slack bot that learns with LlamaIndex, Qdrant and Render", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840"}, "hash": "bc23faa80294e842959fd6eba4822a0e3594aa3a7660dd15a40f9cee9af7da80", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "96e3351b-7ca0-4d92-96b3-eb0e4e243611", "node_type": "1", "metadata": {"filename": "building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.md", "extension": ".md", "title": "Building a Slack bot that learns with LlamaIndex, Qdrant and Render", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840"}, "hash": "5e1984b9808d780cc8f6414ede7dbded22acd8105077e88b0e3d4ab11fe68eff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff00f6f5-830c-4309-bea0-7bf983820985", "node_type": "1", "metadata": {}, "hash": "deaf9483b870827d2b0b1e1ada0d6ccfe79c28ef962ab12ebac326548512a247", "class_name": "RelatedNodeInfo"}}, "text": "We have a very annoying bot that replies to every single thing\nanybody says. We can do better!\n\n#  Step 3: reply only to messages that mention the bot\n\nThis is a pretty simple change on the surface, but Slack\u2019s incoming message\nformat is a little complicated so we have to add a fair bit of code. You can\nsee the final results in ` 3_reply_to_mentions.py ` .\n\nFirst, to tell when our bot is being mentioned, we need our bot\u2019s User ID.\nUnder the hood, Slack doesn\u2019t use user names or even @-handles, but a globally\nunique ID across all Slack installations. We have to get that:\n\n    \n    \n    auth_response = app.client.auth_test()\n    bot_user_id = auth_response[\"user_id\"]\n\nNow we add an annoyingly complicated chunk of code that parses through Slack\u2019s\nmessage object to see what user is mentioned in an incoming message. If it\u2019s\nthe bot, the bot replies, otherwise it just ignores the message. As we go\nfurther, we\u2019ll treat messages to the bot as \u201cqueries\u201d and any other message as\na \u201cfact\u201d for it to store, but we won\u2019t be storing it just yet.\n\n    \n    \n    @app.message()\n    def reply(message, say):\n        if message.get('blocks'):\n            for block in message.get('blocks'):\n                if block.get('type') == 'rich_text':\n                    for rich_text_section in block.get('elements'):\n                        for element in rich_text_section.get('elements'):\n                            if element.get('type') == 'user' and element.get('user_id') == bot_user_id:\n                                for element in rich_text_section.get('elements'):\n                                    if element.get('type') == 'text':\n                                        query = element.get('text')\n                                        print(f\"Somebody asked the bot: {query}\")\n                                        say(\"Yes?\")\n                                        return\n        # otherwise do something else with it\n        print(\"Saw a fact: \", message.get('text'))\n\nOof. That took a while to get right! But now our bot only replies when it\u2019s\nmentioned:\n\n#  Step 4: use LlamaIndex to store facts and answer questions\n\nWe\u2019re all the way at step 4 and we still haven\u2019t done anything with\nLlamaIndex! But now\u2019s the time. In ` 4_incremental_rag.py ` you'll see a\ndemonstration of a simple command-line Python script that uses LlamaIndex to\nstore facts and answer questions. I won't walk you through every line (the\nscript has helpful comments for that), but let's look at the important ones.\nRemember to ` pip install llama-index ` !\n\nFirst we create a new ` VectorStoreIndex ` , an in-memory [ vector store\n](https://docs.llamaindex.ai/en/stable/understanding/indexing/indexing.html#vector-\nstore-index) where we'll be storing our facts. It's empty to start with.\n\n    \n    \n    index = VectorStoreIndex([])\n\nNext we create 3 ` Document ` objects and insert them each into our index.\nReal documents can be huge blocks of text, whole PDFs, even images, but these\nare just some simple, Slack-message-sized facts.\n\n    \n    \n    doc1 = Document(text=\"Molly is a cat\")\n    doc2 = Document(text=\"Doug is a dog\")\n    doc3 = Document(text=\"Carl is a rat\")\n    \n    index.insert(doc1)\n    index.insert(doc2)\n    index.insert(doc3)\n\nAnd finally we create a [ query engine\n](https://docs.llamaindex.ai/en/stable/understanding/querying/querying.html)\nfrom our index and ask it a question:\n\n    \n    \n    # run a query\n    query_engine = index.as_query_engine()\n    response = query_engine.query(\"Who is Molly?\")\n    print(response)\n\nThe result is \u201cMolly is a cat\u201d plus a whole lot of debugging info because we\nturned on noisy debugging in ` 4_incremental_rag.py ` . You can see the prompt\nwe sent to the LLM, the context it retrieved from the index, and the response\nit generated and sent back to us.\n\n#  Step 5: use LlamaIndex to store facts and answer questions in Slack\n\nIn ` 5_rag_in_slack.py ` we are combining the two things we had before: script\n3, where we reply to queries, and script 4, where we store facts and answer\nquestions. Once again we won't walk through every line, but here are the\nimportant changes:\n\nFirst ` pip install llama-index ` if you didn't already, and bring in your\ndeps. Initialize your index while you're at it:\n\n    \n    \n    from llama_index import VectorStoreIndex, Document\n    \n    index = VectorStoreIndex([])\n\nWhere previously we were just replying with \u201cYes?\u201d (line 73) let\u2019s instead\nsend a query to the query engine and reply with the response:\n\n    \n    \n    query = element.get('text')\n    query_engine = index.as_query_engine()\n    response = query_engine.query(query)\n    say(str(response))\n\nAnd where previously we were just noting that we\u2019d seen a fact (line 82),\nlet\u2019s store it in the index:\n\n    \n    \n    index.insert(Document(text=message.get('text')))\n\nThe result is a Slackbot that can answer questions about what it\u2019s been told:\n\nAmazing! You can easily imagine a bot that listens to everybody\u2019s\nconversations and is able to answer questions about things people said weeks\nor months ago, saving everybody time and effort searching through old\nmessages.\n\n#  Step 6: persist our memory\n\nOur bot has a critical flaw though: the index is stored only in memory. If we\nrestart the bot, it forgets everything:\n\nIn ` 6_qdrant.py ` we bring in [ Qdrant ](https://qdrant.tech/) , an open-\nsource, local vector database that stores these facts on disk instead. That\nway if we restart our bot it remembers what was said before. ` pip install\nqdrant-client ` and bring in some new deps:\n\n    \n    \n    import qdrant_client\n    from llama_index.vector_stores.qdrant import QdrantVectorStore\n\nNow we\u2019ll initialize the Qdrant client, attach it to a storage context, and\ngive that storage context to our index when we initialize it:\n\n    \n    \n    client = qdrant_client.QdrantClient(\n        path=\"./qdrant_data\"\n    )\n    vector_store = QdrantVectorStore(client=client, collection_name=\"slack_messages\")\n    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n    \n    index = VectorStoreIndex([],storage_context=storage_context)\n\nThat\u2019s it for this step! Your bot now survives reboots, and remembers that I\ntypoed \u201cDoug\u201d as \u201cDough\u201d and was too lazy to fix it for the screenshot:\n\n#  Step 7: make recent messages more important\n\nWe now have a pretty capable bot! ", "mimetype": "text/plain", "start_char_idx": 7178, "end_char_idx": 13508, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff00f6f5-830c-4309-bea0-7bf983820985": {"__data__": {"id_": "ff00f6f5-830c-4309-bea0-7bf983820985", "embedding": null, "metadata": {"filename": "building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.md", "extension": ".md", "title": "Building a Slack bot that learns with LlamaIndex, Qdrant and Render", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0f766867-73f3-4e06-bd1e-3b879e6bd02a", "node_type": "4", "metadata": {"filename": "building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.md", "extension": ".md", "title": "Building a Slack bot that learns with LlamaIndex, Qdrant and Render", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840"}, "hash": "bc23faa80294e842959fd6eba4822a0e3594aa3a7660dd15a40f9cee9af7da80", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6cf405f-c94a-457e-8995-f94b75d9db5e", "node_type": "1", "metadata": {"filename": "building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.md", "extension": ".md", "title": "Building a Slack bot that learns with LlamaIndex, Qdrant and Render", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840"}, "hash": "5805a78b15ca68feaec19b39b98f97b367bee60d8dbf6dbf9333d7540f21553a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a147782-6e42-4a1c-8732-8bf0ec2a26a7", "node_type": "1", "metadata": {}, "hash": "29ef7cb21fd11ecd3d6e487ca38ddba32082eb02d65d3daa7af323833fe4fc4d", "class_name": "RelatedNodeInfo"}}, "text": "But it has a subtle problem: people can say\nconflicting things, and it doesn\u2019t have a way to decide who was \u201cright\u201d, such\nas when I change my mind about what the dog\u2019s name should be:\n\nIn real Slack conversations, as a situation evolves people might move from\nsaying a project is \u201cin planning\u201d to \u201cunderway\u201d to \u201claunched\u201d. So we need a\nway to tell the bot that more recent messages are more important than older\nones.\n\nTo make this happen we have to do quite a bit of refactoring, the final\nresults of which you can see in ` 7_recency.py ` . First we need a bunch of\nnew deps:\n\n    \n    \n    import datetime, uuid\n    from llama_index.schema import TextNode\n    from llama_index.prompts import PromptTemplate\n    from llama_index.postprocessor import FixedRecencyPostprocessor\n    from llama_index import set_global_handler\n\nTo make recent messages more important, we have to know when a message was\nsent. To do that we are going to stop inserting ` Documents ` into the index\nand instead insert ` Nodes ` , to which we're going to attach the timestamp as\nmetadata (under the hood, our Documents were always being converted into Nodes\nanyway so this doesn't change much):\n\n    \n    \n    dt_object = datetime.datetime.fromtimestamp(float(message.get('ts')))\n    formatted_time = dt_object.strftime('%Y-%m-%d %H:%M:%S')\n    \n    # get the message text\n    text = message.get('text')\n    # create a node with metadata\n    node = TextNode(\n        text=text,\n        id_=str(uuid.uuid4()),\n        metadata={\n            \"when\": formatted_time\n        }\n    )\n    index.insert_nodes([node])\n\nI\u2019ve also factored out the reply logic from message handling into its own\nfunction, ` answer_question ` , just to make things a little easier to read.\nThe first thing we're going to change is the prompt that we give to our LLM:\nwe have to tell it that more recent messages are important. To do this we\ncreate a prompt template:\n\n    \n    \n    template = (\n        \"Your context is a series of chat messages. Each one is tagged with 'who:' \\n\"\n        \"indicating who was speaking and 'when:' indicating when they said it, \\n\"\n        \"followed by a line break and then what they said. There can be up to 20 chat messages.\\n\"\n        \"The messages are sorted by recency, so the most recent one is first in the list.\\n\"\n        \"The most recent messages should take precedence over older ones.\\n\"\n        \"---------------------\\n\"\n        \"{context_str}\"\n        \"\\n---------------------\\n\"\n        \"You are a helpful AI assistant who has been listening to everything everyone has been saying. \\n\"\n        \"Given the most relevant chat messages above, please answer this question: {query_str}\\n\"\n    )\n    qa_template = PromptTemplate(template)\n\nThe fun thing about working with LLMs is how often you end up just describing\nwhat you\u2019re doing in English and that being what you send to the LLM. A prompt\ntemplate will automatically get the ` context_str ` and ` query_str ` from the\nquery engine. But we have to set this template on our query engine, like so:\n\n    \n    \n    query_engine.update_prompts(\n        {\"response_synthesizer:text_qa_template\": qa_template}\n    )\n\nNow there\u2019s two more things we\u2019re going to change. We\u2019re going to take the\nresults we get from the vector store and sort them by recency, something\nLlamaIndex has a built-in class for. It\u2019s called the `\nFixedRecencyPostprocessor ` . We tell it the key that holds the timestamp\n(which we defined earlier on the nodes, above) and how many results it should\nreturn:\n\n    \n    \n    postprocessor = FixedRecencyPostprocessor(\n        top_k=20, \n        date_key=\"when\", # the key in the metadata to find the date\n        service_context=ServiceContext.from_defaults()\n    )\n\nThen we need to create our query engine with the postprocessor attached:\n\n    \n    \n    query_engine = index.as_query_engine(similarity_top_k=20, node_postprocessors=[postprocessor])\n\nWhile we were at it we did our final thing, which was pass `\nsimilarity_top_k=20 ` , which means the vector store will give us 20 Slack\nmessages as context (the default is just 2, because usually the chunks of text\nin a Node are a lot bigger).\n\nTada! ", "mimetype": "text/plain", "start_char_idx": 13508, "end_char_idx": 17672, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a147782-6e42-4a1c-8732-8bf0ec2a26a7": {"__data__": {"id_": "5a147782-6e42-4a1c-8732-8bf0ec2a26a7", "embedding": null, "metadata": {"filename": "building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.md", "extension": ".md", "title": "Building a Slack bot that learns with LlamaIndex, Qdrant and Render", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0f766867-73f3-4e06-bd1e-3b879e6bd02a", "node_type": "4", "metadata": {"filename": "building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.md", "extension": ".md", "title": "Building a Slack bot that learns with LlamaIndex, Qdrant and Render", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840"}, "hash": "bc23faa80294e842959fd6eba4822a0e3594aa3a7660dd15a40f9cee9af7da80", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff00f6f5-830c-4309-bea0-7bf983820985", "node_type": "1", "metadata": {"filename": "building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.md", "extension": ".md", "title": "Building a Slack bot that learns with LlamaIndex, Qdrant and Render", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840"}, "hash": "73459c56d2ac1d0bc5c15f6123b2f1f99506492f5ede0a8a6005b40423c05ecc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "142c0792-e5a7-475c-8a9c-a13eec8e0965", "node_type": "1", "metadata": {}, "hash": "a02c12e39f2bb2c4796c8726dd1eab56c52ac8e7eca10711593f38115c930bdf", "class_name": "RelatedNodeInfo"}}, "text": "Now the bot knows to take more recent statements as the truth.\n\n#  Step 8: draw the rest of the owl\n\nThis bot is working pretty well now, but I was having such fun when building\nit I got carried away and added two more features:\n\n  * I attached metadata about _who_ was speaking, not just when, so the bot can answer questions like \u201cWhat did Logan say about the project?\u201d \n  * My colleagues interacting with the bot tried to ask follow-up questions in a thread, like we do with each other. So I added a way for the bot to understand that it\u2019s in a thread, and treat replies in a thread as follow-up questions, even if the user doesn\u2019t mention the bot directly: \n\nThe code to make both of those happen is in ` 8_rest_of_the_owl.py ` but I'm\nnot going to be stepping through it line by line. We have to deploy this\nthing!\n\n#  Step 9: deploy to Render\n\nUntil now we\u2019ve been working with local scripts running through the ngrok\ntunnel, but even the most dedicated coder turns their laptop off sometimes.\nLet\u2019s put this thing on a real server.\n\n#  Login to Render\n\nWe\u2019ll be deploying to [ Render ](https://render.com/) , a Python-friendly\nhosting service that\u2019s free for small projects. Sign up for an account (I\nrecommend logging in with GitHub).\n\n#  Create a new GitHub repository\n\nRender deploys things from GitHub repositories, so you\u2019ll need to create a new\none and copy 2 files from our existing repo into it:\n\n  * ` pyproject.toml `\n  * ` 8_rest_of_the_owl.py ` which we're going to rename to \"app.py\" for simplicity. \n\nCommit those and push them up to GitHub.\n\n#  Create a new Render web service\n\nIn Render, create a new web service. Connect it to the repo on GitHub you just\ncreated:\n\nRender will probably automatically detect that this is a Python app but you\nshould make sure the following settings are correct:\n\n  * Name: any name you choose \n  * Region: any region is fine \n  * Branch: main \n  * Root directory: (blank, meaning root) \n  * Runtime: Python 3 \n  * Build command: ` poetry install `\n  * Start command: ` gunicorn app:flask_app ` (this will definitely need to be set) \n\nYou\u2019ll also need to scroll down and set some environment variables:\n\n  * PYTHON_VERSION: 3.11.6 (or whatever version you\u2019re using) \n  * OPENAI_API_KEY: your OpenAI API key \n  * SLACK_BOT_TOKEN: your Slack bot token \n  * SLACK_SIGNING_SECRET: your Slack signing secret from before \n\nThen click deploy and away you go!\n\nYou now have a production Slack bot listening to messages, remembering,\nlearning, and replying. ", "mimetype": "text/plain", "start_char_idx": 17672, "end_char_idx": 20176, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "142c0792-e5a7-475c-8a9c-a13eec8e0965": {"__data__": {"id_": "142c0792-e5a7-475c-8a9c-a13eec8e0965", "embedding": null, "metadata": {"filename": "building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.md", "extension": ".md", "title": "Building a Slack bot that learns with LlamaIndex, Qdrant and Render", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0f766867-73f3-4e06-bd1e-3b879e6bd02a", "node_type": "4", "metadata": {"filename": "building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.md", "extension": ".md", "title": "Building a Slack bot that learns with LlamaIndex, Qdrant and Render", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840"}, "hash": "bc23faa80294e842959fd6eba4822a0e3594aa3a7660dd15a40f9cee9af7da80", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a147782-6e42-4a1c-8732-8bf0ec2a26a7", "node_type": "1", "metadata": {"filename": "building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.md", "extension": ".md", "title": "Building a Slack bot that learns with LlamaIndex, Qdrant and Render", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840"}, "hash": "3e0b2241367b34a31480b9c744384479097c0650b6f231e4ea9675069e08fc79", "class_name": "RelatedNodeInfo"}}, "text": "Congratulations!\n\n#  What next?\n\nThere\u2019s a whole bunch of features you could add to this bot, roughly in\nincreasing order of difficulty:\n\n  * Join every channel instead of just one, clearly! \n  * Add a way to tell the bot to forget things (delete nodes) \n  * Give the bot the ability to use more than one index, such as an index of your documentation, or connected to your email, or your calendar \n  * Give the bot \u201ctags\u201d so it can attach metadata to nodes and answer questions only with (or ignore) things that have been tagged a certain way \n  * Add multi-modal abilities, so the bot can read images and even reply with generated images \n  * And tons more! \n\nThis bot is a lot of fun to play with and was a lot of fun to build, I hope\nyou enjoyed learning about Slackbots and LlamaIndex as much as I enjoyed\nwriting this tutorial!\n\n", "mimetype": "text/plain", "start_char_idx": 20176, "end_char_idx": 21010, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ada1dc8-1654-4337-bd86-0cf3bf97db86": {"__data__": {"id_": "1ada1dc8-1654-4337-bd86-0cf3bf97db86", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-01-23-11ee2c211bab.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201323", "date": "Jan 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-23-11ee2c211bab"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "84c4145c-1895-4f42-aa06-9247a881868d", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-01-23-11ee2c211bab.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201323", "date": "Jan 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-23-11ee2c211bab"}, "hash": "ffd8a27e3b83e6b1b8efd378c5dfa07016baa211d41e395aea81284ef69be622", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a55b8e0b-2ceb-4942-8f97-2c1358dc1403", "node_type": "1", "metadata": {}, "hash": "0745e443ff6ed9c243b44dc3171b451f3d05d18255166f067ee5e5afdb8bbbb3", "class_name": "RelatedNodeInfo"}}, "text": "Hello LlamaIndex Explorers ,\n\nAnother exciting week at LlamaIndex, filled with vibrant community\ncontributions and educational resources. Explore our array of new features,\ntutorials, guides, and demos, all tailored to enrich your experience with\nLlamaIndex.\n\nBefore delving into the updates, we have two significant announcements:\n\n  * We\u2019re thrilled to host our [ first in-person hackathon ](https://rag-a-thon.devpost.com/) , set for February 2nd-4th. This is a fantastic opportunity to meet fellow RAG enthusiasts, collaborate, and compete for prizes totaling over $8000! \n  * Don\u2019t miss our [ webinar ](https://lu.ma/lf9iroox) featuring Sehoon Kim and Amir Gholami, scheduled for Thursday at 9 am PT. They will introduce LLMCompiler, an agent compiler for parallel multi-function planning and execution. \n\nWe\u2019re always excited to see your projects, articles, or videos. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 875, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a55b8e0b-2ceb-4942-8f97-2c1358dc1403": {"__data__": {"id_": "a55b8e0b-2ceb-4942-8f97-2c1358dc1403", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-01-23-11ee2c211bab.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201323", "date": "Jan 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-23-11ee2c211bab"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "84c4145c-1895-4f42-aa06-9247a881868d", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-01-23-11ee2c211bab.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201323", "date": "Jan 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-23-11ee2c211bab"}, "hash": "ffd8a27e3b83e6b1b8efd378c5dfa07016baa211d41e395aea81284ef69be622", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ada1dc8-1654-4337-bd86-0cf3bf97db86", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-01-23-11ee2c211bab.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201323", "date": "Jan 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-23-11ee2c211bab"}, "hash": "b6d7b1a048f3a0f0027ae9f9d27a4bbf62e79a7776b751c9a3e63cd728eb8977", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48019a9d-8eb3-4045-9ccf-081eab9d65eb", "node_type": "1", "metadata": {}, "hash": "3f5ef954db29a5c6175f8b62d13c0ca1475dfd33427d15005cea9a556ceebda0", "class_name": "RelatedNodeInfo"}}, "text": "If you\u2019ve\ncreated something you\u2019re proud of, share it with us at [ news@llamaindex.ai\n](mailto:news@llamaindex.ai) . Also, remember to subscribe to our newsletter\non our [ website ](https://www.llamaindex.ai/) to get all the latest news\nstraight to your inbox.\n\n", "mimetype": "text/plain", "start_char_idx": 875, "end_char_idx": 1137, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48019a9d-8eb3-4045-9ccf-081eab9d65eb": {"__data__": {"id_": "48019a9d-8eb3-4045-9ccf-081eab9d65eb", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-01-23-11ee2c211bab.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201323", "date": "Jan 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-23-11ee2c211bab"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "84c4145c-1895-4f42-aa06-9247a881868d", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-01-23-11ee2c211bab.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201323", "date": "Jan 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-23-11ee2c211bab"}, "hash": "ffd8a27e3b83e6b1b8efd378c5dfa07016baa211d41e395aea81284ef69be622", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a55b8e0b-2ceb-4942-8f97-2c1358dc1403", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-01-23-11ee2c211bab.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201323", "date": "Jan 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-23-11ee2c211bab"}, "hash": "f4167ec891ea8d8536e77fe811977bab6772a88490a2071df6815b65569ef2ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b7a9f81-06f4-464d-b4b2-e3f4c402ce29", "node_type": "1", "metadata": {}, "hash": "dd7134309aa3741ee672ae0c66591c80b1f46bfe480c6bd21b82ef4b7d1dde76", "class_name": "RelatedNodeInfo"}}, "text": "**The highlights:**\n\n  1. **RankGPT:** Introducing RankGPT leveraging GPT-3.5 and GPT-4 for top-tier document ranking and a novel sliding window technique for extensive context management. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/node_postprocessor/rankGPT.ipynb) , [ Tweet ](https://x.com/llama_index/status/1747681530347216995?s=20) . \n  2. **Composable Retrievers:** An interface centralizing advanced retrieval and RAG techniques, enhancing RAG setups with IndexNodes for linking diverse retrievers and pipelines. [ Docs ](https://docs.llamaindex.ai/en/stable/examples/retrievers/composable_retrievers.html) , [ Tweet ](https://x.com/llama_index/status/1748019272679649386?s=20) . \n  3. **Advanced QA over Tabular Data Tutorial:** A detailed guide to crafting query pipelines over tabular data, featuring Pandas, SQL, and Query Pipelines for an integrated few-shot, LLM, and custom function setup. [ Text-to-SQL ](https://docs.llamaindex.ai/en/stable/examples/pipeline/query_pipeline_sql.html) , [ Text-to-Pandas ](https://docs.llamaindex.ai/en/stable/examples/pipeline/query_pipeline_pandas.html) . \n  4. **Long-Context Embedding Models:** Explore models like M2-BERT-80M-32k-retrieval tackling the embedding chunking problem in RAG, with a focus on hybrid retrieval methods and hierarchical retrieval approaches. [ Guide ](https://docs.llamaindex.ai/en/latest/examples/retrievers/multi_doc_together_hybrid.html) . \n\n**Feature Releases and Enhancements:**\n\n  * We have introduced RankGPT in our advanced module that utilizes GPT-3.5 and GPT-4 for efficient document ranking, featuring a unique sliding window strategy for handling large contexts. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/node_postprocessor/rankGPT.ipynb) , [ Tweet ](https://x.com/llama_index/status/1747681530347216995?s=20) . \n  * We have launched Composable Retrievers which centralizes various advanced retrieval and RAG techniques into a versatile interface. It simplifies creating complex RAG setups by allowing you to define IndexNodes to link different retrievers or RAG pipelines. [ Docs ](https://docs.llamaindex.ai/en/stable/examples/retrievers/composable_retrievers.html) , [ Tweet ](https://x.com/llama_index/status/1748019272679649386?s=20) . \n  * Anoop Sharma has introduced LlamaPack for Multi-Stock Ticker Analysis for analyzing various stock tickers with a single code line, enabling easy specification of tickers, time frames, and structured queries. [ LlamaPack ](https://llamahub.ai/l/llama_packs-stock_market_data_query_engine?from=llama_packs) , [ Tweet ](https://x.com/llama_index/status/1748422554841448600?s=20) . \n  * LlamaIndex.TS (LITS) supports streaming on all endpoints. [ Tweet ](https://x.com/llama_index/status/1747746779058290800?s=20) . \n  * We announced a new integration with Tonic Validate to allow simple access to LLM-powered evaluations. [ Blog post ](https://www.tonic.ai/blog/tonic-ai-and-llamaindex-join-forces-to-help-developers-build-more-performant-rag-systems)\n\n**Demo:**\n\n  * **RAG-Maestro for ArXiv Research:** Developed by Aymen Kallala, this web app utilizes RAG to efficiently search scientific concepts in ArXiv papers. It extracts keywords using RAKE, queries ArXiv for relevant papers, and offers on-the-fly indexing with in-line citations \u2014 a valuable tool for ML researchers navigating through ArXiv\u2019s extensive library. [ Demo ](https://rag-maestro-o2wbip4gla-uc.a.run.app/) , [ GitHub Repo ](https://github.com/AymenKallala/RAG_Maestro) . \n\n**Guides:**\n\n  * Guide to Advanced QA over Tabular Data which provides a comprehensive tutorial on creating sophisticated query pipelines over tabular data using Pandas or SQL, constructing a query DAG using our Query Pipelines, integrating few-shot examples, linked prompts, LLMs, custom functions, retrievers, and more. [ Text-to-SQL ](https://docs.llamaindex.ai/en/stable/examples/pipeline/query_pipeline_sql.html) , [ Text-to-Pandas ](https://docs.llamaindex.ai/en/stable/examples/pipeline/query_pipeline_pandas.html) . \n  * [ Guide ](https://medium.com/@marco.bertelli/revolutionizing-chatbot-performance-unleashing-three-potent-strategies-for-rag-enhancement-c1188e395d9d) to a Five-Part Series on Building a Full-Stack RAG Chatbot by [ Marco Bertelli ](https://medium.com/@marco.bertelli) , extensive tutorials covering every aspect of creating an RAG chatbot \u2014 from model selection and Flask backend setup to constructing the ChatEngine and optimizing the RAG pipeline. \n  * [ Guide ](https://docs.llamaindex.ai/en/latest/examples/retrievers/multi_doc_together_hybrid.html) to Long-Context Embedding Models: The models, like M2-BERT-80M-32k-retrieval, offer a solution to the embedding chunking issue in RAG by grounding retrieval in broader semantic contexts. Learn about hybrid retrieval, combining chunk and document-level similarity, and other approaches like hierarchical retrieval. \n\n**Tutorials:**\n\n  * [ Wenqi ](https://twitter.com/wenqi_glantz) [ tutorial ](https://towardsdatascience.com/democratizing-llms-4-bit-quantization-for-optimal-llm-inference-be30cf4e0e34) on Democratizing LLMs: 4-bit Quantization for Optimal LLM Inference with LlamaIndex. \n  * [ Andrej ](https://twitter.com/andrejusb) [ tutorial ](https://www.youtube.com/watch?v=vntNI33wrcI) on FastAPI and LlamaIndex RAG: Creating Efficient APIs. \n  * [ Lulia Brezeanu ](https://medium.com/@brezeanu.iulia) [ tutorial ](https://towardsdatascience.com/advanced-query-transformations-to-improve-rag-11adca9b19d1) on Advanced Query Transformations to Improve RAG. \n  * [ Akash Mathur ](https://akash-mathur.medium.com/) in-depth [ tutorial ](https://akash-mathur.medium.com/advanced-rag-query-augmentation-for-next-level-search-using-llamaindex-d362fed7ecc3) on Advanced RAG: Query Augmentation for Next-Level Search using LlamaIndex. \n  * [ Ryan Nguyen ](https://medium.com/@ryanntk) [ tutorial ](https://levelup.gitconnected.com/live-indexing-for-rag-a-guide-for-real-time-indexing-using-llamaindex-and-aws-51353083ace4) on Live Indexing for RAG: A Guide For Real-Time Indexing Using LlamaIndex and AWS. \n  * [ Nipuna ](https://www.youtube.com/watch?v=TOeAe8KB68E) (Paragon AI) tutorial on Building a Full-Stack Complex PDF AI chatbot with LlamaIndex. \n\n**Calling all enterprises:**\n\nAre you building with LlamaIndex? We are working hard to make LlamaIndex, even\nmore, Enterprise-ready and have sneak peeks at our upcoming products available\nfor partners. Interested? ", "mimetype": "text/plain", "start_char_idx": 1137, "end_char_idx": 7636, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b7a9f81-06f4-464d-b4b2-e3f4c402ce29": {"__data__": {"id_": "3b7a9f81-06f4-464d-b4b2-e3f4c402ce29", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-01-23-11ee2c211bab.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201323", "date": "Jan 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-23-11ee2c211bab"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "84c4145c-1895-4f42-aa06-9247a881868d", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-01-23-11ee2c211bab.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201323", "date": "Jan 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-23-11ee2c211bab"}, "hash": "ffd8a27e3b83e6b1b8efd378c5dfa07016baa211d41e395aea81284ef69be622", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48019a9d-8eb3-4045-9ccf-081eab9d65eb", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-01-23-11ee2c211bab.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201323", "date": "Jan 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-23-11ee2c211bab"}, "hash": "43d871b641fb59a7fff8bda03b3ca2fe4f9d3f9670de1beefd8d2ae58986e39f", "class_name": "RelatedNodeInfo"}}, "text": "[ Get in touch.\n](https://docs.google.com/forms/d/e/1FAIpQLScBNdM2a_fn8UZOKmFQt6lBsrd1o6FflvsdPH-\nPn3JkdlN_Rg/viewform)\n\n", "mimetype": "text/plain", "start_char_idx": 7636, "end_char_idx": 7757, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fa32b6fa-21c3-4d3c-99a1-43f261c5781b": {"__data__": {"id_": "fa32b6fa-21c3-4d3c-99a1-43f261c5781b", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-01-16-752195bed96d.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201316", "date": "Jan 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-16-752195bed96d"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "819a17cb-b272-4cca-9b9e-eabf7957697d", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-01-16-752195bed96d.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201316", "date": "Jan 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-16-752195bed96d"}, "hash": "2907c7c033374f8ba49c0eaff74ee6580c25746d8728ad35fd26e1d141218f65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2c23206-e86d-4352-881c-3a20c737a790", "node_type": "1", "metadata": {}, "hash": "2ea881aaae98c37fd1ae58e9b88c80dd1a3750fa08dbcca6c1527f4098d6f0a9", "class_name": "RelatedNodeInfo"}}, "text": "Hello LlamaIndex Enthusiasts ,\n\nGet ready for an exciting week at LlamaIndex, teeming with dynamic community\ncontributions and insightful learning resources. Dive into our range of new\nfeatures, tutorials, guides, and events, all designed to enhance your\nLlamaIndex journey.\n\nWe\u2019re excited to announce our [ very first in-person hackathon ](https://rag-\na-thon.devpost.com/) , scheduled for February 2nd-4th. Join us to connect with\nfellow RAG enthusiasts and compete for prizes totaling over $4,000!\n\nIf you\u2019ve been working on a fascinating project, penned an insightful article,\nor produced an engaging video, we\u2019re eager to see it! Share your contributions\nwith us at [ news@llamaindex.ai ](mailto:news@llamaindex.ai) . Don\u2019t forget to\nsubscribe to our newsletter on our [ website ](https://www.llamaindex.ai/) to\nreceive all the latest updates directly in your inbox.\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 873, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2c23206-e86d-4352-881c-3a20c737a790": {"__data__": {"id_": "d2c23206-e86d-4352-881c-3a20c737a790", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-01-16-752195bed96d.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201316", "date": "Jan 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-16-752195bed96d"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "819a17cb-b272-4cca-9b9e-eabf7957697d", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-01-16-752195bed96d.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201316", "date": "Jan 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-16-752195bed96d"}, "hash": "2907c7c033374f8ba49c0eaff74ee6580c25746d8728ad35fd26e1d141218f65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa32b6fa-21c3-4d3c-99a1-43f261c5781b", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-01-16-752195bed96d.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201316", "date": "Jan 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-16-752195bed96d"}, "hash": "24383bef4545f4cebbc02c6ea80e0fe0f442510eb27e13c9ff906bbedfaeb1a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56c9efa5-38f7-4d0e-a148-18df7f4a28a5", "node_type": "1", "metadata": {}, "hash": "b5eddaee25d138cc2443352a0630b89fe80d40cf93b15401181ed6e8d942c364", "class_name": "RelatedNodeInfo"}}, "text": "**The highlights:**\n\n  1. **Chain-of-Table:** Step-by-step table reasoning and operations for enhanced LLM tabular data understanding. [ LlamaPack ](https://llamahub.ai/l/llama_packs-tables-chain_of_table?from=llama_packs) , [ Tweet ](https://x.com/llama_index/status/1746217167706894467?s=20) . \n  2. **LLM Self-Consistency:** Merges textual and symbolic reasoning with majority voting for precise answers. [ LlamaPack ](https://t.co/pGcRG4ieD4) , [ Tweet ](https://twitter.com/llama_index/status/1746937012798800272?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 873, "end_char_idx": 1402, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56c9efa5-38f7-4d0e-a148-18df7f4a28a5": {"__data__": {"id_": "56c9efa5-38f7-4d0e-a148-18df7f4a28a5", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-01-16-752195bed96d.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201316", "date": "Jan 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-16-752195bed96d"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "819a17cb-b272-4cca-9b9e-eabf7957697d", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-01-16-752195bed96d.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201316", "date": "Jan 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-16-752195bed96d"}, "hash": "2907c7c033374f8ba49c0eaff74ee6580c25746d8728ad35fd26e1d141218f65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2c23206-e86d-4352-881c-3a20c737a790", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-01-16-752195bed96d.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201316", "date": "Jan 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-16-752195bed96d"}, "hash": "5b0b5e54656204ab049fd0c33171fde4051fea1501f44e88f83510c31359a421", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdd78b24-dbcb-41b7-803d-60f35812df3b", "node_type": "1", "metadata": {}, "hash": "dd7134309aa3741ee672ae0c66591c80b1f46bfe480c6bd21b82ef4b7d1dde76", "class_name": "RelatedNodeInfo"}}, "text": "3. **Semantic Text Splitting in RAG:** Greg Kamradt\u2019s embedding similarity method for efficient document splitting. [ LlamaPack ](https://llamahub.ai/l/llama_packs-node_parser-semantic_chunking?from=all) , [ Tweet ](https://x.com/llama_index/status/1745482959237615847?s=20) . \n  4. **Parallel RAG Ingestion:** Up to 15x faster document processing in LlamaIndex. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/ingestion/parallel_execution_ingestion_pipeline.ipynb) , [ Tweet ](https://x.com/llama_index/status/1745849571614539984?s=20) . \n  5. **TogetherAI\u2019s Embeddings Support:** Guide to build retrieval-augmented apps with MistralAI\u2019s 8x7b model and TogetherAI Embeddings. [ Blogpost ](https://www.together.ai/blog/rag-tutorial-llamaindex) , [ Tweet ](https://x.com/llama_index/status/1745551739368222815?s=20) . \n\n**Feature Releases and Enhancements:**\n\n  * We launched Chain-of-Table Framework in LlamaPack for LLM Tabular Data Understanding. This approach enables step-by-step table reasoning and operations like adding columns, row selection, grouping, and sorting, mimicking a data scientist\u2019s method for concise data representation. [ LlamaPack ](https://llamahub.ai/l/llama_packs-tables-chain_of_table?from=llama_packs) , [ Tweet ](https://x.com/llama_index/status/1746217167706894467?s=20) . \n  * We launched LLM Self-Consistency Mechanism for Tabular Data in LlamaPack. This method combines textual and symbolic reasoning, utilizing a novel mix self-consistency approach with majority voting to select the best answer. [ LlamaPack ](https://t.co/pGcRG4ieD4) , [ Tweet ](https://twitter.com/llama_index/status/1746937012798800272?s=20) . \n  * We have Introduced Semantic Text Splitting in RAG with LlamaPack. Check Greg Kamradt\u2019s method of splitting documents based on embedding similarity between sentences. This auto-tuned threshold approach enhances RAG pipelines, soon to be available in LlamaPack using LlamaIndex abstractions. [ LlamaPack ](https://llamahub.ai/l/llama_packs-node_parser-semantic_chunking?from=all) , [ Tweet ](https://x.com/llama_index/status/1745482959237615847?s=20) . \n  * We launched Parallel RAG Ingestion in LlamaIndex for up to 15x Faster Document Processing. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/ingestion/parallel_execution_ingestion_pipeline.ipynb) , [ Tweet ](https://x.com/llama_index/status/1745849571614539984?s=20) . \n  * We have launched Support for TogetherAI\u2019s Embeddings Endpoint. Check the blog for a step-by-step guide on creating a retrieval-augmented generation app with MistralAI\u2019s 8x7b model and TogetherAI Embeddings. [ Blogpost ](https://www.together.ai/blog/rag-tutorial-llamaindex) , [ Tweet ](https://x.com/llama_index/status/1745551739368222815?s=20) . \n  * We integrated AgentSearch-v1 as a data loader and Retriever in LlamaHub, offering a robust alternative for internet content search/retrieval without relying on Bing/Google APIs. [ LlamaPack ](https://llamahub.ai/l/llama_packs-agent_search_retriever?from=llama_packs) , [ Tweet ](https://x.com/llama_index/status/1745903362128617842?s=20) . \n  * Raduaschl introduced Ensembling and Fusion in Advanced RAG with LlamaPack. Learn to build an ensembling + fusion pipeline in about 30 lines of code using QueryPipeline syntax, featuring full async support. [ LlamaPack ](https://t.co/iD1v5FuIdy) , [ Tweet ](https://x.com/llama_index/status/1745228497646449021?s=20) . \n\n**Guides:**\n\n  * [ Guide ](https://www.youtube.com/watch?v=4U8viyAQkJ8) to Building Full-Stack RAG Applications with LlamaIndex and Azure Cosmos DB. \n  * [ Guide ](https://docs.llamaindex.ai/en/stable/examples/retrievers/vectara_auto_retriever.html) showing to combine auto-retrieval for semi-structured retrieval with metadata with MMR to enforce diversity in results. \n  * [ Guide ](https://github.com/mickymultani/RAG-with-Cross-Encoder-Reranker) by [ MountainMicky ](https://twitter.com/MountainMicky) to understanding the Importance of Reranking in Advanced RAG Pipelines. \n\n**Tutorials:**\n\n  * [ Andrej Baranovskij ](https://twitter.com/andrejusb) [ tutorial ](https://www.youtube.com/watch?v=VKeYaIEk82s) on Transforming Invoice Data into JSON with LlamaIndex and Pydantic. \n  * NVIDIA [ tutorial ](https://developer.nvidia.com/blog/supercharging-llm-applications-on-windows-pcs-with-nvidia-rtx-systems/) on Building AI apps with local LLMs running on Windows with LlamaIndex and NVIDIA \n  * [ Harshad Suryawanshi ](https://harshadsuryawanshi.medium.com/) [ tutorial ](/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a) on AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5. \n\nEvents:\n\n  * Ravi Theja gave talk on Building Multi-Tenancy RAG System with LlamaIndex and Qdrant at FOSS United, Bangalore, India. \n\n**Calling all enterprises:**\n\nAre you building with LlamaIndex? We are working hard to make LlamaIndex, even\nmore, Enterprise-ready and have sneak peeks at our upcoming products available\nfor partners. Interested? ", "mimetype": "text/plain", "start_char_idx": 1402, "end_char_idx": 6460, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bdd78b24-dbcb-41b7-803d-60f35812df3b": {"__data__": {"id_": "bdd78b24-dbcb-41b7-803d-60f35812df3b", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-01-16-752195bed96d.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201316", "date": "Jan 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-16-752195bed96d"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "819a17cb-b272-4cca-9b9e-eabf7957697d", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-01-16-752195bed96d.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201316", "date": "Jan 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-16-752195bed96d"}, "hash": "2907c7c033374f8ba49c0eaff74ee6580c25746d8728ad35fd26e1d141218f65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56c9efa5-38f7-4d0e-a148-18df7f4a28a5", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-01-16-752195bed96d.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201316", "date": "Jan 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-16-752195bed96d"}, "hash": "b480e2ecb1653b2e4df3a15c7350289bb8f54605da95f0e66d027f5ff422b318", "class_name": "RelatedNodeInfo"}}, "text": "[ Get in touch.\n](https://docs.google.com/forms/d/e/1FAIpQLScBNdM2a_fn8UZOKmFQt6lBsrd1o6FflvsdPH-\nPn3JkdlN_Rg/viewform)\n\n", "mimetype": "text/plain", "start_char_idx": 6460, "end_char_idx": 6581, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8553ee89-1e4f-4a85-b522-14eac71941ff": {"__data__": {"id_": "8553ee89-1e4f-4a85-b522-14eac71941ff", "embedding": null, "metadata": {"filename": "building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.md", "extension": ".md", "title": "Building Multi-Tenancy RAG System with LlamaIndex", "date": "Jan 15, 2024", "url": "https://www.llamaindex.ai/blog/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a0fee36a-1d60-45bc-a806-3fd5d7e1a65d", "node_type": "4", "metadata": {"filename": "building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.md", "extension": ".md", "title": "Building Multi-Tenancy RAG System with LlamaIndex", "date": "Jan 15, 2024", "url": "https://www.llamaindex.ai/blog/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b"}, "hash": "667402b2ba4c5d200bf8947e8926eb677ce0a2af2ac24ff99f0099c3d4887f50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65b9aeca-613f-4a52-bf71-ed062d7b8ee1", "node_type": "1", "metadata": {}, "hash": "a16db4de86d3637a9a795d2046ed0ef9f03fbef72e4a61af4760c4ea23abe57a", "class_name": "RelatedNodeInfo"}}, "text": "#  Introduction:\n\nThe concept of Multi-Tenancy in RAG (Retriever-Augmented Generation) systems\nhas become increasingly vital, especially when it comes to data security and\nprivacy. Multi-Tenancy, in simple terms, refers to a system\u2019s ability to serve\nmultiple users (\u2018tenants\u2019) independently and securely.\n\nConsider this scenario: In a RAG system, there are two users, User-1 and\nUser-2. Both have their own set of documents which they have indexed into the\nsystem. The critical aspect of Multi-Tenancy here is that when User-1 queries\nthe system, they should only receive answers from the documents they have\nindexed, and not from the documents indexed by User-2, and vice versa. This\nseparation is crucial for maintaining data confidentiality and security, as it\nprevents the accidental or unauthorized cross-referencing of private\ninformation between different users.\n\nIn the context of Multi-Tenancy in RAG systems, this means designing a system\nthat not only understands and retrieves information effectively but also\nstrictly adheres to user-specific data boundaries. Each user\u2019s interaction\nwith the system is isolated, ensuring that the retriever component of the RAG\npipeline accesses only the information relevant and permitted for that\nparticular user. This approach is important in scenarios where sensitive or\nproprietary data is involved, as it safeguards against data leaks and privacy\nbreaches.\n\nIn this blog post, we will look into Building a Multi-Tenancy RAG System with\nLlamaIndex.\n\n##  Solving Multi-Tenancy Challenges\n\nThe key to managing Multi-Tenancy lies within the metadata. When indexing\ndocuments, we incorporate user-specific information into the metadata before\nadding it to the index. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1716, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65b9aeca-613f-4a52-bf71-ed062d7b8ee1": {"__data__": {"id_": "65b9aeca-613f-4a52-bf71-ed062d7b8ee1", "embedding": null, "metadata": {"filename": "building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.md", "extension": ".md", "title": "Building Multi-Tenancy RAG System with LlamaIndex", "date": "Jan 15, 2024", "url": "https://www.llamaindex.ai/blog/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a0fee36a-1d60-45bc-a806-3fd5d7e1a65d", "node_type": "4", "metadata": {"filename": "building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.md", "extension": ".md", "title": "Building Multi-Tenancy RAG System with LlamaIndex", "date": "Jan 15, 2024", "url": "https://www.llamaindex.ai/blog/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b"}, "hash": "667402b2ba4c5d200bf8947e8926eb677ce0a2af2ac24ff99f0099c3d4887f50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8553ee89-1e4f-4a85-b522-14eac71941ff", "node_type": "1", "metadata": {"filename": "building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.md", "extension": ".md", "title": "Building Multi-Tenancy RAG System with LlamaIndex", "date": "Jan 15, 2024", "url": "https://www.llamaindex.ai/blog/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b"}, "hash": "45aecf57b4ac89a2fbca51280a90ad90939711cd18d785a5b3c4b29cf0f16882", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fcacdd5b-b27b-4927-af75-b4fd9f305786", "node_type": "1", "metadata": {}, "hash": "227d2ee9b4e4fecb3727341ed26d0b4596295724c7bb35f12eee18287e149c3e", "class_name": "RelatedNodeInfo"}}, "text": "This ensures that each document is uniquely tied to an\nindividual user.\n\nDuring the query phase, the retriever uses this metadata to filter and only\naccess documents associated with the querying user. Subsequently, it performs\na semantic search to retrieve the most relevant information segments, or\n\u2018top_k chunks\u2019, for that user. By implementing this approach, we effectively\nprevent the unauthorized cross-referencing of private information between\ndifferent users, upholding the integrity and confidentiality of each user\u2019s\ndata.\n\nNow that we\u2019ve discussed the concept, let\u2019s dive into the construction of a\nMulti-Tenancy RAG system. For an in-depth step-by-step guide, feel free to\nfollow along with the subsequent instructions in our [ Google Colab Notebook\n](https://colab.research.google.com/github/run-\nllama/llama_index/blob/main/docs/examples/multi_tenancy/multi_tenancy_rag.ipynb)\n.\n\n##  Download Data:\n\nWe will use ` An LLM Compiler for Parallel Function Calling ` and ` Dense X\nRetrieval: What Retrieval Granularity Should We Use? ` papers for the\ndemonstrations.\n\n    \n    \n    !wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2312.04511.pdf\" -O \"llm_compiler.pdf\"\n    !wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2312.06648.pdf\" -O \"dense_x_retrieval.pdf\"\n\n##  Load Data:\n\nWe will load the data of ` LLMCompiler ` paper for user ` Jerry ` and ` Dense\nX Retrieval ` paper for user ` Ravi `\n\n    \n    \n    reader = SimpleDirectoryReader(input_files=['dense_x_retrieval.pdf'])\n    documents_jerry = reader.load_data()\n    \n    reader = SimpleDirectoryReader(input_files=['llm_compiler.pdf'])\n    documents_ravi = reader.load_data()\n\n##  Create An Empty Index:\n\nWe will initially create an empty index to which we can insert documents, each\ntagged with metadata containing the user information.\n\n    \n    \n    index = VectorStoreIndex.from_documents(documents=[])\n\n##  Ingestion Pipeline:\n\nThe IngestionPipeline is useful for data ingestion and performing\ntransformations, including chunking, metadata extraction, and more. ", "mimetype": "text/plain", "start_char_idx": 1716, "end_char_idx": 3760, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fcacdd5b-b27b-4927-af75-b4fd9f305786": {"__data__": {"id_": "fcacdd5b-b27b-4927-af75-b4fd9f305786", "embedding": null, "metadata": {"filename": "building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.md", "extension": ".md", "title": "Building Multi-Tenancy RAG System with LlamaIndex", "date": "Jan 15, 2024", "url": "https://www.llamaindex.ai/blog/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a0fee36a-1d60-45bc-a806-3fd5d7e1a65d", "node_type": "4", "metadata": {"filename": "building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.md", "extension": ".md", "title": "Building Multi-Tenancy RAG System with LlamaIndex", "date": "Jan 15, 2024", "url": "https://www.llamaindex.ai/blog/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b"}, "hash": "667402b2ba4c5d200bf8947e8926eb677ce0a2af2ac24ff99f0099c3d4887f50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65b9aeca-613f-4a52-bf71-ed062d7b8ee1", "node_type": "1", "metadata": {"filename": "building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.md", "extension": ".md", "title": "Building Multi-Tenancy RAG System with LlamaIndex", "date": "Jan 15, 2024", "url": "https://www.llamaindex.ai/blog/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b"}, "hash": "1ed52204202fe209b7061141cd18f53b37aa007c9ee1543c5beddf8f65d7492a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "840506da-759a-471a-8eaa-82ba31a344b2", "node_type": "1", "metadata": {}, "hash": "62b819969cf41e535ff332987281ffc57ac5692b69372aed0f86a6bb1f78fa79", "class_name": "RelatedNodeInfo"}}, "text": "Here we\nutilize it to create nodes, which are then inserted into the index.\n\n    \n    \n    pipeline = IngestionPipeline(\n        transformations=[\n            SentenceSplitter(chunk_size=512, chunk_overlap=20),\n        ]\n    )\n\n##  Update Metadata and Insert Documents:\n\nWe will update the metadata of the documents with each user and insert the\ndocuments into the index.\n\n    \n    \n    # For user Jerry\n    for document in documents_jerry:\n        document.metadata['user'] = 'Jerry'\n    \n    nodes = pipeline.run(documents=documents_jerry)\n    # Insert nodes into the index\n    index.insert_nodes(nodes)\n    \n    # For user Ravi\n    for document in documents_ravi:\n        document.metadata['user'] = 'Ravi'\n    \n    nodes = pipeline.run(documents=documents_ravi)\n    # Insert nodes into the index\n    index.insert_nodes(nodes)\n\n##  Define Query Engines:\n\nWe will define query engines for both the users with necessary filters.\n\n    \n    \n    # For Jerry\n    jerry_query_engine = index.as_query_engine(\n        filters=MetadataFilters(\n            filters=[\n                ExactMatchFilter(\n                    key=\"user\",\n                    value=\"Jerry\",\n                )\n            ]\n        ),\n        similarity_top_k=3\n    )\n    \n    # For Ravi\n    ravi_query_engine = index.as_query_engine(\n        filters=MetadataFilters(\n            filters=[\n                ExactMatchFilter(\n                    key=\"user\",\n                    value=\"Ravi\",\n                )\n            ]\n        ),\n        similarity_top_k=3\n    )\n\n##  Querying:\n\n    \n    \n    # Jerry has Dense X Rerieval paper and should be able to answer following question.\n    response = jerry_query_engine.query(\n        \"what are propositions mentioned in the paper?\"\n    )\n\n> The paper mentions propositions as an alternative retrieval unit choice.\n> Propositions are defined as atomic expressions of meanings in text that\n> correspond to distinct pieces of meaning in the text. They are minimal and\n> cannot be further split into separate propositions. Each proposition is\n> contextualized and self-contained, including all the necessary context from\n> the text to interpret its meaning. The paper demonstrates the concept of\n> propositions using an example about the Leaning Tower of Pisa, where the\n> passage is split into three propositions, each corresponding to a distinct\n> factoid about the tower.\n    \n    \n    # Ravi has LLMCompiler paper\n    response = ravi_query_engine.query(\"what are steps involved in LLMCompiler?\")\n\n", "mimetype": "text/plain", "start_char_idx": 3760, "end_char_idx": 6271, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "840506da-759a-471a-8eaa-82ba31a344b2": {"__data__": {"id_": "840506da-759a-471a-8eaa-82ba31a344b2", "embedding": null, "metadata": {"filename": "building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.md", "extension": ".md", "title": "Building Multi-Tenancy RAG System with LlamaIndex", "date": "Jan 15, 2024", "url": "https://www.llamaindex.ai/blog/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a0fee36a-1d60-45bc-a806-3fd5d7e1a65d", "node_type": "4", "metadata": {"filename": "building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.md", "extension": ".md", "title": "Building Multi-Tenancy RAG System with LlamaIndex", "date": "Jan 15, 2024", "url": "https://www.llamaindex.ai/blog/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b"}, "hash": "667402b2ba4c5d200bf8947e8926eb677ce0a2af2ac24ff99f0099c3d4887f50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fcacdd5b-b27b-4927-af75-b4fd9f305786", "node_type": "1", "metadata": {"filename": "building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.md", "extension": ".md", "title": "Building Multi-Tenancy RAG System with LlamaIndex", "date": "Jan 15, 2024", "url": "https://www.llamaindex.ai/blog/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b"}, "hash": "c0cd044a820e9b5d37c57cf45a6ed1e1e90e528a6440236c7b0f9b3056d081b4", "class_name": "RelatedNodeInfo"}}, "text": "> LLMCompiler consists of three key components: an LLM Planner, a Task\n> Fetching Unit, and an Executor. The LLM Planner identifies the execution\n> flow by defining different function calls and their dependencies based on\n> user inputs. The Task Fetching Unit dispatches the function calls that can\n> be executed in parallel after substituting variables with the actual outputs\n> of preceding tasks. Finally, the Executor executes the dispatched function\n> calling tasks using the associated tools. These components work together to\n> optimize the parallel function calling performance of LLMs.\n    \n    \n    # This should not be answered as Jerry does not have information about LLMCompiler\n    response = jerry_query_engine.query(\"what are steps involved in LLMCompiler?\")\n\n> I\u2019m sorry, but I couldn\u2019t find any information about the steps involved in\n> LLMCompiler in the given context.\n\nAs demonstrated, if Jerry queries the system regarding a document indexed by\nRavi, the system does not retrieve any answers from that document.\n\n#  What\u2019s Next?\n\nWe have included a [ MultiTenancyRAGPack ](https://github.com/run-llama/llama-\nhub/tree/main/llama_hub/llama_packs/multi_tenancy_rag) within the LlamaPacks\nand [ Replit template ](https://replit.com/@LlamaIndex/LlamaIndex-Multi-\nTenancy-RAG#README.md) which offers a Streamlit interface for hands-on\nexperience. Be sure to explore it.\n\n##  References:\n\n[ Multi-Tenancy RAG with Qdrant and LlamaIndex.\n](https://qdrant.tech/documentation/tutorials/llama-index-multitenancy/)\n\n", "mimetype": "text/plain", "start_char_idx": 6271, "end_char_idx": 7798, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9cf7c8b8-e4cf-4f7e-a882-e401956078d8": {"__data__": {"id_": "9cf7c8b8-e4cf-4f7e-a882-e401956078d8", "embedding": null, "metadata": {"filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c8973c1-3304-48b1-8580-1cb04ed8c797", "node_type": "4", "metadata": {"filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "51210ae3bef64a717cf6af26f81baec17f059a2f72450316fa6c6612126bc69d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cffb6521-86cf-48de-8b3d-2470ba9f3880", "node_type": "1", "metadata": {}, "hash": "048f869a51943db17c53a5782ce759ed0576df984c424bca987a7f06c635b54f", "class_name": "RelatedNodeInfo"}}, "text": "#  Introduction\n\nThe C3 Voice Assistant is my latest project aimed at making Large Language\nModel (LLM) and Retrieval-Augmented Generation (RAG) applications more\naccessible. This voice-activated assistant caters to a broad audience,\nincluding those facing typing challenges or accessibility issues.\n\n#  Features\n\n  * **Voice Activation:** Initiated by saying \u201cC3.\u201d Alternatively, users can click the blue ring to activate the listening mode of the app. The wake word \u201cC3\u201d is configurable and you can choose any other word. \n  * **Universal Accessibility:** Ideal for users preferring voice commands or facing typing challenges. \n  * **LLM Integration:** Capable of general queries and document-specific inquiries (e.g., Nvidia\u2019s FY 2023 10K report). \n  * **User-Friendly Interface:** The interface of the AI voice assistant is designed for simplicity and ease of use, focusing on voice chat interactions. It features a minimalistic and user-friendly React.js layout. Additionally, there is a convenient sidebar that displays the entire chat history in text format, allowing users to review and reflect on their interactions with the AI. \n\n#  The Tech Stack\n\nThe app is built on a robust and flexible tech stack that ensures a smooth,\nreliable, and efficient user experience. Here\u2019s an overview:\n\n  * **Frontend:** The user interface is a custom application developed using React.js. It\u2019s designed to be minimalistic yet highly functional, prioritizing ease of use and accessibility. \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1487, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cffb6521-86cf-48de-8b3d-2470ba9f3880": {"__data__": {"id_": "cffb6521-86cf-48de-8b3d-2470ba9f3880", "embedding": null, "metadata": {"filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c8973c1-3304-48b1-8580-1cb04ed8c797", "node_type": "4", "metadata": {"filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "51210ae3bef64a717cf6af26f81baec17f059a2f72450316fa6c6612126bc69d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9cf7c8b8-e4cf-4f7e-a882-e401956078d8", "node_type": "1", "metadata": {"filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "099da3d81a9a916fa0571eae001b1c8d8ab26340b4091f529602bbc7e216f453", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92429ec3-28cb-4f85-ae09-d00556a1b25e", "node_type": "1", "metadata": {}, "hash": "075c78fdc7f4e1d8dbeb1d69e25aef5ddf757eb07903e4635bbebe63b4b6d8c9", "class_name": "RelatedNodeInfo"}}, "text": "* **Backend:** The server-side operations are powered by Python Flask. I\u2019ve utilized the innovative \u2018create-llama\u2019 feature from LlamaIndex, which significantly streamlines the development process. \n  * **Hosting:** For a seamless performance, the frontend of the C3 Voice Assistant is hosted on Vercel. The backend, on the other hand, is deployed on Render, ensuring efficient management and operation of server-side tasks. \n\n", "mimetype": "text/plain", "start_char_idx": 1487, "end_char_idx": 1913, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "92429ec3-28cb-4f85-ae09-d00556a1b25e": {"__data__": {"id_": "92429ec3-28cb-4f85-ae09-d00556a1b25e", "embedding": null, "metadata": {"filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c8973c1-3304-48b1-8580-1cb04ed8c797", "node_type": "4", "metadata": {"filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "51210ae3bef64a717cf6af26f81baec17f059a2f72450316fa6c6612126bc69d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cffb6521-86cf-48de-8b3d-2470ba9f3880", "node_type": "1", "metadata": {"filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "6e69fa05541179e8ccb9211f6d6b926e87dc43b9c1aebb32617bdcac41eaf900", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "192daad8-2b82-44d7-8eaf-edcee61d1f3f", "node_type": "1", "metadata": {}, "hash": "9bdb03f5ba6abaea1977f1b5cd40b0d55f5ed3c4debfcf759da58e6d848891ad", "class_name": "RelatedNodeInfo"}}, "text": "#  Building the Frontend\n\nThe frontend, built with React.js, focuses on user interaction and\naccessibility. The ` App.js ` script incorporates features like wake word\nrecognition, speech-to-text conversion, state management, and dynamic UI\nelements like speech bubbles and spinners.\n\n##  1\\. Component and State Initialization\n\nThis section sets up the React component and initializes various states, such\nas ` appState ` to track the current state of the app (idle, listening,\nspeaking), and ` transcript ` to store the text transcribed from user speech.\n\n    \n    \n    import React, { useState, useRef, useEffect } from \"react\";\n    import \"./App.css\";\n    \n    const App = () =&gt; {\n      const [appState, setAppState] = useState(\"idle\");\n      const [transcript, setTranscript] = useState(\"\");\n      // Additional state and ref declarations...\n    };\n\n##  2\\. Speech Recognition Setup\n\nIn this useEffect hook, two speech recognition instances are initialized: one\nfor detecting the wake word \u201cC3\u201d and another for the main speech recognition.\nThis setup ensures that the app starts listening for commands when \u201cC3\u201d is\nmentioned.\n\nYou can easily swap \u201cC3\u201d with any other wake word of your choice.\n\n    \n    \n      useEffect(() =&gt; {\n        // Wake word listener setup\n        const WakeWordSpeechRecognition =\n          window.SpeechRecognition || window.webkitSpeechRecognition;\n        if (WakeWordSpeechRecognition &amp;&amp; !wakeWordRecognitionRef.current) {\n          wakeWordRecognitionRef.current = new WakeWordSpeechRecognition();\n          wakeWordRecognitionRef.current.continuous = true;\n          wakeWordRecognitionRef.current.interimResults = false;\n    \n          wakeWordRecognitionRef.current.onresult = (event) =&gt; {\n            const transcript = event.results[event.results.length - 1][0].transcript\n              .trim()\n              .toLowerCase();\n            if (transcript.includes(\"c3\")) {\n              toggleRecording(); // Start the main speech recognition process\n            }\n          };\n    \n          wakeWordRecognitionRef.current.start();\n        }\n    \n        // Main speech recognition setup\n        const SpeechRecognition =\n          window.SpeechRecognition || window.webkitSpeechRecognition;\n        if (SpeechRecognition &amp;&amp; !recognitionRef.current) {\n          recognitionRef.current = new SpeechRecognition();\n          recognitionRef.current.continuous = false;\n          recognitionRef.current.interimResults = false;\n    \n          recognitionRef.current.onresult = (event) =&gt; {\n            const lastResultIndex = event.results.length - 1;\n            const transcriptResult = event.results[lastResultIndex][0].transcript;\n            setTranscript(transcriptResult);\n            setAppState(\"playing\");\n            setShowSpeechBubble(true);\n            setTimeout(() =&gt; setShowSpeechBubble(false), speechBubbleTimeout);\n            fetchResponseFromLLM(transcriptResult);\n          };\n    \n          recognitionRef.current.onend = () =&gt; {\n            setShowSpinner(true);\n          };\n        }\n      }, []);\n\n##  3\\. Handling User Speech and Response\n\n` toggleRecording ` controls the speech recognition process, while `\nfetchResponseFromLLM ` sends the user's speech to the LLM backend and handles\nthe response. This response is then spoken out via speech synthesis and also\nused to update the chat history displayed on the UI.\n\n    \n    \n     const toggleRecording = () =&gt; {\n        try {\n          if (appState === \"idle\") {\n            recognitionRef.current.start();\n            setAppState(\"listening\");\n          } else if (appState === \"listening\") {\n            recognitionRef.current.stop();\n          }\n        } catch (error) {\n        }\n      };\n    \n    \n      const fetchResponseFromLLM = async (text) =&gt; {\n        try {\n          const response = await fetch(\n            `https://c3-python-nostream.onrender.com/api/chat`,\n            {\n              method: \"POST\",\n              headers: { \"Content-Type\": \"application/json\" },\n              body: JSON.stringify({\n                messages: [\n                  {\n                    role: \"user\",\n                    content:\n                      \"You are an AI voice assistant called C3. You can provide any general information as well as answer basic questions about the Nvidia 10k report for year ended Jan 2023\" +\n                      text,\n                  },\n                ],\n              }),\n            }\n          );\n          const data = await response.json();\n    \n          setChatHistory((prevHistory) =&gt; [\n            ...prevHistory,\n            { query: text, response: data.result.content },\n          ]);\n          speak(data.result.content);\n        } catch (error) {\n          console.error(\"Error communicating with LLM:\", error);\n        }\n      };\n\n##  4\\. Speech Synthesis\n\nThe ` speak ` function takes the text response from the LLM and uses the\nSpeechSynthesis API to read it aloud, providing an interactive experience for\nthe user.\n\n    \n    \n      const speak = (text) =&gt; {\n        if (synthRef.current &amp;&amp; text) {\n          const utterance = new SpeechSynthesisUtterance(text);\n    \n          const voices = window.speechSynthesis.getVoices();\n          if (voices.length &gt; 0) {\n            utterance.voice = voices[3]; // You can change this to select different voices\n          }\n    \n          utterance.onstart = () =&gt; {\n            console.log(\"TTS starts speaking\");\n            setShowSpinner(false);\n          };\n    \n          utterance.onend = () =&gt; {\n            setAppState(\"idle\");\n            if (wakeWordRecognitionRef.current) {\n              wakeWordRecognitionRef.current.start(); // Restart wake word listener after speaking\n            }\n          };\n          synthRef.current.speak(utterance);\n        }\n\n##  5\\. UI Rendering\n\nThe return statement of the ` App ` function contains the JSX code for\nrendering the app's UI. This includes buttons for starting/stopping the voice\ninteraction, a display area for the transcript, and a chat sidebar showing the\nhistory of interactions.\n\nBy combining voice recognition, LLM integration, and speech synthesis, this\nfrontend component provides a comprehensive and accessible interface for\ninteracting with the C3 Voice Assistant.\n\n#  Backend Server Setup\n\n  1. Initialize Create-Llama: Run ` npx create-llama@latest ` in your terminal. \n  2. ", "mimetype": "text/plain", "start_char_idx": 1913, "end_char_idx": 8331, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "192daad8-2b82-44d7-8eaf-edcee61d1f3f": {"__data__": {"id_": "192daad8-2b82-44d7-8eaf-edcee61d1f3f", "embedding": null, "metadata": {"filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c8973c1-3304-48b1-8580-1cb04ed8c797", "node_type": "4", "metadata": {"filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "51210ae3bef64a717cf6af26f81baec17f059a2f72450316fa6c6612126bc69d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92429ec3-28cb-4f85-ae09-d00556a1b25e", "node_type": "1", "metadata": {"filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}, "hash": "158b792d8c9cb528fa71870316dd67e5a100903cd4ccecf2b2756e722cc50d44", "class_name": "RelatedNodeInfo"}}, "text": "Follow the prompts to set up a Python FastAPI backend, which we can be integrated with our frontend. \n  3. Use ` poetry install ` and ` poetry shell ` to prepare the environment. \n  4. Create a ` .env ` file with ` OPENAI_API_KEY=<openai_api_key> ` . \n  5. Generate Embeddings (optional): If a ` ./data ` directory exists, run ` python app/engine/generate.py ` . \n  6. Execute ` python main.py ` to start the server. \n  7. Test the API: Use ` curl --location 'localhost:8000/api/chat' --header 'Content-Type: application/json' --data '{ \"messages\": [{ \"role\": \"user\", \"content\": \"Hello\" }] }' ` to test. \n  8. Modify API behavior in ` app/api/routers/chat.py ` . The server supports CORS for all origins, alterable with the ` ENVIRONMENT=prod ` setting. \n\n#  Integration\n\nOnce the backend server is set up, integrating it with the frontend is\nstraightforward. Simply update the ` fetchResponseFromLLM ` function in your\nfrontend's ` App.js ` to call the backend server URL. This change ensures that\nwhen the frontend makes a request, it communicates with your newly configured\nbackend, thus effectively integrating the two components.\n\n#  Final Thoughts\n\nWrapping up, the C3 Voice Assistant isn\u2019t just a tech showcase; it\u2019s a stride\ntowards democratizing AI. It\u2019s about making powerful AI tools, like LLMs and\nRAG, accessible and user-friendly. This project is more than lines of code \u2014\nit\u2019s a push to break down tech barriers and empower everyone.\n\nYour thoughts and feedback are invaluable \u2014 let\u2019s make AI more accessible\ntogether!\n\nLink to Github Repo: [ Frontend ](https://github.com/AI-ANK/C3-Voice-\nAssistant-UI) and [ Backend ](https://github.com/AI-ANK/c3-python-nostream)\n\n[ Connect with Me on LinkedIn\n](https://www.linkedin.com/in/harshadsuryawanshi/)\n\n[ Linkedin Post ](https://www.linkedin.com/posts/harshadsuryawanshi_ai-\nllamaindex-\ngpt3-activity-7149796976442740736-1lXj?utm_source=share&utm_medium=member_desktop)\n\n", "mimetype": "text/plain", "start_char_idx": 8331, "end_char_idx": 10263, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8d32e65-742f-47af-94ba-b9dc5978b82d": {"__data__": {"id_": "a8d32e65-742f-47af-94ba-b9dc5978b82d", "embedding": null, "metadata": {"filename": "join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb.md", "extension": ".md", "title": "Free Advanced RAG Certification course with Activeloop and LlamaIndex", "date": "Jan 11, 2024", "url": "https://www.llamaindex.ai/blog/join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac7e9570-c8ee-402d-a176-7922bacf0a97", "node_type": "4", "metadata": {"filename": "join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb.md", "extension": ".md", "title": "Free Advanced RAG Certification course with Activeloop and LlamaIndex", "date": "Jan 11, 2024", "url": "https://www.llamaindex.ai/blog/join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb"}, "hash": "ada8a57d3b48161aadff13802f6d3baac8d4646973bf3de15a231ac533836a50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40e3233c-b1f9-43da-9d25-deeb842164cd", "node_type": "1", "metadata": {}, "hash": "197a220b1d513cd8ea7e0294788875a93641319730f015319697239e902c9fa3", "class_name": "RelatedNodeInfo"}}, "text": "> Master LlamaIndex with our course developed in collaboration with\n> Activeloop, TowardsAI, & Intel. Learn to apply advanced retrieval across\n> industries in 40+ lessons. This is a guest post from Activeloop.\n\nLlamaIndex is proud to collaborate with Activeloop, Towards AI, and the Intel\nDisruptor Initiative to offer a free course on \u201c [ Advanced Retrieval\nAugmented Generation for Production ](https://learn.activeloop.ai/courses/rag)\n,\u201d a part of the Gen AI 360: Foundational Model Certification series. This\ncomprehensive course takes a hands-on approach to applying RAG techniques\nacross various industries, including legal, biomedical, healthcare,\ne-commerce, and finance.\n\nThe free course is designed for practical learning and invites participants to\ntackle real business challenges, such as developing a multi-modal AI shopping\nassistant. The course has over 40 lessons, 7 interactive projects, and 2 hours\nof video content, including from LlamaIndex CEO Jerry Liu. In 20+ hours of\nlearning, the curriculum is geared towards enabling GenAI tinkerers,\nprofessionals, and executives to apply LlamaIndex and Deep Lake, [ Activeloop\n](http://activeloop.ai) \u2019s database for AI in production. Participants who\ncomplete the course will be awarded a certificate at no cost.\n\n> In the rapidly evolving business landscape, leveraging Retrieval Augmented\n> Generation (RAG) tools like Llamalndex & Deep Lake by Activeloop is\n> essential for enterprises seeking a competitive edge in GenAI. This course\n> is tailored to quickly upskill your team in GenAI workflows, emphasizing the\n> integration of Activeloop\u2019s advanced features like Deep Memory with\n> Llamalndex for unmatched retrieval accuracy. It\u2019s a strategic investment to\n> enhance your team\u2019s capabilities, ensuring your enterprise stays at the\n> forefront of AI innovation\n>\n> \\- Jerry Liu, CEO & Co-Founder, LlamaIndex\n\n#  Highlighted projects across several industries:\n\n  * **Healthcare:** Advanced RAG for Pill Searching. Combine cutting-edge NLP and computer vision techniques to build an AI app that recognizes pills from images, and lists their side effects and instructions to use. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2148, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40e3233c-b1f9-43da-9d25-deeb842164cd": {"__data__": {"id_": "40e3233c-b1f9-43da-9d25-deeb842164cd", "embedding": null, "metadata": {"filename": "join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb.md", "extension": ".md", "title": "Free Advanced RAG Certification course with Activeloop and LlamaIndex", "date": "Jan 11, 2024", "url": "https://www.llamaindex.ai/blog/join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac7e9570-c8ee-402d-a176-7922bacf0a97", "node_type": "4", "metadata": {"filename": "join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb.md", "extension": ".md", "title": "Free Advanced RAG Certification course with Activeloop and LlamaIndex", "date": "Jan 11, 2024", "url": "https://www.llamaindex.ai/blog/join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb"}, "hash": "ada8a57d3b48161aadff13802f6d3baac8d4646973bf3de15a231ac533836a50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8d32e65-742f-47af-94ba-b9dc5978b82d", "node_type": "1", "metadata": {"filename": "join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb.md", "extension": ".md", "title": "Free Advanced RAG Certification course with Activeloop and LlamaIndex", "date": "Jan 11, 2024", "url": "https://www.llamaindex.ai/blog/join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb"}, "hash": "9d348628a31ad524522a3bd8ace3f99e982b0e4eeeec103b2bc2aecfeb8d2480", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b8bc2e4-461c-4678-8845-415d5812e559", "node_type": "1", "metadata": {}, "hash": "b5a383676876781451ae768a807e97263e8a6e20196ffb12c975ee3e9df5c908", "class_name": "RelatedNodeInfo"}}, "text": "This project offers hands-on experience with the latest AI technologies like Segment Anything or GPT-4-vision. \n  * **Legal:** Patent Generation and Search Engine. Gain practical knowledge in constructing a system like PatentPT, which incorporates a fine-tuned LLM to search or create patents. Learn how to build a meta-agent to smartly route user inquiries, ensuring a fluid chat experience with a database of 8 million USPTO patents for comprehensive retrieval and generation capabilities. \n  * **E-commerce:** AI-Powered Shopping Assistant for Outfit Recommendations. Build a multi-modal AI assistant that curates outfit suggestions for any occasion, weather, or budget! \n\n#  What will I learn?\n\n  * **Challenges with Naive RAG:** We\u2019ll address common issues such as low precision, recall, and suboptimal response generation. Strategies for refining data processing, enhancing embedding models, refining retrieval algorithms, and optimizing prompt usage will be explored to improve RAG system performance. \n  * **Advanced RAG with LlamaIndex:** Delve into basic and advanced RAG methods using LlamaIndex. The course covers the essential aspects of LlamaIndex required for RAG application development, complemented by Activeloop\u2019s Deep Memory module, which natively integrates seamlessly with LlamaIndex to enhance retrieval accuracy by an average of 22%. Topics will range from small to large-scale retrieval, handling structured and unstructured data, and querying techniques. \n  * **RAG Agents:** This module focuses on the creation and application of RAG agents with LlamaIndex, including advanced querying, summarizing databases, and designing AI assistants using various APIs. \n\nProduction-grade apps: Learn to fine-tune the LlamaIndex RAG pipeline for\nprofessional deployment, evaluate RAG systems crafted with LlamaIndex, and\nensure your models\u2019 observability and effectiveness.\n\nHere\u2019s a brief introduction to the course by Louis Bouchard from TowardsAI\nteam.\n\n", "mimetype": "text/plain", "start_char_idx": 2148, "end_char_idx": 4120, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b8bc2e4-461c-4678-8845-415d5812e559": {"__data__": {"id_": "1b8bc2e4-461c-4678-8845-415d5812e559", "embedding": null, "metadata": {"filename": "join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb.md", "extension": ".md", "title": "Free Advanced RAG Certification course with Activeloop and LlamaIndex", "date": "Jan 11, 2024", "url": "https://www.llamaindex.ai/blog/join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac7e9570-c8ee-402d-a176-7922bacf0a97", "node_type": "4", "metadata": {"filename": "join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb.md", "extension": ".md", "title": "Free Advanced RAG Certification course with Activeloop and LlamaIndex", "date": "Jan 11, 2024", "url": "https://www.llamaindex.ai/blog/join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb"}, "hash": "ada8a57d3b48161aadff13802f6d3baac8d4646973bf3de15a231ac533836a50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40e3233c-b1f9-43da-9d25-deeb842164cd", "node_type": "1", "metadata": {"filename": "join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb.md", "extension": ".md", "title": "Free Advanced RAG Certification course with Activeloop and LlamaIndex", "date": "Jan 11, 2024", "url": "https://www.llamaindex.ai/blog/join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb"}, "hash": "d953da30f0d71d5b469db0ed0b3d9ea5b841fc594567f66edfd13e2a52f766c7", "class_name": "RelatedNodeInfo"}}, "text": "#  Who Should Enroll?\n\nThis course is ideal for aspiring AI professionals, executives, and\nenthusiasts eager to apply AI in practical scenarios. Whether you want to\nenhance your organization\u2019s AI capabilities or expand your knowledge, this\ncourse offers valuable hands-on experience. A basic understanding of coding\nand Python is recommended.\n\n#  Complimentary Free Trial of Deep Lake\n\nAs a part of the course, all course takers can redeem a free extended trial of\none month for the Activeloop Starter and Growth plans by redeeming the\n**_GENAI360LLAMA_ ** promo code at checkout. Check out the following video to\n[ learn more\n](https://www.loom.com/share/72f563abfe2544a587dd77cd34ece135?sid=f5079a31-ad13-4b9f-a030-429b27a8703c)\n.\n\nJoin thousands of AI engineers in mastering master [ Retrieval Augmented\nGeneration with LlamaIndex ](http://learn.activeloop.ai/courses/rag) . Enroll\nfor free today!\n\n", "mimetype": "text/plain", "start_char_idx": 4120, "end_char_idx": 5022, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "41f386be-0356-4202-9fea-06b51384e45e": {"__data__": {"id_": "41f386be-0356-4202-9fea-06b51384e45e", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-01-09-6209000da2e6.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201309", "date": "Jan 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-09-6209000da2e6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c059752a-faa0-43e5-8e13-9df793b05d79", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-01-09-6209000da2e6.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201309", "date": "Jan 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-09-6209000da2e6"}, "hash": "74923dd63e9c21f60555ae9e8714df0037dfcdb08304e08a4c725457ea16d830", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9ea4b57-cc2c-4a7b-9735-27521df6f2e8", "node_type": "1", "metadata": {}, "hash": "9cdf77e707d3a3cb5f6a62976bfb46646d42c19890fb57c21590b49cfac159cd", "class_name": "RelatedNodeInfo"}}, "text": "Hola, LlamaIndex Lovers ,\n\nWelcome to another thrilling week at LlamaIndex, filled with vibrant community\ncontributions and enriching educational content. Immerse yourself in our\nengaging tutorials, guides, community demos, and webinars, all crafted to\namplify your LlamaIndex experience. Before we jump into our latest updates,\nwe\u2019re thrilled to share two major announcements:\n\n\u200d **Join Our LlamaIndex Community Office Hours** : Struggling with complex\nLLM/RAG queries or have feedback that our documentation doesn\u2019t cover? [\nRegister ](https://t.co/A16nitoIgV) for our community office hours for a\nchance to have an enlightening conversation and get your questions answered!\n\n[ **Explore Our Open-Source Roadmap for 2024** : ](https://github.com/run-\nllama/llama_index/discussions/9888) We\u2019re excited to unveil our ambitious\nroadmap for the LlamaIndex ecosystem. Over the next 3\u20136 months, we aim to\nenhance LlamaIndex\u2019s production readiness, accessibility, and its advanced\nfeatures, including RAG, agents, and more. This [ living document\n](https://github.com/run-llama/llama_index/discussions/9888) is available on\nour GitHub discussions page \u2014 a must-visit to be part of our exciting journey!\n\nAdditionally, if you\u2019ve been working on an interesting project, written an\ninsightful article, or created a captivating video, we\u2019d love to hear about\nit! Please share your work with us at [ news@llamaindex.ai\n](mailto:news@llamaindex.ai) . ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1440, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9ea4b57-cc2c-4a7b-9735-27521df6f2e8": {"__data__": {"id_": "c9ea4b57-cc2c-4a7b-9735-27521df6f2e8", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-01-09-6209000da2e6.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201309", "date": "Jan 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-09-6209000da2e6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c059752a-faa0-43e5-8e13-9df793b05d79", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-01-09-6209000da2e6.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201309", "date": "Jan 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-09-6209000da2e6"}, "hash": "74923dd63e9c21f60555ae9e8714df0037dfcdb08304e08a4c725457ea16d830", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41f386be-0356-4202-9fea-06b51384e45e", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-01-09-6209000da2e6.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201309", "date": "Jan 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-09-6209000da2e6"}, "hash": "8b1418d73d6ffc8c716a4709e998315b78ac19adf6e2e61193e063decefe0a38", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8491b99-8663-41b8-b792-4c866442ec77", "node_type": "1", "metadata": {}, "hash": "ea6b9884df4450a68bdfb4a3851755e03e06718214d0adbfa55bf4cde8e12001", "class_name": "RelatedNodeInfo"}}, "text": "And remember to subscribe to our newsletter\nthrough our [ website ](https://www.llamaindex.ai/) to get all these exciting\nupdates straight to your inbox\n\n**The highlights:**\n\n  1. **Query Pipelines** : Introducing a new declarative API for effortless orchestration of simple to complex RAG query workflows. [ Docs ](https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/root.html#) , [ Blogpost ](/introducing-query-pipelines-025dc2bb0537) , [ Tweet ](https://x.com/llama_index/status/1744406288724017278?s=20) . \n  2. **ETL Pipeline Launch** : New repository for setting up production ETL pipelines in RAG/LLM apps, boasting a 4x speed boost and integrating Hugging Face, RabbitMQ, and AWS EKS. [ Github Repo ](https://github.com/run-llama/llamaindex_aws_ingestion) , [ Blogpost ](/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716) , [ Tweet ](https://x.com/llama_index/status/1742226327900721168?s=20) . \n  3. **Multimodal ReAct Agent** : Launch of an agent capable of processing text and images, enhancing RAG pipeline and web search functionalities using GPT-4V. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/mm_agent.ipynb) , [ Tweet ](https://x.com/llama_index/status/1742588989884989477?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 1440, "end_char_idx": 2713, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8491b99-8663-41b8-b792-4c866442ec77": {"__data__": {"id_": "a8491b99-8663-41b8-b792-4c866442ec77", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-01-09-6209000da2e6.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201309", "date": "Jan 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-09-6209000da2e6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c059752a-faa0-43e5-8e13-9df793b05d79", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-01-09-6209000da2e6.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201309", "date": "Jan 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-09-6209000da2e6"}, "hash": "74923dd63e9c21f60555ae9e8714df0037dfcdb08304e08a4c725457ea16d830", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c9ea4b57-cc2c-4a7b-9735-27521df6f2e8", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-01-09-6209000da2e6.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201309", "date": "Jan 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-09-6209000da2e6"}, "hash": "2fd98b9b857736afd3ca8112a6ab536299a2331783a06fed7d4c9d9214feff69", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37cfcf4c-06ec-469e-89b2-41d186e54067", "node_type": "1", "metadata": {}, "hash": "865c370c994063284a5fbba757a64fbc64b39908919cf2015bd3933197098dbe", "class_name": "RelatedNodeInfo"}}, "text": "4. **RAGatouille LlamaPack** : Introduction of an easy-to-use pack for ColBERT retrieval, enabling one-line code integration in LlamaIndex RAG pipelines. [ Docs ](https://llamahub.ai/l/llama_packs-ragatouille_retriever?from=llama_packs) , [ Tweet ](https://x.com/llama_index/status/1743076579302105338?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 2713, "end_char_idx": 3026, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37cfcf4c-06ec-469e-89b2-41d186e54067": {"__data__": {"id_": "37cfcf4c-06ec-469e-89b2-41d186e54067", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-01-09-6209000da2e6.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201309", "date": "Jan 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-09-6209000da2e6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c059752a-faa0-43e5-8e13-9df793b05d79", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-01-09-6209000da2e6.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201309", "date": "Jan 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-09-6209000da2e6"}, "hash": "74923dd63e9c21f60555ae9e8714df0037dfcdb08304e08a4c725457ea16d830", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8491b99-8663-41b8-b792-4c866442ec77", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-01-09-6209000da2e6.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201309", "date": "Jan 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-09-6209000da2e6"}, "hash": "8cc5782c9806e18fb545cc512f7925f63ff77563268cfda1e82e62d89bebdc59", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b1757a6-9975-490f-a6e8-fe72da5bdd61", "node_type": "1", "metadata": {}, "hash": "dd7134309aa3741ee672ae0c66591c80b1f46bfe480c6bd21b82ef4b7d1dde76", "class_name": "RelatedNodeInfo"}}, "text": "5. [ **Advanced RAG Cheat Sheet** ](/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b) : A comprehensive cheat sheet with techniques for RAG enhancement, perfect for both new and experienced LLM users. \n\n**Feature Releases and Enhancements:**\n\n  * We have introduced Query Pipelines, a declarative API designed to simplify the creation and customization of advanced RAG workflows. This tool enables the orchestration of query workflows, ranging from basic sequential chains to complex DAGs, tailored to specific use cases. [ Docs ](https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/root.html#) , [ Blogpost ](/introducing-query-pipelines-025dc2bb0537) , [ Tweet ](https://x.com/llama_index/status/1744406288724017278?s=20) . \n  * We have launched a repository for easily setting up a production ETL pipeline for RAG/LLM apps, offering a 4x speed increase over laptop-based operations. This solution integrates Hugging Face, RabbitMQ, Llama Index, and AWS EKS, providing fast document indexing and efficient data handling, complete with an AWS Lambda API endpoint. Ideal for RAG apps transitioning to production, especially on AWS. [ Github Repo ](https://github.com/run-llama/llamaindex_aws_ingestion) , [ Blogpost ](/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716) , [ Tweet ](https://x.com/llama_index/status/1742226327900721168?s=20) . \n  * We have launched the Multimodal ReAct Agent, combining GPT-4V with the ability to process both text and images. This agent can perform tasks like querying a RAG pipeline or conducting web searches based on visual and textual inputs. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/mm_agent.ipynb) , [ Tweet ](https://x.com/llama_index/status/1742588989884989477?s=20) . \n  * RAGatouille LlamaPack: RAGatouille simplifies the use of ColBERT, a more advanced retrieval model compared to dense embedding-based retrieval techniques. This pack allows you to build an end-to-end LlamaIndex RAG pipeline with just one line of code by ingesting documents using any of our 150+ data loaders, combined with your preferred LLM for response synthesis. [ Docs ](https://llamahub.ai/l/llama_packs-ragatouille_retriever?from=llama_packs) , [ Tweet ](https://x.com/llama_index/status/1743076579302105338?s=20) . \n  * We have integrated with Pathway\u2019s open data processing framework which enables us to handle dynamic data sources in production, automatically updating indexes based on real-time changes, ensuring up-to-date and accurate query responses. [ Docs ](https://docs.llamaindex.ai/en/stable/examples/data_connectors/PathwayReaderDemo.html) , [ Tweet ](https://x.com/llama_index/status/1741862685103579476?s=20) . \n  * [ **Ian McCrystal** ](https://twitter.com/ianmst) has added the StripeDocsLoader to LlamaHub, enabling a quick setup of RAG over Stripe\u2019s documentation using Llama Index. [ Docs ](https://llamahub.ai/l/stripe_docs) . \n  * [ Jeremy Dyer ](https://twitter.com/mightyjeremy) has integrated NVIDIA\u2019s Triton Inference Server which allows you to run optimized inference on any AI framework. It supports the TensorRT-LLM backend, enhancing LLM performance on Nvidia GPUs. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/nvidia_triton.ipynb) , [ Tweet ](https://x.com/llama_index/status/1744044446029901943?s=20) . \n\n**Community Demos** :\n\n  * Context-Augmented Agent for Food Delivery: A full-stack application guide by lucastonon for creating an RAG agent. This tool performs in-browser tasks like opening restaurant pages and adding food to carts, purely via voice commands, integrating with Llama Index, Pinecone, OpenAI\u2019s Whisper, LLMs, Function Calling, vue.js, and FastAPI. [ Github Repo ](https://github.com/lucastononro/llm-food-delivery) , [ Tweet ](https://x.com/llama_index/status/1741987664272986534?s=20) . \n  * [ GRDN.AI ](https://medium.com/@dheymann314/ai-infused-optimization-in-the-wild-developing-a-companion-planting-app-357e5da29d10) : A fascinating side project from Danielle Heymann, using a genetic algorithm and LLM to optimize plant placement based on compatibility. This project harnesses local models from HuggingFace, accessed through LlamaIndex for the LLM part, combining traditional mathematical strategies with LLMs. [ Blogpost ](https://medium.com/@dheymann314/ai-infused-optimization-in-the-wild-developing-a-companion-planting-app-357e5da29d10) , [ Tweet ](https://x.com/llama_index/status/1742703399081271555?s=20) . \n  * [ Build an AI Shopping Assistant with RAG and Agents ](https://www.activeloop.ai/resources/use-llama-index-to-build-an-ai-shopping-assistant-with-rag-and-agents/) : This assistant can analyze a picture of an item and suggest weather-appropriate accessories. The work by D. Kiedanski and Lucas Micol from Tryolabs explains how to transform APIs into problem-solving tools for a LlamaIndex agent. \n\n**Guides:**\n\n  * [ Guide ](/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b) to Advanced RAG: Our comprehensive cheat sheet offers insights into improving RAG with techniques like optimized retrieval, effective document use in generation, and interleaving generation with retrieval. Ideal for both new and seasoned LLM users, it\u2019s a must-have resource, complete with LlamaIndex links. \n  * [ Guide ](https://github.com/NVIDIA/GenerativeAIExamples/blob/main/notebooks/04_llamaindex_hier_node_parser.ipynb) to building advanced RAG CHATBOT with NVIDIA\u2019S TensorRT-LLM: This chatbot is designed to maintain contiguous document or code blocks, avoiding awkward chunking. It features a stack combining Llama Index\u2019s auto-merging retriever with NVIDIA\u2019s TensorRT-LLM and a custom postprocessor, optimized for RAG using open-source models. \n\n**Tutorials:**\n\n  * BentoML [ tutorial ](https://www.bentoml.com/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm) on Building An Intelligent Query-Response System with LlamaIndex and OpenLLM. \n  * [ Akash Mathur ](https://www.linkedin.com/in/akashmathur22/) [ tutorial ](https://akash-mathur.medium.com/advanced-rag-optimizing-retrieval-with-additional-context-metadata-using-llamaindex-aeaa32d7aa2f) on Advanced RAG: Optimizing Retrieval with Additional Context & MetaData using LlamaIndex. \n\n**Webinars:**\n\n  * Weights & Biases [ podcast ](https://www.youtube.com/watch?v=xejCaLsYzV4) with Jerry Liu on Revolutionizing AI Data Management. \n\n**Calling all enterprises:**\n\nAre you building with LlamaIndex? We are working hard to make LlamaIndex, even\nmore, Enterprise-ready and have sneak peeks at our upcoming products available\nfor partners. Interested? ", "mimetype": "text/plain", "start_char_idx": 3026, "end_char_idx": 9701, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b1757a6-9975-490f-a6e8-fe72da5bdd61": {"__data__": {"id_": "4b1757a6-9975-490f-a6e8-fe72da5bdd61", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-01-09-6209000da2e6.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201309", "date": "Jan 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-09-6209000da2e6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c059752a-faa0-43e5-8e13-9df793b05d79", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-01-09-6209000da2e6.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201309", "date": "Jan 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-09-6209000da2e6"}, "hash": "74923dd63e9c21f60555ae9e8714df0037dfcdb08304e08a4c725457ea16d830", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37cfcf4c-06ec-469e-89b2-41d186e54067", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-01-09-6209000da2e6.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201309", "date": "Jan 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-09-6209000da2e6"}, "hash": "c4e94b85fb50b99c55f22319296b2e6be65c68e8f5c1d5a08ac44954607bee2f", "class_name": "RelatedNodeInfo"}}, "text": "[ Get in touch.\n](https://docs.google.com/forms/d/e/1FAIpQLScBNdM2a_fn8UZOKmFQt6lBsrd1o6FflvsdPH-\nPn3JkdlN_Rg/viewform)\n\n", "mimetype": "text/plain", "start_char_idx": 9701, "end_char_idx": 9822, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b33f739-902c-47d8-894d-7523428b86e9": {"__data__": {"id_": "7b33f739-902c-47d8-894d-7523428b86e9", "embedding": null, "metadata": {"filename": "introducing-query-pipelines-025dc2bb0537.md", "extension": ".md", "title": "Introducing Query Pipelines", "date": "Jan 8, 2024", "url": "https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ea01df2-9d80-441c-9644-23133cd296e9", "node_type": "4", "metadata": {"filename": "introducing-query-pipelines-025dc2bb0537.md", "extension": ".md", "title": "Introducing Query Pipelines", "date": "Jan 8, 2024", "url": "https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537"}, "hash": "1d10a8242ea930dc9e0522a78e6390d701ed21adc725131456e1473a5c92827c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "13df4dc0-ddbe-445e-80a9-7e75c81b3c47", "node_type": "1", "metadata": {}, "hash": "c6e282ab2d06c4e91a3ccfcfad1a50c1f5947a4bcefe6fff69756d2ed0e81397", "class_name": "RelatedNodeInfo"}}, "text": "Today we introduce **Query Pipelines,** a new declarative API within\nLlamaIndex that allows you to **concisely orchestrate simple-to-advanced query\nworkflows over your data for different use cases** (RAG, structured data\nextraction, and more).\n\nAt the core of all this is our ` QueryPipeline ` abstraction. It can take in\nmany LlamaIndex modules (LLMs, prompts, query engines, retrievers, itself). It\ncan create a computational graph over these modules (e.g. a sequential chain\nor a DAG). It has callback support and native support with our [ observability\npartners\n](https://docs.llamaindex.ai/en/latest/module_guides/observability/observability.html)\n.\n\nThe end goal is that it\u2019s even easier to build LLM workflows over your data.\nCheck out our comprehensive [ introduction guide\n](https://docs.llamaindex.ai/en/latest/examples/pipeline/query_pipeline.html)\n, as well as our [ docs page\n](https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/root.html)\nfor more details.\n\nExample `QueryPipeline` setup for an advanced RAG pipeline\n\n#  Context\n\nOver the past year AI engineers have developed customized, complex\norchestration flows with LLMs to solve a variety of different use cases. Over\ntime some common patterns developed. At a top-level, paradigms emerged to\nquery a user\u2019s data \u2014 this includes RAG (in a narrow definition) to query\nunstructured data, and text-to-SQL to query structured data. Other paradigms\nemerged around use cases like structured data extraction (e.g. prompt the LLM\nto output JSON, and parse it), prompt chaining (e.g. chain-of-thought), and\nagents that could interact with external services (combine prompt chaining\n\n**There is a lot of query orchestration in RAG.** Even within RAG itself there\ncan be a lot of work to build an advanced RAG pipeline optimized for\nperformance. Starting from the user query, we may want to run query\nunderstanding/transformations (re-writing, routing). We also may want to run\nmulti-stage retrieval algorithms \u2014 e.g. top-k lookup + reranking. We may also\nwant to use prompts + LLMs to do response synthesis in different ways. Here\u2019s\na great blog on advanced RAG [ components\n](https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-\noverview-04d193d8fec6) .\n\n[ Source: \u201cAdvanced RAG Techniques: an Illustrated Overview\u201d by Ivan Ilin\n](https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-\noverview-04d193d8fec6)\n\n**RAG has become more modular:** Instead of a single way to do retrieval/RAG,\ndevelopers are encouraged to pick and choose the best modules for their use\ncases. This sentiment is echoed in the [ RAG Survey paper by Gao et al.\n](https://arxiv.org/pdf/2312.10997.pdf)\n\nThis leads to creative new patterns like [ DSP\n](https://github.com/stanfordnlp/dspy) , [ Rewrite-Retrieve-Read\n](https://arxiv.org/abs/2305.14283) , or [ interleaving retrieval+generation\nmultiple times ](https://arxiv.org/abs/2305.15294) .\n\n##  Previous State of LlamaIndex\n\nLlamaIndex itself has hundreds of RAG guides and 16+ Llama Pack recipes\nletting users setup [ different RAG pipelines ](/a-cheat-sheet-and-some-\nrecipes-for-building-advanced-rag-803a9d94c41b) , and has been at the\nforefront of establishing advanced RAG patterns.\n\nWe\u2019ve also exposed low-level modules such as [ LLMs\n](https://docs.llamaindex.ai/en/latest/module_guides/models/llms.html) , [\nprompts\n](https://docs.llamaindex.ai/en/stable/module_guides/models/prompts.html#prompts)\n, [ embeddings\n](https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html) ,\n[ postprocessors\n](https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/root.html)\nand easy subclassability of core components like [ retrievers\n](https://docs.llamaindex.ai/en/stable/examples/query_engine/CustomRetrievers.html)\nand [ query engines\n](https://docs.llamaindex.ai/en/stable/examples/query_engine/custom_query_engine.html)\nso that users can define their own workflows.\n\nBut up until now, we didn\u2019t explicitly have an orchestration abstraction.\nUsers were responsible for figuring out their own workflows by reading the API\nguides of each module, converting outputs to the right inputs, and using the\nmodules imperatively.\n\n#  Query Pipeline\n\nAs a result, our QueryPipeline provides a declarative query orchestration\nabstraction. You can use it to compose both sequential chains and directed\nacyclic graphs (DAGs) of arbitrary complexity.\n\nYou can already compose these workflows imperatively with LlamaIndex modules,\nbut the QueryPipeline allows you to do it efficiently with fewer lines of\ncode.\n\nIt has the following benefits:\n\n  * **Express common query workflows with fewer lines of code/boilerplate:** Stop writing converter logic between outputs/inputs, and figuring out the exact typing of arguments for each module! \n  * **Greater readability:** Reduced boilerplate leads to greater readability. \n  * **End-to-end observability:** Get callback integration across the entire pipeline (even for arbitrarily nested DAGs), so you stop fiddling around with our observability integrations. \n  * **[In the future] Easy Serializability:** A declarative interface allows the core components to be serialized/redeployed on other systems much more easily. \n  * **[In the future] Caching:** This interface also allows us to build a caching layer under the hood, allowing input re-use. \n\nVisualization of our advanced RAG QueryPipeline using `networkx` and `pyvis`\n\n#  Usage\n\nThe QueryPipeline allows you to a DAG-based query workflow using LlamaIndex\nmodules. There are two main ways to use it:\n\n  * As a sequential chain (easiest/most concise) \n  * As a full DAG (more expressive) \n\nSee our [ usage pattern guide\n](https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/usage_pattern.html)\nfor more details.\n\n##  Sequential Chain\n\nSome simple pipelines are purely linear in nature \u2014 the output of the previous\nmodule directly goes into the input of the next module.\n\nSome examples:\n\n  * Prompt \u2192 LLM \u2192 Output parsing \n  * Retriever \u2192Response synthesizer \n\nHere\u2019s the most basic example, chaining a prompt with LLM. Simply initialize `\nQueryPipeline ` with the ` chain ` parameter.\n\n    \n    \n    # try chaining basic prompts\n    prompt_str = \"Please generate related movies to {movie_name}\"\n    prompt_tmpl = PromptTemplate(prompt_str)\n    llm = OpenAI(model=\"gpt-3.5-turbo\")\n    \n    p = QueryPipeline(chain=[prompt_tmpl, llm], verbose=True)\n\n##  Setting up a DAG for an Advanced RAG Workflow\n\nGenerally setting up a query workflow will require using our lower-level\nfunctions to build a DAG.\n\nFor instance, to build an \u201cadvanced RAG\u201d consisting of query\nrewriting/retrieval/reranking/synthesis, you\u2019d do something like the\nfollowing.\n\n    \n    \n    from llama_index.postprocessor import CohereRerank\n    from llama_index.response_synthesizers import TreeSummarize\n    from llama_index import ServiceContext\n    \n    # define modules\n    prompt_str = \"Please generate a question about Paul Graham's life regarding the following topic {topic}\"\n    prompt_tmpl = PromptTemplate(prompt_str)\n    llm = OpenAI(model=\"gpt-3.5-turbo\")\n    retriever = index.as_retriever(similarity_top_k=3)\n    reranker = CohereRerank()\n    summarizer = TreeSummarize(\n        service_context=ServiceContext.from_defaults(llm=llm)\n    )\n    \n    # define query pipeline\n    p = QueryPipeline(verbose=True)\n    p.add_modules(\n        {\n            \"llm\": llm,\n            \"prompt_tmpl\": prompt_tmpl,\n            \"retriever\": retriever,\n            \"summarizer\": summarizer,\n            \"reranker\": reranker,\n        }\n    )\n    # add edges \n    p.add_link(\"prompt_tmpl\", \"llm\")\n    p.add_link(\"llm\", \"retriever\")\n    p.add_link(\"retriever\", \"reranker\", dest_key=\"nodes\")\n    p.add_link(\"llm\", \"reranker\", dest_key=\"query_str\")\n    p.add_link(\"reranker\", \"summarizer\", dest_key=\"nodes\")\n    p.add_link(\"llm\", \"summarizer\", dest_key=\"query_str\")\n\nIn this code block we 1) add modules, and then 2) define relationships between\nmodules. Note that by ` source_key ` and ` dest_key ` are **optional** and are\nonly required if first module has more than one output / the second module has\nmore than one input respectively.\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 8177, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13df4dc0-ddbe-445e-80a9-7e75c81b3c47": {"__data__": {"id_": "13df4dc0-ddbe-445e-80a9-7e75c81b3c47", "embedding": null, "metadata": {"filename": "introducing-query-pipelines-025dc2bb0537.md", "extension": ".md", "title": "Introducing Query Pipelines", "date": "Jan 8, 2024", "url": "https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ea01df2-9d80-441c-9644-23133cd296e9", "node_type": "4", "metadata": {"filename": "introducing-query-pipelines-025dc2bb0537.md", "extension": ".md", "title": "Introducing Query Pipelines", "date": "Jan 8, 2024", "url": "https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537"}, "hash": "1d10a8242ea930dc9e0522a78e6390d701ed21adc725131456e1473a5c92827c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b33f739-902c-47d8-894d-7523428b86e9", "node_type": "1", "metadata": {"filename": "introducing-query-pipelines-025dc2bb0537.md", "extension": ".md", "title": "Introducing Query Pipelines", "date": "Jan 8, 2024", "url": "https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537"}, "hash": "868c539dd6ef95c5efad3f88f40e3ac371aff11ef454f402d4bdc1772bfab6c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2cd81065-2b75-4f47-a7a3-6aa57c31fa52", "node_type": "1", "metadata": {}, "hash": "82a79ecfc2b4181b55c0dd1a3bdc3542fe396717c184b1ef2d4b35dfc50f0d42", "class_name": "RelatedNodeInfo"}}, "text": "##  **Running the Pipeline**\n\nIf the pipeline has one \u201croot\u201d node and one output node, use ` run ` . Using\nthe previous example,\n\n    \n    \n    output = p.run(topic=\"YC\")\n    # output type is Response\n    type(output)\n\nIf the pipeline has multiple root nodes and/or multiple output nodes, use `\nrun_multi ` .\n\n    \n    \n    output_dict = p.run_multi({\"llm\": {\"topic\": \"YC\"}})\n    print(output_dict)\n\n##  Defining a Custom Query Component\n\nIt\u2019s super easy to subclass ` CustomQueryComponent ` so you can plug it into\nthe QueryPipeline.\n\nCheck out [ our walkthrough\n](https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/usage_pattern.html#defining-\na-custom-query-component) for more details.\n\n##  Supported Modules\n\nCurrently the following LlamaIndex modules are supported within a\nQueryPipeline. Remember, you can define your own!\n\n  ", "mimetype": "text/plain", "start_char_idx": 8177, "end_char_idx": 9028, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2cd81065-2b75-4f47-a7a3-6aa57c31fa52": {"__data__": {"id_": "2cd81065-2b75-4f47-a7a3-6aa57c31fa52", "embedding": null, "metadata": {"filename": "introducing-query-pipelines-025dc2bb0537.md", "extension": ".md", "title": "Introducing Query Pipelines", "date": "Jan 8, 2024", "url": "https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ea01df2-9d80-441c-9644-23133cd296e9", "node_type": "4", "metadata": {"filename": "introducing-query-pipelines-025dc2bb0537.md", "extension": ".md", "title": "Introducing Query Pipelines", "date": "Jan 8, 2024", "url": "https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537"}, "hash": "1d10a8242ea930dc9e0522a78e6390d701ed21adc725131456e1473a5c92827c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "13df4dc0-ddbe-445e-80a9-7e75c81b3c47", "node_type": "1", "metadata": {"filename": "introducing-query-pipelines-025dc2bb0537.md", "extension": ".md", "title": "Introducing Query Pipelines", "date": "Jan 8, 2024", "url": "https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537"}, "hash": "e057b5203bbbf8d163ac9611a66bfafafbcf76c9997eb97df9da1a3085b86f85", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "161dd6a1-65cb-4f81-b7e8-923f22dc7da7", "node_type": "1", "metadata": {}, "hash": "3dd102ee6669bfffe054aeb1c45b71422c5fe1b9666c574ba6ec6973f6e85f29", "class_name": "RelatedNodeInfo"}}, "text": "1. LLMs (both completion and chat) ( ` LLM ` ) \n  2. Prompts ( ` PromptTemplate ` ) \n  3. Query Engines ( ` BaseQueryEngine ` ) \n  4. Query Transforms ( ` BaseQueryTransform ` ) \n  5. Retrievers ( ` BaseRetriever ` ) \n  6. Output Parsers ( ` BaseOutputParser ` ) \n  7. Postprocessors/Rerankers ( ` BaseNodePostprocessor ` ) \n  8. Response Synthesizers ( ` BaseSynthesizer ` ) \n  9. ", "mimetype": "text/plain", "start_char_idx": 9028, "end_char_idx": 9410, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "161dd6a1-65cb-4f81-b7e8-923f22dc7da7": {"__data__": {"id_": "161dd6a1-65cb-4f81-b7e8-923f22dc7da7", "embedding": null, "metadata": {"filename": "introducing-query-pipelines-025dc2bb0537.md", "extension": ".md", "title": "Introducing Query Pipelines", "date": "Jan 8, 2024", "url": "https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ea01df2-9d80-441c-9644-23133cd296e9", "node_type": "4", "metadata": {"filename": "introducing-query-pipelines-025dc2bb0537.md", "extension": ".md", "title": "Introducing Query Pipelines", "date": "Jan 8, 2024", "url": "https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537"}, "hash": "1d10a8242ea930dc9e0522a78e6390d701ed21adc725131456e1473a5c92827c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2cd81065-2b75-4f47-a7a3-6aa57c31fa52", "node_type": "1", "metadata": {"filename": "introducing-query-pipelines-025dc2bb0537.md", "extension": ".md", "title": "Introducing Query Pipelines", "date": "Jan 8, 2024", "url": "https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537"}, "hash": "7026fbd24c04ccda7ec2b572d58e1fd4486e51ab9f0ebe97819e7f7246d68f8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4029841c-d639-4f67-b4ec-204a13585c56", "node_type": "1", "metadata": {}, "hash": "db2d68b0319a130580a2322e4250296d3b1cf592c5ae51495272b57b7aec748a", "class_name": "RelatedNodeInfo"}}, "text": "Other ` QueryPipeline ` objects \n  10. Custom components ( ` CustomQueryComponent ` ) \n\nCheck out the [ module usage guide\n](https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/module_usage.html)\nfor more details.\n\n#  Walkthrough Example\n\nMake sure to check out our [ Introduction to Query Pipelines guide\n](https://docs.llamaindex.ai/en/latest/examples/pipeline/query_pipeline.html)\nfor full details. We go over all the steps above with concrete examples!\n\nThe notebook guide also logs traces through [ Arize Phoenix\n](https://github.com/Arize-ai/phoenix) . You can see the full run of each\nQueryPipeline in the Phoenix dashboard. Our full callback support throughout\nevery component in a QueryComponent allows you to easily integrate with any\nobservability provider.\n\n#  Related Work\n\nThe idea of a declarative syntax for building LLM-powered pipelines is not\nnew. Related works include [ Haystack\n](https://docs.haystack.deepset.ai/docs/pipelines) as well as the [ LangChain\nExpression Language ](https://python.langchain.com/docs/expression_language/)\n. Other related works include pipelines that are setup in the no-code/low-code\nsetting such as [ Langflow ](https://github.com/logspace-ai/langflow) / [\nFlowise ](https://flowiseai.com/) .\n\nOur main goal here was highlighted above: provide a convenient dev UX to\ndefine common query workflows over your data. There\u2019s a lot of\noptimizations/guides to be done here!\n\n#  FAQ\n\n**What\u2019s the difference between a** ` **QueryPipeline** ` **and** ` [\n**IngestionPipeline**\n](https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/root.html)\n` **?**\n\nGreat question. Currently the IngestionPipeline operates during the data\ningestion stage, and the QueryPipeline operates during the query stage. That\nsaid, there\u2019s potentially some shared abstractions we\u2019ll develop for both!\n\n", "mimetype": "text/plain", "start_char_idx": 9410, "end_char_idx": 11274, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4029841c-d639-4f67-b4ec-204a13585c56": {"__data__": {"id_": "4029841c-d639-4f67-b4ec-204a13585c56", "embedding": null, "metadata": {"filename": "introducing-query-pipelines-025dc2bb0537.md", "extension": ".md", "title": "Introducing Query Pipelines", "date": "Jan 8, 2024", "url": "https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ea01df2-9d80-441c-9644-23133cd296e9", "node_type": "4", "metadata": {"filename": "introducing-query-pipelines-025dc2bb0537.md", "extension": ".md", "title": "Introducing Query Pipelines", "date": "Jan 8, 2024", "url": "https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537"}, "hash": "1d10a8242ea930dc9e0522a78e6390d701ed21adc725131456e1473a5c92827c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "161dd6a1-65cb-4f81-b7e8-923f22dc7da7", "node_type": "1", "metadata": {"filename": "introducing-query-pipelines-025dc2bb0537.md", "extension": ".md", "title": "Introducing Query Pipelines", "date": "Jan 8, 2024", "url": "https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537"}, "hash": "e972b37fa2915cd82b193983cee16683f7a48e1c2e8564b30b12868733fad62f", "class_name": "RelatedNodeInfo"}}, "text": "#  Conclusion + Resources\n\nThat\u2019s it! As mentioned above we\u2019ll be adding a lot more resources and guides\nsoon. In the meantime check out our current guides:\n\n  * [ Query Pipelines Guide ](https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/root.html)\n  * [ Query Pipelines Walkthrough ](https://docs.llamaindex.ai/en/latest/examples/pipeline/query_pipeline.html)\n  * [ Query Pipeline Usage Pattern ](https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/usage_pattern.html)\n  * [ Query Pipelines Module Usage Guide ](https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/module_usage.html)\n\n", "mimetype": "text/plain", "start_char_idx": 11274, "end_char_idx": 11912, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65dacfe5-5846-462d-a0f2-e94162cf3d1e": {"__data__": {"id_": "65dacfe5-5846-462d-a0f2-e94162cf3d1e", "embedding": null, "metadata": {"filename": "a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.md", "extension": ".md", "title": "A Cheat Sheet and Some Recipes For Building Advanced RAG", "date": "Jan 5, 2024", "url": "https://www.llamaindex.ai/blog/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "77fee692-6ef9-4e8d-ad00-def27f2412b5", "node_type": "4", "metadata": {"filename": "a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.md", "extension": ".md", "title": "A Cheat Sheet and Some Recipes For Building Advanced RAG", "date": "Jan 5, 2024", "url": "https://www.llamaindex.ai/blog/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b"}, "hash": "5fb578c23f877e180ed6779fe7079c63f0a3d3a02f524c943bfff8d2aab5ee06", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c44a504a-5623-4337-9e38-27269b6d0e70", "node_type": "1", "metadata": {}, "hash": "e39f2a99bc60a7ea26b80434523cad10385b3c23bd1ea341806a39d578a22d55", "class_name": "RelatedNodeInfo"}}, "text": "It\u2019s the start of a new year and perhaps you\u2019re looking to break into the RAG\nscene by building your very first RAG system. Or, maybe you\u2019ve built Basic RAG\nsystems and are now looking to enhance them to something more advanced in\norder to better handle your user\u2019s queries and data structures.\n\nIn either case, knowing where or how to begin may be a challenge in and of\nitself! If that\u2019s true, then hopefully this blog post points you in the right\ndirection for your next steps, and moreover, provides for you a mental model\nfor you to anchor your decisions when building advanced RAG systems.\n\nThe RAG cheat sheet shared above was greatly inspired by a recent RAG survey\npaper ( [ \u201cRetrieval-Augmented Generation for Large Language Models: A Survey\u201d\nGao, Yunfan, et al. 2023 ](https://arxiv.org/pdf/2312.10997.pdf) ).\n\n#  Basic RAG\n\nMainstream RAG as defined today involves retrieving documents from an external\nknowledge database and passing these along with the user\u2019s query to an LLM for\nresponse generation. In other words, RAG involves a Retrieval component, an\nExternal Knowledge database and a Generation component.\n\n**LlamaIndex Basic RAG Recipe:**\n\n    \n    \n    from llama_index import SimpleDirectoryReader, VectorStoreIndex\n    \n    # load data\n    documents = SimpleDirectoryReader(input_dir=\"...\").load_data()\n    \n    # build VectorStoreIndex that takes care of chunking documents\n    # and encoding chunks to embeddings for future retrieval\n    index = VectorStoreIndex.from_documents(documents=documents)\n    \n    # The QueryEngine class is equipped with the generator\n    # and facilitates the retrieval and generation steps\n    query_engine = index.as_query_engine()\n    \n    # Use your Default RAG\n    response = query_engine.query(\"A user's query\")\n\n#  Success Requirements for RAG\n\nIn order for a RAG system to be deemed as a success (in the sense of providing\nuseful and relevant answers to user questions), there are really only two high\nlevel requirements:\n\n  1. Retrieval must be able to find the most relevant documents to a user query. \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2069, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c44a504a-5623-4337-9e38-27269b6d0e70": {"__data__": {"id_": "c44a504a-5623-4337-9e38-27269b6d0e70", "embedding": null, "metadata": {"filename": "a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.md", "extension": ".md", "title": "A Cheat Sheet and Some Recipes For Building Advanced RAG", "date": "Jan 5, 2024", "url": "https://www.llamaindex.ai/blog/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "77fee692-6ef9-4e8d-ad00-def27f2412b5", "node_type": "4", "metadata": {"filename": "a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.md", "extension": ".md", "title": "A Cheat Sheet and Some Recipes For Building Advanced RAG", "date": "Jan 5, 2024", "url": "https://www.llamaindex.ai/blog/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b"}, "hash": "5fb578c23f877e180ed6779fe7079c63f0a3d3a02f524c943bfff8d2aab5ee06", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65dacfe5-5846-462d-a0f2-e94162cf3d1e", "node_type": "1", "metadata": {"filename": "a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.md", "extension": ".md", "title": "A Cheat Sheet and Some Recipes For Building Advanced RAG", "date": "Jan 5, 2024", "url": "https://www.llamaindex.ai/blog/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b"}, "hash": "d2df5d9dd7244391f185bf91a5a7934e2af6febe3c63e9b669de4f43002bf05b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0e1093b-4fa6-45e0-ad6b-47e63fe5ae85", "node_type": "1", "metadata": {}, "hash": "a405c4b6719b2ce08cd5a590e0f0f9ecf210ffaaab7893e1fbbf4346da1aa570", "class_name": "RelatedNodeInfo"}}, "text": "2. Generation must be able to make good use of the retrieved documents to sufficiently answer the user query. \n\n#  Advanced RAG\n\nWith the success requirements defined, we can then say that building advanced\nRAG is really about the application of more sophisticated techniques and\nstrategies (to the Retrieval or Generation components) to ensure that they are\nultimately met. Furthermore, we can categorize a sophisticated technique as\neither one that addresses one of the two high-level success requirements\nindependent (more or less) of the other, or one that addresses both of these\nrequirements simultaneously.\n\n#  Advanced techniques for Retrieval must be able to find the most relevant\ndocuments to a user query\n\nBelow we briefly describe a couple of the more sophisticated techniques to\nhelp achieve the first success requirement.\n\n  1. **Chunk-Size Optimization:** Since LLMs are restricted by context length, it is necessary to chunk documents when building the External Knowledge database. Chunks that are too big or too small can pose problems for the Generation component leading to in accurate responses. \n\n**LlamaIndex Chunk Size Optimization Recipe** ( [ notebook guide\n](https://github.com/run-\nllama/llama_index/blob/main/docs/examples/param_optimizer/param_optimizer.ipynb)\n) **:**\n\n    \n    \n    from llama_index import ServiceContext\n    from llama_index.param_tuner.base import ParamTuner, RunResult\n    from llama_index.evaluation import SemanticSimilarityEvaluator, BatchEvalRunner\n    \n    ### Recipe\n    ### Perform hyperparameter tuning as in traditional ML via grid-search\n    ### 1. Define an objective function that ranks different parameter combos\n    ### 2. ", "mimetype": "text/plain", "start_char_idx": 2069, "end_char_idx": 3757, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0e1093b-4fa6-45e0-ad6b-47e63fe5ae85": {"__data__": {"id_": "f0e1093b-4fa6-45e0-ad6b-47e63fe5ae85", "embedding": null, "metadata": {"filename": "a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.md", "extension": ".md", "title": "A Cheat Sheet and Some Recipes For Building Advanced RAG", "date": "Jan 5, 2024", "url": "https://www.llamaindex.ai/blog/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "77fee692-6ef9-4e8d-ad00-def27f2412b5", "node_type": "4", "metadata": {"filename": "a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.md", "extension": ".md", "title": "A Cheat Sheet and Some Recipes For Building Advanced RAG", "date": "Jan 5, 2024", "url": "https://www.llamaindex.ai/blog/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b"}, "hash": "5fb578c23f877e180ed6779fe7079c63f0a3d3a02f524c943bfff8d2aab5ee06", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c44a504a-5623-4337-9e38-27269b6d0e70", "node_type": "1", "metadata": {"filename": "a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.md", "extension": ".md", "title": "A Cheat Sheet and Some Recipes For Building Advanced RAG", "date": "Jan 5, 2024", "url": "https://www.llamaindex.ai/blog/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b"}, "hash": "622b5d5450d2f5cdf802ac79d2bfab3f699828c3ad5c0f5886b8972e20278655", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7adf07d5-df32-4cf9-8e17-e8a1c4f756de", "node_type": "1", "metadata": {}, "hash": "6f02dc53311f7703d62d0a96686281996a0d70eaacfb75405b8cbaad8a50bf75", "class_name": "RelatedNodeInfo"}}, "text": "Build ParamTuner object\n    ### 3. Execute hyperparameter tuning with ParamTuner.tune()\n    \n    # 1. Define objective function\n    def objective_function(params_dict):\n        chunk_size = params_dict[\"chunk_size\"]\n        docs = params_dict[\"docs\"]\n        top_k = params_dict[\"top_k\"]\n        eval_qs = params_dict[\"eval_qs\"]\n        ref_response_strs = params_dict[\"ref_response_strs\"]\n    \n        # build RAG pipeline\n        index = _build_index(chunk_size, docs)  # helper function not shown here\n        query_engine = index.as_query_engine(similarity_top_k=top_k)\n      \n        # perform inference with RAG pipeline on a provided questions `eval_qs`\n        pred_response_objs = get_responses(\n            eval_qs, query_engine, show_progress=True\n        )\n    \n        # perform evaluations of predictions by comparing them to reference\n        # responses `ref_response_strs`\n        evaluator = SemanticSimilarityEvaluator(...)\n        eval_batch_runner = BatchEvalRunner(\n            {\"semantic_similarity\": evaluator}, workers=2, show_progress=True\n        )\n        eval_results = eval_batch_runner.evaluate_responses(\n            eval_qs, responses=pred_response_objs, reference=ref_response_strs\n        )\n    \n        # get semantic similarity metric\n        mean_score = np.array(\n            [r.score for r in eval_results[\"semantic_similarity\"]]\n        ).mean()\n    \n        return RunResult(score=mean_score, params=params_dict)\n    \n    # 2. Build ParamTuner object\n    param_dict = {\"chunk_size\": [256, 512, 1024]} # params/values to search over\n    fixed_param_dict = { # fixed hyperparams\n      \"top_k\": 2,\n        \"docs\": docs,\n        \"eval_qs\": eval_qs[:10],\n        \"ref_response_strs\": ref_response_strs[:10],\n    }\n    param_tuner = ParamTuner(\n        param_fn=objective_function,\n        param_dict=param_dict,\n        fixed_param_dict=fixed_param_dict,\n        show_progress=True,\n    )\n    \n    # 3. Execute hyperparameter search\n    results = param_tuner.tune()\n    best_result = results.best_run_result\n    best_chunk_size = results.best_run_result.params[\"chunk_size\"]\n\n**2\\. Structured External Knowledge:** In complex scenarios, it may be\nnecessary to build your external knowledge with a bit more structure than the\nbasic vector index so as to permit recursive retrievals or routed retrieval\nwhen dealing with sensibly separated external knowledge sources.\n\n**LlamaIndex Recursive Retrieval Recipe** ( [ notebook guide\n](https://docs.llamaindex.ai/en/stable/examples/retrievers/recursive_retriever_nodes.html)\n) **:**\n\n    \n    \n    from llama_index import SimpleDirectoryReader, VectorStoreIndex\n    from llama_index.node_parser import SentenceSplitter\n    from llama_index.schema import IndexNode\n    \n    ### Recipe\n    ### Build a recursive retriever that retrieves using small chunks\n    ### but passes associated larger chunks to the generation stage\n    \n    # load data\n    documents = SimpleDirectoryReader(\n      input_file=\"some_data_path/llama2.pdf\"\n    ).load_data()\n    \n    # build parent chunks via NodeParser\n    node_parser = SentenceSplitter(chunk_size=1024)\n    base_nodes = node_parser.get_nodes_from_documents(documents)\n    \n    # define smaller child chunks\n    sub_chunk_sizes = [256, 512]\n    sub_node_parsers = [\n        SentenceSplitter(chunk_size=c, chunk_overlap=20) for c in sub_chunk_sizes\n    ]\n    all_nodes = []\n    for base_node in base_nodes:\n        for n in sub_node_parsers:\n            sub_nodes = n.get_nodes_from_documents([base_node])\n            sub_inodes = [\n                IndexNode.from_text_node(sn, base_node.node_id) for sn in sub_nodes\n            ]\n            all_nodes.extend(sub_inodes)\n        # also add original node to node\n        original_node = IndexNode.from_text_node(base_node, base_node.node_id)\n        all_nodes.append(original_node)\n    \n    # define a VectorStoreIndex with all of the nodes\n    vector_index_chunk = VectorStoreIndex(\n        all_nodes, service_context=service_context\n    )\n    vector_retriever_chunk = vector_index_chunk.as_retriever(similarity_top_k=2)\n    \n    # build RecursiveRetriever\n    all_nodes_dict = {n.node_id: n for n in all_nodes}\n    retriever_chunk = RecursiveRetriever(\n        \"vector\",\n        retriever_dict={\"vector\": vector_retriever_chunk},\n        node_dict=all_nodes_dict,\n        verbose=True,\n    )\n    \n    # build RetrieverQueryEngine using recursive_retriever\n    query_engine_chunk = RetrieverQueryEngine.from_args(\n        retriever_chunk, service_context=service_context\n    )\n    \n    # perform inference with advanced RAG (i.e. query engine)\n    response = query_engine_chunk.query(\n        \"Can you tell me about the key concepts for safety finetuning\"\n    )\n\n**Other useful links**\n\nWe have several of guides demonstrating the application of other advanced\ntechniques to help ensure accurate retrieval in complex cases. ", "mimetype": "text/plain", "start_char_idx": 3757, "end_char_idx": 8651, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7adf07d5-df32-4cf9-8e17-e8a1c4f756de": {"__data__": {"id_": "7adf07d5-df32-4cf9-8e17-e8a1c4f756de", "embedding": null, "metadata": {"filename": "a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.md", "extension": ".md", "title": "A Cheat Sheet and Some Recipes For Building Advanced RAG", "date": "Jan 5, 2024", "url": "https://www.llamaindex.ai/blog/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "77fee692-6ef9-4e8d-ad00-def27f2412b5", "node_type": "4", "metadata": {"filename": "a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.md", "extension": ".md", "title": "A Cheat Sheet and Some Recipes For Building Advanced RAG", "date": "Jan 5, 2024", "url": "https://www.llamaindex.ai/blog/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b"}, "hash": "5fb578c23f877e180ed6779fe7079c63f0a3d3a02f524c943bfff8d2aab5ee06", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0e1093b-4fa6-45e0-ad6b-47e63fe5ae85", "node_type": "1", "metadata": {"filename": "a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.md", "extension": ".md", "title": "A Cheat Sheet and Some Recipes For Building Advanced RAG", "date": "Jan 5, 2024", "url": "https://www.llamaindex.ai/blog/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b"}, "hash": "7f1d42e7b9060f6a1339dc97cdaa2c6cb1e3cf6d7699e4a69b3da529043244ef", "class_name": "RelatedNodeInfo"}}, "text": "Here are links\nto a select few of them:\n\n  1. [ Building External Knowledge using Knowledge Graphs ](https://docs.llamaindex.ai/en/stable/examples/query_engine/knowledge_graph_rag_query_engine.html)\n  2. [ Performing Mixed Retrieval with Auto Retrievers ](https://docs.llamaindex.ai/en/stable/examples/vector_stores/elasticsearch_auto_retriever.html)\n  3. [ Building Fusion Retrievers ](https://docs.llamaindex.ai/en/stable/examples/retrievers/simple_fusion.html)\n  4. [ Fine-tuning Embedding Models used in Retrieval ](https://docs.llamaindex.ai/en/stable/examples/finetuning/embeddings/finetune_embedding.html)\n  5. [ Transforming Query Embeddings (HyDE) ](https://docs.llamaindex.ai/en/stable/examples/query_transformations/HyDEQueryTransformDemo.html)\n\n#  Advanced techniques for Generation must be able to make good use of the\nretrieved documents\n\nSimilar to previous section, we provide a couple of examples of the\nsophisticated techniques under this category, which can be described as\nensuring that the retrieved documents are well aligned to the LLM of the\nGenerator.\n\n  1. **Information Compression:** Not only are LLMs are restricted by context length, but there can be response degradation if the retrieved documents carry too much noise (i.e. irrelevant information). \n\n**LlamaIndex Information Compression Recipe** ( [ notebook guide\n](https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/LongLLMLingua.html)\n) **:**\n\n    \n    \n    from llama_index import SimpleDirectoryReader, VectorStoreIndex\n    from llama_index.query_engine import RetrieverQueryEngine\n    from llama_index.postprocessor import LongLLMLinguaPostprocessor\n    \n    ### Recipe\n    ### Define a Postprocessor object, here LongLLMLinguaPostprocessor\n    ### Build QueryEngine that uses this Postprocessor on retrieved docs\n    \n    # Define Postprocessor\n    node_postprocessor = LongLLMLinguaPostprocessor(\n        instruction_str=\"Given the context, please answer the final question\",\n        target_token=300,\n        rank_method=\"longllmlingua\",\n        additional_compress_kwargs={\n            \"condition_compare\": True,\n            \"condition_in_question\": \"after\",\n            \"context_budget\": \"+100\",\n            \"reorder_context\": \"sort\",  # enable document reorder\n        },\n    )\n    \n    # Define VectorStoreIndex\n    documents = SimpleDirectoryReader(input_dir=\"...\").load_data()\n    index = VectorStoreIndex.from_documents(documents)\n    \n    # Define QueryEngine\n    retriever = index.as_retriever(similarity_top_k=2)\n    retriever_query_engine = RetrieverQueryEngine.from_args(\n        retriever, node_postprocessors=[node_postprocessor]\n    )\n    \n    # Used your advanced RAG\n    response = retriever_query_engine.query(\"A user query\")\n\n**2\\. Result Re-Rank:** LLMs suffer from the so-called \u201cLost in the Middle\u201d\nphenomena which stipulates that LLMs focus on the extreme ends of the prompts.\nIn light of this, it is beneficial to re-rank retrieved documents before\npassing them off to the Generation component.\n\n**LlamaIndex Re-Ranking For Better Generation Recipe** ( [ notebook guide\n](https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/CohereRerank.html)\n) **:**\n\n    \n    \n    import os\n    from llama_index import SimpleDirectoryReader, VectorStoreIndex\n    from llama_index.postprocessor.cohere_rerank import CohereRerank\n    from llama_index.postprocessor import LongLLMLinguaPostprocessor\n    \n    ### Recipe\n    ### Define a Postprocessor object, here CohereRerank\n    ### Build QueryEngine that uses this Postprocessor on retrieved docs\n    \n    # Build CohereRerank post retrieval processor\n    api_key = os.environ[\"COHERE_API_KEY\"]\n    cohere_rerank = CohereRerank(api_key=api_key, top_n=2)\n    \n    # Build QueryEngine (RAG) using the post processor\n    documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n    index = VectorStoreIndex.from_documents(documents=documents)\n    query_engine = index.as_query_engine(\n        similarity_top_k=10,\n        node_postprocessors=[cohere_rerank],\n    )\n    \n    # Use your advanced RAG\n    response = query_engine.query(\n        \"What did Sam Altman do in this essay?\"\n    )\n\n#  Advanced techniques for simultaneously addressing Retrieval and Generation\nsuccess requirements\n\nIn this sub section, we consider sophisticated methods that use the synergy of\nretrieval and generation in order to achieve both better retrieval as well as\nmore accurate generated responses to user queries).\n\n  1. **Generator-Enhanced Retrieval:** These techniques make use of the LLM\u2019s inherent reasoning abilities to refine the user query before retrieval is performed so as to better indicate what exactly it requires to provide a useful response. \n\n**LlamaIndex Generator-Enhanced Retrieval Recipe** ( [ notebook guide\n](https://docs.llamaindex.ai/en/stable/examples/query_engine/flare_query_engine.html)\n) **:**\n\n    \n    \n    from llama_index.llms import OpenAI\n    from llama_index.query_engine import FLAREInstructQueryEngine\n    from llama_index import (\n        VectorStoreIndex,\n        SimpleDirectoryReader,\n        ServiceContext,\n    )\n    ### Recipe\n    ### Build a FLAREInstructQueryEngine which has the generator LLM play\n    ### a more active role in retrieval by prompting it to elicit retrieval\n    ### instructions on what it needs to answer the user query.\n    \n    # Build FLAREInstructQueryEngine\n    documents = SimpleDirectoryReader(\"./data/paul_graham\").load_data()\n    index = VectorStoreIndex.from_documents(documents)\n    index_query_engine = index.as_query_engine(similarity_top_k=2)\n    service_context = ServiceContext.from_defaults(llm=OpenAI(model=\"gpt-4\"))\n    flare_query_engine = FLAREInstructQueryEngine(\n        query_engine=index_query_engine,\n        service_context=service_context,\n        max_iterations=7,\n        verbose=True,\n    )\n    \n    # Use your advanced RAG\n    response = flare_query_engine.query(\n        \"Can you tell me about the author's trajectory in the startup world?\"\n    )\n\n**2\\. Iterative Retrieval-Generator RAG:** For some complex cases, multi-step\nreasoning may be required to provide a useful and relevant answer to the user\nquery.\n\n**LlamaIndex Iterative Retrieval-Generator Recipe (** [ notebook guide\n](https://docs.llamaindex.ai/en/stable/examples/evaluation/RetryQuery.html#retry-\nquery-engine) **):**\n\n    \n    \n    from llama_index.query_engine import RetryQueryEngine\n    from llama_index.evaluation import RelevancyEvaluator\n    \n    ### Recipe\n    ### Build a RetryQueryEngine which performs retrieval-generation cycles\n    ### until it either achieves a passing evaluation or a max number of\n    ### cycles has been reached\n    \n    # Build RetryQueryEngine\n    documents = SimpleDirectoryReader(\"./data/paul_graham\").load_data()\n    index = VectorStoreIndex.from_documents(documents)\n    base_query_engine = index.as_query_engine()\n    query_response_evaluator = RelevancyEvaluator() # evaluator to critique \n                                                    # retrieval-generation cycles\n    retry_query_engine = RetryQueryEngine(\n        base_query_engine, query_response_evaluator\n    )\n    \n    # Use your advanced rag\n    retry_response = retry_query_engine.query(\"A user query\")\n\n#  Measurement Aspects of RAG\n\nEvaluating RAG systems are, of course, of utmost importance. In their survey\npaper, Gao, Yunfan et al. indicate 7 measurement aspects as seen in the top-\nright portion of the attached RAG cheat sheet. The llama-index library\nconsists of several evaluation abstractions as well as integrations to RAGAs\nin order to help builders gain an understanding of the level to which their\nRAG system achieves the success requirements through the lens of these\nmeasurement aspects. Below, we list a select few of the evaluation notebook\nguides.\n\n  1. [ Answer Relevancy and Context Relevancy ](https://docs.llamaindex.ai/en/latest/examples/evaluation/answer_and_context_relevancy.html)\n  2. [ Faithfulness ](https://www.notion.so/LlamaIndex-Platform-0754edd9af1c4159bde12649c184c8ef?pvs=21)\n  3. [ Retrieval Evaluation ](https://github.com/run-llama/llama_index/blob/main/docs/examples/evaluation/retrieval/retriever_eval.ipynb)\n  4. [ Batch Evaluations with BatchEvalRunner ](https://docs.llamaindex.ai/en/stable/examples/evaluation/batch_eval.html)\n\n#  You\u2019re Now Equipped To Do Advanced RAG\n\nAfter reading this blog post, we hope that you feel more equipped and\nconfident to apply some of these sophisticated techniques for building\nAdvanced RAG systems!\n\n", "mimetype": "text/plain", "start_char_idx": 8651, "end_char_idx": 17176, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ef9d429-a42d-4a86-b132-0bcf9b9df7d5": {"__data__": {"id_": "0ef9d429-a42d-4a86-b132-0bcf9b9df7d5", "embedding": null, "metadata": {"filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "20a8b548-65c5-4f9c-a2df-1920263843c1", "node_type": "4", "metadata": {"filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "hash": "cc550ed7a5ae4885396bdc46e84eb7813dc58af8619e23e1ca18efac283d7302", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eee7c626-81be-42b9-a77d-2f0cf7d5140a", "node_type": "1", "metadata": {}, "hash": "e59b3093b1fe83332074f52b00df538ef5c3a23c7b5612447236a5959fc28d1c", "class_name": "RelatedNodeInfo"}}, "text": "Over the past year, Large Language Models (LLMs) like GPT-4 have not only\ntransformed how we interact with machines but also have redefined the\npossibilities within the realm of natural language processing (NLP). A notable\ntrend in this evolution is the increasing popularity of open-source LLMs like\nLlama 2, Falcon, OPT and Yi. Some may prefer them over their commercial\ncounterparts in terms of accessibility, data security and privacy,\ncustomization potential, cost, and vendor dependency. Among the tools gaining\nincreasing traction in the LLM space are OpenLLM and LlamaIndex \u2014 two powerful\nplatforms that, when combined, unlock new use cases for building AI-driven\napplications.\n\n[ OpenLLM ](https://github.com/bentoml/OpenLLM) is an open-source platform for\ndeploying and operating any open-source LLMs in production. Its flexibility\nand ease of use make it an ideal choice for AI application developers seeking\nto harness the power of LLMs. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 950, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eee7c626-81be-42b9-a77d-2f0cf7d5140a": {"__data__": {"id_": "eee7c626-81be-42b9-a77d-2f0cf7d5140a", "embedding": null, "metadata": {"filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "20a8b548-65c5-4f9c-a2df-1920263843c1", "node_type": "4", "metadata": {"filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "hash": "cc550ed7a5ae4885396bdc46e84eb7813dc58af8619e23e1ca18efac283d7302", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ef9d429-a42d-4a86-b132-0bcf9b9df7d5", "node_type": "1", "metadata": {"filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "hash": "7930ade8bbad9994f602f3124fe4c7b1ba6a5ba0a7700d4220708a0a561a6d8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b00db98-5a7e-4558-a318-7418081dc0d7", "node_type": "1", "metadata": {}, "hash": "aa5c4d04245fb74c51178a800147a68a8594b7f02cd25b127b17f3abff0795e9", "class_name": "RelatedNodeInfo"}}, "text": "You can easily fine-tune, serve, deploy, and\nmonitor LLMs in a wide range of creative and practical applications.\n\n[ LlamaIndex ](https://github.com/run-llama/llama_index) provides a\ncomprehensive framework for managing and retrieving private and domain-\nspecific data. It acts as a bridge between the extensive knowledge of LLMs and\nthe unique, contextual data needs of specific applications.\n\nOpenLLM\u2019s support for a diverse range of open-source LLMs and LlamaIndex\u2019s\nability to seamlessly integrate custom data sources provide great\ncustomization for developers in both communities. This combination allows them\nto create AI solutions that are both highly intelligent and properly tailored\nto specific data contexts, which is very important for query-response systems.\n\nIn this blog post, I will explain how you can leverage the combined strengths\nof OpenLLM and LlamaIndex to build an intelligent query-response system. This\nsystem can understand, process, and respond to queries by tapping into a\ncustom corpus.\n\n#  Setting up the environment\n\nThe first step is to create a virtual environment in your machine, which helps\nprevent conflicts with other Python projects you might be working on. Let\u2019s\njust call it ` llamaindex-openllm ` and activate it.\n\n    \n    \n    python -m venv llamaindex-openllm\n    source llamaindex-openllm/bin/activate\n\nInstall the required packages. This command installs OpenLLM with the optional\n` vllm ` component (I will explain it later).\n\n    \n    \n    pip install \"openllm[vllm]\" llama-index llama-index-llms-openllm llama-index-embeddings-huggingface\n\nFor handling requests, you need to have an LLM server. Here, I use the\nfollowing command to start a Llama 2 7B local server at [\nhttp://localhost:3000 ](http://localhost:3000) . Feel free to choose any model\nthat fits your needs. ", "mimetype": "text/plain", "start_char_idx": 950, "end_char_idx": 2771, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b00db98-5a7e-4558-a318-7418081dc0d7": {"__data__": {"id_": "9b00db98-5a7e-4558-a318-7418081dc0d7", "embedding": null, "metadata": {"filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "20a8b548-65c5-4f9c-a2df-1920263843c1", "node_type": "4", "metadata": {"filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "hash": "cc550ed7a5ae4885396bdc46e84eb7813dc58af8619e23e1ca18efac283d7302", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eee7c626-81be-42b9-a77d-2f0cf7d5140a", "node_type": "1", "metadata": {"filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "hash": "93aac50e2612fe4a073f6388a426d4f9a4acf3377f9725358e4aa71ace2d459f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29a6eecc-32a8-4e33-a07e-19600e980267", "node_type": "1", "metadata": {}, "hash": "15cacb69c9b1ea6aa77a5a481576495475734010d65bbf360bd4831ea7d9c417", "class_name": "RelatedNodeInfo"}}, "text": "If you already have a remote LLM server, you can skip\nthis step.\n\n    \n    \n    openllm start meta-llama/Llama-2-7b-chat-hf --backend vllm\n\nOpenLLM automatically selects the most suitable runtime implementation for the\nmodel. For models with vLLM support, OpenLLM uses vLLM by default. Otherwise,\nit falls back to PyTorch. vLLM is a high-throughput and memory-efficient\ninference and serving engine for LLMs. According to [ this report\n](https://www.anyscale.com/blog/continuous-batching-llm-inference) , you can\nachieve 23x LLM inference throughput while reducing P50 latency using vLLM.\n\n> **_Note_ ** _: To use the vLLM backend, you need a GPU with at least the\n> Ampere architecture (or newer) and CUDA version 11.8. This demo uses a\n> machine with an Ampere A100\u201380G GPU. If your machine has a compatible GPU,\n> you can also choose vLLM. Otherwise, simply install the standard OpenLLM\n> package (_ ` _pip install openllm_ ` _) in the previous command._\n\n#  v1: Creating a simple completion service\n\nBefore building a query-response system, let\u2019s get familiar with the\nintegration of OpenLLM and LlamaIndex and use it to create a simple completion\nservice.\n\nThe integration offers two APIs for interactions with LLMs:\n\n1\\. ` OpenLLM ` : This can be used to initiate a local LLM server directly\nwithout the need for starting a separate one using commands like ` openllm\nstart ` . Here\u2019s how you can use it:\n\n    \n    \n    from llama_index.llms.openllm import OpenLLM\n    llm = OpenLLM('meta-llama/Llama-2-7b-chat-hf')\n\n2\\. ` OpenLLMAPI ` : This can be used to interact with a server hosted\nelsewhere, like the Llama 2 7B model I started previously.\n\nLet\u2019s try the ` complete ` endpoint and see if the Llama 2 7B model is able to\ntell what OpenLLM is by completing the sentence \u201cOpenLLM is an open source\ntool for\u201d.\n\n    \n    \n    from llama_index.llms.openllm import OpenLLMAPI\n    \n    remote_llm = OpenLLMAPI(address=\"http://localhost:3000\")\n    \n    completion_response = remote_llm.complete(\"OpenLLM is an open source tool for\", max_new_tokens=1024)\n    print(completion_response)\n\nRun this script and here is the output:\n\n    \n    \n    learning lifelong learning models. It is designed to be easy to use, even for those without extensive knowledge of machine learning. OpenLLM allows users to train, evaluate, and deploy lifelong learning models using a variety of datasets and algorithms.\n    \n    OpenLLM provides a number of features that make it useful for learning lifelong learning models. Some of these features include:\n    \n    1. Easy-to-use interface: OpenLLM provides an easy-to-use interface that makes it simple to train, evaluate, and deploy lifelong learning models.\n    2. Support for a variety of datasets: OpenLLM supports a variety of datasets, including images, text, and time-series data.\n    3. Support for a variety of algorithms: OpenLLM supports a variety of algorithms for lifelong learning, including neural networks, decision trees, and support vector machines.\n    4. Evaluation tools: OpenLLM provides a number of evaluation tools that allow users to assess the performance of their lifelong learning models.\n    5. Deployment tools: OpenLLM provides a number of deployment tools that allow users to deploy their lifelong learning models in a variety of environments.\n    \n    OpenLLM is written in Python and is available under an open source license. It is designed to be used in a variety of settings, including research, education, and industry.\n    \n    Some potential use cases for OpenLLM include:\n    \n    1. Training lifelong learning models for image classification: OpenLLM could be used to train a lifelong learning model to classify images based on their content.\n    2. Training lifelong learning models for natural language processing: OpenLLM could be used to train a lifelong learning model to process and analyze natural language text.\n    3. Training lifelong learning models for time-series data: OpenLLM could be used to train a lifelong learning model to predict future values in a time-series dataset.\n    4. Deploying lifelong learning models in a production environment: OpenLLM could be used to deploy a lifelong learning model in a production environment, such as a recommendation system or a fraud detection system.\n    \n    Overall, OpenLLM is a powerful tool for learning lifelong learning models. Its ease of use, flexibility, and support for a variety of datasets and algorithms make it a valuable resource for researchers and practitioners in a variety of fields.\n\nObviously, the model couldn\u2019t correctly explain OpenLLM with some\nhallucinations . Nevertheless, the code works well as the server outputs a\nresponse for the request. This is a good start as we proceed with building our\nsystem.\n\n#  v2: Enhancing with a query-response system\n\nThe initial version revealed a key limitation: the model\u2019s lack of specific\nknowledge about OpenLLM. One solution is to feed the model with domain-\nspecific information, allowing it to learn and respond according to topic-\nspecific queries. This is where LlamaIndex comes into play, enabling you to\nbuild a local knowledge base with pertinent information. Specifically, you\ncreate a directory (for example, ` data ` ) and build an index for all the\ndocuments in the folder.\n\nCreate a folder and let\u2019s import the GitHub README file of OpenLLM into the\nfolder:\n\n    \n    \n    mkdir data\n    cd data\n    wget https://github.com/bentoml/OpenLLM/blob/main/README.md\n\nGo back to the previous directory and create a script called ` starter.py `\nlike the following:\n\n    \n    \n    from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n    from llama_index.llms.openllm import OpenLLMAPI\n    from llama_index.core.node_parser import SentenceSplitter\n    \n    # Change the address to your OpenLLM server\n    llm = OpenLLMAPI(address=\"http://localhost:3000\")\n    \n    # Break down the document into manageable chunks (each of size 1024 characters, with a 20-character overlap)\n    text_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n    \n    # Create a ServiceContext with the custom model and all the configurations\n    service_context = ServiceContext.from_defaults(\n        llm=llm,\n        embed_model=\"local\",\n        text_splitter=text_splitter,\n        context_window=8192,\n        num_output=4096,\n    )\n    \n    # Load documents from the data directory\n    documents = SimpleDirectoryReader(\"data\").load_data()\n    \n    # Build an index over the documents using the customized LLM in the ServiceContext\n    index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n    \n    # Query your data using the built index\n    query_engine = index.as_query_engine()\n    response = query_engine.query(\"What is OpenLLM?\")\n    print(response)\n\nTo improve the quality of your response, I recommend you define a `\nSentenceSplitter ` to provide finer control over the input processing, leading\nto better output quality.\n\n", "mimetype": "text/plain", "start_char_idx": 2771, "end_char_idx": 9761, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29a6eecc-32a8-4e33-a07e-19600e980267": {"__data__": {"id_": "29a6eecc-32a8-4e33-a07e-19600e980267", "embedding": null, "metadata": {"filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "20a8b548-65c5-4f9c-a2df-1920263843c1", "node_type": "4", "metadata": {"filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "hash": "cc550ed7a5ae4885396bdc46e84eb7813dc58af8619e23e1ca18efac283d7302", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b00db98-5a7e-4558-a318-7418081dc0d7", "node_type": "1", "metadata": {"filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "hash": "30de291013721ed5fccc375a4d3bc04a79494bc817da4b45c6cc60299e8c5f6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c087d458-7a77-4982-a459-ac995ad7b74e", "node_type": "1", "metadata": {}, "hash": "ccfd8c48db02dba8d01958695d5d77f2d4652cd6b6d8fa242f81adf73acb4e9f", "class_name": "RelatedNodeInfo"}}, "text": "In addition, you can set ` streaming=True ` to stream your response:\n\n    \n    \n    query_engine = index.as_query_engine(streaming=True)\n    response = query_engine.query(\"What is OpenLLM?\")\n    response.print_response_stream()\n\nYour directory structure should look like this now:\n\n    \n    \n     starter.py\n     data\n         README.md\n\nRun ` starter.py ` to test the query-response system. The output should be\nconsistent with the content of the OpenLLM README. Here is the response I\nreceived:\n\n> OpenLLM is an open-source platform for deploying and managing large language\n> models (LLMs) in a variety of environments, including on-premises, cloud,\n> and edge devices. It provides a comprehensive suite of tools and features\n> for fine-tuning, serving, deploying, and monitoring LLMs, simplifying the\n> end-to-end deployment workflow for LLMs.\n\n#  Conclusion\n\nThe exploration in this article underscores the importance of customizing AI\ntools to fit specific needs. By using OpenLLM for flexible deployment of LLMs\nand LlamaIndex for data management, I have demonstrated how to create an AI-\npowered system. It not only understands and processes queries but also\ndelivers responses based on a unique knowledge base. I hope this blog post has\ninspired you to explore more capabilities and use cases of OpenLLM and\nLlamaIndex. Happy coding! ", "mimetype": "text/plain", "start_char_idx": 9761, "end_char_idx": 11104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c087d458-7a77-4982-a459-ac995ad7b74e": {"__data__": {"id_": "c087d458-7a77-4982-a459-ac995ad7b74e", "embedding": null, "metadata": {"filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "20a8b548-65c5-4f9c-a2df-1920263843c1", "node_type": "4", "metadata": {"filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "hash": "cc550ed7a5ae4885396bdc46e84eb7813dc58af8619e23e1ca18efac283d7302", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29a6eecc-32a8-4e33-a07e-19600e980267", "node_type": "1", "metadata": {"filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}, "hash": "0d760ae6fc04384296be758c4d0f4edb4465834a3d9e93d47147a9c3d1bb5c17", "class_name": "RelatedNodeInfo"}}, "text": "\u2328\n\n", "mimetype": "text/plain", "start_char_idx": 11104, "end_char_idx": 11107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9bd8085-0ca1-4438-b1c9-1e1b0a348030": {"__data__": {"id_": "a9bd8085-0ca1-4438-b1c9-1e1b0a348030", "embedding": null, "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "25398961-e7fd-4608-acd6-cd550a028140", "node_type": "4", "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "c15c0cc7c3c802851f12ee4a799c82e531e6e57691aead1dd58c81e29dfa8a12", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67f27054-92f9-44e7-90cc-a76ddc333734", "node_type": "1", "metadata": {}, "hash": "d8363334ebb067f19c9352136eaecf3b32699c06ec867ac02dd701bed6e19460", "class_name": "RelatedNodeInfo"}}, "text": "Over the holidays, I was running some retrieval benchmarks with LlamaIndex. I\nfound myself rebuilding an index repeatedly with 30K documents, and finding\nwaiting 10\u201320 minutes each time was too grating.\n\nSo to solve this, issue, I decided to bite the bullet and figure out how to\ndeploy LlamaIndex to AWS, and create a scalable ETL pipeline for indexing my\ndata. This brought the processing time down to around 5 minutes!\n\nProposed system architecture\n\nIf you want to skip the detailed steps, you can jump to the code at the\nfollowing repository:\n\n[ https://github.com/run-llama/llamaindex_aws_ingestion\n](https://github.com/run-llama/llamaindex_aws_ingestion)\n\n**NOTE:** I am not an AWS expert, and had zero experience with it before this\nproject. There are likely ways to improve upon the design I came up with. This\nblog merely documents my first foray into getting a system working on AWS. My\nhope is that this helps other people get started, and opens the door for other\nengineers deploying more scale-able systems.\n\n#  Step 1: Figuring out how AWS works\n\nTo use AWS effectively, there are several packages and tools that you will\nneed:\n\n  1. [ AWS account signup ](https://portal.aws.amazon.com/billing/signup#/start/email)\n  2. [ Install AWS CLI ](https://docs.aws.amazon.com/eks/latest/userguide/setting-up.html)\n  3. Used to authenticate your AWS account for CLI tools \n  4. [ Install eksctl ](https://eksctl.io/installation/)\n  5. Used to create ` EKS ` clusters easily \n  6. [ Install kubectl ](https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html)\n  7. Used to configure and debug deployments, pods, services, etc. \n  8. [ Install Docker ](https://www.docker.com/products/docker-desktop/)\n\nAs you will see, nearly all AWS deployments revolve around ` yaml ` files that\ndescribe what you are deploying and how they connect together, as well as some\nCLI commands to actually run the deployment.\n\nIf at any time you aren\u2019t sure what\u2019s going on, I found it helpful to visit\nthe AWS dashboard and explore the resources I had actually deployed. Usually,\nyou will want to visit. I had the pages below favourited in AWS. Also,\nremember to set your region properly in the top right!\n\nMy AWS console favourites\n\n##  Note on how deployments work\n\nFor a majority of deployments, you will typically have\n\n  1. The cluster \n  2. The deployed app, scaled to X replicas \n  3. A load balancer, to balance the incoming requests between X replicas \n\nIn the examples below, most will have a ` yaml ` for the deployed app, a `\nyaml ` for the load balancer, and a command to create the cluster you want to\nrun on.\n\n##  Helpful CLI Commands\n\nA few CLI commands proved to be extremely helpful for debugging and monitoring\ndeployments.\n\n    \n    \n    # get the state of pods/deployments\n    kubectl get pods\n    kubectl get deployments\n    \n    # useful for seeing logs/events of pods + full yaml config\n    kubectl describe pod <pod name>\n    kubectl logs <pod name>\n    \n    # list clusters kubectl knows about\n    kubectl config get-contexts\n    \n    # switch kubectl to another cluster\n    kubectl config use-context <context name>\n    \n    # delete things\n    kubectl delete <pod/deployment/service> <name>\n\n#  Step 2: Deploying Text Embeddings Interface\n\nIn order to run embeddings fast, we will deploy an embeddings server using\nHuggingFace\u2019s [ Text Embedding Interface\n](https://github.com/huggingface/text-embeddings-inference) (TEI). This server\nhas production-level features and optimizations out-of-the-box, including\ncontinuous batching, flash-attention, rust implementation, and more.\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3603, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67f27054-92f9-44e7-90cc-a76ddc333734": {"__data__": {"id_": "67f27054-92f9-44e7-90cc-a76ddc333734", "embedding": null, "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "25398961-e7fd-4608-acd6-cd550a028140", "node_type": "4", "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "c15c0cc7c3c802851f12ee4a799c82e531e6e57691aead1dd58c81e29dfa8a12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a9bd8085-0ca1-4438-b1c9-1e1b0a348030", "node_type": "1", "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "cf92d3e7bb42d9bc38c95bfe773b668ef8312944aac1aba1f6f4fdc0aa4d6cda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b2507da-9367-4259-ad7c-c18653a28d62", "node_type": "1", "metadata": {}, "hash": "443846dbebc93fbdf6b8adda9c5caa47e8df8b61430efbd0b3dbcbbfc2904993", "class_name": "RelatedNodeInfo"}}, "text": "HuggingFace provides prebuilt docker images to simplify deployment.\n\nHowever, the first step to running embeddings fast is to have a GPU. If you\njust signed up for AWS, you will have to request a quota increase. For me, I\nrequested a few times for G5 instances (which run an Nvidia A10G GPU), and\nafter a few days of testing on CPU, AWS gave me access to use up to 4 G5\ninstances.\n\nOnce you have a quota for GPU instances (like G5 nodes), you can create your\ncluster and deploy\n\n    \n    \n    eksctl create cluster --name embeddings --node-type=g5.xlarge --nodes 1\n    sleep 5\n    kubectl create -f ./tei-deployment.yaml\n    sleep 5\n    kubectl create -f ./tei-service.yaml\n    sleep 5\n    echo \"Embeddings URL is: &lt;http://$&gt;(kubectl get svc tei-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\"\n\nThe code above will create a cluster, a deployment (i.e. our TEI server) and a\nload balancer server.\n\nYou can see the yaml configs in [ the repo ](https://github.com/run-\nllama/llamaindex_aws_ingestion/tree/main/tei) , and you can edit them as\nneeded.\n\n**NOTE:** Make sure to write down the URL printed at the end! ", "mimetype": "text/plain", "start_char_idx": 3603, "end_char_idx": 4738, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b2507da-9367-4259-ad7c-c18653a28d62": {"__data__": {"id_": "5b2507da-9367-4259-ad7c-c18653a28d62", "embedding": null, "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "25398961-e7fd-4608-acd6-cd550a028140", "node_type": "4", "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "c15c0cc7c3c802851f12ee4a799c82e531e6e57691aead1dd58c81e29dfa8a12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67f27054-92f9-44e7-90cc-a76ddc333734", "node_type": "1", "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "25b96277154b4ad4e4e138636718d104f3eb673faf44c18014b74d5c95f48a6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc63fa5b-4ef5-412f-b702-5f4372553f47", "node_type": "1", "metadata": {}, "hash": "beecc23b5e4ca237dee94d9cb5f6113383c28107020fbd44b1445e8ce6e6290b", "class_name": "RelatedNodeInfo"}}, "text": "If you forget,\nyou can get the URL in the ` EKS ` page on AWS. You\u2019ll want the external IP\nfor the load balancer.\n\n#  Step 3: Deploying RabbitMQ\n\nRabbitMQ is where we will queue documents to be ingested. RabbitMQ is a\nmessage broker system that allows for powerful yet simple queuing of tasks.\nSince some ingestion tasks (like metadata extraction, embeddings) can be slow,\nthe more naive approach of a REST API would leave connections open while data\nis processed. Instead, using a queue allows us to quickly upload data and\noffload processing to scalable message consumer(s). It also allows us to add\nparallelism with ease, where in our system, each ` Document ` object is\nprocessed independently by a consumer.\n\nDeploying RabbitMQ on ` EKS ` was a little tricky, but using the RabbitMQ\noperator installed with ` krew ` , many things are abstracted away.\n\nFirst, you need to create your cluster. For whatever reason, this didn\u2019t work\nunless I also specified the zones\n\n    \n    \n    eksctl create cluster \\\n      --name mqCluster \\\n      --zones us-east-1a,us-east-1b,us-east-1c,us-east-1d,us-east-1f\n\nSince RabbitMQ needs storage, and each replica needs to share the same\nstorage, we should give our cluster permission to provision and use ` EBS `\nfor storage. This was a frustrating step to figure out since most existing\nguides skip this detail!\n\n    \n    \n    eksctl utils associate-iam-oidc-provider \\\n      --cluster=mqCluster \\\n      --region us-east-1 \\\n      --approve\n    sleep 5\n    eksctl create iamserviceaccount \\\n        --name ebs-csi-controller-sa \\\n        --namespace kube-system \\\n        --cluster mqCluster \\\n        --role-name AmazonEKS_EBS_CSI_DriverRole \\\n        --role-only \\\n        --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \\\n        --approve\n    sleep 5\n    eksctl create addon \\\n      --name aws-ebs-csi-driver \\\n      --cluster mqCluster \\\n      --service-account-role-arn arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):role/AmazonEKS_EBS_CSI_DriverRole \\\n      --force\n\nFrom there, we can install the RabbitMQ operator and create our deployment\n\n    \n    \n    kubectl apply -f &lt;https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml&gt;\n    sleep 5\n    kubectl apply -f rabbitmqcluster.yaml\n    sleep 5\n    echo \"RabbitMQ URL is: $(kubectl get svc production-rabbitmqcluster -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\"\n\nAs usual, the code for all this can be found in the [ git repo\n](https://github.com/run-llama/llamaindex_aws_ingestion/tree/main/rabbitmq) .\n\n**NOTE:** Make sure to write down the URL printed at the end! ", "mimetype": "text/plain", "start_char_idx": 4738, "end_char_idx": 7419, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc63fa5b-4ef5-412f-b702-5f4372553f47": {"__data__": {"id_": "cc63fa5b-4ef5-412f-b702-5f4372553f47", "embedding": null, "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "25398961-e7fd-4608-acd6-cd550a028140", "node_type": "4", "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "c15c0cc7c3c802851f12ee4a799c82e531e6e57691aead1dd58c81e29dfa8a12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b2507da-9367-4259-ad7c-c18653a28d62", "node_type": "1", "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "bb8e6b835fc2e8f2bd5b6b76331047afa9ddddaf51f1cb832e596bc2c68ad188", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12be728d-efb4-43a5-a7d0-7e7e501c8bc2", "node_type": "1", "metadata": {}, "hash": "04d3f0feb07b7836ddddce7ec2b36fff54d123b38d9eb407d4e422fe8aace9c9", "class_name": "RelatedNodeInfo"}}, "text": "If you forget,\nyou can get the URL in the ` EKS ` page on AWS. You\u2019ll want the external IP\nfor the load balancer.\n\nYou can monitor your RabbitMQ queues by visiting \u201c<rabbitmq_url>:15672\u201d and\nsigning in with \u201cguest\u201d/\u201dguest\u201d.\n\n#  Step 4: Deploying IngestionPipeline Workers\n\nThis is where the real meat of work comes in. We need to create a ` consumer `\nthat will endlessly pull from our ` RabbitMQ ` queue, ingest data with the\nhelp of TEI, and then put that data into our vector db.\n\nTo do this, we can make a FastAPI server that does two things\n\n  1. Starts a thread to consume from our queue \n  2. Starts a webserver, to enable us to specify a readiness check, and gives us room to add more features in the future (i.e. probing queue status, logs, etc.) \n\nFirst, we write our code, as you can see in [ worker.py\n](https://github.com/run-\nllama/llamaindex_aws_ingestion/blob/main/worker/worker.py)\n\nThen, we dockerize our app by creating a simple [ Dockerfile\n](https://github.com/run-\nllama/llamaindex_aws_ingestion/blob/main/worker/Dockerfile) and running:\n\n    \n    \n    docker build -t <image_name> .\n    docker tag <image_name>:latest <image_name>:<version>\n    docker push <image_name>:<version>\n\nWith our app dockerized, we can complete the ` worker-deployment.yaml ` file\nby filling in\n\n  * Our embeddings URL under ` TEI_URL `\n  * Our rabbit-mq URL under ` RABBITMQ_URL `\n  * Our image name under container image \n  * Our cluster details (in this case, a weaviate URL and API key) \n\nWith the ` yaml ` file complete, now we can properly deploy the worker\n\n    \n    \n    eksctl create cluster --name mq-workers --zones us-east-1a,us-east-1b,us-east-1c,us-east-1d,us-east-1f\n    sleep 5\n    kubectl create -f ./worker-deployment.yaml\n    sleep 5\n    kubectl create -f ./worker-service.yaml\n\n#  Step 5: Making a User-Facing Lambda Function\n\nOur lambda function will rely on a single external dependency \u2014 ` pika ` \u2014\nwhich is used to communicate with RabbitMQ.\n\nCreate a python file called ` lambda_function.py ` with the following code:\n\n    \n    \n    import pika\n    import json\n    \n    def lambda_handler(event, context):\n        try:\n            body = json.loads(event.get('body', '{}'))\n        except:\n            body = event.get('body', {})\n            \n        user = body.get('user', '')\n        documents = body.get('documents', [])\n        if not user or not documents:\n            return {\n                'statusCode': 400,\n                'body': json.dumps('Missing user or documents')\n            }\n        \n        credentials = pika.PlainCredentials(\"guest\", \"guest\")\n        parameters = pika.ConnectionParameters(\n            host=\"hostname.amazonaws.com\", \n            port=5672, \n            credentials=credentials\n        )\n        \n        connection = pika.BlockingConnection(parameters=parameters)\n        channel = connection.channel()\n        channel.queue_declare(queue='etl')\n    \n        for document in documents:\n            data = {\n                'user': user,\n                'documents': [document]\n            }\n            channel.basic_publish(\n                exchange=\"\", \n                routing_key='etl', \n                body=json.dumps(data)\n            )\n    \n        return {\n            'statusCode': 200,\n            'body': json.dumps('Documents queued for ingestion')\n        }\n\nThe function above processes incoming requests, and publishes each document as\na single message in our rabbitmq cluster.\n\nTo deploy a lambda file with dependencies, we need to create a zip of our\nlambda function + all dependencies. To do this, we can create a `\nrequirements.txt ` file with our dependencies and run:\n\n    \n    \n    pip install -r requirements.txt -t .\n    zip -r9 ../ingestion_lambda.zip . -x \"*.git*\" \"*setup.sh*\" \"*requirements.txt*\" \"*.zip*\"\n\nWith our code and zip file in hand, head over to the Lambda AWS page in your\nbrowser.\n\n  1. ", "mimetype": "text/plain", "start_char_idx": 7419, "end_char_idx": 11316, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12be728d-efb4-43a5-a7d0-7e7e501c8bc2": {"__data__": {"id_": "12be728d-efb4-43a5-a7d0-7e7e501c8bc2", "embedding": null, "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "25398961-e7fd-4608-acd6-cd550a028140", "node_type": "4", "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "c15c0cc7c3c802851f12ee4a799c82e531e6e57691aead1dd58c81e29dfa8a12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc63fa5b-4ef5-412f-b702-5f4372553f47", "node_type": "1", "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "658d7b9ef5c977138732abf5f25ed7afe7c06d289d3d790cd06df16ddd3000e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "304d1902-4c36-4fa1-8179-9086a042d84a", "node_type": "1", "metadata": {}, "hash": "55416f0b0b5d406690b3e6cf632a3a1e6a9d127f6c419bfd6fbcd5eccf6b59ea", "class_name": "RelatedNodeInfo"}}, "text": "Select ` Create function `\n  2. Give it a name, select a python runtime (I used Python 3.11) \n  3. Click ` Create function ` at the bottom \n  4. In the code editor, you\u2019ll see an ` Upload from ` button \u2014 click that, and upload your zip file \n  5. Click test, give the test a name, and paste the following JSON \n\n    \n    \n    {\n        \"body\": {\"user\": \"Test\", \"documents\": [{\"text\": \"test\"}]}\n    }\n\nOnce the test works, the ` Deploy ` button will not be grayed out, and you can\nclick it.\n\nYour public URL will be listed in the upper right pane under ` Function URL `\n\u2014 this is the URL you can use to call your lambda function from anywhere!\n\n#  Step 6: Reap the Scaling Benefits\n\nNow, we can run our system end-to-end!\n\nTo ingest data, you can run:\n\n    \n    \n    import requests\n    from llama_index import Document, SimpleDirectoryReader\n    \n    documents = SimpleDirectoryReader(\"./data\").load_data()\n    \n    # this will also be the namespace for the vector store \n    # -- for weaviate, it needs to start with a captial and only alpha-numeric\n    user = \"Loganm\" \n    \n    # upload in batches\n    for batch_idx in range(0, len(documents), 30):\n      documents_batch = documents[batch_idx:batch_idx+30]\n      body = {\n        'user': user,\n        'documents': [doc.json() for doc in documents_batch]\n      }\n    \n     # use the URL of our lambda function here\n     response = requests.post(\"&lt;lambda_url&gt;\", json=body)\n     print(response.text)\n\nThen, to use our data:\n\n    \n    \n    from llama_index import VectorStoreIndex\n    from llama_index.vector_stores import WeaviateVectorStore\n    import weaviate\n    \n    auth_config = weaviate.AuthApiKey(api_key=\"...\")\n    client = weaviate.Client(url=\"...\", auth_client_secret=auth_config)\n    vector_store = WeaviateVectorStore(weaviate_client=client, class_prefix=\"&lt;user&gt;\")\n    index = VectorStoreIndex.from_vector_store(vector_store)\n\n#  Step 7: Clean-up\n\nAWS doesn\u2019t make it easy to estimate costs of all this. But after running and\ntesting things for a few days, I had only spent ~$40CAD. ", "mimetype": "text/plain", "start_char_idx": 11316, "end_char_idx": 13375, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "304d1902-4c36-4fa1-8179-9086a042d84a": {"__data__": {"id_": "304d1902-4c36-4fa1-8179-9086a042d84a", "embedding": null, "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "25398961-e7fd-4608-acd6-cd550a028140", "node_type": "4", "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "c15c0cc7c3c802851f12ee4a799c82e531e6e57691aead1dd58c81e29dfa8a12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12be728d-efb4-43a5-a7d0-7e7e501c8bc2", "node_type": "1", "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}, "hash": "b73545d674fa95e18cad0bcb2771138ff8e283a6fe2d46c925a5228b380ff78b", "class_name": "RelatedNodeInfo"}}, "text": "This included leaving\nsome services running overnight (whoops!).\n\nWhen you are done with your deployment, you\u2019ll want to delete the resources so\nthat you aren\u2019t charged for things you aren\u2019t using. To delete my clusters, I\nran the following:\n\n    \n    \n    eksctl delete cluster embeddings\n    eksctl delete cluster mq-worker\n    kubectl rabbitmq delete production-rabbitmqcluster\n\nThen, in the AWS UI console, I deleted any remaining resources on the ` EC2 `\nand ` CloudFormation ` pages, as well as double-checking that everything was\ndeleted on the ` EKS ` page.\n\n#  Conclusion\n\nUsing this setup, I was able to reduce index-construction times for creating\nlarge indexes dramatically. Before, it would take about 10\u201320 minutes to\ncreate the index for 25K documents, and with this setup (2 rabbitmq nodes, 2\nworkers, 2 embeddings), it was down to 5 minutes! And with more scaling, it\ncould be even faster.\n\n#  Next Steps\n\nFrom here, there are several improvements that I can think of\n\n  * better secrets management \n  * adding auto-scaling \n  * adding a retrieval lambda function (would require making a docker image for lambda + llama-index) \n  * adding queue stats to the fastapi server \n  * deploying redis for document management on the IngestionPipeline \n\nI encourage anyone to take this work and build off it. Be sure share any\nimprovement on the github repository as well!\n\n", "mimetype": "text/plain", "start_char_idx": 13375, "end_char_idx": 14757, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb5a9f0e-e306-4cf4-b6a2-7b1b0a309ae4": {"__data__": {"id_": "fb5a9f0e-e306-4cf4-b6a2-7b1b0a309ae4", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-01-02-f349db8c1842.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201302", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-02-f349db8c1842"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c26f422-ea35-4946-917e-5016eab0ed12", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-01-02-f349db8c1842.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201302", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-02-f349db8c1842"}, "hash": "e5739314b5ad4c4779c8479561babb94aa9da824bb3842722b56f412672f3688", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "953e3a37-dc50-429b-ac34-e17ffb8ffeae", "node_type": "1", "metadata": {}, "hash": "a771334424f8bc9e8f6665f5879a166cc649587179507cfa7538b99399553091", "class_name": "RelatedNodeInfo"}}, "text": "Hello, Llama Lovers ,\n\nHappy New Year! As we step into 2024, we\u2019re thrilled to bring you a special\nedition of our newsletter, packed with updates from the last two weeks of\n2023. This edition is brimming with the latest features, community demos,\ncourses, insightful tutorials, guides, and webinars that we\u2019ve curated for\nyou.\n\nHave you been working on an interesting project, written an engaging article,\nor created a video? ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 426, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "953e3a37-dc50-429b-ac34-e17ffb8ffeae": {"__data__": {"id_": "953e3a37-dc50-429b-ac34-e17ffb8ffeae", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-01-02-f349db8c1842.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201302", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-02-f349db8c1842"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c26f422-ea35-4946-917e-5016eab0ed12", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-01-02-f349db8c1842.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201302", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-02-f349db8c1842"}, "hash": "e5739314b5ad4c4779c8479561babb94aa9da824bb3842722b56f412672f3688", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb5a9f0e-e306-4cf4-b6a2-7b1b0a309ae4", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-01-02-f349db8c1842.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201302", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-02-f349db8c1842"}, "hash": "eb6d45da8da728a048941477df611bf54190b547a9e481cf44637eea9e762a59", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4917c373-bb8d-4f5d-936b-9d5f16ae2f1e", "node_type": "1", "metadata": {}, "hash": "d6e9e4a45e1aa3e82d699f749c5f4d0b1998b2be40aed4883ce52faf1e4f3e87", "class_name": "RelatedNodeInfo"}}, "text": "We can\u2019t wait to hear about it! Please share your work\nwith us at [ news@llamaindex.ai ](mailto:news@llamaindex.ai) . Don\u2019t forget to\nsubscribe to our newsletter via our [ website ](https://www.llamaindex.ai/) to\nreceive all these exciting updates directly in your inbox.\n\n**First, the highlights:**\n\n  1. **LLMCompiler Implementation:** A SOTA agent implementation for faster, efficient handling of complex queries. [ Notebook ](https://github.com/run-llama/llama-hub/blob/main/llama_hub/llama_packs/agents/llm_compiler/llm_compiler.ipynb) , [ Tweet ](https://x.com/llama_index/status/1740778394856648843?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 426, "end_char_idx": 1043, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4917c373-bb8d-4f5d-936b-9d5f16ae2f1e": {"__data__": {"id_": "4917c373-bb8d-4f5d-936b-9d5f16ae2f1e", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-01-02-f349db8c1842.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201302", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-02-f349db8c1842"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c26f422-ea35-4946-917e-5016eab0ed12", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-01-02-f349db8c1842.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201302", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-02-f349db8c1842"}, "hash": "e5739314b5ad4c4779c8479561babb94aa9da824bb3842722b56f412672f3688", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "953e3a37-dc50-429b-ac34-e17ffb8ffeae", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-01-02-f349db8c1842.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201302", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-02-f349db8c1842"}, "hash": "d57aec0786be9ec5761694f53aa46273780ad2edcc5f046cbfff54e86da609fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b939c0f-7774-4aa9-b73c-3415eee41198", "node_type": "1", "metadata": {}, "hash": "5bf544d540f33089c5336b6656c8e1d55c1296d8d1916d34cd102ca54ee6251d", "class_name": "RelatedNodeInfo"}}, "text": "2. **MultiDocAutoRetrieverPack:** A RAG template for structured retrieval and dynamic responses to large documents and metadata. [ Tweet ](https://x.com/llama_index/status/1739307699773518201?s=20) , [ LlamaPack ](https://llamahub.ai/l/llama_packs-multidoc_autoretrieval?from=llama_packs) . \n  3. **Structured Hierarchical RAG:** New RAG technique for optimized retrieval over multiple documents, ensuring precise, relevant responses. [ Docs ](https://docs.llamaindex.ai/en/latest/examples/query_engine/multi_doc_auto_retrieval/multi_doc_auto_retrieval.html) , [ Tweet ](https://x.com/llama_index/status/1737515390664872040?s=20) . \n  4. **Custom Agents:** A simple abstraction for custom agent reasoning loops, enabling easy integration with RAG, SQL, and other systems, and enhancing response refinement for complex queries. [ Docs ](https://docs.llamaindex.ai/en/latest/examples/agent/custom_agent.html) , [ Tweet ](https://x.com/llama_index/status/1741141394558001414?s=20) . \n  5. **New lower-level agent API:** For enhanced transparency, debuggability, and control, supporting step-wise execution and task modification. [ Docs ](https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/agent_runner.html) , [ Tweet ](https://x.com/llama_index/status/1736809248947155076?s=20) . \n\n**Feature Releases and Enhancements:**\n\n  * We have introduced a simple abstraction for building custom agent reasoning loops, surpassing prepackaged frameworks like ReAct. This tool allows for easy integration with RAG, SQL, or other systems, and we demonstrated how to build an agent with retry logic for routers, enhancing its ability to manage complex, multi-part questions and refine query responses. [ Docs ](https://docs.llamaindex.ai/en/latest/examples/agent/custom_agent.html) , [ Tweet ](https://x.com/llama_index/status/1741141394558001414?s=20) . \n  * We have implemented the LLMCompiler project, a SOTA agent framework enabling DAG-based planning and parallel function execution. This surpasses traditional sequential methods in speed, allowing for quicker and more efficient handling of complex queries in any LLM and data pipeline. [ Notebook ](https://github.com/run-llama/llama-hub/blob/main/llama_hub/llama_packs/agents/llm_compiler/llm_compiler.ipynb) , [ Tweet ](https://x.com/llama_index/status/1740778394856648843?s=20) . \n  * We have introduced MultiDocAutoRetrieverPack, a RAG template for efficiently handling large documents and metadata, offering structured retrieval and dynamic responses tailored to specific queries. [ Tweet ](https://x.com/llama_index/status/1739307699773518201?s=20) , [ LlamaPack ](https://llamahub.ai/l/llama_packs-multidoc_autoretrieval?from=llama_packs) . \n  * We have introduced a Structured Hierarchical RAG technique, optimizing RAG over multiple documents. It involves modeling documents as structured metadata for auto-retrieval, indexed in a vector database. This method dynamically selects documents based on inferred properties and performs recursive retrieval within each document for precise, relevant responses in your RAG pipeline. [ Docs ](https://docs.llamaindex.ai/en/latest/examples/query_engine/multi_doc_auto_retrieval/multi_doc_auto_retrieval.html) , [ Tweet ](https://x.com/llama_index/status/1737515390664872040?s=20) . \n  * We have launched a new feature for advanced RAG that allows step-wise feedback for complex query executions, improving interpretability and control. This is particularly beneficial for weaker models that struggle with multi-part tasks. ", "mimetype": "text/plain", "start_char_idx": 1043, "end_char_idx": 4575, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b939c0f-7774-4aa9-b73c-3415eee41198": {"__data__": {"id_": "0b939c0f-7774-4aa9-b73c-3415eee41198", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-01-02-f349db8c1842.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201302", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-02-f349db8c1842"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c26f422-ea35-4946-917e-5016eab0ed12", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-01-02-f349db8c1842.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201302", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-02-f349db8c1842"}, "hash": "e5739314b5ad4c4779c8479561babb94aa9da824bb3842722b56f412672f3688", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4917c373-bb8d-4f5d-936b-9d5f16ae2f1e", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-01-02-f349db8c1842.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201302", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-02-f349db8c1842"}, "hash": "62896624619bd6529b59a5f55fd7132fee6a794e88abdc99f8ecdb038f020ae4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b29f7e93-d59e-4d60-aaa6-36ef6e7ef66c", "node_type": "1", "metadata": {}, "hash": "dd7134309aa3741ee672ae0c66591c80b1f46bfe480c6bd21b82ef4b7d1dde76", "class_name": "RelatedNodeInfo"}}, "text": "We also introduced a step-by-step chat interface for enhanced user interaction and control. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/agent/agent_runner/agent_runner_rag_controllable.ipynb) , [ Tweet ](https://x.com/llama_index/status/1737161312944468412?s=20) . \n  * We have integrated with OpenRouterAI, offering a unified API for easy LLM access, cost efficiency, and reliable fallback options. OpenRouterAI allows users to compare costs, latency, and throughput for various models, like mixtral-8x7b, directly on their platform. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/openrouter.ipynb) , [ Tweet ](https://x.com/llama_index/status/1737176999712731349?s=20) . \n  * We have introduced a new lower-level agent API that enhances transparency, debuggability, and control. This API allows for granular control over agents, decouples task creation from execution, and supports step-wise execution. It also enables viewing each step, upcoming steps, and soon, modifying intermediate steps with human feedback. [ Docs ](https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/agent_runner.html) , [ Tweet ](https://x.com/llama_index/status/1736809248947155076?s=20) . \n\n**Community Demos** :\n\n  * **Automated LeetCode Crash Course:** The Project integrates advanced ML with traditional algorithms to streamline LeetCode study for technical interviews. It involves extracting and summarizing LeetCode problems using an LLM, organizing these summaries in a vector store, and employing scikit-learn for clustering. [ Blog ](https://medium.com/@kevinchwong/from-machine-learning-to-learning-machine-483eaa4b2855) , [ Code ](https://github.com/kevinchwong/leetcode-intensive/) . \n  * **RAG Assisted Auto Developer** : A project by [ **Ocean Li** ](https://twitter.com/quantoceanli) for building a devbot that understands and writes code. It integrates various tools: LlamaIndex for indexing codebases, Autogen / OpenAI Code Interpreter for code writing and testing, and [ lionagi.ai ](http://lionagi.ai/) for orchestration. [ Notebook ](https://github.com/lion-agi/lionagi/blob/main/notebooks/AutoDev_with_llama_autogen_assistant.ipynb) . \n\n**Courses:**\n\n  * We\u2019ve partnered with [ ActiveLoop AI ](https://twitter.com/activeloopai) to provide a [ free course ](https://learn.activeloop.ai/courses/rag) on retrieval-augmented generation for production, featuring 33 lessons, 7 hands-on assignments, and a certification upon completion. \n  * Beginner-friendly [ course ](https://cognitiveclass.ai/courses/course-v1:IBMSkillsNetwork+GPXX0OLGEN+v1#about-course) from [ IBM Skills Network ](https://twitter.com/skillsnetworkhq) on using LlamaIndex with IBM Watsonx to create effective product recommendations. \n\n**Guides:**\n\n  * [ Guide ](https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/structured_image_retrieval.ipynb) to Semi-Structured Image QA with Gemini: Learn to extract data from unlabeled images and query it, using multi-modal models and advanced retrieval techniques, as demonstrated with the SROIE v2 dataset which contains images of receipts/invoices. \n  * [ Guide ](https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6) to Advanced RAG Concepts: A comprehensive survey by [ Ivan Ilin ](https://twitter.com/ivanilin9) , covering twelve core concepts including chunking, hierarchical indexing, query rewriting, and more. Each section provides resources and guides from our system for deeper understanding and practical application. \n  * [ Guide ](https://github.com/run-llama/llama_index/blob/main/docs/examples/vector_stores/qdrant_hybrid.ipynb) to Building Hybrid Search: Learn to create a hybrid search for RAG from scratch. The process involves generating sparse vectors, fusing sparse and dense queries, and implementing this in a Qdrant engine database for effective RAG integration. \n  * [ Guide ](https://github.com/run-llama/llama_index/blob/main/docs/examples/vector_stores/pinecone_auto_retriever.ipynb) to Building Structured Retrieval with LLMs: Set up auto-retrieval in Pinecone vector database, monitor prompts with Arize AI Phoenix, and tailor prompts for specific queries to enhance your document handling and structured data analysis. \n  * [ Guide ](/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5) on Evaluating LLM Evaluators: our new evaluation method and dataset bundle, are designed to benchmark LLMs as evaluators against human annotations. This involves comparing LLM judge predictions (1\u20135 score) with ground-truth judgments, using metrics like Correlation, Hamming Distance, and Agreement Rate. \n\n**Tutorials:**\n\n  * [ Ryan Nguyen ](https://medium.com/@ryanntk) [ tutorial ](https://levelup.gitconnected.com/a-guide-to-processing-tables-in-rag-pipelines-with-llamaindex-and-unstructuredio-3500c8f917a7) on Processing Tables in RAG Pipelines with LlamaIndex and UnstructuredIO. \n  * [ Wenqi Glantz ](https://www.linkedin.com/in/wenqi-glantz-b5448a5a/) [ tutorial ](https://towardsdatascience.com/safeguarding-your-rag-pipelines-a-step-by-step-guide-to-implementing-llama-guard-with-llamaindex-6f80a2e07756) on Safeguarding RAG Pipelines: A Step-by-Step Guide to Implementing Llama Guard with LlamaIndex. \n  * [ Wenqi Glantz ](https://www.linkedin.com/in/wenqi-glantz-b5448a5a/) [ tutorial ](https://towardsdatascience.com/safeguarding-your-rag-pipelines-a-step-by-step-guide-to-implementing-llama-guard-with-llamaindex-6f80a2e07756) on 10+ Ways to Run Open-Source Models with LlamaIndex. \n  * Jina AI [ tutorial ](https://jina.ai/news/full-stack-rag-with-jina-embeddings-v2-and-llamaindex/) on enhancing RAG applications by integrating Jina v2 embeddings with LlamaIndex and Mixtral LLM via Hugging Face. \n  * [ Ankush Singal ](https://medium.com/@andysingal) [ tutorial ](https://ai.gopubby.com/benchmarking-rag-pipelines-with-a-evaluation-pack-in-forward-looking-active-retrieval-augmented-a8bd057c856b) on Benchmarking RAG Pipelines With A Evaluation Pack in Forward-Looking Active Retrieval Augmented Generation (FLARE). \n  * [ Laurie\u2019s ](https://twitter.com/seldo) [ tutorial ](/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab) on Effortlessly Running Mistral AI\u2019s Mixtral 8x7b: Learn to use OLLAMA with LlamaIndex for a one-line setup of a local, open-source retrieval-augmented generation app with API, featuring Qdrant engine integration for vector storage. \n  * [ Tomaz Bratanic ](https://twitter.com/tb_tomaz) [ tutorial ](/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206) on Multimodal RAG pipeline with LlamaIndex and Neo4j. \n  * [ Sudarshan Koirala ](https://twitter.com/mesudarshan) video [ tutorial ](https://www.youtube.com/watch?v=N8is20i2tqA) on using Mistral API with LlamaIndex. \n  * [ Chia Jeng Yang ](https://chiajy.medium.com/) [ tutorial ](https://medium.com/enterprise-rag/a-first-intro-to-complex-rag-retrieval-augmented-generation-a8624d70090f) on Technical Considerations for Complex RAG. \n\n**Webinars:**\n\n  * [ Webinar ](https://www.youtube.com/watch?v=fdpaHJlN0PQ) with Google Developers on advanced RAG applications and multi-modal settings with Google Gemini. \n  * [ Webinar ](https://www.youtube.com/watch?v=kZxl4gpe3OM) of Jerry Liu with Louis-Fran\u00e7ois on the Future of AI: LlamaIndex, LLMs, RAG, Prompting, and more. \n\n**Calling all enterprises:**\n\nAre you building with LlamaIndex? We are working hard to make LlamaIndex even\nmore Enterprise-ready and have sneak peeks at our upcoming products available\nfor partners. Interested? ", "mimetype": "text/plain", "start_char_idx": 4575, "end_char_idx": 12161, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b29f7e93-d59e-4d60-aaa6-36ef6e7ef66c": {"__data__": {"id_": "b29f7e93-d59e-4d60-aaa6-36ef6e7ef66c", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2024-01-02-f349db8c1842.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201302", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-02-f349db8c1842"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c26f422-ea35-4946-917e-5016eab0ed12", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2024-01-02-f349db8c1842.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201302", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-02-f349db8c1842"}, "hash": "e5739314b5ad4c4779c8479561babb94aa9da824bb3842722b56f412672f3688", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b939c0f-7774-4aa9-b73c-3415eee41198", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2024-01-02-f349db8c1842.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201302", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-02-f349db8c1842"}, "hash": "948793b41e56330602a1962e2d58edc31ec14697627a306838657ef35be483ac", "class_name": "RelatedNodeInfo"}}, "text": "[ Get in touch.\n](https://docs.google.com/forms/d/e/1FAIpQLScBNdM2a_fn8UZOKmFQt6lBsrd1o6FflvsdPH-\nPn3JkdlN_Rg/viewform)\n\n", "mimetype": "text/plain", "start_char_idx": 12161, "end_char_idx": 12282, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58bea4e7-1166-4ae8-aa0c-a54346ab7bfb": {"__data__": {"id_": "58bea4e7-1166-4ae8-aa0c-a54346ab7bfb", "embedding": null, "metadata": {"filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "19393bcc-7aaf-4a70-a123-7165df7cae9f", "node_type": "4", "metadata": {"filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "hash": "68f12f61c892078b4cd4328018a58573934c863f42aba1d1d4bada744c788a9a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ff0d37c-c7ea-4ec6-bfae-4ada7afe0e76", "node_type": "1", "metadata": {}, "hash": "15ebdb8c800f0a21d886979ba1a92533a13c49d8e49fd8828aa7594fa2f22f30", "class_name": "RelatedNodeInfo"}}, "text": "You may have heard the fuss about the latest release from European AI\npowerhouse [ Mistral AI ](https://mistral.ai/) : it\u2019s called Mixtral 8x7b, a\n\u201cmixture of experts\u201d model \u2014 eight of them, each trained with 7 billion\nparameters, hence the name. Released originally as a [ mic-drop tweet\n](https://twitter.com/MistralAI/status/1733150512395038967) they followed up a\nfew days later with a [ blog post ](https://mistral.ai/news/mixtral-of-\nexperts/) that showed it matching or exceeding GPT-3.5 as well as the much\nlarger Llama2 70b on a number of benchmarks.\n\nHere at LlamaIndex we\u2019re naturally fans of open source software, so open\nmodels with permissive licenses like Mixtral are right up our alley. We\u2019ve had\na few questions about how to get Mixtral working with LlamaIndex, so this post\nis here to get you up and running with a totally local model.\n\n#  Step 1: Install Ollama\n\nPreviously getting a local model installed and working was a huge pain, but\nwith the release of [ Ollama ](https://ollama.ai/) , it\u2019s suddenly a snap!\nAvailable for MacOS and Linux (and soon on Windows, though you can use it\ntoday on Windows via [ Windows Subsystem For Linux\n](https://learn.microsoft.com/en-us/windows/wsl/install) ), it is itself open\nsource and a [ free download ](https://ollama.ai/download) .\n\nOnce downloaded, you can get Mixtral with a single command:\n\n    \n    \n    ollama run mixtral\n\nThe first time you run this command it will have to download the model, which\ncan take a long time, so go get a snack. Also note that it requires a hefty\n48GB of RAM to run smoothly! If that\u2019s too much for your machine, consider\nusing its smaller but still very capable cousin **Mistral 7b** , which you\ninstall and run the same way:\n\n    \n    \n    ollama run mistral\n\nWe\u2019ll assume you\u2019re using Mixtral for the rest of this tutorial, but Mistral\nwill also work.\n\nOnce the model is running Ollama will automatically let you chat with it.\nThat\u2019s fun, but what\u2019s the point of having a model if it can\u2019t work with your\ndata? That\u2019s where LlamaIndex comes in. The next few steps will take you\nthrough the code line by line, but if you\u2019d prefer to save all the copying and\npasting, all of this code is available in an [ open-source repo\n](https://github.com/run-llama/mixtral_ollama) that you can clone to follow\nalong there.\n\n#  Step 2: Install your dependencies\n\nYou\u2019re going to need LlamaIndex installed, obviously! We\u2019ll also get you going\nwith a handful of other dependencies that are about to come in handy:\n\n    \n    \n    pip install llama-index qdrant_client torch transformers\n\n#  Step 3: Smoke test\n\nIf you\u2019ve got Ollama running and LlamaIndex properly installed, the following\nquick script will make sure everything is in order by asking it a quick \u201csmoke\ntest\u201d question in a script all by itself:\n\n    \n    \n    # Just runs .complete to make sure the LLM is listening\n    from llama_index.llms import Ollama\n    \n    llm = Ollama(model=\"mixtral\")\n    response = llm.complete(\"Who is Laurie Voss?\")\n    print(response)\n\n#  Step 4: Load some data and index it\n\nNow you\u2019re ready to load in some real data! ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3102, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ff0d37c-c7ea-4ec6-bfae-4ada7afe0e76": {"__data__": {"id_": "3ff0d37c-c7ea-4ec6-bfae-4ada7afe0e76", "embedding": null, "metadata": {"filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "19393bcc-7aaf-4a70-a123-7165df7cae9f", "node_type": "4", "metadata": {"filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "hash": "68f12f61c892078b4cd4328018a58573934c863f42aba1d1d4bada744c788a9a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58bea4e7-1166-4ae8-aa0c-a54346ab7bfb", "node_type": "1", "metadata": {"filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "hash": "e95123b7f945d7ffebda567ef1386a956873740a981f3e8ccb799ce0beec0bd1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a6dcc48-3cd1-4245-bad8-18f28281a24b", "node_type": "1", "metadata": {}, "hash": "70b1c9b951dd738e5df6065867177d2d3a30812e565d1316a1ba64c5d676f22f", "class_name": "RelatedNodeInfo"}}, "text": "You can use any data you want; in\nthis case I\u2019m using a [ small collection of my own tweets\n](https://www.dropbox.com/scl/fi/6sos49fluvfilj3sqcvoj/tinytweets.json?rlkey=qmxlaqp000kmx8zktvaj4u1vh&dl=0)\nwhich you can download, or use your own! We\u2019re going to be storing our data in\nthe nifty, open source [ Qdrant ](https://github.com/qdrant/qdrant) vector\ndatabase (which is why we got you to install it earlier). Create a new python\nfile, and load in all our dependencies:\n\n    \n    \n    from pathlib import Path\n    import qdrant_client\n    from llama_index import (\n        VectorStoreIndex,\n        ServiceContext,\n        download_loader,\n    )\n    from llama_index.llms import Ollama\n    from llama_index.storage.storage_context import StorageContext\n    from llama_index.vector_stores.qdrant import QdrantVectorStore\n\nThen load our tweets out of our JSON file using a nifty JSONReader from [\nLlamaHub ](https://llamahub.ai/l/file-json?from=all) , our convenient\ncollection of open source data connectors. This will give you a pile of\ndocuments ready to be embedded and indexed:\n\n    \n    \n    JSONReader = download_loader(\"JSONReader\")\n    loader = JSONReader()\n    documents = loader.load_data(Path('./data/tinytweets.json'))\n\nGet Qdrant ready for action by initializing it and passing it into a Storage\nContext we\u2019ll be using later:\n\n    \n    \n    client = qdrant_client.QdrantClient(\n        path=\"./qdrant_data\"\n    )\n    vector_store = QdrantVectorStore(client=client, collection_name=\"tweets\")\n    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n\nNow set up our Service Context. We\u2019ll be passing it Mixtral as the LLM so we\ncan test that things are working once we\u2019ve finished indexing; indexing itself\ndoesn\u2019t need Mixtral. By passing ` embed_model=\"local\" ` we\u2019re specifying that\nLlamaIndex will embed your data locally, which is why you needed ` torch ` and\n` transformers ` .\n\n    \n    \n    llm = Ollama(model=\"mixtral\")\n    service_context = ServiceContext.from_defaults(llm=llm,embed_model=\"local\")\n\nNow bring it all together: build the index from the documents you loaded using\nthe service and storage contexts you already set up, and give it a query:\n\n    \n    \n    index = VectorStoreIndex.from_documents(documents,service_context=service_context,storage_context=storage_context)\n    \n    query_engine = index.as_query_engine()\n    response = query_engine.query(\"What does the author think about Star Trek? Give details.\")\n    print(response)\n\nOllama will need to fire up Mixtral to answer the query, which can take a\nlittle while, so be patient! You should get output something like this (but\nwith more details):\n\n    \n    \n    Based on the provided context information, the author has a mixed opinion about Star Trek.\n\n#  Verify our index\n\nNow to prove it\u2019s not all smoke and mirrors, let\u2019s use our pre-built index.\nStart a new python file and load in dependencies again:\n\n    \n    \n    import qdrant_client\n    from llama_index import (\n        VectorStoreIndex,\n        ServiceContext,\n    )\n    from llama_index.llms import Ollama\n    from llama_index.vector_stores.qdrant import QdrantVectorStore\n\nThis time we won\u2019t need to load the data, that\u2019s already done! We will need\nthe Qdrant client and of course Mixtral again:\n\n    \n    \n    client = qdrant_client.QdrantClient(\n        path=\"./qdrant_data\"\n    )\n    vector_store = QdrantVectorStore(client=client, collection_name=\"tweets\")\n    \n    llm = Ollama(model=\"mixtral\")\n    service_context = ServiceContext.from_defaults(llm=llm,embed_model=\"local\")\n\nThis time instead of creating our index from documents we load it directly\nfrom the vector store using ` from_vector_store ` . We\u2019re also passing `\nsimilarity_top_k=20 ` to the query engine; this will mean it will fetch 20\ntweets at a time (the default is 2) to get more context and better answer the\nquestion.\n\n    \n    \n    index = VectorStoreIndex.from_vector_store(vector_store=vector_store,service_context=service_context)\n    query_engine = index.as_query_engine(similarity_top_k=20)\n    response = query_engine.query(\"Does the author like SQL? Give details.\")\n    print(response)\n\n#  Build a little web service\n\nIt\u2019s no good having an index that just runs as a script! ", "mimetype": "text/plain", "start_char_idx": 3102, "end_char_idx": 7337, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a6dcc48-3cd1-4245-bad8-18f28281a24b": {"__data__": {"id_": "5a6dcc48-3cd1-4245-bad8-18f28281a24b", "embedding": null, "metadata": {"filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "19393bcc-7aaf-4a70-a123-7165df7cae9f", "node_type": "4", "metadata": {"filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "hash": "68f12f61c892078b4cd4328018a58573934c863f42aba1d1d4bada744c788a9a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ff0d37c-c7ea-4ec6-bfae-4ada7afe0e76", "node_type": "1", "metadata": {"filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}, "hash": "7f0a1c5ac57dd5a742a6efb3c8647561af5c832e10f1fa9106483534cffcab54", "class_name": "RelatedNodeInfo"}}, "text": "Let\u2019s make an API out\nof this thing. We\u2019ll need two new dependencies:\n\n    \n    \n    pip install flask flask-cors\n\nLoad in our dependencies as before into a new file:\n\n    \n    \n    from flask import Flask, request, jsonify\n    from flask_cors import CORS, cross_origin\n    import qdrant_client\n    from llama_index.llms import Ollama\n    from llama_index import (\n        VectorStoreIndex,\n        ServiceContext,\n    )\n    from llama_index.vector_stores.qdrant import QdrantVectorStore\n\nGet the vector store, the LLM and the index loaded:\n\n    \n    \n    # re-initialize the vector store\n    client = qdrant_client.QdrantClient(\n        path=\"./qdrant_data\"\n    )\n    vector_store = QdrantVectorStore(client=client, collection_name=\"tweets\")\n    \n    # get the LLM again\n    llm = Ollama(model=\"mixtral\")\n    service_context = ServiceContext.from_defaults(llm=llm,embed_model=\"local\")\n    # load the index from the vector store\n    index = VectorStoreIndex.from_vector_store(vector_store=vector_store,service_context=service_context)\n\nSet up a really basic Flask server:\n\n    \n    \n    app = Flask(__name__)\n    cors = CORS(app)\n    app.config['CORS_HEADERS'] = 'Content-Type'\n    \n    # This is just so you can easily tell the app is running\n    @app.route('/')\n    def hello_world():\n        return 'Hello, World!'\n\nAnd add a route that accepts a query (as form data), queries the LLM and\nreturns the response:\n\n    \n    \n    @app.route('/process_form', methods=['POST'])\n    @cross_origin()\n    def process_form():\n        query = request.form.get('query')\n        if query is not None:\n            query_engine = index.as_query_engine(similarity_top_k=20)\n            response = query_engine.query(query)\n            return jsonify({\"response\": str(response)})\n        else:\n            return jsonify({\"error\": \"query field is missing\"}), 400\n    \n    if __name__ == '__main__':\n        app.run()\n\nNote those last two lines, they\u2019re important! ` flask run ` is incompatible\nwith the way LlamaIndex loads dependencies, so you will need to run this API\ndirectly like so (assuming your file is called ` app.py ` )\n\n    \n    \n    python app.py\n\nWith your API up and running, you can use cURL to send a request and verify\nit:\n\n    \n    \n    curl --location '&lt;http://127.0.0.1:5000/process_form&gt;' \\\\\n    --form 'query=\"What does the author think about Star Trek?\"'\n\n#  You\u2019re done!\n\nWe covered a few things here:\n\n  * Getting Ollama to run Mixtral locally \n  * Using LlamaIndex to query Mixtral 8x7b \n  * Building and querying an index over your data using Qdrant vector store \n  * Wrapping your index into a very simple web API \n  * All open-source, free, and running locally! \n\nI hope this was a fun, quick introduction to running local models with\nLlamaIndex!\n\n", "mimetype": "text/plain", "start_char_idx": 7337, "end_char_idx": 10107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d616b854-2510-473b-9127-2149199f9782": {"__data__": {"id_": "d616b854-2510-473b-9127-2149199f9782", "embedding": null, "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "814ddc3d-e890-4b5f-8252-2560c4136e7a", "node_type": "4", "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "9fb2edde61de382b2df9e3f6054378477ffe6b3d62bce33dc4213cc5993ef185", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6666610a-8d21-430e-bb48-4f2822ef035b", "node_type": "1", "metadata": {}, "hash": "9f1c2c288b636d288ae224ec471c6a4c7b1cd3ff5de6ea6e025cd4eeecb0419c", "class_name": "RelatedNodeInfo"}}, "text": "(Authored by Andrei Fajardo at LlamaIndex)\n\nThe llama-dataset collection. Each labelled llama-dataset is comprised of its\nassociated labelled examples. With examples, we make predictions with the\nappropriate object depending on the task. After making predictions, we can\nevaluate the performance of the object by measuring some distance between\npredictions and the corresponding references.\n\n#  Intro\n\nA few weeks back, we [ launched ](/introducing-llama-datasets-aadb9994ad9e)\nour very first set of llama-datasets, namely the ` LabelledRagDataset ` . The\nmain purpose of these llama-datasets is to provide builders with the means to\nbenchmark their LLM systems in an effective and efficient manner. In the\ncouple of weeks since that launch date, we\u2019ve amassed over a dozen `\nLabelledRagDataset ` s via both staff and community contributions (all of\nwhich are available for download through [ LlamaHub ](https://llamahub.ai) )!\n\nThe fun doesn\u2019t stop there though: today we\u2019re introducing two new llama-\ndataset types: ` LabelledEvaluatorDataset ` and the `\nLabelledPairwiseEvaluatorDataset ` . These new llama-dataset types are meant\nfor evaluating or benchmarking an LLM evaluator. Indeed, the adopted standard\nfor evaluating LLM responses is to use a strong LLM as an evaluator. This\napproach is certainly more scalable, faster, and cheaper than using human\nevaluators via crowdsourcing. However, these LLM evaluators themselves must\nalso be continuously evaluated rather than blindly trusted.\n\nIn this post, we provide a brief overview of the new llama-datasets as well as\nprovide some very interesting results from benchmarking Google\u2019s Gemini and\nOpenAI\u2019s GPT models as LLM evaluators on the MT-Bench datasets which we\u2019ve\nconverted into the new llama-dataset types.\n\n#  A primer on the new llama-datasets\n\nBefore getting into the new llama-datasets, recall that with `\nLabelledRagDataset ` our end goal was to use it to evaluate or benchmark a\nRetrieval-Augmented Generation (RAG) system. The way to do that with our\nllama-dataset abstractions is to build a ` QueryEngine ` (i.e., a RAG system)\nand then use it to make \u201cpredictions\u201d over the ` LabelledRagDataset ` . With\nthe predictions in hand, we can evaluate the quality of these predictions by\ncomparing it to the corresponding reference attributes of the `\nLabelledRagDataset ` .\n\nBenchmarking flow with LabelledRagDataset. With a query engine, predictions\nare made over every labelled example. We can then compare predicted responses\nand contexts with the reference versions (i.e., labels). This flow is\nconveniently handled via the RagEvaluatorPack.\n\nIn a similar vein, the new llama-datasets are meant to benchmark an LLM\nevaluator. Let\u2019s go through the first kind, the ` LabelledEvaluatorDataset ` .\nHere, instead of the RAG system making predictions on a ` LabelledRagDataset `\nwe have an LLM evaluator making \u201cpredictions\u201d over a `\nLabelledEvaluatorDataset ` \u2014 predictions in this context means that the LLM\nevaluator is evaluating the response produced by another LLM model to a given\nquery. As before, with the predictions in hand, we can measure the goodness of\nthe LLM evaluator\u2019s evaluations by comparing it to the corresponding reference\nattributes of the ` LabelledEvaluatorDataset ` .\n\nBenchmarking flow with LabelledEvaluatorDataset. With a supplied evaluator,\npredictions are made over every example. In this context, a prediction is an\nevaluation of the answer to the query and optional contexts and ground truth\nanswer. With these predictions in hand, we can evaluate how good the\nevaluations are by comparing them to the reference feedbacks and scores. A\nllama-pack called EvaluatorBenchmarkerPack makes benchmarking a one-liner.\n\nThe second llama-dataset we\u2019re introducing today can be considered an\nextension of the first one. The ` LabelledPairwiseEvaluatorDataset ` is\nsimilarly used for benchmarking an LLM evaluator. However, there is a subtle\ndifference in the evaluation task as here the LLM evaluator compares two\ngenerated answers from two separate LLMs. Outside of this difference, the flow\nfor using this llama-dataset to benchmark an evaluator remains the same.\n\nBenchmarking flow with LabelledPairwiseEvaluatorDataset. With a supplied\nevaluator, predictions are made over every example. In this context, a\nprediction is an evaluation of two answers to the query and optional contexts\nand ground truth answer. That is, the LLM evaluator ranks the two answers to\ndetermine the superior one. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4483, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6666610a-8d21-430e-bb48-4f2822ef035b": {"__data__": {"id_": "6666610a-8d21-430e-bb48-4f2822ef035b", "embedding": null, "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "814ddc3d-e890-4b5f-8252-2560c4136e7a", "node_type": "4", "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "9fb2edde61de382b2df9e3f6054378477ffe6b3d62bce33dc4213cc5993ef185", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d616b854-2510-473b-9127-2149199f9782", "node_type": "1", "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "25cf978af0cba8af60347b471be1d35989968272c0cdb1d2b204ad05199789f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2be8bf24-10bd-4b0b-8b54-0728121e10ee", "node_type": "1", "metadata": {}, "hash": "6a8060000300f1606108475b2de03eb24dc29e1ec35ecc572471006aa4a55fef", "class_name": "RelatedNodeInfo"}}, "text": "With these predictions in hand, we can evaluate\nhow good the evaluations are by comparing them to the reference feedbacks and\nscores. A llama-pack called EvaluatorBenchmarkerPack makes benchmarking a one-\nliner.\n\n#  Benchmarking Gemini and GPT models as LLM evaluators: Gemini achieves\nGPT-3.5 performance!\n\nIn this section, we will put our new llama-dataset types to use in order to\npit Gemini Pro against GPT models. For this, we\u2019re going to use slightly\nadapted versions of the MT-Bench dataset. These adapted versions have been\nmade available for download and use through [ LlamaHub ](https://llamahub.ai)\nalong with today\u2019s release!\n\n#  Mini MT-Bench Single Grading Dataset\n\nThis llama-dataset is a ` LabelledEvaluatorDataset ` and is a miniature\nversion of the MT-Bench single-grading dataset. In particular, we consider all\nof the 160 original questions (i.e., 80 x 2, since MT Bench is a two-turn\nquestion dataset), but only the responses produced by Llama2-70b. For the\nreference evaluations, we use GPT-4. As with the original ` LabelledRagDataset\n` , we\u2019ve produced a new llama-pack ` EvaluatorBenchmarkerPack ` (of course,\nalso made available in today\u2019s release!) to make benchmarking an LLM evaluator\non the new llama-datasets relatively effortless. The below snippet of code is\nhow you can replicate the results of this benchmark\n\n    \n    \n    from llama_index.llama_dataset import download_llama_dataset\n    from llama_index.llama_pack import download_llama_pack\n    from llama_index.evaluation import CorrectnessEvaluator\n    from llama_index.llms import Gemini\n    from llama_index import ServiceContext\n    \n    \n    # download dataset\n    evaluator_dataset, _ = download_llama_dataset(\n        \"MiniMtBenchSingleGradingDataset\", \"./mini_mt_bench_data\"\n    )# define evaluator\n    gemini_pro_context = ServiceContext.from_defaults(\n        llm = Gemini(model=\"models/gemini-pro\", temperature=0)\n    )\n    evaluator = CorrectnessEvaluator(service_context=gemini_pro_context)# download EvaluatorBenchmarkerPack and define the benchmarker\n    EvaluatorBenchmarkerPack = download_llama_pack(\"EvaluatorBenchmarkerPack\", \"./pack\")\n    evaluator_benchmarker = EvaluatorBenchmarkerPack(\n        evaluator=evaluators[\"gpt-3.5\"],\n        eval_dataset=evaluator_dataset,\n        show_progress=True,\n    )# produce the benchmark result\n    benchmark_df = await evaluator_benchmarker.arun(\n    \t\tbatch_size=5,\n    \t\tsleep_time_in_seconds=0.5\n    )\n\n#  Benchmark Results\n\nInvalid_predictions occurs whenever the LLM evaluator fails to produce the\ndesired output structure and as well as other exceptions. Correlations\nrepresent the correlations with the scores produced by each of the evaluators\nwith the reference scores produced by the reference evaluator GPT-4.\n", "mimetype": "text/plain", "start_char_idx": 4483, "end_char_idx": 7253, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2be8bf24-10bd-4b0b-8b54-0728121e10ee": {"__data__": {"id_": "2be8bf24-10bd-4b0b-8b54-0728121e10ee", "embedding": null, "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "814ddc3d-e890-4b5f-8252-2560c4136e7a", "node_type": "4", "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "9fb2edde61de382b2df9e3f6054378477ffe6b3d62bce33dc4213cc5993ef185", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6666610a-8d21-430e-bb48-4f2822ef035b", "node_type": "1", "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "80b8acbda7cec3c3b1675c576fd4121e1ee0dcc770d6999eb45e7bf7f7e2446b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "20ea7e3b-a294-4238-b615-d6e3f8a98d23", "node_type": "1", "metadata": {}, "hash": "94c88a4aa8d83a8a5c8080e135e7e463128d581b676534cd30db0fd157f6f1a4", "class_name": "RelatedNodeInfo"}}, "text": "Similarly, the remaining two metrics, MAE (i.e., mean absolute error, which is\na sum of the absolute differences between each pair of evaluator and reference\nscores) and Hamming (i.e., which counts how many times evaluator and reference\nscores are equivalent), are computed with the scores produced by the\nevaluators and those from the reference evaluator.\n\n**Observations**\n\n  * It seems that Gemini-Pro and GPT-3.5 are quite close in terms of their closeness to the reference evaluator GPT-4! \n  * As for GPT-4 versus the reference GPT-4, this is mostly used for assessing self-consistency of the LLM, for which we see it does a fairly good job at that. \n\n#  MT-Bench Human Judgement Dataset\n\nFor this benchmark, we\u2019ll evaluate the LLM evaluators on the task of ranking\ntwo LLM responses, to determine which of the two is the better one. And it is\nfor this such task that ` LabelledPairwiseEvaluatorDataset ` exists. The\nllama-dataset that we\u2019ve curated here is a slightly adapted version of the\noriginal MT-Bench Human Judgement dataset. Specifically, in the original\ndataset, there are some replications with respect to the triple (query, model\nA, model B) examples since for some of these more than one human evaluation\nwas provided. Since our prompt allows the LLM evaluator to deem a tie, and to\nour knowledge, this wasn\u2019t made an option for the human evaluators, we have\naggregated the results across the different human evaluations to get the\nproportion of times model A wins versus model B for each triple (query, model\nA, model B). We then say that human evaluators deem a tie if the proportion\nlies between 0.4 and 0.6. It should be emphasized here that the reference\nevaluations are provided by humans, and so the benchmark metrics that we\nproduce and share here represent the LLM agreement with humans.\n\n(We skip showing the code snipped to produce the results here, because they\u2019re\nessentially the same as the previously shared code snipper with the exception\nof requiring a ` PairwiseComparisonEvaluator ` instead of a `\nCorrectnessEvaluator ` .)\n\n#  Benchmark Results\n\nInvalid_predictions are as defined in the previous benchmark. Inconclusive\u2019s\nrepresent the case when an LLM evaluator flips its winner after prompting it\nwith the same evaluation task but instead flipping the order of presentation\nof the two answers (i.e. to mitigate against position bias). Two agreement\nrates, with the inclusion and exclusion of ties, are also produced \u2014 note that\nthese are both conditional in the event that the prediction (or evaluation) is\nvalid.\n\n**Observations**\n\n  * In terms of agreement rates, all three models seem quite close. Note again that these are conditional on the prediction/evaluation being valid. ", "mimetype": "text/plain", "start_char_idx": 7253, "end_char_idx": 9977, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "20ea7e3b-a294-4238-b615-d6e3f8a98d23": {"__data__": {"id_": "20ea7e3b-a294-4238-b615-d6e3f8a98d23", "embedding": null, "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "814ddc3d-e890-4b5f-8252-2560c4136e7a", "node_type": "4", "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "9fb2edde61de382b2df9e3f6054378477ffe6b3d62bce33dc4213cc5993ef185", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2be8bf24-10bd-4b0b-8b54-0728121e10ee", "node_type": "1", "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "85d2aa8a144c04b12bfeb55ed8e6f3e2045a4f72b91312f0464863433e29f528", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0fbe445-6905-4625-8859-842f44ce3799", "node_type": "1", "metadata": {}, "hash": "0288b7ca5063a1980822e53b4ebc94b872cca623b2db658375c8377ce7d11425", "class_name": "RelatedNodeInfo"}}, "text": "And so, one should \u201cdiscount\u201d these with the invalid and inconclusive counts. \n  * Gemini Pro and GPT-3.5 seem to be a bit more assertive than GPT-4 resulting in only 50\u201360 ties to GPT-4\u2019s 100 ties. \n  * Overall, it seems that Gemini Pro is up to snuff with GPT models, and would say that it outperforms GPT-3.5! \n\n#  Go now and evaluate your evaluators (and eat your veggies)!\n\n", "mimetype": "text/plain", "start_char_idx": 9977, "end_char_idx": 10356, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0fbe445-6905-4625-8859-842f44ce3799": {"__data__": {"id_": "f0fbe445-6905-4625-8859-842f44ce3799", "embedding": null, "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "814ddc3d-e890-4b5f-8252-2560c4136e7a", "node_type": "4", "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "9fb2edde61de382b2df9e3f6054378477ffe6b3d62bce33dc4213cc5993ef185", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20ea7e3b-a294-4238-b615-d6e3f8a98d23", "node_type": "1", "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}, "hash": "0b87b7138967781fe73c85a7d902cf89d0faa12ba9a1631979ebf37789654e18", "class_name": "RelatedNodeInfo"}}, "text": "It is, for obvious reasons, important to evaluate your LLM evaluators, as\nthese are now being relied upon to evaluate the performance of our LLM systems\n\u2014 a broken compass is not really helpful! With these newly introduced llama-\ndatasets, we hope that it is easy for you to compile your own benchmark\ndatasets on your own data, and then even easier to produce your benchmark\nmetrics. As mentioned before, the two llama-datasets discussed in this blog\nare available for download and use through [ LlamaHub ](https://llamahub.ai) .\nBe sure to visit and make use of the datasets there to build an exhaustive\nbenchmark suite! (We welcome contributed llama-datasets as well!)\n\n#  Related Links\n\n  * [ MT-Bench Human Judgement Benchmarking Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/evaluation/mt_bench_human_judgement.ipynb)\n  * [ MT-Bench Single Grading Benchmarking Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/evaluation/mt_bench_single_grading.ipynb)\n\n", "mimetype": "text/plain", "start_char_idx": 10356, "end_char_idx": 11372, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a30755c0-9451-431d-8aa2-5ee3bdac4daa": {"__data__": {"id_": "a30755c0-9451-431d-8aa2-5ee3bdac4daa", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-12-19-2965a2d03726.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201319", "date": "Dec 19, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-19-2965a2d03726"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a0e4afe-6a76-4665-9740-12633d9b8098", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-12-19-2965a2d03726.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201319", "date": "Dec 19, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-19-2965a2d03726"}, "hash": "83c9b4253118fa6429cc7509eb254076bc7764d87254602f7ba5ea9bb703d632", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45c16d28-b885-484a-814f-2aab8ab2d46a", "node_type": "1", "metadata": {}, "hash": "a12004793b0857919241dc7274ca93dd9e61c7c5041076da463beae2a1108319", "class_name": "RelatedNodeInfo"}}, "text": "What\u2019s up, Llama Followers ,\n\nWe\u2019re excited to bring you another week packed with the latest updates,\nfeatures, exciting community demos, insightful tutorials, guides, and\nwebinars. This week, don\u2019t miss our special holiday [ workshop\n](https://lu.ma/0eru1il4) on 12/21, where we\u2019ll dive into innovative LLM + RAG\nuse cases with Google Gemini team.\n\nGot a groundbreaking project, compelling article, or captivating video? ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 422, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45c16d28-b885-484a-814f-2aab8ab2d46a": {"__data__": {"id_": "45c16d28-b885-484a-814f-2aab8ab2d46a", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-12-19-2965a2d03726.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201319", "date": "Dec 19, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-19-2965a2d03726"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a0e4afe-6a76-4665-9740-12633d9b8098", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-12-19-2965a2d03726.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201319", "date": "Dec 19, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-19-2965a2d03726"}, "hash": "83c9b4253118fa6429cc7509eb254076bc7764d87254602f7ba5ea9bb703d632", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a30755c0-9451-431d-8aa2-5ee3bdac4daa", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-12-19-2965a2d03726.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201319", "date": "Dec 19, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-19-2965a2d03726"}, "hash": "9532a5b0ebdadde2320b996f202e73a8f710c7276dec617b8df8820e7bb62ccd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63c215d4-ab98-4e1f-940d-61506cc1c84c", "node_type": "1", "metadata": {}, "hash": "f2fa7b2dbc2b459c7f37672d0e1e5ec6086484c13f61eda309bcdfa28e2a0a5e", "class_name": "RelatedNodeInfo"}}, "text": "We\u2019re\nall ears! Reach out to us at [ news@llamaindex.ai ](mailto:news@llamaindex.ai)\n. Remember to subscribe to our newsletter via our [ website\n](https://www.llamaindex.ai/) to get all these exciting developments straight\nto your inbox.\n\n", "mimetype": "text/plain", "start_char_idx": 422, "end_char_idx": 661, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63c215d4-ab98-4e1f-940d-61506cc1c84c": {"__data__": {"id_": "63c215d4-ab98-4e1f-940d-61506cc1c84c", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-12-19-2965a2d03726.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201319", "date": "Dec 19, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-19-2965a2d03726"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a0e4afe-6a76-4665-9740-12633d9b8098", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-12-19-2965a2d03726.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201319", "date": "Dec 19, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-19-2965a2d03726"}, "hash": "83c9b4253118fa6429cc7509eb254076bc7764d87254602f7ba5ea9bb703d632", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45c16d28-b885-484a-814f-2aab8ab2d46a", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-12-19-2965a2d03726.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201319", "date": "Dec 19, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-19-2965a2d03726"}, "hash": "180c070ced7b1d00bbfc67058fcbc0a649b1c4854d846ed5a050919ed5d2f7b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2638c94-7b1e-429d-b4d2-6fad2484d474", "node_type": "1", "metadata": {}, "hash": "dd7134309aa3741ee672ae0c66591c80b1f46bfe480c6bd21b82ef4b7d1dde76", "class_name": "RelatedNodeInfo"}}, "text": "**First, the highlights:**\n\n  1. **Google Gemini Partnership:** Now offering day 1 support for Gemini API on LlamaIndex, complete with comprehensive cookbooks for advanced RAG capabilities. [ Tweet ](https://x.com/llama_index/status/1734965271004340610?s=20) . \n  2. **MistralAI Integrations:** Introduced day-0 integrations with MistralAI LLMs and Embedding model for building RAG solutions on LlamaIndex. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/mistralai.ipynb) , [ Tweet ](https://x.com/llama_index/status/1734387934722499022?s=20) . \n  3. **Docugami Multi-Doc Llama Dataset:** Launched the Multi-Doc SEC 10Q Dataset by Taqi Jaffri, offering a range of question complexities for advanced RAG research. [ Docs ](https://llamahub.ai/l/llama_datasets-docugami_kg_rag-sec_10_q) , [ Tweet ](https://twitter.com/llama_index/status/1735370350316405058?s=20) . \n  4. **Proposition-Based Retrieval:** Implemented a new retrieval unit based on propositions, enhancing QA performance with LLMs. [ Docs ](https://llamahub.ai/l/llama_packs-dense_x_retrieval?from=llama_packs) , [ Tweet ](https://x.com/llama_index/status/1735102459000013283?s=20) . \n  5. **RAG Pipeline Enhancement Guide:** Introduced a guide featuring modules like Routing, Query-Rewriting, and Agent Reasoning for more complex QA over documents. [ Docs ](https://docs.llamaindex.ai/en/latest/examples/query_transformations/query_transform_cookbook.html) . \n\n**Feature Releases and Enhancements:**\n\n  * We launched a partnership with Google Gemini, offering day 1 support for the Gemini API on LlamaIndex, including full-feature support for Gemini (text and multi-modal) and Semantic Retriever API, complemented by three comprehensive cookbooks: [ Gemini LLM ](https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/gemini.ipynb) , [ Gemini Multi-modal ](https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/gemini.ipynb) , and [ Semantic Retriever API ](https://github.com/run-llama/llama_index/blob/main/docs/examples/managed/GoogleDemo.ipynb) , promising advanced RAG capabilities and multi-modal integrations. [ Tweet ](https://x.com/llama_index/status/1734965271004340610?s=20) . \n  * We introduced day-0 integrations with the MistralAI LLMs (mistral-tiny, mistral-small, mistral-medium) and the MistralAI Embedding model for building RAG solutions with LlamaIndex both on Python and Typescript versions. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/mistralai.ipynb) , [ Tweet ](https://x.com/llama_index/status/1734387934722499022?s=20) . \n  * We launched the COVID-QA dataset on LlamaHub, a human-annotated, substantial set of 300+ QA pairs about COVID from various web articles, complete with source URLs for easy integration into RAG pipelines, offering ample scope for improvement. [ Docs ](https://llamahub.ai/l/llama_datasets-covidqa?from=llama_datasets) , [ Tweet ](https://x.com/llama_index/status/1734383167711441000?s=20) . \n  * We launched a new multi-modal template in Create-llama, enabling image input and output generation using the latest GPT-4-vision model from OpenAI, expanding possibilities for diverse use cases. [ Docs ](https://github.com/run-llama/create_llama_projects/tree/main/nextjs-multi-modal) , [ Tweet ](https://x.com/llama_index/status/1735017333180223585?s=20) . \n  * We have introduced Proposition-Based Retrieval in LlamaIndex: Implementing a new retrieval unit based on propositions, as introduced in the \u2018Dense X Retrieval\u2019 paper, enhancing QA performance with LLMs by indexing propositions and linking to the underlying text. [ Docs ](https://llamahub.ai/l/llama_packs-dense_x_retrieval?from=llama_packs) , [ Tweet ](https://x.com/llama_index/status/1735102459000013283?s=20) . \n  * We partnered with Docugami to launch a new Multi-Doc SEC 10Q Dataset by [ Taqi Jaffri ](https://twitter.com/tjaffri) , aimed at advancing QA datasets for RAG evaluation. This dataset offers a range of question complexities: Single-Doc, Single-Chunk RAG; Single-Doc, Multi-Chunk RAG; and Multi-Doc RAG, addressing the need for more intricate datasets in RAG research. [ Docs ](https://llamahub.ai/l/llama_datasets-docugami_kg_rag-sec_10_q) , [ Tweet ](https://twitter.com/llama_index/status/1735370350316405058?s=20) . \n  * We launched a SharePoint data loader, enabling direct integration of SharePoint files into LLM/RAG pipelines. [ Docs ](https://llamahub.ai/l/microsoft_sharepoint?from=all) , [ Tweet ](https://x.com/llama_index/status/1735829020187767092?s=20) . \n\n**Community Demos** :\n\n  * MemoryCache: Mozilla\u2019s new experimental project that curates your online experience into a private, on-device RAG application using PrivateGPT_AI and LlamaIndex, enhancing personal knowledge management while maintaining privacy. [ Website ](https://memorycache.ai/) , [ Repo ](https://github.com/Mozilla-Ocho/Memory-Cache) . \n  * OpenBB Finance showcases its enhanced chat widget feature in Terminal Pro, utilizing LlamaIndex\u2019s data chunking combined with Cursor AI for improved large context management and accuracy. [ Tweet ](https://x.com/josedonato__/status/1734992691090325616?s=20)\n  * AI Chatbot Starter (from the DataStax team), a web server powered by AstraDB and LlamaIndex, allows easy setup for chatting over web documentation. It can be used as a standalone service or integrated into full-stack applications, with simple credential setup and document ingestion. [ Repo ](https://github.com/datastax/ai-chatbot-starter) , [ Tweet ](https://x.com/llama_index/status/1735350801609179371?s=20) . \n  * Na2SQL (by [ **Harshad** ](https://twitter.com/HarshadSurya1c) **)** to ****Build an End-to-End SQL Analyst App on Streamlit featuring interactive database viewing, SQL query displays, and integration with Llama Index. [ Blog ](/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9) , [ Repo ](https://github.com/AI-ANK/Na2SQL/tree/main) . \n  * LionAGI (by [ **Ocean Li** ](https://twitter.com/quantoceanli) ) is an agent framework for efficient data operations and support for concurrent calls and JSON mode with OpenAI. Check it to integrate it with a Llama Index RAG pipeline for automated AI assistants like an ArXiv research assistant. [ Docs ](https://lionagi.readthedocs.io/en/latest/index.html) , [ Repo ](https://github.com/lion-agi/lionagi) . \n  * Local RAG for Windows (from Marklysze): A comprehensive resource for integrating advanced LLMs into RAG workflows using Windows Subsystem for Linux, featuring five detailed cookbooks. \n\n**Guides:**\n\n  * [ Guide ](https://docs.llamaindex.ai/en/latest/examples/query_transformations/query_transform_cookbook.html) for enhancing RAG pipelines with a Query Understanding Layer, featuring modules like Routing, Query-Rewriting, Sub-Question creation, and Agent Reasoning, all designed to enable more complex and \u2018agentic\u2019 QA over documents. \n  * [ Guide ](https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/gemini.ipynb) to Building a Restaurant Recommendation QA System with Gemini to extract structured image data and utilize multi-modal Retrieval-Augmented Generation for enhanced query responses. \n  * [ Guide ](https://github.com/run-llama/llama_index/blob/main/docs/examples/managed/GoogleDemo.ipynb) to building Advanced RAG with Safety Guardrails to create constrained RAG systems with Gemini API\u2019s semantic search, safety features, and Google Semantic Retriever integrations. \n  * Guide on [ Qdrant\u2019s Multitenancy with LlamaIndex ](https://qdrant.tech/documentation/tutorials/llama-index-multitenancy/) on setting up payload-based partitioning for user data isolation in vector services. \n  * [ Guide ](/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277) on using Prometheus \u2014 an open-source 13B LLM for RAG Evaluations, comparing it with GPT-4 evaluation with insights on its performance in terms of cost-effectiveness, accuracy, and scoring biases. \n\n**Tutorials:**\n\n  * [ Laurie\u2019s ](https://twitter.com/seldo) [ Advanced Querying & Retrieval Techniques comprehensive code-level tutorial ](https://www.youtube.com/watch?v=Y0FL7BcSigI) on 7 advanced querying and retrieval techniques including SubQuestionQuery Engine, Small-to-big retrieval, Metadata filtering, Hybrid search, Recursive Retrieval, Text to SQL, and Multi-document agents. \n  * [ Hubel Labs ](https://www.youtube.com/@hubel-labs) \u2019 Advanced RAG [ video tutorial ](https://www.youtube.com/watch?v=oDzWsynpOyI) with Llamaindex & OpenAI GPT: Sentence Window Retrieval vs Basic Chunking \n  * [ Developers Digest ](https://twitter.com/Dev__Digest) [ video tutorial ](https://www.youtube.com/watch?v=i1qTOKpTUWo) on getting started with llamaindex.ts . \n  * [ Anil\u2019s ](https://twitter.com/matchaman11) [ tutorial ](/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070) on How to train a custom GPT on your data with EmbedAI + LlamaIndex. \n\n**Webinars:**\n\n  * [ Tony Kipkemboi ](https://twitter.com/tonykipkemboi) (Streamlit) and [ Yi Ding ](https://twitter.com/yi_ding) (LlamaIndex) [ webinar ](https://www.youtube.com/watch?v=PLKkudXYCNI) on Demystifying RAG apps with LlamaIndex! \n\n**Calling all enterprises:**\n\nAre you building with LlamaIndex? We are working hard to make LlamaIndex even\nmore Enterprise-ready and have sneak peeks at our upcoming products available\nfor partners. Interested? ", "mimetype": "text/plain", "start_char_idx": 661, "end_char_idx": 10130, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2638c94-7b1e-429d-b4d2-6fad2484d474": {"__data__": {"id_": "f2638c94-7b1e-429d-b4d2-6fad2484d474", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-12-19-2965a2d03726.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201319", "date": "Dec 19, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-19-2965a2d03726"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a0e4afe-6a76-4665-9740-12633d9b8098", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-12-19-2965a2d03726.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201319", "date": "Dec 19, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-19-2965a2d03726"}, "hash": "83c9b4253118fa6429cc7509eb254076bc7764d87254602f7ba5ea9bb703d632", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63c215d4-ab98-4e1f-940d-61506cc1c84c", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-12-19-2965a2d03726.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201319", "date": "Dec 19, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-19-2965a2d03726"}, "hash": "edfd608cba2d6a8acd3bb63f30f43c6f44a4df215f3d3c75c078892f97b33161", "class_name": "RelatedNodeInfo"}}, "text": "[ Get in touch.\n](https://docs.google.com/forms/d/e/1FAIpQLScBNdM2a_fn8UZOKmFQt6lBsrd1o6FflvsdPH-\nPn3JkdlN_Rg/viewform)\n\n", "mimetype": "text/plain", "start_char_idx": 10130, "end_char_idx": 10251, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf650bd9-2ee0-4f89-9c2a-cae16a23b4fc": {"__data__": {"id_": "cf650bd9-2ee0-4f89-9c2a-cae16a23b4fc", "embedding": null, "metadata": {"filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "58f9b62f-4cd5-4a6d-868b-9c239209fc9d", "node_type": "4", "metadata": {"filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}, "hash": "04da8dd77b08637b4f37b48c597e93d228ef7a1effb485386553065b23148fea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f82eb55a-66bf-499a-a7f1-163a883839c6", "node_type": "1", "metadata": {}, "hash": "3c8776d048e265512120a8d8b1dc8f05fbd43b55dfa11e7815deeb143073a31d", "class_name": "RelatedNodeInfo"}}, "text": "The field of AI and large language models is evolving rapidly. One year ago,\nnobody ever used an LLM to enhance their productivity. Today, most of us can\u2019t\nimagine working without or not offloading at least some minor tasks to LLMs.\nDue to much research and interest, LLMs are getting better and wiser every\nday. Not only that, but their comprehension is starting to span across\nmultiple modalities. With the introduction of GPT-4-Vision and other LLMs that\nfollowed it, it seems that LLMs today can tackle and comprehend images very\nwell. Here\u2019s one example of ChatGPT describing what\u2019s in the image.\n\nUsing ChatGPT to describe images.\n\nAs you can observe, ChatGPT is quite good at comprehending and describing\nimages. We can use its ability to understand images in an RAG application,\nwhere instead of relying only on text to generate an accurate and up-to-date\nanswer, we can now combine information from text and pictures to generate more\naccurate answers than ever before. Using LlamaIndex, implementing multimodal\nRAG pipelines is as easy as it gets. Inspired by their [ multimodal cookbook\nexample ](https://github.com/run-\nllama/llama_index/blob/main/docs/examples/multi_modal/gpt4v_multi_modal_retrieval.ipynb)\n, I decided to test if I could implement a multimodal RAG application with\nNeo4j as the database.\n\nTo implement a multimodal RAG pipeline with LlamaIndex, you simply instantiate\ntwo vector stores, one for images and one for text, and then query both of\nthem in order to retrieve relevant information to generate the final answer.\n\nWorkflow diagram for the blog post. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1587, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f82eb55a-66bf-499a-a7f1-163a883839c6": {"__data__": {"id_": "f82eb55a-66bf-499a-a7f1-163a883839c6", "embedding": null, "metadata": {"filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "58f9b62f-4cd5-4a6d-868b-9c239209fc9d", "node_type": "4", "metadata": {"filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}, "hash": "04da8dd77b08637b4f37b48c597e93d228ef7a1effb485386553065b23148fea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf650bd9-2ee0-4f89-9c2a-cae16a23b4fc", "node_type": "1", "metadata": {"filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}, "hash": "998f3962a8170c4d5a5f043d4e981c02fa5b697f9e7148f167a7f6b9a3cbf985", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b05f8e9-63ab-4e60-9362-05647e0c0eed", "node_type": "1", "metadata": {}, "hash": "b746ea7454f5ca5cd691001abbaf425ef4a65af51a6d7fd1cfde1182cdb38a5b", "class_name": "RelatedNodeInfo"}}, "text": "Image by author.\n\nArticles are first split into images and text. These elements are then\nconverted into vector representations and indexed separately. For text we will\nuse _ada-002_ text embedding model, while for images we will be using [ dual\nencoder model CLIP ](https://github.com/openai/CLIP) , which can embed both\ntext and images in the same embedding space. When a question is posed by an\nend user, two vector similarity search are performed; one to find relevant\nimages and the other for documents. The results are fed into a multimodal LLM,\nwhich generates an answer for the user, demonstrating an integrated approach\nto processing and utilizing mixed media for information retrieval and response\ngeneration.\n\n_The_ [ _code is available on GitHub_\n](https://github.com/tomasonjo/blogs/blob/master/llm/neo4j_llama_multimodal.ipynb)\n_._\n\n##  Data preprocessing\n\nWe will use my Medium articles from 2022 and 2023 as the [ grounding dataset\n](https://github.com/tomasonjo/blog-datasets/blob/main/articles.zip) for an\nRAG application. The articles contain vast information about Neo4j Graph Data\nScience library and combining Neo4j with LLM frameworks. When you download\nyour own articles from Medium, you get them in an HTML format. ", "mimetype": "text/plain", "start_char_idx": 1587, "end_char_idx": 2826, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b05f8e9-63ab-4e60-9362-05647e0c0eed": {"__data__": {"id_": "3b05f8e9-63ab-4e60-9362-05647e0c0eed", "embedding": null, "metadata": {"filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "58f9b62f-4cd5-4a6d-868b-9c239209fc9d", "node_type": "4", "metadata": {"filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}, "hash": "04da8dd77b08637b4f37b48c597e93d228ef7a1effb485386553065b23148fea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f82eb55a-66bf-499a-a7f1-163a883839c6", "node_type": "1", "metadata": {"filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}, "hash": "ebfc33b4027f34eff6f0c09059711b09f028ee4a7914cca7170ba740f58d7a71", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80bdd7c7-89c1-4c22-8cb9-7b040ffda2fd", "node_type": "1", "metadata": {}, "hash": "2781a0b48adfc07096184d9288c3c6d6c1e734b9684f9794087523d3f8747237", "class_name": "RelatedNodeInfo"}}, "text": "Therefore, we\nneed to employ a bit of coding to extract text and images separately.\n\n    \n    \n    def process_html_file(file_path):\n        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n            soup = BeautifulSoup(file, \"html.parser\")\n    \n        # Find the required section\n        content_section = soup.find(\"section\", {\"data-field\": \"body\", \"class\": \"e-content\"})\n    \n        if not content_section:\n            return \"Section not found.\"\n    \n        sections = []\n        current_section = {\"header\": \"\", \"content\": \"\", \"source\": file_path.split(\"/\")[-1]}\n        images = []\n        header_found = False\n    \n        for element in content_section.find_all(recursive=True):\n            if element.name in [\"h1\", \"h2\", \"h3\", \"h4\"]:\n                if header_found and (current_section[\"content\"].strip()):\n                    sections.append(current_section)\n                current_section = {\n                    \"header\": element.get_text(),\n                    \"content\": \"\",\n                    \"source\": file_path.split(\"/\")[-1],\n                }\n                header_found = True\n            elif header_found:\n                if element.name == \"pre\":\n                    current_section[\"content\"] += f\"```{element.get_text().strip()}```\\n\"\n                elif element.name == \"img\":\n                    img_src = element.get(\"src\")\n                    img_caption = element.find_next(\"figcaption\")\n                    caption_text = img_caption.get_text().strip() if img_caption else \"\"\n                    images.append(ImageDocument(image_url=img_src))\n                elif element.name in [\"p\", \"span\", \"a\"]:\n                    current_section[\"content\"] += element.get_text().strip() + \"\\n\"\n    \n        if current_section[\"content\"].strip():\n            sections.append(current_section)\n    \n        return images, sections\n\nI won\u2019t go into details for the parsing code, but we split the text based on\nheaders **h1\u2013h4** and extract image links. Then, we simply run all the\narticles through this function to extract all relevant information.\n\n    \n    \n    all_documents = []\n    all_images = []\n    \n    # Directory to search in (current working directory)\n    directory = os.getcwd()\n    \n    # Walking through the directory\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith(\".html\"):\n                # Update the file path to be relative to the current directory\n                images, documents = process_html_file(os.path.join(root, file))\n                all_documents.extend(documents)\n                all_images.extend(images)\n    \n    text_docs = [Document(text=el.pop(\"content\"), metadata=el) for el in all_documents]\n    print(f\"Text document count: {len(text_docs)}\") # Text document count: 252\n    print(f\"Image document count: {len(all_images)}\") # Image document count: 328\n\nWe get a total of 252 text chunks and 328 images. It\u2019s a bit surprising that I\ncreated so many photos, but I know that some are only images of table results.\n", "mimetype": "text/plain", "start_char_idx": 2826, "end_char_idx": 5871, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80bdd7c7-89c1-4c22-8cb9-7b040ffda2fd": {"__data__": {"id_": "80bdd7c7-89c1-4c22-8cb9-7b040ffda2fd", "embedding": null, "metadata": {"filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "58f9b62f-4cd5-4a6d-868b-9c239209fc9d", "node_type": "4", "metadata": {"filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}, "hash": "04da8dd77b08637b4f37b48c597e93d228ef7a1effb485386553065b23148fea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b05f8e9-63ab-4e60-9362-05647e0c0eed", "node_type": "1", "metadata": {"filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}, "hash": "b364277e16296fb0b372b8f65672e3a0dec5041cb021c1820bf0bd5e92756f25", "class_name": "RelatedNodeInfo"}}, "text": "We could use a vision model to filter out irrelevant photos, but I skipped\nthis step here.\n\n##  Indexing data vectors\n\nAs mentioned, we have to instantiate two vector stores, one for images and the\nother for text. The CLIP embedding model has a dimension of 512, while the\nada-002 has 1536 dimension.\n\n    \n    \n    text_store = Neo4jVectorStore(\n        url=NEO4J_URI,\n        username=NEO4J_USERNAME,\n        password=NEO4J_PASSWORD,\n        index_name=\"text_collection\",\n        node_label=\"Chunk\",\n        embedding_dimension=1536\n    )\n    image_store = Neo4jVectorStore(\n        url=NEO4J_URI,\n        username=NEO4J_USERNAME,\n        password=NEO4J_PASSWORD,\n        index_name=\"image_collection\",\n        node_label=\"Image\",\n        embedding_dimension=512\n    \n    )\n    storage_context = StorageContext.from_defaults(vector_store=text_store)\n\nNow that the vector stores have been initiated, we use the\n**MultiModalVectorStoreIndex** to index both modalities of information we\nhave.\n\n    \n    \n    # Takes 10 min without GPU / 1 min with GPU on Google collab\n    index = MultiModalVectorStoreIndex.from_documents(\n        text_docs + all_images, storage_context=storage_context, image_vector_store=image_store\n    )\n\nUnder the hood, **MultiModalVectorStoreIndex** uses text and image embedding\nmodels to calculate the embeddings and store and index the results in Neo4j.\nOnly the URLs are stored for images, not actual base64 or other\nrepresentations of images.\n\n##  Multimodal RAG pipeline\n\nThis piece of code is copied directly from the LlamaIndex multimodal cookbook.\nWe begin by defining a multimodal LLM and the prompt template and then combine\neverything as a query engine.\n\n    \n    \n    openai_mm_llm = OpenAIMultiModal(\n        model=\"gpt-4-vision-preview\", max_new_tokens=1500\n    )\n    \n    qa_tmpl_str = (\n        \"Context information is below.\\n\"\n        \"---------------------\\n\"\n        \"{context_str}\\n\"\n        \"---------------------\\n\"\n        \"Given the context information and not prior knowledge, \"\n        \"answer the query.\\n\"\n        \"Query: {query_str}\\n\"\n        \"Answer: \"\n    )\n    qa_tmpl = PromptTemplate(qa_tmpl_str)\n    \n    query_engine = index.as_query_engine(\n        multi_modal_llm=openai_mm_llm, text_qa_template=qa_tmpl\n    )\n\nNow we can go ahead and test how well it performs.\n\n    \n    \n    query_str = \"How do vector RAG application work?\"\n    response = query_engine.query(query_str)\n    print(response)\n\n_Response_\n\nGenerated response by an LLM.\n\nWe can also visualize which images the retrieval fetched and were used to help\ninform the final answer.\n\nImage input to LLM.\n\nThe LLM got two identical images as input, which just shows that I reuse some\nof my diagrams. However, I am pleasantly surprised by CLIP embeddings as they\nwere able to retrieve he most relevant image out of the collection. In a more\nproduction setting, you might want to clean and deduplicate images, but that\nis beyond the scope of this article.\n\n##  Conclusion\n\nLLMs are evolving faster than what we are historically used to and are\nspanning across multiple modalities. I firmly believe that by the end of the\nnext year, LLMs will be soon able to comprehend videos, and be therefore able\nto pick up non-verbal cues while talking to you. On the other hand, we can use\nimages as input to RAG pipeline and enhance the variety of information passed\nto an LLM, making responses better and more accurate. The multimodal RAG\npipelines implementation with LlamaIndex and Neo4j is as easy as it gets.\n\nThe code is available on [ GitHub\n](https://github.com/tomasonjo/blogs/blob/master/llm/neo4j_llama_multimodal.ipynb)\n.\n\n", "mimetype": "text/plain", "start_char_idx": 5871, "end_char_idx": 9513, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "344e4e8d-1eb9-4a20-b3b3-8a7053f52d7b": {"__data__": {"id_": "344e4e8d-1eb9-4a20-b3b3-8a7053f52d7b", "embedding": null, "metadata": {"filename": "transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.md", "extension": ".md", "title": "Transforming Natural Language to SQL and Insights for E-commerce with LlamaIndex, GPT3.5, and Streamlit", "date": "Dec 17, 2023", "url": "https://www.llamaindex.ai/blog/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ffcfae4d-8856-4f8e-a7a2-1c4ecc49fd5c", "node_type": "4", "metadata": {"filename": "transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.md", "extension": ".md", "title": "Transforming Natural Language to SQL and Insights for E-commerce with LlamaIndex, GPT3.5, and Streamlit", "date": "Dec 17, 2023", "url": "https://www.llamaindex.ai/blog/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9"}, "hash": "7e55b9e98ebac800ea4e55e2944df80da6a132946567d40936e802e3bd520b3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b452c1da-d789-453c-8f86-81899915cae8", "node_type": "1", "metadata": {}, "hash": "e6fbc83d6df21d01afad20bc731e01269695a983bd966d6cf5f8370f059d1df0", "class_name": "RelatedNodeInfo"}}, "text": "In the dynamic world of AI and data analytics, the ability to bridge the gap\nbetween complex data queries and non-technical users is a game-changer. My\nlatest project, Na2SQL, showcases this exciting advancement. Leveraging the\npower of LlamaIndex and OpenAI\u2019s GPT-3.5, this app allows users, regardless of\ntheir SQL knowledge, to derive valuable insights from a database using simple\nnatural language.\n\n#  Features\n\n  * **Intuitive Natural Language Queries:** The core of this application is its ability to understand and process natural language queries. Users can ask questions in plain English and receive SQL queries and insights in return, all without any prior SQL experience. \n  * **Advanced Data Processing:** The app doesn\u2019t just stop at generating SQL queries; it executes these queries and analyzes the results to provide meaningful insights, making it a powerful tool for data analysis. \n  * **User-Friendly Interface with Streamlit:** I chose Streamlit for its simplicity and effectiveness in creating interactive web applications. The app\u2019s interface is straightforward, ensuring a smooth user experience. \n  * **Database Viewer:** An interactive database viewer in the sidebar on the left allows users to explore the database structure, enhancing their understanding and interaction with the data. \n\n#  The Tech Stack\n\nThis project harmoniously integrates several advanced technologies:\n\n  1. **OpenAI\u2019s GPT-3.5:** At the heart of the application is GPT-3.5, enabling the app to understand natural natural language queries and transform them into valid SQL queries. Furthermore, it also generates the final analysis considering both the user\u2019s query and the SQL output, thereby providing a comprehensive and relevant response. \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1746, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b452c1da-d789-453c-8f86-81899915cae8": {"__data__": {"id_": "b452c1da-d789-453c-8f86-81899915cae8", "embedding": null, "metadata": {"filename": "transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.md", "extension": ".md", "title": "Transforming Natural Language to SQL and Insights for E-commerce with LlamaIndex, GPT3.5, and Streamlit", "date": "Dec 17, 2023", "url": "https://www.llamaindex.ai/blog/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ffcfae4d-8856-4f8e-a7a2-1c4ecc49fd5c", "node_type": "4", "metadata": {"filename": "transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.md", "extension": ".md", "title": "Transforming Natural Language to SQL and Insights for E-commerce with LlamaIndex, GPT3.5, and Streamlit", "date": "Dec 17, 2023", "url": "https://www.llamaindex.ai/blog/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9"}, "hash": "7e55b9e98ebac800ea4e55e2944df80da6a132946567d40936e802e3bd520b3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "344e4e8d-1eb9-4a20-b3b3-8a7053f52d7b", "node_type": "1", "metadata": {"filename": "transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.md", "extension": ".md", "title": "Transforming Natural Language to SQL and Insights for E-commerce with LlamaIndex, GPT3.5, and Streamlit", "date": "Dec 17, 2023", "url": "https://www.llamaindex.ai/blog/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9"}, "hash": "fdf0e8f3d9bc43a68e5af6f56628f2d92993dd3754e0f99b797ad6f26bd84d88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10f1776e-bea1-4521-833c-9832bfcb42c5", "node_type": "1", "metadata": {}, "hash": "3cc66963885281120365a303ae020ffd4f53c63d0a6778142643daff28228410", "class_name": "RelatedNodeInfo"}}, "text": "2. **LlamaIndex:** A pivotal component of the app is LlamaIndex\u2019s SQLTableQueryEngine. This powerful tool translates natural language queries into SQL, handles the execution of these queries, and plays a significant role in the subsequent analysis using GPT 3.5. Its integration ensures a smooth transition from user inputs to database insights, culminating in a meaningful final analysis that encapsulates the entire natural language-to-SQL-to-execution process. \n  ", "mimetype": "text/plain", "start_char_idx": 1746, "end_char_idx": 2213, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10f1776e-bea1-4521-833c-9832bfcb42c5": {"__data__": {"id_": "10f1776e-bea1-4521-833c-9832bfcb42c5", "embedding": null, "metadata": {"filename": "transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.md", "extension": ".md", "title": "Transforming Natural Language to SQL and Insights for E-commerce with LlamaIndex, GPT3.5, and Streamlit", "date": "Dec 17, 2023", "url": "https://www.llamaindex.ai/blog/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ffcfae4d-8856-4f8e-a7a2-1c4ecc49fd5c", "node_type": "4", "metadata": {"filename": "transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.md", "extension": ".md", "title": "Transforming Natural Language to SQL and Insights for E-commerce with LlamaIndex, GPT3.5, and Streamlit", "date": "Dec 17, 2023", "url": "https://www.llamaindex.ai/blog/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9"}, "hash": "7e55b9e98ebac800ea4e55e2944df80da6a132946567d40936e802e3bd520b3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b452c1da-d789-453c-8f86-81899915cae8", "node_type": "1", "metadata": {"filename": "transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.md", "extension": ".md", "title": "Transforming Natural Language to SQL and Insights for E-commerce with LlamaIndex, GPT3.5, and Streamlit", "date": "Dec 17, 2023", "url": "https://www.llamaindex.ai/blog/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9"}, "hash": "76e7cf64c15c02933dc257854be4d83313871999bf79adfa7301ecbe1613288e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f6d6e1f-d2f6-4789-bde9-43b234346f30", "node_type": "1", "metadata": {}, "hash": "36d95f2654716a3956e889f1f8553e9dec6768a480fe032475e1c811568d9b1b", "class_name": "RelatedNodeInfo"}}, "text": "3. **LlamaIndex\u2019s Streamlit LlamaPack:** Using LlamaIndex\u2019s Streamlit LlamaPack, we quickly assemble and highly functional Streamlit UI. This framework significantly simplifies the UI development process, allowing for rapid deployment and an enhanced user experience. \n  ", "mimetype": "text/plain", "start_char_idx": 2213, "end_char_idx": 2484, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f6d6e1f-d2f6-4789-bde9-43b234346f30": {"__data__": {"id_": "8f6d6e1f-d2f6-4789-bde9-43b234346f30", "embedding": null, "metadata": {"filename": "transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.md", "extension": ".md", "title": "Transforming Natural Language to SQL and Insights for E-commerce with LlamaIndex, GPT3.5, and Streamlit", "date": "Dec 17, 2023", "url": "https://www.llamaindex.ai/blog/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ffcfae4d-8856-4f8e-a7a2-1c4ecc49fd5c", "node_type": "4", "metadata": {"filename": "transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.md", "extension": ".md", "title": "Transforming Natural Language to SQL and Insights for E-commerce with LlamaIndex, GPT3.5, and Streamlit", "date": "Dec 17, 2023", "url": "https://www.llamaindex.ai/blog/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9"}, "hash": "7e55b9e98ebac800ea4e55e2944df80da6a132946567d40936e802e3bd520b3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10f1776e-bea1-4521-833c-9832bfcb42c5", "node_type": "1", "metadata": {"filename": "transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.md", "extension": ".md", "title": "Transforming Natural Language to SQL and Insights for E-commerce with LlamaIndex, GPT3.5, and Streamlit", "date": "Dec 17, 2023", "url": "https://www.llamaindex.ai/blog/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9"}, "hash": "a719122a97a475fa93d252198ef3fd8fd50a9480ba97c9d398a503f29ec9c4cd", "class_name": "RelatedNodeInfo"}}, "text": "4. **SQLite Database:** The app interacts with an dummy SQLite ecommerce database, showcasing its ability to work with real-world data. \n\n#  Deep Dive into the Code\n\nIn the heart of the application lies ` app.py ` , a script that brings to life\nthe seamless interaction between natural language processing and SQL query\ngeneration.\n\nThis code is an evolution of the **Streamlit chatbot LlamaPack** available on\n[ Llama Hub ](https://github.com/run-llama/llama-\nhub/blob/main/llama_hub/llama_packs/streamlit_chatbot/base.py) , further\ntailored to meet the specific needs of ecommerce data analytics. Let's dive\ninto some key portions of the ` app.py ` script:\n\n##  1\\. Initial Imports and Setup\n\nThe script begins by importing necessary modules such as Streamlit, SQLAlchemy\nfor database interaction, LlamaIndex for language model services, and other\nessential libraries.\n\n    \n    \n    import streamlit as st\n    from sqlalchemy import create_engine, inspect\n    from typing import Dict, Any\n    \n    from llama_index import (\n        VectorStoreIndex,\n        ServiceContext,\n        download_loader,\n    )\n    from llama_index.llama_pack.base import BaseLlamaPack\n    from llama_index.llms import OpenAI\n    import openai\n    import os\n    import pandas as pd\n\n##  2\\. StreamlitChatPack Class\n\nThe ` StreamlitChatPack ` class extends the base LlamaPack, setting up the\npage and modules necessary for the app's functionality.\n\n    \n    \n    class StreamlitChatPack(BaseLlamaPack):\n    \n        def __init__(\n            self,\n            page: str = \"Natural Language to SQL Query\",\n            run_from_main: bool = False,\n            **kwargs: Any,\n        ) -&gt; None:\n            \"\"\"Init params.\"\"\"\n            self.page = page\n    \n        # ... other methods ...\n\n##  3\\. The ` run ` Method\n\nThis method is where the magic happens. It sets up the Streamlit page\nconfiguration and initializes the chat functionality.\n\n    \n    \n    def run(self, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"Run the pipeline.\"\"\"\n        import streamlit as st\n    \n        st.set_page_config(\n            page_title=f\"{self.page}\",\n            layout=\"centered\",\n            initial_sidebar_state=\"auto\",\n            menu_items=None,\n        )\n        \n        # ... rest of the run method ...\n\n##  4\\. Database Schema Viewer in the Sidebar\n\nA helpful feature is the Database Schema Viewer, conveniently located in the\nsidebar. This viewer serves as a reference tool, allowing users to see the\nstructure and content of the database tables, enhancing their understanding of\nthe data they\u2019re querying.\n\n    \n    \n    # Sidebar for database schema viewer\n    st.sidebar.markdown(\"## Database Schema Viewer\")\n    \n    # Create an inspector object\n    inspector = inspect(engine)\n    \n    # Get list of tables in the database\n    table_names = inspector.get_table_names()\n    \n    # Sidebar selection for tables\n    selected_table = st.sidebar.selectbox(\"Select a Table\", table_names)\n    \n    db_file = 'ecommerce_platform1.db'\n    conn = sqlite3.connect(db_file)\n    \n    # Display the selected table\n    if selected_table:\n        df = get_table_data(selected_table, conn)\n        st.sidebar.text(f\"Data for table '{selected_table}':\")\n        st.sidebar.dataframe(df)\n    \n    # Close the connection\n    conn.close()\n\n##  5\\. Database Interaction and LLM Integration\n\nThis part of the code loads the database from disk and initializes the LLM and\nthe service context for use with Llamaindex. I\u2019ve used GPT3.5 here, but you\ncan easily swap it out with any other LLM of your choice.\n\n    \n    \n    # Function to load database and LLM\n    def load_db_llm():\n        engine = create_engine(\"sqlite:///ecommerce_platform1.db\")\n        sql_database = SQLDatabase(engine)  # Include all tables\n        llm2 = OpenAI(temperature=0.1, model=\"gpt-3.5-turbo-1106\")\n        service_context = ServiceContext.from_defaults(llm=llm2)\n        return sql_database, service_context, engine\n    \n    sql_database, service_context, engine = load_db_llm()\n\n##  6\\. Initializing the NLSQLTableQueryEngine\n\nOne of the most critical aspects of the application is the initialization of\nthe ` NLSQLTableQueryEngine ` . This is where the app sets up the engine\nresponsible for converting natural language queries into SQL queries,\nexecuting them and generating the final response, all with the help of GPT\n3.5.\n\n    \n    \n    # Initializing the query engine\n    if \"query_engine\" not in st.session_state:\n        st.session_state[\"query_engine\"] = NLSQLTableQueryEngine(\n            sql_database=sql_database,\n            synthesize_response=True,\n            service_context=service_context\n        )\n\n##  7\\. User Interaction and Displaying Results\n\nThe script provides an interactive interface for users to input natural\nlanguage queries, which are then translated into SQL queries and executed.\n\nThe app concludes by displaying the SQL queries and responses, offering an\ninformative and engaging user experience\n\n    \n    \n    if prompt := st.chat_input(\n        \"Enter your natural language query about the database\"\n    ):  # Prompt for user input and save to chat history\n        with st.chat_message(\"user\"):\n            st.write(prompt)\n        add_to_message_history(\"user\", prompt)\n    \n    # If last message is not from assistant, generate a new response\n    if st.session_state[\"messages\"][-1][\"role\"] != \"assistant\":\n        with st.spinner():\n            with st.chat_message(\"assistant\"):\n                response = st.session_state[\"query_engine\"].query(\"User Question:\"+prompt+\". \")\n                sql_query = f\"```sql\\n{response.metadata['sql_query']}\\n```\\n**Response:**\\n{response.response}\\n\"\n                response_container = st.empty()\n                response_container.write(sql_query)\n                add_to_message_history(\"assistant\", sql_query)\n\n#  Wrapping Up\n\nThis app is more than just a tool; it\u2019s a step towards making data analytics\naccessible to a broader audience. It embodies the potential of AI in\nsimplifying complex data interactions. I invite you to explore this\napplication, witness its capabilities, and join me in this journey towards a\nmore inclusive data-driven future.\n\n[ Link to Github Repo ](https://github.com/AI-ANK/Na2SQL/tree/main)\n\n[ Connect with Me on LinkedIn\n](https://www.linkedin.com/in/harshadsuryawanshi/)\n\n[ Linkedin Post: ](https://www.linkedin.com/posts/harshadsuryawanshi_ai-\nllamaindex-streamlit-\nactivity-7141801596006440960-mCjT?utm_source=combined_share_message&utm_medium=member_desktop)\n\n## [ Harshad S. on LinkedIn: #ai #llamaindex #streamlit #largelanguagemodels\u2026\nAI Prototype 6: Transforming Natural Language to SQL and Insights for\nE-commerce with LlamaIndex, OpenAI GPT3.5, and\u2026  www.linkedin.com\n](https://www.linkedin.com/posts/harshadsuryawanshi_ai-llamaindex-streamlit-\nactivity-7141801596006440960-mCjT?utm_source=combined_share_message&utm_medium=member_desktop&source=post_page\n-----e08edefa21f9--------------------------------)\n\n", "mimetype": "text/plain", "start_char_idx": 2484, "end_char_idx": 9469, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f559afe9-d5dd-434b-930a-90abcd55113b": {"__data__": {"id_": "f559afe9-d5dd-434b-930a-90abcd55113b", "embedding": null, "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c09340b-c9bc-4e99-81d0-4d3b5f27a93f", "node_type": "4", "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "hash": "40ab5d3b57a9f84bc51dc27ed0ea7afb6cdedd3504ce981ef4b8756ec50cb055", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50d9a68a-f6dd-4452-bb2f-e2c67611c944", "node_type": "1", "metadata": {}, "hash": "0706bfd19e3dd85db750d393bd1b089b416235340b2de36f1971660d257b85a1", "class_name": "RelatedNodeInfo"}}, "text": "#  Introduction\n\nEvaluation is a critical component in enhancing your Retrieval-Augmented\nGeneration (RAG) pipeline, traditionally reliant on GPT-4. However, the open-\nsource [ Prometheus model ](https://huggingface.co/kaist-\nai/prometheus-13b-v1.0) has recently emerged as a notable alternative for such\nevaluation tasks.\n\nIn this blog post, we will demonstrate how to effectively use the Prometheus\nmodel for evaluation purposes, integrating it smoothly with the LlamaIndex\nframework by comparing it with GPT-4 evaluation. Our primary focus will be on\nassessing RAG using our standard metrics: Correctness, Faithfulness, and\nContext Relevancy. To provide a clearer understanding, here\u2019s what each metric\nentails:\n\n  1. **Correctness** : Assesses whether the generated answer aligns with the reference answer, given the query (this necessitates labeled data). \n  2. **Faithfulness** : Measures if the answer remains true to the retrieved contexts, essentially checking for the absence of hallucinations. \n  3. **Context Relevancy** : Evaluate the relevance of both the retrieved context and the answer to the query. \n\nFor an in-depth exploration, our documentation is available [ here\n](https://docs.llamaindex.ai/en/stable/module_guides/evaluating/root.html) .\n\nFor those who are exploring the Prometheus model for the first time, the paper\nsummary by [ Andrei ](https://www.linkedin.com/in/nerdai/) is an excellent\nresource to gain a better understanding.\n\nA crucial aspect to remember when using the Prometheus model is its dependence\non rubric scores within the prompt for effective evaluation. An example of\nsuch Rubric scores in the context of ` Correctness Evaluation ` is as follows:\n\n> ###Score Rubrics: Score 1: If the generated answer is not relevant to the\n> user query and reference answer. Score 2: If the generated answer is\n> according to reference answer but not relevant to user query. Score 3: If\n> the generated answer is relevant to the user query and reference answer but\n> contains mistakes. Score 4: If the generated answer is relevant to the user\n> query and has the exact same metrics as the reference answer, but it is not\n> as concise. Score 5: If the generated answer is relevant to the user query\n> and fully correct according to the reference answer.\n\nYou\u2019ll find comprehensive details on this in the prompts section of this\ntutorial.\n\nFor a detailed walkthrough of the code, feel free to follow along with our [\nGoogle Colab Notebook ](https://colab.research.google.com/github/run-\nllama/llama_index/blob/main/docs/examples/evaluation/prometheus_evaluation.ipynb)\naccompanying this blog post. In the notebook, we conducted evaluations on both\nthe ` Paul Graham Essay Text ` and the ` Llama2 Paper ` . However, for this\nblog post, we\u2019ll focus exclusively on the Llama2 Paper, as it revealed some\nparticularly interesting insights.\n\n#  Outline:\n\n  1. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2882, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50d9a68a-f6dd-4452-bb2f-e2c67611c944": {"__data__": {"id_": "50d9a68a-f6dd-4452-bb2f-e2c67611c944", "embedding": null, "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c09340b-c9bc-4e99-81d0-4d3b5f27a93f", "node_type": "4", "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "hash": "40ab5d3b57a9f84bc51dc27ed0ea7afb6cdedd3504ce981ef4b8756ec50cb055", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f559afe9-d5dd-434b-930a-90abcd55113b", "node_type": "1", "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "hash": "6922769d3dc425cf4139c8d02cb101608ff3da755041fa12a4f1390dac05ca9b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec2fb48a-0649-452b-89c3-3f38ccae78c8", "node_type": "1", "metadata": {}, "hash": "c7578128e508f7818ea986b5965838f64a10ae41890d54d91f7421a91cb6ec7e", "class_name": "RelatedNodeInfo"}}, "text": "Setup Evaluation Pipeline. \n\n  * Download Dataset. \n  * Define LLMs (Prometheus, GPT-4) needed for evaluation. \n  * Define Correctness, Faithfulness, and Relevancy prompt templates. \n  * Define Prometheus, GPT-4 Evaluators, and Batch Eval Runner. \n  * Run the Correctness, Faithfulness, and Relevancy Evaluation over the Llama2 dataset. \n\n2\\. Results\n\n  * Correctness Evaluation score distribution between Prometheus and GPT-4. \n  * Feedback comparison between Prometheus and GPT-4 for correctness evaluation. \n  * Faithfulness and Relevancy Evaluation scores with Prometheus and GPT-4. \n  * Hamming Distance comparison between Prometheus and GPT-4. \n  * Feedback comparison between Prometheus and GPT-4 for Faithfulness and Relevancy \n\n3\\. Summary with Cost Analysis.\n\n#  Setup Evaluation Pipeline\n\nPlease be aware that certain functions mentioned here are not defined in\ndetail within the blog post. We have showcased only the essential parts of the\npipeline to provide an overview of its setup. For a comprehensive code\nwalkthrough, we recommend visiting our [ Google Colab Notebook\n](https://colab.research.google.com/github/run-\nllama/llama_index/blob/main/docs/examples/evaluation/prometheus_evaluation.ipynb)\n.\n\n##  Download Dataset\n\nWe will use the Llama2 paper dataset from Llama Datasets which contains 100\nquestions and their reference answers.\n\n    \n    \n    from llama_index.llama_dataset import download_llama_dataset\n    \n    llama2_rag_dataset, llama2_documents = download_llama_dataset(\n        \"Llama2PaperDataset\", \"./data/llama2\"\n    )\n\n##  Define Prometheus LLM hosted on HuggingFace And OpenAI for creating an\nIndex (RAG) pipeline\n\nWe need to host the model on HF Inference endpoint using Nvidia A100 GPU, 80\nGB RAM.\n\n    \n    \n    from llama_index.llms import HuggingFaceInferenceAPI\n    import os\n    \n    HF_TOKEN = \"YOUR HF TOKEN\"\n    HF_ENDPOINT_URL = \"HF END POINT URL\"\n    \n    prometheus_llm = HuggingFaceInferenceAPI(\n        model_name=HF_ENDPOINT_URL,\n        token=HF_TOKEN,\n        temperature=0.1,\n        do_sample=True,\n        top_p=0.95,\n        top_k=40,\n        repetition_penalty=1.1,\n    )\n    \n    os.environ[\"OPENAI_API_KEY\"] = \"YOUR OPENAI API KEY\"\n    \n    from llama_index.llms import OpenAI\n    \n    gpt4_llm = OpenAI(\"gpt-4\")\n\n##  Prompt templates.\n\nWe will use the same prompts for the Prometheus model and GPT-4 to make\nconsistent performance comparisons.\n\n**Correctness Evaluation Prompt:**\n\n    \n    \n    prometheus_correctness_eval_prompt_template = \"\"\"###Task Description: An instruction (might include an Input inside it), a query, a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given. \n       1. ", "mimetype": "text/plain", "start_char_idx": 2882, "end_char_idx": 5615, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec2fb48a-0649-452b-89c3-3f38ccae78c8": {"__data__": {"id_": "ec2fb48a-0649-452b-89c3-3f38ccae78c8", "embedding": null, "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c09340b-c9bc-4e99-81d0-4d3b5f27a93f", "node_type": "4", "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "hash": "40ab5d3b57a9f84bc51dc27ed0ea7afb6cdedd3504ce981ef4b8756ec50cb055", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50d9a68a-f6dd-4452-bb2f-e2c67611c944", "node_type": "1", "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "hash": "0ab452b12bba3104530ffc62c1233aad654c93fca88590c1c2a119fd74ab9fca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9988d12a-5944-4e91-abf0-d66469d3ca5a", "node_type": "1", "metadata": {}, "hash": "3c418ac01dfb90bed76d76b81ce56f9a99e8e3306c9f428bfebb0cb785b28dbf", "class_name": "RelatedNodeInfo"}}, "text": "Write a detailed feedback that assesses the quality of the response strictly based on the given score rubric, not evaluating in general. \n       2. After writing a feedback, write a score that is either 1 or 2 or 3 or 4 or 5. You should refer to the score rubric. \n       3. The output format should look as follows: 'Feedback: (write a feedback for criteria) [RESULT] (1 or 2 or 3 or 4 or 5)'\n       4. Please do not generate any other opening, closing, and explanations. \n       ", "mimetype": "text/plain", "start_char_idx": 5615, "end_char_idx": 6096, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9988d12a-5944-4e91-abf0-d66469d3ca5a": {"__data__": {"id_": "9988d12a-5944-4e91-abf0-d66469d3ca5a", "embedding": null, "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c09340b-c9bc-4e99-81d0-4d3b5f27a93f", "node_type": "4", "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "hash": "40ab5d3b57a9f84bc51dc27ed0ea7afb6cdedd3504ce981ef4b8756ec50cb055", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec2fb48a-0649-452b-89c3-3f38ccae78c8", "node_type": "1", "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "hash": "726ce6dee94cedae15976d0f3431d3ff84cc4cb27b6549f8213f347025e95a2e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4529d783-3a57-43d8-9913-a01811e3cf29", "node_type": "1", "metadata": {}, "hash": "51e368e69fa3cf81fb01bf76f711487fb851e0f9c91e88cc6f8cd69df71c2fa6", "class_name": "RelatedNodeInfo"}}, "text": "5. Only evaluate on common things between generated answer and reference answer. Don't evaluate on things which are present in reference answer but not in generated answer.\n    \n       ###The instruction to evaluate: Your task is to evaluate the generated answer and reference answer for the query: {query}\n       \n       ###Generate answer to evaluate: {generated_answer} \n    \n       ###Reference Answer (Score 5): {reference_answer}\n                \n       ###Score Rubrics: \n       Score 1: If the generated answer is not relevant to the user query and reference answer.\n       Score 2: If the generated answer is according to reference answer but not relevant to user query.\n       Score 3: If the generated answer is relevant to the user query and reference answer but contains mistakes.\n       Score 4: If the generated answer is relevant to the user query and has the exact same metrics as the reference answer, but it is not as concise.\n       Score 5: If the generated answer is relevant to the user query and fully correct according to the reference answer.\n        \n       ###Feedback:\"\"\"\n    \n\n**Faithfulness Evaluation Prompt:**\n\n    \n    \n    prometheus_faithfulness_eval_prompt_template= \"\"\"###Task Description: An instruction (might include an Input inside it), an information, a context, and a score rubric representing evaluation criteria are given.\n    1. ", "mimetype": "text/plain", "start_char_idx": 6096, "end_char_idx": 7472, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4529d783-3a57-43d8-9913-a01811e3cf29": {"__data__": {"id_": "4529d783-3a57-43d8-9913-a01811e3cf29", "embedding": null, "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c09340b-c9bc-4e99-81d0-4d3b5f27a93f", "node_type": "4", "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "hash": "40ab5d3b57a9f84bc51dc27ed0ea7afb6cdedd3504ce981ef4b8756ec50cb055", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9988d12a-5944-4e91-abf0-d66469d3ca5a", "node_type": "1", "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "hash": "a922ee4633d7b486ff29e5449a23544ff015ae99d2f02bbd465f7e95e8494926", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9f7b424-d60b-417d-a6e0-934fc2d8b648", "node_type": "1", "metadata": {}, "hash": "3b5069911888b0328740e181a5e6abcb4a44f887c66ac5953db3192cc48fb7ed", "class_name": "RelatedNodeInfo"}}, "text": "You are provided with evaluation task with the help of information, context information to give result based on score rubrics.\n    2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general.\n    3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric.\n    4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)\u201d\n    5. Please do not generate any other opening, closing, and explanations.\n    \n    ###The instruction to evaluate: Your task is to evaluate if the given piece of information is supported by context.\n    \n    ###Information: {query_str}\n    \n    ###Context: {context_str}\n    \n    ###Score Rubrics:\n    Score YES: If the given piece of information is supported by context.\n    Score NO: If the given piece of information is not supported by context\n    \n    ###Feedback: \"\"\"\n    \n    prometheus_faithfulness_refine_prompt_template= \"\"\"###Task Description: An instruction (might include an Input inside it), a information, a context information, an existing answer, and a score rubric representing a evaluation criteria are given.\n    1. ", "mimetype": "text/plain", "start_char_idx": 7472, "end_char_idx": 8668, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9f7b424-d60b-417d-a6e0-934fc2d8b648": {"__data__": {"id_": "b9f7b424-d60b-417d-a6e0-934fc2d8b648", "embedding": null, "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c09340b-c9bc-4e99-81d0-4d3b5f27a93f", "node_type": "4", "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "hash": "40ab5d3b57a9f84bc51dc27ed0ea7afb6cdedd3504ce981ef4b8756ec50cb055", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4529d783-3a57-43d8-9913-a01811e3cf29", "node_type": "1", "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "hash": "fce450b88ba50b806bde41b692a3886602b88620fbcf8b564ea3edda919dd5bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4de9fca-ab0c-4684-8fbc-9bdea3f64b4b", "node_type": "1", "metadata": {}, "hash": "83d68dd86dd7855b8ebcaf45c896a623cfd3f6e1dab8e1f82979f8efab0174f1", "class_name": "RelatedNodeInfo"}}, "text": "You are provided with evaluation task with the help of information, context information and an existing answer.\n    2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general.\n    3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric.\n    4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)\"\n    5. Please do not generate any other opening, closing, and explanations.\n    \n    ###The instruction to evaluate: If the information is present in the context and also provided with an existing answer.\n    \n    ###Existing answer: {existing_answer}\n    \n    ###Information: {query_str}\n    \n    ###Context: {context_msg}\n    \n    ###Score Rubrics:\n    Score YES: If the existing answer is already YES or If the Information is present in the context.\n    Score NO: If the existing answer is NO and If the Information is not present in the context.\n    \n    ###Feedback: \"\"\"\n\n**Relevancy Evaluation Prompt:**\n\n    \n    \n    prometheus_relevancy_eval_prompt_template = \"\"\"###Task Description: An instruction (might include an Input inside it), a query with response, context, and a score rubric representing evaluation criteria are given. \n           1. ", "mimetype": "text/plain", "start_char_idx": 8668, "end_char_idx": 9965, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4de9fca-ab0c-4684-8fbc-9bdea3f64b4b": {"__data__": {"id_": "b4de9fca-ab0c-4684-8fbc-9bdea3f64b4b", "embedding": null, "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c09340b-c9bc-4e99-81d0-4d3b5f27a93f", "node_type": "4", "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "hash": "40ab5d3b57a9f84bc51dc27ed0ea7afb6cdedd3504ce981ef4b8756ec50cb055", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9f7b424-d60b-417d-a6e0-934fc2d8b648", "node_type": "1", "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "hash": "6ea3062a699bbff4cfe57ede01fdab07f86a03ffb35daf5c14ffb0715a8a1ab8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d1eadd2-a059-450e-80f6-c1f8a176eabc", "node_type": "1", "metadata": {}, "hash": "bec06c8a846b8165a3b4bd7e9fdff80c7bec7502f6882d21a901ff24767668d7", "class_name": "RelatedNodeInfo"}}, "text": "You are provided with evaluation task with the help of a query with response and context.\n           2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general. \n           3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric. \n           4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)\u201d \n           5. Please do not generate any other opening, closing, and explanations. \n    \n            ###The instruction to evaluate: Your task is to evaluate if the response for the query is in line with the context information provided.\n    \n            ###Query and Response: {query_str} \n    \n            ###Context: {context_str}\n                \n            ###Score Rubrics: \n            Score YES: If the response for the query is in line with the context information provided.\n            Score NO: If the response for the query is not in line with the context information provided.\n        \n            ###Feedback: \"\"\"\n    \n    prometheus_relevancy_refine_prompt_template = \"\"\"###Task Description: An instruction (might include an Input inside it), a query with response, context, an existing answer, and a score rubric representing a evaluation criteria are given. \n       1. ", "mimetype": "text/plain", "start_char_idx": 9965, "end_char_idx": 11297, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d1eadd2-a059-450e-80f6-c1f8a176eabc": {"__data__": {"id_": "8d1eadd2-a059-450e-80f6-c1f8a176eabc", "embedding": null, "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c09340b-c9bc-4e99-81d0-4d3b5f27a93f", "node_type": "4", "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "hash": "40ab5d3b57a9f84bc51dc27ed0ea7afb6cdedd3504ce981ef4b8756ec50cb055", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b4de9fca-ab0c-4684-8fbc-9bdea3f64b4b", "node_type": "1", "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "hash": "29b6ef7865f32498fb9947dd97361baeaa01faa177cd808f5b1b3c95f7251af1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62724080-a637-43af-a485-d5d285d3b98f", "node_type": "1", "metadata": {}, "hash": "a2225f5dc0abac92be2963bd0f313d6d3e5bad680f398d1006f5e65a2e811e77", "class_name": "RelatedNodeInfo"}}, "text": "You are provided with evaluation task with the help of a query with response and context and an existing answer.\n       2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general. \n       3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric. \n       4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)\" \n       5. Please do not generate any other opening, closing, and explanations. \n    \n       ###The instruction to evaluate: Your task is to evaluate if the response for the query is in line with the context information provided.\n    \n       ###Query and Response: {query_str} \n    \n       ###Context: {context_str}\n                \n       ###Score Rubrics: \n       Score YES: If the existing answer is already YES or If the response for the query is in line with the context information provided.\n       Score NO: If the existing answer is NO and If the response for the query is in line with the context information provided.\n        \n       ###Feedback: \"\"\"\n\n##  Define Correctness, FaithFulness, Relevancy Evaluators\n\n    \n    \n    from llama_index import ServiceContext\n    from llama_index.evaluation import (\n        CorrectnessEvaluator,\n        FaithfulnessEvaluator,\n        RelevancyEvaluator,\n    )\n    from llama_index.callbacks import CallbackManager, TokenCountingHandler\n    import tiktoken\n    \n    # Provide Prometheus model in service_context\n    prometheus_service_context = ServiceContext.from_defaults(llm=prometheus_llm)\n    \n    # CorrectnessEvaluator with Prometheus model\n    prometheus_correctness_evaluator = CorrectnessEvaluator(\n        service_context=prometheus_service_context,\n        parser_function=parser_function,\n        eval_template=prometheus_correctness_eval_prompt_template,\n    )\n    \n    # FaithfulnessEvaluator with Prometheus model\n    prometheus_faithfulness_evaluator = FaithfulnessEvaluator(\n        service_context=prometheus_service_context,\n        eval_template=prometheus_faithfulness_eval_prompt_template,\n        refine_template=prometheus_faithfulness_refine_prompt_template,\n    )\n    \n    # RelevancyEvaluator with Prometheus model\n    prometheus_relevancy_evaluator = RelevancyEvaluator(\n        service_context=prometheus_service_context,\n        eval_template=prometheus_relevancy_eval_prompt_template,\n        refine_template=prometheus_relevancy_refine_prompt_template,\n    )\n    \n    # Set the encoding model to `gpt-4` for token counting.\n    token_counter = TokenCountingHandler(\n        tokenizer=tiktoken.encoding_for_model(\"gpt-4\").encode\n    )\n    \n    callback_manager = CallbackManager([token_counter])\n    \n    # Provide GPT-4 model in service_context\n    gpt4_service_context = ServiceContext.from_defaults(\n        llm=gpt4_llm, callback_manager=callback_manager\n    )\n    \n    # CorrectnessEvaluator with GPT-4 model\n    gpt4_correctness_evaluator = CorrectnessEvaluator(\n        service_context=gpt4_service_context,\n        # parser_function=parser_function,\n    )\n    \n    # FaithfulnessEvaluator with GPT-4 model\n    gpt4_faithfulness_evaluator = FaithfulnessEvaluator(\n        service_context=gpt4_service_context,\n        eval_template=prometheus_faithfulness_eval_prompt_template,\n        refine_template=prometheus_faithfulness_refine_prompt_template,\n    )\n    \n    # RelevancyEvaluator with GPT-4 model\n    gpt4_relevancy_evaluator = RelevancyEvaluator(\n        service_context=gpt4_service_context,\n        eval_template=prometheus_relevancy_eval_prompt_template,\n        refine_template=prometheus_relevancy_refine_prompt_template,\n    )\n    \n    # create a dictionary of evaluators\n    prometheus_evaluators = {\n        \"correctness\": prometheus_correctness_evaluator,\n        \"faithfulness\": prometheus_faithfulness_evaluator,\n        \"relevancy\": prometheus_relevancy_evaluator,\n    }\n    \n    gpt4_evaluators = {\n        \"correctness\": gpt4_correctness_evaluator,\n        \"faithfulness\": gpt4_faithfulness_evaluator,\n        \"relevancy\": gpt4_relevancy_evaluator,\n    }\n\n##  Function to run batch evaluations on defined evaluators\n\n    \n    \n    from llama_index.evaluation import BatchEvalRunner\n    \n    \n    async def batch_eval_runner(\n        evaluators, query_engine, questions, reference=None, num_workers=8\n    ):\n        batch_runner = BatchEvalRunner(\n            evaluators, workers=num_workers, show_progress=True\n        )\n    \n        eval_results = await batch_runner.aevaluate_queries(\n            query_engine, queries=questions, reference=reference\n        )\n    \n        return eval_results\n\n##  Get Query Engine, Questions, and References.\n\n    \n    \n    query_engine, rag_dataset = create_query_engine_rag_dataset(\"./data/llama2\")\n    \n    questions = [example.query for example in rag_dataset.examples]\n    \n    reference = [[example.reference_answer] for example in rag_dataset.examples]\n\n##  Compute Correctness, Faithfulness, and Relevancy Evaluation.\n\n    \n    \n    prometheus_eval_results = await batch_eval_runner(\n        prometheus_evaluators, query_engine, questions, reference\n    )\n    \n    gpt4_eval_results = await batch_eval_runner(\n        gpt4_evaluators, query_engine, questions, reference\n    )\n\n#  Results\n\n##  Correctness Evaluation score distribution.\n\n##  With the Prometheus Model:\n\n> 3.0: 56.0, 1.0: 26.0, 5.0: 9.0, 4.0: 8.0, 2.0: 1.0\n\n##  With GPT-4 Model:\n\n> 4.5: 57.99, 1.0: 6.0, 4.0: 12.0, 5.0: 10.0, 2.0: 5.0, 3.5: 5.0, 2.5: 3.0,\n> 3.0: 1.0\n\n##  **Observation:**\n\nPrometheus Evaluator gives more ` 3.0 ` scores compared to GPT-4 which gives `\n4.5 ` . Let\u2019s investigate it with the help of feedback.\n\n##  Feedback comparison between Prometheus and GPT-4 for correctness.\n\n> **_Query:_ ** _Based on the abstract of \u201cLlama 2: Open Foundation and Fine-\n> Tuned Chat Models,\u201d what are the two primary objectives achieved in this\n> work, and what is the range of parameters for the large language models\n> developed?_\n>\n> **_Generated Answer:_ ** _The two primary objectives achieved in this work\n> are the development and release of Llama 2, a collection of pretrained and\n> fine-tuned large language models (LLMs), and the optimization of these\n> models for dialogue use cases. The range of parameters for the large\n> language models developed is from 7 billion to 70 billion._\n>\n> **_Reference Answer:_ ** _The two primary objectives achieved in the work\n> described in the abstract of \u201cLlama 2: Open Foundation and Fine-Tuned Chat\n> Models\u201d are: 1. The development and release of a collection of pretrained\n> and fine-tuned large language models (LLMs) specifically optimized for\n> dialogue use cases. 2. The demonstration that these fine-tuned LLMs,\n> referred to as Llama 2-Chat, outperform open-source chat models on most\n> benchmarks tested and may be a suitable substitute for closed-source models,\n> particularly in terms of helpfulness and safety based on human evaluations.\n> The range of parameters for the large language models developed in this work\n> is from 7 billion to 70 billion parameters._\n>\n> **_Prometheus Feedback:_ ** _The generated response is relevant to the user\n> query and correctly identifies the two primary objectives of the work\n> described in the abstract of \u201cLlama 2: Open Foundation and Fine-Tuned Chat\n> Models.\u201d However, it does not mention the demonstration of the fine-tuned\n> LLMs outperforming open-source chat models on most benchmarks tested, which\n> is a key point in the reference response. The range of parameters for the\n> large language models developed is correctly identified, but the response\n> does not mention the specific models referred to as Llama 2-Chat. So the\n> overall score is 3._\n>\n> **_Prometheus Score:_ ** _3.0_\n>\n> **_GPT-4 Feedback:_ ** _The generated answer is relevant and almost fully\n> correct. It correctly identifies the two primary objectives and the range of\n> parameters for the large language models. However, it misses the detail\n> about Llama 2-Chat outperforming other models on most benchmarks and\n> potentially being a suitable substitute for closed-source models._\n>\n> **_GPT-4 Score:_ ** _4.5_\n\n**Observation:**\n\nThe feedback from Prometheus is a little more precise compared to GPT-4 and it\npenalizes and gives a score of ` 3.0 ` but GPT-4 gives a score of ` 4.5 ` .\nPrometheus penalizes more even if some of the text in the reference answer is\nmissed in the generated answer.\n\n##  Faithfulness and Relevancy Evaluation scores.\n\n##  With the Prometheus Model:\n\n> faithfulness Score: 0.39 relevancy Score: 0.57\n\n##  With GPT-4 Model:\n\n> faithfulness Score: 0.93 relevancy Score: 0.98\n\n**Observation:**\n\nWe see a higher faithfulness and relevancy score with GPT-4 compared to the\nPrometheus model. Let\u2019s investigate it through feedback.\n\n##  Hamming Distance comparison between Prometheus and GPT-4:\n\n> Faithfulness Hamming Distance: 58 Relevancy Hamming Distance: 41\n\n##  Observation:\n\nThe comparison reveals that approximately ` 42% ` of the scores in case of `\nFaithfulness ` and ` 59% ` in case of ` Relevancy ` are common between\nPrometheus and GPT-4 evaluations. This indicates a decent amount of\ncorrelation in terms of faithfulness and relevance scoring between the\nPrometheus and GPT-4 models.\n\n##  Feedback comparison between Prometheus and GPT-4 for Faithfulness and\nRelevancy\n\n> **Query:** Based on the abstract of \u201cLlama 2: Open Foundation and Fine-Tuned\n> Chat Models,\u201d what are the two primary objectives achieved in this work, and\n> what is the range of parameters for the large language models developed?\n> Generated Answer: The two primary objectives achieved in this work are the\n> development and release of Llama 2, a collection of pretrained and fine-\n> tuned large language models (LLMs), and the optimization of these models for\n> dialogue use cases. The range of parameters for the large language models\n> developed is from 7 billion to 70 billion.\n>\n> **Context-1:** Llama 2 : Open Foundation and Fine-Tuned Chat Models Hugo\n> Touvron\u2217Louis Martin\u2020Kevin Stone\u2020 Peter Albert Amjad Almahairi Yasmine\n> Babaei Nikolay Bashlykov Soumya Batra Prajjwal Bhargava Shruti Bhosale Dan\n> Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen Guillem Cucurull David\n> Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller Cynthia Gao Vedanuj\n> Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou Hakan Inan\n> Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n> Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana\n> Liskovich Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar\n> Mishra Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta\n> Kalyan Saladi Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian\n> Xiaoqing Ellen Tan Binh Tang Ross Taylor Adina Williams Jian Xiang Kuan\n> Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang Angela Fan Melanie Kambadur\n> Sharan Narang Aurelien Rodriguez Robert Stojnic Sergey Edunov Thomas\n> Scialom\u2217 GenAI, Meta Abstract In this work, we develop and release Llama 2,\n> a collection of pretrained and fine-tuned large language models (LLMs)\n> ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned\n> LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our models\n> outperform open-source chat models on most benchmarks we tested, and based\n> on\n> ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-\n> source models. We provide a detailed description of our approach to fine-\n> tuning and safety improvements of Llama 2-Chat in order to enable the\n> community to build on our work and contribute to the responsible development\n> of LLMs. \u2217Equal contribution, corresponding authors: {tscialom,\n> htouvron}@meta.com \u2020Second author Contributions for all the authors can be\n> found in Section A.1.arXiv:2307.09288v2 [cs.CL] 19 Jul 2023\n>\n> **Context-2:** (2021)alsoilluminatesthedifficultiestiedtochatbot-oriented\n> LLMs, with concerns ranging from privacy to misleading expertise claims.\n> Deng et al. ", "mimetype": "text/plain", "start_char_idx": 11297, "end_char_idx": 23370, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62724080-a637-43af-a485-d5d285d3b98f": {"__data__": {"id_": "62724080-a637-43af-a485-d5d285d3b98f", "embedding": null, "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c09340b-c9bc-4e99-81d0-4d3b5f27a93f", "node_type": "4", "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "hash": "40ab5d3b57a9f84bc51dc27ed0ea7afb6cdedd3504ce981ef4b8756ec50cb055", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d1eadd2-a059-450e-80f6-c1f8a176eabc", "node_type": "1", "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "hash": "e1299a4e40d78553829859da3a36f722b573341fc97fe62215f4808cbce8e4ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78e4a0df-c49d-4182-bd2d-e64424e27f44", "node_type": "1", "metadata": {}, "hash": "9f9ea2b00fdccf4f42b5f7a6a497c901a7ec1ac3c0d45f88d3767b61e6b68148", "class_name": "RelatedNodeInfo"}}, "text": "(2023) proposes a taxonomic framework to tackle these issues,\n> and Bergman et al. (2022) delves into the balance between potential positive\n> and negative impacts from releasing dialogue models.\n> InvestigationsintoredteamingrevealspecificchallengesintunedLLMs,withstudiesbyGangulietal.", "mimetype": "text/plain", "start_char_idx": 23370, "end_char_idx": 23657, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78e4a0df-c49d-4182-bd2d-e64424e27f44": {"__data__": {"id_": "78e4a0df-c49d-4182-bd2d-e64424e27f44", "embedding": null, "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c09340b-c9bc-4e99-81d0-4d3b5f27a93f", "node_type": "4", "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "hash": "40ab5d3b57a9f84bc51dc27ed0ea7afb6cdedd3504ce981ef4b8756ec50cb055", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62724080-a637-43af-a485-d5d285d3b98f", "node_type": "1", "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "hash": "1829eee7ce292e59c5e2a7c65dddb66e4e224907c199a84781d111839d176a61", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbab962b-3ab5-45d4-bfe7-0328168d8d39", "node_type": "1", "metadata": {}, "hash": "8f19df421909ef2698d43917d901fbd04b00bcc0ffbd70ef99cad944e63337ef", "class_name": "RelatedNodeInfo"}}, "text": "(2022)\n> and Zhuoet al. (2023) showcasing a variety ofsuccessful attack typesand\n> their effects onthe generation of harmful content. National security\n> agencies and various researchers, such as (Mialon et al., 2023), have also\n> raisedredflagsaroundadvancedemergentmodelbehaviors,cyberthreats,andpotentialmisuseinareaslike\n> biological warfare. Lastly, broader societal issues like job displacement\n> due to accelerated AI research and an over-reliance on LLMs leading to\n> training data degradation are also pertinent considerations (Acemoglu\n> andRestrepo,2018;AutorandSalomons,2018;Webb,2019;Shumailovetal.,2023).\n> Wearecommittedto continuing our work engaging with the broader policy,\n> academic, and industry community on these issues. 7 Conclusion\n> Inthisstudy,wehaveintroduced Llama 2,anewfamilyofpretrainedandfine-\n> tunedmodelswithscales of7billionto70billionparameters.\n> Thesemodelshavedemonstratedtheircompetitivenesswithexisting open-source chat\n> models, as well as competency that is equivalent to some proprietary models\n> on evaluation setsweexamined,althoughtheystilllagbehindothermodelslikeGPT-4.\n> Wemeticulouslyelaboratedonthe\n> methodsandtechniquesappliedinachievingourmodels,withaheavyemphasisontheiralignmentwiththe\n> principlesofhelpfulnessandsafety.\n> Tocontributemoresignificantlytosocietyandfosterthepaceofresearch,\n> wehaveresponsiblyopenedaccessto Llama 2 andLlama 2-Chat .\n> Aspartofourongoingcommitmentto transparency and safety, we plan to make\n> further improvements to Llama 2-Chat in future work. 36\n>\n> **Prometheus Faithfulness Feedback:** The information provided in the\n> context is not supported by the given information. The context is about the\n> development and release of Llama 2, a collection of pretrained and fine-\n> tuned large language models (LLMs), and the optimization of these models for\n> dialogue use cases. However, the information provided in the context does\n> not align with the given information. The context does not mention the range\n> of parameters for the large language models developed, which is the primary\n> objective mentioned in the information. The context only talks about the\n> development and release of Llama 2 and its optimization for dialogue use\n> cases, but it does not provide any information about the range of parameters\n> for the large language models developed. So the overall score is NO.\n> [RESULT] NO\n>\n> **Prometheus Faithfulness Score:** 0.0\n>\n> **Prometheus Relevancy Feedback:** The response is not in line with the\n> context information provided. The query asked for the two primary objectives\n> achieved in the work and the range of parameters for the large language\n> models developed. ", "mimetype": "text/plain", "start_char_idx": 23657, "end_char_idx": 26342, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbab962b-3ab5-45d4-bfe7-0328168d8d39": {"__data__": {"id_": "bbab962b-3ab5-45d4-bfe7-0328168d8d39", "embedding": null, "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c09340b-c9bc-4e99-81d0-4d3b5f27a93f", "node_type": "4", "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "hash": "40ab5d3b57a9f84bc51dc27ed0ea7afb6cdedd3504ce981ef4b8756ec50cb055", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78e4a0df-c49d-4182-bd2d-e64424e27f44", "node_type": "1", "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "hash": "5e1bfd12cb91c7b02e30467b1c6290ef9883d0d4a5fa218e98472eb8f779bbb9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24093838-9e4d-4daf-8656-8a5f0dc0c2b3", "node_type": "1", "metadata": {}, "hash": "613675064522bdf4ab0f2b58126a646002c8c75e473229f081d38289bfc2cf57", "class_name": "RelatedNodeInfo"}}, "text": "However, the response provided the abstract of the paper\n> and mentioned the authors, which is not relevant to the query. The response\n> also did not mention the two primary objectives achieved in the work or the\n> range of parameters for the large language models developed. So the overall\n> score is NO. [RESULT] NO\n>\n> **Prometheus Relevancy Score:** 0.0\n>\n> **GPT-4 Faithfulness Feedback:** The given piece of information is well\n> supported by the context. The context clearly states that Llama 2, a\n> collection of pretrained and fine-tuned large language models (LLMs), was\n> developed and released. It also mentions that these models range in scale\n> from 7 billion to 70 billion parameters. Furthermore, the context confirms\n> that these models are optimized for dialogue use cases. Therefore, the\n> information provided is accurate and is corroborated by the context.\n> [RESULT] YES\n>\n> **GPT-4 Faithfulness Score:** 1.0\n>\n> **GPT-4 Relevancy Feedback:** The response accurately reflects the context\n> provided. The response correctly identifies the two primary objectives of\n> the work as the development and release of Llama 2, a collection of\n> pretrained and fine-tuned large language models (LLMs), and the optimization\n> of these models for dialogue use cases. This is in line with the information\n> provided in the abstract of the context. The response also correctly states\n> the range of parameters for the large language models developed as being\n> from 7 billion to 70 billion, which is also confirmed in the context.\n> Therefore, the response is in line with the context information provided.\n> [RESULT] YES\n>\n> **GPT-4 Relevancy Score:** 1.0\n\n#  Observation:\n\n  1. Prometheus: If you compare the feedback and contexts, there is mention of a range of parameters in the context and response but the feedback says the model could not find such information. \n  2. GPT-4: Evaluates it correctly, unlike the Prometheus model. \n\n#  Summary\n\n  1. The cost for evaluation (approx.): ` $1.5 ` for Prometheus Model and ` $15 ` for GPT4. \n  2. The Prometheus model, though offering more detailed feedback than GPT-4, occasionally provides incorrect feedback, necessitating cautious application. \n  3. If a generated answer lacks certain facts present in the reference answer, the Prometheus model applies stricter penalties to scores than GPT-4. \n  4. The faithfulness and relevancy feedback of Prometheus shows more hallucinations/ wrong interpretations in the feedback compared to GPT-4. \n\n#  **Note:**\n\n  * You can check detailed analysis with code on [ Google Colab Notebook ](https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/evaluation/prometheus_evaluation.ipynb) . \n  * The endpoint on HF is served on AWS Nvidia A100G \u00b7 1x GPU \u00b7 80 GB which costs $6.5/h. (We extend our gratitude to the Hugging Face team for their assistance whenever we encounter issues.) \n  ", "mimetype": "text/plain", "start_char_idx": 26342, "end_char_idx": 29259, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24093838-9e4d-4daf-8656-8a5f0dc0c2b3": {"__data__": {"id_": "24093838-9e4d-4daf-8656-8a5f0dc0c2b3", "embedding": null, "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c09340b-c9bc-4e99-81d0-4d3b5f27a93f", "node_type": "4", "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "hash": "40ab5d3b57a9f84bc51dc27ed0ea7afb6cdedd3504ce981ef4b8756ec50cb055", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbab962b-3ab5-45d4-bfe7-0328168d8d39", "node_type": "1", "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}, "hash": "346f6cd34ba31b5ea94a7be61ebcbb7ea2a58cde8cad8de3c13fc8b1a467ef66", "class_name": "RelatedNodeInfo"}}, "text": "* We used the [ Prometheus model ](https://huggingface.co/kaist-ai/prometheus-13b-v1.0) for the analysis here. We also made a similar analysis with the [ GPTQ Quantized version ](https://huggingface.co/TheBloke/prometheus-13B-v1.0-GPTQ) of the [ Prometheus model ](https://huggingface.co/kaist-ai/prometheus-13b-v1.0) and observed a bit more hallucinations in feedback compared to the original unquantized model. Thanks to the authors of the paper for open-sourcing the model and [ Tom Jobbins ](https://twitter.com/TheBlokeAI) for the quantized version of the model. \n\n#  References:\n\n  * [ Prometheus paper ](https://arxiv.org/abs/2310.08491) . \n  * [ Prometheus model on HuggingFace. ](https://huggingface.co/kaist-ai/prometheus-13b-v1.0)\n\n", "mimetype": "text/plain", "start_char_idx": 29259, "end_char_idx": 30002, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "34ca7382-0f07-4d95-878f-79c5ac5a1109": {"__data__": {"id_": "34ca7382-0f07-4d95-878f-79c5ac5a1109", "embedding": null, "metadata": {"filename": "how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.md", "extension": ".md", "title": "How to train a custom GPT on your data with EmbedAI + LlamaIndex", "date": "Dec 14, 2023", "url": "https://www.llamaindex.ai/blog/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f11e9218-db47-48f8-bf87-3c3d700db39c", "node_type": "4", "metadata": {"filename": "how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.md", "extension": ".md", "title": "How to train a custom GPT on your data with EmbedAI + LlamaIndex", "date": "Dec 14, 2023", "url": "https://www.llamaindex.ai/blog/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070"}, "hash": "2f086e1f021a552cbfd362e725277d69546579862aa4f26bf3894f1122af3042", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "537a9f6c-37f1-4b96-98ce-76bd3379e7fa", "node_type": "1", "metadata": {}, "hash": "382ee2dceabb2a0265b28ac561199c06b751163846913b878d3f69f2dd2c1d32", "class_name": "RelatedNodeInfo"}}, "text": "ChatGPT, developed by OpenAI, has changed the way we interact online. Being a\ngeneral purpose chatbot, ChatGPT is limited to answering generic queries. But\nit becomes even more useful if you can get it to answer your questions\nspecific to your business. To do that, you need to train ChatGPT on your data.\n\n[ EmbedAI ](https://www.thesamur.ai/) is a no-code platform for creating AI\nchatbots trained on your business data. This includes data sourced from web\npages, PDFs, Notion documents, or YouTube videos, allowing EmbedAI to adapt to\na wide range of information sources.\n\nIn this blog post, we\u2019ll show you how we used [ LlamaIndex\n](https://www.llamaindex.ai/) with [ EmbedAI ](https://thesamur.ai/) to enable\nus to train ChatGPT on your own data, helping you create a customized and\neffective AI chatbot tailored for your business needs.\n\n#  Chat with your data use-cases\n\nThere\u2019s a variety of ways that a chatbot trained on your data could be\nhelpful, including:\n\n  1. **Customer Support Bot** : Manages frequently asked questions about a product, addressing customer support inquiries efficiently. \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1108, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "537a9f6c-37f1-4b96-98ce-76bd3379e7fa": {"__data__": {"id_": "537a9f6c-37f1-4b96-98ce-76bd3379e7fa", "embedding": null, "metadata": {"filename": "how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.md", "extension": ".md", "title": "How to train a custom GPT on your data with EmbedAI + LlamaIndex", "date": "Dec 14, 2023", "url": "https://www.llamaindex.ai/blog/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f11e9218-db47-48f8-bf87-3c3d700db39c", "node_type": "4", "metadata": {"filename": "how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.md", "extension": ".md", "title": "How to train a custom GPT on your data with EmbedAI + LlamaIndex", "date": "Dec 14, 2023", "url": "https://www.llamaindex.ai/blog/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070"}, "hash": "2f086e1f021a552cbfd362e725277d69546579862aa4f26bf3894f1122af3042", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "34ca7382-0f07-4d95-878f-79c5ac5a1109", "node_type": "1", "metadata": {"filename": "how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.md", "extension": ".md", "title": "How to train a custom GPT on your data with EmbedAI + LlamaIndex", "date": "Dec 14, 2023", "url": "https://www.llamaindex.ai/blog/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070"}, "hash": "3e48148e1132d8f13a24e0e4833cc06e0ebbce653211eb7b074111a98fe3ea24", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67924a08-df78-449a-81d0-67f61824f6a5", "node_type": "1", "metadata": {}, "hash": "8c86cd6d26a18ca969773d93bdd20b74fea9d97b846a75aff7d833065ea7beca", "class_name": "RelatedNodeInfo"}}, "text": "2. **Company Search Engine** : Finds internal company documents and information fast, boosting workplace efficiency. \n  3. **Personalized Learning Assistant** : Offers tailored educational support and study guidance based on specific course content. \n  4. **Technical Support assistant** : Provides in-depth help for complex software issues, from troubleshooting to usage tips. \n  5. **Healthcare Assistant** : Gives general health advice and information, based on medical literature and FAQs. \n  6. **Finance Chatbot** : Assists with financial queries, offering advice on products, market trends, and investment strategies by training on financial data \n\nLet\u2019s delve into creating our own chat apps that integrate with various data\nsources like PDFs, Notion documents, videos, webpages, and more.\n\n#  Case 1: Custom ChatGPT for your site\n\nTo train ChatGPT on your website content, we need to scrape the content from\nall the relevant webpages. The steps to do this are:\n\n  * Extract all the URLs from your website, such as from your sitemap \n  * Include only relevant URLs which you need to train on \n  * Use SimpleWebPageReader from [ LlamaIndex ](https://www.llamaindex.ai/) to download the content from these URLs \n\nHere\u2019s some sample code to do that:\n\nOnce the data is ready, an AI chatbot can be trained on these documents by\nusing LlamaIndex\u2019s VectorStoreIndex class.\n\nTo create a ChatGPT chatbot on your website without coding you can use EmbedAI\nas outlined below which uses LlamaIndex internally:\n\n#  Case 2: Custom ChatGPT for your PDF documents\n\nIf your business specific data is stored in PDF documents and you wish to\ncreate a chatbot that can surface the information in them we can do that with\nLlamaIndex using the PDFMiner library. This time the steps are:\n\n  * Upload your PDFs and store them in the cloud \n  * Install the PDFMiner library \n  * Fetch the uploaded PDFs and extract the document text using LlamaIndex loader \n\nHere\u2019s the code for creating an AI chatbot trained on PDF documents with\nLlamaIndex\n\nIf you want to create a ChatGPT chatbot on your PDF content without coding you\ncan use EmbedAI as in the demo below which uses LlamaIndex internally\n\n#  Case 3: Custom ChatGPT for your videos\n\nOften, valuable information is embedded in videos, which isn\u2019t as accessible\nfor users searching for information. However, by training an AI chatbot with\nthis content, it can become an incredibly rich resource for your users,\nsignificantly enhancing their experience.\n\nLet\u2019s see how we can fetch the information from our youtube videos to train an\nAI chatbot using LlamaIndex. The steps are:\n\n  * Find your Channel ID \n  * Install ` scrapetube ` and pass it your channel ID to get your list of videos \n  * Install the Youtube transcript api and pass the video URLs from above to LlamaIndex loader to get a list of documents \n\nThe code looks like this:\n\nNow you can train an AI chatbot on these documents by using SimpleVectorIndex\nfrom LlamaIndex to create a ChatGPT bot trained on your youtube videos, and as\nbefore, you can use EmbedAI to create a chatbot with no code.\n\n#  Case 4: Custom ChatGPT for Notion\n\nIn many modern companies, a significant portion of their content is stored in\nNotion. ", "mimetype": "text/plain", "start_char_idx": 1108, "end_char_idx": 4325, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67924a08-df78-449a-81d0-67f61824f6a5": {"__data__": {"id_": "67924a08-df78-449a-81d0-67f61824f6a5", "embedding": null, "metadata": {"filename": "how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.md", "extension": ".md", "title": "How to train a custom GPT on your data with EmbedAI + LlamaIndex", "date": "Dec 14, 2023", "url": "https://www.llamaindex.ai/blog/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f11e9218-db47-48f8-bf87-3c3d700db39c", "node_type": "4", "metadata": {"filename": "how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.md", "extension": ".md", "title": "How to train a custom GPT on your data with EmbedAI + LlamaIndex", "date": "Dec 14, 2023", "url": "https://www.llamaindex.ai/blog/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070"}, "hash": "2f086e1f021a552cbfd362e725277d69546579862aa4f26bf3894f1122af3042", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "537a9f6c-37f1-4b96-98ce-76bd3379e7fa", "node_type": "1", "metadata": {"filename": "how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.md", "extension": ".md", "title": "How to train a custom GPT on your data with EmbedAI + LlamaIndex", "date": "Dec 14, 2023", "url": "https://www.llamaindex.ai/blog/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070"}, "hash": "1e92284086c22d2ba615d75724a0e34b4aa41d7648f3c818b1719e04a75d061b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "538124e1-771b-44f9-8773-7a265f55e1ca", "node_type": "1", "metadata": {}, "hash": "bde0dd7497b38cfc6fb65d1f5b7c697598b92dd96910cd1d9ff943aed50e4a47", "class_name": "RelatedNodeInfo"}}, "text": "As this content grows, quickly locating specific information becomes\nincreasingly challenging. To address this, we can develop a chatbot for Notion\nto streamline the process of finding the necessary information.\n\nSteps to prepare the data:\n\n  * Fetch an access token from Notion following [ their instructions ](https://www.notion.so/help/create-integrations-with-the-notion-api)\n  * Using the Notion API, parse data from Notion and generate LlamaIndex documents \n  * Train a chatbot on these using VectorStoreIndex \n\nIf you prefer a No-code way to train a chatbot on your Notion documents, you\ncan use EmbedAI as in the demo below which uses LlamaIndex internally:\n\nThis doesn\u2019t stop here. With EmbedAI, you can connect data from even more\nsources like Google Docs, Shopify or even use Zapier to connect with 6000+\ntools and chat with their data. ", "mimetype": "text/plain", "start_char_idx": 4325, "end_char_idx": 5173, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "538124e1-771b-44f9-8773-7a265f55e1ca": {"__data__": {"id_": "538124e1-771b-44f9-8773-7a265f55e1ca", "embedding": null, "metadata": {"filename": "how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.md", "extension": ".md", "title": "How to train a custom GPT on your data with EmbedAI + LlamaIndex", "date": "Dec 14, 2023", "url": "https://www.llamaindex.ai/blog/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f11e9218-db47-48f8-bf87-3c3d700db39c", "node_type": "4", "metadata": {"filename": "how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.md", "extension": ".md", "title": "How to train a custom GPT on your data with EmbedAI + LlamaIndex", "date": "Dec 14, 2023", "url": "https://www.llamaindex.ai/blog/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070"}, "hash": "2f086e1f021a552cbfd362e725277d69546579862aa4f26bf3894f1122af3042", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67924a08-df78-449a-81d0-67f61824f6a5", "node_type": "1", "metadata": {"filename": "how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.md", "extension": ".md", "title": "How to train a custom GPT on your data with EmbedAI + LlamaIndex", "date": "Dec 14, 2023", "url": "https://www.llamaindex.ai/blog/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070"}, "hash": "80c8c0b855fdc1a79d57e8a5bd9b5f8e41db7e00eda557d3242e14238b5c672c", "class_name": "RelatedNodeInfo"}}, "text": "You can achieve this by choosing your specific\ndata connector from [ LlamaHub ](https://llamahub.ai/)\n\n#  Challenges while building EmbedAI\n\n  * In EmbedAI, while connecting with a data source like Notion, the data can keep changing regularly which needs to be auto-refreshed. So the data needs a periodic refresh to add new documents or edit existing documents which needs to be handled internally. Likewise, when indexing website data it can be refreshed regularly. LlamaIndex makes it easy to handle these scenarios. LlamaIndex has a [ guide to handling continuous ingestion ](https://docs.llamaindex.ai/en/stable/examples/ingestion/redis_ingestion_pipeline.html) . \n  * Querying over tabular data in EmbedAI is a major issue when dealing with PDF content containing tables. Naive chunking can give sub-optimal results and even hallucinations. LlamaIndex provides [ a guide on how to deal with PDFs containing both text and tables ](https://docs.llamaindex.ai/en/stable/examples/query_engine/sec_tables/tesla_10q_table.html#joint-tabular-semantic-qa-over-tesla-10k) and achieve optimal results while querying. \n  * Shopify integration in EmbedAI needed hybrid search, as we needed to search not only on product description but also on product metadata. Thus a combination of semantic search and keyword search is needed to obtain optimal results. LlamaIndex provides a simple framework to build a hybrid search application, such as in [ this example ](https://docs.llamaindex.ai/en/stable/examples/vector_stores/PineconeIndexDemo-Hybrid.html) . \n\n#  Custom trained chatbots can help your business\n\nTraining ChatGPT with your own data provides a significant advantage for your\nbusiness. From enhancing customer support with bots trained on specific\nproduct knowledge to creating sophisticated company search engines, the\napplications are as diverse as they are impactful. LlamaIndex provides a lot\nof abstractions to help with building a custom chatbot trained on your data,\nand we use them heavily at EmbedAI. For those seeking a no-code solution to\ndevelop an AI chatbot tailored to their data, starting with EmbedAI is a\nstraightforward option and we encourage you to [ try it out\n](https://www.thesamur.ai/) .\n\n", "mimetype": "text/plain", "start_char_idx": 5173, "end_char_idx": 7390, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b64e4239-e8f2-4e03-86c6-374d0a68be7f": {"__data__": {"id_": "b64e4239-e8f2-4e03-86c6-374d0a68be7f", "embedding": null, "metadata": {"filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "02424788-2019-4be0-b478-3a4403d07983", "node_type": "4", "metadata": {"filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "hash": "ab296a557358963296718282d5289068d8d29711128dd40ebd31fd83489f4bf4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "270d7f32-048e-4298-a8f8-d5475ed6f4db", "node_type": "1", "metadata": {}, "hash": "aa21910c9adcc3d72aabdd7466acf725aef36f0c90edb2a529a32d7bf34c8587", "class_name": "RelatedNodeInfo"}}, "text": "(co-authored by Jerry Liu, Haotian Zhang, Logan Markewich, and Laurie Voss @\nLlamaIndex)\n\nToday is Google\u2019s [ public release ](https://blog.google/technology/ai/gemini-\napi-developers-cloud/) of its latest AI model, Gemini. We\u2019re excited to be a\nday 1 launch partner for Gemini, with support immediately available in\nLlamaIndex today!\n\nAs of 0.9.15, LlamaIndex offers full support for all currently **released and\nupcoming Gemini models** (Gemini Pro, Gemini Ultra). We support both a \u201ctext-\nonly\u201d Gemini variant with a text-in/text-out format as well as a multimodal\nvariant that takes in both text and images as input, and outputs text. We\u2019ve\nmade some fundamental multi-modal abstraction changes to support the Gemini\nmulti-modal interface, which allows users to input multiple images along with\ntext. Our Gemini integrations are also **feature-complete:** they support\n(non-streaming, streaming), (sync, async), and (text completion, chat message)\nformats \u2014 8 combinations in total.\n\nIn addition, we also support the brand-new **Semantic Retriever API,** which\nbundles storage, embedding models, retrieval, and LLM in a RAG pipeline. We\nshow you how it can be used on its own, or decomposed+bundled with LlamaIndex\ncomponents to create advanced RAG pipelines.\n\nHuge shoutout to the Google Labs and Semantic Retriever teams for helping us\nget setup with early access.\n\n  * **Google Labs:** Mark McDonald, Josh Gordon, Arthur Soroken \n  * **Semantic Retriever:** Lawrence Tsang, Cher Hu \n\nThe below sections contain a detailed walkthrough of both our brand-new Gemini\nand Semantic Retriever abstractions in LlamaIndex. If you don\u2019t want to read\nthat now, make sure you bookmark our detailed notebook guides below!\n\n  * [ Gemini (text-only) Guide ](https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/gemini.ipynb)\n  * [ Gemini (multi-modal) Guide ](https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/gemini.ipynb)\n  * [ Semantic Retriever Guide ](https://github.com/run-llama/llama_index/blob/main/docs/examples/managed/GoogleDemo.ipynb)\n\n#  Gemini Release and Support\n\nThere\u2019s been a ton of press around Gemini, which boasts [ impressive\nperformance ](https://blog.google/technology/ai/google-gemini-ai/#performance)\nat a variety of benchmarks. The Ultra variants (which are not yet publicly\navailable) outperform GPT-4 on benchmarks from MMLU to Big-Bench Hard to math\nand coding tasks. Their [ multimodal demos\n](https://www.youtube.com/watch?v=K4pX1VAxaAI) demonstrate joint image/text\nunderstanding from domains like scientific paper understanding to literature\nreview.\n\nLet\u2019s walk through examples of using Gemini in LlamaIndex. We walk through\nboth the text model ( ` from llama_index.llms import Gemini ` ) as well as the\nmulti-modal model ( ` from llama_index.multi_modal_llms.gemini import\nGeminiMultiModal ` )\n\n#  Text Model\n\n[ Full Notebook Guide Here ](https://github.com/run-\nllama/llama_index/blob/main/docs/examples/llm/gemini.ipynb)\n\nWe start with the text model. In the code snippet below, we show a bunch of\ndifferent configurations, from completion to chat to streaming to async.\n\n    \n    \n    from llama_index.llms import Gemini\n    \n    # completion\n    resp = Gemini().complete(\"Write a poem about a magic backpack\")\n    # chat\n    messages = [\n        ChatMessage(role=\"user\", content=\"Hello friend!\"),\n        ChatMessage(role=\"assistant\", content=\"Yarr what is shakin' matey?\"),\n        ChatMessage(\n            role=\"user\", content=\"Help me decide what to have for dinner.\"\n        ),\n    ]\n    resp = Gemini().chat(messages)\n    # streaming (completion)\n    llm = Gemini()\n    resp = llm.stream_complete(\n        \"The story of Sourcrust, the bread creature, is really interesting. It all started when...\"\n    )\n    # streaming (chat)\n    llm = Gemini()\n    messages = [\n        ChatMessage(role=\"user\", content=\"Hello friend!\"),\n        ChatMessage(role=\"assistant\", content=\"Yarr what is shakin' matey?\"),\n        ChatMessage(\n            role=\"user\", content=\"Help me decide what to have for dinner.\"\n        ),\n    ]\n    resp = llm.stream_chat(messages)\n    # async completion\n    resp = await llm.acomplete(\"Llamas are famous for \")\n    print(resp)\n    # async streaming (completion)\n    resp = await llm.astream_complete(\"Llamas are famous for \")\n    async for chunk in resp:\n        print(chunk.text, end=\"\")\n\nThe ` Gemini ` class of course has parameters that can be set. This includes `\nmodel_name ` , ` temperature ` , ` max_tokens ` , and ` generate_kwargs ` .\n\nAs an example, you can do:\n\n    \n    \n    llm = Gemini(model=\"models/gemini-ultra\")\n\n#  Multi-modal Model\n\n[ Full Notebook Guide Here ](https://github.com/run-\nllama/llama_index/blob/main/docs/examples/multi_modal/gemini.ipynb)\n\nIn this notebook, we test out the ` gemini-pro-vision ` variant that features\n**multi-modal inputs.** It contains the following features:\n\n  * supports both ` complete ` and ` chat ` capabilities \n  * supports streaming and async \n  * Supports feeding in **multiple images** in addition to text in the completion endpoint \n  * Future work: multi-turn chat interleaving text and images is supported within our abstraction, but is not yet enabled for gemini-pro-vision. \n\nLet\u2019s walk through a concrete example. Let\u2019s say we are given a picture of the\n[ following scene ](https://storage.googleapis.com/generativeai-\ndownloads/data/scene.jpg) :\n\nScene from a street in New York City\n\nWe can then initialize our Gemini Vision model, and ask it a question:\n\u201cIdentify the city where this photo was taken\u201d:\n\n    \n    \n    from llama_index.multi_modal_llms.gemini import GeminiMultiModal\n    from llama_index.multi_modal_llms.generic_utils import (\n        load_image_urls,\n    )\n    \n    image_urls = [\n        \"&lt;https://storage.googleapis.com/generativeai-downloads/data/scene.jpg&gt;\",\n        # Add yours here!\n    ]\n    image_documents = load_image_urls(image_urls)\n    gemini_pro = GeminiMultiModal(model=\"models/gemini-pro\")\n    complete_response = gemini_pro.complete(\n        prompt=\"Identify the city where this photo was taken.\",\n        image_documents=image_documents,\n    )\n\nOur response is the following:\n\n    \n    \n    New York City\n\nWe can insert multiple images too. Here\u2019s an example with an image of Messi\nand the Colosseum.\n\n    \n    \n    image_urls = [\n        \"&lt;https://www.sportsnet.ca/wp-content/uploads/2023/11/CP1688996471-1040x572.jpg&gt;\",\n        \"&lt;https://res.cloudinary.com/hello-tickets/image/upload/c_limit,f_auto,q_auto,w_1920/v1640835927/o3pfl41q7m5bj8jardk0.jpg&gt;\",\n    ]\n    image_documents_1 = load_image_urls(image_urls)\n    response_multi = gemini_pro.complete(\n        prompt=\"is there any relationship between those images?\",\n        image_documents=image_documents_1,\n    )\n    print(response_multi)\n\n#  Multi-Modal Use Cases (Structured Outputs, RAG)\n\n[ Full Notebook Guide Here ](https://github.com/run-\nllama/llama_index/blob/main/docs/examples/multi_modal/gemini.ipynb)\n\nWe\u2019ve created extensive resources about different [ multi-modal use cases\n](https://docs.llamaindex.ai/en/latest/use_cases/multimodal.html) , from\nstructured output extraction to RAG.\n\nThanks to Haotian Zhang, we have examples for **_both_ ** these use cases with\nGemini. Please see our extensive notebook guides for more details. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 7328, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "270d7f32-048e-4298-a8f8-d5475ed6f4db": {"__data__": {"id_": "270d7f32-048e-4298-a8f8-d5475ed6f4db", "embedding": null, "metadata": {"filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "02424788-2019-4be0-b478-3a4403d07983", "node_type": "4", "metadata": {"filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "hash": "ab296a557358963296718282d5289068d8d29711128dd40ebd31fd83489f4bf4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b64e4239-e8f2-4e03-86c6-374d0a68be7f", "node_type": "1", "metadata": {"filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "hash": "fb05fd341aa7be3a2ec1b00d4612d0318d6e2584c6b9f88a22ef0653485599fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a76b6744-d2b0-42b7-af6a-eb74b01c1472", "node_type": "1", "metadata": {}, "hash": "cd9e88b96e5fa373e529814463f82fc31c3c8087014ab8822f0a94d0575af996", "class_name": "RelatedNodeInfo"}}, "text": "In the\nmeantime here\u2019s the final results!\n\n**Structured Data Extraction with Gemini Pro Vision**\n\nScreenshot of a Google Maps Restaurant Listing\n\nOutput:\n\n    \n    \n    ('restaurant', 'La Mar by Gaston Acurio')\n    ('food', 'South American')\n    ('location', '500 Brickell Key Dr, Miami, FL 33131')\n    ('category', 'Restaurant')\n    ('hours', 'Open \u22c5 Closes 11 PM')\n    ('price', 4.0)\n    ('rating', 4)\n    ('review', '4.4 (2,104)')\n    ('description', 'Chic waterfront find offering Peruvian & fusion fare, plus bars for cocktails, ceviche & anticucho.')\n    ('nearby_tourist_places', 'Brickell Key Park')\n\n**Multi-Modal RAG**\n\nWe run our structured output extractor on multiple restaurant images, index\nthese nodes, and then ask a question \u201cRecommend a Orlando restaurant for me\nand its nearby tourist places\u201d\n\n    \n    \n    I recommend Mythos Restaurant in Orlando. It is an American restaurant located at 6000 Universal Blvd, Orlando, FL 32819, United States. ", "mimetype": "text/plain", "start_char_idx": 7328, "end_char_idx": 8293, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a76b6744-d2b0-42b7-af6a-eb74b01c1472": {"__data__": {"id_": "a76b6744-d2b0-42b7-af6a-eb74b01c1472", "embedding": null, "metadata": {"filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "02424788-2019-4be0-b478-3a4403d07983", "node_type": "4", "metadata": {"filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "hash": "ab296a557358963296718282d5289068d8d29711128dd40ebd31fd83489f4bf4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "270d7f32-048e-4298-a8f8-d5475ed6f4db", "node_type": "1", "metadata": {"filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "hash": "58b714ddb2069dbc0e5372037390ba04a58018379dc65445cf921af944f6c755", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48100188-165e-4180-b1fa-44109f792ded", "node_type": "1", "metadata": {}, "hash": "c544bce404a1be3dc509805dc046e82fba86b62c0b10ff40d913fb3287ad54fa", "class_name": "RelatedNodeInfo"}}, "text": "It has a rating of 4 and a review score of 4.3 based on 2,115 reviews. The restaurant offers a mythic underwater-themed dining experience with a view of Universal Studios' Inland Sea. It is located near popular tourist places such as Universal's Islands of Adventure, Skull Island: Reign of Kong, The Wizarding World of Harry Potter, Jurassic Park River Adventure, Hollywood Rip Ride Rockit, and Universal Studios Florida.\n\n#  Semantic Retriever\n\nThe Generative Language Semantic Retriever offers specialized embedding models\nfor high-quality retrieval, and a tuned LLM for producing grounded-output with\nsafety settings.\n\nIt can be used out of the box (with our ` GoogleIndex ` ) or decomposed into\ndifferent components ( ` GoogleVectorStore ` and ` GoogleTextSynthesizer ` )\nand combined with LlamaIndex abstractions!\n\nOur full [ semantic retriever notebook guide is here.\n](https://github.com/run-\nllama/llama_index/blob/main/docs/examples/managed/GoogleDemo.ipynb)\n\n#  Out of the Box Configuration\n\nYou can use it out of the box with very few lines of setup. Simply define the\nindex, insert nodes, and then get a query engine:\n\n    \n    \n    from llama_index.indices.managed.google.generativeai import GoogleIndex\n    \n    index = GoogleIndex.from_corpus(corpus_id=\"&lt;corpus_id&gt;\")\n    index.insert_documents(nodes)\n    query_engine = index.as_query_engine(...)\n    response = query_engine.query(\"&lt;query&gt;\")\n\nA cool feature here is that Google\u2019s query engine supports different\n**answering styles** as well as **safety settings** .\n\n**Answering Styles:**\n\n  * ABSTRACTIVE (succinct but abstract) \n  * EXTRACTIVE (brief and extractive) \n  * VERBOSE (extra details) \n\n**Safety Settings**\n\nYou can specify safety settings in the query engine, which let you define\nguardrails on whether the answer is explicit in different settings. See the `\n[ generative-ai-python ](https://github.com/google/generative-ai-python) `\nlibrary for more information.\n\n#  Decomposing into Different Components\n\nThe ` GoogleIndex ` is built upon two components: a vector store ( `\nGoogleVectorStore ` ) and the response synthesizer ( ` GoogleTextSynthesizer `\n). You can use these as modular components in conjunction with LlamaIndex\nabstractions to create **advanced RAG** .\n\nThe notebook guide highlights three **advanced RAG use cases** :\n\n  * **Google Retriever + Reranking:** Use the Semantic Retriever to return relevant results, but then use our r [ eranking modules ](https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/root.html) to process/filter results before feeding it to response synthesis. \n  * **Multi-Query + Google Retriever:** Use our multi-query capabilities, like our ` MultiStepQueryEngine ` to break a complex question into multiple steps, and execute each step against the semantic retriever. \n  * **HyDE + Google Retriever:** HyDE is a popular query transformation technique that hallucinates an answer from a query, and uses the hallucinated answer for embedding lookup. Use that as a step before the retrieval step from the Semantic Retriever. \n\n", "mimetype": "text/plain", "start_char_idx": 8293, "end_char_idx": 11383, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48100188-165e-4180-b1fa-44109f792ded": {"__data__": {"id_": "48100188-165e-4180-b1fa-44109f792ded", "embedding": null, "metadata": {"filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "02424788-2019-4be0-b478-3a4403d07983", "node_type": "4", "metadata": {"filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "hash": "ab296a557358963296718282d5289068d8d29711128dd40ebd31fd83489f4bf4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a76b6744-d2b0-42b7-af6a-eb74b01c1472", "node_type": "1", "metadata": {"filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}, "hash": "5605972c436f5d2318d04e29b615c21a98102f2be52e8255c8dee09225540c1a", "class_name": "RelatedNodeInfo"}}, "text": "#  Conclusion\n\nThere\u2019s a **lot** in here, and even then the blog post doesn\u2019t even cover half\nof what we\u2019ve released today.\n\nPlease please make sure to check out our extensive notebook guides! Linking\nthe resources again below:\n\n  * [ Gemini (text-only) Guide ](https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/gemini.ipynb)\n  * [ Gemini (multi-modal) Guide ](https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/gemini.ipynb)\n  * [ Semantic Retriever Guide ](https://github.com/run-llama/llama_index/blob/main/docs/examples/managed/GoogleDemo.ipynb)\n\nAgain, huge shoutout to the Google teams and Haotian Zhang, Logan Markewich\nfrom the LlamaIndex team for putting together everything for this release.\n\n", "mimetype": "text/plain", "start_char_idx": 11383, "end_char_idx": 12131, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bce8eb25-2a90-4f2f-af59-4ffc56ca64b3": {"__data__": {"id_": "bce8eb25-2a90-4f2f-af59-4ffc56ca64b3", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-12-12-4a5d542fbb1e.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201312", "date": "Dec 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-12-4a5d542fbb1e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "80ed9bec-3de7-42b1-bc6c-6f7c62e1ca1e", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-12-12-4a5d542fbb1e.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201312", "date": "Dec 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-12-4a5d542fbb1e"}, "hash": "f6d63ea1f44af737665644175c202cb003911aa8956d203998d0e6118dc39f3a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd754407-c8c6-42d6-8b21-88e7d36e945d", "node_type": "1", "metadata": {}, "hash": "b8e7b3024e266ddfd1c668a2fd34d9dccc96b015d89ee71c577e1a4a14639f21", "class_name": "RelatedNodeInfo"}}, "text": "Howdy, Llama Enthusiasts ,\n\nWe are thrilled to announce another exciting week filled with full of the\nlatest updates, features, insightful tutorials, guides, webinars, and so much\nmore. Have a groundbreaking project, compelling article, or captivating video?\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 259, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd754407-c8c6-42d6-8b21-88e7d36e945d": {"__data__": {"id_": "bd754407-c8c6-42d6-8b21-88e7d36e945d", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-12-12-4a5d542fbb1e.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201312", "date": "Dec 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-12-4a5d542fbb1e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "80ed9bec-3de7-42b1-bc6c-6f7c62e1ca1e", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-12-12-4a5d542fbb1e.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201312", "date": "Dec 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-12-4a5d542fbb1e"}, "hash": "f6d63ea1f44af737665644175c202cb003911aa8956d203998d0e6118dc39f3a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bce8eb25-2a90-4f2f-af59-4ffc56ca64b3", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-12-12-4a5d542fbb1e.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201312", "date": "Dec 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-12-4a5d542fbb1e"}, "hash": "db0e2f64c32ad19b5453bc492dfb65282f540d15bbeac7292661cd4ab8d2e225", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2d34e56-51d6-43f1-bc18-6531d2e772c9", "node_type": "1", "metadata": {}, "hash": "5471a8983a42f905f519c4f0bf5aee20a1586db16c252511051a3f452ef22c73", "class_name": "RelatedNodeInfo"}}, "text": "We\u2019re all ears! Reach out to us at [ news@llamaindex.ai\n](mailto:news@llamaindex.ai) .\n\nDon\u2019t forget to subscribe to our newsletter via our [ website\n](https://www.llamaindex.ai/) to have all these exciting developments\ndelivered directly to your inbox.\n\n**First, the highlights:**\n\n  * **Llama Datasets:** A diverse collection of community-contributed datasets for benchmarking RAG pipelines. [ Blog ](/introducing-llama-datasets-aadb9994ad9e) , [ Tweet ](https://x.com/llama_index/status/1731718080223707148?s=20) . \n  * **RAGs v5:** Enables multi-modal data handling with natural language for both text and image sources. [ Tweet ](https://x.com/llama_index/status/1731843485115064531?s=20) . \n  * **Production RAG Pipeline:** New features and a guide for efficient RAG while handling updates to your data, including incremental re-indexing for Google Docs and enhanced transformation and caching processes. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/ingestion/ingestion_gdrive.ipynb) , [ Tweet ](https://x.com/llama_index/status/1732121799033487361?s=20) . \n  * **Revamped LlamaHub:** A community-driven hub with universal data loaders, a new user interface, and a range of tools, templates, and datasets. [ Tweet ](https://x.com/llama_index/status/1732814499235962907?s=20) . \n  * **AutoTranslateDoc:** An open-source project for translating GitHub repository documentation into over 15 languages. [ Blog ](/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8) , [ Repo ](https://github.com/run-llama/automatic-doc-translate) , [ Tweet ](https://x.com/jerryjliu0/status/1732926141118472448?s=20)\n\n**Feature Releases and Enhancements:**\n\n  * We launched **Llama Datasets** , a collection of community-contributed datasets tailored for benchmarking RAG pipelines in various use cases. These datasets offer flexibility in selecting the most appropriate one for specific LLM applications. The initial release includes a diverse range, such as Code Help Desk, FinanceBench, Mini TruthfulQA, Mini Squad V2, Blockchain Solana, Uber 10K, Llama 2 Paper, Paul Graham Essay, Origin of COVID-19, CovidQADataset, MiniCovidQADataset and LLM Survey Paper. Each dataset, designed as a QA set, integrates smoothly with Llama Index abstractions, providing a platform for comprehensive benchmarking across multiple metrics. All datasets are available on LlamaHub for easy download and evaluation. [ Blog ](/introducing-llama-datasets-aadb9994ad9e) , [ Tweet ](https://x.com/llama_index/status/1731718080223707148?s=20) . \n  * We launched **RAGs v5** , enabling multi-modal data handling with natural language for both text and image sources. Key features include enhanced multi-modal indexing, the capability to view sources in any RAG agent, and support for loading entire directories, not just single files. [ Tweet ](https://x.com/llama_index/status/1731843485115064531?s=20) . \n  * We have launched new features and a guide for building a **production RAG pipeline** , enabling efficient question-answering with LLMs on production data even while it is continuously updated. This includes incremental re-indexing for Google Docs changes and enhanced transformation and caching processes in our updated ` **IngestionPipeline** ` . [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/ingestion/ingestion_gdrive.ipynb) , [ Tweet ](https://x.com/llama_index/status/1732121799033487361?s=20) . \n  * We launched a one-click, full-stack LlamaIndex template now available on **Replit** ! This template features a full-stack Next.js app in TypeScript, capable of reading any files you provide, and includes a chat interface for querying those documents. It\u2019s completely customizable and based on our popular create-llama generator. ", "mimetype": "text/plain", "start_char_idx": 259, "end_char_idx": 4064, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2d34e56-51d6-43f1-bc18-6531d2e772c9": {"__data__": {"id_": "a2d34e56-51d6-43f1-bc18-6531d2e772c9", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-12-12-4a5d542fbb1e.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201312", "date": "Dec 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-12-4a5d542fbb1e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "80ed9bec-3de7-42b1-bc6c-6f7c62e1ca1e", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-12-12-4a5d542fbb1e.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201312", "date": "Dec 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-12-4a5d542fbb1e"}, "hash": "f6d63ea1f44af737665644175c202cb003911aa8956d203998d0e6118dc39f3a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd754407-c8c6-42d6-8b21-88e7d36e945d", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-12-12-4a5d542fbb1e.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201312", "date": "Dec 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-12-4a5d542fbb1e"}, "hash": "d1e22a0df7443a497e3109d444e103346994b86a8ce548fc577006d758bb2743", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16337efa-18e4-40f9-84ec-c47f5a031f51", "node_type": "1", "metadata": {}, "hash": "4ada133dd26220c2cadd4a456e920bf54b95648f458b332ca220c6fe5c4374d7", "class_name": "RelatedNodeInfo"}}, "text": "[ Replit Template ](https://replit.com/@LlamaIndex/createllama) , [ Tweet ](https://x.com/llama_index/status/1732150579928150247?s=20) . \n  * We have introduced ` **RAGEvaluatorPack** ` to easily benchmark your RAG pipeline on any dataset with a single line of code, offering metrics like correctness, relevancy, and context similarity. [ Docs ](https://llamahub.ai/l/llama_packs-rag_evaluator) , [ Tweet ](https://x.com/llama_index/status/1732210229574824357?s=20) . \n  * We released community templates for create-llama, offering a selection of community-contributed starter templates during setup. Current examples include ` embedded-tables ` for analyzing complex tables in large PDFs, and ` multi-document-agent ` for comparing multiple documents. [ Tweet ](https://x.com/llama_index/status/1732480745804022240?s=20) . \n  * We launched multi-modal support in create-llama, our user-friendly command-line tool for generating full-stack LlamaIndex apps. Now, easily integrate GPT-4-vision in your app, allowing you to upload images to the web interface and receive answers about them in just seconds. [ Tweet ](https://x.com/llama_index/status/1732480613763215783?s=20) . \n  * We launched the Ollama LlamaPack, a new offering that integrates local LLMs and embeddings into a fully local RAG pipeline, enhancing language model accessibility and capabilities. [ Docs ](https://docs.llamaindex.ai/en/latest/examples/llama_hub/llama_pack_ollama.html) , [ Tweet ](https://x.com/llama_index/status/1732565478546223322?s=20) . \n  * We launched the revamped [ LlamaHub ](https://llamahub.ai/) , a hub for community-driven modules to enhance LLM app development, featuring universal data loaders, a new user interface, and a range of tools, templates, and datasets. [ Tweet ](https://x.com/llama_index/status/1732814499235962907?s=20) . \n  * We introduced AutoTranslateDoc, an open-source project for translating GitHub repository documentation into over 15 languages, including Chinese, Spanish, and French. This tool, successfully implemented in our own LlamaIndex.TS docs, simplifies the internationalization process for open-source projects. [ Blog ](/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8) , [ Repo ](https://github.com/run-llama/automatic-doc-translate) , [ Tweet ](https://x.com/jerryjliu0/status/1732926141118472448?s=20)\n  * We released support for exact match and range queries in 4 vector databases including Weaviate, Chroma, Qdrant and Pinecone, allowing auto-retrieval via metadata filters, elevating the functionality of structured and unstructured data querying. [ Tweet ](https://x.com/llama_index/status/1733289204380311703?s=20) . \n\n**Guides:**\n\n  * [ Guide ](https://docs.google.com/presentation/d/1o4OeOvyaXAGNF1Dlbw4s5KYOMAE-2fUzqxo2lproprw/edit#slide=id.g2a2d0d2fc2a_0_0) on building LLM apps for financial data which is presented at MindsDB event. Learn to query diverse financial data using advanced RAG with techniques for multi-document comparisons, embedded tables, and converting text queries into domain-specific languages. \n  * [ Guide ](https://docs.google.com/presentation/d/1IJ1bpoLmHfFzKM3Ef6OoWGwvrwDwLV7EcoOHxLZzizE/edit#slide=id.g23d546514bd_0_290) on advanced RAG Cheat Sheet, a concise guide offering solutions for different RAG-related pain points and techniques. It\u2019s part of our Snowflake BUILD talk and PyData Global talk. \n\n**Tutorials:**\n\n  * [ Blog ](/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82) by [ Waii.ai ](http://waii.ai/) on creating an agent that queries both enterprise databases and PDF data, combining advanced text-to-SQL techniques and a Llama Index RAG pipeline, for effective analysis of structured and unstructured data like retail sales trends. \n  * Wenqi Glantz\u2019s [ tutorial ](https://levelup.gitconnected.com/a-simpler-way-to-query-neo4j-knowledge-graphs-99c0a8bbf1d7) on using LLMs for querying knowledge graphs introduces seven strategies, now easily accessible through our LlamaPacks and featured in our Neo4j query engine. \n  * An hour comprehensive workshop [ tutorial ](https://www.youtube.com/watch?v=oa82yoJ6zYc) by [ AIMakerspace ](https://twitter.com/AIMakerspace) on RAG strategies over complex documents through recursive retrieval. \n  * [ Laurie\u2019s ](https://twitter.com/seldo) [ video ](https://www.notion.so/Content-roadmap-6f27c002d67e428497326d972afa7eb6?pvs=21) on using LlamaIndex for multi-modal retrieval-augmented generation apps teaches you to build indexes and retrieve data from text and images, for enhanced query responses. \n  * [ Ravi Theja\u2019s ](https://twitter.com/ravithejads) [ video ](https://www.youtube.com/watch?v=_vU-biwMoGk) on Understanding LlamaIndex 0.9v abstractions and features. \n\n**Integrations:**\n\n  * We integrated AssemblyAI with Llama Index TS, enhancing the capabilities and offering new, innovative solutions. [ Blog ](https://www.assemblyai.com/blog/announcing-the-assemblyai-integration-for-llamaindex-ts/) . \n  * We integrated Panel, a powerful framework for building interactive data apps as a LlamaPack. This provides you with a robust chat interface for talking to your data with full streaming support in a single line of code. ", "mimetype": "text/plain", "start_char_idx": 4064, "end_char_idx": 9310, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16337efa-18e4-40f9-84ec-c47f5a031f51": {"__data__": {"id_": "16337efa-18e4-40f9-84ec-c47f5a031f51", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-12-12-4a5d542fbb1e.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201312", "date": "Dec 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-12-4a5d542fbb1e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "80ed9bec-3de7-42b1-bc6c-6f7c62e1ca1e", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-12-12-4a5d542fbb1e.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201312", "date": "Dec 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-12-4a5d542fbb1e"}, "hash": "f6d63ea1f44af737665644175c202cb003911aa8956d203998d0e6118dc39f3a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2d34e56-51d6-43f1-bc18-6531d2e772c9", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-12-12-4a5d542fbb1e.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201312", "date": "Dec 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-12-4a5d542fbb1e"}, "hash": "644abc85682997be52e6320949f0c375d32cdf3834e91422e0caadf804e17536", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5bd74849-d524-4659-a25e-fe95b3e5380d", "node_type": "1", "metadata": {}, "hash": "dd7134309aa3741ee672ae0c66591c80b1f46bfe480c6bd21b82ef4b7d1dde76", "class_name": "RelatedNodeInfo"}}, "text": "[ Docs ](https://t.co/QSVdFxvfAi) , [ Tweet ](https://x.com/llama_index/status/1733894204076720551?s=20) . \n  * We integrated FlagEmbeddingReranker to further boost your RAG pipeline. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/node_postprocessor/FlagEmbeddingReranker.ipynb) , [ Tweet ](https://x.com/llama_index/status/1734019264166953421?s=20) . \n\n**Webinars:**\n\nWebinar featuring Haotian Liu, the author of LLaVa which includes a deep dive\ninto the open-source multi-modal models of LLaVa, which are competitive with\nGPT-4V, and a presentation on multi-modal use cases with LLaVa + LlamaIndex by\nHaotian Zhang from the LlamaIndex team.\n\n**Calling all enterprises:**\n\nAre you building with LlamaIndex? We are working hard to make LlamaIndex even\nmore Enterprise-ready and have sneak peeks at our upcoming products available\nfor partners. Interested? ", "mimetype": "text/plain", "start_char_idx": 9310, "end_char_idx": 10197, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5bd74849-d524-4659-a25e-fe95b3e5380d": {"__data__": {"id_": "5bd74849-d524-4659-a25e-fe95b3e5380d", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-12-12-4a5d542fbb1e.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201312", "date": "Dec 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-12-4a5d542fbb1e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "80ed9bec-3de7-42b1-bc6c-6f7c62e1ca1e", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-12-12-4a5d542fbb1e.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201312", "date": "Dec 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-12-4a5d542fbb1e"}, "hash": "f6d63ea1f44af737665644175c202cb003911aa8956d203998d0e6118dc39f3a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16337efa-18e4-40f9-84ec-c47f5a031f51", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-12-12-4a5d542fbb1e.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201312", "date": "Dec 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-12-4a5d542fbb1e"}, "hash": "7ae998a1e878c314cfeef45dec62d4d3c86ee4bdd944f4c93dd8aca538668d5b", "class_name": "RelatedNodeInfo"}}, "text": "[ Get in touch.\n](https://docs.google.com/forms/d/e/1FAIpQLScBNdM2a_fn8UZOKmFQt6lBsrd1o6FflvsdPH-\nPn3JkdlN_Rg/viewform)\n\n", "mimetype": "text/plain", "start_char_idx": 10197, "end_char_idx": 10318, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f90e571-61d1-4103-ac8c-31b801915973": {"__data__": {"id_": "3f90e571-61d1-4103-ac8c-31b801915973", "embedding": null, "metadata": {"filename": "bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.md", "extension": ".md", "title": "Bridging the Language Gap in Programming: Introducing AutoTranslateDoc", "date": "Dec 8, 2023", "url": "https://www.llamaindex.ai/blog/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "033e4d61-87ff-431e-a45d-74bf4eea80d0", "node_type": "4", "metadata": {"filename": "bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.md", "extension": ".md", "title": "Bridging the Language Gap in Programming: Introducing AutoTranslateDoc", "date": "Dec 8, 2023", "url": "https://www.llamaindex.ai/blog/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8"}, "hash": "effb51b17e4f4766f6ddfe3d17667dc98e1c4b0713b487aa20097b1c03e39cdd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "607b2025-b5f4-4850-afd3-73e1e9e5b377", "node_type": "1", "metadata": {}, "hash": "32fc44bd5179202e14e827af56641d6b896418b786a1118a00eb427e1e0ec167", "class_name": "RelatedNodeInfo"}}, "text": "Author: [ Pierre-Loic Doulcet ](https://twitter.com/hexapode)\n\nAs programmers, we often find ourselves limited by language barriers.\nDocumentation for various programming frameworks and tools is predominantly\navailable in English, and increasingly in languages like Chinese, creating\nchallenges for non-native speakers. I faced similar obstacles in my early\nprogramming days, and it was only through community efforts like [ traduc.org\n](https://traduc.org/) \u2019s translation of man pages that I could surmount them.\n\nToday, we are excited to unveil a solution to this pervasive issue: [\nAutoTranslateDoc ](https://www.npmjs.com/package/autotranslatedoc) , a\ncommand-line tool designed to democratize access to technical documentation by\nbreaking down language barriers.\n\n**How AutoTranslateDoc Works**\n\n  1. Collect the Documentation: The tool connects to GitHub, identifying and downloading .md and .mdx files from any repository. \n  2. Chunk and Prepare: The documentation is then chunked or split for translation. \n  3. Translate Efficiently: Utilizing the power of LLMs like GPT-3.5 and GPT-4, each chunk of documentation is translated accurately. \n  4. Verify and Enhance: The translation is automatically verified, with retranslation if needed, ensuring the highest quality. \n  5. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "607b2025-b5f4-4850-afd3-73e1e9e5b377": {"__data__": {"id_": "607b2025-b5f4-4850-afd3-73e1e9e5b377", "embedding": null, "metadata": {"filename": "bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.md", "extension": ".md", "title": "Bridging the Language Gap in Programming: Introducing AutoTranslateDoc", "date": "Dec 8, 2023", "url": "https://www.llamaindex.ai/blog/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "033e4d61-87ff-431e-a45d-74bf4eea80d0", "node_type": "4", "metadata": {"filename": "bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.md", "extension": ".md", "title": "Bridging the Language Gap in Programming: Introducing AutoTranslateDoc", "date": "Dec 8, 2023", "url": "https://www.llamaindex.ai/blog/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8"}, "hash": "effb51b17e4f4766f6ddfe3d17667dc98e1c4b0713b487aa20097b1c03e39cdd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f90e571-61d1-4103-ac8c-31b801915973", "node_type": "1", "metadata": {"filename": "bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.md", "extension": ".md", "title": "Bridging the Language Gap in Programming: Introducing AutoTranslateDoc", "date": "Dec 8, 2023", "url": "https://www.llamaindex.ai/blog/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8"}, "hash": "8006aaa5ed63231a5167d90b84670dee66dbe90103fca1ecb787919ff2431e03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8720746f-1e9c-4f9c-8065-81233cf191ae", "node_type": "1", "metadata": {}, "hash": "5e53ae68f5d646880b154d07b8b55845f3547ed032a11a49f5e0673acceeea35", "class_name": "RelatedNodeInfo"}}, "text": "Consolidate: Finally, the chunks are amalgamated back into a cohesive document. \n\nOur initial tests on translating the llamaIndexTS documentation have been\nhighly promising. You can now read our docs in over a dozen languages\nincluding [ Chinese ](https://ts.llamaindex.ai/zh-Hans/) , [ French\n](https://ts.llamaindex.ai/fr/) , and [ Spanish\n](https://ts.llamaindex.ai/es/) !\n\n**Getting Started**\n\nInstall AutoTranslateDoc easily via npm, or clone the repo ( [\nhttps://github.com/run-llama/automatic-doc-translate ](https://github.com/run-\nllama/automatic-doc-translate) ) :\n\n    \n    \n    npm install -g autotranslatedoc\n\nTry it out with run-lama/LlamaIndexTS or your favorite repo! You will need a [\nGitHub Personal Access Token\n](https://docs.github.com/en/authentication/keeping-your-account-and-data-\nsecure/managing-your-personal-access-tokens) and an [ OpenAI API Key\n](https://platform.openai.com/api-keys) (the tool will prompt you to set\nthese):\n\n    \n    \n    # Translate\n    autotranslatedoc translate run-llama LlamaIndexTS -d apps/docs -l fr\n    \n    \n    #build\n    autotranslatedoc build run-llama LlamaIndexTS -d apps/docs -l fr\n\nThis translates the directory ` apps/docs ` in the GitHub repo ` run-\nllama/LlamaIndexTS ` .\n\n**Improving Accuracy and Consistency**\n\nOur commitment to improving translation accuracy led us to innovate in both\nthe translation process and verification methods.\n\n", "mimetype": "text/plain", "start_char_idx": 1286, "end_char_idx": 2694, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8720746f-1e9c-4f9c-8065-81233cf191ae": {"__data__": {"id_": "8720746f-1e9c-4f9c-8065-81233cf191ae", "embedding": null, "metadata": {"filename": "bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.md", "extension": ".md", "title": "Bridging the Language Gap in Programming: Introducing AutoTranslateDoc", "date": "Dec 8, 2023", "url": "https://www.llamaindex.ai/blog/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "033e4d61-87ff-431e-a45d-74bf4eea80d0", "node_type": "4", "metadata": {"filename": "bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.md", "extension": ".md", "title": "Bridging the Language Gap in Programming: Introducing AutoTranslateDoc", "date": "Dec 8, 2023", "url": "https://www.llamaindex.ai/blog/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8"}, "hash": "effb51b17e4f4766f6ddfe3d17667dc98e1c4b0713b487aa20097b1c03e39cdd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "607b2025-b5f4-4850-afd3-73e1e9e5b377", "node_type": "1", "metadata": {"filename": "bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.md", "extension": ".md", "title": "Bridging the Language Gap in Programming: Introducing AutoTranslateDoc", "date": "Dec 8, 2023", "url": "https://www.llamaindex.ai/blog/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8"}, "hash": "5d334ebb1b217ec3bcdf3e496f7ecb08aa43014dd3ae8f2b646b15928e5ba118", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "393c563d-253f-4de9-a4e4-099b04f03be6", "node_type": "1", "metadata": {}, "hash": "d579494cb1e82a80e106b5f7ae530fff983974eafcdf5411e85319805aaab5a0", "class_name": "RelatedNodeInfo"}}, "text": "**Strategic Document Splitting:**\n\nWe approach translation by dividing each page of the documentation into\nsections. To provide enhanced context and coherence, each section\u2019s title\nhierarchy is appended to its respective chunk during translation. This\ntechnique ensures that the translated content maintains the original structure\nand thematic relevance.\n\n**Rigorous Translation Verification:**\n\nOur verification process is designed to rigorously assess the accuracy of\ntranslations. We employ several checks on the translated documentation:\n\n  * Translation Length Check: We compare the length of the translated text with the original to ensure consistency. \n  * Title Hierarchy Analysis: We verify that no new sections are inadvertently added in the translation. \n  * Link Count Validation: The number of hyperlinks is matched against the original to ensure none are missed or added unnecessarily. \n  * Code Block Accuracy: The presence and correctness of code blocks in the translation are checked against the original document. \n\nThese checks address common issues with LLMs, such as hallucination or\nomission, and prompt retranslation when necessary. This rigorous process\nsignificantly enhances the accuracy of our translations. Moreover, we\nincorporate a unique self-critique feature, where the LLM evaluates its own\ntranslation output, further refining the quality.\n\nThis dual approach of meticulous chunking and thorough verification ensures\nthat our translations are not only accurate but also contextually relevant,\nmaintaining the integrity and utility of the original documentation.\n\n**Managing Documentation Updates: Keeping Translations Current**\n\nDocumentation, by its nature, is a dynamic entity that evolves over time.\nRecognizing this, we\u2019ve integrated a robust system into AutoDocTranslate to\nmanage documentation updates efficiently.\n\n**Historical Tracking through JSON:**\n\nWhen translating a repository using our tool, a .json file is generated,\nchronicling the history of translations. This file is crucial for tracking\nchanges and versions in the documentation. It serves as a foundation for\ndifferential translation, a process that identifies and translates only the\nnewly added or modified content. This feature can be accessed through the `\nautotranslatedoc update ` command, streamlining the maintenance of up-to-date\ntranslations.\n\n**Future Enhancements:**\n\nWe are actively working on enhancing this system with the following features:\n\n**Manual Change Integration:** Recognizing that translations might undergo\nmanual edits post-generation, we are developing functionality to account for\nthese manual changes during updates. This will ensure that any human revisions\nare retained and only new or altered sections from the source documentation\nare translated in subsequent updates.\n\n**GUI for Translation Management:** To further simplify the process of\ntranslation editing, tracking, and verification, we\u2019re in the early stages of\ndeveloping a graphical user interface (GUI). This interface will allow users\nto interact more intuitively with the translations. An experimental version of\nthis feature can be accessed through the ` autotranslatedoc serve ` command.\nThis GUI will enable users to visually navigate through the translations, make\nedits, and verify the accuracy of the content more efficiently.\n\nBy continually updating and refining these features, AutoDocTranslate aims to\nstay at the forefront of making technical documentation universally accessible\nand easy to maintain in multiple languages.\n\n**The Future of Technical Documentation**\n\nAutoDocTranslate is more than a tool; it\u2019s a step towards an inclusive,\nbarrier-free tech world where language is no longer an impediment to learning\nand growth. We\u2019re excited to see how it empowers programmers across the globe.\n\n", "mimetype": "text/plain", "start_char_idx": 2694, "end_char_idx": 6507, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "393c563d-253f-4de9-a4e4-099b04f03be6": {"__data__": {"id_": "393c563d-253f-4de9-a4e4-099b04f03be6", "embedding": null, "metadata": {"filename": "bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.md", "extension": ".md", "title": "Bridging the Language Gap in Programming: Introducing AutoTranslateDoc", "date": "Dec 8, 2023", "url": "https://www.llamaindex.ai/blog/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "033e4d61-87ff-431e-a45d-74bf4eea80d0", "node_type": "4", "metadata": {"filename": "bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.md", "extension": ".md", "title": "Bridging the Language Gap in Programming: Introducing AutoTranslateDoc", "date": "Dec 8, 2023", "url": "https://www.llamaindex.ai/blog/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8"}, "hash": "effb51b17e4f4766f6ddfe3d17667dc98e1c4b0713b487aa20097b1c03e39cdd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8720746f-1e9c-4f9c-8065-81233cf191ae", "node_type": "1", "metadata": {"filename": "bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.md", "extension": ".md", "title": "Bridging the Language Gap in Programming: Introducing AutoTranslateDoc", "date": "Dec 8, 2023", "url": "https://www.llamaindex.ai/blog/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8"}, "hash": "6f6ccfefe725536b2dcd20a47d6bfc79474459e050ad90e039766e89bc64cae3", "class_name": "RelatedNodeInfo"}}, "text": "Join us in this journey and contribute to a more accessible programming\ncommunity!\n\n", "mimetype": "text/plain", "start_char_idx": 6507, "end_char_idx": 6591, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "951cfd40-1b14-4096-a3ac-8edefbd45b61": {"__data__": {"id_": "951cfd40-1b14-4096-a3ac-8edefbd45b61", "embedding": null, "metadata": {"filename": "llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.md", "extension": ".md", "title": "LlamaIndex + Waii: Combining Structured Data from your Database with PDFs for Enhanced Data Analysis", "date": "Dec 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc551b46-b793-437e-9b58-99461950ca6f", "node_type": "4", "metadata": {"filename": "llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.md", "extension": ".md", "title": "LlamaIndex + Waii: Combining Structured Data from your Database with PDFs for Enhanced Data Analysis", "date": "Dec 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82"}, "hash": "117108173563c63de81da06be7e49271af15340be0d02f515876c3f6e0c1c458", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8d02e82-9ebb-4540-bc3f-20043ad27aaa", "node_type": "1", "metadata": {}, "hash": "cbde0eb85e6a11c1fe05d5ab5ccb91bb70f8487d2e019d7b15e1daa2a3bc55ed", "class_name": "RelatedNodeInfo"}}, "text": "#  Introduction\n\nIn many enterprises, data primarily resides in databases and generally, it\u2019s\nvery difficult to combine database data with other forms of data, such as\nPDFs, when trying to generate actionable insights.\n\nWe envision the development of an agent that empowers anyone to leverage data\nfrom all of these data sources for informed decision-making. Imagine an agent\nproficient in creating documents by merging data from diverse sources,\nincluding JIRA and databases, further enriched with the latest internet-\nsourced information.\n\nAt [ waii.ai ](http://waii.ai/) , we are committed to delivering an enterprise\ntext-to-SQL API with the most complete and accurate translation of plain\nEnglish to SQL available. Waii allows companies to build text-to-SQL right\ninto their products as well as enable no-code analytics for their internal\ndata/business teams. Waii works out of the box and can be self-hosted/on-prem.\n\nLlamaIndex introduces a remarkable RAG framework, facilitating the connection\nof various customer data sources, such as PDFs, Notion, and internal knowledge\nbases, to large language models (LLMs). This advancement simplifies the\ncreation of data-augmented chatbots and analysis agents.\n\nThis opens up a prime opportunity to develop an enterprise agent that can\naccess data from multiple sources, including your preferred database. We will\nexplore this further in the rest of the blog.\n\n#  Why a New Text-to-SQL LlamaIndex Plugin?\n\nTo enable the Llama Index agent to utilize text-to-SQL APIs, a plugin is\nessential. LlamaIndex already has a built-in text-to-SQL plugin, but why did\nwe decide to create a new LlamaHub plugin?\n\nThe existing text-to-SQL plugin in LlamaIndex has been suitable for handling\nsimple databases (less than 10 tables, 100 columns) with straightforward SQL\nqueries. However, managing medium to large databases, which can include 100s\nof tables and 1000s of columns, presents a complex challenge. Limitations\narise due to the restricted context windows of LLMs, and even those with large\ncontext windows, like GPT-4-turbo with its 128K tokens, can suffer from\ninaccuracies and regression in task retrieval when overloaded with content.\nThis issue is discussed in a [ LlamaIndex study\n](https://x.com/jerryjliu0/status/1722657583331491860?s=20) .\n\nIn contrast, Waii focuses on making query generation more efficient. We have\ndeveloped a built-in compiler to deal with compilation errors from LLMs to\nsupport multiple dialects. Our internal knowledge graph, created from database\nmetadata, constraints, and query history, aids in table/schema selection.\nUsers can also apply semantic rules to schema/table/column, or integrate with\ntheir data catalog services, ensuring the semantic correctness of generated\nqueries, in addition to syntactic correctness.\n\nTo utilize our service, users simply need to connect their database to Waii\nand copy a Waii API key to create a LlamaIndex agent.\n\n#  LlamaIndex + Waii Agent\n\nWe are thrilled to showcase the integration of Waii with LlamaIndex to create\nan agent capable of executing various text-to-SQL tasks and validating the\ndata based on a PDF.\n\nWe\u2019ll be analyzing customers\u2019 top-purchased categories during Christmas time,\nand compare it with [ Deloitte\u2019s holiday retail survey\n](https://www2.deloitte.com/content/dam/insights/articles/us176694_cic_holiday-\nretail-survey/DI_2023-Deloitte-holiday-retail-survey.pdf) report.\n\n#  Architecture of LlamaIndex + Waii\n\nBefore diving into the code example, let\u2019s look at the architecture first:\n\nThe LlamaIndex agent operates on the client side, accompanied by a number of\ntools: Each tool provides function specifications and allows functions to be\nselected based on context and the user\u2019s input to ` **chat(\"\u2026\")** ` . For\nexample, if the question indicates information needs to be retrieved from the\n\u201cinternet\u201d, the Google search tool will be chosen. Internally it uses LLM\nwhich returns selected functions with parameters for a given context.\n\nWhen the Waii tool is chosen, whether for describing a dataset, generating a\nquery, or running a query, it sends the API request to the Waii Service.\n\nThe Waii Service can be deployed as a hosted SaaS or as Docker containers\nrunning in your on-premises environment. The components of the Waii Service\ninclude:\n\n  * **The Query Generator:** coordinates the entire workflow of query generation and communicates with the LLM for this purpose. \n  * **Knowledge Graph / Metadata Management** : connects to databases, extracting metadata and query history as a knowledge graph to assist the Query Generator in choosing the right tables and schemas. \n  * **Semantic Rules** : These aid the Query Generator in producing semantically correct queries. \n  * **Waii Compiler** : After a query is generated by the LLM, the Waii Compiler patches identified issues in the query. If a compilation issue is not fixable, it regenerates the query with an articulated error message. \n\n#  Create LlamaIndex agent with Waii + PDF Loader\n\nLet\u2019s first create two LlamaHub tools \u2014 Waii and PDF Loader. LlamaHub tools\ninclude specs to identify available functions along with their parameters, the\nagent will select and execute which function to use based on available\nfunctions and context.\n\nLet\u2019s start with creating an agent which includes the Waii tool:\n\n    \n    \n    from llama_hub.tools.google_search import GoogleSearchToolSpecfrom llama_hub.tools.waii import WaiiToolSpecfrom llama_index.agent import OpenAIAgentfrom llama_index.llms import OpenAIwaii_tool = WaiiToolSpec(    api_key='waii_api_key',    # Connection key of WAII connected database, see     # https://github.com/waii-ai/waii-sdk-py#get-connections    database_key='database_to_use',    verbose=True)\n\nAnd then create a PDF tool:\n\n    \n    \n    from pathlib import Pathfrom llama_index import download_loaderfrom llama_index import VectorStoreIndexPDFReader = download_loader(\"PDFReader\")loader = PDFReader()documents = loader.load_data(file=Path('DI_2023-Deloitte-holiday-retail-survey.pdf'))index = VectorStoreIndex.from_documents(documents)engine = index.as_query_engine(similarity_top_k=5)deloitte_retail_survey_tool = QueryEngineTool(        query_engine=engine,        metadata=ToolMetadata(            name=\"deloitte_retail_survey\",            description=(                \"Provides retail survey report for holiday sales based on Deloitte's data\"                \"Use a detailed plain text question as input to the tool, and output using plain text based on pdf data\"            ),        ),    )\n\nAnd at last, create an agent which combines Waii and PDF tools:\n\n    \n    \n    agent = OpenAIAgent.from_tools(  [deloitte_retail_survey_tool] + waii_tool.to_tool_list(),   llm=OpenAI(model='gpt-4-1106-preview', temperature=0),   verbose=True)\n\n` [deloitte_retail_survey_tool] + waii_tool.to_tool_list() ` indicate using\nall functions (such as getting answers from the database, generating a query,\nexecuting a query, describing datasets, etc.) provided by Waii and PDF Search.\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 7007, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8d02e82-9ebb-4540-bc3f-20043ad27aaa": {"__data__": {"id_": "c8d02e82-9ebb-4540-bc3f-20043ad27aaa", "embedding": null, "metadata": {"filename": "llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.md", "extension": ".md", "title": "LlamaIndex + Waii: Combining Structured Data from your Database with PDFs for Enhanced Data Analysis", "date": "Dec 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc551b46-b793-437e-9b58-99461950ca6f", "node_type": "4", "metadata": {"filename": "llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.md", "extension": ".md", "title": "LlamaIndex + Waii: Combining Structured Data from your Database with PDFs for Enhanced Data Analysis", "date": "Dec 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82"}, "hash": "117108173563c63de81da06be7e49271af15340be0d02f515876c3f6e0c1c458", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "951cfd40-1b14-4096-a3ac-8edefbd45b61", "node_type": "1", "metadata": {"filename": "llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.md", "extension": ".md", "title": "LlamaIndex + Waii: Combining Structured Data from your Database with PDFs for Enhanced Data Analysis", "date": "Dec 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82"}, "hash": "8e0f94af092955fcbc7fc0617fe1a407019ee842fd907b54bab673e1b45fa116", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37c06b17-7703-4678-b96f-d344ad99bb12", "node_type": "1", "metadata": {}, "hash": "017090c36bb82108bd00a85eee905ec567daf7018a9874872e5fa03f3215b41b", "class_name": "RelatedNodeInfo"}}, "text": "#  Understand your dataset\n\nThe first step in doing data analysis is to get a better understanding of your\ndataset.\n\nYou can start asking questions to your agent:\n\n    \n    \n    agent.chat(\"Describe my database\")\n\nThe output of which is:\n\n    \n    \n    STARTING TURN 1---------------=== Calling Function ===Calling function: describe_dataset with args:   {\"ask\":\"Can you describe the whole database?\"}...========================STARTING TURN 2---------------The database I have access to consists of several schemas, each with its own set of tables and domains:1. **CRUNCHBASE_2016**: This schema includes tables related to acquisitions,    companies, investments, and funding rounds in the Crunchbase database for   the year 2016. It's useful for analyzing acquisition trends, company    information, investment activity, and funding trends.2. **RETAIL_DATA**: This schema contains tables related to retail operations,    such as call centers, customers, addresses, demographics, inventory,    items, promotions, stores, returns, sales, and warehouses. It can be used    to analyze call center performance, customer demographics, inventory    management, sales performance, and other retail operations.3. ...\n\nAs you can see, the agent understood the request, called ` describe_dataset `\nfunction provided by Waii, and generated a summary of the dataset.\n\nI\u2019m interested in the ` RETAIL_DATA ` schema, so let me ask more of the schema\n-\n\n    \n    \n    agent.chat(\"What can i do with the retail_data schema\")\n\nAnd I get this:\n\n    \n    \n    The RETAIL_DATA schema in the TWEAKIT_PLAYGROUND database is designed to support a wide range of analyses related to retail operations. Here are some of the capabilities and types of analyses you can perform with this schema:1. **Call Center Analysis**: Evaluate the performance of call centers,    understand call volumes, and assess customer service efficiency.2. **Customer Demographics**: Analyze customer profiles, including    demographics, purchasing behaviors, and preferences. This can help    in targeted marketing and customer segmentation.Specific questions that can be addressed using the RETAIL_DATA schema include:- What is the total number of call centers?- How many customers have a preferred customer flag?- What is the average price of items?\n\nLet me do some more data analysis.\n\n#  Generate an SQL query and run it\n\nLet\u2019s generate a SQL query (asking top 10 item categories sold during\nChristmas time):\n\n    \n    \n    agent.chat(\"Top 10 item category sold during christmas time across all years\")\n\nNow it calls ` get_answer ` function from Waii tool:\n\n    \n    \n    === Calling Function ===Calling function: get_answer with args:   {\"ask\":\"What are the top 10 item categories sold during           Christmas time across all years?\"}\n\nAnd got a SQL like:\n\n    \n    \n    WITH christmas_sales AS (    SELECT        ss_item_sk,        SUM(ss_quantity) AS total_quantity    FROM tweakit_playground.retail_data.store_sales AS ss    INNER JOIN tweakit_playground.retail_data.date_dim AS dd        ON ss_sold_date_sk = d_date_sk    WHERE        d_holiday = 'Y' AND d_date ILIKE '%-12-25'    GROUP BY        ss_item_sk),ranked_categories AS (    SELECT        i_category,        SUM(total_quantity) AS category_quantity    FROM christmas_sales AS cs    INNER JOIN tweakit_playground.retail_data.item AS i        ON ss_item_sk = i_item_sk    GROUP BY        i_category    ORDER BY        category_quantity DESC    LIMIT 10)SELECT    i_category AS category,    category_quantityFROM ranked_categories\n\nLet me ask it to run the query:\n\n    \n    \n    agent.chat(\"Run it\")\n\nI got a Dataframe along with its summary:\n\n    \n    \n    The top 10 item categories sold during Christmas time across all years, based on the total quantity of items, are:1. Women: 1,487,8912. ", "mimetype": "text/plain", "start_char_idx": 7007, "end_char_idx": 10821, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37c06b17-7703-4678-b96f-d344ad99bb12": {"__data__": {"id_": "37c06b17-7703-4678-b96f-d344ad99bb12", "embedding": null, "metadata": {"filename": "llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.md", "extension": ".md", "title": "LlamaIndex + Waii: Combining Structured Data from your Database with PDFs for Enhanced Data Analysis", "date": "Dec 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc551b46-b793-437e-9b58-99461950ca6f", "node_type": "4", "metadata": {"filename": "llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.md", "extension": ".md", "title": "LlamaIndex + Waii: Combining Structured Data from your Database with PDFs for Enhanced Data Analysis", "date": "Dec 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82"}, "hash": "117108173563c63de81da06be7e49271af15340be0d02f515876c3f6e0c1c458", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8d02e82-9ebb-4540-bc3f-20043ad27aaa", "node_type": "1", "metadata": {"filename": "llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.md", "extension": ".md", "title": "LlamaIndex + Waii: Combining Structured Data from your Database with PDFs for Enhanced Data Analysis", "date": "Dec 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82"}, "hash": "0653392cd7274a454beee1c3311bc4029426eef7fdf7d7606411e410d89e6907", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b654dc8-9db5-4a77-b5de-0c4f300e8d5c", "node_type": "1", "metadata": {}, "hash": "4ffc3e72717c42ab69db51c4266f2d96980e6d856e96d41c22a05022c83b2b36", "class_name": "RelatedNodeInfo"}}, "text": "Sports: 1,486,6443. Children: 1,483,5314. Electronics: 1,478,4635. Music: 1,476,0456. Books: 1,472,9117. Home: 1,471,3488. Jewelry: 1,459,0259. Shoes: 1,456,29610. Men: 1,451,285\n\n#  Use with a PDF report\n\nLet\u2019s try to cross-check the result with the [ holiday retailer report (in PDF\nform) from Deloitte\n](https://www2.deloitte.com/content/dam/insights/articles/us176694_cic_holiday-\nretail-survey/DI_2023-Deloitte-holiday-retail-survey.pdf) \u2014 Are Deloitte\u2019s\ntop-purchased gift categories the same as what we saw from the database?\n\nTop gift categories during the holiday\n\nThere\u2019s one page from the pdf we are interested in \u2014 let\u2019s see if we can\nretrieve this page\n\n    \n    \n    agent.chat(\"\"\"Give me top holiday gift categories based on Deloitte's holiday retail survey, and how it is different from the top 10 item category sold during christmas time across all years from my database\"\"\")\n\nThe output looks like the following, which indicates it uses the `\ndeloitte_retail_survey ` tool to obtain insights from the PDF.\n\n    \n    \n    STARTING TURN 1---------------=== Calling Function ===Calling function: deloitte_retail_survey with args: {\"input\": \"What are the top holiday gift categories based on Deloitte's holiday retail survey?\"}...\n\nIt gives the following summary:\n\n    \n    \n    Based on Deloitte's holiday retail survey, the top holiday gift categories are:1. Clothing & Accessories2. Gift Cards & Other3. Food & Beverage...From your database, the top 10 item categories sold during Christmas time across all years are:1. Women2. ", "mimetype": "text/plain", "start_char_idx": 10821, "end_char_idx": 12366, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b654dc8-9db5-4a77-b5de-0c4f300e8d5c": {"__data__": {"id_": "5b654dc8-9db5-4a77-b5de-0c4f300e8d5c", "embedding": null, "metadata": {"filename": "llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.md", "extension": ".md", "title": "LlamaIndex + Waii: Combining Structured Data from your Database with PDFs for Enhanced Data Analysis", "date": "Dec 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc551b46-b793-437e-9b58-99461950ca6f", "node_type": "4", "metadata": {"filename": "llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.md", "extension": ".md", "title": "LlamaIndex + Waii: Combining Structured Data from your Database with PDFs for Enhanced Data Analysis", "date": "Dec 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82"}, "hash": "117108173563c63de81da06be7e49271af15340be0d02f515876c3f6e0c1c458", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37c06b17-7703-4678-b96f-d344ad99bb12", "node_type": "1", "metadata": {"filename": "llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.md", "extension": ".md", "title": "LlamaIndex + Waii: Combining Structured Data from your Database with PDFs for Enhanced Data Analysis", "date": "Dec 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82"}, "hash": "dfdfabcf2cd2ef3ab128062099a366704abdd049c24baedf01554c3e7df43fae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60d1748f-c66a-4077-967f-01b8cedbc4a9", "node_type": "1", "metadata": {}, "hash": "e23cc51d7a1da07edeb4083e6c7a498968106c58e3657a8392e5209ed4ed54f2", "class_name": "RelatedNodeInfo"}}, "text": "Sports3. Children...Comparing the two lists, we can see some differences and similarities:- \"Clothing & Accessories\" from Deloitte's survey could correspond to   \"Women,\" \"Men,\" and possibly \"Children\" from your database.- \"Electronics & Accessories\" is a common category in both lists.- \"Gift Cards & Other\" and \"Food & Beverage\" from Deloitte's survey do   not have a direct match in the top categories from your database....\n\nBingo! Now we can compare the results from our database with PDFs. And I love\nseeing how the agent can correlate the two lists and tell me that my store\ndoesn\u2019t have the \u201cGift Cards & Other\u201d and \u201cFood & Beverage\u201d categories!\n\nYou can find the code from the Colab notebook [ link\n](https://colab.research.google.com/drive/1hL_Ztb1DRcdyeO2JZwsc9osW0Y3chP9c#scrollTo=fvye9sqAcn5j)\n\n#  Wrapping up\n\nThe integration of Waii\u2019s text-to-SQL API with LlamaIndex\u2019s RAG framework\nmarks a significant advancement in enterprise data analytics. This powerful\ncombination enables companies to effortlessly merge and analyze data from\nvarious sources, including databases, PDFs, and the Internet. ", "mimetype": "text/plain", "start_char_idx": 12366, "end_char_idx": 13476, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60d1748f-c66a-4077-967f-01b8cedbc4a9": {"__data__": {"id_": "60d1748f-c66a-4077-967f-01b8cedbc4a9", "embedding": null, "metadata": {"filename": "llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.md", "extension": ".md", "title": "LlamaIndex + Waii: Combining Structured Data from your Database with PDFs for Enhanced Data Analysis", "date": "Dec 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc551b46-b793-437e-9b58-99461950ca6f", "node_type": "4", "metadata": {"filename": "llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.md", "extension": ".md", "title": "LlamaIndex + Waii: Combining Structured Data from your Database with PDFs for Enhanced Data Analysis", "date": "Dec 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82"}, "hash": "117108173563c63de81da06be7e49271af15340be0d02f515876c3f6e0c1c458", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b654dc8-9db5-4a77-b5de-0c4f300e8d5c", "node_type": "1", "metadata": {"filename": "llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.md", "extension": ".md", "title": "LlamaIndex + Waii: Combining Structured Data from your Database with PDFs for Enhanced Data Analysis", "date": "Dec 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82"}, "hash": "11bd4b013e75538d86ee913e5db75bbb9e8d444efa317d4791871ca1f860a771", "class_name": "RelatedNodeInfo"}}, "text": "We demonstrated\nthe agent\u2019s capability to generate SQL queries, understand complex datasets,\nand correlate findings with external reports. This innovation not only\nsimplifies data analysis but also opens new avenues for informed decision-\nmaking in the digital era.\n\nTo learn more about Waii, please contact us here: [\nhttps://www.waii.ai/#request-demo ](https://www.waii.ai/#request-demo)\n\n", "mimetype": "text/plain", "start_char_idx": 13476, "end_char_idx": 13867, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e496535-e2ec-4ab9-97b9-5b5113debbce": {"__data__": {"id_": "2e496535-e2ec-4ab9-97b9-5b5113debbce", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-12-05-faf5ab930264.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201305", "date": "Dec 5, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-05-faf5ab930264"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1f669af6-4a8c-4e5b-a391-e6ed61f214a2", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-12-05-faf5ab930264.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201305", "date": "Dec 5, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-05-faf5ab930264"}, "hash": "51cea6ad3bedb71934cededff05ef54070ae7c93498a506d9806fbf728f4f7c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46a6aea0-e272-40a6-9fe2-f36e92efd13c", "node_type": "1", "metadata": {}, "hash": "032f9676641c3aebe794c067a3ce6a4f48c9f8a980c5c0ed100db0aa445a88de", "class_name": "RelatedNodeInfo"}}, "text": "Hello Llama Community ,\n\nWe are excited to collaborate with DeepLearningAI and TruEraAI to launch an\nextensive course on advanced Retrieval-Augmented Generation (RAG) and its\nevaluations. The course includes Sentence Window Retrieval, Auto-merging\nRetrieval, and Evaluations with TruLensML, providing practical tools for\nenhanced learning and application. To make the most of this learning\nopportunity, we invite you to [ take the course\n](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-\nrag/?utm_campaign=truerallamaindex-\nlaunch&utm_medium=video&utm_source=youtube&utm_content=teaser) .\n\nWe appreciate your support and are always excited to see your projects and\nvideos. Feel free to share them at [ news@llamaindex.ai\n](mailto:news@llamaindex.ai) . Also, remember to subscribe to our newsletter\non our [ website ](https://www.llamaindex.ai/) for the latest updates and to\nconnect with our vibrant community.\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 936, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "46a6aea0-e272-40a6-9fe2-f36e92efd13c": {"__data__": {"id_": "46a6aea0-e272-40a6-9fe2-f36e92efd13c", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-12-05-faf5ab930264.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201305", "date": "Dec 5, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-05-faf5ab930264"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1f669af6-4a8c-4e5b-a391-e6ed61f214a2", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-12-05-faf5ab930264.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201305", "date": "Dec 5, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-05-faf5ab930264"}, "hash": "51cea6ad3bedb71934cededff05ef54070ae7c93498a506d9806fbf728f4f7c7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e496535-e2ec-4ab9-97b9-5b5113debbce", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-12-05-faf5ab930264.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201305", "date": "Dec 5, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-05-faf5ab930264"}, "hash": "656f82f5e64a29e36901ebd032e7a8bb1ad09c7e4fb22f4af48e4a198daf3a51", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5d8d0a7-5d3d-4be9-9044-219e60047a74", "node_type": "1", "metadata": {}, "hash": "a170e78418f4e8f70cb178826d487dece1dc0c7ae8a70894769faa642c29cfb8", "class_name": "RelatedNodeInfo"}}, "text": "**First, the highlights:**\n\n  1. **Launch of Seven Advanced Retrieval LlamaPacks** : Simplifies building advanced RAG systems to nearly a single line of code, offering techniques like Hybrid Fusion and Auto-merging Retriever. [ Tweet ](https://x.com/llama_index/status/1729303619760259463?s=20) . \n  2. **Introduction of the OpenAI Cookbook** : A comprehensive guide for evaluating RAG systems with LlamaIndex, covering system understanding, building, and performance evaluation. [ Blog ](/openai-cookbook-evaluating-rag-systems-fe393c61fb93) , [ Notebook ](https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb)\n  3. **Speed Enhancement in Structured Metadata Extraction** : Achieved 2x to 10x faster processing in extracting structured metadata from text, boosting RAG performance. [ Docs ](https://t.co/sBVWeO8jKo) , [ Tweet ](https://x.com/llama_index/status/1730400634757939691?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 936, "end_char_idx": 1883, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5d8d0a7-5d3d-4be9-9044-219e60047a74": {"__data__": {"id_": "d5d8d0a7-5d3d-4be9-9044-219e60047a74", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-12-05-faf5ab930264.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201305", "date": "Dec 5, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-05-faf5ab930264"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1f669af6-4a8c-4e5b-a391-e6ed61f214a2", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-12-05-faf5ab930264.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201305", "date": "Dec 5, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-05-faf5ab930264"}, "hash": "51cea6ad3bedb71934cededff05ef54070ae7c93498a506d9806fbf728f4f7c7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46a6aea0-e272-40a6-9fe2-f36e92efd13c", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-12-05-faf5ab930264.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201305", "date": "Dec 5, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-05-faf5ab930264"}, "hash": "680243faf168370d54cd018e24667409c6d3e332bf8ed3e32f5d23b7d814085a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "968ae840-c8ad-4899-a9ec-ad73d3d7032a", "node_type": "1", "metadata": {}, "hash": "180468a090ea25a167e075105edf27353b0c7eff32183da835972644966376cf", "class_name": "RelatedNodeInfo"}}, "text": "4. We launched versions 3 of [ RAGs ](https://github.com/run-llama/rags) , our project that lets you use natural language to generate a RAG bot customized to your needs. This version incorporates web search, so your bot can incorporate answers fresh from the web. [ Tweet ](https://x.com/llama_index/status/1730320635279331524?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 1883, "end_char_idx": 2221, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "968ae840-c8ad-4899-a9ec-ad73d3d7032a": {"__data__": {"id_": "968ae840-c8ad-4899-a9ec-ad73d3d7032a", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-12-05-faf5ab930264.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201305", "date": "Dec 5, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-05-faf5ab930264"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1f669af6-4a8c-4e5b-a391-e6ed61f214a2", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-12-05-faf5ab930264.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201305", "date": "Dec 5, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-05-faf5ab930264"}, "hash": "51cea6ad3bedb71934cededff05ef54070ae7c93498a506d9806fbf728f4f7c7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5d8d0a7-5d3d-4be9-9044-219e60047a74", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-12-05-faf5ab930264.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201305", "date": "Dec 5, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-05-faf5ab930264"}, "hash": "4305a2f2c796853b138d1c8b50a2b0dc5c6cc2383cbbc7fd70132c4d122baefb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ccd7967-6a64-41f5-a71b-ea4d025e145c", "node_type": "1", "metadata": {}, "hash": "9ce862b6ba95b6c154f413e517033e61ebada458d0cf52fb8f9952c942e882e0", "class_name": "RelatedNodeInfo"}}, "text": "5. **Core** [ **guide** ](https://docs.llamaindex.ai/en/latest/community/full_stack_projects.html#) **for Full-Stack LLM App Development** : Simplifies complex app development with tools like \u2018create-llama\u2019 for full-stack apps, \u2018SEC Insights\u2019 for multi-document processing, and \u2018LlamaIndex Chat\u2019 for chatbot customization. \n\n**Feature Releases and Enhancements:**\n\n  * We\u2019ve launched seven advanced retrieval LlamaPacks, serving as templates to easily build advanced RAG systems. These packs simplify the process to almost a single line of code, moving away from the traditional notebook approach. The techniques include Hybrid Fusion, Query Rewriting + Fusion, Retrieval with Embedded Tables, Auto-merging Retriever, Sentence Window Retriever, Node Reference Retriever, and Multi-Document Agents for handling complex queries. [ Tweet ](https://x.com/llama_index/status/1729303619760259463?s=20) . \n  * We introduce new abstractions for structured output extraction in multi-modal settings, enabling the transformation of images into structured Pydantic objects. This enhancement is particularly useful for applications like product reviews, restaurant listings, and OCR. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/multi_modal_pydantic.ipynb) , [ Tweet ](https://x.com/llama_index/status/1729535952912290050?s=20) . \n  * We introduce the [ OpenAI Cookbook ](https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb) , a guide focused on evaluating RAG systems using LlamaIndex. It encompasses understanding RAG systems, building them with LlamaIndex, and evaluating their performance in retrieval and response generation. [ Blog ](/openai-cookbook-evaluating-rag-systems-fe393c61fb93) , [ Notebook ](https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb) , [ Tweet ](https://x.com/llama_index/status/1729587400240967761?s=20) . \n  * We launched RAGs v3 \u2014 a bot that transcends traditional limits by incorporating web search capabilities. This bot, designed to operate in natural language rather than code, offers an enhanced experience compared to the combination of ChatGPT and Bing. Leveraging our integration with Metaphor Systems \u2014 a search engine tailored for Large Language Models (LLMs) \u2014 the bot can retrieve relevant text from the internet to provide answers beyond its internal corpus. Additionally, users can now view the tools the agent uses, with the web search feature exclusively accessible to our OpenAI agent. [ Repo ](https://t.co/838BDVOEbA) , [ Tweet ](https://x.com/llama_index/status/1730320635279331524?s=20) . \n  * We have significantly improved the speed of extracting structured metadata (like titles and summaries) from text to enhance RAG performance. Our new implementation offers 2x to 10x faster processing, overcoming the limitations of previous slower methods. [ Docs ](https://t.co/sBVWeO8jKo) , [ Tweet ](https://x.com/llama_index/status/1730400634757939691?s=20) . \n  * We have made it incredibly easy to set up a RAG + Streamlit app, now possible with just a single line of code using our ` **StreamlitChatPack** ` . This pack provides a ready-to-use RAG pipeline and a Streamlit chat interface, customizable in terms of data sources and retrieval algorithms. [ Docs ](https://llamahub.ai/l/llama_packs-streamlit_chatbot) , [ Tweet ](https://x.com/llama_index/status/1731121252142878982?s=20) . \n\n**Demo:**\n\nAInimal Go \u2014 an innovative multi-modal app inspired by Pokemon-Go. This\ninteractive application, developed by [ Harshad Suryawanshi\n](https://harshadsuryawanshi.medium.com/) , lets users capture or upload\nimages of animals, classify them using the ResNet-18 model, and engage in\nconversations with the animals, augmented by a knowledge base of over 200\nWikipedia articles. Notably, the app employs a targeted ResNet model for\nclassification, offering enhanced speed and cost efficiency, instead of using\nGPT-4V.\n\n[ Blog ](/multimodal-rag-building-ainimal-go-fecf8404ed97) , [ Repo\n](https://github.com/AI-ANK/AInimalGo-Chat-with-Animals) , [ HuggingFace Space\n](https://huggingface.co/spaces/AI-ANK/AInimal_Go) , [ Tweet\n](https://x.com/llama_index/status/1729246724911477165?s=20) .\n\n**Guides:**\n\n  * We introduce a core [ guide ](https://docs.llamaindex.ai/en/latest/community/full_stack_projects.html#) within the LlamaIndex ecosystem, designed to simplify \u201cfull-stack\u201d app development, which is notably more complex than notebook development. This includes \u2018create-llama\u2019 for building full-stack apps with advanced templates, \u2018SEC Insights\u2019 for multi-document handling of over 10,000 filings, and \u2018LlamaIndex Chat\u2019 for a customizable chatbot experience. All tools are open-source with full guides and tutorials available. \n  * [ Guide ](https://t.co/2Ygxs6bPoX) on using the Table Transformer model with GPT-4V for advanced RAG applications in parsing tables from PDFs: Our method involves CLIP for page retrieval, Table Transforms for table image extraction, and GPT-4V for answer synthesis. This approach is compared with three other multi-modal table understanding techniques, including using CLIP for whole page retrieval, text extraction and indexing with GPT-4V, and OCR on table images for context. \n  * [ Guide ](https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/multi_modal_pydantic.ipynb) on analyzing various multi-modal models for their ability to extract structured data from complex product images on an Amazon page. The models compared include GPT-4V, Fuyu-8B, MiniGPT4, CogVLM-4, and LLaVa-13B. Key findings reveal that all models incorrectly identified the number of reviews (correct answer: 5685), only GPT-4V and Fuyu accurately determined the price, each model\u2019s product description varied from the original, and Mini-GPT4 incorrectly assessed the product rating. \n\n**Tutorials:**\n\n  * [ Jo Kristian Bergum ](https://www.linkedin.com/in/jo-bergum/) [ blog post ](https://blog.vespa.ai/scaling-personal-ai-assistants-with-streaming-mode/) on Hands-On RAG guide for personal data with Vespa and LLamaIndex. \n  * [ Wenqi Glantz ](https://twitter.com/wenqi_glantz) made a [ tutorial ](https://levelup.gitconnected.com/llama-packs-the-low-code-solution-to-building-your-llm-apps-269eec05557b) on Llama Packs: The Low-Code Solution to Building Your LLM Apps. \n  * [ Liza Shulyayeva ](https://twitter.com/Lazer) \u2019s in-depth [ tutorial ](https://www.daily.co/blog/search-your-video-content-library-with-llamaindex-and-chroma/) on building and deploying a retrieval-augmented generation (RAG) app to conversationally query the contents of your video library \n\n**Webinars:**\n\n  * [ Webinar ](https://www.youtube.com/watch?v=0zGHrcE-Zy4) on PrivateGPT \u2014 Production RAG with Local Models. \n\n**Hackathons:**\n\n  * Your reminder that there\u2019s still time to join [ the TruEra Challenge ](https://lablab.ai/event/truera-challenge-build-llm-applications?utm_medium=post&utm_source=twitter&utm_campaign=truera_challenge_hackathon&utm_term=hackathon_page&utm_content=event_promo) , an online hackathon from Dec 1st to 8th, and explore AI observability with technology from TruEra AI and Google Vertex AI. Use the LlamaIndex framework to enhance your LLM-based app. ", "mimetype": "text/plain", "start_char_idx": 2221, "end_char_idx": 9511, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ccd7967-6a64-41f5-a71b-ea4d025e145c": {"__data__": {"id_": "7ccd7967-6a64-41f5-a71b-ea4d025e145c", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-12-05-faf5ab930264.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201305", "date": "Dec 5, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-05-faf5ab930264"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1f669af6-4a8c-4e5b-a391-e6ed61f214a2", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-12-05-faf5ab930264.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201305", "date": "Dec 5, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-05-faf5ab930264"}, "hash": "51cea6ad3bedb71934cededff05ef54070ae7c93498a506d9806fbf728f4f7c7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "968ae840-c8ad-4899-a9ec-ad73d3d7032a", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-12-05-faf5ab930264.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201305", "date": "Dec 5, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-05-faf5ab930264"}, "hash": "b049d25baf92e4a005e3120ee564cd7f2ae448dd20d8cc36301c1552aa85d719", "class_name": "RelatedNodeInfo"}}, "text": "Participants receive $30 in Google Cloud credits, plus an additional $100 upon solution submission. Winners share a $9,000 cash prize pool and $14,000 in Google Cloud credits. \n  * We partnered with Zilliz Universe to participate in their [ Advent of Code ](https://t.co/hGzBA1acf6) event. This December, explore 25 open-source projects, with daily challenges to build something in 30 minutes or less. It\u2019s a great opportunity to learn new skills and have winter fun. For tips, tutorials, and resources, visit the Advent of Code channel in Discord each day. \n\n", "mimetype": "text/plain", "start_char_idx": 9511, "end_char_idx": 10071, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "448dd6b3-e0bb-4d21-a69d-e73ec33d4532": {"__data__": {"id_": "448dd6b3-e0bb-4d21-a69d-e73ec33d4532", "embedding": null, "metadata": {"filename": "introducing-llama-datasets-aadb9994ad9e.md", "extension": ".md", "title": "Introducing Llama Datasets \ud83e\udd99\ud83d\udcdd", "date": "Dec 4, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llama-datasets-aadb9994ad9e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fe69c12-a585-49f8-b1c8-90f65d411cc5", "node_type": "4", "metadata": {"filename": "introducing-llama-datasets-aadb9994ad9e.md", "extension": ".md", "title": "Introducing Llama Datasets \ud83e\udd99\ud83d\udcdd", "date": "Dec 4, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llama-datasets-aadb9994ad9e"}, "hash": "ba4720e4ec89447f3ca5f979a878dfac87e2814803fe348dfa2931393f879f3e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d80b8c2-2c42-4132-b57e-5ef41908869c", "node_type": "1", "metadata": {}, "hash": "871ba675f37ae2f7485514f35027a46f5932f14bb8950a604975f55e2df360b0", "class_name": "RelatedNodeInfo"}}, "text": "(Authors: Andrei Fajardo and Jerry Liu @ LlamaIndex)\n\nToday we\u2019re excited to introduce **Llama Datasets** \u2014 a set of community-\ncontributed datasets that allow users to easily benchmark their RAG pipelines\nfor different use cases. A dataset consists of both question-answer pairs as\nwell as source context. To use them, download them from LlamaHub; then\nevaluate your RAG pipeline using the dataset + a set of evaluation metrics.\n\nWe\u2019re launching with an initial set of 10 evaluation datasets and we\u2019ll be\nadding more! We\u2019ve also made it super easy to contribute your own dataset \u2014\nupload your source documents + QA pairs (generated manually or synthetically).\n\n#  Context\n\nA big problem in building production RAG is evaluation. Unlike traditional\nsoftware systems, LLM systems (and ML systems more generally) are stochastic\nblack-boxes designed to model noisy real-world signals. This means that\ndevelopers can\u2019t easily define unit tests that assert deterministic behavior \u2014\nthere may always be an input that causes an error. Because developers don\u2019t\nquite know what goes out given what goes in, they need to define an\n**evaluation dataset** that\u2019s reflective of their production use cases, and\nevaluate their system over this dataset using a set of **evaluation metrics**\n.\n\nWe\u2019ve presented [ extensively on this topic\n](https://docs.google.com/presentation/d/1wtlEJC6SXLsoGkDdCkxC-\nY_V8bvyYsi9Ozh59VMI22I/edit?usp=sharing) \u2014 every AI engineer should setup\nevaluation before trying to optimize their LLM or RAG application with\nadvanced techniques.\n\nBut we\u2019ve increasingly found that defining the **right evaluation dataset is\nhard and use-case dependent** . Evaluating over academic benchmarks, like BEIR\nand HotpotQA oftentimes fail to generalize to specific use cases. Certain\nparameters that work well on certain data domains (e.g. SEC filings) may fail\non others (e.g. research papers).\n\nThat\u2019s what inspired us to create Llama Datasets. Instead of being\nprescriptive on the data you must use, we\u2019ve decided to create a hub where you\ncan easily pick and choose the right datasets for your use case!\n\nLlama Datasets on LlamaHub\n\n#  Overview\n\nToday\u2019s launch includes the set of Llama Datasets on [ LlamaHub\n](https://llamahub.ai/) , an accompanying ` RagEvaluatorPack ` to help compute\nmetrics over a dataset, as well as accompanying dataset abstractions that you\ncan also use on their own.\n\n  * To use a Llama Dataset, download it off LlamaHub and run our ` RagEvaluatorPack ` (or run your own evaluation modules). \n  * To generate a Llama Dataset, define a ` LabelledRagDataset ` with a set of ` LabelledRagDataExample ` objects. \n  * To contribute a Llama Dataset, submit a \u201cdata card\u201d to LlamaHub and upload your raw dataset files to our ` llama_datasets ` repository. \n\nCheck out the below sections for a walkthrough over an example dataset.\n\nWe\u2019re launching with 10 initial datasets:\n\n  * [ Blockchain Solana Dataset ](https://llamahub.ai/l/llama_datasets-blockchain_solana)\n  * [ Coda Help Desk Dataset (with Braintrust) ](https://llamahub.ai/l/llama_datasets-braintrust_coda)\n  * [ FinanceBench Dataset (Patronus AI) ](https://llamahub.ai/l/llama_datasets-patronus_financebench)\n  * [ Paul Graham Essay Dataset ](https://llamahub.ai/l/llama_datasets-paul_graham_essay)\n  * [ Llama 2 Paper Dataset ](https://llamahub.ai/l/llama_datasets-llama2_paper)\n  * [ Uber/Lyft 2021 10K Filings Dataset ](https://llamahub.ai/l/llama_datasets-10k-uber_2021)\n  * [ Mini Truthful QA Dataset (Arize AI) ](https://llamahub.ai/l/llama_datasets-mini_truthfulqa)\n  * [ Mini Squad V2 Dataset (Arize AI) ](https://llamahub.ai/l/llama_datasets-mini_squadv2)\n  * [ Origin of COVID-19 ](https://llamahub.ai/l/llama_datasets-origin_of_covid19)\n  * [ LLM Survey Paper Dataset ](https://llamahub.ai/l/llama_datasets-eval_llm_survey_paper)\n\nExample Llama Dataset page\n\n#  Example Walkthrough\n\nLet\u2019s walk through the different steps of using/contributing a Llama Dataset.\n\n##  1\\. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3965, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d80b8c2-2c42-4132-b57e-5ef41908869c": {"__data__": {"id_": "8d80b8c2-2c42-4132-b57e-5ef41908869c", "embedding": null, "metadata": {"filename": "introducing-llama-datasets-aadb9994ad9e.md", "extension": ".md", "title": "Introducing Llama Datasets \ud83e\udd99\ud83d\udcdd", "date": "Dec 4, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llama-datasets-aadb9994ad9e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fe69c12-a585-49f8-b1c8-90f65d411cc5", "node_type": "4", "metadata": {"filename": "introducing-llama-datasets-aadb9994ad9e.md", "extension": ".md", "title": "Introducing Llama Datasets \ud83e\udd99\ud83d\udcdd", "date": "Dec 4, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llama-datasets-aadb9994ad9e"}, "hash": "ba4720e4ec89447f3ca5f979a878dfac87e2814803fe348dfa2931393f879f3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "448dd6b3-e0bb-4d21-a69d-e73ec33d4532", "node_type": "1", "metadata": {"filename": "introducing-llama-datasets-aadb9994ad9e.md", "extension": ".md", "title": "Introducing Llama Datasets \ud83e\udd99\ud83d\udcdd", "date": "Dec 4, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llama-datasets-aadb9994ad9e"}, "hash": "763821731b50dcd3aaafcf42744df123c01fa25563ca91022b5d9accd567af20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "231b47c6-6612-4685-b050-7df9309db9c3", "node_type": "1", "metadata": {}, "hash": "d84238106bafb03c4a28d7315e73a932c146781d1be7a78c8c83a5d1985fd77e", "class_name": "RelatedNodeInfo"}}, "text": "Downloading and Using a Llama Dataset\n\n[ Follow the full notebook here. ](https://github.com/run-\nllama/llama_index/blob/main/docs/examples/llama_dataset/downloading_llama_datasets.ipynb)\n\nDownloading a dataset is simple, do the following command (here we download\nPaul Graham).\n\n    \n    \n    from llama_index.llama_dataset import download_llama_dataset\n    \n    # download and install dependencies\n    rag_dataset, documents = download_llama_dataset(\n        \"PaulGrahamEssayDataset\", \"./paul_graham\"\n    )\n\nThis downloads a ` rag_dataset ` which contains the QA pairs (+ reference\ncontext), and ` documents ` which is the source document corpus.\n\nLet\u2019s inspect the ` rag_dataset ` with ` to_pandas() ` :\n\nSample rows from `rag_dataset`\n\nGenerating predictions over the RAG dataset is straightforward. You can easily\nplug in any query engine into ` amake_predictions_with ` :\n\n    \n    \n    from llama_index import VectorStoreIndex\n    \n    # a basic RAG pipeline, uses service context defaults\n    index = VectorStoreIndex.from_documents(documents=documents)\n    query_engine = index.as_query_engine()\n    \n    # generate prediction dataset\n    prediction_dataset = await rag_dataset.amake_predictions_with(\n        query_engine=query_engine, show_progress=True\n    )\n\nThe ` prediction_dataset ` is a ` RagPredictionDataset ` object that looks\nlike the following:\n\nPrediction Dataset\n\nGiven the ` rag_dataset ` and ` prediction_dataset ` , you can use our\nevaluation modules to measure performance across a variety of metrics (e.g.\nfaithfulness, correctness, relevancy).\n\n    \n    \n    for example, prediction in tqdm.tqdm(\n        zip(rag_dataset.examples, prediction_dataset.predictions)\n    ):\n        correctness_result = judges[\"correctness\"].evaluate(\n            query=example.query,\n            response=prediction.response,\n            reference=example.reference_answer,\n        )\n\nTo eliminate the boilerplate of writing all these evaluation modules, we\u2019ve\nalso provided a LlamaPack that will do all this for you!\n\n    \n    \n    from llama_index.llama_pack import download_llama_pack\n    \n    RagEvaluatorPack = download_llama_pack(\"RagEvaluatorPack\", \"./pack\")\n    rag_evaluator = RagEvaluatorPack(\n        query_engine=query_engine, rag_dataset=rag_dataset\n    )\n    benchmark_df = await rag_evaluator.arun()\n\n##  2\\. Generating a Llama Dataset\n\n[ Follow the full notebook here. ](https://github.com/run-\nllama/llama_index/blob/main/docs/examples/llama_dataset/labelled-rag-\ndatasets.ipynb)\n\nYou can use our ` LabelledRagDataExample ` and ` LabelledRagDataset `\nabstractions to create your own dataset.\n\nHere\u2019s an example of adding an example manually.\n\n    \n    \n    from llama_index.llama_dataset import (\n        LabelledRagDataExample,\n        CreatedByType,\n        CreatedBy,\n    )\n    \n    # constructing a LabelledRagDataExample\n    query = \"This is a test query, is it not?\"\n    query_by = CreatedBy(type=CreatedByType.AI, model_name=\"gpt-4\")\n    reference_answer = \"Yes it is.\"\n    reference_answer_by = CreatedBy(type=CreatedByType.HUMAN)\n    reference_contexts = [\"This is a sample context\"]\n    \n    rag_example = LabelledRagDataExample(\n        query=query,\n        query_by=query_by,\n        reference_contexts=reference_contexts,\n        reference_answer=reference_answer,\n        reference_answer_by=reference_answer_by,\n    )\n    \n    \n    from llama_index.llama_dataset.rag import LabelledRagDataset\n    \n    rag_dataset = LabelledRagDataset(examples=[rag_example, rag_example_2])\n\nYou can also synthetically generate a dataset over any document corpus with\nGPT-4:\n\n    \n    \n    # generate questions against chunks\n    from llama_index.llama_dataset.generator import RagDatasetGenerator\n    from llama_index.llms import OpenAI\n    from llama_index import ServiceContext\n    \n    # set context for llm provider\n    gpt_4_context = ServiceContext.from_defaults(\n        llm=OpenAI(model=\"gpt-4\", temperature=0.3)\n    )\n    \n    # instantiate a DatasetGenerator\n    dataset_generator = RagDatasetGenerator.from_documents(\n        documents,\n        service_context=gpt_4_context,\n        num_questions_per_chunk=2,  # set the number of questions per nodes\n        show_progress=True,\n    )\n\n##  3\\. Contributing a Llama Dataset\n\nWe\u2019ve provided a [ ready-made submission notebook template here\n](https://github.com/run-\nllama/llama_index/blob/main/docs/examples/llama_dataset/ragdataset_submission_template.ipynb)\n\u2014 just fill in the blanks with your dataset!\n\nIf you\u2019re interested in contributing a dataset, we\u2019d love to feature it! You\njust need to follow these steps:\n\n  1. **Create the dataset:** To create a ` LabelledRagDataset ` , you can create it from scratch either manually or with synthetically generated examples, or create it from an existing dataset. \n  2. **Generate a baseline evaluation dataset:** Benchmark a basic top-k RAG pipeline over your dataset, and report the numbers. This will serve as a point of reference for others. ", "mimetype": "text/plain", "start_char_idx": 3965, "end_char_idx": 8944, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "231b47c6-6612-4685-b050-7df9309db9c3": {"__data__": {"id_": "231b47c6-6612-4685-b050-7df9309db9c3", "embedding": null, "metadata": {"filename": "introducing-llama-datasets-aadb9994ad9e.md", "extension": ".md", "title": "Introducing Llama Datasets \ud83e\udd99\ud83d\udcdd", "date": "Dec 4, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llama-datasets-aadb9994ad9e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fe69c12-a585-49f8-b1c8-90f65d411cc5", "node_type": "4", "metadata": {"filename": "introducing-llama-datasets-aadb9994ad9e.md", "extension": ".md", "title": "Introducing Llama Datasets \ud83e\udd99\ud83d\udcdd", "date": "Dec 4, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llama-datasets-aadb9994ad9e"}, "hash": "ba4720e4ec89447f3ca5f979a878dfac87e2814803fe348dfa2931393f879f3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d80b8c2-2c42-4132-b57e-5ef41908869c", "node_type": "1", "metadata": {"filename": "introducing-llama-datasets-aadb9994ad9e.md", "extension": ".md", "title": "Introducing Llama Datasets \ud83e\udd99\ud83d\udcdd", "date": "Dec 4, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llama-datasets-aadb9994ad9e"}, "hash": "cc7f87c4ab54c3095d381697e6998b7370357f242218e72324c86d72e108f555", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ae08cc1-28db-4050-84e3-7f12a0acf85f", "node_type": "1", "metadata": {}, "hash": "5ff61675d4da47142e904009ab570a8cf0bda2008f7520678b82b53591914090", "class_name": "RelatedNodeInfo"}}, "text": "You can use the ` RagEvaluatorPack ` for this purpose. \n  3. **Prepare the dataset card (** ` **card.json** ` **) and** ` **README.md** ` **:** These will be shown on the LlamaHub page for this dataset. If you want to auto-generate this given some inputs, check out our ` [ LlamaDatasetMetadata ](https://llamahub.ai/l/llama_packs-llama_dataset_metadata) ` LlamaPack. \n  4. Submit a PR into ` llama-hub ` to register the ` LlamaDataset ` . \n  5. Submit a PR into ` llama-datasets ` to upload the ` LlamaDataset ` and its source files. \n\nYou can follow all of these steps in our [ notebook template above\n](https://github.com/run-\nllama/llama_index/blob/main/docs/examples/llama_dataset/ragdataset_submission_template.ipynb)\n\u2014 simply substitute your own data.\n\n#  Conclusion\n\nWe\u2019d love for you to check out our datasets and let us know your feedback!\n", "mimetype": "text/plain", "start_char_idx": 8944, "end_char_idx": 9794, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ae08cc1-28db-4050-84e3-7f12a0acf85f": {"__data__": {"id_": "8ae08cc1-28db-4050-84e3-7f12a0acf85f", "embedding": null, "metadata": {"filename": "introducing-llama-datasets-aadb9994ad9e.md", "extension": ".md", "title": "Introducing Llama Datasets \ud83e\udd99\ud83d\udcdd", "date": "Dec 4, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llama-datasets-aadb9994ad9e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fe69c12-a585-49f8-b1c8-90f65d411cc5", "node_type": "4", "metadata": {"filename": "introducing-llama-datasets-aadb9994ad9e.md", "extension": ".md", "title": "Introducing Llama Datasets \ud83e\udd99\ud83d\udcdd", "date": "Dec 4, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llama-datasets-aadb9994ad9e"}, "hash": "ba4720e4ec89447f3ca5f979a878dfac87e2814803fe348dfa2931393f879f3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "231b47c6-6612-4685-b050-7df9309db9c3", "node_type": "1", "metadata": {"filename": "introducing-llama-datasets-aadb9994ad9e.md", "extension": ".md", "title": "Introducing Llama Datasets \ud83e\udd99\ud83d\udcdd", "date": "Dec 4, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llama-datasets-aadb9994ad9e"}, "hash": "f96ec05f039d7db1bdcd58e38ec8f4379282dd377da30836f5b62d8ee1ee6d1d", "class_name": "RelatedNodeInfo"}}, "text": "We\u2019d love your contributions as well.\n\n##  Resources\n\nHere are the resources mentioned in the blog post.\n\n  * [ Llama Datasets on LlamaHub ](https://llamahub.ai/) (make sure to select \u201cLlama Datasets\u201d from the dropdown) \n  * [ Downloading a Llama Dataset Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/llama_dataset/downloading_llama_datasets.ipynb)\n  * [ Creating a Llama Dataset Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/llama_dataset/labelled-rag-datasets.ipynb)\n  * [ Contributing a Llama Dataset Notebook Template ](https://github.com/run-llama/llama_index/blob/main/docs/examples/llama_dataset/ragdataset_submission_template.ipynb)\n  * [ README on Contributing a Llama Dataset ](https://github.com/run-llama/llama-hub#how-to-add-a-llama-dataset)\n\n", "mimetype": "text/plain", "start_char_idx": 9794, "end_char_idx": 10610, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2d1d84e-259d-4a30-9999-26c83d3336c4": {"__data__": {"id_": "c2d1d84e-259d-4a30-9999-26c83d3336c4", "embedding": null, "metadata": {"filename": "openai-cookbook-evaluating-rag-systems-fe393c61fb93.md", "extension": ".md", "title": "OpenAI Cookbook: Evaluating RAG systems", "date": "Nov 28, 2023", "url": "https://www.llamaindex.ai/blog/openai-cookbook-evaluating-rag-systems-fe393c61fb93"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9020d3c0-ea98-458e-b093-bfb19ba950a5", "node_type": "4", "metadata": {"filename": "openai-cookbook-evaluating-rag-systems-fe393c61fb93.md", "extension": ".md", "title": "OpenAI Cookbook: Evaluating RAG systems", "date": "Nov 28, 2023", "url": "https://www.llamaindex.ai/blog/openai-cookbook-evaluating-rag-systems-fe393c61fb93"}, "hash": "6eaa3f402c8e9051d8e42ce48fa2c63862923a37d1dfdaf0afd81c73c575f548", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b40da8aa-59cb-4a86-b053-bae827b6e345", "node_type": "1", "metadata": {}, "hash": "b0b3ca4c1be0e664e37efc731be032f7c3b0d7b758881033fb2975da300e6686", "class_name": "RelatedNodeInfo"}}, "text": "We\u2019re excited to unveil our [ OpenAI Cookbook\n](https://github.com/openai/openai-\ncookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb) , a\nguide to evaluating Retrieval-Augmented Generation (RAG) systems using\nLlamaIndex. We hope you\u2019ll find it useful in enhancing the effectiveness of\nyour RAG systems, and we\u2019re thrilled to share it with you.\n\nThe OpenAI Cookbook has three sections:\n\n  1. **Understanding Retrieval-Augmented Generation (RAG):** provides a detailed overview of RAG systems, including the various stages involved in building the RAG system. \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 584, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b40da8aa-59cb-4a86-b053-bae827b6e345": {"__data__": {"id_": "b40da8aa-59cb-4a86-b053-bae827b6e345", "embedding": null, "metadata": {"filename": "openai-cookbook-evaluating-rag-systems-fe393c61fb93.md", "extension": ".md", "title": "OpenAI Cookbook: Evaluating RAG systems", "date": "Nov 28, 2023", "url": "https://www.llamaindex.ai/blog/openai-cookbook-evaluating-rag-systems-fe393c61fb93"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9020d3c0-ea98-458e-b093-bfb19ba950a5", "node_type": "4", "metadata": {"filename": "openai-cookbook-evaluating-rag-systems-fe393c61fb93.md", "extension": ".md", "title": "OpenAI Cookbook: Evaluating RAG systems", "date": "Nov 28, 2023", "url": "https://www.llamaindex.ai/blog/openai-cookbook-evaluating-rag-systems-fe393c61fb93"}, "hash": "6eaa3f402c8e9051d8e42ce48fa2c63862923a37d1dfdaf0afd81c73c575f548", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2d1d84e-259d-4a30-9999-26c83d3336c4", "node_type": "1", "metadata": {"filename": "openai-cookbook-evaluating-rag-systems-fe393c61fb93.md", "extension": ".md", "title": "OpenAI Cookbook: Evaluating RAG systems", "date": "Nov 28, 2023", "url": "https://www.llamaindex.ai/blog/openai-cookbook-evaluating-rag-systems-fe393c61fb93"}, "hash": "bd9a3d5321231254d8f61a64b6dcd79dc33223782e784fb8f30a0deb3f39202e", "class_name": "RelatedNodeInfo"}}, "text": "2. **Building RAG with LlamaIndex:** Here, we dive into the practical aspects, demonstrating how to construct a RAG system using LlamaIndex, specifically applied to Paul Graham\u2019s essay, utilizing the ` VectorStoreIndex ` . \n  3. **Evaluating RAG with LlamaIndex:** The final section focuses on assessing the RAG system\u2019s performance in two critical areas: _the Retrieval System_ and _Response Generation._\n\nWe use our unique synthetic dataset generation method, `\ngenerate_question_context_pairs ` to conduct thorough evaluations in these\nareas.\n\nOur goal with this [ cookbook ](https://github.com/openai/openai-\ncookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb) is\nto provide the community with an essential resource for effectively evaluating\nand enhancing RAG systems developed using LlamaIndex.\n\nJoin us in exploring the depths of RAG system evaluation and discover how to\nleverage the full potential of your RAG implementations with LlamaIndex.\n\n> Keep building with LlamaIndex!\n\n", "mimetype": "text/plain", "start_char_idx": 584, "end_char_idx": 1594, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d82a1ae-9669-4d49-8154-fa057703200d": {"__data__": {"id_": "0d82a1ae-9669-4d49-8154-fa057703200d", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-11-28-a31be430a786.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201328", "date": "Nov 28, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-28-a31be430a786"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d682bfd-2112-4f76-b0c4-1d3fc55aa6d8", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-11-28-a31be430a786.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201328", "date": "Nov 28, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-28-a31be430a786"}, "hash": "5ed8305da8b476d50a8c111307b7e9131abadbc1ff5176ae08a3c478e28b77ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a7e9471-e116-443c-bd82-f85609fc5639", "node_type": "1", "metadata": {}, "hash": "f0b6b352eb01ee16bc5aa72a58ddd40db8b47f28408ff8354c6b6c2f1f79f58f", "class_name": "RelatedNodeInfo"}}, "text": "Hello to Our Llama Community!\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 31, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a7e9471-e116-443c-bd82-f85609fc5639": {"__data__": {"id_": "8a7e9471-e116-443c-bd82-f85609fc5639", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-11-28-a31be430a786.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201328", "date": "Nov 28, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-28-a31be430a786"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d682bfd-2112-4f76-b0c4-1d3fc55aa6d8", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-11-28-a31be430a786.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201328", "date": "Nov 28, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-28-a31be430a786"}, "hash": "5ed8305da8b476d50a8c111307b7e9131abadbc1ff5176ae08a3c478e28b77ac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d82a1ae-9669-4d49-8154-fa057703200d", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-11-28-a31be430a786.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201328", "date": "Nov 28, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-28-a31be430a786"}, "hash": "91f7e341032219bdc5490acaf39dd1bc9ec72ebdc08483da4cb3030ff9aa4746", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72c08c84-aef1-4c33-aa1d-ce9415dd09fd", "node_type": "1", "metadata": {}, "hash": "149c5ac97afe0ee1d24e5ec2e1829443d08e9ff0026941d90504f73080e05d7b", "class_name": "RelatedNodeInfo"}}, "text": "Hope your Thanksgiving was delightful! We\u2019re thrilled to announce a major\nmilestone: LlamaIndex has hit **1 million monthly downloads** on our Python\npackage! A big thank you to everyone for your support, feedback, and\ncontributions that have fueled our journey. ", "mimetype": "text/plain", "start_char_idx": 31, "end_char_idx": 294, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72c08c84-aef1-4c33-aa1d-ce9415dd09fd": {"__data__": {"id_": "72c08c84-aef1-4c33-aa1d-ce9415dd09fd", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-11-28-a31be430a786.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201328", "date": "Nov 28, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-28-a31be430a786"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d682bfd-2112-4f76-b0c4-1d3fc55aa6d8", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-11-28-a31be430a786.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201328", "date": "Nov 28, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-28-a31be430a786"}, "hash": "5ed8305da8b476d50a8c111307b7e9131abadbc1ff5176ae08a3c478e28b77ac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a7e9471-e116-443c-bd82-f85609fc5639", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-11-28-a31be430a786.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201328", "date": "Nov 28, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-28-a31be430a786"}, "hash": "f6196cb264dc2ee52bc812f855fcfb80ecdc7abffa4c1b1866c06735bcb9b6f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4eb55d3-e7d9-4439-8126-af7e0488f989", "node_type": "1", "metadata": {}, "hash": "ab3c35539daca0346dd1b258fc45d9bd1bf871f86ff320a36e43c21a78a8ec78", "class_name": "RelatedNodeInfo"}}, "text": "Stay tuned for more exciting new\nproducts and features coming your way.\n\nIf you have a fascinating project or video you\u2019d like to share, we\u2019d love to\nsee it! Feel free to send it to us at [ news@llamaindex.ai\n](mailto:news@llamaindex.ai) . And remember to subscribe to our newsletter on\nour [ website ](https://www.llamaindex.ai/) to stay in the loop. We can\u2019t wait\nto connect with you there!\n\n**First, the highlights:**\n\n  * **Launched Llama Packs:** Prepackaged modules and templates streamlining LLM app development. [ Blog ](/introducing-llama-packs-e14f453b913a) , [ Tweet ](https://x.com/llama_index/status/1727365908119917016?s=20) . \n  * **RAGs Project:** Build your own retrieval augmented generation app just by talking. [ Project ](https://github.com/run-llama/rags) , [ Tweet ](https://x.com/llama_index/status/1727502719706132516?s=20) . \n  * **Introduced FuzzyCitationEnginePack:** Precisely aligns LLM responses to source sentences via fuzzy matching, available as an easy-to-implement LlamaPack. [ Docs ](https://t.co/xiGJCjNCfc) , [ Tweet ](https://x.com/llama_index/status/1729182899470311541?s=20) . \n\nComing up this week: on Thursday 30th our very own Yi Ding will be giving a\nworkshop on Building an Open Source RAG Application Using LlamaIndex. [ Sign\nup for free here ](https://www.datastax.com/workshops/building-an-open-source-\nrag-application-using-\nllamaindex?utm_medium=social_organic&utm_source=linkedin&utm_campaign=workshop&utm_content=llamaindex-\nchannels)\n\n**Feature Releases and Enhancements:**\n\n  * We introduced Llama Packs , a series of prepackaged modules and templates designed to jumpstart your LLM app development. These packs eliminate the need for assembling and tuning custom components for each use case. [ Blog ](/introducing-llama-packs-e14f453b913a) , [ Tweet ](https://x.com/llama_index/status/1727365908119917016?s=20) . \n  * We have introduced the RAGs project for programming AI agents using natural language, inspired by the interest in OpenAI\u2019s GPTs. Our approach involves a \u2018Builder Agent\u2019 that crafts a \u2018Custom Agent\u2019 tailored to specific tasks, incorporating tools for system prompt setting, data loading, model configuration, and RAG parameter adjustments. [ Project ](https://github.com/run-llama/rags) , [ Tweet ](https://x.com/llama_index/status/1727502719706132516?s=20) . \n  * We introduced a LlamaPack that enables the setup of a fully local RAG pipeline with just one line of code. This pack includes Zephyr-7b as the LLM and bge-base as the embedding model. [ Docs ](https://t.co/IGIyPl5iE2) , [ Tweet ](https://x.com/llama_index/status/1728931304211951944?s=20) . \n  * We introduced ` **FuzzyCitationEnginePack** ` that maps parts of an LLM-generated response from a RAG pipeline to the exact sentences in the source context using fuzzy matching. This innovation elevates citation accuracy and is now available as a LlamaPack for easy implementation with just one line of code. ", "mimetype": "text/plain", "start_char_idx": 294, "end_char_idx": 3239, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4eb55d3-e7d9-4439-8126-af7e0488f989": {"__data__": {"id_": "b4eb55d3-e7d9-4439-8126-af7e0488f989", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-11-28-a31be430a786.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201328", "date": "Nov 28, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-28-a31be430a786"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d682bfd-2112-4f76-b0c4-1d3fc55aa6d8", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-11-28-a31be430a786.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201328", "date": "Nov 28, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-28-a31be430a786"}, "hash": "5ed8305da8b476d50a8c111307b7e9131abadbc1ff5176ae08a3c478e28b77ac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72c08c84-aef1-4c33-aa1d-ce9415dd09fd", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-11-28-a31be430a786.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201328", "date": "Nov 28, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-28-a31be430a786"}, "hash": "5f12da0178c77e1780bf24b834c1b120c0a9a34c8cee332b0cd887023bec9c81", "class_name": "RelatedNodeInfo"}}, "text": "[ Docs ](https://t.co/xiGJCjNCfc) , [ Tweet ](https://x.com/llama_index/status/1729182899470311541?s=20) . \n\n**Demo:**\n\n  * AI-Einblick Prompt is a JupyterLab extension that uses OpenAI\u2019s GPT 3.5 and 4, powered by LlamaIndex, to assist in data science workflows by generating, modifying, and fixing code, creating charts, and building models, seamlessly integrated within the JupyterLab environment. [ Project ](https://pypi.org/project/ai-einblick-prompt/) , [ Tweet ](https://x.com/llama_index/status/1727492316242583571?s=20) . \n  * [ **Ranya Khemiri** ](https://twitter.com/khemiri_ranya) uploaded a research paper to RAGs to help with a school assignment and observed results better than file retrieval with ChatGPT. [ Blog ](https://raniaprojects.wixsite.com/raniakhemiri/post/i-set-up-a-rag-pipeline-to-help-with-a-school-assignment) , [ Tweet ](https://x.com/llama_index/status/1728581037863961019?s=20) . \n\n**Integrations:**\n\n  * [ CogniSwitch ](https://twitter.com/CogniSwitch?lang=en) introduced a fusion RAG approach combining vectors, knowledge graphs, and rules for streamlined ingestion and retrieval. This allows for flexible usage, either as an independent query engine or as an integrated tool within an agent with LlamaIndex. [ Docs ](https://docs.llamaindex.ai/en/stable/examples/query_engine/cogniswitch_query_engine.html) , [ Tweet ](https://x.com/llama_index/status/1727127794289959396?s=20) . \n\n**Guides:**\n\n  * [ Guide ](/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d) on shipping your RAG application to production with create-llama. \n  * [ Guide ](https://t.co/ija26e25PR) on multi-modal models: Our comparison tables detail differences in image reasoning, embeddings, and synthesis capabilities. We also provide insights into multi-modal support for vector stores, focusing on image support with future audio/video integration. \n  * [ Guide ](https://gradient.ai/blog/rag-101-for-enterprise) on getting started with AI in your enterprise from Gradient AI. This introductory guide explains retrieval-augmented generation (RAG), its relevance for businesses, and how to balance fine-tuning, prompt engineering, and RAG for optimal results, along with strategies for RAG optimization. \n\n**Tutorials:**\n\n  * [ Ankush k Singal ](https://medium.com/@andysingal) made a [ tutorial ](https://t.co/qabwKVuPSJ) on Document Extraction with Zephyr 7b LLM using LlamaIndex. \n  * [ Wenqi Glantz ](https://twitter.com/wenqi_glantz) made a [ tutorial ](https://t.co/93XM9BnaCF) on Automating Hyperparameter Tuning with LlamaIndex. \n  * [ Tonic AI ](https://twitter.com/tonicfakedata) [ analysis ](https://www.tonic.ai/blog/rag-evaluation-series-validating-rag-performance-openai-vs-llamaindex) on OpenAI Assistant API vs LlamaIndex RAG. \n  * [ **Pradip Nichite** ](https://twitter.com/pradip_nichite) made ****a ****video [ tutorial ](https://t.co/v4TsPF9xmt) on using RAGs which provides easy-to-follow instructions on how to build or customize a chatbot capable of advanced summarization over your data, making it accessible even for non-developers. \n\n**Webinars:**\n\n  * Jerry Liu presented a [ webinar ](https://arize.com/resource/advanced-llm-evals/) with Arize AI on LLM Retrieval Evaluations. \n\n", "mimetype": "text/plain", "start_char_idx": 3239, "end_char_idx": 6500, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3486ff03-0424-440a-a08c-02f49b68da72": {"__data__": {"id_": "3486ff03-0424-440a-a08c-02f49b68da72", "embedding": null, "metadata": {"filename": "multimodal-rag-building-ainimal-go-fecf8404ed97.md", "extension": ".md", "title": "Multimodal RAG: Building \u2018AInimal Go!\u2019, a Pok\u00e9mon Go-Inspired App with ResNet, Cohere and Llamaindex", "date": "Nov 27, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-building-ainimal-go-fecf8404ed97"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f2ac32cb-8853-4f0b-bf40-3a908cd68c45", "node_type": "4", "metadata": {"filename": "multimodal-rag-building-ainimal-go-fecf8404ed97.md", "extension": ".md", "title": "Multimodal RAG: Building \u2018AInimal Go!\u2019, a Pok\u00e9mon Go-Inspired App with ResNet, Cohere and Llamaindex", "date": "Nov 27, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-building-ainimal-go-fecf8404ed97"}, "hash": "cbec53b840718fc009a5121f08266eaedfbed6820aa69fb348ca8ace2f105420", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2d7354d-d242-421e-9294-b9be74b90c74", "node_type": "1", "metadata": {}, "hash": "7824d8874fa24b9200ba6991591053eca33c40726fbfbe3f2a0fca095bb99c98", "class_name": "RelatedNodeInfo"}}, "text": "In the current landscape where GPT-4 Vision (GPT-4V) use cases are everywhere,\nI wanted to explore an alternative approach: pairing deep learning vision\nmodels with large language models (LLMs). My latest project, \u2018AInimal Go!\u2019, is\nan attempt to showcase how a specialized vision model like ResNet18 can\nseamlessly integrate with an LLM, using LlamaIndex as the orchestration layer\nand Wikipedia articles as the knowledge base.\n\n#  Project Overview\n\n\u2018AInimal Go!\u2019 is an interactive app that allows users to either capture or\nupload images of animals. Upon uploading an image, the ResNet18 model swiftly\nclassifies the animal. Following this, the Cohere LLM API, adeptly\norchestrated by LlamaIndex, takes over. It roleplays as the identified animal,\nenabling users to engage in unique conversations about and with the animal.\nThe dialogue is informed and enriched by a knowledge base of nearly 200\nWikipedia articles, providing accurate and relevant responses to user queries.\n\n#  Why Not GPT-4V?\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 997, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2d7354d-d242-421e-9294-b9be74b90c74": {"__data__": {"id_": "b2d7354d-d242-421e-9294-b9be74b90c74", "embedding": null, "metadata": {"filename": "multimodal-rag-building-ainimal-go-fecf8404ed97.md", "extension": ".md", "title": "Multimodal RAG: Building \u2018AInimal Go!\u2019, a Pok\u00e9mon Go-Inspired App with ResNet, Cohere and Llamaindex", "date": "Nov 27, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-building-ainimal-go-fecf8404ed97"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f2ac32cb-8853-4f0b-bf40-3a908cd68c45", "node_type": "4", "metadata": {"filename": "multimodal-rag-building-ainimal-go-fecf8404ed97.md", "extension": ".md", "title": "Multimodal RAG: Building \u2018AInimal Go!\u2019, a Pok\u00e9mon Go-Inspired App with ResNet, Cohere and Llamaindex", "date": "Nov 27, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-building-ainimal-go-fecf8404ed97"}, "hash": "cbec53b840718fc009a5121f08266eaedfbed6820aa69fb348ca8ace2f105420", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3486ff03-0424-440a-a08c-02f49b68da72", "node_type": "1", "metadata": {"filename": "multimodal-rag-building-ainimal-go-fecf8404ed97.md", "extension": ".md", "title": "Multimodal RAG: Building \u2018AInimal Go!\u2019, a Pok\u00e9mon Go-Inspired App with ResNet, Cohere and Llamaindex", "date": "Nov 27, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-building-ainimal-go-fecf8404ed97"}, "hash": "b00195c282cffc9f6d270e73cda07c9594f22b2b1e1aa140f433d753f3366a31", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94c2d12c-4bad-4e32-9294-6f3f24feda9d", "node_type": "1", "metadata": {}, "hash": "a28e44f73b15aea995e8ea98cd421fe921de02ce970519e155fe6c09ef4cbcec", "class_name": "RelatedNodeInfo"}}, "text": "Amidst the surge in GPT-4 Vision use cases, I wanted to explore an efficient\nyet powerful alternative. It is important to choose the right tool for the job\n\u2014 using GPT-4V for every multimodal task can be overkill, like using a\nsledgehammer to crack a nut. My approach was to harness the agility and\nprecision of ResNet18 for animal identification. This method not only\n**curtails costs** but also underscores the adaptability of specialized models\nin multi-modal realms.\n\n#  Tools and Tech\n\n  * **ResNet for Animal Detection:** A blazing-fast implementation to identify animals in images, utilizing the ImageNet classification scheme. \n  * **Cohere LLM:** For generating engaging, informative conversations based on the identified animal. \n  * **LlamaIndex:** Seamlessly orchestrates the workflow, managing the retrieval of information from pre-indexed Wikipedia articles about animals. \n  * **Streamlit for UI**\n\nGif showing the demo in action\n\n#  Deep Dive into app.py\n\nThe heart of \u2018AInimal Go!\u2019 lies in the ` app.py ` script, where ResNet, Cohere\nLLM, and LlamaIndex seamlessly come together. Now, let\u2019s delve into the key\naspects of the code:\n\n##  1\\. Image Capture/Upload\n\nIn \u2018AInimal Go!\u2019, the flow begins with the user uploading an image or\ncapturing one using their device\u2019s camera. This is a crucial step as it sets\nthe stage for the subsequent interaction with the identified animal.\n\nThe code snippet below illustrates how Streamlit is used to create a UI for\nimage upload and capture. It offers two options: a file uploader for selecting\nan image file and a camera input for real-time capture. Once an image is\nprovided through either method, it\u2019s converted into a byte stream ( ` BytesIO\n` ) for processing. This streamlining ensures a seamless user experience,\nwhether the image is uploaded from a gallery or captured on the spot.\n\n    \n    \n    # Image upload section.\n        image_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"], key=\"uploaded_image\", on_change=on_image_upload)\n        \n        col1, col2, col3 = st.columns([1, 2, 1])\n        with col2:  # Camera input will be in the middle column\n            camera_image = st.camera_input(\"Take a picture\", on_change=on_image_upload)\n            \n        \n        # Determine the source of the image (upload or camera)\n        if image_file is not None:\n            image_data = BytesIO(image_file.getvalue())\n        elif camera_image is not None:\n            image_data = BytesIO(camera_image.getvalue())\n        else:\n            image_data = None\n        \n        if image_data:\n            # Display the uploaded image at a standard width.\n            st.session_state['assistant_avatar'] = image_data\n            st.image(image_data, caption='Uploaded Image.', width=200)\n\n##  2\\. Initializing ResNet for Image Classification\n\nOnce the user uploads or captures an image, the next critical step is\nidentifying the animal within it. This is where ResNet18, a robust deep\nlearning model for image classification, comes into play.\n\nThe function ` load_model_and_labels ` performs two key tasks:\n\n  * **Loading Animal Labels:** It starts by loading a subset of ImageNet labels specific to animals. These labels are stored in a dictionary, mapping class IDs to their corresponding animal names. This mapping is essential for interpreting the output of the ResNet model. \n  * **Initializing ResNet18:** The function then initializes the feature extractor and the ResNet18 model. The feature extractor preprocesses the images to the format required by ResNet18, while the model itself is responsible for the actual classification task. \n\n    \n    \n    def load_model_and_labels():\n        # Load animal labels as a dictionary\n        animal_labels_dict = {}\n        with open('imagenet_animal_labels_subset.txt', 'r') as file:\n            for line in file:\n                parts = line.strip().split(':')\n                class_id = int(parts[0].strip())\n                label_name = parts[1].strip().strip(\"'\")\n                animal_labels_dict[class_id] = label_name\n    \n        # Initialize feature extractor and model\n        feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/resnet-18\")\n        model = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-18\")\n    \n        return feature_extractor, model, animal_labels_dict\n    \n    feature_extractor, model, animal_labels_dict = load_model_and_labels()\n\nBy integrating ResNet18 in this manner, \u2018AInimal Go!\u2019 leverages its speed and\naccuracy for the crucial task of identifying the animal in the user\u2019s image.\nThis sets the foundation for the engaging and informative conversations that\nfollow.\n\n##  3\\. Animal Detection with ResNet18\n\nAfter initializing ResNet18, the next step is to use it for detecting the\nanimal in the uploaded image. The function ` get_image_caption ` handles this\ntask.\n\n  * **Image Preprocessing:** The uploaded image is first opened and then preprocessed using the feature extractor initialized earlier. This preprocessing adapts the image to the format required by ResNet18. \n  * **Animal Detection:** The preprocessed image is then fed into ResNet18, which predicts the class of the image. The logits (the model\u2019s raw output) are processed to find the class with the highest probability, which corresponds to the predicted animal. \n  * **Retrieving the Animal Name:** The predicted class ID is mapped to the corresponding animal name using the label dictionary created earlier. This name is then displayed to the user. \n\n    \n    \n    def get_image_caption(image_data):\n        image = Image.open(image_data)\n        inputs = feature_extractor(images=image, return_tensors=\"pt\")\n    \n        with torch.no_grad():\n            logits = model(**inputs).logits\n    \n        predicted_label_id = logits.argmax(-1).item()\n        predicted_label_name = model.config.id2label[predicted_label_id]\n        st.write(predicted_label_name)\n        # Return the predicted animal name\n        return predicted_label_name, predicted_label_id\n\n##  4\\. Validating Animal Presence in Images\n\nTo ensure that the conversation in \u2018AInimal Go!\u2019 is relevant and engaging,\nit\u2019s crucial to verify that the uploaded image indeed depicts an animal. This\nverification is handled by the ` is_animal ` function.\n\n    \n    \n    def is_animal(predicted_label_id):\n        # Check if the predicted label ID is within the animal classes range\n        return 0 &lt;= predicted_label_id &lt;= 398\n\nThe function checks if the predicted label ID from ResNet18 falls within the\nrange of animal classes (0 to 398 in ImageNet\u2019s classification). This simple\nyet effective check is essential for maintaining the app\u2019s focus on animal\ninteractions.\n\nFurther in the script, this function is utilized to validate the detected\nobject:\n\n    \n    \n    if not (is_animal(label_id)):\n        st.error(\"Please upload image of an animal!\")\n        st.stop()\n\nIf the uploaded image does not depict an animal, the app prompts the user to\nupload an appropriate image, ensuring that the conversation remains on track.\n\n##  5\\. Initializing LLM\n\nThe ` init_llm ` function initializes the Cohere LLM along with the necessary\ncontexts for storage and service (specify llm and embed_model). It also loads\nthe pre-indexed Wikipedia articles for about 200 animals. The function sets up\nthe environment in which the LLM operates, preparing it for generating\nresponses.\n\n    \n    \n    def init_llm(api_key):\n        llm = Cohere(model=\"command\", api_key=st.secrets['COHERE_API_TOKEN'])\n    \n        service_context = ServiceContext.from_defaults(llm=llm, embed_model=\"local\")\n        storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n        index = load_index_from_storage(storage_context, index_id=\"index\", service_context=service_context)\n        \n        return llm, service_context, storage_context, index\n\nThis function is critical for setting up the LLM, ensuring that all necessary\ncomponents are in place for the chat functionality.\n\n##  6\\. ", "mimetype": "text/plain", "start_char_idx": 997, "end_char_idx": 9012, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94c2d12c-4bad-4e32-9294-6f3f24feda9d": {"__data__": {"id_": "94c2d12c-4bad-4e32-9294-6f3f24feda9d", "embedding": null, "metadata": {"filename": "multimodal-rag-building-ainimal-go-fecf8404ed97.md", "extension": ".md", "title": "Multimodal RAG: Building \u2018AInimal Go!\u2019, a Pok\u00e9mon Go-Inspired App with ResNet, Cohere and Llamaindex", "date": "Nov 27, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-building-ainimal-go-fecf8404ed97"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f2ac32cb-8853-4f0b-bf40-3a908cd68c45", "node_type": "4", "metadata": {"filename": "multimodal-rag-building-ainimal-go-fecf8404ed97.md", "extension": ".md", "title": "Multimodal RAG: Building \u2018AInimal Go!\u2019, a Pok\u00e9mon Go-Inspired App with ResNet, Cohere and Llamaindex", "date": "Nov 27, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-building-ainimal-go-fecf8404ed97"}, "hash": "cbec53b840718fc009a5121f08266eaedfbed6820aa69fb348ca8ace2f105420", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2d7354d-d242-421e-9294-b9be74b90c74", "node_type": "1", "metadata": {"filename": "multimodal-rag-building-ainimal-go-fecf8404ed97.md", "extension": ".md", "title": "Multimodal RAG: Building \u2018AInimal Go!\u2019, a Pok\u00e9mon Go-Inspired App with ResNet, Cohere and Llamaindex", "date": "Nov 27, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-building-ainimal-go-fecf8404ed97"}, "hash": "fb2a7fe40f5648bd0fac35a77f79be26a3d532ef02936b870cb07f41bf9a2abf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4e17092-a87d-4fa5-9695-907c6cd848ed", "node_type": "1", "metadata": {}, "hash": "64d8fbf141d7ad6e927a5fdc28b242c8bd6f70eebd5723e73a0675156926b931", "class_name": "RelatedNodeInfo"}}, "text": "Creating the Chat Engine\n\nThe ` create_chat_engine ` function takes the animal description and utilizes\nit to create a query engine. This engine is responsible for handling user\nqueries and generating responses based on the identified animal.\n\n    \n    \n    def create_chat_engine(img_desc, api_key):\n        doc = Document(text=img_desc)\n        \n        query_engine = CitationQueryEngine.from_args(\n            index,\n            similarity_top_k=3,\n            citation_chunk_size=512,\n            verbose=True\n        )\n        \n        return query_engine\n    \n    \n    system_prompt=f\"\"\"\n                  You are a chatbot, able to have normal interactions. Do not make up information.\n                  ", "mimetype": "text/plain", "start_char_idx": 9012, "end_char_idx": 9724, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e4e17092-a87d-4fa5-9695-907c6cd848ed": {"__data__": {"id_": "e4e17092-a87d-4fa5-9695-907c6cd848ed", "embedding": null, "metadata": {"filename": "multimodal-rag-building-ainimal-go-fecf8404ed97.md", "extension": ".md", "title": "Multimodal RAG: Building \u2018AInimal Go!\u2019, a Pok\u00e9mon Go-Inspired App with ResNet, Cohere and Llamaindex", "date": "Nov 27, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-building-ainimal-go-fecf8404ed97"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f2ac32cb-8853-4f0b-bf40-3a908cd68c45", "node_type": "4", "metadata": {"filename": "multimodal-rag-building-ainimal-go-fecf8404ed97.md", "extension": ".md", "title": "Multimodal RAG: Building \u2018AInimal Go!\u2019, a Pok\u00e9mon Go-Inspired App with ResNet, Cohere and Llamaindex", "date": "Nov 27, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-building-ainimal-go-fecf8404ed97"}, "hash": "cbec53b840718fc009a5121f08266eaedfbed6820aa69fb348ca8ace2f105420", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94c2d12c-4bad-4e32-9294-6f3f24feda9d", "node_type": "1", "metadata": {"filename": "multimodal-rag-building-ainimal-go-fecf8404ed97.md", "extension": ".md", "title": "Multimodal RAG: Building \u2018AInimal Go!\u2019, a Pok\u00e9mon Go-Inspired App with ResNet, Cohere and Llamaindex", "date": "Nov 27, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-building-ainimal-go-fecf8404ed97"}, "hash": "136d5f999c3def3cf564ad277dcc71d4af4dc80670f79484e33af1579126cd78", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "381ac082-ae46-47e5-bae7-c73907c6df1f", "node_type": "1", "metadata": {}, "hash": "9d9c5a6258f2ad5f0b39e5cfef974d6e5b4855329b9b78e19d2d20c445fa98f0", "class_name": "RelatedNodeInfo"}}, "text": "You always answer in great detail and are polite. Your job is to roleplay as an {img_desc}. \n                  Remember to make {img_desc} sounds while talking but dont overdo it.\n                  \"\"\"\n                        \n    response = chat_engine.query(f\"{system_prompt}. {user_input}\")\n\nBy creating a query engine specific to the identified animal, this function\nensures that the conversations in the app are relevant, informative, and\nengaging. I have used the CitationQueryEngine to provide the future\npossibility of showing the sources as well, making the conversations not only\nengaging but also informative with credible references.\n\n##  7\\. Bringing It All Together\n\nWith all the technical components in place, \u2018AInimal Go!\u2019 combines everything\ninto a user-friendly chat interface. Here, users can interact directly with\nthe AI, asking questions and receiving responses about the identified animal.\nThis final interaction loop, skillfully managed by Streamlit, perfectly\nshowcases the seamless integration of vision and language models in the app.\n\n#  Wrapping Up\n\n\u2018AInimal Go!\u2019 represents an exciting fusion of vision models, language models,\nand Wikipedia, with LlamaIndex serving as the orchestrator that seamlessly\nintegrates ResNet for animal identification and Cohere\u2019s LLM for engaging\nconversations. This app is a stepping stone to even more innovative visual-\nlanguage applications. ", "mimetype": "text/plain", "start_char_idx": 9724, "end_char_idx": 11130, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "381ac082-ae46-47e5-bae7-c73907c6df1f": {"__data__": {"id_": "381ac082-ae46-47e5-bae7-c73907c6df1f", "embedding": null, "metadata": {"filename": "multimodal-rag-building-ainimal-go-fecf8404ed97.md", "extension": ".md", "title": "Multimodal RAG: Building \u2018AInimal Go!\u2019, a Pok\u00e9mon Go-Inspired App with ResNet, Cohere and Llamaindex", "date": "Nov 27, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-building-ainimal-go-fecf8404ed97"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f2ac32cb-8853-4f0b-bf40-3a908cd68c45", "node_type": "4", "metadata": {"filename": "multimodal-rag-building-ainimal-go-fecf8404ed97.md", "extension": ".md", "title": "Multimodal RAG: Building \u2018AInimal Go!\u2019, a Pok\u00e9mon Go-Inspired App with ResNet, Cohere and Llamaindex", "date": "Nov 27, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-building-ainimal-go-fecf8404ed97"}, "hash": "cbec53b840718fc009a5121f08266eaedfbed6820aa69fb348ca8ace2f105420", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4e17092-a87d-4fa5-9695-907c6cd848ed", "node_type": "1", "metadata": {"filename": "multimodal-rag-building-ainimal-go-fecf8404ed97.md", "extension": ".md", "title": "Multimodal RAG: Building \u2018AInimal Go!\u2019, a Pok\u00e9mon Go-Inspired App with ResNet, Cohere and Llamaindex", "date": "Nov 27, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-building-ainimal-go-fecf8404ed97"}, "hash": "f4c2a567d1820ece7f3c039d66861bdf9b33377fe3ffbb050d46a33c95491947", "class_name": "RelatedNodeInfo"}}, "text": "The possibilities are boundless, and your insights can\nshape its future. I encourage you to explore the demo, experiment with the\ncode, and join me in pushing the boundaries of what AI can achieve in the\nrealm of multimodal interactions.\n\n[ GitHub Repo ](https://github.com/AI-ANK/AInimalGo-Chat-with-Animals)\n\n[ Connect with Me on LinkedIn\n](https://www.linkedin.com/in/harshadsuryawanshi/)\n\n[ LinkedIn Post\n](https://www.linkedin.com/posts/harshadsuryawanshi_llamaindex-ai-\ndeeplearning-\nactivity-7134632983495327744-M7yy?utm_source=share&utm_medium=member_desktop)\n\n[ Live Demo ](https://huggingface.co/spaces/AI-ANK/AInimal_Go)\n\n", "mimetype": "text/plain", "start_char_idx": 11130, "end_char_idx": 11763, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3325ba3d-d290-4020-9e3d-e594243ca8eb": {"__data__": {"id_": "3325ba3d-d290-4020-9e3d-e594243ca8eb", "embedding": null, "metadata": {"filename": "introducing-llama-packs-e14f453b913a.md", "extension": ".md", "title": "Introducing Llama Packs", "date": "Nov 22, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llama-packs-e14f453b913a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "786afdd8-7c14-4b62-8622-503cacbf0cdb", "node_type": "4", "metadata": {"filename": "introducing-llama-packs-e14f453b913a.md", "extension": ".md", "title": "Introducing Llama Packs", "date": "Nov 22, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llama-packs-e14f453b913a"}, "hash": "dfd84dc63aac309773d38d58f1cab23f8f019e6c3b55400f94f42b152c0e9c5e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8e581fe-c49b-4e6b-8b36-2e53bc33e2eb", "node_type": "1", "metadata": {}, "hash": "b4d75ca4ac6322ca14234e49bc5e4ebaebee574d7b739cb90890e0a028acb896", "class_name": "RelatedNodeInfo"}}, "text": "Today we\u2019re excited to introduce **Llama Packs \u2014** a community-driven hub of\nprepackaged modules that you can use to kickstart your LLM application. Import\nthem for a wide variety of use cases, from building a Streamlit app to\nbuilding advanced retrieval over Weaviate to a resume parser that does\nstructured data extraction. Just as important, inspect and customize them to\nyour liking.\n\nThey\u2019re available on [ LlamaHub ](https://llamahub.ai/) : we\u2019ve launched 16+\ntemplates with our launch partners already, and we\u2019re going to be adding a lot\nmore!\n\n(To those of you in the states, Happy Thanksgiving )\n\n#  Context\n\nThere are so many choices when building an LLM app that it can be daunting to\nget started building for a specific use case. Even for RAG the user needs to\nmake the following decisions:\n\n  * Which LLM should I use? Embedding model? \n  * Vector database? \n  * Chunking/parsing strategy \n  * Retrieval Algorithm \n  * Wrapping in surrounding application \n\nEvery use case requires different parameters, and LlamaIndex as a core LLM\nframework offers a comprehensive set of unopinionated modules to let users\ncompose an application.\n\nBut we needed a way for users to get started more easily for their use case.\nAnd that\u2019s exactly where Llama Packs comes in.\n\n#  Overview\n\nLlama Packs can be described in two ways:\n\n  * On one hand, they are prepackaged **modules** that can be initialized with parameters and run out of the box to achieve a given use case (whether that\u2019s a full RAG pipeline, application template, and more). You can also import **submodules** (e.g. LLMs, query engines) to use directly **.**\n  * On another hand, LlamaPacks are **templates** that you can inspect, modify, and use. \n\nThey can be downloaded either through our ` llama_index ` Python library or\nthe CLI in _one line of code:_\n\n**CLI:**\n\n    \n    \n    llamaindex-cli download-llamapack <pack_name> --download-dir <pack_directory>\n\n**Python**\n\n    \n    \n    from llama_index.llama_pack import download_llama_pack\n    \n    # download and install dependencies\n    VoyageQueryEnginePack = download_llama_pack(\n      \"&lt;pack_name&gt;\", \"&lt;pack_directory&gt;\"\n    )\n\nLlama Packs can span abstraction levels \u2014 some are full prepackaged templates\n(full Streamlit / Gradio apps), and some combine a few smaller modules\ntogether (e.g. our SubQuestionQueryEngine with Weaviate). All of them are\nfound in [ LlamaHub ](https://llamahub.ai/) . You can filter by packs by\nselecting \u201cLlama Packs\u201d from the dropdown.\n\nLlama Packs on LlamaHub\n\nWe\u2019re excited to partner with the following companies/contributors for our\nlaunch, featuring **16+ templates.** We highlight some examples below:\n\n  * **Streamlit / Snowflake (Caroline F.):** [ Streamlit Chatbot ](https://llamahub.ai/l/llama_packs-streamlit_chatbot)\n  * **Arize (Mikyo K., Xander S.):** [ Arize Phoenix ](https://llamahub.ai/l/llama_packs-arize_phoenix_query_engine)\n  * **ActiveLoop / DeepLake (Mikayel H., Adhilkhan S.):** [ DeepMemory Pack ](https://llamahub.ai/l/llama_packs-deeplake_deepmemory_retriever) , [ Multi-modal Retrieval ](https://llamahub.ai/l/llama_packs-deeplake_multimodal_retrieval)\n  * **Weaviate (Erika C.):** [ Sub Question Query Engine ](https://llamahub.ai/l/llama_packs-sub_question_weaviate) , [ Retry Query Engine ](https://llamahub.ai/l/llama_packs-retry_engine_weaviate)\n  * **Voyage AI (Hong L.):** [ Voyage AI Pack ](https://llamahub.ai/l/llama_packs-voyage_query_engine)\n  * **TruEra (Josh R.):** [ TruLens Eval Pack ](https://llamahub.ai/l/llama_packs-trulens_eval_packs) (this is 3 packs in one) \n  * **Timescale (Matvey A.):** [ Timescale Vector AutoRetrieval ](https://llamahub.ai/l/llama_packs-timescale_vector_autoretrieval)\n  * **Wenqi G.:** [ LLaVa Completion Pack ](https://llamahub.ai/l/llama_packs-llava_completion)\n\nThere\u2019s not enough room in this blog post to feature every template, we\u2019ll be\nrunning features on every pack in the next few days.\n\nSpecial thanks to Logan Markewich and Andrei Fajardo on the LlamaIndex team\nfor getting Llama Packs up and running.\n\n#  **Example Walkthrough**\n\nThe best way to highlight LlamaPack features is to showcase an example. We\u2019ll\nwalk through a simple [ Llama Pack ](https://llamahub.ai/l/llama_packs-\nvoyage_query_engine) that gives the user a RAG pipeline setup with Voyage AI\nembeddings.\n\nVoyage AI Pack. Every Pack has a detailed README on how to use / modules.\n\nFirst, we download and initialize the Pack over a set of documents:\n\n    \n    \n    from llama_index.llama_pack import download_llama_pack\n    \n    # download pack\n    VoyageQueryEnginePack = download_llama_pack(\"VoyageQueryEnginePack\", \"./voyage_pack\")\n    # initialize pack (assume documents is defined)\n    voyage_pack = VoyageQueryEnginePack(documents)\n\nEvery Llama Pack implements a ` get_modules() ` function allowing you to\ninspect/use the modules.\n\n    \n    \n    modules = voyage_pack.get_modules()\n    display(modules)\n    \n    # get LLM, vector index\n    llm = modules[\"llm\"]\n    vector_index = modules[\"index\"]\n\nThe Llama Pack can be run in an **out of the box** fashion. By calling ` run `\n, we\u2019ll execute the RAG pipeline and get back a response. In this setting, you\ndon\u2019t need to worry about the internals.\n\n    \n    \n    # this will run the full pack\n    response = voyage_pack.run(\"What did the author do growing up?\", similarity_top_k=2)\n    print(str(response))\n    \n    \n    \n    The author spent his time outside of school mainly writing and programming. He wrote short stories and attempted to write programs on an IBM 1401. Later, he started programming on a TRS-80, creating simple games and a word processor. He also painted still lives while studying at the Accademia.\n\nThe second important thing is that you have **full access to the code of the\nLlama Pack** . ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5779, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8e581fe-c49b-4e6b-8b36-2e53bc33e2eb": {"__data__": {"id_": "d8e581fe-c49b-4e6b-8b36-2e53bc33e2eb", "embedding": null, "metadata": {"filename": "introducing-llama-packs-e14f453b913a.md", "extension": ".md", "title": "Introducing Llama Packs", "date": "Nov 22, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llama-packs-e14f453b913a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "786afdd8-7c14-4b62-8622-503cacbf0cdb", "node_type": "4", "metadata": {"filename": "introducing-llama-packs-e14f453b913a.md", "extension": ".md", "title": "Introducing Llama Packs", "date": "Nov 22, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llama-packs-e14f453b913a"}, "hash": "dfd84dc63aac309773d38d58f1cab23f8f019e6c3b55400f94f42b152c0e9c5e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3325ba3d-d290-4020-9e3d-e594243ca8eb", "node_type": "1", "metadata": {"filename": "introducing-llama-packs-e14f453b913a.md", "extension": ".md", "title": "Introducing Llama Packs", "date": "Nov 22, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llama-packs-e14f453b913a"}, "hash": "1b5924297134ae8ca13c82cbafc272cbb120cc9fb016d48c37514c3f7ed42918", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cfa03656-ff5d-4368-a396-6cb3b356f5f0", "node_type": "1", "metadata": {}, "hash": "b4295506a26fb7faac1facb9a5604ed3bef4957f3884e5f1f3339a55780d4e09", "class_name": "RelatedNodeInfo"}}, "text": "This allows you to customize the Llama Pack, rip out code, or\njust use it as reference to build your own app. Let\u2019s take a look at the\ndownloaded pack in ` voyage_pack/base.py ` , and swap out the OpenAI LLM for\nAnthropic:\n\n    \n    \n    from llama_index.llms import Anthropic\n    ...\n    \n    class VoyageQueryEnginePack(BaseLlamaPack):\n        def __init__(self, documents: List[Document]) -&gt; None:\n            llm = Anthropic()\n            embed_model = VoyageEmbedding(\n                model_name=\"voyage-01\", voyage_api_key=os.environ[\"VOYAGE_API_KEY\"]\n            )\n            service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n            self.llm = llm\n            self.index = VectorStoreIndex.from_documents(\n                documents, service_context=service_context\n            )\n    \n        def get_modules(self) -&gt; Dict[str, Any]:\n            \"\"\"Get modules.\"\"\"\n            return {\"llm\": self.llm, \"index\": self.index}\n    \n        def run(self, query_str: str, **kwargs: Any) -&gt; Any:\n            \"\"\"Run the pipeline.\"\"\"\n            query_engine = self.index.as_query_engine(**kwargs)\n            return query_engine.query(query_str)\n\nYou can re-import the module directly and run it again:\n\n    \n    \n    from voyage_pack.base import VoyageQueryEnginePack\n    \n    voyage_pack = VoyageQueryEnginePack(documents)\n    response = voyage_pack.run(\"What did the author do during his time in RISD?\")\n    print(str(response))\n\n#  Conclusion\n\nTry it out and let us know what you think!\n\n", "mimetype": "text/plain", "start_char_idx": 5779, "end_char_idx": 7315, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cfa03656-ff5d-4368-a396-6cb3b356f5f0": {"__data__": {"id_": "cfa03656-ff5d-4368-a396-6cb3b356f5f0", "embedding": null, "metadata": {"filename": "introducing-llama-packs-e14f453b913a.md", "extension": ".md", "title": "Introducing Llama Packs", "date": "Nov 22, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llama-packs-e14f453b913a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "786afdd8-7c14-4b62-8622-503cacbf0cdb", "node_type": "4", "metadata": {"filename": "introducing-llama-packs-e14f453b913a.md", "extension": ".md", "title": "Introducing Llama Packs", "date": "Nov 22, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llama-packs-e14f453b913a"}, "hash": "dfd84dc63aac309773d38d58f1cab23f8f019e6c3b55400f94f42b152c0e9c5e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8e581fe-c49b-4e6b-8b36-2e53bc33e2eb", "node_type": "1", "metadata": {"filename": "introducing-llama-packs-e14f453b913a.md", "extension": ".md", "title": "Introducing Llama Packs", "date": "Nov 22, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llama-packs-e14f453b913a"}, "hash": "e7f72ee4cfffc4dee55e157afe494ab233de2af911d7f3b148c9f182c1ec502b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43ec9c28-fec0-494b-880d-3edaecb1e4c9", "node_type": "1", "metadata": {}, "hash": "92ec6ba78db2620dd34434f986b1d0de4fada3fb46bc9ebe35e6dfa7a271b48c", "class_name": "RelatedNodeInfo"}}, "text": "##  Contributing\n\nNot on here yet? ", "mimetype": "text/plain", "start_char_idx": 7315, "end_char_idx": 7350, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43ec9c28-fec0-494b-880d-3edaecb1e4c9": {"__data__": {"id_": "43ec9c28-fec0-494b-880d-3edaecb1e4c9", "embedding": null, "metadata": {"filename": "introducing-llama-packs-e14f453b913a.md", "extension": ".md", "title": "Introducing Llama Packs", "date": "Nov 22, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llama-packs-e14f453b913a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "786afdd8-7c14-4b62-8622-503cacbf0cdb", "node_type": "4", "metadata": {"filename": "introducing-llama-packs-e14f453b913a.md", "extension": ".md", "title": "Introducing Llama Packs", "date": "Nov 22, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llama-packs-e14f453b913a"}, "hash": "dfd84dc63aac309773d38d58f1cab23f8f019e6c3b55400f94f42b152c0e9c5e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cfa03656-ff5d-4368-a396-6cb3b356f5f0", "node_type": "1", "metadata": {"filename": "introducing-llama-packs-e14f453b913a.md", "extension": ".md", "title": "Introducing Llama Packs", "date": "Nov 22, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llama-packs-e14f453b913a"}, "hash": "20c87d82f2346aa3929ba047731dc86c77690385c455fe09107c46a7f049b85a", "class_name": "RelatedNodeInfo"}}, "text": "We\u2019d _love_ to feature you! If you have any templates with\nLlamaIndex, adding it is almost as simple as copying/pasting your existing\ncode over into a ` BaseLlamaPack ` subclass. Take a look at this folder for a\nfull set of examples: [ https://github.com/run-llama/llama-\nhub/tree/main/llama_hub/llama_packs ](https://github.com/run-llama/llama-\nhub/tree/main/llama_hub/llama_packs)\n\n##  Resources\n\nAll Llama Packs can be found on LlamaHub: [ https://llamahub.ai/\n](https://llamahub.ai/)\n\nThe full notebook walkthrough is here: [ https://github.com/run-\nllama/llama_index/blob/main/docs/examples/llama_hub/llama_packs_example.ipynb\n](https://github.com/run-\nllama/llama_index/blob/main/docs/examples/llama_hub/llama_packs_example.ipynb)\n\n", "mimetype": "text/plain", "start_char_idx": 7350, "end_char_idx": 8088, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fcf0a4a5-36da-4ee3-a857-4fd021be8f66": {"__data__": {"id_": "fcf0a4a5-36da-4ee3-a857-4fd021be8f66", "embedding": null, "metadata": {"filename": "introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.md", "extension": ".md", "title": "Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data", "date": "Nov 21, 2023", "url": "https://www.llamaindex.ai/blog/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3724df83-66e6-4336-ad5c-6eb97505d14c", "node_type": "4", "metadata": {"filename": "introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.md", "extension": ".md", "title": "Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data", "date": "Nov 21, 2023", "url": "https://www.llamaindex.ai/blog/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1"}, "hash": "9a1acdd5595574f8a91ade435b6858064d62696bd8dcdadb224537a3905d98b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc3d2d64-4f33-4c92-87d3-1e1daa229155", "node_type": "1", "metadata": {}, "hash": "79008f77e32f66c15d5a537efaf5d98c7a032e8551a32cc0e2504283dd10c440", "class_name": "RelatedNodeInfo"}}, "text": "Today we introduce [ **RAGs** ](https://github.com/run-llama/rags) **,** a\nStreamlit app that allows you to create and customize your own RAG pipeline\nand then use it over your own data \u2014 all with natural language! This means you\ncan now setup a \u201cChatGPT over your data\u201d without needing to code.\n\nSetup and query a RAG pipeline in three simple steps:\n\n  1. **Easy Task Description** : Simply describe your task (like \u201cload this web page\u201d) and define the parameters for your RAG systems (like retrieving a certain number of documents). \n  2. **Configurable Settings** : Dive into the configuration view to see and alter the automatically generated parameters, such as top-k retrieval, summarization options, and more. \n  3. **Interactive RAG Agent:** Once set up, you can interact with your RAG agent, asking questions and getting responses based on your data. \n\nThe app is designed for both **less-technical and technical users:** if you\u2019re\nless-technical, you still need to clone the repo and pip install it, but you\ndon\u2019t need to worry about what\u2019s going on under the hood. On the other hand,\nif you are technical, you can inspect and customize specific parameter\nsettings (e.g. top-k, data).\n\nHome page for RAGs\n\n#  Detailed Overview\n\nThe app contains the following sections, corresponding to the steps listed\nabove.\n\n##  [1] Home Page\n\nThis is the section where you build a RAG pipeline by instructing the \u201cbuilder\nagent\u201d. Typically to setup a RAG pipeline you need the following components:\n\n  1. **Describe the dataset** : Currently we support either _a single local file_ or a _web page_ . We\u2019re open to suggestions here! \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1632, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc3d2d64-4f33-4c92-87d3-1e1daa229155": {"__data__": {"id_": "cc3d2d64-4f33-4c92-87d3-1e1daa229155", "embedding": null, "metadata": {"filename": "introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.md", "extension": ".md", "title": "Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data", "date": "Nov 21, 2023", "url": "https://www.llamaindex.ai/blog/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3724df83-66e6-4336-ad5c-6eb97505d14c", "node_type": "4", "metadata": {"filename": "introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.md", "extension": ".md", "title": "Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data", "date": "Nov 21, 2023", "url": "https://www.llamaindex.ai/blog/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1"}, "hash": "9a1acdd5595574f8a91ade435b6858064d62696bd8dcdadb224537a3905d98b0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fcf0a4a5-36da-4ee3-a857-4fd021be8f66", "node_type": "1", "metadata": {"filename": "introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.md", "extension": ".md", "title": "Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data", "date": "Nov 21, 2023", "url": "https://www.llamaindex.ai/blog/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1"}, "hash": "4ac26d1d31c6b81b15f0e1d83462d0a4a6a7a47be306b3ade7ba7552124bfc77", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "15c9e7d2-1c89-4e8a-bde9-de53cdc7dc35", "node_type": "1", "metadata": {}, "hash": "5bf679ad36631b01791d259b450ac91840f3d25d519e90afd107b83716e5ad9c", "class_name": "RelatedNodeInfo"}}, "text": "2. **Define the Task** : Your description here initializes the \u201csystem prompt\u201d of the LLM powering the RAG pipeline. \n  3. **Set RAG Parameters** : Configure typical RAG setup parameters, such as top-k retrieval, chunk size, and summarization options. See below for the full list of parameters. \n\n##  [2] RAG Config: Tailoring Your Experience\n\nAfter setting up the basics, you move to the RAG Config section. This part of\nthe app provides an intuitive UI where you can:\n\n  * **View Generated Parameters:** The builder agent suggests parameters based on your initial setup. \n  * **Edit and Customize** : You have complete freedom to tweak these settings, ensuring the RAG agent behaves exactly as you need. \n  * **Update the Agent:** Any changes you make can be instantly applied by hitting the \u201cUpdate Agent\u201d button. \n\nThis is the current set of parameters:\n\n  * System Prompt \n  * Include Summarization: whether to also add a summarization tool (instead of only doing top-k retrieval.) \n  * Top-K \n  * Chunk Size \n  * Embed Model \n  * LLM \n\n##  [3] Generated RAG Agent: Interacting with Your Data\n\nThe final piece of the RAGs experience is the Generated RAG Agent section.\nHere\u2019s what you can expect:\n\n  * **Interactive Chatbot Interface:** Just like ChatGPT, engage in conversations with your RAG agent. \n  * **Data-Driven Responses:** The agent utilizes top-k vector search and optional summarization tools to answer your queries based on the underlying data. \n  * **Seamless Integration:** The agent dynamically picks the right tools to fulfill your queries, ensuring a smooth and intelligent interaction with your dataset. \n\n#  Architecture\n\nWe\u2019ll cover the architecture in more detail in followups. At a high-level:\n\n  * We have a **builder agent** equipped with **builder tools \u2014** tools necessary to construct a RAG pipeline. \n  * The builder agent will use these tools to set the configuration state. At the end of the initial conversational flow these parameters are then used to initialize the RAG agent. \n\n#  Let\u2019s Walk through an Example!\n\n##  Installation and Setup\n\nGetting RAGs up and running is straightforward:\n\n  1. Clone the RAGs project and navigate to the ` rags ` project folder: [ https://github.com/run-llama/rags ](https://github.com/run-llama/rags)\n  2. Install the required packages: \n\n    \n    \n    pip install -r requirements.txt\n\n3\\. Launch the app:\n\n    \n    \n    streamlit run 1__Home.py\n\n##  Build the RAG Agent\n\nIn the below diagram we show a sequence of commands to \u201cbuild\u201d a RAG pipeline.\n\n  1. ", "mimetype": "text/plain", "start_char_idx": 1632, "end_char_idx": 4164, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "15c9e7d2-1c89-4e8a-bde9-de53cdc7dc35": {"__data__": {"id_": "15c9e7d2-1c89-4e8a-bde9-de53cdc7dc35", "embedding": null, "metadata": {"filename": "introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.md", "extension": ".md", "title": "Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data", "date": "Nov 21, 2023", "url": "https://www.llamaindex.ai/blog/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3724df83-66e6-4336-ad5c-6eb97505d14c", "node_type": "4", "metadata": {"filename": "introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.md", "extension": ".md", "title": "Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data", "date": "Nov 21, 2023", "url": "https://www.llamaindex.ai/blog/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1"}, "hash": "9a1acdd5595574f8a91ade435b6858064d62696bd8dcdadb224537a3905d98b0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc3d2d64-4f33-4c92-87d3-1e1daa229155", "node_type": "1", "metadata": {"filename": "introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.md", "extension": ".md", "title": "Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data", "date": "Nov 21, 2023", "url": "https://www.llamaindex.ai/blog/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1"}, "hash": "d73ab5d7eda190c8e48cd8368519cf59dd8e64007a6bd26938464a7e1995fc4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66fba681-53a6-48d7-a112-798324730a22", "node_type": "1", "metadata": {}, "hash": "52039f93f960ac0919fd896c9cb446f7cf92521b4e31d75cc27e7f3e8eacb9bc", "class_name": "RelatedNodeInfo"}}, "text": "Say that you want to build a chatbot \n  2. Define the dataset (here it\u2019s a web page, can also be a local file) \n  3. Define the task \n  4. Define params (chunk size 512, top-k = 3) \n\nScreenshot of the home page \u2014 build a RAG agent  Followup questions to set\nparameters and build the RAG agent\n\n##  View the Configuration\n\nWe can see the generated configuration in the below page, and view/edit them\nas necessary!\n\nE.g. ", "mimetype": "text/plain", "start_char_idx": 4164, "end_char_idx": 4583, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66fba681-53a6-48d7-a112-798324730a22": {"__data__": {"id_": "66fba681-53a6-48d7-a112-798324730a22", "embedding": null, "metadata": {"filename": "introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.md", "extension": ".md", "title": "Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data", "date": "Nov 21, 2023", "url": "https://www.llamaindex.ai/blog/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3724df83-66e6-4336-ad5c-6eb97505d14c", "node_type": "4", "metadata": {"filename": "introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.md", "extension": ".md", "title": "Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data", "date": "Nov 21, 2023", "url": "https://www.llamaindex.ai/blog/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1"}, "hash": "9a1acdd5595574f8a91ade435b6858064d62696bd8dcdadb224537a3905d98b0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "15c9e7d2-1c89-4e8a-bde9-de53cdc7dc35", "node_type": "1", "metadata": {"filename": "introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.md", "extension": ".md", "title": "Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data", "date": "Nov 21, 2023", "url": "https://www.llamaindex.ai/blog/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1"}, "hash": "a0eab953f57ac8bbad22b68393354006616a7b2caabc994f7e247680155bfb53", "class_name": "RelatedNodeInfo"}}, "text": "we can set ` include_summarization ` to True.\n\n##  Test It Out\n\nNow we can ask questions! We can ask both specific questions as well as\nsummarization questions.\n\nThis uses both the vector search and summarization tools to answer the\nrequisite questions.\n\nQuestion about a specific detail (performs vector search)  Summarization\nquestion over the entire document\n\n#  Conclusion\n\nIn general RAGs is an initial take towards a world where LLM applications are\nbuilt by and powered by natural language. Let us know your thoughts and\nfeedback!\n\n##  Resources\n\nRAGs repo: [ https://github.com/run-llama/rags ](https://github.com/run-\nllama/rags)\n\n##  Contributions and Support\n\nWe\u2019re committed to improving and expanding RAGs. If you encounter any issues\nor have suggestions, feel free to file a Github issue or join our [ Discord\ncommunity ](https://discord.gg/dGcwcsnxhU) .\n\n", "mimetype": "text/plain", "start_char_idx": 4583, "end_char_idx": 5453, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7018f358-532c-4074-9d27-b36655a82e22": {"__data__": {"id_": "7018f358-532c-4074-9d27-b36655a82e22", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-11-21-aa3a71e339f8.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201321", "date": "Nov 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-21-aa3a71e339f8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40c02fcc-f4c0-4628-9529-19a1b7cee237", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-11-21-aa3a71e339f8.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201321", "date": "Nov 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-21-aa3a71e339f8"}, "hash": "cd92d64a613408c90780c0c16c39a137d4c875a80bde18eb609664c4d1bc74d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8aaaefde-e2dd-4933-9b15-06de41397939", "node_type": "1", "metadata": {}, "hash": "8ee1ac2e536ca3aecdde4b69e9c3f3b6d4e2175a3da7cb86d4563e97b333888b", "class_name": "RelatedNodeInfo"}}, "text": "Hello Llama Fam\n\nWhat an amazing week we\u2019ve had! We\u2019re excited to share that, according to the\n[ Retool State of AI 2023 survey ](https://retool.com/reports/state-of-\nai-2023) , 1 in 12 respondents are now using LlamaIndex. We\u2019re grateful for\nall your support.\n\nIf you have a fascinating project or video you\u2019d like to share, we\u2019d love to\nsee it! Feel free to send it to us at [ news@llamaindex.ai\n](mailto:news@llamaindex.ai) . And remember to subscribe to our newsletter on\nour [ website ](https://www.llamaindex.ai/) to stay in the loop. We can\u2019t wait\nto connect with you there!\n\n**First, the highlights:**\n\n  1. **LlamaIndex 0.9 Release:** we introduced LlamaIndex version 0.9 featuring streamlined data handling with a new IngestionPipeline, automated caching, improved text processing interfaces, tokenizer updates, PyPi packaging enhancements, consistent import paths, and a beta version of MultiModal RAG Modules. [ Blog post ](/announcing-llamaindex-0-9-719f03282945) , [ Tweet ](https://twitter.com/llama_index/status/1724836383259582548?s=20) . \n  2. **Multi-Modal Evaluation Tools:** we launched multi-modal evaluation with the introduction of MultiModalRelevancyEvaluator and MultiModalFaithfulnessEvaluator, plus a guide for their application in multi-modal settings. [ Blog post ](https://t.co/gw4txOw0gY) , [ Tweet ](https://x.com/llama_index/status/1725249971551879601?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1397, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8aaaefde-e2dd-4933-9b15-06de41397939": {"__data__": {"id_": "8aaaefde-e2dd-4933-9b15-06de41397939", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-11-21-aa3a71e339f8.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201321", "date": "Nov 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-21-aa3a71e339f8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40c02fcc-f4c0-4628-9529-19a1b7cee237", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-11-21-aa3a71e339f8.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201321", "date": "Nov 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-21-aa3a71e339f8"}, "hash": "cd92d64a613408c90780c0c16c39a137d4c875a80bde18eb609664c4d1bc74d7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7018f358-532c-4074-9d27-b36655a82e22", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-11-21-aa3a71e339f8.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201321", "date": "Nov 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-21-aa3a71e339f8"}, "hash": "ac35f5623e7bfa07d1d2d5b2ce5065c73755223c5915389ebf2e0e040b6dbbe6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "732720c5-d8ca-4898-8730-dd6bc86812c1", "node_type": "1", "metadata": {}, "hash": "1a4fb5b552fa189800aadfec267b001a7ca316fd8f182ec23517958c1ce37a8c", "class_name": "RelatedNodeInfo"}}, "text": "3. ` **create-llama** ` **CLI Tool:** we unveiled ` **create-llama** ` , a versatile CLI tool for building full-stack LLM apps with options like FastAPI, ExpressJS, and Next.js for backends and a Next.js frontend with Vercel AI SDK components. [ Blog post ](https://t.co/JpH5Trq4Yb) , [ Tweet ](https://x.com/jerryjliu0/status/1724481554528014660?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 1397, "end_char_idx": 1755, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "732720c5-d8ca-4898-8730-dd6bc86812c1": {"__data__": {"id_": "732720c5-d8ca-4898-8730-dd6bc86812c1", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-11-21-aa3a71e339f8.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201321", "date": "Nov 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-21-aa3a71e339f8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40c02fcc-f4c0-4628-9529-19a1b7cee237", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-11-21-aa3a71e339f8.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201321", "date": "Nov 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-21-aa3a71e339f8"}, "hash": "cd92d64a613408c90780c0c16c39a137d4c875a80bde18eb609664c4d1bc74d7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8aaaefde-e2dd-4933-9b15-06de41397939", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-11-21-aa3a71e339f8.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201321", "date": "Nov 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-21-aa3a71e339f8"}, "hash": "ecc3a34cb83915e2701f8a11eb15d4787acfe85cf3856852e5c4adc87acd44e5", "class_name": "RelatedNodeInfo"}}, "text": "4. **Cohere Reranker Fine-Tuning:** we enhanced RAG pipeline retrieval performance with the fine-tuning of the Cohere reranker. [ Blog post ](/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b) , [ Tweet ](https://x.com/llama_index/status/1725189652003610888?s=20) . \n\nComing up this week: we have a YouTube live event in partnership with [ AI\nMakerspace ](https://www.youtube.com/channel/UCbDZFHUjTCCUKyXgcp3g50Q)\nexploring the potential of LlamaIndex to handle complex PDFs with tables,\ncharts and more. [ Register for free! ](https://lu.ma/RAG4PDF)\n\n**Feature Releases and Enhancements:**\n\n  * We introduced the LlamaIndex 0.9 version with updates on streamlined data handling with new IngestionPipeline, automated caching, improved interfaces for text processing, tokenizer updates, enhanced PyPi packaging, consistent import paths, and a beta of MultiModal RAG Modules for text and image integration. [ Blog post ](/announcing-llamaindex-0-9-719f03282945) , [ Tweet ](https://twitter.com/llama_index/status/1724836383259582548?s=20) . \n  * We introduced multi-modal evaluation which includes MultiModalRelevancyEvaluator and MultiModalFaithfulnessEvaluator, and a guide on using them in multi-modal applications. [ Blog post ](https://t.co/gw4txOw0gY) , [ Tweet ](https://x.com/llama_index/status/1725249971551879601?s=20) . \n  * We introduced ` **create-llama** ` , a CLI tool for easily building full-stack LLM apps, offering choices like FastAPI, ExpressJS, and Next.js backends with Llama Index, and a Next.js frontend with Vercel AI SDK components, enabling extensive customization for AI engineers. [ Blog post ](https://t.co/JpH5Trq4Yb) , [ Tweet ](https://x.com/jerryjliu0/status/1724481554528014660?s=20) . \n  * We introduced fine-tuning of the cohere reranker to improve retrieval performance in the RAG pipeline. [ Blog post ](/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b) , [ Tweet ](https://x.com/llama_index/status/1725189652003610888?s=20) . \n\nIntegrations:\n\n  * We integrated with Chroma\u2019s multi-modal collections which allows for indexing both text and images in a single collection, enhancing RAG pipelines by combining text and image information for use with multi-modal models like GPT-4V, LLaVa, and Fuyu. [ Docs ](https://docs.llamaindex.ai/en/latest/examples/multi_modal/ChromaMultiModalDemo.html) , [ Tweet ](https://x.com/llama_index/status/1724228502168518857?s=20) . \n\n**Guides:**\n\n  * [ Guide ](https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/multi_modal_retrieval.ipynb) on Multi-Modal Retrieval using GPT text embedding and CLIP image embedding for Wikipedia Articles. \n  * [ Guide ](https://nanonets.com/blog/llamaindex/#using-index-to-chat-with-data) on LlamaIndex by Nanonets covering over 12 key areas such as data management, indexing/storage, querying with top-k RAG, structured outputs, chat functionalities with memory, and agent development incorporating tool use. \n  * [ Guide ](https://github.com/run-llama/llama_index/blob/main/docs/examples/transforms/TransformsEval.ipynb) on using Ingestion pipeline focusing on showcasing experiments on chunk overlaps and the use of metadata extractors, including title, summary, and other elements. \n  * [ Guide ](https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/perplexity.ipynb) on using Perplexity API with LlamaIndex by [ Vishhvak ](https://twitter.com/vishhvak) . \n  * [ Guide ](https://docs.llamaindex.ai/en/stable/community/integrations/fleet_libraries_context.html) on using [ Fleet Context ](https://twitter.com/fleet_ai) to download the embeddings for LlamaIndex\u2019s documentation and build a hybrid dense/sparse vector retrieval engine on top of it. \n  * [ Guide ](https://github.com/jerryjliu/create_llama_projects) on building a full-stack financial analysis bot using ` **create-llama** ` and Llama Index's RAG, capable of querying text and tables across SEC filings. \n\n**Tutorials:**\n\n  * [ Wenqi Glantz ](https://medium.com/@wenqiglantz) made a [ tutorial ](https://levelup.gitconnected.com/llava-vs-gpt-4v-amidst-snow-geese-migration-c2561b16113d) on LLaVA vs. GPT-4V Amidst Snow Geese Migration. \n  * [ Glenn Parham\u2019s ](https://twitter.com/glenn__parham) [ cookbook ](https://github.com/deptofdefense/LLMs-at-DoD/blob/main/tutorials/Chatting%20with%20your%20Docs.ipynb) on LlamaIndex, hosted in the Department of Defense\u2019s official repository, showcases methods for applying RAG on unclassified DoD policy documents. \n  * [ Sudarshan Koirala ](https://www.youtube.com/watch?v=PHEZ6AHR57w) made a tutorial on Using Perplexity API with LlamaIndex. \n  * [ Ravi Theja ](https://twitter.com/ravithejads) [ analysis ](/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9) on GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques \n\n**Webinars:**\n\n  * Check out our CEO \u2014 [ Jerry Liu\u2019s ](https://twitter.com/jerryjliu0) talk on Building Production-Ready RAG Applications at [ AI.engineer ](https://t.co/46VrFt8GCV) Summit. \n\n", "mimetype": "text/plain", "start_char_idx": 1755, "end_char_idx": 6916, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c946997-cc83-41de-a59c-6ed89059e7ed": {"__data__": {"id_": "9c946997-cc83-41de-a59c-6ed89059e7ed", "embedding": null, "metadata": {"filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe05bb37-1ac3-4bbb-95b3-55d05f180eb1", "node_type": "4", "metadata": {"filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}, "hash": "adff9486539f8783cb7bd0696d17b40f22b60ff1d1acf2eaa43443d41078c908", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "352b5a2c-f77d-43b9-bff4-f7a9dfe07ee5", "node_type": "1", "metadata": {}, "hash": "47129b933a9c0dec7fd76959bc70e0633563e855616236779ac3c34b11b21bd4", "class_name": "RelatedNodeInfo"}}, "text": "##  **Introduction**\n\nIn the domain of document handling, accurately extracting crucial information\nfrom images has posed an enduring obstacle. Despite Optical Character\nRecognition (OCR) advancements in converting images to editable text, it faces\nnumerous intricacies with diverse document formats and quality. Here enters\nZephyr 7b LLM, a pioneering remedy that, coupled with LlamaIndex, directly\naddresses these hurdles, heralding a transformative era in image-based\ndocument extraction.\n\nSource: [ Zephyr-llama-index ](https://corca.substack.com/p/top-llm-papers-of-\nthe-week-95e?utm_source=profile&utm_medium=reader2)\n\n##  **The OCR Dilemma: Obstacles and Constraints Optical Character**\n\nRecognition (OCR), though potent, faces impediments such as:\n\n  1. **Diverse Document Formats** : Documents exhibit intricate layouts, fonts, and structures, posing challenges for traditional OCR to precisely interpret and extract information. \n  2. **Quality and Clarity** : Images with low resolution, blurriness, or skewed angles hinder OCR\u2019s accuracy in deciphering text. \n  3. **Handwritten and Cursive Content** : OCR often struggles with handwritten text or cursive fonts, resulting in errors or incomplete extraction. \n  4. **Multilingual Complexity** : Processing documents in multiple languages poses a challenge for OCR systems lacking proficiency in recognizing and extracting varied linguistic content. \n\nSource: Created by Author using MidJourney\n\n##  **Zephyr 7b LLM: Narrowing the Divide**\n\nZephyr 7b LLM revolutionizes the landscape by tackling these inherent\nconstraints of OCR technology:\n\n  1. **Advanced Machine Learning Algorithms:**\n\nEmploying state-of-the-art machine learning algorithms, Zephyr 7b LLM\nundergoes extensive training with diverse document formats and languages. This\nequips it to adapt and learn from various document structures, resulting in\nheightened accuracy and robust extraction capabilities.\n\n**2\\. Contextual Comprehension:**\n\nDiverging from conventional OCR, Zephyr 7b LLM doesn\u2019t merely identify\nindividual characters; it comprehends the context in which these characters\nexist. This contextual understanding significantly reduces errors, ensuring\nprecise extraction even from intricate document layouts.\n\n**3\\. Adaptive Image Processing:**\n\nThe fusion with LlamaIndex amplifies Zephyr 7b LLM\u2019s ability to handle images\nof varying resolutions or qualities. Leveraging adaptive image processing\ntechniques, it rectifies distortions, enhances clarity, and optimizes images\nfor meticulous OCR analysis.\n\n**4\\. Multilingual Proficiency:**\n\nZephyr 7b LLM surpasses language barriers. Its multilingual proficiency\nfacilitates seamless content extraction from documents in various languages,\nextending global accessibility for businesses dealing with multilingual\ndocumentation.\n\nSource: Created by Author using MidJourney\n\n##  **Implementation of Code**\n\nThe collaboration between Zephyr 7b LLM and LlamaIndex signifies a pivotal\ntransformation in document extraction. By merging Zephyr\u2019s advanced OCR\ncapabilities with LlamaIndex\u2019s image enhancement and data organization\nfeatures, this integration presents a comprehensive solution:\n\n  1. **Augmented Precision** : The fusion of Zephyr\u2019s machine learning expertise and LlamaIndex\u2019s image enhancement markedly heightens the accuracy of extracted data, diminishing errors and enhancing overall efficiency. \n  2. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3400, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "352b5a2c-f77d-43b9-bff4-f7a9dfe07ee5": {"__data__": {"id_": "352b5a2c-f77d-43b9-bff4-f7a9dfe07ee5", "embedding": null, "metadata": {"filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe05bb37-1ac3-4bbb-95b3-55d05f180eb1", "node_type": "4", "metadata": {"filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}, "hash": "adff9486539f8783cb7bd0696d17b40f22b60ff1d1acf2eaa43443d41078c908", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c946997-cc83-41de-a59c-6ed89059e7ed", "node_type": "1", "metadata": {"filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}, "hash": "f5403a000732d3926d7b1d7c68c3891e4b5106754268e7d60985582f74ed9e2f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17681f3e-8afb-4626-a2c9-971b0b60d1a6", "node_type": "1", "metadata": {}, "hash": "57250f8491ef8a18745edf0c0cf333391a58617ed47ae0da65e0532a762a5915", "class_name": "RelatedNodeInfo"}}, "text": "**Efficient Workflow** : Users experience an optimized workflow, enabling swift extraction and conversion of image-based documents into structured, actionable data, facilitating expedited decision-making processes. \n  3. **Adaptability Across Document Varieties** : This integration empowers users to handle diverse document formats and languages effortlessly, granting access to previously challenging document types for extraction and analysis. \n\nSource: Image created by Author using MidJourney\n\n**Step 1: Install and Import Libraries**\n\n    \n    \n    !pip install llama-index transformers accelerate sentencepiece bitsandbytes -q\n\n**Step 2: Load the Model**\n\n    \n    \n    import torchfrom transformers import BitsAndBytesConfigfrom llama_index.prompts import PromptTemplatefrom llama_index.llms import HuggingFaceLLMquantization_config = BitsAndBytesConfig(    load_in_4bit=True,    bnb_4bit_compute_dtype=torch.float16,    bnb_4bit_quant_type=\"nf4\",    bnb_4bit_use_double_quant=True,)def messages_to_prompt(messages):  prompt = \"\"  for message in messages:    if message.role == 'system':      prompt += f\"<|system|>\\n{message.content}</s>\\n\"    elif message.role == 'user':      prompt += f\"<|user|>\\n{message.content}</s>\\n\"    elif message.role == 'assistant':      prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"  # ensure we start with a system prompt, insert blank if needed  if not prompt.startswith(\"<|system|>\\n\"):    prompt = \"<|system|>\\n</s>\\n\" + prompt  # add final assistant prompt  prompt = prompt + \"<|assistant|>\\n\"  return promptllm = HuggingFaceLLM(    model_name=\"HuggingFaceH4/zephyr-7b-alpha\",    tokenizer_name=\"HuggingFaceH4/zephyr-7b-alpha\",    query_wrapper_prompt=PromptTemplate(\"<|system|>\\n</s>\\n<|user|>\\n{query_str}</s>\\n<|assistant|>\\n\"),    context_window=3900,    max_new_tokens=2000,    model_kwargs={\"quantization_config\": quantization_config},    # tokenizer_kwargs={},    generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},    messages_to_prompt=messages_to_prompt,    device_map=\"auto\",)\n    \n    \n    from llama_index import ServiceContext, set_global_service_contextservice_context = ServiceContext.from_defaults(llm=llm, embed_model=\"local:BAAI/bge-small-en-v1.5\")\n    \n    \n    set_global_service_context(service_context)\n\n**Step 3: Storing your index**\n\n    \n    \n    from llama_index import SimpleDirectoryReader, VectorStoreIndexfrom llama_index.readers.file.base import (    DEFAULT_FILE_READER_CLS,    ImageReader,)from llama_index.response.notebook_utils import (    display_response,    display_image,)from llama_index.indices.query.query_transform.base import (    ImageOutputQueryTransform,)filename_fn = lambda filename: {\"file_name\": filename}llama_reader = SimpleDirectoryReader(    input_dir=\"/content/llama\",    file_metadata=filename_fn,)llama_documents = llama_reader.load_data()llama_index = VectorStoreIndex.from_documents(llama_documents)\n\n**Step 4: Query** [ **Transformations**\n](https://github.com/andysingal/CV_public/tree/main/zephyr-7b-alpha)\n\n    \n    \n    from llama_index.query_engine import TransformQueryEnginequery_engine = llama_index.as_query_engine(similarity_top_k=2)query_engine = TransformQueryEngine(    query_engine, query_transform=ImageOutputQueryTransform(width=400))llama_response = query_engine.query(    \"Show an image to illustrate how tree index works and explain briefly\",)display_response(llama_response)#OutputFinal Response: I am not capable of displaying images. however, i can provide you with an explanation of how tree index works.tree index is a data structure that organizes data in a hierarchical manner, similar to a tree. ", "mimetype": "text/plain", "start_char_idx": 3400, "end_char_idx": 7050, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17681f3e-8afb-4626-a2c9-971b0b60d1a6": {"__data__": {"id_": "17681f3e-8afb-4626-a2c9-971b0b60d1a6", "embedding": null, "metadata": {"filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe05bb37-1ac3-4bbb-95b3-55d05f180eb1", "node_type": "4", "metadata": {"filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}, "hash": "adff9486539f8783cb7bd0696d17b40f22b60ff1d1acf2eaa43443d41078c908", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "352b5a2c-f77d-43b9-bff4-f7a9dfe07ee5", "node_type": "1", "metadata": {"filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}, "hash": "9b91196ba197a429dabc6636802cc6f9a02a653a5e8d06cc77e3864a0bc85066", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63cd6adc-43b1-42ac-9116-933b85930fb4", "node_type": "1", "metadata": {}, "hash": "9419ccf41a53fb5b5808ede1964c257910e62ae3dcad66944b18dc52d7f401d8", "class_name": "RelatedNodeInfo"}}, "text": "it is commonly used in databases to improve query performance.when querying a tree index, the process involves traversing from the root node down to the leaf nodes. the number of child nodes chosen per parent node is determined by the child_branch_factor parameter.for example, if child_branch_factor=1, a query will choose one child node given a parent node. if child_branch_factor=2, a query will choose two child nodes per parent.the following image illustrates how a tree index works:! Tree Index Examplein this example, the tree index is built from a set of nodes (which become leaf nodes in this tree). when querying this index, the process involves traversing from the root node down to the leaf nodes. for instance, if we want to find a specific node with the value \"x\", we would start at the root node and follow the left branch (since \"x\" is less than \"a\") to the next level. we would then follow the left branch again to reach the leaf node with the value \"x\".i hope this helps clarify how tree index works!\n\n**Step 5: Lets read the** [ **receipts**\n](https://github.com/andysingal/CV_public/tree/main/zephyr-7b-alpha)\n\n    \n    \n    from llama_index.readers.file.base import DEFAULT_FILE_READER_CLSfrom llama_index.readers.file.image_reader import ImageReaderimage_parser =ImageReader(    keep_image=True,    parse_text=True    )file_extractor = DEFAULT_FILE_READER_CLSfile_extractor.update({    \".jpg\": image_parser,    \".png\": image_parser,    \".jpeg\": image_parser,    })receipt_reader = SimpleDirectoryReader(    input_dir=\"/content/data\",    file_metadata=filename_fn,    file_extractor=file_extractor,)receipt_documents = receipt_reader.load_data()print(len(receipt_documents))#Output3\n    \n    \n    receipts_index = VectorStoreIndex.from_documents(receipt_documents)from llama_index.query_engine import TransformQueryEnginequery_engine = receipts_index.as_query_engine()receipts_response = query_engine.query(    \"When was the last time I went to RESTAURANT and how much did I spend? this data is in your latest vector index.\",)display_response(receipts_response)# Output Final Response: Based on the given context information, the last time the querying individual went to RESTAURANT was on July 5, 2019, and they spent $164.00.\n\n##  Conclusion\n\nIn summary, the fusion of Zephyr 7b LLM and LlamaIndex initiates a new chapter\nin image-based document extraction. Beyond addressing OCR\u2019s inherent\nchallenges, it enhances the precision and efficiency of data extraction from\nimages, fostering improved productivity and decision-making in document-\nfocused workflows.\n\n", "mimetype": "text/plain", "start_char_idx": 7050, "end_char_idx": 9634, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63cd6adc-43b1-42ac-9116-933b85930fb4": {"__data__": {"id_": "63cd6adc-43b1-42ac-9116-933b85930fb4", "embedding": null, "metadata": {"filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe05bb37-1ac3-4bbb-95b3-55d05f180eb1", "node_type": "4", "metadata": {"filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}, "hash": "adff9486539f8783cb7bd0696d17b40f22b60ff1d1acf2eaa43443d41078c908", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17681f3e-8afb-4626-a2c9-971b0b60d1a6", "node_type": "1", "metadata": {"filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}, "hash": "ff0baf621e67ba96549814242080a295a687de25dbbc3382049a3bbb051a2a6d", "class_name": "RelatedNodeInfo"}}, "text": "\u201cStay connected and support my work through various platforms:\n\n  * [ GitHub ](https://medium.com/u/8df3bf3c40ae) : For all my open-source projects and Notebooks, you can visit my GitHub profile at [ https://github.com/andysingal ](https://github.com/andysingal) . If you find my content valuable, don\u2019t hesitate to leave a star. \n  * Patreon: If you\u2019d like to provide additional support, you can consider becoming a patron on my Patreon page at [ https://www.patreon.com/AndyShanu ](https://www.patreon.com/AndyShanu) . \n  * [ Medium ](https://medium.com/u/504c7870fdb6) : You can read my latest articles and insights on Medium at [ https://medium.com/@andysingal ](https://medium.com/@andysingal) . \n  * [ The Kaggle ](https://medium.com/u/29b47aa8cce3) : Check out my Kaggle profile for data science and machine learning projects at [ https://www.kaggle.com/alphasingal ](https://www.kaggle.com/alphasingal) . \n  * [ Hugging Face ](https://medium.com/u/b1574f0c6c5e) : For natural language processing and AI-related projects, you can explore my Huggingface profile at [ https://huggingface.co/Andyrasika ](https://huggingface.co/Andyrasika) . \n  * YouTube: To watch my video content, visit my YouTube channel at [ https://www.youtube.com/@andy111007 ](https://www.youtube.com/@andy111007) . \n  * LinkedIn: To stay updated on my latest projects and posts, you can follow me on LinkedIn. Here is the link to my profile: [ https://www.linkedin.com/in/ankushsingal/.\" ](https://www.linkedin.com/in/ankushsingal/.%22)\n\nRequests and questions: If you have a project in mind that you\u2019d like me to\nwork on or if you have any questions about the concepts I\u2019ve explained, don\u2019t\nhesitate to let me know. I\u2019m always looking for new ideas for future Notebooks\nand I love helping to resolve any doubts you might have.\n\nRemember, each \u201cLike\u201d, \u201cShare\u201d, and \u201cStar\u201d greatly contributes to my work and\nmotivates me to continue producing more quality content. Thank you for your\nsupport!\n\nIf you enjoyed this story, feel free [ to subscribe\n](https://medium.com/@andysingal) to Medium, and you will get notifications\nwhen my new articles will be published, as well as full access to thousands of\nstories from other authors.\n\nResource:\n\n  * [ Data used for above code ](https://github.com/andysingal/CV_public/tree/main/zephyr-7b-alpha)\n  * [ llama-index ](https://gpt-index.readthedocs.io/en/stable/)\n\n", "mimetype": "text/plain", "start_char_idx": 9634, "end_char_idx": 12019, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "339c273f-e338-4c9b-a835-a980ade1db00": {"__data__": {"id_": "339c273f-e338-4c9b-a835-a980ade1db00", "embedding": null, "metadata": {"filename": "shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d.md", "extension": ".md", "title": "Shipping your Retrieval-Augmented Generation app to production with create-llama", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f71218bd-cd96-4a39-8a5c-a83ac287d7e8", "node_type": "4", "metadata": {"filename": "shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d.md", "extension": ".md", "title": "Shipping your Retrieval-Augmented Generation app to production with create-llama", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d"}, "hash": "a9a739217e722f88bec09734a06ab11c277e64d18c53eebfbbff95375f0259a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0cfbf6b0-53c7-41f1-b190-6604128f2790", "node_type": "1", "metadata": {}, "hash": "06c69280346ded45fe82bfceb6d4241c287c5363479fd51cb40e0d36e3ea4710", "class_name": "RelatedNodeInfo"}}, "text": "It\u2019s a llama on a ship, geddit?\n\nLast week [ we released create-llama ](/create-llama-a-command-line-tool-to-\ngenerate-llamaindex-apps-8f7683021191) , a command-line tool to generate a\nfull-stack LlamaIndex application for Retrieval-Augmented Generation (RAG).\nThe response was fantastic, so we\u2019ll be following up with more templates and\nmore features. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 353, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0cfbf6b0-53c7-41f1-b190-6604128f2790": {"__data__": {"id_": "0cfbf6b0-53c7-41f1-b190-6604128f2790", "embedding": null, "metadata": {"filename": "shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d.md", "extension": ".md", "title": "Shipping your Retrieval-Augmented Generation app to production with create-llama", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f71218bd-cd96-4a39-8a5c-a83ac287d7e8", "node_type": "4", "metadata": {"filename": "shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d.md", "extension": ".md", "title": "Shipping your Retrieval-Augmented Generation app to production with create-llama", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d"}, "hash": "a9a739217e722f88bec09734a06ab11c277e64d18c53eebfbbff95375f0259a9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "339c273f-e338-4c9b-a835-a980ade1db00", "node_type": "1", "metadata": {"filename": "shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d.md", "extension": ".md", "title": "Shipping your Retrieval-Augmented Generation app to production with create-llama", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d"}, "hash": "c455ccf34392daa08cebf7a7c70ce06be544f1338feaeafdc317daae8297752a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a834743-33b6-44a1-af90-680d35765b31", "node_type": "1", "metadata": {}, "hash": "0f7a31a6b5cb287ccca8c682447e9b7530da47ebc76ee702cc901820bf94032a", "class_name": "RelatedNodeInfo"}}, "text": "We also wanted to show you just how easy it is to get your\ngenerated app all the way to production. So here\u2019s a step by step guide, for\neach of the three backends we currently support: Next.js serverless, Express,\nand Python.\n\n#  Next.js backend\n\nThe serverless full-stack Next.js application is the simplest version to\ndeploy as you only have one artifact to deploy. Because it\u2019s a Next.js app\nwe\u2019ll be deploying to [ Vercel ](https://vercel.com) , the home of Next.js.\n\n##  Step 1: run create-llama\n\nFirst run create-llama to generate your app. We strongly recommend generating\na new [ OpenAI API key ](https://platform.openai.com/api-keys) and supplying\nit at generation time (create-llama apps can be customized to use other LLMs\nbut that\u2019s out of scope for this tutorial).\n\n##  Step 2: create a GitHub repository and push your app to it\n\nThe easiest way to deploy on Vercel is from a linked GitHub repository. Your\ngenerated app is already set up as a git repo, so all you have to do after\ncreating a new empty repo is follow the instructions to push it up. This\nshould give you a repo that looks a bit like this:\n\n##  Step 3: import your repo into Vercel\n\nSelect the option to create a new project from a git repo:\n\nand select the repo you just created after authorizing:\n\n##  Step 4: configure your project\n\nBecause this is a Next.js app and this is Vercel, there\u2019s very little you need\nto do! The only thing you need to remember is to click \u201cenvironment variables\u201d\nand create a variable called ` OPENAI_API_KEY ` with your key.\n\n##  Step 5: Deploy!\n\n", "mimetype": "text/plain", "start_char_idx": 353, "end_char_idx": 1911, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a834743-33b6-44a1-af90-680d35765b31": {"__data__": {"id_": "5a834743-33b6-44a1-af90-680d35765b31", "embedding": null, "metadata": {"filename": "shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d.md", "extension": ".md", "title": "Shipping your Retrieval-Augmented Generation app to production with create-llama", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f71218bd-cd96-4a39-8a5c-a83ac287d7e8", "node_type": "4", "metadata": {"filename": "shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d.md", "extension": ".md", "title": "Shipping your Retrieval-Augmented Generation app to production with create-llama", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d"}, "hash": "a9a739217e722f88bec09734a06ab11c277e64d18c53eebfbbff95375f0259a9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0cfbf6b0-53c7-41f1-b190-6604128f2790", "node_type": "1", "metadata": {"filename": "shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d.md", "extension": ".md", "title": "Shipping your Retrieval-Augmented Generation app to production with create-llama", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d"}, "hash": "cd579eae5cd3e3e05697bdf1c1d8b255b2d315644606fc5a37396a0a3b571d1d", "class_name": "RelatedNodeInfo"}}, "text": "That\u2019s it! Deploying a Next.js app to Vercel is pretty easy.\n\nYour deployed app should look like this:\n\nCongratulations, you\u2019ve deployed a full-stack RAG application!\n\n#  Express backend\n\nIf you chose to generate an Express backend with a Next.js frontend instead,\nlet\u2019s get you into production with those. We\u2019ll be deploying both frontend and\nbackend to [ Render ](https://render.com/) , a fantastic service for both\nstatic sites and dynamic web applications.\n\nThere will be 3 big things to do here:\n\n  * Deploy the static frontend \n  * Deploy the backend, and give the frontend permission to call it \n  * Tell the frontend where the backend is located \n\nWe promise you\u2019ll be production in no time.\n\n##  Step 1: run create-llama\n\nThis is just like the same step in Next.js\n\n##  Step 2: push the code to a new GitHub repo\n\nLike Vercel, the easiest way to push a site to production is from a linked git\nrepo. Your generated app already has a git repo initiated with ` frontend `\nand ` backend ` folders, so you can go ahead and push them both to a single\nGitHub repository just as in the Next.js backend.\n\n##  Step 3: Start a new static site\n\nWe\u2019ll be deploying your frontend first as a static site. After authorizing,\nselect the repository where you pushed your frontend and backend; we\u2019ll\nspecify that we\u2019re deploying the frontend in the next step.\n\n##  Step 4: configure your static site\n\nThere are several changes you need to make to the default configuration to\nsuccessfully publish your static frontend:\n\n  * Name your site something memorable, it will become the URL of your site once it\u2019s deployed \n  * Set your root directory to ` frontend `\n  * Set your build command to ` npm install; npm run build `\n  * Set your publish directory to ` out `\n  * Finally, click \u201cAdvanced\u201d and set an environment variable called ` NODE_VERSION ` to ` 20 ` . The default on Render is a much older version of Node so don\u2019t skip this step! \n\n##  Step 5: deploy your static frontend\n\nClick the Deploy button and watch your site build! You should now have a site\nlive at a URL something like ` frontend-name-you-picked.onrender.com ` .\n\n##  Step 6: start a new web service\n\n", "mimetype": "text/plain", "start_char_idx": 1911, "end_char_idx": 4073, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "192d1825-da2c-4b53-950a-9890490f33bf": {"__data__": {"id_": "192d1825-da2c-4b53-950a-9890490f33bf", "embedding": null, "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c44fa078-fabe-4a04-b679-98a3d6e9d1e7", "node_type": "4", "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "hash": "22b28ff47d1c2fbe7873fa6740c7b26b9dba50da3999fba93922d5e657fcfaa5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e44d3a86-03ec-4479-be4e-8f2cf1868a29", "node_type": "1", "metadata": {}, "hash": "c375ee7d106c6a414fbda1e0c4dabc57a1054fd2d76ad39af7a24b1cce2c243e", "class_name": "RelatedNodeInfo"}}, "text": "#  Introduction\n\nGPT-4V has amazed us with its ability to analyze images and even generate\nwebsite code from visuals.\n\nThis blog post investigates GPT-4V\u2019s proficiency in interpreting bar charts,\nscatter plots, and tables. We aim to assess whether specific questioning and\nchain of thought prompting can yield better responses compared to broader\ninquiries. Our demonstration seeks to determine if GPT-4V can exceed these\nknown limitations with precise questioning and systematic reasoning\ntechniques.\n\nWe observed in these experiments that asking specific questions, rather than\ngeneral ones, yields better answers. Let\u2019s delve into these experiments.\n\nYou can also follow along with this blog post in our [ Google Colab Notebook\n](https://colab.research.google.com/github/run-\nllama/llama_index/blob/main/docs/examples/multi_modal/gpt4v_experiments_cot.ipynb)\n.\n\n> N  OTE: This blog post aims to inform the community about GPT-4V\u2019s\n> performance, though the results might not be universally applicable. We\n> strongly advise conducting tests with similar questions on your own dataset\n> before drawing conclusions.\n\n#  Experiment:\n\nWhat we\u2019ll do is take an image of each of a bar chart, scatter plot, and table\nand analyze them by asking three types of questions.\n\n  1. General Question: Simply ask, \u201cAnalyse the image.\u201d \n  2. Specific Question: Specific Question the performance of a certain category by providing more details. \n  3. Chain of Thought Prompting: Use a step-by-step reasoning method to walk through the analysis. \n\nThese guidelines aim to test how different questioning techniques might\nimprove the precision of the information we gather from the images.\n\n#  Data for experiments:\n\nTo test out we selected the following images from [ Llama2\n](https://arxiv.org/pdf/2307.09288.pdf) and [ MistralAI\n](https://arxiv.org/pdf/2310.06825.pdf) papers.\n\n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1865, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e44d3a86-03ec-4479-be4e-8f2cf1868a29": {"__data__": {"id_": "e44d3a86-03ec-4479-be4e-8f2cf1868a29", "embedding": null, "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c44fa078-fabe-4a04-b679-98a3d6e9d1e7", "node_type": "4", "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "hash": "22b28ff47d1c2fbe7873fa6740c7b26b9dba50da3999fba93922d5e657fcfaa5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "192d1825-da2c-4b53-950a-9890490f33bf", "node_type": "1", "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "hash": "e260fbb6f436fc0a2b03d9cc6a1549a196f4f883181f33b775cb22b49e54be7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09bfdbab-8c90-409c-8d33-47b476827205", "node_type": "1", "metadata": {}, "hash": "c2a7c2b3bcd3415958c1af83ae89fe5f06b5704cf6990b2f4729718e616e36d0", "class_name": "RelatedNodeInfo"}}, "text": "1. Violation percentage of safety with different LLMs across categories. (Llama2 paper) \n  2. Llama2 vs Mistral model performances across various NLP tasks.(Mistral paper) \n  3. Performances of different LLMs across various NLP tasks. (Llama2 paper) \n\nLet\u2019s go through each of these data points one by one.\n\n#  Image 1 \u2014 Violation percentage of safety with different LLMs across\ncategories.\n\n#  General Question:\n\n**Question:** Analyse the Image.\n\n**Answer:**\n\n> The image you\u2019ve provided is a bar chart displaying data grouped into three\n> categories of online violations: hateful and harmful, illicit and criminal\n> activity, and unqualified advice. Each of these categories has a number in\n> parentheses, indicating the sample size of reported incidents for each type\n> of violation (395, 728, and 311, respectively). The x-axis of the chart\n> specifies various social media platforms or services, such as \u201cVideo\n> sharing\u201d, \u201cSocial networking\u201d, \u201cGaming\u201d, \u201cDating\u201d, \u201cForums & boards\u201d,\n> \u201cCommercial Websites\u201d, \u201cMedia sharing\u201d, \u201cP2P/File sharing\u201d, \u201cWiki\u201d, and\n> \u201cOther\u201d. It appears to measure how often these categories of violations\n> occur on each type of platform. The y-axis measures the percentage of\n> violations reported, ranging from 0% to 60%. Each platform/service has three\n> bars corresponding to the three violation categories, showing their\n> respective percentages. The bars also have error bars, which typically\n> represent the variability of the data, such as standard deviation, standard\n> error, or confidence interval, indicating the precision of the estimates.\n> The chart has a legend indicating the color corresponding to each of the\n> three categories of violations. This visual representation helps to compare\n> the prevalence of different types of violations across the different types\n> of online services. Without specific numbers, it\u2019s not possible to provide\n> exact percentages, but we can observe trends, such as: \u2014 \u201cForums & boards\u201d\n> and \u201cSocial networking\u201d platforms have notably higher percentages across all\n> three types of violations compared to other platforms. \u2014 \u201cCommercial\n> Websites\u201d seem to have lower percentages of reported hateful and harmful\n> activities and unqualified advice but higher percentages of illicit and\n> criminal activities when compared to other platforms like \u201cWiki\u201d or\n> \u201cP2P/File sharing\u201d. \u2014 \u201cGaming\u201d appears to have a moderate percentage of\n> hateful and harmful violations, lower levels of illicit and criminal\n> activity, and relatively higher levels of unqualified advice. Overall, this\n> chart is a visual tool that likely aims to inform about the prevalence of\n> certain types of online violations across different digital platforms.\n\n##  Observation:\n\nAs you can see though the categories hateful and harmful, illicit and criminal\nactivity, and unqualified advice but it hallucinated with x-axis values with \u2014\n\u201cVideo sharing\u201d, \u201cSocial networking\u201d, \u201cGaming\u201d, \u201cDating\u201d, \u201cForums & boards\u201d,\n\u201cCommercial Websites\u201d, \u201cMedia sharing\u201d, \u201cP2P/File sharing\u201d, \u201cWiki\u201d, and\n\u201cOther\u201d.\n\n#  Specific Questions:\n\n**Question:** Which model among llama2 and vicuna models does better in terms\nof violation percentages in Hateful and harmful category.\n\n**Answer:**\n\n> From the provided bar chart, we can analyze the violation percentage for\n> both the Llama2 and Vicuna models in the \u201cHateful and harmful\u201d category. To\n> determine which model does better (i.e., has a lower violation percentage),\n> you would look at the height of the bars corresponding to each model within\n> that category. In the \u201cHateful and harmful\u201d category (which is the first\n> group of bars on the left), you can compare the blue bar (representing\n> Llama2) to the light blue bar (representing Vicuna). The model with the\n> shorter bar in this category will have a lower violation percentage, and\n> hence, better performance with respect to minimizing hateful and harmful\n> content. Please note that I cannot physically see the chart, so I\u2019m unable\n> to specify which model has the lower violation percentage. If you provide\n> the actual percentages or describe the relative bar lengths for Llama2 and\n> Vicuna in the \u201cHateful and harmful\u201d category, I could then tell you which\n> model performs better in that respect.\n\n##  **Observation:**\n\nIt failed to accurately capture the information, mistakenly identifying the\nlight blue bar as representing Vicuna when, in fact, it is the light blue bar\nthat represents Llama2.\n\nLet us give more information on the question and see if it can improve the\nanswer.\n\n**Question:** In the image provided to you depicts about the violation rate\nperformance of various AI models across Hateful and harmful, Illicit and\ncriminal activity, Unqualified advice categories. Hateful and harmful category\nis in first column. Bars with light blue are with Llama2 model and dark blue\nare with Vicuna models. With this information, Can you compare about Llama2\nand Vicuna models in Hateful and harmful category.\n\n**Answer:**\n\n> In the image you provided, which shows a bar graph for the violation rate\n> performance of various AI models in the Hateful and harmful category, we can\n> analyze the light blue bars that represent the Llama2 model and the dark\n> blue bars that represent the Vicuna model. Based on the visual information\n> given, I can compare the two models within this category by observing the\n> heights of the light blue bars (Llama2) versus the heights of the dark blue\n> bars (Vicuna) for each subsection within the category. A lower bar indicates\n> a lower violation rate, which could be interpreted as better performance in\n> minimizing violations for the given criteria. Since I can\u2019t give you the\n> exact numbers or percentages, I would describe their relative performances.\n> It appears that for some subsections, the Llama2 bars are shorter than the\n> Vicuna bars, suggesting that the Llama2 model could have a lower violation\n> rate in those areas. Conversely, in other subsections, the Vicuna bars might\n> be shorter than the Llama2 bars, indicating a lower violation rate for the\n> Vicuna model in those areas. The exact subsections where one model\n> outperforms the other would depend on their relative bar heights, which\n> should be compared individually for the given information.\n\n##  **Observation:**\n\nIt did answer the question correctly.\n\n", "mimetype": "text/plain", "start_char_idx": 1865, "end_char_idx": 8191, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09bfdbab-8c90-409c-8d33-47b476827205": {"__data__": {"id_": "09bfdbab-8c90-409c-8d33-47b476827205", "embedding": null, "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c44fa078-fabe-4a04-b679-98a3d6e9d1e7", "node_type": "4", "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "hash": "22b28ff47d1c2fbe7873fa6740c7b26b9dba50da3999fba93922d5e657fcfaa5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e44d3a86-03ec-4479-be4e-8f2cf1868a29", "node_type": "1", "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "hash": "080c3825a06f5bed3e67b09c215aac90009d8b3dd8dba0b5e90bdd9d94d9be5b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc4d3507-759d-45cd-a9d8-aa16b220a4e1", "node_type": "1", "metadata": {}, "hash": "cad0b6edd235fd4b550e5039d7adabda60571839e5a9922a2f70b57108264cbc", "class_name": "RelatedNodeInfo"}}, "text": "#  Chain Of Thought Prompting:\n\n**Question:** Based on the image provided. Follow the steps and answer the\nquery \u2014 which model among llama2 and vicuna does better in terms of violation\npercentages in \u2018Hateful and harmful\u2019.\n\nExamine the Image: Look at the mentioned category in the query in the Image.\n\n", "mimetype": "text/plain", "start_char_idx": 8191, "end_char_idx": 8493, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc4d3507-759d-45cd-a9d8-aa16b220a4e1": {"__data__": {"id_": "bc4d3507-759d-45cd-a9d8-aa16b220a4e1", "embedding": null, "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c44fa078-fabe-4a04-b679-98a3d6e9d1e7", "node_type": "4", "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "hash": "22b28ff47d1c2fbe7873fa6740c7b26b9dba50da3999fba93922d5e657fcfaa5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09bfdbab-8c90-409c-8d33-47b476827205", "node_type": "1", "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "hash": "e82f08d2d2dbc691d0fa91ae53a02d3421f87a14153e228df4a23bb5aaf0c20e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4719f466-7177-4845-9a4f-833732001f7c", "node_type": "1", "metadata": {}, "hash": "92aa5adc389ab30061a70d5ff7ca7bec4a7e6c58bbf4bf057f47037a29fad923", "class_name": "RelatedNodeInfo"}}, "text": "Identify Relevant Data: Note the violation percentages.\n\nEvaluate: Compare if there is any comparison required as per the query.\n\nDraw a Conclusion: Now draw the conclusion based on the whole data.\n\n**Answer:**\n\n> Examine the Image: The image is a bar chart depicting violation percentages\n> in three categories: \u201cHateful and harmful,\u201d \u201cIllicit and criminal activity,\u201d\n> and \u201cUnqualified advice.\u201d Each category has two bars next to each other\n> representing two models named \u201cllama2\u201d and \u201cvicuna.\u201d Identify Relevant Data:\n> We are specifically interested in the \u201cHateful and harmful\u201d category, which\n> is the first set of bars on the left. The two bars indicate the violation\n> percentages for \u201cllama2\u201d and \u201cvicuna.\u201d Evaluate: By visually inspecting the\n> bar corresponding to \u201cHateful and harmful,\u201d we see that the blue bar\n> (indicating \u201cllama2\u201d) has a lower percentage than the red bar (indicating\n> \u201cvicuna\u201d). The exact percentages are not clear, but we can discern the\n> relative performance between the two. Draw a Conclusion: From the data\n> visible, \u201cllama2\u201d has a lower violation percentage than \u201cvicuna\u201d in the\n> \u201cHateful and harmful\u201d category, thereby doing better in that respect\n> according to the given image.\n\n##  Observation:\n\nWith chain of thought prompting it did hallicunate with bar colours but\nanswered correctly saying Llama2 has lower violation compared to vicuna in\nHateful and harmful though for a section Llama2 has higher violation compared\nto vicuna.\n\n#  Image2 \u2014 Llama2 vs Mistral model performances across various NLP tasks.\n\n#  General Question:\n\n**Question:** Analyze the Image.\n\n**Answer:**\n\n> The image appears to be a set of four line graphs that compare the\n> performance of different natural language processing (NLP) models across\n> four different metrics: Natural Language Understanding (NLU), Reasoning,\n> Knowledge, and Commonsense. The models compared are LLaMA 2, LLaMA 13B,\n> Mistral, and GPT-3. Each graph plots the performance metric (on the y-axis)\n> against the model size measured in billions of effective parameters (on the\n> x-axis). In general, these plots demonstrate that as the model size\n> increases, the performance on each metric improves. Here are specific\n> observations for each graph: 1. Natural Language Understanding (Top left):\n> All models show an increase in NLU performance as the model size increases.\n> Mistral is depicted as the highest-performing model at each size benchmark.\n> 2. Reasoning (Top right): Again, larger models perform better on reasoning\n> tasks, with Mistral consistently outperforming the other models mentioned.\n> 3. Knowledge (Bottom left): Similar to the NLU and reasoning tasks, the\n> trend is for better performance with larger models, with Mistral leading\n> across the different sizes. 4. Commonsense (Bottom right): This graph\n> follows the same trend with the Mistral model performing better at each size\n> level compared to the other models. Based on these graphs, Mistral appears\n> to be the top-performing model in all four metrics, suggesting that it might\n> be more efficient or effective than LLaMA 2, LLaMA 13B, and GPT-3, at least\n> within the scope of the parameters and tasks represented here. On a\n> technical note, it\u2019s important to mention that the y-axis shows different\n> scales for the metrics (ranging from 45\u201375% for NLU, Knowledge, and\n> Commonsense, and from 50\u201372% for Reasoning), which suggests that these tasks\n> might have different levels of difficulty or different benchmarks for\n> success.\n\n##  Observation:\n\nIt did answer the query but hallucinated with the NLU task which is the MMLU\ntask and assumed Mistral is available across all different model parameters.\n\n#  Specific Questions:\n\n**Question:** How well does mistral model compared to llama2 model?\n\n**Answer:**\n\n> The image you\u2019ve provided contains four graphs, each plotting different\n> performance metrics for evaluating language models. It compares two models:\n> Mistral and LLaMA2 across four different aspects \u2014 MMLU (Multi-tasking\n> multiple-choice), Reasoning, Knowledge, and Commonsense. In each graph, the\n> performance metric is plotted on the y-axis while the effective model size\n> in billion parameters is plotted on the x-axis. Here\u2019s a summary based on\n> the trends observed in the graphs: 1. **MMLU (Top Left Graph)**: LLaMA2\n> shows a steeper performance curve as the model size increases, starting at\n> around 61% for the smallest size and reaching upwards of 66% for the largest\n> model. Mistral also shows performance gains with model size but plateaus\n> earlier, reaching a level just shy of 62%. 2. **Reasoning (Top Right\n> Graph)**: In the reasoning task, LLaMA2 again has a steeper improvement\n> curve, starting from about 61% and surpassing 72% for the largest model.\n> Mistral, while improving, seems to plateau near 70% for the largest model.\n> 3. **Knowledge (Bottom Left Graph)**: This graph reflects a similar trend to\n> the previous ones, with LLaMA2 beginning at a lower performance around 46%\n> and eclipsing 52%. Mistral starts higher at around 48% and appears to\n> plateau near 52%. 4. **Commonsense (Bottom Right Graph)**: Here, LLaMA2\n> starts its performance at approximately 62% and reaches just above 66%.\n> Mistral seems to start at a slightly higher point than LLaMA2 but ends at a\n> similar level to LLaMA2\u2019s largest model. Overall, the LLaMA2 model appears\n> to show a greater degree of improvement in performance as the model size\n> increases compared to Mistral across these metrics. Meanwhile, Mistral\n> starts at a higher performance for some metrics but tends to plateau\n> earlier, suggesting that LLaMA2 may scale better with size in terms of\n> performance gains.\n\n##  Observation:\n\nIncorrect answers and percentages are not accurate enough and again assumed\nmistral is available across all parameter models.\n\n**Question:** Assuming mistral is available in 7B series. How well does\nmistral model compared to llama2 model?\n\n**Answer:**\n\n> This image appears to be a set of four graphs, each comparing the\n> performance of three different language models on various tasks: Mistral,\n> LLaMA-2, and a hypothetical \u201cSize 7B\u201d model. The graphs are labeled with\n> task-specific performance metrics: \u201cMLM U\u201d, \u201cReasoning\u201d, \u201cKnowledge\u201d, and\n> \u201cCommonsense\u201d. The x-axes on the graphs represent model size in terms of the\n> number of parameters, with three points that likely correspond to the sizes\n> of the models being compared. The y-axes represent performance as a\n> percentage, which could mean accuracy, precision, recall, or another\n> relevant performance metric depending on the specific task. The graphs\n> appear to show that Mistral performs better than LLaMA-2 and the Size 7B\n> model across all metrics. This indicates that within the context of these\n> measurements and tasks, Mistral is a stronger model. The exact nature of the\n> tasks or what \u201cMLM U\u201d, \u201cReasoning\u201d, \u201cKnowledge\u201d, and \u201cCommonsense\u201d\n> specifically refer to are not detailed in the image, but they likely\n> correspond to standard NLP tasks designed to test understanding of language,\n> ability to reason, knowledge recall, and commonsense reasoning,\n> respectively. The performance improvements are depicted as increasingly\n> significant with larger model sizes.\n\n", "mimetype": "text/plain", "start_char_idx": 8493, "end_char_idx": 15757, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4719f466-7177-4845-9a4f-833732001f7c": {"__data__": {"id_": "4719f466-7177-4845-9a4f-833732001f7c", "embedding": null, "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c44fa078-fabe-4a04-b679-98a3d6e9d1e7", "node_type": "4", "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "hash": "22b28ff47d1c2fbe7873fa6740c7b26b9dba50da3999fba93922d5e657fcfaa5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc4d3507-759d-45cd-a9d8-aa16b220a4e1", "node_type": "1", "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "hash": "91e36155dbdb800dae1c5e74d709933abbe4d071a704c392b0fcc388fd4a167a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16203309-295c-455e-a3da-8f5451dcce82", "node_type": "1", "metadata": {}, "hash": "4b99fbb4f12a53f312ec73c152c1227e7c8d6067cc2e6c31cf6553497638f728", "class_name": "RelatedNodeInfo"}}, "text": "##  Observation:\n\nNow by giving the detail that mistral is available in 7B series, it is able to\nanswer correctly.\n\n#  Chain of thought prompting:\n\n**Question:** Based on the image provided. Follow the steps and answer the\nquery \u2014 Assuming mistral is available in 7B series. How well does mistral\nmodel compared to llama2 model?.\n\nExamine the Image: Look at the mentioned category in the query in the Image.\n\n", "mimetype": "text/plain", "start_char_idx": 15757, "end_char_idx": 16166, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16203309-295c-455e-a3da-8f5451dcce82": {"__data__": {"id_": "16203309-295c-455e-a3da-8f5451dcce82", "embedding": null, "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c44fa078-fabe-4a04-b679-98a3d6e9d1e7", "node_type": "4", "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "hash": "22b28ff47d1c2fbe7873fa6740c7b26b9dba50da3999fba93922d5e657fcfaa5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4719f466-7177-4845-9a4f-833732001f7c", "node_type": "1", "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "hash": "bae33cbc93f2d526940f9551127fa11cedb95c368fd6cdb6c45685f3fc04c4af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa01bc3a-ef98-4438-9b6b-5a0a4073ef22", "node_type": "1", "metadata": {}, "hash": "0de3f84edd1da6870b174985134d0515a43f4f45d786a25b85a2247880c80fb2", "class_name": "RelatedNodeInfo"}}, "text": "Identify Relevant Data: Note the respective percentages.\n\nEvaluate: Compare if there is any comparision required as per the query.\n\n", "mimetype": "text/plain", "start_char_idx": 16166, "end_char_idx": 16298, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa01bc3a-ef98-4438-9b6b-5a0a4073ef22": {"__data__": {"id_": "aa01bc3a-ef98-4438-9b6b-5a0a4073ef22", "embedding": null, "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c44fa078-fabe-4a04-b679-98a3d6e9d1e7", "node_type": "4", "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "hash": "22b28ff47d1c2fbe7873fa6740c7b26b9dba50da3999fba93922d5e657fcfaa5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16203309-295c-455e-a3da-8f5451dcce82", "node_type": "1", "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "hash": "a211c02971d6de3aa300d0a8e65c5e0cc81535f9a28f2d7e2caa0747f3829153", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "801743ea-ad6a-4814-9da0-e8ff56843ecb", "node_type": "1", "metadata": {}, "hash": "f05b667d98a682bfa1e47d5066abc384444c4ed041954bfdc566223e5d419e14", "class_name": "RelatedNodeInfo"}}, "text": "Draw a Conclusion: Now draw the conclusion based on the whole data.\n\n**Answer:**\n\n> Examine the Image: The image contains four graphs, each graph compares the\n> performance of three different models \u2014 Llama 2, Mistral, and an unnamed\n> third model \u2014 across different metrics: mAP@L (mean Average Precision at L),\n> Reason@L (presumably a reasoning score at L), Knowledge@L, and\n> Comprehension@L. Each graph shows performance as a function of model size\n> (in terms of billion parameters). Identify Relevant Data: We need to focus\n> on the Mistral and Llama 2 models across all four graphs to extract the\n> relevant data. For mAP@L: \u2014 Llama 2 reaches above 65% when reaching 70\n> billion parameters. \u2014 Mistral starts at around 5 billion parameters with\n> about 48% and reaches almost 65% by the time it gets to 34 billion\n> parameters. For Reason@L: \u2014 Llama 2 reaches about 68% when reaching 70\n> billion parameters. \u2014 Mistral starts at around 5 billion parameters with\n> about 52% and reaches almost 68% by the time it gets to 34 billion\n> parameters. For Knowledge@L: \u2014 Llama 2 starts at about 45% with 1.3 billion\n> parameters and reaches just slightly under 60% by 70 billion parameters. \u2014\n> Mistral is not plotted in this graph, so no direct comparison can be made.\n> For Comprehension@L: \u2014 Llama 2 starts just above 40% with 1.3 billion\n> parameters and reaches just under 60% by 70 billion parameters. \u2014 Mistral\n> starts around 13 billion parameters with about 52% and continues above 62%\n> by 34 billion parameters. Evaluate: From the graphs, we can see that Mistral\n> and Llama 2 are very comparable in performance in the metrics where both are\n> shown. However, Mistral is not displayed on the Knowledge@L graph, so no\n> comparison can be made for that particular metric. Draw a Conclusion: In the\n> graphs where both models can be compared (mAP@L, Reason@L, and\n> Comprehension@L), Mistral appears to perform comparably to Llama 2, reaching\n> similar levels of performance at smaller parameter sizes relative to the\n> plotted sizes for Llama 2. This suggests that Mistral could be more\n> parameter-efficient. However, since there is no data for Mistral in the\n> Knowledge@L graph, we cannot draw a complete comparison\n\n##  Observation:\n\nThere is a hallucination with a number of model parameters and percentage\npoints though the final conclusion is partially correct.\n\n#  Image 3 \u2014 Performances of different LLMs across various NLP tasks.\n\n#  General Question:\n\n**Question:** Analyze the Image.\n\n**Answer:**\n\n> The image appears to be a table containing numerical data, which seems to be\n> a comparative analysis of various models across multiple parameters or\n> tests. Each row represents a different model or configuration as indicated\n> by names such as \u201cMPT\u201d and \u201cFalcon\u201d followed by a size specification like\n> \u201c70B,\u201d \u201c7B,\u201d or some other parameter. The columns are labeled with test\n> names or evaluation metrics, such as \u201cSize,\u201d \u201cAvg,\u201d \u201cAQuA-RAT,\u201d \u201cLogiQA,\u201d\n> \u201cL-SAT-AR,\u201d \u201cL-SAT-IC,\u201d \u201cL-SAT-RC,\u201d \u201cSAT-en (w/o Psg.),\u201d and \u201cSAT-math.\u201d The\n> data is likely related to performance scores of these models on these tests,\n> where higher numbers probably indicate better performance. Without\n> additional context, it is difficult to provide a comprehensive analysis of\n> this data, but it seems clear that it is intended to provide a performance\n> comparison between different models on various tasks, possibly in the field\n> of machine learning or artificial intelligence evaluations, where such\n> models are typically assessed on reasoning, comprehension, or problem-\n> solving capabilities. The \u201c70B\u201d and \u201c7B\u201d demarcations could refer to the\n> size of the model in terms of the number of parameters, commonly used in\n> assessing language models. A deeper evaluation would require further\n> insights into the specific nature of these tests and models, along with the\n> intended use-case for which they were being compared.\n\n##  **Observation:**\n\nIt did not analyze the image specifically but understood the overall data\npresent in the image to some extent.\n\n#  Specific Questions:\n\n**Question:** which model has higher performance in SAT-en?\n\n**Answer:**\n\n> In the table you\u2019ve provided, the models are compared based on their\n> performance on several benchmarks, including SAT-en (SAT Analogies, or \u201cSAT\u201d\n> in the table). To determine which model has higher performance specifically\n> on the SAT-en benchmark, you\u2019ll need to look at the corresponding column. In\n> the SAT-en column (second from the right), the two models with the highest\n> scores are: \u2014 LLaMA1 65B: with a score of 57.9 \u2014 LLaMA2 70B: with a score of\n> 63.4 Between these two, the LLaMA2 model with 70 billion parameters shows\n> the higher performance on the SAT-en benchmark with a score of 63.4.\n\n##  Observation:\n\nIt did answer correctly but the numbers are being hallucinated.\n\n**Question:** which model has higher performance in SAT-en in 7B series\nmodels?\n\n**Answer:**\n\n> The table you\u2019ve provided shows performance benchmark scores for different\n> model sizes across various AI models such as MPT, Falcon, and LLaMA on\n> different tasks, such as Aqua-RAT, LogiQA, LastAR, SAT-en, and SAT-math. For\n> the SAT-en task specifically, you asked which model in the 7B series has the\n> highest performance. To find out, we need to look at the column labeled\n> \u201cSAT-en (w/o Ps_8).\u201d In the 7B series of models, here are the scores: \u2014 MPT\n> 7B: 37.1 \u2014 Falcon 7B: 37.3 \u2014 LLaMA 7B: 63.9 \u2014 Model2 7B: 37.4 The LLaMA 7B\n> model outperforms the other 7B models on the SAT-en (w/o Ps_8) task with a\n> score of 63.9.\n\n##  **Observation:**\n\nIncorrect answer and recognized Llama series of models and values incorrectly.\n\n#  Chain of thought prompting:\n\n**Question:** Based on the image provided. Follow the steps and answer the\nquery \u2014 which model has higher performance in SAT-en in 7B series models?\n\nExamine the Image: Look at the mentioned category in the query in the Image.\n\n", "mimetype": "text/plain", "start_char_idx": 16298, "end_char_idx": 22248, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "801743ea-ad6a-4814-9da0-e8ff56843ecb": {"__data__": {"id_": "801743ea-ad6a-4814-9da0-e8ff56843ecb", "embedding": null, "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c44fa078-fabe-4a04-b679-98a3d6e9d1e7", "node_type": "4", "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "hash": "22b28ff47d1c2fbe7873fa6740c7b26b9dba50da3999fba93922d5e657fcfaa5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa01bc3a-ef98-4438-9b6b-5a0a4073ef22", "node_type": "1", "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "hash": "4606fe8a17abbe962fb386ff4ea459252f8b2e2e620e3d4f4367e19ff04bce52", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ec20dc4-df55-48bd-b572-9b002bfad77a", "node_type": "1", "metadata": {}, "hash": "cd91abc8705a1a177547c70622ad37fe25027415c5346ea1c71beee3dd6014e6", "class_name": "RelatedNodeInfo"}}, "text": "Identify Relevant Data: Note the respective percentages.\n\nEvaluate: Compare if there is any comparison required as per the query.\n\n", "mimetype": "text/plain", "start_char_idx": 22248, "end_char_idx": 22379, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ec20dc4-df55-48bd-b572-9b002bfad77a": {"__data__": {"id_": "1ec20dc4-df55-48bd-b572-9b002bfad77a", "embedding": null, "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c44fa078-fabe-4a04-b679-98a3d6e9d1e7", "node_type": "4", "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "hash": "22b28ff47d1c2fbe7873fa6740c7b26b9dba50da3999fba93922d5e657fcfaa5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "801743ea-ad6a-4814-9da0-e8ff56843ecb", "node_type": "1", "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "hash": "43493989d89676a7eb7ba1c18a23b298cbc340b31f90ac7822feda11b079f2c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38df1837-5840-4389-a189-7896b715a876", "node_type": "1", "metadata": {}, "hash": "169f420a5c93227ee3830e931c757e48fb86b5b500759ad90cc3f6855a2a4341", "class_name": "RelatedNodeInfo"}}, "text": "Draw a Conclusion: Now draw the conclusion based on the whole data.\n\n**Answer:**\n\n> To answer which model has higher performance in SAT-en in the 7B series\n> models, I will follow the provided steps: Examine the Image: The categories\n> in the image include model names and sizes, and performance scores for\n> various tasks, including the SAT-en category. Identify Relevant Data: The\n> relevant data for the SAT-en category are the percentages listed under it\n> for each 7B series model. Evaluate: I will compare the SAT-en percentages of\n> each 7B series model. Draw a Conclusion: The SAT-en scores for the 7B series\n> models are as follows: \u2014 MPT 7B: 63.1% \u2014 Falcon 7B: 73.4% \u2014 LLama 1 7B: No\n> data present for this category. \u2014 LLama 2 7B: 76.6% Based on the data, the\n> LLama 2 7B model has the highest SAT-en performance among the 7B series\n> models with a score of 76.6%.\n\n##  Observation:\n\nWith chain of thought prompting we are able to get the right conclusion though\nit should be noted that it picked up wrong values.\n\n", "mimetype": "text/plain", "start_char_idx": 22379, "end_char_idx": 23406, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "38df1837-5840-4389-a189-7896b715a876": {"__data__": {"id_": "38df1837-5840-4389-a189-7896b715a876", "embedding": null, "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c44fa078-fabe-4a04-b679-98a3d6e9d1e7", "node_type": "4", "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "hash": "22b28ff47d1c2fbe7873fa6740c7b26b9dba50da3999fba93922d5e657fcfaa5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ec20dc4-df55-48bd-b572-9b002bfad77a", "node_type": "1", "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}, "hash": "4c8fdb485b6439621009d3390dae13acdc96d965420b845a49e4286a4e570b7b", "class_name": "RelatedNodeInfo"}}, "text": "#  Final Observations:\n\nObservations made based on experiments on Hallucination and correctness.\n(Please note that these observations are specific to the images used and\ncannot be generalized, as they vary depending on the images.)\n\n#  Summary\n\nIn this blog post, we have showcased experiments ranging from general\ninquiries to systematic questions and chain of thought prompting techniques\nand observed Hallucination and correctness metrics.\n\nHowever, it should be noted that the outputs from GPT-4V can be somewhat\ninconsistent, and the levels of hallucination are slightly elevated.\nTherefore, repeating the same experiment could result in different answers,\nparticularly with generalized questions.\n\n", "mimetype": "text/plain", "start_char_idx": 23406, "end_char_idx": 24110, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "015e6aaa-24b6-4db3-90d3-419abafe4559": {"__data__": {"id_": "015e6aaa-24b6-4db3-90d3-419abafe4559", "embedding": null, "metadata": {"filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "294d42ff-4eee-42ef-8901-13835558b8ea", "node_type": "4", "metadata": {"filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "hash": "d6772b63975af35a2a7cc00ea18a7f02900a2dfb9c17e6b3713f476752c70b28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b54977b4-c6c6-4312-ab5f-b78ad510318d", "node_type": "1", "metadata": {}, "hash": "359e7a272631fb02c71e67cf4bf930f050f771f330a03b3910f66635aef5ffe9", "class_name": "RelatedNodeInfo"}}, "text": "A few days ago, we published a blog on [ Multi-Modal RAG ](/multi-modal-\nrag-621de7525fea) (Retrieval-Augmented Generation) and our latest (still in\nbeta) abstractions to help enable and simplify building them. In this post, we\nnow go over the important topic of how one can sensibly evaluate Multi-Modal\nRAG systems.\n\nA natural starting point is to consider how evaluation was done in\ntraditional, text-only RAG and then ask ourselves how this ought to be\nmodified to suit the multi-modal scenario (e.g., in text-only RAG, we use an\nLLM, but in multi-modal RAG we require a Large Multi-Modal Model or LMM for\nshort). This is exactly what we\u2019ll do next and as you\u2019ll see, the overarching\nevaluation framework stays the same as it was in the text-only RAG, requiring\nonly a few additions and modifications in order to make it more multi-modal\nappropriate.\n\n#  Primer: Multi-Modal RAG vs Text-Only RAG\n\nIllustration of text-only RAG versus multi-modal RAG. In multi-modal RAG,\nimages modality can show up in the user query, the retrieved context, as well\nas the final answer.\n\nLet\u2019s consider the main differences between multi-modal and text-only RAG.\nBelow are two tables that describe the RAG build considerations as well as\nquery-time pipeline and compares and contrasts multi-modal and text-only cases\nagainst them.\n\nTable 1: Build considerations for RAG systems and how they differ text-only\nversus multi-modal scenarios.  Table 2: The pipeline for querying a RAG and\nhow they differ text-only versus multi-modal scenarios.\n\n#  Evaluation Of Text-Only RAG\n\nFor text-only RAG, the standard approach is to separately consider the\nevaluation of two stages: Retrieval and Generation.\n\n**Retriever Evaluation:** are the retrieved documents relevant to the user\nquery?\n\nSome of the more popular metrics for retrieval evaluation include **recall,\nhit rate** , **mean reciprocal rank, mean average precision, and normalized\ndiscounted cumulative gain.** The first two of these metrics recall and hit\nrate, don\u2019t consider the position (or ranking) of the relevant documents,\nwhereas all the others do in their own respective ways.\n\n**Generator Evaluation:** does the response use the retrieved documents to\nsufficiently answer the user query?\n\nIn abstractive question-answering systems, like the kinds we\u2019re talking about\nin this blog, measuring the generated response is made more tricky due to the\nfact that there isn\u2019t just one way to sufficiently answer a query in written\nlanguage \u2014 there\u2019s plenty!\n\nSo, in this case, our measurement relies on subjective judgement, which can be\nperformed by humans, though this is costly and unscalable. An alternative\napproach, is to use an LLM judge to measure things like _relevancy_ and\n_faithfulness_ .\n\n  * Relevancy: considers textual context and evaluates how much the generated response matches the query. \n  * Faithfulness: evaluates how much the generated response matches the retrieved textual context. \n\nFor both of these, the retrieved context as well as the query and generated\nresponse are passed to the LLM judge. (This pattern of using an LLM to judge\nthe responses has been termed by some researchers of the space as LLM-As-A-\nJudge ( [ Zheng et al., 2023 ](https://arxiv.org/abs/2306.05685) ).)\n\nCurrently, the ` llama-index (v0.9.2) ` library supports hit-rate and mean\nreciprocal rank for retrieval evaluation, as well as relevancy, faithfulness\nand a few others for generator evaluation. (Check out our Evaluation guides in\nour [ docs\n](https://docs.llamaindex.ai/en/stable/optimizing/evaluation/evaluation.html)\n!", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3570, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b54977b4-c6c6-4312-ab5f-b78ad510318d": {"__data__": {"id_": "b54977b4-c6c6-4312-ab5f-b78ad510318d", "embedding": null, "metadata": {"filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "294d42ff-4eee-42ef-8901-13835558b8ea", "node_type": "4", "metadata": {"filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "hash": "d6772b63975af35a2a7cc00ea18a7f02900a2dfb9c17e6b3713f476752c70b28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "015e6aaa-24b6-4db3-90d3-419abafe4559", "node_type": "1", "metadata": {"filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "hash": "5386f089844ea834715bda68d20c08ca3a886e399e403902793aeb465a9d17a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbba97c1-b7ba-4d8a-882e-fa143267550f", "node_type": "1", "metadata": {}, "hash": "e9638c99821a8677fb808b8b81fb8cec070cee42023e1ca1e8ad662e53f2a07b", "class_name": "RelatedNodeInfo"}}, "text": ").\n\n#  Evaluation Of Multi-Modal RAG\n\nFor the multi-modal case, the evaluation can (and should) still be carried out\nwith respect to the different stages of retrieval and generation.\n\n**Separating Out Retrieval Evaluation For Text and Image Modalities**\n\nNow that retrieved documents can come in two forms, it would seem most\nsensible to consider computing the usual retrieval evaluation metrics\nseparately for images and text. In this way, you have more knowledge as to\nwhich aspect of the multi-modal retriever is working well and what isn\u2019t. One\ncan then apply a desired weighting scheme to establish a single aggregated\nretrieval score per metric.\n\nHit Rate Mean Reciprocal Rank Text 0.95 0.88 Images 0.88 0.75\n\nTable 3: Retrieval evaluation in multi-modal scenario.\n\n**Using Multi-Modal LLMs For Generator Evaluations (LMM-As-A-Judge)**\n\nMulti-modal models (i.e., LMMs) like OpenAI\u2019s GPT-4V or open-source\nalternatives like LLaVA are able to take in both input and image context to\nproduce an answer the user query. As in text-only RAG, we are also concerned\nabout the \u201crelevancy\u201d and \u201cfaithfulness\u201d of these generated answers. But in\norder to be able to compute such metrics in the multi-modal case, we would\nneed a judge model that is also able to take in the context images and text\ndata. Thus, in the multi-modal case, we adopt the LMM-As-A-Judge pattern in\norder to compute relevancy and faithfulness as well as other related metrics!\n\n  * Relevancy (multi-modal): considers **textual and visual context** and evaluates how much the generated response matches the query. \n  * Faithfulness (multi-modal): evaluates how much the generated response matches the retrieved **textual and visual context** . \n\nIf you want to test these out, then you\u2019re in luck as we\u2019ve recently released\nour beta Multi-Modal Evaluator abstractions! ", "mimetype": "text/plain", "start_char_idx": 3570, "end_char_idx": 5406, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fbba97c1-b7ba-4d8a-882e-fa143267550f": {"__data__": {"id_": "fbba97c1-b7ba-4d8a-882e-fa143267550f", "embedding": null, "metadata": {"filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "294d42ff-4eee-42ef-8901-13835558b8ea", "node_type": "4", "metadata": {"filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "hash": "d6772b63975af35a2a7cc00ea18a7f02900a2dfb9c17e6b3713f476752c70b28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b54977b4-c6c6-4312-ab5f-b78ad510318d", "node_type": "1", "metadata": {"filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "hash": "e57b28400202f8705e4de3c3571b0a1cec8fdc67c5165b76817ecba2698c9087", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "abf0869e-6647-4429-9896-d90ff94409c1", "node_type": "1", "metadata": {}, "hash": "96820a06d647728fe3a19df53b2719d700266b56504e8ef21f9f34c8d784bb25", "class_name": "RelatedNodeInfo"}}, "text": "See the code snippet below for\nhow one can use these abstractions to perform their respective evaluations on\na generated response to a given query.\n\n    \n    \n    from llama_index.evaluation.multi_modal import (\n    \tMultiModalRelevancyEvaluator,\n    \tMultiModalFaithfulnessEvaluator\n    )\n    from llama_index.multi_modal_llm import OpenAIMultiModal\n    \n    relevancy_judge = MultiModalRelevancyEvaluator(\n        multi_modal_llm=OpenAIMultiModal(\n            model=\"gpt-4-vision-preview\",\n            max_new_tokens=300,\n        )\n    )\n    \n    faithfulness_judge = MultiModalRelevancyEvaluator(\n        multi_modal_llm=OpenAIMultiModal(\n            model=\"gpt-4-vision-preview\",\n            max_new_tokens=300,\n        )\n    )\n    \n    # Generated response to a query and its retrieved context information\n    query = ...\n    response = ...\n    contexts = ...  # retrieved text contexts\n    image_paths = ...  # retrieved image contexts\n    \n    # Evaluations\n    relevancy_eval = relevancy_judge.evaluate(\n     query=query,\n     response=response,\n     contexts=contexts,\n     image_paths=image_paths\n    )\n    \n    faithfulness_eval = faithfulness_judge.evaluate(\n     query=query,\n     response=response,\n     contexts=contexts,\n     image_paths=image_paths\n    )\n\n#  A Few Important Remarks\n\nFirst, it is worth mentioning that using LLMs or LMMs to judge generated\nresponses has its drawbacks. These judges are generative models themselves and\ncan suffer from hallucinations and other inconsistencies. ", "mimetype": "text/plain", "start_char_idx": 5406, "end_char_idx": 6917, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abf0869e-6647-4429-9896-d90ff94409c1": {"__data__": {"id_": "abf0869e-6647-4429-9896-d90ff94409c1", "embedding": null, "metadata": {"filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "294d42ff-4eee-42ef-8901-13835558b8ea", "node_type": "4", "metadata": {"filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "hash": "d6772b63975af35a2a7cc00ea18a7f02900a2dfb9c17e6b3713f476752c70b28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fbba97c1-b7ba-4d8a-882e-fa143267550f", "node_type": "1", "metadata": {"filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}, "hash": "55f07bfb28a7a3d30f0f603a2e2cd81ddd4a1228ea3c59f9abac8f1ec435366d", "class_name": "RelatedNodeInfo"}}, "text": "Though studies have\nshown that strong LLMs can align to human judgments at a relatively high rate\n( [ Zheng et al., 2023 ](https://arxiv.org/abs/2306.05685) ), using them in\nproduction systems should be handled with higher standards of care. At time of\nwriting, there has no been study to show that strong LMMs can also align well\nto human judgements.\n\nSecondly, the evaluation of a generator touches mostly on the evaluation of\nits knowledge and reasoning capabilities. There are other important dimensions\non which to evaluate LLMs and LMMs, including Alignment and Safety \u2014 see [\nEvaluating LMMs: A Comprehensive Survey\n](https://arxiv.org/pdf/2310.19736.pdf) for more information.\n\n#  Go forth and evaluate\n\nIn this post, we covered how evaluation can be performed on multi-modal RAG\nsystems. We believe that separating out the retrieval evaluations per\nmodalities for increased visibility as well as the LMM-As-A-Judge represent a\nsensible extension of the evaluation framework for text-only RAG. We encourage\nyou to check out our practical notebook guides as well as docs for more\ninformation on how you can not only build Multi-Modal RAGs but also adequately\nevaluate them!\n\n  * [ Notebook guide for evaluating Multi-Modal RAG systems with LlamaIndex ](https://docs.llamaindex.ai/en/stable/examples/evaluation/multi_modal/multi_modal_rag_evaluation.html)\n  * [ Intro to Multi-Modal RAG ](/multi-modal-rag-621de7525fea)\n  * [ Docs/guides on Multi-Modal Abstractions ](https://docs.llamaindex.ai/en/stable/module_guides/models/multi_modal.html)\n\n", "mimetype": "text/plain", "start_char_idx": 6917, "end_char_idx": 8468, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a5e9221e-3013-40ca-8b49-f0847546dc00": {"__data__": {"id_": "a5e9221e-3013-40ca-8b49-f0847546dc00", "embedding": null, "metadata": {"filename": "improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.md", "extension": ".md", "title": "Improving Retrieval Performance by Fine-tuning Cohere Reranker with LlamaIndex", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d0ec3e5a-73d6-496c-96d5-f72de017f5c6", "node_type": "4", "metadata": {"filename": "improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.md", "extension": ".md", "title": "Improving Retrieval Performance by Fine-tuning Cohere Reranker with LlamaIndex", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b"}, "hash": "6d9d6d197d824d8a61003850fc5abc390ae6b815231ef8cfcb4808c136c4aa38", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a85a2431-4949-4b1d-9c35-b8c0b90fb203", "node_type": "1", "metadata": {}, "hash": "1a0130030bdd274f953f2429ef3099a264a68ea9fb0e998976d5d2310b73a228", "class_name": "RelatedNodeInfo"}}, "text": "#  Introduction:\n\nAchieving an efficient Retrieval-Augmented-Generation (RAG) pipeline is\nheavily dependent on robust retrieval performance. As we explored in our\nprevious [ blog post ](https://medium.com/llamaindex-blog/boosting-rag-\npicking-the-best-embedding-reranker-models-42d079022e83) , rerankers have a\nsignificant impact on boosting retrieval performance. But what if we could\ntake it a step further? What if our reranker was not just any reranker, but\none tuned specifically to our domain or dataset? Could this specialization\nenhance the retrieval performance even more?\n\nTo answer these questions, we turn to CohereAI\u2019s beta release of fine-tuning\nreranker(Custom reranker) models. By integrating these with LlamaIndex, we now\noffer the ability to build your very own Cohere custom reranker using our\nstreamlined process.\n\nIn this blog post, we\u2019ll guide you through the steps to create a Cohere custom\nreranker with LlamaIndex and evaluate the retrieval performance.\n\nFor a hands-on walkthrough, you can follow the tutorial on [ Google Colab\nNotebook ](https://colab.research.google.com/github/run-\nllama/llama_index/blob/main/docs/examples/finetuning/rerankers/cohere_custom_reranker.ipynb)\n.\n\nLet\u2019s start fine-tuning a Cohere reranker (custom reranker) with LlamaIndex.\n\n> N  OTE: This is a guide for fine-tuning a Cohere reranker (custom reranker).\n> The results presented at the end of this tutorial are unique to the chosen\n> dataset and parameters. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1467, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a85a2431-4949-4b1d-9c35-b8c0b90fb203": {"__data__": {"id_": "a85a2431-4949-4b1d-9c35-b8c0b90fb203", "embedding": null, "metadata": {"filename": "improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.md", "extension": ".md", "title": "Improving Retrieval Performance by Fine-tuning Cohere Reranker with LlamaIndex", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d0ec3e5a-73d6-496c-96d5-f72de017f5c6", "node_type": "4", "metadata": {"filename": "improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.md", "extension": ".md", "title": "Improving Retrieval Performance by Fine-tuning Cohere Reranker with LlamaIndex", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b"}, "hash": "6d9d6d197d824d8a61003850fc5abc390ae6b815231ef8cfcb4808c136c4aa38", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5e9221e-3013-40ca-8b49-f0847546dc00", "node_type": "1", "metadata": {"filename": "improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.md", "extension": ".md", "title": "Improving Retrieval Performance by Fine-tuning Cohere Reranker with LlamaIndex", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b"}, "hash": "0c9d6d2db65e4bd734c2c6e904092802805fdca7f3fc9a373eea26b955592400", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc447517-f0f7-4792-8fd3-b484c8495f2c", "node_type": "1", "metadata": {}, "hash": "5f8bfeb7a49ebe75e0ff5cc7f3dfd985f1df501c5c0357f89a2a516c2dc4402f", "class_name": "RelatedNodeInfo"}}, "text": "We suggest experimenting with your dataset and\n> various parameters before deciding to incorporate it into your RAG pipeline.\n\n#  Setting Up the Environment\n\n    \n    \n    !pip install llama-index cohere pypdf\n\n#  Setting Up the Keys\n\n    \n    \n    openai_api_key = 'YOUR OPENAI API KEY'\n    cohere_api_key = 'YOUR COHEREAI API KEY'\n    \n    import os\n    \n    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n    os.environ[\"COHERE_API_KEY\"] = cohere_api_key\n\n#  Download the Data\n\nWe will use Lyft 2021 10K SEC Filings for training and Uber 2021 10K SEC\nFilings for evaluation.\n\n    \n    \n    !mkdir -p 'data/10k/'\n    !wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'\n    !wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/lyft_2021.pdf' -O 'data/10k/lyft_2021.pdf'\n\n#  Load the Data\n\n    \n    \n    lyft_docs = SimpleDirectoryReader(input_files=['./data/10k/lyft_2021.pdf']).load_data()\n    uber_docs = SimpleDirectoryReader(input_files=['./data/10k/uber_2021.pdf']).load_data()\n\n#  Data Curation\n\n**Create Nodes.**\n\nThe [ documentation ](https://docs.cohere.com/docs/rerank-models) mentions\nthat Query + Relevant Passage/ Query + Hard Negatives should be less than 510\ntokens. To accommodate that we limit ` chunk_size ` to 400 tokens. (Each chunk\nwill eventually be treated as a Relevant Passage/ Hard Negative)\n\n    \n    \n    # Limit chunk size to 400\n    node_parser = SimpleNodeParser.from_defaults(chunk_size=400)\n    \n    # Create nodes\n    lyft_nodes = node_parser.get_nodes_from_documents(lyft_docs)\n    uber_nodes = node_parser.get_nodes_from_documents(uber_docs)\n\nWe will use gpt-4 to create questions from chunks.\n\n    \n    \n    llm = OpenAI(api_key=openai_api_key, temperature=0, model='gpt-4')\n\nPrompt to generate questions from each Node/ chunk.\n\n    \n    \n    # Prompt to generate questions\n    qa_generate_prompt_tmpl = \"\"\"\\\n    Context information is below.\n    \n    ---------------------\n    {context_str}\n    ---------------------\n    \n    Given the context information and not prior knowledge.\n    generate only questions based on the below query.\n    \n    You are a Professor. Your task is to setup \\\n    {num_questions_per_chunk} questions for an upcoming \\\n    quiz/examination. The questions should be diverse in nature \\\n    across the document. The questions should not contain options, not start with Q1/ Q2. \\\n    Restrict the questions to the context information provided.\\\n    \"\"\"\n\nIt expects a minimum of 256 (Query + Relevant passage) pairs with or without\nhard negatives for training and 64 pairs for validation. Please note that the\nvalidation is optional.\n\n**Training:** We use the first 256 nodes from Lyft for creating training\npairs.\n\n**Validation:** We will use the next 64 nodes from Lyft for validation.\n\n**Testing:** We will use the first 150 nodes from Uber.\n\n    \n    \n    # Training dataset\n    qa_dataset_lyft_train = generate_question_context_pairs(\n        lyft_nodes[:256], llm=llm, num_questions_per_chunk=1, qa_generate_prompt_tmpl=qa_generate_prompt_tmpl\n    )\n    \n    # Save [Optional]\n    qa_dataset_lyft_train.save_json(\"lyft_train_dataset.json\")\n    \n    # Validation dataset\n    qa_dataset_lyft_val = generate_question_context_pairs(\n        lyft_nodes[257:321], llm=llm, num_questions_per_chunk=1, qa_generate_prompt_tmpl=qa_generate_prompt_tmpl\n    )\n    \n    # Save [Optional]\n    qa_dataset_lyft_val.save_json(\"lyft_val_dataset.json\")\n    \n    # Testing dataset\n    qa_dataset_uber_val = generate_question_context_pairs(\n        uber_nodes[:150], llm=llm, num_questions_per_chunk=1, qa_generate_prompt_tmpl=qa_generate_prompt_tmpl\n    )\n    \n    # Save [Optional]\n    qa_dataset_uber_val.save_json(\"uber_val_dataset.json\")\n\nNow that we have compiled questions from each chunk, we will format the data\naccording to the specifications required for training and validation.\n\n#  Data Format and Requirements\n\nFor both training and validation, it currently accepts data in the format of\ntriplets, every row should have the following\n\n**query:** This represents the question or target.\n\n**relevant_passages:** This represents a list of documents or passages that\ncontain information that answers the query. For every query, there must be at\nleast one relevant_passage\n\n**hard_negatives:** This represents chunks or passages that don\u2019t contain\nanswers for the query. It should be noted that Hard negatives are optional but\nproviding at least ~5 hard negatives will lead to meaningful improvement.\n\nYou can check the [ documentation ](https://docs.cohere.com/docs/rerank-\nmodels) for more details.\n\nWe need to have an embedding model for creating hard negatives with a cosine\nsimilarity approach.\n\n    \n    \n    # Initialize the Cohere embedding model which we use it for creating Hard Negatives.\n    embed_model = CohereEmbedding(\n        cohere_api_key=cohere_api_key,\n        model_name=\"embed-english-v3.0\",\n        input_type=\"search_document\",\n    )\n\nLet\u2019s create 3 datasets.\n\n  1. ", "mimetype": "text/plain", "start_char_idx": 1467, "end_char_idx": 6534, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc447517-f0f7-4792-8fd3-b484c8495f2c": {"__data__": {"id_": "cc447517-f0f7-4792-8fd3-b484c8495f2c", "embedding": null, "metadata": {"filename": "improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.md", "extension": ".md", "title": "Improving Retrieval Performance by Fine-tuning Cohere Reranker with LlamaIndex", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d0ec3e5a-73d6-496c-96d5-f72de017f5c6", "node_type": "4", "metadata": {"filename": "improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.md", "extension": ".md", "title": "Improving Retrieval Performance by Fine-tuning Cohere Reranker with LlamaIndex", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b"}, "hash": "6d9d6d197d824d8a61003850fc5abc390ae6b815231ef8cfcb4808c136c4aa38", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a85a2431-4949-4b1d-9c35-b8c0b90fb203", "node_type": "1", "metadata": {"filename": "improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.md", "extension": ".md", "title": "Improving Retrieval Performance by Fine-tuning Cohere Reranker with LlamaIndex", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b"}, "hash": "8e9d858016b51c47f4f62a3259a51f35d01a07d6d4fffc70d95e3e036febd75c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c2d01c4-84de-495b-b34d-79f90ed3c3a1", "node_type": "1", "metadata": {}, "hash": "3972d908a604b4b09f44e4776d5b757d53aeeec1165e0e15246dff3f22bed3a6", "class_name": "RelatedNodeInfo"}}, "text": "Dataset without hard negatives. \n  2. Dataset with hard negatives selected at random. \n  3. Dataset with hard negatives selected based on cosine similarity. \n\n    \n    \n    # Train and val datasets without hard negatives.\n    generate_cohere_reranker_finetuning_dataset(\n        qa_dataset_lyft_train,\n        finetune_dataset_file_name = \"train.jsonl\"\n    )\n    \n    generate_cohere_reranker_finetuning_dataset(\n        qa_dataset_lyft_val,\n        finetune_dataset_file_name = \"val.jsonl\"\n    )\n    \n    # Train and val datasets with hard negatives selected at random.\n    generate_cohere_reranker_finetuning_dataset(\n        qa_dataset_lyft_train,\n        num_negatives = 5,\n        hard_negatives_gen_method = \"random\",\n        finetune_dataset_file_name = \"train_5_random.jsonl\",\n        embed_model = embed_model,\n    )\n    \n    generate_cohere_reranker_finetuning_dataset(\n        qa_dataset_lyft_val,\n        num_negatives = 5,\n        hard_negatives_gen_method = \"random\",\n        finetune_dataset_file_name = \"val_5_random.jsonl\",\n        embed_model = embed_model,\n    )\n    \n    # Train and val datasets with hard negatives selected based on cosine similarity.\n    generate_cohere_reranker_finetuning_dataset(\n        qa_dataset_lyft_train,\n        num_negatives = 5,\n        hard_negatives_gen_method = \"cosine_similarity\",\n        finetune_dataset_file_name = \"train_5_cosine_similarity.jsonl\",\n        embed_model = embed_model,\n    )\n    \n    generate_cohere_reranker_finetuning_dataset(\n        qa_dataset_lyft_val,\n        num_negatives = 5,\n        hard_negatives_gen_method = \"cosine_similarity\",\n        finetune_dataset_file_name = \"val_5_cosine_similarity.jsonl\",\n        embed_model = embed_model,\n    )\n\n#  Fine-tuning Reranker (Custom Reranker)\n\nWith our training and validation datasets ready, we\u2019re set to proceed with the\ntraining process. Be aware that this training is expected to take\napproximately 25 to 45 minutes.\n\n    \n    \n    # Reranker model with 0 hard negatives.\n    finetune_model_no_hard_negatives = CohereRerankerFinetuneEngine(\n        train_file_name=\"train.jsonl\",\n        val_file_name=\"val.jsonl\",\n        model_name=\"lyft_reranker_0_hard_negatives1\",\n        model_type=\"RERANK\",\n        base_model=\"english\",\n        api_key = cohere_api_key\n    )\n    finetune_model_no_hard_negatives.finetune()\n    \n    # Reranker model with 5 hard negatives selected at random\n    finetune_model_random_hard_negatives = CohereRerankerFinetuneEngine(\n        train_file_name=\"train_5_random.jsonl\",\n        val_file_name=\"val_5_random.jsonl\",\n        model_name=\"lyft_reranker_5_random_hard_negatives1\",\n        model_type=\"RERANK\",\n        base_model=\"english\",\n    )\n    finetune_model_random_hard_negatives.finetune()\n    \n    # Reranker model with 5 hard negatives selected based on cosine similarity\n    finetune_model_cosine_hard_negatives = CohereRerankerFinetuneEngine(\n        train_file_name=\"train_5_cosine_similarity.jsonl\",\n        val_file_name=\"val_5_cosine_similarity.jsonl\",\n        model_name=\"lyft_reranker_5_cosine_hard_negatives1\",\n        model_type=\"RERANK\",\n        base_model=\"english\",\n    )\n    finetune_model_cosine_hard_negatives.finetune()\n\nOnce the jobs are submitted, you can check the training status in the ` models\n` section of the [ dashboard ](https://dashboard.cohere.com/models) . You can\ncheck the status of the job in the dashboard and you should see an image\nsomething similar to the following one.\n\nYou then need to get the Cohere Reranker model for testing.\n\n    \n    \n    reranker_base = CohereRerank(top_n=5)\n    reranker_model_0 = finetune_model_no_hard_negatives.get_finetuned_model(\n        top_n=5\n    )\n    reranker_model_5_random = (\n        finetune_model_random_hard_negatives.get_finetuned_model(top_n=5)\n    )\n    reranker_model_5_cosine = (\n        finetune_model_cosine_hard_negatives.get_finetuned_model(top_n=5)\n    )\n\n#  Testing\n\nWe will conduct tests on the first 150 nodes from Uber using the following\ndifferent rerankers.\n\n  1. ", "mimetype": "text/plain", "start_char_idx": 6534, "end_char_idx": 10563, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c2d01c4-84de-495b-b34d-79f90ed3c3a1": {"__data__": {"id_": "7c2d01c4-84de-495b-b34d-79f90ed3c3a1", "embedding": null, "metadata": {"filename": "improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.md", "extension": ".md", "title": "Improving Retrieval Performance by Fine-tuning Cohere Reranker with LlamaIndex", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d0ec3e5a-73d6-496c-96d5-f72de017f5c6", "node_type": "4", "metadata": {"filename": "improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.md", "extension": ".md", "title": "Improving Retrieval Performance by Fine-tuning Cohere Reranker with LlamaIndex", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b"}, "hash": "6d9d6d197d824d8a61003850fc5abc390ae6b815231ef8cfcb4808c136c4aa38", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc447517-f0f7-4792-8fd3-b484c8495f2c", "node_type": "1", "metadata": {"filename": "improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.md", "extension": ".md", "title": "Improving Retrieval Performance by Fine-tuning Cohere Reranker with LlamaIndex", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b"}, "hash": "295a8275df450f172f30b5f6da8b7fc0427f1c4129d74acdf23e2ef9eaef6ad6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0a032b4-e6fb-4b96-8c24-19d369466173", "node_type": "1", "metadata": {}, "hash": "91cd0b222d83b1ec3fa018167c0f6c777dc700bf72e9c71721a09ba40a06fc85", "class_name": "RelatedNodeInfo"}}, "text": "Without Reranker. \n  2. Cohere Reranker. \n  3. Fine-tuned reranker (Custom reranker) without hard negatives. \n  4. Fine-tuned reranker (Custom reranker) with hard negatives selected at random. \n  5. Fine-tuned reranker (Custom reranker) with hard negatives selected based on cosine similarity. \n\nLet\u2019s define the rerankers.\n\n    \n    \n    RERANKERS = {\n        \"WithoutReranker\": \"None\",\n        \"CohereRerank\": reranker_base,\n        \"CohereRerank_0\": reranker_model_0,\n        \"CohereRerank_5_random\": reranker_model_5_random,\n        \"CohereRerank_5_cosine\": reranker_model_5_cosine,\n    }\n\nCreate an Index and Retriever for evaluation purposes.\n\n    \n    \n    # Initialize the Cohere embedding model, `input_type` is different for indexing and retrieval.\n    index_embed_model = CohereEmbedding(\n        cohere_api_key=cohere_api_key,\n        model_name=\"embed-english-v3.0\",\n        input_type=\"search_document\",\n    )\n    \n    query_embed_model = CohereEmbedding(\n        cohere_api_key=cohere_api_key,\n        model_name=\"embed-english-v3.0\",\n        input_type=\"search_query\",\n    )\n    \n    service_context_index = ServiceContext.from_defaults(llm=None, embed_model=index_embed_model)\n    service_context_query = ServiceContext.from_defaults(llm=None, embed_model=query_embed_model)\n    \n    vector_index = VectorStoreIndex(uber_nodes[:150], service_context=service_context_index)\n    vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=10, service_context=service_context_query)\n\nDefine a function to display the results\n\n    \n    \n    def display_results(embedding_name, reranker_name, eval_results):\n        \"\"\"Display results from evaluate.\"\"\"\n    \n        metric_dicts = []\n        for eval_result in eval_results:\n            metric_dict = eval_result.metric_vals_dict\n            metric_dicts.append(metric_dict)\n    \n        full_df = pd.DataFrame(metric_dicts)\n    \n        hit_rate = full_df[\"hit_rate\"].mean()\n        mrr = full_df[\"mrr\"].mean()\n    \n        metric_df = pd.DataFrame(\n            {\"Embedding\": [embedding_name], \"Reranker\": [reranker_name], \"hit_rate\": [hit_rate], \"mrr\": [mrr]}\n        )\n    \n        return metric_df\n\nLoop over different rerankers and evaluate retrieval performance using Custom\nRetriever.\n\n    \n    \n    results_df = pd.DataFrame()\n    \n    embed_name = 'CohereEmbedding'\n    \n    # Loop over rerankers\n    for rerank_name, reranker in RERANKERS.items():\n    \n        print(f\"Running Evaluation for Reranker: {rerank_name}\")\n    \n        # Define Retriever\n        class CustomRetriever(BaseRetriever):\n            \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"\n    \n            def __init__(\n                self,\n                vector_retriever: VectorIndexRetriever,\n            ) -&gt; None:\n                \"\"\"Init params.\"\"\"\n    \n                self._vector_retriever = vector_retriever\n    \n            def _retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:\n                \"\"\"Retrieve nodes given query.\"\"\"\n    \n                retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n    \n                if reranker != 'None':\n                    retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n                else:\n                    retrieved_nodes = retrieved_nodes[:5]\n    \n                return retrieved_nodes\n    \n            async def _aretrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:\n                \"\"\"Asynchronously retrieve nodes given query.\n                \"\"\"\n                return self._retrieve(query_bundle)\n    \n            async def aretrieve(self, str_or_query_bundle: QueryType) -&gt; List[NodeWithScore]:\n                if isinstance(str_or_query_bundle, str):\n                    str_or_query_bundle = QueryBundle(str_or_query_bundle)\n                return await self._aretrieve(str_or_query_bundle)\n    \n        custom_retriever = CustomRetriever(vector_retriever)\n    \n        retriever_evaluator = RetrieverEvaluator.from_metric_names(\n            [\"mrr\", \"hit_rate\"], retriever=custom_retriever\n        )\n        eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset_uber_val)\n    \n        current_df = display_results(embed_name, rerank_name, eval_results)\n        results_df = pd.concat([results_df, current_df], ignore_index=True)\n\n#  Results:\n\nFrom the above table (1- without reranker, 2 \u2014 with base cohere reranker, 3\u20135:\nFine-tuned rerankers (Custom rerankers)), we can see that the Fine-tuned\nrerankers (custom rerankers) have resulted in performance improvements. It\u2019s\ncrucial to note that the choice of the optimal number of hard negatives, as\nwell as the decision between random or cosine sampling, should be grounded in\nempirical evidence. This guide offers a structured approach for improving\nretrieval systems through the fine-tuning of the Cohere re-ranker.\n\n#  Summary:\n\nIn this blog post, we\u2019ve demonstrated fine-tuning a Cohere reranker (custom\nreranker) using LlamaIndex, which has improved retrieval performance metrics.\nWe eagerly anticipate the community\u2019s use of these abilities to boost their\nretrieval efficiency within RAG pipelines. ", "mimetype": "text/plain", "start_char_idx": 10563, "end_char_idx": 15777, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0a032b4-e6fb-4b96-8c24-19d369466173": {"__data__": {"id_": "d0a032b4-e6fb-4b96-8c24-19d369466173", "embedding": null, "metadata": {"filename": "improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.md", "extension": ".md", "title": "Improving Retrieval Performance by Fine-tuning Cohere Reranker with LlamaIndex", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d0ec3e5a-73d6-496c-96d5-f72de017f5c6", "node_type": "4", "metadata": {"filename": "improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.md", "extension": ".md", "title": "Improving Retrieval Performance by Fine-tuning Cohere Reranker with LlamaIndex", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b"}, "hash": "6d9d6d197d824d8a61003850fc5abc390ae6b815231ef8cfcb4808c136c4aa38", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c2d01c4-84de-495b-b34d-79f90ed3c3a1", "node_type": "1", "metadata": {"filename": "improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.md", "extension": ".md", "title": "Improving Retrieval Performance by Fine-tuning Cohere Reranker with LlamaIndex", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b"}, "hash": "2e893ae1e9666e0cab59454d0ff1665dbf984674c6eb9d2317c80e8f5eb367fe", "class_name": "RelatedNodeInfo"}}, "text": "Additionally, there is room for\nadvancement in selecting hard negatives, and we invite the community to\ncontribute.\n\n", "mimetype": "text/plain", "start_char_idx": 15777, "end_char_idx": 15894, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc6dde66-2c7b-48d3-a4b2-e73b88f13dfe": {"__data__": {"id_": "dc6dde66-2c7b-48d3-a4b2-e73b88f13dfe", "embedding": null, "metadata": {"filename": "announcing-llamaindex-0-9-719f03282945.md", "extension": ".md", "title": "Announcing LlamaIndex 0.9", "date": "Nov 15, 2023", "url": "https://www.llamaindex.ai/blog/announcing-llamaindex-0-9-719f03282945"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4043b8fe-9170-4006-a512-902fbe629cbd", "node_type": "4", "metadata": {"filename": "announcing-llamaindex-0-9-719f03282945.md", "extension": ".md", "title": "Announcing LlamaIndex 0.9", "date": "Nov 15, 2023", "url": "https://www.llamaindex.ai/blog/announcing-llamaindex-0-9-719f03282945"}, "hash": "5e49a3415c2d7d43f6bcb8e6a604fc8666886f8a2a73294112a3dd03ecdcf2f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "def7a4bb-3a1a-4781-9470-e40c53c84d92", "node_type": "1", "metadata": {}, "hash": "b4921b4b18279c3a734540cb4c8aab0d1bcd67c561c71dbe44170352a168f63e", "class_name": "RelatedNodeInfo"}}, "text": "Our hard-working team is delighted to announce our latest major release,\nLlamaIndex 0.9! You can get it right now:\n\n` pip install --upgrade llama_index `\n\nIn LlamaIndex v0.9, we are taking the time to refine several key aspects of\nthe user experience, including token counting, text splitting, and more!\n\nAs part of this, there are some new features and minor changes to current\nusage that developers should be aware of:\n\n  * New ` IngestionPipline ` concept for ingesting and transforming data \n  * Data ingestion and transforms are now automatically cached \n  * Updated interface for node parsing/text splitting/metadata extraction modules \n  * Changes to the default tokenizer, as well as customizing the tokenizer \n  * Packaging/Installation changes with PyPi (reduced bloat, new install options) \n  * More predictable and consistent import paths \n  * Plus, in beta: MultiModal RAG Modules for handling text and images! \n\nHave questions or concerns? ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 954, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "def7a4bb-3a1a-4781-9470-e40c53c84d92": {"__data__": {"id_": "def7a4bb-3a1a-4781-9470-e40c53c84d92", "embedding": null, "metadata": {"filename": "announcing-llamaindex-0-9-719f03282945.md", "extension": ".md", "title": "Announcing LlamaIndex 0.9", "date": "Nov 15, 2023", "url": "https://www.llamaindex.ai/blog/announcing-llamaindex-0-9-719f03282945"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4043b8fe-9170-4006-a512-902fbe629cbd", "node_type": "4", "metadata": {"filename": "announcing-llamaindex-0-9-719f03282945.md", "extension": ".md", "title": "Announcing LlamaIndex 0.9", "date": "Nov 15, 2023", "url": "https://www.llamaindex.ai/blog/announcing-llamaindex-0-9-719f03282945"}, "hash": "5e49a3415c2d7d43f6bcb8e6a604fc8666886f8a2a73294112a3dd03ecdcf2f4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc6dde66-2c7b-48d3-a4b2-e73b88f13dfe", "node_type": "1", "metadata": {"filename": "announcing-llamaindex-0-9-719f03282945.md", "extension": ".md", "title": "Announcing LlamaIndex 0.9", "date": "Nov 15, 2023", "url": "https://www.llamaindex.ai/blog/announcing-llamaindex-0-9-719f03282945"}, "hash": "53b35c952f58107ce3d5ea33c90c0679c768f3c0d01420bc75c2f7ac6126aba8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16c592b8-2b29-4a3c-b6f9-62b014f337f0", "node_type": "1", "metadata": {}, "hash": "316c4ee3c2f821091e3b97ad443309d08d5a942b3b63f12e3f632ef5d9e02dfe", "class_name": "RelatedNodeInfo"}}, "text": "You can [ report an issue\n](https://github.com/run-llama/llama_index/issues) on GitHub or [ ask a\nquestion on our Discord ](https://discord.com/invite/eN6D2HQ4aX) !\n\nRead on for more details on our new features and changes.\n\n#  IngestionPipeline \u2014 New abstraction for purely ingesting data\n\nSometimes, all you want is to ingest and embed nodes from data sources, for\ninstance if your application allows users to upload new data. New in\nLlamaIndex V0.9 is the concept of an ` IngestionPipepline ` .\n\nAn ` IngestionPipeline ` uses a new concept of ` Transformations ` that are\napplied to input data.\n\nWhat is a ` Transformation ` though? It could be a:\n\n  * text splitter \n  * node parser \n  * metadata extractor \n  * embeddings model \n\nHere\u2019s a quick example of the basic usage pattern:\n\n    \n    \n    from llama_index import Document\n    from llama_index.embeddings import OpenAIEmbedding\n    from llama_index.text_splitter import SentenceSplitter\n    from llama_index.extractors import TitleExtractor\n    from llama_index.ingestion import IngestionPipeline, IngestionCache\n    \n    pipeline = IngestionPipeline(\n        transformations=[\n            SentenceSplitter(chunk_size=25, chunk_overlap=0),\n            TitleExtractor(),\n            OpenAIEmbedding(),\n        ]\n    )\n    nodes = pipeline.run(documents=[Document.example()])\n\n#  Transformation Caching\n\nEach time you run the same ` IngestionPipeline ` object, it caches a hash of\nthe input nodes + transformations and the output of that transformation for\neach transformation in the pipeline.\n\nIn subsequent runs, if there is a cache hit, that transformation will be\nskipped and the cached result will be used instead. The greatly speeds up\nduplicate runs, and can help improve iteration times when deciding which\ntransformations to use.\n\nHere\u2019s an example with a saving and loading a local cache:\n\n    \n    \n    from llama_index import Document\n    from llama_index.embeddings import OpenAIEmbedding\n    from llama_index.text_splitter import SentenceSplitter\n    from llama_index.extractors import TitleExtractor\n    from llama_index.ingestion import IngestionPipeline, IngestionCache\n    \n    pipeline = IngestionPipeline(\n        transformations=[\n            SentenceSplitter(chunk_size=25, chunk_overlap=0),\n            TitleExtractor(),\n            OpenAIEmbedding(),\n        ]\n    )\n    # will only execute full pipeline once\n    nodes = pipeline.run(documents=[Document.example()])\n    nodes = pipeline.run(documents=[Document.example()])\n    # save and load\n    pipeline.cache.persist(\"./test_cache.json\")\n    new_cache = IngestionCache.from_persist_path(\"./test_cache.json\")\n    new_pipeline = IngestionPipeline(\n        transformations=[\n            SentenceSplitter(chunk_size=25, chunk_overlap=0),\n            TitleExtractor(),\n        ],\n        cache=new_cache,\n    )\n    # will run instantly due to the cache\n    nodes = pipeline.run(documents=[Document.example()])\n\nAnd here\u2019s another example using Redis as a cache and Qdrant as a vector\nstore. Running this will directly insert the nodes into your vector store and\ncache each transformation step in Redis.\n\n    \n    \n    from llama_index import Document\n    from llama_index.embeddings import OpenAIEmbedding\n    from llama_index.text_splitter import SentenceSplitter\n    from llama_index.extractors import TitleExtractor\n    from llama_index.ingestion import IngestionPipeline, IngestionCache\n    from llama_index.ingestion.cache import RedisCache\n    from llama_index.vector_stores.qdrant import QdrantVectorStore\n    \n    import qdrant_client\n    client = qdrant_client.QdrantClient(location=\":memory:\")\n    vector_store = QdrantVectorStore(client=client, collection_name=\"test_store\")\n    pipeline = IngestionPipeline(\n        transformations=[\n            SentenceSplitter(chunk_size=25, chunk_overlap=0),\n            TitleExtractor(),\n            OpenAIEmbedding(),\n        ],\n        cache=IngestionCache(cache=RedisCache(), collection=\"test_cache\"),\n        vector_store=vector_store,\n    )\n    # Ingest directly into a vector db\n    pipeline.run(documents=[Document.example()])\n    # Create your index\n    from llama_index import VectorStoreIndex\n    index = VectorStoreIndex.from_vector_store(vector_store)\n\n#  Custom Transformations\n\nImplementing custom transformations is easy! Let\u2019s add a transform to remove\nspecial characters from the text before calling embeddings.\n\nThe only real requirement for transformations is that they must accept a list\nof nodes and return a list of nodes.\n\n    \n    \n    import re\n    from llama_index import Document\n    from llama_index.embeddings import OpenAIEmbedding\n    from llama_index.text_splitter import SentenceSplitter\n    from llama_index.ingestion import IngestionPipeline\n    from llama_index.schema import TransformComponent\n    \n    class TextCleaner(TransformComponent):\n      def __call__(self, nodes, **kwargs):\n        for node in nodes:\n          node.text = re.sub(r'[^0-9A-Za-z ]', \"\", node.text)\n        return nodes\n    pipeline = IngestionPipeline(\n        transformations=[\n            SentenceSplitter(chunk_size=25, chunk_overlap=0),\n            TextCleaner(),\n            OpenAIEmbedding(),\n        ],\n    )\n    nodes = pipeline.run(documents=[Document.example()])\n\n#  Node Parsing/Text Splitting \u2014 Flattened and Simplified Interface\n\nWe\u2019ve made our interface for parsing and splitting text a lot cleaner.\n\n#  Before:\n\n    \n    \n    from llama_index.node_parser import SimpleNodeParser\n    from llama_index.node_parser.extractors import (\n    \tMetadataExtractor, TitleExtractor\n    ) \n    from llama_index.text_splitter import SentenceSplitter\n    \n    node_parser = SimpleNodeParser(\n      text_splitter=SentenceSplitter(chunk_size=512),\n      metadata_extractor=MetadataExtractor(\n      extractors=[TitleExtractor()]\n     ),\n    )\n    nodes = node_parser.get_nodes_from_documents(documents)\n\n#  After:\n\n    \n    \n    from llama_index.text_splitter import SentenceSplitter\n    from llama_index.extractors import TitleExtractor \n    \n    node_parser = SentenceSplitter(chunk_size=512)\n    extractor = TitleExtractor()\n    \n    # use transforms directly\n    nodes = node_parser(documents)\n    nodes = extractor(nodes)\n\nPreviously, the ` NodeParser ` object in LlamaIndex had become extremely\nbloated, holding both text splitters and metadata extractors, which caused\nboth pains for users when changing these components, and pains for us trying\nto maintain and develop them.\n\nIn V0.9, we have **flattened** the entire interface into a single `\nTransformComponent ` abstraction, so that these transformations are easier to\nsetup, use, and customize.\n\nWe\u2019ve done our best to minimize the impacts on users, but the main thing to\nnote is that ` **SimpleNodeParser** ` **has been removed** , and other node\nparsers and text splitters have been elevated to have the same features, just\nwith different parsing and splitting techniques.\n\nAny old imports of ` SimpleNodeParser ` will redirect to the most equivalent\nmodule, ` SentenceSplitter ` .\n\nFurthermore, the wrapper object ` **MetadataExtractor** ` **has been removed**\n, in favour of using extractors directly.\n\nFull documentation for all this can be found below:\n\n  * [ Node Parsers and Text Splitters ](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules.html)\n  * [ Metadata Extractors ](https://docs.llamaindex.ai/en/stable/module_guides/indexing/metadata_extraction.html)\n\n#  Tokenization and Token Counting \u2014 Improved defaults and Customization\n\nA big pain point in LlamaIndex previously was tokenization. Many components\nused a non-configurable ` gpt2 ` tokenizer for token counting, causing\nheadaches for users using non-OpenAI models, or even some hacky fixes [ like\nthis ](https://github.com/run-\nllama/llama_index/blob/336a88db4f13cfc598c473f9b5a3bc073b5d7ef4/llama_index/indices/prompt_helper.py#L119)\nfor OpenAI models too!\n\nIn LlamaIndex V0.9, this **global tokenizer is now configurable and defaults\nto the CL100K tokenizer** to match our default GPT-3.5 LLM.\n\nThe single requirement for a tokenizer is that it is a callable function, that\ntakes a string, and returns a list.\n\nSome examples of configuring this are below:\n\n    \n    \n    from llama_index import set_global_tokenizer\n    \n    # tiktoken\n    import tiktoken\n    set_global_tokenizer(\n      tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n    )\n    # huggingface\n    from transformers import AutoTokenizer\n    set_global_tokenizer(\n      AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\").encode\n    )\n\nFurthermore, the ` TokenCountingHandler ` has gotten an upgrade with better\ntoken counting, as well as using token counts from API responses directly when\navailable.\n\n#  Packaging \u2014 Reduced Bloat\n\nIn an effort to modernize the packaging of LlamaIndex, V0.9 also comes with\nchanges to installation.\n\n", "mimetype": "text/plain", "start_char_idx": 954, "end_char_idx": 9827, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16c592b8-2b29-4a3c-b6f9-62b014f337f0": {"__data__": {"id_": "16c592b8-2b29-4a3c-b6f9-62b014f337f0", "embedding": null, "metadata": {"filename": "announcing-llamaindex-0-9-719f03282945.md", "extension": ".md", "title": "Announcing LlamaIndex 0.9", "date": "Nov 15, 2023", "url": "https://www.llamaindex.ai/blog/announcing-llamaindex-0-9-719f03282945"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4043b8fe-9170-4006-a512-902fbe629cbd", "node_type": "4", "metadata": {"filename": "announcing-llamaindex-0-9-719f03282945.md", "extension": ".md", "title": "Announcing LlamaIndex 0.9", "date": "Nov 15, 2023", "url": "https://www.llamaindex.ai/blog/announcing-llamaindex-0-9-719f03282945"}, "hash": "5e49a3415c2d7d43f6bcb8e6a604fc8666886f8a2a73294112a3dd03ecdcf2f4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "def7a4bb-3a1a-4781-9470-e40c53c84d92", "node_type": "1", "metadata": {"filename": "announcing-llamaindex-0-9-719f03282945.md", "extension": ".md", "title": "Announcing LlamaIndex 0.9", "date": "Nov 15, 2023", "url": "https://www.llamaindex.ai/blog/announcing-llamaindex-0-9-719f03282945"}, "hash": "ecd8022e3053c4a65b36b0f623b0d15ea50cbc8b5a781d6d3155b1f83d3df99e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b16f0bc6-0c24-4392-b001-071fc5cbaec3", "node_type": "1", "metadata": {}, "hash": "36c2193d0eecc0fec1186a04f951de1f587854a283aa1e8758225fc874c0e18c", "class_name": "RelatedNodeInfo"}}, "text": "The biggest change here is that ` LangChain ` is now an optional package, and\nwill not be installed by default.\n\nTo install ` LangChain ` as part of your llama-index installation you can\nfollow the example below. There are also other installation options depending\non your needs, and we are welcoming further contributions to the extras in the\nfuture.\n\n    \n    \n    # installs langchain\n    pip install llama-index[langchain]\n     \n    # installs tools needed for running local models\n    pip install llama-index[local_models]\n    \n    # installs tools needed for postgres\n    pip install llama-index[postgres]\n    \n    # combinations!\n    pip isntall llama-index[local_models,postgres]\n\n**If you were previously importing** ` **langchain** ` **modules** in your\ncode, please update your project packaging requirements appropriately.\n\n#  Import Paths \u2014 More Consistent and Predictable\n\nWe are making two changes to our import paths:\n\n  1. We\u2019ve removed uncommonly used imports from the root level to make importing ` llama_index ` faster \n  2. We now have a consistent policy for making \u201cuser-facing\u201d concepts import-able at level-1 modules. \n\n    \n    \n    from llama_index.llms import OpenAI, ...\n    from llama_index.embeddings import OpenAIEmbedding, ...\n    from llama_index.prompts import PromptTemplate, ...\n    from llama_index.readers import SimpleDirectoryReader, ...\n    from llama_index.text_splitter import SentenceSplitter, ...\n    from llama_index.extractors import TitleExtractor, ...\n    from llama_index.vector_stores import SimpleVectorStore, ...\n\nWe still expose some of the most commonly used modules at the root level.\n\n    \n    \n    from llama_index import SimpleDirectoryReader, VectorStoreIndex, ...\n\n#  MultiModal RAG\n\nGiven the recent announcements of the GPT-4V API, multi-modal use cases are\nmore accessible than ever before.\n\nTo help users use these features, we\u2019ve started to introduce a number of new\nmodules to help support use-cases for MultiModal RAG:\n\n  * MultiModal LLMs (GPT-4V, Llava, Fuyu, etc.) \n  * MultiModal Embeddings (i.e clip) for join image-text embedding/retrieval \n  * MultiModal RAG, combining indexes and query engines \n\nOur documentation has a [ full guide to multi-modal retrieval\n](https://docs.llamaindex.ai/en/latest/examples/multi_modal/gpt4v_multi_modal_retrieval.html)\n.\n\n#  Thanks for all your support!\n\n", "mimetype": "text/plain", "start_char_idx": 9827, "end_char_idx": 12193, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b16f0bc6-0c24-4392-b001-071fc5cbaec3": {"__data__": {"id_": "b16f0bc6-0c24-4392-b001-071fc5cbaec3", "embedding": null, "metadata": {"filename": "announcing-llamaindex-0-9-719f03282945.md", "extension": ".md", "title": "Announcing LlamaIndex 0.9", "date": "Nov 15, 2023", "url": "https://www.llamaindex.ai/blog/announcing-llamaindex-0-9-719f03282945"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4043b8fe-9170-4006-a512-902fbe629cbd", "node_type": "4", "metadata": {"filename": "announcing-llamaindex-0-9-719f03282945.md", "extension": ".md", "title": "Announcing LlamaIndex 0.9", "date": "Nov 15, 2023", "url": "https://www.llamaindex.ai/blog/announcing-llamaindex-0-9-719f03282945"}, "hash": "5e49a3415c2d7d43f6bcb8e6a604fc8666886f8a2a73294112a3dd03ecdcf2f4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16c592b8-2b29-4a3c-b6f9-62b014f337f0", "node_type": "1", "metadata": {"filename": "announcing-llamaindex-0-9-719f03282945.md", "extension": ".md", "title": "Announcing LlamaIndex 0.9", "date": "Nov 15, 2023", "url": "https://www.llamaindex.ai/blog/announcing-llamaindex-0-9-719f03282945"}, "hash": "a40f965a2d53b00b2eded10d6aa30b8054733fd2dc04a79d36c921422648291e", "class_name": "RelatedNodeInfo"}}, "text": "As an open-source project we couldn\u2019t exist without our [ hundreds of\ncontributors ](https://github.com/run-llama/llama_index/graphs/contributors) .\nWe are so grateful for them and the support of the hundreds of thousands of\nLlamaIndex users around the world. See you on the Discord!\n\n", "mimetype": "text/plain", "start_char_idx": 12193, "end_char_idx": 12478, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b4eb05c-b17b-4d06-868b-0b11ec8d3674": {"__data__": {"id_": "7b4eb05c-b17b-4d06-868b-0b11ec8d3674", "embedding": null, "metadata": {"filename": "create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191.md", "extension": ".md", "title": "create-llama, a command line tool to generate LlamaIndex apps", "date": "Nov 14, 2023", "url": "https://www.llamaindex.ai/blog/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "822085ed-0a59-4fff-ac44-48beec94b85a", "node_type": "4", "metadata": {"filename": "create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191.md", "extension": ".md", "title": "create-llama, a command line tool to generate LlamaIndex apps", "date": "Nov 14, 2023", "url": "https://www.llamaindex.ai/blog/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191"}, "hash": "d4e785376399408a25c7cf38a6a0981644b95e52e4acf5fabac5f6cb80af0abe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8f3a337-bf5d-48a8-9498-c59c12b222db", "node_type": "1", "metadata": {}, "hash": "12b71ce7b6ca275203e0b3689039365f7de7935acbdd99cf7d72cdddf1d7504e", "class_name": "RelatedNodeInfo"}}, "text": "Introducing ` [ create-llama ](https://www.npmjs.com/package/create-llama) ` ,\nthe easiest way to get started with LlamaIndex!\n\n_Update 2023\u201311\u201320: we now have a_ [ _guide to deploying your create-llama\napps_ ](/shipping-your-retrieval-augmented-generation-app-to-production-with-\ncreate-llama-7bbe43b6287d) _!_\n\nWant to use the power of LlamaIndex to load, index and chat with your data\nusing LLMs like GPT-4? It just got a lot easier! We\u2019ve created a simple to use\ncommand-line tool that will generate a full-stack app just for you \u2014 just\nbring your own data! To get started, run:\n\n    \n    \n    npx create-llama\n\nThe app will then ask you a series of questions about what kind of app you\nwant. You\u2019ll need to supply your own [ OpenAI API key\n](https://platform.openai.com/api-keys) (or you can customize it to use a\ndifferent LLM), and make a few decisions.\n\n#  How does it get my data?\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 891, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8f3a337-bf5d-48a8-9498-c59c12b222db": {"__data__": {"id_": "b8f3a337-bf5d-48a8-9498-c59c12b222db", "embedding": null, "metadata": {"filename": "create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191.md", "extension": ".md", "title": "create-llama, a command line tool to generate LlamaIndex apps", "date": "Nov 14, 2023", "url": "https://www.llamaindex.ai/blog/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "822085ed-0a59-4fff-ac44-48beec94b85a", "node_type": "4", "metadata": {"filename": "create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191.md", "extension": ".md", "title": "create-llama, a command line tool to generate LlamaIndex apps", "date": "Nov 14, 2023", "url": "https://www.llamaindex.ai/blog/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191"}, "hash": "d4e785376399408a25c7cf38a6a0981644b95e52e4acf5fabac5f6cb80af0abe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b4eb05c-b17b-4d06-868b-0b11ec8d3674", "node_type": "1", "metadata": {"filename": "create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191.md", "extension": ".md", "title": "create-llama, a command line tool to generate LlamaIndex apps", "date": "Nov 14, 2023", "url": "https://www.llamaindex.ai/blog/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191"}, "hash": "53cd90672756daa6bc2fdfede026d3fbbe6f1168565759a63a5ef9fa9e4c74a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad53a63e-01fe-44c5-b758-26938a14f9ed", "node_type": "1", "metadata": {}, "hash": "b9593e862cbb79155b4cb09462ce56d4989717b2a78d1dd79e7b8f52e3c175b5", "class_name": "RelatedNodeInfo"}}, "text": "The generated app has a ` data ` folder where you can put as many files as you\nwant; the app will automatically index them at build time and after that you\ncan quickly chat with them. If you\u2019re using LlamaIndex.TS as the back-end (see\nbelow), you\u2019ll be able to ingest PDF, text, CSV, Markdown, Word and HTML\nfiles. If you\u2019re using the Python backend, you can read even more types,\nincluding audio and video files!\n\n#  Technical details\n\nThe front-end it generates is a Next.js application, with your choice of [\nshadcn/ui ](https://ui.shadcn.com/) or vanilla HTML and CSS for styling.\n\nFor the back-end, you have 3 options:\n\n  * **Next.js** : if you select this option, you\u2019ll have a full stack Next.js application that you can deploy to a host like [ Vercel ](https://vercel.com/) in just a few clicks. This uses [ LlamaIndex.TS ](https://ts.llamaindex.ai/) , our TypeScript library. \n  * **Express** : if you want a more traditional Node.js application you can generate an Express backend. This also uses LlamaIndex.TS. \n  * **Python FastAPI** : if you select this option you\u2019ll get a backend powered by the [ llama-index python package ](https://pypi.org/project/llama-index/) , which you can deploy to a service like [ Render ](https://render.com/) or [ fly.io ](https://fly.io/) . \n\nThere are a couple of other questions you\u2019ll be asked:\n\n  * Streaming or non-streaming: if you\u2019re not sure, you\u2019ll probably want a streaming backend. \n  ", "mimetype": "text/plain", "start_char_idx": 891, "end_char_idx": 2332, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad53a63e-01fe-44c5-b758-26938a14f9ed": {"__data__": {"id_": "ad53a63e-01fe-44c5-b758-26938a14f9ed", "embedding": null, "metadata": {"filename": "create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191.md", "extension": ".md", "title": "create-llama, a command line tool to generate LlamaIndex apps", "date": "Nov 14, 2023", "url": "https://www.llamaindex.ai/blog/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "822085ed-0a59-4fff-ac44-48beec94b85a", "node_type": "4", "metadata": {"filename": "create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191.md", "extension": ".md", "title": "create-llama, a command line tool to generate LlamaIndex apps", "date": "Nov 14, 2023", "url": "https://www.llamaindex.ai/blog/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191"}, "hash": "d4e785376399408a25c7cf38a6a0981644b95e52e4acf5fabac5f6cb80af0abe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8f3a337-bf5d-48a8-9498-c59c12b222db", "node_type": "1", "metadata": {"filename": "create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191.md", "extension": ".md", "title": "create-llama, a command line tool to generate LlamaIndex apps", "date": "Nov 14, 2023", "url": "https://www.llamaindex.ai/blog/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191"}, "hash": "c92bf02ff2b6ae87f170a42d8a1c0b864d0192927563569f77265015104d5010", "class_name": "RelatedNodeInfo"}}, "text": "* ` SimpleChatEngine ` or ` ContextChatEngine ` : the ContextChatEngine is the one that uses your data. If you just want to chat with GPT, you can use the ` SimpleChatEngine ` . \n\n#  Go forth and customize!\n\nOnce you\u2019ve got your app up and running, you can customize it to your heart\u2019s\ncontent! By default, for cost reasons, the app will use GPT-3.5-Turbo. If\nyou\u2019d like to use GPT-4 you can configure that by modifying the file `\napp/api/chat/llamaindex-stream.ts ` (in the Next.js backend) or you can\nconfigure it to use a different LLM entirely! LlamaIndex has integrations with\ndozens of LLMs, both APIs and local.\n\n", "mimetype": "text/plain", "start_char_idx": 2332, "end_char_idx": 2952, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "206f525c-3084-440b-a80c-aa8daf5ec796": {"__data__": {"id_": "206f525c-3084-440b-a80c-aa8daf5ec796", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-11-14-dad06ae4284a.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201314", "date": "Nov 14, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-14-dad06ae4284a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "163f57e3-00fa-46b9-af06-92027d7739ab", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-11-14-dad06ae4284a.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201314", "date": "Nov 14, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-14-dad06ae4284a"}, "hash": "51be5609b1e2814f99951b512d12c4ec416fbf941e2e02db3ce820b2ac19b373", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c333a53-d5ef-4abd-b2d4-049b72fbebb9", "node_type": "1", "metadata": {}, "hash": "b702512111501c84f321fa5b2e0301093cf5bf90f9535db2f8d9ac086cb4a898", "class_name": "RelatedNodeInfo"}}, "text": "Hello Llama Friends\n\nLlamaIndex is 1 year old this week! To celebrate, we\u2019re taking a stroll down\nmemory lane on our [ blog ](/llamaindex-turns-1-f69dcdd45fe3) with twelve\nmilestones from our first year. Be sure to check it out.\n\nLast week we had a blast with all the new things from OpenAI Dev day to learn\nand explore at LlamaIndex. There was a [ special edition newsletter\n](/llamaindex-news-special-edition-openai-developer-day-e955f16db4e2) with the\nthings we released the same day as the conference, but this week\u2019s newsletter\nis full of follow-up releases and explorations \u2014 don\u2019t miss our slide deck\nsumming up all the new features!\n\nAs always, if you\u2019ve got a cool project or a video to share we\u2019d love to see\nit! Just drop us a line at [ news@llamaindex.ai ](mailto:news@llamaindex.ai) .\n\n**First, the highlights:**\n\n  1. **Multi-Modal RAG Stack:** we unveiled Multi-Modal RAG ****for complex Q&A on documents and images, with new text/image queries and retrieval solutions. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/gpt4v_multi_modal_retrieval.ipynb) , [ Tweet ](https://x.com/jerryjliu0/status/1723076174698672417?s=20) , [ Blog post ](/multi-modal-rag-621de7525fea) . \n  2. **OpenAIAssistantAgent Abstractions:** we released new abstractions to connect OpenAI Assistant API with any vector database. [ Docs ](https://t.co/W78d2WCpnn) , [ Tweet ](https://twitter.com/jerryjliu0/status/1722276583883657388?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1476, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c333a53-d5ef-4abd-b2d4-049b72fbebb9": {"__data__": {"id_": "2c333a53-d5ef-4abd-b2d4-049b72fbebb9", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-11-14-dad06ae4284a.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201314", "date": "Nov 14, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-14-dad06ae4284a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "163f57e3-00fa-46b9-af06-92027d7739ab", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-11-14-dad06ae4284a.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201314", "date": "Nov 14, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-14-dad06ae4284a"}, "hash": "51be5609b1e2814f99951b512d12c4ec416fbf941e2e02db3ce820b2ac19b373", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "206f525c-3084-440b-a80c-aa8daf5ec796", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-11-14-dad06ae4284a.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201314", "date": "Nov 14, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-14-dad06ae4284a"}, "hash": "ec8a30481151a06e0ec8edaa185bab27c81cf321dad94a576423edd66f67dafa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "388ef834-e2bc-4373-ad73-c45a5660a97e", "node_type": "1", "metadata": {}, "hash": "66b6c1bba404a0c43bdbc83905c28ae261d4ac62ac7f328bf3b1d70df712ae4b", "class_name": "RelatedNodeInfo"}}, "text": "3. **Parallel Function Calling:** we enhanced our data extraction and tool execution using OpenAI\u2019s parallel function calling. [ Tweet ](https://x.com/llama_index/status/1722686015276753073?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 1476, "end_char_idx": 1677, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "388ef834-e2bc-4373-ad73-c45a5660a97e": {"__data__": {"id_": "388ef834-e2bc-4373-ad73-c45a5660a97e", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-11-14-dad06ae4284a.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201314", "date": "Nov 14, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-14-dad06ae4284a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "163f57e3-00fa-46b9-af06-92027d7739ab", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-11-14-dad06ae4284a.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201314", "date": "Nov 14, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-14-dad06ae4284a"}, "hash": "51be5609b1e2814f99951b512d12c4ec416fbf941e2e02db3ce820b2ac19b373", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c333a53-d5ef-4abd-b2d4-049b72fbebb9", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-11-14-dad06ae4284a.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201314", "date": "Nov 14, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-14-dad06ae4284a"}, "hash": "183ec4d4d3906451d7b0f7cd8d71f77c491d1e6622722bcdba03cda8c8fd8b88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e360c4b2-5438-4fe5-93c8-29dc68a0f7cb", "node_type": "1", "metadata": {}, "hash": "5ceba1fab5e511c65bc3ba7a8bf0d68c98fe13b0042abeb3d2a3e92af98bb705", "class_name": "RelatedNodeInfo"}}, "text": "4. **MechGPT Project:** Prof. [ **Markus J. Buehler** ](https://twitter.com/ProfBuehlerMIT) \u2019s work merges LLM fine-tuning with knowledge graphs for scientific discovery. [ Tweet ](https://x.com/llama_index/status/1723379654550245719?s=20) , [ Paper ](https://t.co/l8J55BqUfn) . \n  ", "mimetype": "text/plain", "start_char_idx": 1677, "end_char_idx": 1959, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e360c4b2-5438-4fe5-93c8-29dc68a0f7cb": {"__data__": {"id_": "e360c4b2-5438-4fe5-93c8-29dc68a0f7cb", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-11-14-dad06ae4284a.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201314", "date": "Nov 14, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-14-dad06ae4284a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "163f57e3-00fa-46b9-af06-92027d7739ab", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-11-14-dad06ae4284a.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201314", "date": "Nov 14, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-14-dad06ae4284a"}, "hash": "51be5609b1e2814f99951b512d12c4ec416fbf941e2e02db3ce820b2ac19b373", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "388ef834-e2bc-4373-ad73-c45a5660a97e", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-11-14-dad06ae4284a.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201314", "date": "Nov 14, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-14-dad06ae4284a"}, "hash": "5ec65c8ab410e6745bb4f9ec3fd00d34f2313bf7a2d8188ceabe57468df51eb0", "class_name": "RelatedNodeInfo"}}, "text": "5. **Feature Slide Deck:** Released a [ slide deck ](https://docs.google.com/presentation/d/1i1bUDWXeCYPd6O8pio57ST6AQIuSTWXM3rvvkvrBpBM/edit#slide=id.p) with 10+ new features and guides post-OpenAI updates. \n\n**Feature Releases and Enhancements:**\n\n  * We introduced a multi-modal RAG stack for complex document and image QA, featuring text/image queries, joint text/ image embeddings, and versatile storage and retrieval options. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/gpt4v_multi_modal_retrieval.ipynb) , [ Tweet ](https://x.com/jerryjliu0/status/1723076174698672417?s=20) , [ Blog post ](/multi-modal-rag-621de7525fea) . \n  * We now offer experimental GPT-4-vision support in [ chat.llamaindex.ai ](http://chat.llamaindex.ai) . Users can now upload images for enhanced chatbot interactions. [ Tweet ](https://x.com/llama_index/status/1723120887988384177?s=20) . \n  * We integrated OpenAI\u2019s parallel function calling for efficient extraction of structured data from unstructured text and improving tool execution with agents. [ Tweet ](https://x.com/llama_index/status/1722686015276753073?s=20) . \n  * We introduced ` **OpenAIAssistantAgent** ` abstractions for seamless connection of OpenAI Assistants API with your chosen vector database. [ Docs ](https://t.co/W78d2WCpnn) , [ Tweet ](https://twitter.com/jerryjliu0/status/1722276583883657388?s=20) . \n  * We introduced a new agent leveraging OpenAI Assistants API with features like in-house code interpretation, file retrieval, and function calling for external tools integration. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/agent/openai_assistant_agent.ipynb) , [ Tweet ](https://x.com/llama_index/status/1721949693754917035?s=20) . \n\n**** Demos:\n\n  * MechGPT by Professor [ **Markus J. Buehler** ](https://twitter.com/ProfBuehlerMIT) showcases the integration of LLM fine-tuning and knowledge graph creation with LlamaIndex, leading to interesting insights in cross-disciplinary scientific research and hypothesis generation. [ Tweet ](https://x.com/llama_index/status/1723379654550245719?s=20) , [ Paper ](https://t.co/l8J55BqUfn) . \n\n**Guides:**\n\n  * We released a concise [ slide deck ](https://docs.google.com/presentation/d/1i1bUDWXeCYPd6O8pio57ST6AQIuSTWXM3rvvkvrBpBM/edit#slide=id.p) that aggregates over 10+ newly shipped features, guides, and analyses, complete with links to accompanying notebooks for developer use based on OpenAI\u2019s recent updates. \n  * We also released a full [ cookbook ](https://docs.llamaindex.ai/en/latest/examples/agent/openai_assistant_query_cookbook.html) showing how you can build advanced RAG with the Assistants API \u2014 beyond just using the in-house Retrieval tool. \n  * We produced a [ guide ](https://docs.llamaindex.ai/en/latest/examples/agent/openai_retrieval_benchmark.html) on evaluating the OpenAI Assistant API vs RAG with LlamaIndex. \n  * Here\u2019s a [ guide ](https://github.com/run-llama/llama_index/blob/main/docs/examples/response_synthesizers/long_context_test.ipynb) on evaluating How well long-context LLMs (gpt-4-turbo, claude-2) recall specifics in BIG documents? (>= 250k tokens). \n  * Here\u2019s another [ guide ](https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/openai_json_vs_function_calling.ipynb) that highlights how function calling simplifies structured data extraction, while JSON mode ensures format correctness without schema enforcement. \n  * Finally, we released a guide to craft a GPT Builder, enabling an agent to programmatically construct another task-specific agent. This builder streamlines the creation of systems for specific functions. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/agent/agent_builder.ipynb) , [ Tweet ](https://x.com/jerryjliu0/status/1721639447207583882?s=20) . \n\n**Tutorials:**\n\n  * [ **Bhavesh Bhat** ](https://twitter.com/_bhaveshbhatt) gave us a [ tutorial ](https://twitter.com/_bhaveshbhatt/status/1721551513103839392) on How to Chat with YouTube Videos Using LlamaIndex. \n  * [ David Garnitz ](https://twitter.com/DGarnitz) \u2019s tutorial blog explores the use of VectorFlow alongside ArizePhoenix, Weaviate, and LlamaIndex to manage large data sets. \n  * [ Harshad Suryawanshi ](https://harshadsuryawanshi.medium.com/) \u2019s [ tutorial ](/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566) covers Building My Own ChatGPT Vision with PaLM, KOSMOS-2 and LlamaIndex. \n  * [ Sudarshan Koirala ](https://twitter.com/mesudarshan) \u2019s made a [ tutorial ](https://www.youtube.com/watch?v=LRP-0iSVQaA) on Creating OpenAI Assistant Agent with LlamaIndex. \n  * Our own [ Ravi Theja ](https://twitter.com/ravithejads) released his [ tutorial ](/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83) on Boosting RAG with Embeddings & Rerankers. \n\n**** Webinars:\n\n  * Check out our [ webinar ](https://www.youtube.com/watch?v=upPK6pRbZYQ) with Dan Shipper, CEO of [ every ](http://every.to/) to talk about the implications of OpenAI\u2019s release updates. \n  * A second [ webinar ](https://www.youtube.com/watch?v=rBpZvMAim5E) with Victoria Lin, author of the RA-DIT paper on Fine-tuning + RAG. \n  * Last but not least, [ Mayo Oshin ](https://twitter.com/mayowaoshin) \u2019s [ webinar ](https://www.youtube.com/watch?v=xT6JpDELKPg&t=61s) with [ Jerry Liu ](https://twitter.com/jerryjliu0) on How to Analyze Tables In Large Financial Reports Using GPT-4. \n\n", "mimetype": "text/plain", "start_char_idx": 1959, "end_char_idx": 7425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f62b90cb-44d9-4643-b66f-15b2cb832f11": {"__data__": {"id_": "f62b90cb-44d9-4643-b66f-15b2cb832f11", "embedding": null, "metadata": {"filename": "llamaindex-turns-1-f69dcdd45fe3.md", "extension": ".md", "title": "LlamaIndex turns 1!", "date": "Nov 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-turns-1-f69dcdd45fe3"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "15668294-2e00-4059-8c83-b1f6c9f7058a", "node_type": "4", "metadata": {"filename": "llamaindex-turns-1-f69dcdd45fe3.md", "extension": ".md", "title": "LlamaIndex turns 1!", "date": "Nov 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-turns-1-f69dcdd45fe3"}, "hash": "6cc0ca0c9276f05c4089dead2e76a08086cfd4dd7b27b16167c0e62ca9f2ffde", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93fa4e33-c1cc-4b3b-9b32-e98bc20264bb", "node_type": "1", "metadata": {}, "hash": "cfb4e2b913860884c7ecad62f981390b7de3b9f36089e7f99e368e304c94b4da", "class_name": "RelatedNodeInfo"}}, "text": "It\u2019s our birthday! One year ago, Jerry pushed his [ first commit\n](https://github.com/run-\nllama/llama_index/tree/2e62c6987808797611e9bb7c1ae8c86e72a88727) to GPT Index,\nthe project that would become LlamaIndex. It worked with GPT-3, the state of\nthe art model available at the time. That initial version was very simple, but\nthe problem statement \u2014 and the solution \u2014 remain the same:\n\n> _one fundamental limitation of GPT-3 is the context size [\u2026] the ability to\n> feed \u201cknowledge\u201d to GPT-3 is mostly limited to this limited prompt size [\u2026]\n> But what if GPT-3 can have access to potentially a much larger database of\n> knowledge[\u2026]?_\n\nTwelve months have passed and there\u2019s been a tsunami of new developments in\nthe world of generative AI and LLMs, but the reason LlamaIndex was invented\nremains: even the most sophisticated model isn\u2019t trained on **your** data,\nwhich can be locked behind an API or in a SQL database, and even the latest\nGPT-4-Turbo context size of 128,000 tokens isn\u2019t enough to hold even a\nrelatively modest dataset. Retrieval-Augmented Generation (RAG) is here to\nstay.\n\n#  Big numbers\n\nAt just 1 year old, LlamaIndex has gotten very big. How big? Here\u2019s some\nnumbers:\n\n  * Over [ 450 contributors ](https://github.com/run-llama/llama_index/graphs/contributors) to our open-source library! \n  * Nearly 3,000 open-source projects [ depend on LlamaIndex ](https://github.com/run-llama/llama_index/network/dependents) ! \n  * Nearly 4,000 members in our Discord ( [ come join us! ](https://discord.com/invite/eN6D2HQ4aX) ) \n  * 47,000 lines of Python in the library! (Don\u2019t worry, it\u2019s still just [ 0.5MB to download ](https://pypi.org/project/llama-index/#files) ) \n  * Nearly [ 900,000 downloads every month ](https://pypistats.org/packages/llama-index) ! \n  * RAG deployed among [ popular ](https://openbb.co/blog/breaking-barriers-with-openbb-and-llamaIndex) [ open-source ](https://github.com/imartinez/privateGPT) [ projects ](https://github.com/TransformerOptimus/SuperAGI) , as well as in [ production ](https://www.gunder.com/news/gunderson-dettmer-launches-chatgd-a-homegrown-generative-ai-chat-app-to-its-lawyers/) in [ enterprise ](https://www.springworks.in/albus/) [ settings ](https://mqube.com/blog/4-lessons-from-launching-an-llm-chatbot) . \n\n#  Big thanks\n\nBut big numbers aside, the thing we\u2019re proudest of is our community: we have\nusers in (nearly) every country in the world, from single hobby developers to\nFortune 500 companies and everyone in between. LlamaIndex\u2019s founder, Jerry\nLiu, says:\n\n> _Our community is everything at LlamaIndex. We love seeing the amazing\n> things people are building every day! ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2649, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93fa4e33-c1cc-4b3b-9b32-e98bc20264bb": {"__data__": {"id_": "93fa4e33-c1cc-4b3b-9b32-e98bc20264bb", "embedding": null, "metadata": {"filename": "llamaindex-turns-1-f69dcdd45fe3.md", "extension": ".md", "title": "LlamaIndex turns 1!", "date": "Nov 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-turns-1-f69dcdd45fe3"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "15668294-2e00-4059-8c83-b1f6c9f7058a", "node_type": "4", "metadata": {"filename": "llamaindex-turns-1-f69dcdd45fe3.md", "extension": ".md", "title": "LlamaIndex turns 1!", "date": "Nov 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-turns-1-f69dcdd45fe3"}, "hash": "6cc0ca0c9276f05c4089dead2e76a08086cfd4dd7b27b16167c0e62ca9f2ffde", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f62b90cb-44d9-4643-b66f-15b2cb832f11", "node_type": "1", "metadata": {"filename": "llamaindex-turns-1-f69dcdd45fe3.md", "extension": ".md", "title": "LlamaIndex turns 1!", "date": "Nov 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-turns-1-f69dcdd45fe3"}, "hash": "efdcbcd6efeae67e4158bc4bd16f3ae0ce6fbb64681c31820cd957a330e75562", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9577e07c-b4dc-4143-b677-6268ebda0584", "node_type": "1", "metadata": {}, "hash": "e3255e51c1d77b5e2c7739e2307862b30b42055cb0b3fd9caae04d379b997063", "class_name": "RelatedNodeInfo"}}, "text": "It\u2019s what gets us up in the morning\n> and keeps us motivated to keep pushing the boundaries of what developers can\n> do with GenAI. And we\u2019re especially grateful to the developers who give back\n> by pushing PRs, issues and bug reports. They\u2019re what makes the open source\n> world go round._\n\n#  Big milestones\n\nWhat\u2019s happened in a year? Well, everything! But here\u2019s some highlights:\n\n  * November 2022: Launched [ GPT Tree Index ](https://twitter.com/jerryjliu0/status/1590192512639332353?lang=en) , a way of organizing information into a tree. Based on the initial interest/traction, we expanded this into a List Index and Keyword Index. Then ChatGPT launched in November \n  * December 2022: Some big feature releases: support for [ indexing embeddings + vector stores ](https://docs.llamaindex.ai/en/stable/understanding/indexing/indexing.html#what-is-an-embedding) , and initial data loaders for Notion, Slack, and Google Drive \n  * January 2023: LlamaIndex hits Github trending for the first time! \n  * February 2023: We launched [ LlamaHub ](https://x.com/jerryjliu0/status/1622981509849444354?s=20) with Jesse Zhang, containing an initial repository of data loaders for users to access. We ran a sweepstakes with OctoML and got 50+ data loader submissions! \n  * March 2023: [ ChatGPT API launched ](https://openai.com/blog/introducing-chatgpt-and-whisper-apis) and then [ Plugins ](https://openai.com/blog/chatgpt-plugins) . We scrambled to support the new API + [ Plugin ](https://gpt-index.readthedocs.io/en/v0.6.3/how_to/integrations/chatgpt_plugins.html) integrations. \n  * April 2023: We incorporated! \n  * May 2023: At the end of April, we launched [ 0.6.0 ](https://betterprogramming.pub/llamaindex-0-6-0-a-new-query-interface-over-your-data-331996d47e89) , where we completely rewrote the entire framework from the ground-up for greater modularity and composability for different levels of abstraction. \n  * June 2023: We announced that we raised $8.5M in funding! \n  * July 2023: We launched [ Data Agents ](/data-agents-eed797d7972f) \\+ Agent Tools on LlamaHub. We also launched a [ Typescript package ](https://x.com/jerryjliu0/status/1683560483071328256?s=20)\n  * August 2023: We integrated with [ OpenAI fine-tuning and launched a variety of LLM and embedding fine-tuning abstractions ](https://x.com/jerryjliu0/status/1694370574808887496?s=20) . \n  * September 2023: We [ launched ](https://twitter.com/llama_index/status/1699116440056651976) [ secinsights.ai ](http://secinsights.ai) \u2014 a production-ready full-stack application \n  * October 2023: We launched [ LlamaIndex Chat ](https://twitter.com/jerryjliu0/status/1719022164203196824) \u2014 a full-stack Typescript template. \n  * November 2023: Went [ fully multi-modal ](https://docs.llamaindex.ai/en/stable/module_guides/models/multi_modal.html) with the release of GPT-4-vision! \n\n#  Big plans\n\nWith all that growth and all those features, what\u2019s next for us? ", "mimetype": "text/plain", "start_char_idx": 2649, "end_char_idx": 5582, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9577e07c-b4dc-4143-b677-6268ebda0584": {"__data__": {"id_": "9577e07c-b4dc-4143-b677-6268ebda0584", "embedding": null, "metadata": {"filename": "llamaindex-turns-1-f69dcdd45fe3.md", "extension": ".md", "title": "LlamaIndex turns 1!", "date": "Nov 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-turns-1-f69dcdd45fe3"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "15668294-2e00-4059-8c83-b1f6c9f7058a", "node_type": "4", "metadata": {"filename": "llamaindex-turns-1-f69dcdd45fe3.md", "extension": ".md", "title": "LlamaIndex turns 1!", "date": "Nov 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-turns-1-f69dcdd45fe3"}, "hash": "6cc0ca0c9276f05c4089dead2e76a08086cfd4dd7b27b16167c0e62ca9f2ffde", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93fa4e33-c1cc-4b3b-9b32-e98bc20264bb", "node_type": "1", "metadata": {"filename": "llamaindex-turns-1-f69dcdd45fe3.md", "extension": ".md", "title": "LlamaIndex turns 1!", "date": "Nov 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-turns-1-f69dcdd45fe3"}, "hash": "04dd1ef24f369231cb2f5384c7125e040290222600e0f94f9dc3e04412a14063", "class_name": "RelatedNodeInfo"}}, "text": "Stay tuned!\n\n", "mimetype": "text/plain", "start_char_idx": 5582, "end_char_idx": 5595, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cb55c0c7-f019-4d66-93e2-62766d2ca7b1": {"__data__": {"id_": "cb55c0c7-f019-4d66-93e2-62766d2ca7b1", "embedding": null, "metadata": {"filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92d1e148-39e4-4191-bffe-29e0c382ed6b", "node_type": "4", "metadata": {"filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "hash": "c0a57d501815026a752633916dc58a4891d8ae23da978f92c50cba58ca6960c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "534cce68-c8bf-45a6-8f8d-625ec15b52cb", "node_type": "1", "metadata": {}, "hash": "801cf49e2bfcc18526df411221f5af440115a3b354347265e846c7d0cf2f80bf", "class_name": "RelatedNodeInfo"}}, "text": "(co-authored by Haotian Zhang, Laurie Voss, and Jerry Liu @ LlamaIndex)\n\n#  Overview\n\nIn this blog we\u2019re excited to present a fundamentally new paradigm: multi-\nmodal Retrieval-Augmented Generation (RAG). We present new abstractions in\nLlamaIndex that now enable the following:\n\n  * Multi-modal LLMs and Embeddings \n  * Multi-modal Indexing and Retrieval (integrates with vector dbs) \n\n#  Multi-Modal RAG\n\nOne of the most exciting announcements at OpenAI Dev Day was the release of\nthe [ GPT-4V API ](https://platform.openai.com/docs/guides/vision) . GPT-4V is\na _multi-modal_ model that takes in both text/images, and can output text\nresponses. It\u2019s the latest model in a recent series of advances around multi-\nmodal models: [ LLaVa ](https://github.com/haotian-liu/LLaVA) , and [ Fuyu-8B\n](https://www.adept.ai/blog/fuyu-8b) .\n\nThis extends the capabilities of LLMs in exciting new directions. In the past\nyear, entire application stacks have emerged around the text-in/text-out\nparadigm. One of the most notable examples is Retrieval Augmented Generation\n(RAG) \u2014 combine an LLM with an external text corpus to reason over data that\nthe model isn\u2019t trained on. One of the most significant impacts of RAG for\nend-users was how much it accelerated **time-to-insight** on unstructured text\ndata. By processing an arbitrary document (PDF, web page), loading it into\nstorage, and feeding it into the context window of an LLM, you could extract\nout any insights you wanted from it.\n\nThe introduction of GPT-4V API allows us to extend RAG concepts into the\nhybrid image/text domain, and unlock value from an even greater corpus of data\n(including images).\n\nThink about all the steps in a standard RAG pipeline and how it can be\nextended to a multi-modal setting.\n\n  * **Input:** The input can be text or images. \n  * **Retrieval:** The retrieved context can be text or images. \n  * **Synthesis:** The answer can be synthesized over both text and images. \n  * **Response:** The returned result can be text and/or images. \n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2018, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "534cce68-c8bf-45a6-8f8d-625ec15b52cb": {"__data__": {"id_": "534cce68-c8bf-45a6-8f8d-625ec15b52cb", "embedding": null, "metadata": {"filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92d1e148-39e4-4191-bffe-29e0c382ed6b", "node_type": "4", "metadata": {"filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "hash": "c0a57d501815026a752633916dc58a4891d8ae23da978f92c50cba58ca6960c5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb55c0c7-f019-4d66-93e2-62766d2ca7b1", "node_type": "1", "metadata": {"filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "hash": "e72613b8e53d66c56f3f7fc094a8e97ee344fe6ab7f975fdfcce017c472b6078", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25d3115f-bcb6-45da-9182-1dde299cca90", "node_type": "1", "metadata": {}, "hash": "22cf72a1803518f7627db7d9f89d9e8a5d9162b85a8fae0d46d9ba890f926634", "class_name": "RelatedNodeInfo"}}, "text": "This is just a small part of the overall space too. You can have\nchained/sequential calls that interleave between image and text reasoning,\nsuch as [ Retrieval Augmented Image Captioning\n](https://twitter.com/jerryjliu0/status/1717205234269983030) or multi-modal\nagent loops.\n\n#  Abstractions in LlamaIndex\n\nWe\u2019re excited to present new abstractions in LlamaIndex that help make multi-\nmodal RAG possible. For each abstraction, we explicitly note what we\u2019ve done\nso far and what\u2019s still to come.\n\n##  Multi-modal LLM\n\nWe have direct support for GPT-4V via our ` OpenAIMultiModal ` class and\nsupport for open-source multi-modal models via our ` ReplicateMultiModal `\nclass (currently in beta, so that name might change). Our `\nSimpleDirectoryReader ` has long been able to ingest audio, images and video,\nbut now you can pass them directly to GPT-4V and ask questions about them,\nlike this:\n\n    \n    \n    from llama_index.multi_modal_llms import OpenAIMultiModal\n    from llama_index import SimpleDirectoryReader\n    \n    image_documents = SimpleDirectoryReader(local_directory).load_data()\n    \n    openai_mm_llm = OpenAIMultiModal(\n        model=\"gpt-4-vision-preview\", api_key=OPENAI_API_TOKEN, max_new_tokens=300\n    )\n    response = openai_mm_llm.complete(\n        prompt=\"what is in the image?\", image_documents=image_documents\n    ) \n\nThis is a new base model abstraction. Unlike our default ` LLM ` class, which\nhas standard completion/chat endpoints, the multi-modal model ( `\nMultiModalLLM ` ) can take in both image and text as input.\n\nThis also unifies the interface between both GPT-4V and open-source models.\n\n**Resources**\n\nWe have initial implementations for both GPT-4V and vision models hosted on\nReplicate. We also have a docs page for multi-modal models:\n\n  * [ Multi-modal docs page ](https://docs.llamaindex.ai/en/latest/module_guides/models/multi_modal.html)\n  * [ GPT-4V ](https://docs.llamaindex.ai/en/latest/examples/multi_modal/openai_multi_modal.html)\n  * [ Replicate ](https://docs.llamaindex.ai/en/latest/examples/multi_modal/replicate_multi_modal.html)\n\nDisplayed image and example output from GPT-4V given text query \u201cDescribe\nimage as alternative text\u201d\n\n**What\u2019s still to come:**\n\n  * More multi-modal LLM integrations \n  * Chat endpoints \n  * Streaming \n\n##  Multi-Modal Embeddings\n\nWe introduce a new ` MultiModalEmbedding ` base class that can embed both text\nand images. It contains all the methods as our existing embedding models\n(subclasses ` BaseEmbedding ` ) but also exposes ` get_image_embedding ` .\n\n", "mimetype": "text/plain", "start_char_idx": 2018, "end_char_idx": 4563, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25d3115f-bcb6-45da-9182-1dde299cca90": {"__data__": {"id_": "25d3115f-bcb6-45da-9182-1dde299cca90", "embedding": null, "metadata": {"filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92d1e148-39e4-4191-bffe-29e0c382ed6b", "node_type": "4", "metadata": {"filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "hash": "c0a57d501815026a752633916dc58a4891d8ae23da978f92c50cba58ca6960c5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "534cce68-c8bf-45a6-8f8d-625ec15b52cb", "node_type": "1", "metadata": {"filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "hash": "36212c88c787c1debbe7892ea8a023ed5eb16cbcfa9fa95df67665b21151f594", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8b823b7-5a55-49f8-8f11-5549440d9f91", "node_type": "1", "metadata": {}, "hash": "4168bae5b4a82666542ce973f5c4106d109832f318d5d4978f0f4407ebf16264", "class_name": "RelatedNodeInfo"}}, "text": "Our primary implementation here is ` ClipEmbedding ` with the CLIP model. ", "mimetype": "text/plain", "start_char_idx": 4563, "end_char_idx": 4637, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8b823b7-5a55-49f8-8f11-5549440d9f91": {"__data__": {"id_": "f8b823b7-5a55-49f8-8f11-5549440d9f91", "embedding": null, "metadata": {"filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92d1e148-39e4-4191-bffe-29e0c382ed6b", "node_type": "4", "metadata": {"filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "hash": "c0a57d501815026a752633916dc58a4891d8ae23da978f92c50cba58ca6960c5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25d3115f-bcb6-45da-9182-1dde299cca90", "node_type": "1", "metadata": {"filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}, "hash": "d49a8d193ea896ae0ed61fd554c295ad495c2bdea3c1a48d5188ab58fef1f188", "class_name": "RelatedNodeInfo"}}, "text": "See\nbelow for a guide on using this in action.\n\n**What\u2019s still to come**\n\n  * More multi-modal embedding integrations \n\n##  Multi-Modal Indexing and Retrieval\n\nWe create a new index, a ` MultiModalVectorIndex ` that can index both text\nand images into underlying storage systems \u2014 specifically a vector database\nand docstore.\n\nUnlike our existing (most popular) index, the ` VectorStoreIndex ` , this new\nindex can store both text and image documents. Indexing text is unchanged \u2014 it\nis embedded using a text embedding model and stored in a vector database.\nIndexing images involves a separate process:\n\n  1. Embed the image using CLIP \n  2. Represent the image node as a base64 encoding or path, and store it along with its embedding in a vector db (separate collection from text). \n\nWe store images and text separately since we may want to use a text-only\nembedding model for text as opposed to CLIP embeddings (e.g. ada or sbert).\n\nDuring retrieval-time, we do the following:\n\n  1. Retrieve text via vector search on the text embeddings \n  2. Retrieve images via vector search on the image embeddings \n\nBoth text and images are returned as Nodes in the result list. We can then\nsynthesize over these results.\n\n**What\u2019s still to Come**\n\n  * More native ways to store images in a vector store (beyond base64 encoding) \n  * More flexible multi-modal retrieval abstractions (e.g. combining image retrieval with any text retrieval method) \n  * Multi-modal response synthesis abstractions. Currently the way to deal with long text context is to do \u201ccreate-and-refine\u201d or \u201ctree-summarize\u201d over it. It\u2019s unclear what generic response synthesis over multiple images and text looks like. \n\n#  Notebook Walkthrough\n\nLet\u2019s walk through a notebook example. Here we go over a use case of querying\nTesla given screenshots of its website/vehicles, SEC fillings, and Wikipedia\npages.\n\nWe load the documents as a mix of text docs and images:\n\n    \n    \n    documents = SimpleDirectoryReader(\"./mixed_wiki/\").load_data()\n\nWe then define two separate vector database collections in Qdrant: a\ncollection for text docs, and a collection for images. We then define a `\nMultiModalVectorStoreIndex ` .\n\n    \n    \n    # Create a local Qdrant vector store\n    client = qdrant_client.QdrantClient(path=\"qdrant_mm_db\")\n    \n    text_store = QdrantVectorStore(\n        client=client, collection_name=\"text_collection\"\n    )\n    image_store = QdrantVectorStore(\n        client=client, collection_name=\"image_collection\"\n    )\n    storage_context = StorageContext.from_defaults(vector_store=text_store)\n    \n    # Create the MultiModal index\n    index = MultiModalVectorStoreIndex.from_documents(\n        documents, storage_context=storage_context, image_vector_store=image_store\n    )\n\nWe can then ask questions over our multi-modal corpus.\n\n##  Example 1: Retrieval Augmented Captioning\n\nHere we copy/paste an initial image caption as the input to get a retrieval-\naugmented output:\n\n    \n    \n    retriever_engine = index.as_retriever(\n        similarity_top_k=3, image_similarity_top_k=3\n    )\n    # retrieve more information from the GPT4V response\n    retrieval_results = retriever_engine.retrieve(query_str)\n\nThe retrieved results contain both images and text:\n\nRetrieved Text/Image Results\n\nWe can feed this to GPT-4V to ask a followup question or synthesize a coherent\nresponse:\n\nSynthesized Result\n\n##  Example 2: Multi-Modal RAG Querying\n\nHere we ask a question and get a response from the entire multi-modal RAG\npipeline. The ` SimpleMultiModalQueryEngine ` first retrieves the set of\nrelevant images/text, and feeds the input to a vision model in order to\nsynthesize a response.\n\n    \n    \n    from llama_index.query_engine import SimpleMultiModalQueryEngine\n    \n    query_engine = index.as_query_engine(\n        multi_modal_llm=openai_mm_llm,\n        text_qa_template=qa_tmpl\n    )\n    \n    query_str = \"Tell me more about the Porsche\"\n    response = query_engine.query(query_str)\n\nThe generated result + sources are shown below:\n\n", "mimetype": "text/plain", "start_char_idx": 4637, "end_char_idx": 8654, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22225985-0132-49ce-8a92-8fc72c2ac428": {"__data__": {"id_": "22225985-0132-49ce-8a92-8fc72c2ac428", "embedding": null, "metadata": {"filename": "building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.md", "extension": ".md", "title": "Building My Own ChatGPT Vision with PaLM, KOSMOS-2 and LlamaIndex", "date": "Nov 8, 2023", "url": "https://www.llamaindex.ai/blog/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "49984c49-8047-4f24-b368-c323ea28475e", "node_type": "4", "metadata": {"filename": "building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.md", "extension": ".md", "title": "Building My Own ChatGPT Vision with PaLM, KOSMOS-2 and LlamaIndex", "date": "Nov 8, 2023", "url": "https://www.llamaindex.ai/blog/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566"}, "hash": "33155ebae933e4763e31fec680448975a2428c0ccdbc94e9061c8b5a49c6b4aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad9e678c-aa5a-45d0-aafd-05149455b1b5", "node_type": "1", "metadata": {}, "hash": "4a6b06ecf76c12eeb16fc807020b0089cde93ae402481821d72f318f0ecd784b", "class_name": "RelatedNodeInfo"}}, "text": "In the ever-evolving landscape of AI, OpenAI\u2019s ChatGPT with vision\ncapabilities has opened a new chapter. It\u2019s an exciting time for developers\nand creators as we explore the fusion of visual understanding with\nconversational AI. Inspired by this innovation, I set out to build my own\nmulti-modal prototype, not just as a replica but as a launchpad for more\nadvanced and tailored visual-language applications.\n\nThe tools at our disposal are nothing short of extraordinary. **KOSMOS-2** a\ntrue powerhouse in painting vivid narratives from mere pixels, making image\ncaptioning seem almost magical. Then there\u2019s the Google **PaLM API** ,\nbringing a level of conversational depth that truly understands and responds\nwith relevance. And of course, there\u2019s **LlamaIndex** \\- the brains of the\noperation, orchestrating these elements with such finesse that the interaction\nflows as naturally as a conversation between old friends.\n\n#  Features Overview\n\nThe outcome of my curiosity and coding is a Streamlit app \u2014 a prototype that\nstands as an homage and alternative to ChatGPT\u2019s vision capabilities. Here\u2019s\nwhat it brings to the table:\n\n  * **Real-Time Image Interaction:** Upload your images and instantly dive into a dialogue about them. \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1236, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad9e678c-aa5a-45d0-aafd-05149455b1b5": {"__data__": {"id_": "ad9e678c-aa5a-45d0-aafd-05149455b1b5", "embedding": null, "metadata": {"filename": "building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.md", "extension": ".md", "title": "Building My Own ChatGPT Vision with PaLM, KOSMOS-2 and LlamaIndex", "date": "Nov 8, 2023", "url": "https://www.llamaindex.ai/blog/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "49984c49-8047-4f24-b368-c323ea28475e", "node_type": "4", "metadata": {"filename": "building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.md", "extension": ".md", "title": "Building My Own ChatGPT Vision with PaLM, KOSMOS-2 and LlamaIndex", "date": "Nov 8, 2023", "url": "https://www.llamaindex.ai/blog/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566"}, "hash": "33155ebae933e4763e31fec680448975a2428c0ccdbc94e9061c8b5a49c6b4aa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22225985-0132-49ce-8a92-8fc72c2ac428", "node_type": "1", "metadata": {"filename": "building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.md", "extension": ".md", "title": "Building My Own ChatGPT Vision with PaLM, KOSMOS-2 and LlamaIndex", "date": "Nov 8, 2023", "url": "https://www.llamaindex.ai/blog/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566"}, "hash": "1796a4dfe296405ce6bc12723d391033b58d6779ff445eade25fb41fd2884502", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a31b7a5-2f0d-4ad7-8b74-a0baae7b555f", "node_type": "1", "metadata": {}, "hash": "944f6f109b77cb45d1f901402ce26c671f94e50d47a643b9cc0e2bb3dbb19250", "class_name": "RelatedNodeInfo"}}, "text": "* **Automatic Captioning with KOSMOS-2:** Microsoft\u2019s AI model offers a descriptive base for the conversation. \n  * **Conversational Depth with PaLM:** Google\u2019s language model ensures the chat is as rich and nuanced as the images themselves. \n  * **User-Friendly Interface:** Streamlit powers an intuitive and clean UI, making it easy for anyone to navigate and interact. \n\n#  Deep Dive into the Tech Stack\n\nThe project is a symphony of technologies, each playing a crucial role:\n\n  * Microsoft AI **KOSMOS-2** via Replicate breathes life into images by providing them a narrative. \n  * Google **PaLM API** adds the layer of linguistic intelligence, making the conversation about the images insightful and engaging. \n  * **LlamaIndex** acts as the maestro, coordinating the models to work in harmony. \n\n#  Unveiling ` app.py ` : The Core of the Application\n\nThe ` app.py ` script is the heart of the app, where we bring together\nKOSMOS-2 and PaLM with Llamaindex to create a seamless multimodal experience.\nLet\u2019s walk through it, from start to finish.\n\n", "mimetype": "text/plain", "start_char_idx": 1236, "end_char_idx": 2289, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a31b7a5-2f0d-4ad7-8b74-a0baae7b555f": {"__data__": {"id_": "6a31b7a5-2f0d-4ad7-8b74-a0baae7b555f", "embedding": null, "metadata": {"filename": "building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.md", "extension": ".md", "title": "Building My Own ChatGPT Vision with PaLM, KOSMOS-2 and LlamaIndex", "date": "Nov 8, 2023", "url": "https://www.llamaindex.ai/blog/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "49984c49-8047-4f24-b368-c323ea28475e", "node_type": "4", "metadata": {"filename": "building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.md", "extension": ".md", "title": "Building My Own ChatGPT Vision with PaLM, KOSMOS-2 and LlamaIndex", "date": "Nov 8, 2023", "url": "https://www.llamaindex.ai/blog/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566"}, "hash": "33155ebae933e4763e31fec680448975a2428c0ccdbc94e9061c8b5a49c6b4aa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad9e678c-aa5a-45d0-aafd-05149455b1b5", "node_type": "1", "metadata": {"filename": "building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.md", "extension": ".md", "title": "Building My Own ChatGPT Vision with PaLM, KOSMOS-2 and LlamaIndex", "date": "Nov 8, 2023", "url": "https://www.llamaindex.ai/blog/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566"}, "hash": "8c398bb50b5217525778abe66ca4ef1130b17f7b2b5071bccc28e0819ae58043", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b220322-7f99-48ff-b12a-b8bf18050123", "node_type": "1", "metadata": {}, "hash": "34c832181be03adbecb4c2cb2e0693ecd8474b25b8d15124d9166b0b5971ba8d", "class_name": "RelatedNodeInfo"}}, "text": "**1\\. Initial Setup**\n\nWe start by importing the necessary libraries and setting up our Streamlit\npage. Here, we lay the groundwork for image processing and conversation\nmanagement.\n\n    \n    \n    import streamlit as st\n    import extra_streamlit_components as stx\n    import requests\n    from PIL import Image\n    from transformers import AutoProcessor, AutoModelForVision2Seq\n    from io import BytesIO\n    import replicate\n    from llama_index.llms.palm import PaLM\n    from llama_index import ServiceContext, VectorStoreIndex, Document\n    from llama_index.memory import ChatMemoryBuffer\n    import os\n    import datetime\n    \n    st.set_page_config(layout=\"wide\")\n    st.write(\"My version of ChatGPT vision. You can upload an image and start chatting with the LLM about the image\")\n\n**2\\. User Interface**\n\nNext, we craft the sidebar and the main area, ensuring that the user knows who\ncreated the app and has access to other projects, enhancing credibility and\nengagement.\n\n    \n    \n    # Sidebar\n    st.sidebar.markdown('## Created By')\n    st.sidebar.markdown(\"[Harshad Suryawanshi](https://www.linkedin.com/in/harshadsuryawanshi/)\")\n    st.sidebar.markdown('## Other Projects')\n    # ...sidebar content continues\n\n**3\\. Image Upload and Processing**\n\nUpon uploading an image, the app not only displays it but also invokes the `\nget_image_caption ` function to generate a relevant caption. This function,\ndecorated with ` @st.cache ` for caching, uses the KOSMOS-2 model through\nReplicate to provide a brief description of the uploaded image. The\ndescription is then used as the basis for the initial conversation with the\nuser.\n\n    \n    \n    @st.cache\n    def get_image_caption(image_data):\n        input_data = {\n            \"image\": image_data,\n            \"description_type\": \"Brief\"\n        }\n        output = replicate.run(\n            \"lucataco/kosmos-2:3e7b211c29c092f4bcc8853922cc986baa52efe255876b80cac2c2fbb4aff805\",\n            input=input_data\n        )\n        # Split the output string on the newline character and take the first item\n        text_description = output.split('\\n\\n')[0]\n        return text_description\n\n**4\\. Conversational Flow with PaLM and Llamaindex**\n\nWith the image caption in hand, the ` create_chat_engine ` function is called\nto set up the chat engine. This function is crucial as it establishes the\ncontext for the conversation and initializes the PaLM API for interaction.\n\n    \n    \n    @st.cache_resource\n    def create_chat_engine(img_desc, api_key):\n        llm = PaLM(api_key=api_key)\n        service_context = ServiceContext.from_defaults(llm=llm)\n        doc = Document(text=img_desc)\n        index = VectorStoreIndex.from_documents([doc], service_context=service_context)\n        chatmemory = ChatMemoryBuffer.from_defaults(token_limit=1500)\n        \n        chat_engine = index.as_chat_engine(\n            chat_mode=\"context\",\n            system_prompt=(\n                f\"You are a chatbot, able to have normal interactions, as well as talk. \"\n                \"You always answer in great detail and are polite. ", "mimetype": "text/plain", "start_char_idx": 2289, "end_char_idx": 5362, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b220322-7f99-48ff-b12a-b8bf18050123": {"__data__": {"id_": "2b220322-7f99-48ff-b12a-b8bf18050123", "embedding": null, "metadata": {"filename": "building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.md", "extension": ".md", "title": "Building My Own ChatGPT Vision with PaLM, KOSMOS-2 and LlamaIndex", "date": "Nov 8, 2023", "url": "https://www.llamaindex.ai/blog/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "49984c49-8047-4f24-b368-c323ea28475e", "node_type": "4", "metadata": {"filename": "building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.md", "extension": ".md", "title": "Building My Own ChatGPT Vision with PaLM, KOSMOS-2 and LlamaIndex", "date": "Nov 8, 2023", "url": "https://www.llamaindex.ai/blog/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566"}, "hash": "33155ebae933e4763e31fec680448975a2428c0ccdbc94e9061c8b5a49c6b4aa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a31b7a5-2f0d-4ad7-8b74-a0baae7b555f", "node_type": "1", "metadata": {"filename": "building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.md", "extension": ".md", "title": "Building My Own ChatGPT Vision with PaLM, KOSMOS-2 and LlamaIndex", "date": "Nov 8, 2023", "url": "https://www.llamaindex.ai/blog/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566"}, "hash": "c6d70e9684905f060129be18a763c17107cc8a5bb3d9e10463de766420847ced", "class_name": "RelatedNodeInfo"}}, "text": "Your responses always descriptive. \"\n                \"Your job is to talk about an image the user has uploaded. Image description: {img_desc}.\"\n            ),\n            verbose=True,\n            memory=chatmemory\n        )\n        return chat_engine\n\nThe ` create_chat_engine ` function builds the infrastructure for our app's\nconversation capabilities. It starts by instantiating a PaLM object with the\nprovided API key, setting up the service context, and creating a document with\nthe image description. This document is then indexed to prepare it for\nLlamaindex\u2019s context chat engine. Finally, the chat engine is configured with\na prompt that instructs the AI on how to engage in the conversation,\nreferencing the image description and defining the chatbot's behavior.\n\n**5\\. User Interaction and Message Handling**\n\nThe application ensures an engaging and controlled user experience by limiting\nthe number of messages to 20 per session in the demo version. If this limit is\nreached, it gracefully notifies the user and disables further input to manage\nresources effectively.\n\n    \n    \n    if message_count &gt;= 20:\n        st.error(\"Notice: The maximum message limit for this demo version has been reached.\")\n        # Disabling the uploader and input by not displaying them\n        image_uploader_placeholder = st.empty()  # Placeholder for the uploader\n        chat_input_placeholder = st.empty()      # Placeholder for the chat input\n\nHowever, when the message count is within the limit, the application provides\na clear chat option and handles the image upload process. Upon uploading, it\nimmediately processes the image to get a caption, sets up the chat engine, and\nupdates the user interface to reflect the successful upload.\n\n    \n    \n    else:\n        # Add a clear chat button\n        if st.button(\"Clear Chat\"):\n            clear_chat()\n    \n        # Image upload section\n        image_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"], key=\"uploaded_image\", on_change=on_image_upload)\n        # ...code for image upload and display\n\nFor each user input, the message is added to the chat history, and the chat\nengine is queried for a response. The app ensures that each message \u2014 whether\nfrom the user or the assistant \u2014 is displayed in the chat interface,\nmaintaining a coherent conversation flow.\n\n    \n    \n    # ...code for handling user input and displaying chat history\n    \n    # Call the chat engine to get the response if an image has been uploaded\n    if image_file and user_input:\n        try:\n            with st.spinner('Waiting for the chat engine to respond...'):\n                # Get the response from your chat engine\n                response = chat_engine.chat(user_input)\n            # ...code for appending and displaying the assistant's response\n        except Exception as e:\n            st.error(f'An error occurred.')\n            # ...exception handling code\n\n#  Wrapping Up\n\nThis app is the foundation, a springboard for more complex visual-language\napplications. The potential is limitless, and your insights can shape its\nfuture. I invite you to dive into the demo, tinker with the code, and join me\nin pushing the envelope of what AI can do.\n\n[ Link to GitHub Repo ](https://github.com/AI-ANK/PaLM-Kosmos-Vision)\n\n[ Connect with Me on LinkedIn\n](https://www.linkedin.com/in/harshadsuryawanshi/)\n\n[ LinkedIn Post ](https://www.linkedin.com/posts/harshadsuryawanshi_ai-\ncomputervision-streamlit-activity-7126698614541680640-y5kB) :\n\n", "mimetype": "text/plain", "start_char_idx": 5362, "end_char_idx": 8864, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "34a49539-768e-4306-a218-d966d40bb1eb": {"__data__": {"id_": "34a49539-768e-4306-a218-d966d40bb1eb", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-11-07-cf20b9a833aa.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023-11\u201307", "date": "Nov 8, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-07-cf20b9a833aa"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eeca3be1-f689-470a-8579-92f5bd5b94f8", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-11-07-cf20b9a833aa.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023-11\u201307", "date": "Nov 8, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-07-cf20b9a833aa"}, "hash": "bd85325d0c2aa85efa8035e9e9f247c38881dce8e15c725a80cf2452e159193a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a82c9c59-5dfd-4d1f-af51-657f3fab04ea", "node_type": "1", "metadata": {}, "hash": "7d524d6e94f9f70afba87914ec767850c067a4c34db120ee23f3c08cfdc598b5", "class_name": "RelatedNodeInfo"}}, "text": "Hi again Llama Fans!\n\nWe hope you enjoyed our [ OpenAI Dev Day special edition ](/llamaindex-news-\nspecial-edition-openai-developer-day-e955f16db4e2) yesterday! Here\u2019s our wrap-\nup of everything else that happened last week. As always, if you\u2019ve got a\nproject, article, or video that\u2019s turning heads? We\u2019re all ears! Drop us a\nline at [ news@llamaindex.ai ](mailto:news@llamaindex.ai) .\n\nAnd for all this goodness delivered directly to you, don\u2019t forget to subscribe\nto our newsletter via our [ website ](https://www.llamaindex.ai/) .\n\n**First, the highlights:**\n\n  1. **LlamaIndex Chat:** We unveiled a customizable LLM chatbot template with system prompts and avatars, all within an open-source MIT-licensed framework using LlamaIndex for TypeScript. Explore the [ Demo ](https://chat.llamaindex.ai/) or check the [ Tweet ](https://x.com/llama_index/status/1719021921462067654?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 890, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a82c9c59-5dfd-4d1f-af51-657f3fab04ea": {"__data__": {"id_": "a82c9c59-5dfd-4d1f-af51-657f3fab04ea", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-11-07-cf20b9a833aa.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023-11\u201307", "date": "Nov 8, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-07-cf20b9a833aa"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eeca3be1-f689-470a-8579-92f5bd5b94f8", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-11-07-cf20b9a833aa.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023-11\u201307", "date": "Nov 8, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-07-cf20b9a833aa"}, "hash": "bd85325d0c2aa85efa8035e9e9f247c38881dce8e15c725a80cf2452e159193a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "34a49539-768e-4306-a218-d966d40bb1eb", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-11-07-cf20b9a833aa.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023-11\u201307", "date": "Nov 8, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-07-cf20b9a833aa"}, "hash": "85c2eb7325feb38907f1b8c0a62e50890b8253cb67ccc3505bf46bb6e6497a93", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f69d6d7a-1a66-4cc1-9764-e2e458a7295f", "node_type": "1", "metadata": {}, "hash": "e3c2fb87564df735766aab0289d7e234b5b2ad00b9a5ff9310f33665a4120353", "class_name": "RelatedNodeInfo"}}, "text": "2. **Evaluator Fine-Tuning:** We launched a method to enhance LLM output assessment by distilling GPT-4 into GPT-3.5, optimizing both cost and speed. See our [ Tweet ](https://x.com/llama_index/status/1719868813318271242?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 890, "end_char_idx": 1122, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f69d6d7a-1a66-4cc1-9764-e2e458a7295f": {"__data__": {"id_": "f69d6d7a-1a66-4cc1-9764-e2e458a7295f", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-11-07-cf20b9a833aa.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023-11\u201307", "date": "Nov 8, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-07-cf20b9a833aa"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eeca3be1-f689-470a-8579-92f5bd5b94f8", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-11-07-cf20b9a833aa.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023-11\u201307", "date": "Nov 8, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-07-cf20b9a833aa"}, "hash": "bd85325d0c2aa85efa8035e9e9f247c38881dce8e15c725a80cf2452e159193a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a82c9c59-5dfd-4d1f-af51-657f3fab04ea", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-11-07-cf20b9a833aa.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023-11\u201307", "date": "Nov 8, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-07-cf20b9a833aa"}, "hash": "419d5be3cfdf2fec175e25a3223af4a173e9f034d890764bda3a9956bd0693fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b223462-52a5-4956-8c3e-810a14c77536", "node_type": "1", "metadata": {}, "hash": "f026d7a2603f98a00d8ceb6679d5bfb697007d330160994e6d1308209c2c5e64", "class_name": "RelatedNodeInfo"}}, "text": "3. **ParamTuner:** We introduced a new hyperparameter tuning abstraction to refine RAG pipeline performance, featuring objective functions, grid search, and Ray Tune integration. Check out the [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/param_optimizer/param_optimizer.ipynb) and [ Tweet ](https://twitter.com/llama_index/status/1721209688703062234?s=20) . \n  4. **CohereAI Embed v3 & Voyage AI Integration: ** We strengthened the LlamaIndex RAG pipeline with two powerful embedding model additions: the latest Embed v3 from CohereAI and the high-performing embedding model from Voyage AI. [ Tweet ](https://twitter.com/llama_index/status/1720216603584069875?s=20) and [ tweet ](https://x.com/llama_index/status/1720578050180686129?s=20) . \n\n**Feature Releases and Enhancements:**\n\n  * We introduced LlamaIndex Chat, a new feature allowing users to create and share custom LLM chatbots tailored to their data, complete with personalized system prompts and avatars. Additionally, we\u2019re proud to share that it\u2019s a fully open-source template under the MIT license, crafted using LlamaIndexTS for a seamless start to LLM application development. [ Demo ](https://chat.llamaindex.ai/) , [ Tweet ](https://x.com/llama_index/status/1719021921462067654?s=20) . \n  * We introduced a method for fine-tuning an Evaluator to distill GPT-4 into GPT-3.5, enhancing LLM output assessment while reducing costs and improving speed. [ Tweet ](https://x.com/llama_index/status/1719868813318271242?s=20) . \n  * We introduced ` ParamTuner ` , a hyperparameter tuning abstraction for LlamaIndex RAG, streamlining the process with objective functions and support for grid search, including integration with Ray Tune for enhanced optimization. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/param_optimizer/param_optimizer.ipynb) , [ Tweet ](https://twitter.com/llama_index/status/1721209688703062234?s=20) . \n\n**** Demos:\n\n  * GPTDiscord is a versatile LLM-powered Discord bot with over 20 features, including multi-modal image understanding and advanced data analysis. It boasts an infinite conversational memory and the ability to interact with various file types and internet services. ", "mimetype": "text/plain", "start_char_idx": 1122, "end_char_idx": 3353, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b223462-52a5-4956-8c3e-810a14c77536": {"__data__": {"id_": "9b223462-52a5-4956-8c3e-810a14c77536", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-11-07-cf20b9a833aa.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023-11\u201307", "date": "Nov 8, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-07-cf20b9a833aa"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eeca3be1-f689-470a-8579-92f5bd5b94f8", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-11-07-cf20b9a833aa.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023-11\u201307", "date": "Nov 8, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-07-cf20b9a833aa"}, "hash": "bd85325d0c2aa85efa8035e9e9f247c38881dce8e15c725a80cf2452e159193a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f69d6d7a-1a66-4cc1-9764-e2e458a7295f", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-11-07-cf20b9a833aa.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023-11\u201307", "date": "Nov 8, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-07-cf20b9a833aa"}, "hash": "f0c0cc164e64dd40f0fdf8d84f50f57d3815d40cfec45e57c8846cf54fa0185a", "class_name": "RelatedNodeInfo"}}, "text": "[ Tweet ](https://twitter.com/llama_index/status/1720151524280881335?s=20) . \n\n**Guides:**\n\n  * We shared a [ guide ](https://docs.llamaindex.ai/en/latest/examples/retrievers/deep_memory.html) for integrating Activeloop\u2019s Deep Memory with LlamaIndex, a module that enhances your embeddings at ingestion and can improve RAG metrics by 15%, all while seamlessly fitting into LlamaIndex\u2019s automated dataset and vector store features. \n  * We shared a [ guide ](https://docs.llamaindex.ai/en/latest/examples/prompts/prompt_optimization.html) inspired by [ **Chengrun Yang** ](https://twitter.com/chengrun_yang) and GoogleDeepMind\u2019s ` Optimization by Prompting ` paper, demonstrating how to automate prompt tuning in LlamaIndex RAG pipelines using meta-prompting, boosting evaluation performance while acknowledging the experimental nature of this technique. \n  * We shared a [ guide ](https://docs.llamaindex.ai/en/latest/examples/prompts/emotion_prompt.html) on how to implement Emotion Prompting in LlamaIndex, allowing you to enhance your RAG pipeline with various emotional stimuli and evaluate their impact on task performance. \n  * We showcased MongoDB [ starter ](https://github.com/run-llama/mongodb-demo) kit, a comprehensive LlamaIndex RAG setup with Flask backend, Next frontend, and easy deployment to Render. \n\n**Tutorials:**\n\n  * [ Wenqi Glantz ](https://medium.com/@wenqiglantz) made a [ blog post ](https://levelup.gitconnected.com/optimizing-text-embeddings-with-huggingfaces-text-embeddings-inference-server-and-llamaindex-ef7df35882a4) on deploying the HuggingFace ` **text-embeddings-inference** ` server on an AWS EC2 GPU instance, enhancing LlamaIndex RAG pipeline's performance and results. \n  * [ Sophia Yang\u2019s ](https://twitter.com/sophiamyang) [ tutorial ](https://www.youtube.com/watch?v=QqDZVg9S_Vk) on Zephyr-7b-beta showcases its leading capabilities in LLM technology, including how it\u2019s benchmarked with LlamaIndex for diverse AI tasks. \n  * [ Sudarshan Koirala ](https://twitter.com/mesudarshan) gave a [ tutorial ](https://www.youtube.com/watch?v=vJz9WVgsu9g) on how to build a multi-modal retrieval system with LlamaIndex, Qdrant, and bge/CLIP embeddings. \n  * [ Sophia Yang\u2019s ](https://twitter.com/sophiamyang) gave another [ tutorial ](https://www.youtube.com/watch?v=ihSiRrOUwmg) , this time on Small-to-Big Retrieval with LlamaIndex in building advanced RAG systems. \n  * [ Ravi Theja\u2019s ](https://www.linkedin.com/in/ravidesetty/) [ tutorial ](https://www.youtube.com/watch?v=X8BHWGXXdW0) on the Router Query Engine that helps you to set up multiple indices/ query engines for your dataset, allowing the LLM to choose the most suitable one for each specific question. \n\n**Integrations & Collaborations: **\n\n  * We integrated the [ **Tavily AI** ](https://tavily.com/) research API into the LlamaIndex RAG pipeline, offering a robust tool for web research to enhance LLM agent automation. [ Notebook ](https://github.com/run-llama/llama-hub/blob/main/llama_hub/tools/notebooks/tavily.ipynb) , [ Tweet ](https://x.com/llama_index/status/1719745197729599681?s=20) . \n  * We integrated [ **Noam Gat** ](https://twitter.com/noamgat) \u2019s LLM Enforcer into the LlamaIndex RAG pipeline to ensure structured outputs for various models. [ Docs ](https://docs.llamaindex.ai/en/latest/community/integrations/lmformatenforcer.html) , [ Tweet ](https://twitter.com/llama_index/status/1720103157412647265?s=20) . \n  * We integrated the latest Embed v3 model from CohereAI, enhancing document retrieval quality within the LlamaIndex RAG pipeline. [ Notebook ](https://t.co/NOQxN9RJi3) , [ Tweet ](https://twitter.com/llama_index/status/1720216603584069875?s=20) . \n  * We integrated the new Voyage AI embedding model, a top-performing option for RAG pipelines. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/embeddings/voyageai.ipynb) , [ Tweet ](https://x.com/llama_index/status/1720578050180686129?s=20) . \n\n", "mimetype": "text/plain", "start_char_idx": 3353, "end_char_idx": 7312, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8f02931-738a-488e-a5d0-4c28cc65e2ed": {"__data__": {"id_": "d8f02931-738a-488e-a5d0-4c28cc65e2ed", "embedding": null, "metadata": {"filename": "llamaindex-news-special-edition-openai-developer-day-e955f16db4e2.md", "extension": ".md", "title": "LlamaIndex news special edition: OpenAI developer day!", "date": "Nov 7, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-news-special-edition-openai-developer-day-e955f16db4e2"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39b782aa-ccc7-48e3-a0bd-7c22c696b36f", "node_type": "4", "metadata": {"filename": "llamaindex-news-special-edition-openai-developer-day-e955f16db4e2.md", "extension": ".md", "title": "LlamaIndex news special edition: OpenAI developer day!", "date": "Nov 7, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-news-special-edition-openai-developer-day-e955f16db4e2"}, "hash": "257c5c47abcbbb3fd720ce4154fa2b07be6b3851620d32d3461c5fb459f0642f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "371fd1a5-8921-46a4-83ac-8a9e0788ab95", "node_type": "1", "metadata": {}, "hash": "c9079003cb6bd4c3d5944acfde3458f6e00cc4d5256010aa46f76ed2a5c9a438", "class_name": "RelatedNodeInfo"}}, "text": "Hello Llama fans!\n\nYesterday was a big day in the world of LLMs; OpenAI held their [ developer\nday conference ](https://devday.openai.com/) and there were a slew of new\nfeatures. The team were all hands on deck to bring support for these features\nto the library as fast as possible \u2014 which is to say, [ the same day\n](https://twitter.com/llama_index/status/1721737484961480836) !\n\nIn case you missed our tweet about it, if you install the [ latest build\n](https://github.com/run-llama/llama_index/releases) of LlamaIndex you\u2019ll get\neverything below:\n\n#  Support for two new models released today\n\n  * ` gpt-4-1106-preview ` , aka **GPT-4 Turbo** , the latest GPT-4 model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and a 128,000 token context window \n  * ` gpt-4-vision-preview ` , aka **GPT 4 Turbo with vision** with long-awaited multimodal support, has the ability to understand images in addition to all the other GPT-4 Turbo capabilities. \n\nYou can use these models just as you would any other OpenAI model:\n\n    \n    \n    from llama_index.llms import OpenAI\n    from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n    \n    llm = OpenAI(model=\"gpt-4-1106-preview\")\n    service_context = ServiceContext.from_defaults(llm=llm)\n    documents = SimpleDirectoryReader(\"data\").load_data()\n    index = VectorStoreIndex.from_documents(\n        documents, service_context=service_context\n    )\n\n#  Azure OpenAI endpoints\n\nCheck out the [ OpenAI Azure notebook\n](https://docs.llamaindex.ai/en/stable/examples/llm/azure_openai.html) for\nexamples.\n\n#  New embeddings abstractions\n\nIncluding Azure embeddings.\n\n#  Function calling\n\nCheck out [ our notebook\n](https://docs.llamaindex.ai/en/stable/examples/llm/openai.html#function-\ncalling) for examples.\n\n#  SEC insights\n\nOur demo of the power of retrieval-augmented generation for financial filings,\n[ SEC Insights ](https://www.secinsights.ai/) , has been updated to use the\nlatest version of GPT-4! Watch as you instantly get deeper insights and more\nrelevant responses.\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2098, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "371fd1a5-8921-46a4-83ac-8a9e0788ab95": {"__data__": {"id_": "371fd1a5-8921-46a4-83ac-8a9e0788ab95", "embedding": null, "metadata": {"filename": "llamaindex-news-special-edition-openai-developer-day-e955f16db4e2.md", "extension": ".md", "title": "LlamaIndex news special edition: OpenAI developer day!", "date": "Nov 7, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-news-special-edition-openai-developer-day-e955f16db4e2"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39b782aa-ccc7-48e3-a0bd-7c22c696b36f", "node_type": "4", "metadata": {"filename": "llamaindex-news-special-edition-openai-developer-day-e955f16db4e2.md", "extension": ".md", "title": "LlamaIndex news special edition: OpenAI developer day!", "date": "Nov 7, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-news-special-edition-openai-developer-day-e955f16db4e2"}, "hash": "257c5c47abcbbb3fd720ce4154fa2b07be6b3851620d32d3461c5fb459f0642f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8f02931-738a-488e-a5d0-4c28cc65e2ed", "node_type": "1", "metadata": {"filename": "llamaindex-news-special-edition-openai-developer-day-e955f16db4e2.md", "extension": ".md", "title": "LlamaIndex news special edition: OpenAI developer day!", "date": "Nov 7, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-news-special-edition-openai-developer-day-e955f16db4e2"}, "hash": "1016ebd8168a62638e48234800cd669144801e1c001d4d30e4465d13d0f0485b", "class_name": "RelatedNodeInfo"}}, "text": "Look out for more OpenAI updates soon! Our regular newsletter will also be\nposted tomorrow.\n\n", "mimetype": "text/plain", "start_char_idx": 2098, "end_char_idx": 2191, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "87df5e70-2944-4abc-a728-e8d839c47103": {"__data__": {"id_": "87df5e70-2944-4abc-a728-e8d839c47103", "embedding": null, "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "324cafbc-56c7-404b-b679-7ed0ea4b23d9", "node_type": "4", "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "a859d48892dc6e360c88b587077ec9d9f4e21bc29f58b3da751982d8adfd78d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "985ae504-25c1-4f63-a8f8-c7b28dd690b9", "node_type": "1", "metadata": {}, "hash": "ccc39a567089983b2105938be7ba4d7c0d00f395167a8d43c146bb76a1cc0aae", "class_name": "RelatedNodeInfo"}}, "text": "In  **_the RAG, after the retrieval phase, it\u2019s necessary to perform Re-\nranking + Fine-Grained Prompt Compression + Subsequence Recovery to enhance\nLLM\u2019s perception of key information, which is LongLLMLingua._ **\n\n> TL;DR: While Retrieval-Augmented Generation (RAG) is highly effective in\n> various scenarios, it still has drawbacks such as 1) Performance drop, like\n> the \u201cLost in the middle\u201d issue, 2) High costs, both financially and in terms\n> of latency, and 3) Context windows limitation. LongLLMLingua offers a\n> solution to these problems in RAG or Long Context scenarios via prompt\n> compression. It can boost accuracy by as much as 21.4% while only using \u00bc of\n> the tokens. In long context situations, it can save $28 for every 1000\n> examples.\n\nSee real-world cases on the [ **project page** ](https://llmlingua.com/) .\n\nWe previously wrote a [ blog post ](https://wyydsb.xin/NLP/LLMLingua_en.html)\nintroducing the design of **LLMLingua** , which started from the perspective\nof **designing a special language for LLMs** . This time, our focus will be on\nthe scenarios involving RAG.\n\nRetrieval-Augmented Generation is currently the most reliable and proven\ntechnique for creating AI-agents that are grounded on any specific collection\nof text. Frameworks like **LlamaIndex** provide comprehensive RAG solutions to\nhelp users utilize specialized data in LLMs more conveniently.\n\nA common misunderstanding is that retrieving as many relevant documents as\npossible during the RAG process and stitching them together to form a long\nretrieved prompt is beneficial, especially as more and more LLMs support\nlonger context windows. However, this method can introduce more **noise** into\nthe prompt and **weaken** **the LLM\u2019s perception of key information** ,\nleading to issues such as \u2018 **lost in the middle** \u2019[1].\n\nThese issues become more apparent in real-world scenarios involving RAG.\nBetter retrieval mechanisms can introduce higher quality noise documents,\nwhich can more easily lead to a drop in performance.\n\n#  Re-ranking is an intuitive concept.\n\nOne intuitive idea is to reposition the most relevant information to the sides\nof the prompt through re-ranking. This concept of re-ranking has already been\nimplemented in frameworks such as [ **LlamaIndex** ](https://github.com/run-\nllama/llama_index/blob/main/llama_index/indices/postprocessor/sbert_rerank.py)\nand [ **LangChain**\n](https://python.langchain.com/docs/modules/data_connection/document_transformers/post_retrieval/long_context_reorder)\n.\n\nHowever, according to our experiments, it\u2019s difficult for an embedding model\nto serve as a \u2018good\u2019 re-ranker. The underlying reason is the lack of an\ninteraction process between the query and the document. The dual-tower\nstructure of embeddings is not suitable for re-ranking in general scenarios,\nalthough it may be effective after fine-tuning.\n\nUsing LLMs directly as a re-ranker may also lead to misjudgments due to\n**hallucinations** . Recently, some re-ranking models have been extended from\nembedding models, such as [ **bge-rerank** ](https://huggingface.co/BAAI/bge-\nreranker-large) . ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "985ae504-25c1-4f63-a8f8-c7b28dd690b9": {"__data__": {"id_": "985ae504-25c1-4f63-a8f8-c7b28dd690b9", "embedding": null, "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "324cafbc-56c7-404b-b679-7ed0ea4b23d9", "node_type": "4", "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "a859d48892dc6e360c88b587077ec9d9f4e21bc29f58b3da751982d8adfd78d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "87df5e70-2944-4abc-a728-e8d839c47103", "node_type": "1", "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "82abb4b380c1dbd198f85b3aa8e54f83ca990110ca3c2e4fa681bb6d1c419aca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09743600-2dbe-4a7b-bea5-3e1d09390023", "node_type": "1", "metadata": {}, "hash": "69bc4d15776c231505fdecd41653efbc5e8e35494fe178d770ee110ef0b07aeb", "class_name": "RelatedNodeInfo"}}, "text": "However, such re-ranking models generally have context\nwindow limitations.\n\nTo address the above issues, we propose a Question-aware Coarse-Grained prompt\ncompression method. This method evaluates the relevance between the context\nand the question based on the perplexity corresponding to the question.\n\nTo mitigate the hallucination problem in smaller LLMs, we append a restrictive\nstatement, specifically \u201c _We can get the answer to this question in the given\ndocuments_ \u201d, after the question to limit the latent space caused by related\nhallucinations.\n\n**Figure 1.", "mimetype": "text/plain", "start_char_idx": 3111, "end_char_idx": 3678, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09743600-2dbe-4a7b-bea5-3e1d09390023": {"__data__": {"id_": "09743600-2dbe-4a7b-bea5-3e1d09390023", "embedding": null, "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "324cafbc-56c7-404b-b679-7ed0ea4b23d9", "node_type": "4", "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "a859d48892dc6e360c88b587077ec9d9f4e21bc29f58b3da751982d8adfd78d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "985ae504-25c1-4f63-a8f8-c7b28dd690b9", "node_type": "1", "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "8aff81dfc10e4455a34e5ae9ccc0e43abf5ed103326447dd7e0e5cdbd094bea8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8b58843-89ce-4db3-b16a-31dca33f9e38", "node_type": "1", "metadata": {}, "hash": "e513b7987f31340da42b88256ecf3754b6ce2fa185c98f03ce2059c64ab9144e", "class_name": "RelatedNodeInfo"}}, "text": "** The accuracy of different methods for ranking documents from\nMulti-documemnt QA dataset, which increases from top to bottom in terms of\nRecall@1. Different colors represent different types of methods. Among them,\nyellow represents traditional relevance methods, green signifies embedding-\nbased methods, and red denotes rerank-based methods. You can find the script\nin [ this link\n](https://github.com/microsoft/LLMLingua/blob/main/examples/Retrieval.ipynb) .\n\nResults show that this approach significantly outperforms both embedding\nmodels and re-ranking models. We\u2019ve added some recently released embedding and\nreranking models. As you can see, the performance of **bge-rerank-large** is\nvery close to that of **LongLLMLingua** . Reranking models generally perform\nbetter than embedding models. Currently, **Jina** is the best performing\nmethod among the embedding models.\n\n#  Compress unrelated and unimportant information\n\nBesides recalling as many relevant documents as possible, another approach is\nto **compress** irrelevant or unimportant contexts as much as possible.\n\nPrevious work on long context has focused on how to extend LLMs to support\nlonger context windows. However, almost no work has explored whether this can\nactually improve the performance of downstream tasks. Some previous studies\nhave shown that the presence of more noise in the prompt, as well as the\nposition of key information in the prompt, can affect the performance of LLMs.\n\nFrom the perspective of prompt compression, Selective Context[2] and\nLLMLingua[3] estimate the importance of elements by using a small language\nmodel to calculate the mutual information or perplexity of the prompt.\nHowever, in scenarios like RAG or long context scenarios, this method can\neasily lose key information because it cannot perceive the question\ninformation.\n\nIn recent submissions to ICLR\u201924, there have been some similar practices. For\nexample, Recomp[4] reduces the use of tokens in RAG scenarios by jointly\ntraining compressors of two different granularities. RAG in Long Context[5]\ndecomposes the long context into a series of chunks and uses retrieval methods\nfor compression, which is actually the retrieval-based method implemented in\nthe LongLLMLingua paper. In addition, Walking Down the Memory Maze[6] also\ndesigned a hierarchical summarization tree to enhance the LLM\u2019s perception of\nkey information.\n\n#  Question-aware Fine-grained Prompt Compression\n\nIn order to make token-level prompt compression also perceive the information\nof the question, we propose a **contrastive perplexity** , which compares the\ndifference between the perplexity distribution corresponding to the document\nand the perplexity distribution corresponding to the document with the\nquestion.\n\nAn intuitive feeling is that when the question serves as context, the\nperplexity corresponding to the relevant tokens in the document will decrease.\n", "mimetype": "text/plain", "start_char_idx": 3678, "end_char_idx": 6581, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8b58843-89ce-4db3-b16a-31dca33f9e38": {"__data__": {"id_": "a8b58843-89ce-4db3-b16a-31dca33f9e38", "embedding": null, "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "324cafbc-56c7-404b-b679-7ed0ea4b23d9", "node_type": "4", "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "a859d48892dc6e360c88b587077ec9d9f4e21bc29f58b3da751982d8adfd78d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09743600-2dbe-4a7b-bea5-3e1d09390023", "node_type": "1", "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "1d25376a0c2c43131d2ceb1689a7f93bef63490eda7d2646e55b5ca4a60b6960", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "089fbe81-bfa7-4cb1-9b5a-9aa4a231ce5e", "node_type": "1", "metadata": {}, "hash": "b876144cdf5689613fcbf0068b0ce1b853cc8edeca0729e3ac0b063216451380", "class_name": "RelatedNodeInfo"}}, "text": "This decrease in magnitude represents the importance of the tokens in the\ndocument relative to the question.\n\n**Figure 3.** Comparison between perplexities and contrastive perplexities of\ntokens in the prompt from Multi-documemnt QA dataset. The document with the\nground truth is located on the left side of the dashed line.\n\nFigure 3 shows the distribution difference in extracting key tokens between\nperplexity and contrastive perplexity.\n\n#  How to reduce the loss in the middle\n\nSince Coarse-grained Prompt compression far exceeds other retrieval methods in\nterms of accuracy, it is a very natural idea to use this ranking information\nto rearrange the documents that are more related to the question to the\nbeginning and end of the prompt. However, through our testing, we found that\nrearranging to the beginning of the prompt is more effective than evenly\ndistributing at both ends. So, we choose to reorder the most related document\nto the beginning of the prompt.\n\n#  How to achieve adaptive granular control during compression?\n\nIn order to better use the information from the two grained compressions, in\nthe fine-grained prompt compression, we dynamically allocate different\ncompression ratios to different documents based on the rank information\nobtained from the coarse-grained compression, thereby preserving more\nimportant information from important documents.\n\n#  How to improve the integrity of key information?\n\nSince LongLLMLingua is a token-level prompt compression, it will inevitably\ndelete some tokens of the word, which may result in some retrieval-related\ntasks not getting complete results. But this can actually be recovered through\na simple subsequence matching method. ", "mimetype": "text/plain", "start_char_idx": 6581, "end_char_idx": 8278, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "089fbe81-bfa7-4cb1-9b5a-9aa4a231ce5e": {"__data__": {"id_": "089fbe81-bfa7-4cb1-9b5a-9aa4a231ce5e", "embedding": null, "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "324cafbc-56c7-404b-b679-7ed0ea4b23d9", "node_type": "4", "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "a859d48892dc6e360c88b587077ec9d9f4e21bc29f58b3da751982d8adfd78d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8b58843-89ce-4db3-b16a-31dca33f9e38", "node_type": "1", "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "78760133b7a0a0594d2858882b56485ecc5028f4e98a1b2ca1adc416e5bd04ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8325e61-2985-4bfb-987a-a8ae2fa3fc43", "node_type": "1", "metadata": {}, "hash": "b414574c0d29bceff22d7e573562c6a297023bfd2582a01c9929e0fb0141ce20", "class_name": "RelatedNodeInfo"}}, "text": "Specifically, there is a subsequence\nrelationship between the original prompt, compressed prompt and response. By\nestablishing the mapping relationship between the response subsequence that\nappears in the compressed prompt and the subsequence of the original prompt,\nthe original prompt content can be effectively recovered.\n\n#  Experiments\n\nTo evaluate the effectiveness of LongLLMLingua, we conducted detailed tests in\nMulti-document QA (RAG) and two long Context benchmarks. Particularly, the\ndataset chosen for Multi-document QA is very close to the actual RAG scenario\n(e.g. Bing Chat), where **Contriever** (one of the state-of-the-art retrieval\nsystems) is used to recall 20 relevant documents including one ground-truth.\nThe original documents have a high semantic relevance with the question.\n\nAs can be seen, compared to Retrieval-based methods and compression-based\nmethods, LongLLMLingua improves performance more in the RAG scenario, and can\nincrease up to 21.4 points at a 4x compression rate, avoiding the original\n\u201clost in the middle\u201d situation.\n\nThe results of the two benchmarks, LongBench and ZeroScrolls, also reached\nsimilar conclusions. LongLLMLingua is better at retaining key information\nrelated to the question in long context scenarios.\n\nBesides, LongLLMLingua is very efficient and can speed up the end-to-end\ninference process.\n\n#  Used in LlamaIndex\n\nThank [ Jerry Liu ](https://medium.com/u/e76da1c45ef7?source=post_page-----\n54b559b9ddf7--------------------------------) for your help with the\nLongLLMLingua project. Now you can use LongLLMLingua as a\n**NodePostprocessor** in this widely used RAG framework. For specific usage,\nyou can refer to the [ example 1\n](https://github.com/microsoft/LLMLingua/blob/main/examples/RAGLlamaIndex.ipynb)\n, [ example 2 ](https://github.com/run-\nllama/llama_index/blob/main/docs/examples/node_postprocessor/LongLLMLingua.ipynb)\nand the following code.\n\n    \n    \n    from llama_index.query_engine import RetrieverQueryEngine\n    from llama_index.response_synthesizers import CompactAndRefine\n    from llama_index.indices.postprocessor import LongLLMLinguaPostprocessor\n    \n    node_postprocessor = LongLLMLinguaPostprocessor(\n        instruction_str=\"Given the context, please answer the final question\",\n        target_token=300,\n        rank_method=\"longllmlingua\",\n        additional_compress_kwargs={\n            \"condition_compare\": True,\n            \"condition_in_question\": \"after\",\n            \"context_budget\": \"+100\",\n            \"reorder_context\": \"sort\",  # enable document reorder\n            \"dynamic_context_compression_ratio\": 0.4, # enable dynamic compression ratio\n        },\n    )\n\n#  References\n\n[1] Lost in the Middle: How Language Models Use Long Contexts. Nelson F. Liu\netc. ", "mimetype": "text/plain", "start_char_idx": 8278, "end_char_idx": 11045, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8325e61-2985-4bfb-987a-a8ae2fa3fc43": {"__data__": {"id_": "f8325e61-2985-4bfb-987a-a8ae2fa3fc43", "embedding": null, "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "324cafbc-56c7-404b-b679-7ed0ea4b23d9", "node_type": "4", "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "a859d48892dc6e360c88b587077ec9d9f4e21bc29f58b3da751982d8adfd78d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "089fbe81-bfa7-4cb1-9b5a-9aa4a231ce5e", "node_type": "1", "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}, "hash": "5704de50b053db066db0b5d810a740f92eae75e3ede1d0b5f9d22c60a305d663", "class_name": "RelatedNodeInfo"}}, "text": "[2] Compressing Context to Enhance Inference Efficiency of Large Language\nModels. Yucheng Li etc. [3] LLMLingua: Compressing Prompts for Accelerated\nInference of Large Language Models. Huiqiang Jiang, Qianhui Wu etc. [4]\nRECOMP: Improving Retrieval-Augmented LMs with Compression and Selective\nAugmentation. Fangyuan Xu etc. [5] Retrieval meets Long Context Large Language\nModels. Peng Xu etc. [6] Walking Down the Memory Maze: Beyond Context Limit\nthrough Interactive Reading. Howard Chen etc.\n\n", "mimetype": "text/plain", "start_char_idx": 11045, "end_char_idx": 11541, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd358be3-afc4-4fb7-9b49-55586b6c9298": {"__data__": {"id_": "cd358be3-afc4-4fb7-9b49-55586b6c9298", "embedding": null, "metadata": {"filename": "boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.md", "extension": ".md", "title": "Boosting RAG: Picking the Best Embedding & Reranker models", "date": "Nov 3, 2023", "url": "https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "524180ae-69c6-4a66-87dc-435ff90bbbe2", "node_type": "4", "metadata": {"filename": "boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.md", "extension": ".md", "title": "Boosting RAG: Picking the Best Embedding & Reranker models", "date": "Nov 3, 2023", "url": "https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83"}, "hash": "144d30c532e0adf91d7eb771ce57dd8cb54f6d894b95775e9e45cd06dfdd2187", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c722851-0779-44b8-a2ab-896ceff2ba75", "node_type": "1", "metadata": {}, "hash": "ec9ba4bed6b00c38bf84237a84ca27b0bacaff104e1d47769f6e03581cdd92f7", "class_name": "RelatedNodeInfo"}}, "text": "**UPDATE** : The pooling method for the Jina AI embeddings has been adjusted\nto use mean pooling, and the results have been updated accordingly. Notably,\nthe ` JinaAI-v2-base-en ` with ` bge-reranker-large ` now exhibits a Hit Rate\nof 0.938202 and an MRR (Mean Reciprocal Rank) of 0.868539 and with `\nCohereRerank ` exhibits a Hit Rate of 0.932584, and an MRR of 0.873689.\n\nWhen building a Retrieval Augmented Generation (RAG) pipeline, one key\ncomponent is the Retriever. We have a variety of embedding models to choose\nfrom, including OpenAI, CohereAI, and open-source sentence transformers.\nAdditionally, there are several rerankers available from CohereAI and sentence\ntransformers.\n\nBut with all these options, how do we determine the best mix for top-notch\nretrieval performance? How do we know which embedding model fits our data\nbest? Or which reranker boosts our results the most?\n\nIn this blog post, we\u2019ll use the ` Retrieval Evaluation ` module from\nLlamaIndex to swiftly determine the best combination of embedding and reranker\nmodels. Let's dive in!\n\nLet\u2019s first start with understanding the metrics available in ` Retrieval\nEvaluation `\n\n#  Understanding Metrics in Retrieval Evaluation:\n\nTo gauge the efficacy of our retrieval system, we primarily relied on two\nwidely accepted metrics: **Hit Rate** and **Mean Reciprocal Rank (MRR)** .\nLet\u2019s delve into these metrics to understand their significance and how they\noperate.\n\n**Hit Rate:**\n\nHit rate calculates the fraction of queries where the correct answer is found\nwithin the top-k retrieved documents. In simpler terms, it\u2019s about how often\nour system gets it right within the top few guesses.\n\n**Mean Reciprocal Rank (MRR):**\n\nFor each query, MRR evaluates the system\u2019s accuracy by looking at the rank of\nthe highest-placed relevant document. Specifically, it\u2019s the average of the\nreciprocals of these ranks across all the queries. So, if the first relevant\ndocument is the top result, the reciprocal rank is 1; if it\u2019s second, the\nreciprocal rank is 1/2, and so on.\n\nNow that we\u2019ve established the scope and familiarized ourselves with the\nmetrics, it\u2019s time to dive into the experiment. For a hands-on experience, you\ncan also follow along using our [ Google Colab Notebook\n](https://colab.research.google.com/drive/1TxDVA__uimVPOJiMEQgP5fwHiqgKqm4-?usp=sharing)\n\n#  Setting Up the Environment\n\n    \n    \n    !pip install llama-index sentence-transformers cohere anthropic voyageai protobuf pypdf\n\n#  Setting Up the Keys\n\n    \n    \n    openai_api_key = 'YOUR OPENAI API KEY'\n    cohere_api_key = 'YOUR COHEREAI API KEY'\n    anthropic_api_key = 'YOUR ANTHROPIC API KEY'\n    openai.api_key = openai_api_key\n\n#  Download the Data\n\nWe will use Llama2 paper for this experiment. Let\u2019s download the paper.\n\n    \n    \n    ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2786, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c722851-0779-44b8-a2ab-896ceff2ba75": {"__data__": {"id_": "1c722851-0779-44b8-a2ab-896ceff2ba75", "embedding": null, "metadata": {"filename": "boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.md", "extension": ".md", "title": "Boosting RAG: Picking the Best Embedding & Reranker models", "date": "Nov 3, 2023", "url": "https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "524180ae-69c6-4a66-87dc-435ff90bbbe2", "node_type": "4", "metadata": {"filename": "boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.md", "extension": ".md", "title": "Boosting RAG: Picking the Best Embedding & Reranker models", "date": "Nov 3, 2023", "url": "https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83"}, "hash": "144d30c532e0adf91d7eb771ce57dd8cb54f6d894b95775e9e45cd06dfdd2187", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd358be3-afc4-4fb7-9b49-55586b6c9298", "node_type": "1", "metadata": {"filename": "boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.md", "extension": ".md", "title": "Boosting RAG: Picking the Best Embedding & Reranker models", "date": "Nov 3, 2023", "url": "https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83"}, "hash": "68f89a12d706e4e26fe96e1f77a4318f9c8dc0399188ea9d6a0066bb7d0b1a22", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "54e3f422-fa81-4877-a646-9e4f48b07b9d", "node_type": "1", "metadata": {}, "hash": "69d2b5313d0ddd740a7d48e75b8bbabf71b2e15d6f745b01a4c940835af6fab3", "class_name": "RelatedNodeInfo"}}, "text": "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"llama2.pdf\"\n\n#  Load the Data\n\nLet\u2019s load the data. We will use Pages from start to 36 for the experiment\nwhich excludes table of contents, references, and appendix.\n\nThis data was then parsed by converted to nodes, which represent chunks of\ndata we\u2019d like to retrieve. We did use chunk_size as 512.\n\n    \n    \n    documents = SimpleDirectoryReader(input_files=[\"llama2.pdf\"]).load_data()\n    \n    node_parser = SimpleNodeParser.from_defaults(chunk_size=512)\n    nodes = node_parser.get_nodes_from_documents(documents)\n\n#  Generating Question-Context Pairs:\n\nFor evaluation purposes, we created a dataset of question-context pairs. This\ndataset can be seen as a set of questions and their corresponding context from\nour data. To remove bias for the evaluation of embedding(OpenAI/ CohereAI) and\nReranker (CohereAI), we use Anthropic LLM to generate Question-Context Pairs.\n\nLet\u2019s initialize a prompt template to generate question-context pairs.\n\n    \n    \n    # Prompt to generate questions\n    qa_generate_prompt_tmpl = \"\"\"\\\n    Context information is below.\n    \n    ---------------------\n    {context_str}\n    ---------------------\n    \n    Given the context information and not prior knowledge.\n    generate only questions based on the below query.\n    \n    ", "mimetype": "text/plain", "start_char_idx": 2786, "end_char_idx": 4120, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "54e3f422-fa81-4877-a646-9e4f48b07b9d": {"__data__": {"id_": "54e3f422-fa81-4877-a646-9e4f48b07b9d", "embedding": null, "metadata": {"filename": "boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.md", "extension": ".md", "title": "Boosting RAG: Picking the Best Embedding & Reranker models", "date": "Nov 3, 2023", "url": "https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "524180ae-69c6-4a66-87dc-435ff90bbbe2", "node_type": "4", "metadata": {"filename": "boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.md", "extension": ".md", "title": "Boosting RAG: Picking the Best Embedding & Reranker models", "date": "Nov 3, 2023", "url": "https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83"}, "hash": "144d30c532e0adf91d7eb771ce57dd8cb54f6d894b95775e9e45cd06dfdd2187", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c722851-0779-44b8-a2ab-896ceff2ba75", "node_type": "1", "metadata": {"filename": "boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.md", "extension": ".md", "title": "Boosting RAG: Picking the Best Embedding & Reranker models", "date": "Nov 3, 2023", "url": "https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83"}, "hash": "3a9b9e921ccc42c04f72b5f075fbeb4821aa41c37e97c61416a2a505a6f6b1d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7446b39-edf0-46c4-bbfa-49b4ec76863e", "node_type": "1", "metadata": {}, "hash": "f47aaeac03d9018e72f6bbd46a38e902e4ec7c5fa9c9584a45992ca366cb7fc7", "class_name": "RelatedNodeInfo"}}, "text": "You are a Professor. Your task is to setup \\\n    {num_questions_per_chunk} questions for an upcoming \\\n    quiz/examination. The questions should be diverse in nature \\\n    across the document. The questions should not contain options, not start with Q1/ Q2. \\\n    Restrict the questions to the context information provided.\\\n    \"\"\"\n    \n    \n    llm = Anthropic(api_key=anthropic_api_key)\n    qa_dataset = generate_question_context_pairs(\n        nodes, llm=llm, num_questions_per_chunk=2\n    )\n\nFunction to filter out sentences such as \u2014 ` Here are 2 questions based on\nprovided context `\n\n    \n    \n    # function to clean the dataset\n    def filter_qa_dataset(qa_dataset):\n        \"\"\"\n        Filters out queries from the qa_dataset that contain certain phrases and the corresponding\n        entries in the relevant_docs, and creates a new EmbeddingQAFinetuneDataset object with\n        the filtered data.\n    \n        :param qa_dataset: An object that has 'queries', 'corpus', and 'relevant_docs' attributes.\n        :return: An EmbeddingQAFinetuneDataset object with the filtered queries, corpus and relevant_docs.\n        \"\"\"\n    \n        # Extract keys from queries and relevant_docs that need to be removed\n        queries_relevant_docs_keys_to_remove = {\n            k for k, v in qa_dataset.queries.items()\n            if 'Here are 2' in v or 'Here are two' in v\n        }\n    \n        # Filter queries and relevant_docs using dictionary comprehensions\n        filtered_queries = {\n            k: v for k, v in qa_dataset.queries.items()\n            if k not in queries_relevant_docs_keys_to_remove\n        }\n        filtered_relevant_docs = {\n            k: v for k, v in qa_dataset.relevant_docs.items()\n            if k not in queries_relevant_docs_keys_to_remove\n        }\n    \n        # Create a new instance of EmbeddingQAFinetuneDataset with the filtered data\n        return EmbeddingQAFinetuneDataset(\n            queries=filtered_queries,\n            corpus=qa_dataset.corpus,\n            relevant_docs=filtered_relevant_docs\n        )\n    \n    # filter out pairs with phrases `Here are 2 questions based on provided context`\n    qa_dataset = filter_qa_dataset(qa_dataset)\n\n#  Custom Retriever:\n\nTo identify the optimal retriever, we employ a combination of an embedding\nmodel and a reranker. Initially, we establish a base ` VectorIndexRetriever `\n. ", "mimetype": "text/plain", "start_char_idx": 4120, "end_char_idx": 6492, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7446b39-edf0-46c4-bbfa-49b4ec76863e": {"__data__": {"id_": "c7446b39-edf0-46c4-bbfa-49b4ec76863e", "embedding": null, "metadata": {"filename": "boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.md", "extension": ".md", "title": "Boosting RAG: Picking the Best Embedding & Reranker models", "date": "Nov 3, 2023", "url": "https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "524180ae-69c6-4a66-87dc-435ff90bbbe2", "node_type": "4", "metadata": {"filename": "boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.md", "extension": ".md", "title": "Boosting RAG: Picking the Best Embedding & Reranker models", "date": "Nov 3, 2023", "url": "https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83"}, "hash": "144d30c532e0adf91d7eb771ce57dd8cb54f6d894b95775e9e45cd06dfdd2187", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54e3f422-fa81-4877-a646-9e4f48b07b9d", "node_type": "1", "metadata": {"filename": "boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.md", "extension": ".md", "title": "Boosting RAG: Picking the Best Embedding & Reranker models", "date": "Nov 3, 2023", "url": "https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83"}, "hash": "37037d13978d7240d6e7c77d48ff62b005607eeddce4f56d09ab871389204808", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7ee99ec-2767-4115-8495-04f916ddc3cc", "node_type": "1", "metadata": {}, "hash": "56449b7ba5ff0dec522d3b1a542b1a1f662b921d3334de73c9073df6869cdff4", "class_name": "RelatedNodeInfo"}}, "text": "Upon retrieving the nodes, we then introduce a reranker to further refine\nthe results. It\u2019s worth noting that for this particular experiment, we\u2019ve set\nsimilarity_top_k to 10 and picked top-5 with reranker. However, feel free to\nadjust this parameter based on the needs of your specific experiment. We are\nshowing the code here with ` OpenAIEmbedding ` , please refer to the [\nnotebook\n](https://colab.research.google.com/drive/1TxDVA__uimVPOJiMEQgP5fwHiqgKqm4-?usp=sharing)\nfor code with other embeddings.\n\n    \n    \n    embed_model = OpenAIEmbedding()\n    service_context = ServiceContext.from_defaults(llm=None, embed_model = embed_model)\n    vector_index = VectorStoreIndex(nodes, service_context=service_context)\n    vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k = 10)\n    \n    \n    class CustomRetriever(BaseRetriever):\n        \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"\n    \n        def __init__(\n            self,\n            vector_retriever: VectorIndexRetriever,\n        ) -&gt; None:\n            \"\"\"Init params.\"\"\"\n    \n            self._vector_retriever = vector_retriever\n    \n        def _retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:\n            \"\"\"Retrieve nodes given query.\"\"\"\n    \n        retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n    \n        if reranker != 'None':\n          retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n           else:\n              retrieved_nodes = retrieved_nodes[:5]\n             \n           return retrieved_nodes\n    \n        async def _aretrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:\n            \"\"\"Asynchronously retrieve nodes given query.\n    \n            Implemented by the user.\n    \n            \"\"\"\n            return self._retrieve(query_bundle)\n    \n        async def aretrieve(self, str_or_query_bundle: QueryType) -&gt; List[NodeWithScore]:\n            if isinstance(str_or_query_bundle, str):\n                str_or_query_bundle = QueryBundle(str_or_query_bundle)\n            return await self._aretrieve(str_or_query_bundle)\n    \n    custom_retriever = CustomRetriever(vector_retriever)\n\n#  Evaluation:\n\nTo evaluate our retriever, we computed the Mean Reciprocal Rank (MRR) and Hit\nRate metrics:\n\n    \n    \n    retriever_evaluator = RetrieverEvaluator.from_metric_names(\n        [\"mrr\", \"hit_rate\"], retriever=custom_retriever\n    )\n    eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)\n\n#  Results:\n\nWe put various embedding models and rerankers to the test. Here are the models\nwe considered:\n\n**Embedding Models** :\n\n  * [ OpenAI Embedding ](https://platform.openai.com/docs/guides/embeddings)\n  * [ Voyage Embedding ](https://www.voyageai.com/)\n  * [ CohereAI Embedding ](https://txt.cohere.com/introducing-embed-v3/) (v2.0/ v3.0) \n  * [ Jina Embeddings ](https://huggingface.co/jinaai/jina-embeddings-v2-small-en) (small/ base) \n  * [ BAAI/bge-large-en ](https://huggingface.co/BAAI/bge-large-en)\n  * [ Google PaLM Embedding ](https://developers.generativeai.google/tutorials/embeddings_quickstart)\n\n**Rerankers** :\n\n  * [ CohereAI ](https://txt.cohere.com/rerank/)\n  * [ bge-reranker-base ](https://huggingface.co/BAAI/bge-reranker-base)\n  * [ bge-reranker-large ](https://huggingface.co/BAAI/bge-reranker-large)\n\n> It\u2019s worth mentioning that these results provide a solid insight into\n> performance for this particular dataset and task. However, actual outcomes\n> may differ based on data characteristics, dataset size, and other variables\n> like chunk_size, similarity_top_k, and so on.\n\nThe table below showcases the evaluation results based on the metrics of Hit\nRate and Mean Reciprocal Rank (MRR):\n\n#  Analysis:\n\n##  **Performance by Embedding:**\n\n  * **OpenAI** : Showcases top-tier performance, especially with the ` **CohereRerank** ` (0.926966 hit rate, 0.86573 MRR) and ` **bge-reranker-large** ` (0.910112 hit rate, 0.855805 MRR), indicating strong compatibility with reranking tools. \n  * **bge-large** : Experiences significant improvement with rerankers, with the best results from ` **CohereRerank** ` (0.876404 hit rate, 0.822753 MRR). \n  * **llm-embedder** : Benefits greatly from reranking, particularly with ` **CohereRerank** ` (0.882022 hit rate, 0.830243 MRR), which offers a substantial performance boost. \n  * **Cohere** : Cohere\u2019s latest v3.0 embeddings outperform v2.0 and, with the integration of native CohereRerank, significantly improve its metrics, boasting a 0.88764 hit rate and a 0.836049 MRR. \n  * **Voyage** : Has strong initial performance that is further amplified by ` **CohereRerank** ` (0.91573 hit rate, 0.851217 MRR), suggesting high responsiveness to reranking. \n  * **JinaAI** : Very strong performance, sees notable gains with ` **bge-reranker-large** ` (0.938202 hit rate, 0.868539 MRR) and ` **CohereRerank** ` (0.932584 hit rate, 0.873689), indicating that reranking significantly boosts its performance. \n  * **Google-PaLM** : The model demonstrates strong performance, with measurable gains when using the ` **CohereRerank** ` (0.910112 hit rate, 0.855712 MRR). This indicates that reranking provides a clear boost to its overall results. \n\n##  **Impact of Rerankers** :\n\n  * **WithoutReranker** : This provides the baseline performance for each embedding. \n  * **bge-reranker-base** : Generally improves both hit rate and MRR across embeddings. \n  * **bge-reranker-large** : This reranker frequently offers the highest or near-highest MRR for embeddings. For several embeddings, its performance rivals or surpasses that of the ` **CohereRerank** ` . \n  * **CohereRerank** : Consistently enhances performance across all embeddings, often providing the best or near-best results. \n\n##  **Necessity of Rerankers** :\n\n  * The data clearly indicates the significance of rerankers in refining search results. Nearly all embeddings benefit from reranking, showing improved hit rates and MRRs. \n  * Rerankers, especially ` **CohereRerank** ` , have demonstrated their capability to transform any embedding into a competitive one. \n\n##  **Overall Superiority** :\n\n  * When considering both hit rate and MRR, the combinations of ` **OpenAI + CohereRerank** ` and ` **JinaAI-Base + bge-reranker-large/ CohereRerank** ` emerge as top contenders. \n  * However, the consistent improvement brought by the ` **CohereRerank/ bge-reranker-large** ` rerankers across various embeddings make them the standout choice for enhancing search quality, regardless of the embedding in use. \n\nIn summary, to achieve the peak performance in both hit rate and MRR, the\ncombination of ` **OpenAI** ` or ` **JinaAI-Base** ` embeddings with the `\n**CohereRerank/bge-reranker-large** ` reranker stands out.\n\n> Please be aware that our benchmarks are intended to offer a reproducible\n> script for your own data. ", "mimetype": "text/plain", "start_char_idx": 6492, "end_char_idx": 13377, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7ee99ec-2767-4115-8495-04f916ddc3cc": {"__data__": {"id_": "c7ee99ec-2767-4115-8495-04f916ddc3cc", "embedding": null, "metadata": {"filename": "boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.md", "extension": ".md", "title": "Boosting RAG: Picking the Best Embedding & Reranker models", "date": "Nov 3, 2023", "url": "https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "524180ae-69c6-4a66-87dc-435ff90bbbe2", "node_type": "4", "metadata": {"filename": "boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.md", "extension": ".md", "title": "Boosting RAG: Picking the Best Embedding & Reranker models", "date": "Nov 3, 2023", "url": "https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83"}, "hash": "144d30c532e0adf91d7eb771ce57dd8cb54f6d894b95775e9e45cd06dfdd2187", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7446b39-edf0-46c4-bbfa-49b4ec76863e", "node_type": "1", "metadata": {"filename": "boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.md", "extension": ".md", "title": "Boosting RAG: Picking the Best Embedding & Reranker models", "date": "Nov 3, 2023", "url": "https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83"}, "hash": "74ac1f33c1913d430e4afd729f4c96d0d7d2e2d81d1889d9483aaed270906005", "class_name": "RelatedNodeInfo"}}, "text": "Nevertheless, treat these figures as estimates and\n> proceed with caution when interpreting them.\n\n#  Conclusions:\n\nIn this blog post, we have demonstrated how to evaluate and enhance retriever\nperformance using various embeddings and rerankers. Below are our final\nconclusions.\n\n  * **Embeddings** : The ` **OpenAI** ` and ` **JinaAI-Base** ` embeddings, especially when paired with the ` **CohereRerank/bge-reranker-large** ` reranker, set the gold standard for both hit rate and MRR. \n  * **Rerankers** : The influence of rerankers, particularly ` **CohereRerank/bge-reranker-large** ` , cannot be overstated. They play a key role in improving the MRR for many embeddings, showing their importance in making search results better. \n  * **Foundation is Key** : Choosing the right embedding for the initial search is essential; even the best reranker can\u2019t help much if the basic search results aren\u2019t good. \n  * **Working Together:** To get the best out of retrievers, it\u2019s important to find the right mix of embeddings and rerankers. This study shows how important it is to carefully test and find the best pairing. \n\n", "mimetype": "text/plain", "start_char_idx": 13377, "end_char_idx": 14498, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "595c0b11-b3e7-4840-9c73-bb1ac92b924a": {"__data__": {"id_": "595c0b11-b3e7-4840-9c73-bb1ac92b924a", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-10-31-36244e2b3f0c.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201331", "date": "Oct 31, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-31-36244e2b3f0c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0af58c14-b89b-4e71-b0c0-f44aecc01df8", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-10-31-36244e2b3f0c.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201331", "date": "Oct 31, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-31-36244e2b3f0c"}, "hash": "c484a7a71882aca88dab7390a7c2cba4e9d8f45135dcf9d23a16f149713ef431", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e511e3c2-9221-4837-ba18-6686c38be5ac", "node_type": "1", "metadata": {}, "hash": "68d9d167e4887fadf3b6a6594a0f84125142ba6aad0a842624f9fe51b05d0223", "class_name": "RelatedNodeInfo"}}, "text": "Greetings Llama Enthusiasts !\n\nAnother week has zoomed past, and here we are with our latest roundup of\nupdates, features, tutorials, and so much more. Have a noteworthy project,\narticle, or video to share? ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 207, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e511e3c2-9221-4837-ba18-6686c38be5ac": {"__data__": {"id_": "e511e3c2-9221-4837-ba18-6686c38be5ac", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-10-31-36244e2b3f0c.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201331", "date": "Oct 31, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-31-36244e2b3f0c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0af58c14-b89b-4e71-b0c0-f44aecc01df8", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-10-31-36244e2b3f0c.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201331", "date": "Oct 31, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-31-36244e2b3f0c"}, "hash": "c484a7a71882aca88dab7390a7c2cba4e9d8f45135dcf9d23a16f149713ef431", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "595c0b11-b3e7-4840-9c73-bb1ac92b924a", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-10-31-36244e2b3f0c.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201331", "date": "Oct 31, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-31-36244e2b3f0c"}, "hash": "dd42827fdd129213912eeda0e4b1a902243df35b310b9ac1b91721eff95b8f4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b636bcb5-d351-4b53-803c-7d1eb05678ff", "node_type": "1", "metadata": {}, "hash": "fb7e96d79594fb2b7cc34f4d03309faca772e9ed106e95dbd37157dd6553522b", "class_name": "RelatedNodeInfo"}}, "text": "We\u2019d love to feature it! Reach out to us at [\nnews@llamaindex.ai ](mailto:news@llamaindex.ai) .\n\nWant these updates straight to your inbox? Simply subscribe to our newsletter\non our [ homepage ](https://www.llamaindex.ai/) .\n\n**First, the highlights:**\n\n  1. **Revamped Documentation:** Overhauled [ docs ](https://docs.llamaindex.ai/en/stable/) for smoother LLM/RAG app development. \n  2. **Contribution Board:** Our new [ board ](https://github.com/orgs/run-llama/projects/2) welcomes community-driven LlamaIndex enhancements. \n  3. **Zephyr-7b-beta Insights:** [ Tested and verified ](https://colab.research.google.com/drive/1UoPcoiA5EOBghxWKWduQhChliMHxla7U?usp=sharing) for unmatched ReAct agent task efficiency on LlamaIndex. \n  4. **Image Captioning Boost For RAG:** LLaVa\u2019s outputs are now supercharged with knowledge-based augmentation. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/llava_multi_modal_tesla_10q.ipynb) , [ Tweet ](https://x.com/jerryjliu0/status/1717205234269983030?s=20)\n\n**Feature Releases and Enhancements:**\n\n  * We introduced Retrieval-Augmented Image Captioning, enhancing LLaVa multi-modal model outputs with knowledge base insights. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/llava_multi_modal_tesla_10q.ipynb) , [ Tweet ](https://x.com/jerryjliu0/status/1717205234269983030?s=20) . \n  * We introduced the ability to view and set prompts for LlamaIndex modules in just two lines of code. [ Docs ](https://github.com/run-llama/llama_index/blob/main/docs/examples/prompts/prompt_mixin.ipynb) , [ Tweet ](https://twitter.com/llama_index/status/1716847554401628516?s=20) . \n  * We introduced the integration of our ` OpenAILike ` class, allowing users to tap into various open-source LLM projects with OpenAI-compatible APIs, irrespective of the model provider. [ Tweet ](https://x.com/llama_index/status/1716950167474356715?s=20) . \n  * We introduced Prompt Compression for RAG: with LongLLMLingua, which helps to cut token usage and latency by up to 20x. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/node_postprocessor/LongLLMLingua.ipynb) , [ Tweet ](https://x.com/llama_index/status/1717601235828973681?s=20) . \n  * We introduced a method to refine open-source LLMs like llama2 for structured data outputs. Using LlamaIndex, transform llama2\u20137b to produce Pydantic objects without PyTorch. Our guide covers synthetic dataset creation, fine-tuning, and RAG pipeline integration. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/finetuning/gradient/gradient_structured.ipynb) , [ Tweet ](https://x.com/llama_index/status/1718299602884137182?s=20) . \n\n**** Demos:\n\n  * Harshad Suryawanshi did a [ demo ](https://ai-eqty-rsrch-anlyst.streamlit.app/) on equity research report generator using LlamaIndex and Streamlit. \n  * [ Bharat Ramanathan ](https://twitter.com/ParamBharat) built [ Wandbot ](https://x.com/llama_index/status/1717940770270011898?s=20) , a live RAG app enabling chat over Weights & Biases documentation, integrated with Discord and SlackHQ. Key features include periodic data ingestion, custom document and code parsing, model fallback, and logging with Weights and biases. \n\n**Guides:**\n\n  * We introduced a revamped documentation structure tailored to guide users from prototyping to production of LLM/RAG apps using LlamaIndex. Dive into our 200+ guides to enhance your app. [ Docs ](https://docs.llamaindex.ai/en/stable/) , [ Tweet ](https://twitter.com/llama_index/status/1717627450690269284?s=20) . \n  * We unveil our new Request For Contribution Github board [ here ](https://github.com/orgs/run-llama/projects/2) . It\u2019s your guide to contribute to LlamaIndex, streamlining community suggestions. \n  * We released the [ guide ](https://github.com/run-llama/llama_index/blob/main/docs/examples/embeddings/jina_embeddings.ipynb) on using the Jina 8k open-source text embedding model with LlamaIndex. \n  * We introduce our comprehensive survey of llama2-chat models across varying capacities in LlamaIndex. The major insight: While reasoning is enhanced with more parameters, structured outputs remain a challenge. [ Tweet ](https://twitter.com/llama_index/status/1717337923853664573?s=20) . \n  * We share a [ guide ](https://colab.research.google.com/drive/1UoPcoiA5EOBghxWKWduQhChliMHxla7U?usp=sharing) to test the newly released HuggingFace Zephyr-7b-beta model on LlamaIndex RAG/agent tasks, it stood out as the only 7B LLM capable of handling ReAct agent tasks over data. \n  * We share a new [ guide ](https://github.com/run-llama/llama_index/blob/main/docs/examples/prompts/prompts_rag.ipynb) on Advanced Prompt Engineering for RAG. Learn about understanding, customizing, and extending RAG prompts, from QA templates to few-shot examples and context/query transformations. [ Tweet ](https://x.com/llama_index/status/1718657660697149509?s=20) . \n\n**Tutorials:**\n\n  * [ Kiran ](https://www.linkedin.com/in/kirannpanicker/) made a [ blog post ](/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125) on Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser. \n  * [ Wenqi Glantz ](https://medium.com/@wenqiglantz) gave us an excellent [ blog post ](https://levelup.gitconnected.com/optimizing-text-embeddings-with-huggingfaces-text-embeddings-inference-server-and-llamaindex-ef7df35882a4) on Optimizing Text Embeddings with HuggingFace\u2019s text-embeddings-inference Server and LlamaIndex. \n  * [ Ravi Theja\u2019s ](https://www.linkedin.com/in/ravidesetty/) [ blog post ](/nvidia-research-rag-with-long-context-llms-7d94d40090c4) delves into NVIDIA Research on RAG vs Long Context LLMs, questioning the necessity of RAG in the presence of long-context LLMs. \n  * [ Sudarshan Koirala ](https://twitter.com/mesudarshan) has a tutorial on Extracting Tables + Texts from .htm pages for RAG Using LlamaIndex. \n  * [ Wenqi Glantz ](https://medium.com/@wenqiglantz) also made a second [ blog post ](https://levelup.gitconnected.com/multimodal-retrieval-with-text-embedding-and-clip-image-embedding-for-backyard-birds-599f19057a70) on Multimodal Retrieval with Text Embedding and CLIP Image Embedding for Backyard Birds. \n\n**Integrations & Collaborations: **\n\n  * We introduced our new cookbooks in partnership with Gradient AI, enabling effortless fine-tuning of open-source LLMs like Llama 2 and integration into your LlamaIndex RAG pipeline. ", "mimetype": "text/plain", "start_char_idx": 207, "end_char_idx": 6737, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b636bcb5-d351-4b53-803c-7d1eb05678ff": {"__data__": {"id_": "b636bcb5-d351-4b53-803c-7d1eb05678ff", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-10-31-36244e2b3f0c.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201331", "date": "Oct 31, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-31-36244e2b3f0c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0af58c14-b89b-4e71-b0c0-f44aecc01df8", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-10-31-36244e2b3f0c.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201331", "date": "Oct 31, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-31-36244e2b3f0c"}, "hash": "c484a7a71882aca88dab7390a7c2cba4e9d8f45135dcf9d23a16f149713ef431", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e511e3c2-9221-4837-ba18-6686c38be5ac", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-10-31-36244e2b3f0c.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201331", "date": "Oct 31, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-31-36244e2b3f0c"}, "hash": "5ea33ef73f1a1898101bda85f5884a222cafded1bc6005b703dafbaf2b43b977", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b1973930-6a86-4dfc-b1a4-0cf7d4760fac", "node_type": "1", "metadata": {}, "hash": "e824318f24a0fe971d7164431e9269e908d2a6fec211dd3ff382309addc58d20", "class_name": "RelatedNodeInfo"}}, "text": "[ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/finetuning/gradient/gradient_text2sql.html) , [ Tweet ](https://x.com/jerryjliu0/status/1716476285046952049?s=20) . \n  * We introduced integration with HuggingFace Inference API which gives access to over 150,000 models. Now you can plugin any ` conversational ` , ` text_generation ` , ` feature_extraction ` endpoints into your LlamaIndex app. [ Docs ](https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/huggingface.ipynb) , [ Tweet ](https://twitter.com/llama_index/status/1716847554401628516?s=20) . \n\n**Webinars:**\n\n  * [ Mayo Oshin ](https://twitter.com/mayowaoshin) and [ Jerry Liu ](https://twitter.com/jerryjliu0) gave a [ webinar ](https://www.crowdcast.io/c/n0roka37yfw0) on Unlocking ChatGPT for Business. \n\nWorkshops:\n\n  * Jerry Liu and Simon conducted a Multipart LlamaIndex workshop in collaboration with Anyscale. \n  ", "mimetype": "text/plain", "start_char_idx": 6737, "end_char_idx": 7649, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1973930-6a86-4dfc-b1a4-0cf7d4760fac": {"__data__": {"id_": "b1973930-6a86-4dfc-b1a4-0cf7d4760fac", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-10-31-36244e2b3f0c.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201331", "date": "Oct 31, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-31-36244e2b3f0c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0af58c14-b89b-4e71-b0c0-f44aecc01df8", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-10-31-36244e2b3f0c.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201331", "date": "Oct 31, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-31-36244e2b3f0c"}, "hash": "c484a7a71882aca88dab7390a7c2cba4e9d8f45135dcf9d23a16f149713ef431", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b636bcb5-d351-4b53-803c-7d1eb05678ff", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-10-31-36244e2b3f0c.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201331", "date": "Oct 31, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-31-36244e2b3f0c"}, "hash": "ea78b13e44d53070850ed6179416b36649d0a01ca378e0a5a3a65637eccfe83f", "class_name": "RelatedNodeInfo"}}, "text": "* Ravi Theja conducted a day-long [ workshop ](https://hasgeek.com/fifthelephant/llamaindex-workshop-10-28/) on Retrieval Augmented Generation with LlamaIndex. \n\n", "mimetype": "text/plain", "start_char_idx": 7649, "end_char_idx": 7811, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6fe93808-454e-4eaf-b721-806769a9bb15": {"__data__": {"id_": "6fe93808-454e-4eaf-b721-806769a9bb15", "embedding": null, "metadata": {"filename": "newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.md", "extension": ".md", "title": "NewsGPT(Neotice): Summarize news articles with LlamaIndex \u2014 Hackathon winning app", "date": "Oct 27, 2023", "url": "https://www.llamaindex.ai/blog/newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a6424e4-e254-446e-ad5f-1516a078a21c", "node_type": "4", "metadata": {"filename": "newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.md", "extension": ".md", "title": "NewsGPT(Neotice): Summarize news articles with LlamaIndex \u2014 Hackathon winning app", "date": "Oct 27, 2023", "url": "https://www.llamaindex.ai/blog/newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11"}, "hash": "12f96da1279528149438dd096fb7a637affe637a82da475bf9eab818e6206765", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "def23383-0da5-463e-8a2f-248cd2cfe5bc", "node_type": "1", "metadata": {}, "hash": "28d281c554ce8cfd81add8dec093ece793bb629d5f388764bcf7eb7d8cbaaa75", "class_name": "RelatedNodeInfo"}}, "text": "We\u2019re excited to share with you the thought process and solution design of [\n**NewsGPT** ](https://newsgpt-clickbait-buster.streamlit.app/) **(** [\n**Neotice** ](https://www.neotice.app) **) \u2014 Clickbait Buster** , a Streamlit\nLLM hackathon-winning app powered by LlamaIndex, Streamlit, and Qdrant. In\nthis article, we\u2019ll define the problem we\u2019re trying to solve and discuss how\nwe approached it. Lastly, we offer a workaround to enable LlamaIndex streaming\non Streamlit Chat Bot and the all the code can be found [ here\n](https://github.com/timho102003/NewsGPT.git) . We hope you\u2019ll find our\ninsights helpful and informative.\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 627, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "def23383-0da5-463e-8a2f-248cd2cfe5bc": {"__data__": {"id_": "def23383-0da5-463e-8a2f-248cd2cfe5bc", "embedding": null, "metadata": {"filename": "newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.md", "extension": ".md", "title": "NewsGPT(Neotice): Summarize news articles with LlamaIndex \u2014 Hackathon winning app", "date": "Oct 27, 2023", "url": "https://www.llamaindex.ai/blog/newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a6424e4-e254-446e-ad5f-1516a078a21c", "node_type": "4", "metadata": {"filename": "newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.md", "extension": ".md", "title": "NewsGPT(Neotice): Summarize news articles with LlamaIndex \u2014 Hackathon winning app", "date": "Oct 27, 2023", "url": "https://www.llamaindex.ai/blog/newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11"}, "hash": "12f96da1279528149438dd096fb7a637affe637a82da475bf9eab818e6206765", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6fe93808-454e-4eaf-b721-806769a9bb15", "node_type": "1", "metadata": {"filename": "newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.md", "extension": ".md", "title": "NewsGPT(Neotice): Summarize news articles with LlamaIndex \u2014 Hackathon winning app", "date": "Oct 27, 2023", "url": "https://www.llamaindex.ai/blog/newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11"}, "hash": "32ef42b7fd938fefa5383d99732cff3a81d18d049d7823ac0e67f92f68ffd36d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ed8b746-3bb7-4c03-87a4-9ac4af8fc31e", "node_type": "1", "metadata": {}, "hash": "c74af43550290e9059e23223428f75758200366d847ba4166d50c477bd98316e", "class_name": "RelatedNodeInfo"}}, "text": "#  Introduction\n\n##  **Problem Statement**\n\nIt\u2019s evident that people\u2019s habits of consuming information have changed over\ntime. Previously, we read lengthy articles content and watched long videos,\nsuch as newspapers and YouTube videos. However, we currently prefer reading\ntitles and consuming short-form content, such as TikTok and YouTube shorts.\nAlthough this shift has made it easier to get more information in less time,\nit has also led to clickbait headlines that often contain incorrect\ninformation.\n\nWhen we started developing NewsGPT, our primary focus was to solve the above-\nstated pain points and provide a solution that 1) **provides accurate\ninformation** and 2) **saves time for users** .\n\n##  Neotice\n\nWe are excited to announce that the beta version of the [ **Neotice**\n](https://www.neotice.app) app, which is the production version of NewsGPT, is\nnow available for users to try out! We are grateful to the Streamlit Hackathon\nfor showcasing our prototype and philosophy. With the help of this platform,\nwe are confident that our app will revolutionize the way people consume news.\n\n##  **Why NewsGPT Stands Out**\n\nNewsGPT has four main components: Reliable News Sources, Tailored News\nRecommendations, Efficient Information Retrieval, and Time Saver.\n\n**Reliable News Sources:**\n\n  * We\u2019ve established a dynamic data pipeline designed to ingest daily news, ensuring our information is up-to-date and relevant. \n  * Sophisticated **Named-Entity Recognition, Text Embedding with OpenAI API,** and **asynchronous article embedding processes** are incorporated. This data is systematically stored in the [ Qdrant ](https://www.linkedin.com/company/qdrant/) Vector Database, promoting accuracy and efficiency. \n\n**Tailored News Recommendations:**\n\n  * Our system does more than just present news; it learns from you. By analyzing your reading habits, we leverage article embeddings to curate a personal news feed tailored to your interests. \n  * A versatile **search bar** is always at your disposal, letting users explore any news topics that capture their interest. \n\n**Efficient Information Retrieval:**\n\n  * With just a single click on an article of interest, NewsGPT gets to work. It collates **similar news from multiple sources (3\u20135)** and activates a Streamlit chatbot. \n  * Your engagement begins immediately: the first query is autonomously forwarded to our chatbot to fetch a concise news summary. \n  * For ease of user experience, we display **predefined prompts** as clickable buttons. This means users can receive information without the need for manual input. \n  * Curiosity welcomed: any questions users may have about the news article will be addressed as long as the answers are detailed within the source articles. \n\n**Time-Saving Reminder & Category Distribution Chart: **\n\n  * To keep you informed, our sidebar displays the time saved using NewsGPT and visually represents news category distribution. \n\n#  Delving Deep into Architecture\n\n##  Data Pipeline\n\nWe start with a reliable and sustainable data pipeline to support the users to\nget fresh news with two powerful libraries, **pygooglenews,** and\n**newspaper3k.**\n\n    \n    \n    pip install pygooglenews --upgrade\n    pip install newspaper3k\n\nBy utilizing **Spark batch processing** , we efficiently process data with\n**NER(Named-Entity Recognition)** and create embeddings via **OpenAI API (Ada\nmodel)** . After the preprocessing of the data, we collect the metadata,\nincluding keywords, NER results, summary, body, title, author, and so on, in\nthe payload and push the payload with embedding to the Qdrant Vector Database.\n\nWe will skip the part on how to create embeddings with the OpenAI Ada model,\nas there are many existing tutorials available. To perform Named Entity\nRecognition, we utilize the **dslim/bert-base-NER** model from HuggingFace.\n\n    \n    \n    from transformers import pipeline\n    from transformers import AutoTokenizer, AutoModelForTokenClassification\n    tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n    model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n    nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, batch_size=batch_size)\n    ner_results = nlp(texts)\n\n##  Personalization\n\nWhen accessing NewsGPT, we have two options: we can either create a new\naccount or use a guest login to test the service. It should be noted, however,\nthat using the guest login will not provide personalization. ", "mimetype": "text/plain", "start_char_idx": 627, "end_char_idx": 5092, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ed8b746-3bb7-4c03-87a4-9ac4af8fc31e": {"__data__": {"id_": "0ed8b746-3bb7-4c03-87a4-9ac4af8fc31e", "embedding": null, "metadata": {"filename": "newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.md", "extension": ".md", "title": "NewsGPT(Neotice): Summarize news articles with LlamaIndex \u2014 Hackathon winning app", "date": "Oct 27, 2023", "url": "https://www.llamaindex.ai/blog/newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a6424e4-e254-446e-ad5f-1516a078a21c", "node_type": "4", "metadata": {"filename": "newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.md", "extension": ".md", "title": "NewsGPT(Neotice): Summarize news articles with LlamaIndex \u2014 Hackathon winning app", "date": "Oct 27, 2023", "url": "https://www.llamaindex.ai/blog/newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11"}, "hash": "12f96da1279528149438dd096fb7a637affe637a82da475bf9eab818e6206765", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "def23383-0da5-463e-8a2f-248cd2cfe5bc", "node_type": "1", "metadata": {"filename": "newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.md", "extension": ".md", "title": "NewsGPT(Neotice): Summarize news articles with LlamaIndex \u2014 Hackathon winning app", "date": "Oct 27, 2023", "url": "https://www.llamaindex.ai/blog/newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11"}, "hash": "28c1b7cbf38f0dba74101517aada126e247badcc7b4ca223136d5234db520ad0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "369b8ffe-a535-4ab4-8ff2-1b9f7b88d747", "node_type": "1", "metadata": {}, "hash": "39d83a393f3a5a30067c5c533f5ac15d1f44318224b039c4f44bb95166990eeb", "class_name": "RelatedNodeInfo"}}, "text": "If we choose to\nsign up, we will be asked about our preferred news categories, which will help\nthe service make initial recommendations during the cold start.\n\nRecommendation Pipeline for Personalization\n\nAfter users sign up and log in, we query the preferences and activities from\nGoogle Firebase, if any; otherwise, use the favorite categories for\nrecommendation cold start. If the activities and preferences are available, we\nwill call the recommendation API hosted on AWS Lambda and generate\npersonalized articles for the users. The activities related to the user\nreading history. Users can indicate their preference for articles by clicking\nthumbs-up or down buttons on the article page. Some of the recommendation\nlogic can be found in the [ code\n](https://github.com/timho102003/NewsGPT/blob/main/utils.py#L174) .\n\nBesides the personalized feeds, users can also choose different categories or\nsearch for specific topics using the search bar.\n\n##  Chat with Article Powered by LlamaIndex\n\nLlamaIndex Pipeline\n\nUnlike other news aggregator apps, NewsGPT offers a unique service by\nproviding users with a summary and discrepancy key takeaway of a topic from\nvarious news sources. This feature not only saves users time by eliminating\nthe need to read multiple articles but also helps identify which information\ncan be trusted by comparing discrepancies. What is under the hood is when the\nuser clicks on the \u201cchat with article\u201d button, the system first uses a search\nAPI hosted on AWS Lambda to find related articles from various sources. Then,\nthe system utilizes the [ **LlamaIndex** ](https://www.llamaindex.ai) library\nto create a vector store and a query engine, which can be integrated with the\n**Streamlit** chat component to create an **RAG** application for information\nretrieval.\n\n##  Streaming Output with LlamaIndex and Streamlit\n\nThanks to the powerful library, S [ treamlit-Extras\n](https://extras.streamlit.app) , which provides additional functionality not\nofficially supported. To enhance the user experience and make it more like\nchatting with ChatGPT, we use the [ **streaming_write**\n](https://arnaudmiribel.github.io/streamlit-extras/extras/streaming_write/)\nfunctions from the Streamlit-Extras library. Additionally, we set\n**streaming=True** for the **query_engine** to ensure a smoother experience.\nLet\u2019s take a look at the code.\n\n    \n    \n    pip install streamlit-extras\n\nTo begin with, we set up the **service_context** .\n\n    \n    \n    from llama_index import ServiceContext\n    \n    st.session_state[\"service_context\"] = ServiceContext.from_defaults(\n                llm=OpenAI(\n                    model=\"gpt-3.5-turbo\",\n                    temperature=0.2,\n                    chunk_size=1024,\n                    chunk_overlap=100,\n                    system_prompt=\"As an expert current affairs commentator and analyst,\\\n                                   your task is to summarize the articles and answer the questions from the user related to the news articles\",\n                ),\n                chunk_size=256, \n                chunk_overlap=20\n            )\n\nNext, we create a text splitter using **TokenTextSplitter** and a node parser\nusing **SimpleNodeParser** to parse multiple articles.\n\n    \n    \n    from llama_index.text_splitter import TokenTextSplitter\n    from llama_index.node_parser import SimpleNodeParser\n    \n    text_splitter = TokenTextSplitter(separator=\" \", chunk_size=256, chunk_overlap=20)\n    #create node parser to parse nodes from document\n    node_parser = SimpleNodeParser(text_splitter=text_splitter)\n    nodes = node_parser.get_nodes_from_documents(documents)\n\nIn the third step, we create an index using **VectorStoreIndex** . To enable\nstreaming capability, ensure to set **streaming=True** while setting up the\n**query_engine** .\n\n    \n    \n    from llama_index import VectorStoreIndex\n    \n    index = VectorStoreIndex(\n            nodes=nodes,\n            service_context=st.session_state[\"service_context\"]\n        )\n    st.session_state[\"chat_engine\"] = index.as_query_engine(streaming=True)\n\nTo add streaming capability to Streamlit chat components, we took inspiration\nfrom this [ code ](https://arnaudmiribel.github.io/streamlit-\nextras/extras/streaming_write/) . Instead of using **st.write()** from the\nregular chat implementation, we replaced it with the **write** function from\n**streaming_write** .\n\n    \n    \n    response = st.session_state[\"chat_engine\"].query(prompt)\n    def stream_example():\n        for word in response.response_gen:\n            yield word\n    write(stream_example)\n\nStreaming Demo\n\n##  Predefined Prompts\n\nThree different prompts are predefined in the article chat page, allowing\nusers to select from a drop-down menu to ask questions without typing. The\nprompts are 5W1H, Similar Viewpoints, and Discrepancy Viewpoints.\n\n    \n    \n    prompt_content = {\n        \"5W1H\": 'Summarize the content details in the \"5W1H\" approach (Who, What, When, Where, Why, and How) in bullet points',\n        \"Similar Viewpoints\": \"Compare between the articles and provide the similar viewpoints in bullet points\",\n        \"Discrepency Viewpoints\": \"Compare between the articles and provide the discrepency viewpoints in bullet points\"\n    }\n\n#  What\u2019s Next\n\nHuge thanks to LlamaIndex and Streamlit for generously providing a massive\nplatform that allows more people to gain awareness of the organic news digest\nand save valuable time through [ **NewsGPT** ](https://newsgpt-clickbait-\nbuster.streamlit.app) . If you enjoyed reading the article and agree with our\nconcept, please do not hesitate to leave a clap for the article and join [\n**Neotice** ](https://www.neotice.app) , the production app of NewsGPT, to\nsupport us. ", "mimetype": "text/plain", "start_char_idx": 5092, "end_char_idx": 10812, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "369b8ffe-a535-4ab4-8ff2-1b9f7b88d747": {"__data__": {"id_": "369b8ffe-a535-4ab4-8ff2-1b9f7b88d747", "embedding": null, "metadata": {"filename": "newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.md", "extension": ".md", "title": "NewsGPT(Neotice): Summarize news articles with LlamaIndex \u2014 Hackathon winning app", "date": "Oct 27, 2023", "url": "https://www.llamaindex.ai/blog/newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a6424e4-e254-446e-ad5f-1516a078a21c", "node_type": "4", "metadata": {"filename": "newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.md", "extension": ".md", "title": "NewsGPT(Neotice): Summarize news articles with LlamaIndex \u2014 Hackathon winning app", "date": "Oct 27, 2023", "url": "https://www.llamaindex.ai/blog/newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11"}, "hash": "12f96da1279528149438dd096fb7a637affe637a82da475bf9eab818e6206765", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ed8b746-3bb7-4c03-87a4-9ac4af8fc31e", "node_type": "1", "metadata": {"filename": "newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.md", "extension": ".md", "title": "NewsGPT(Neotice): Summarize news articles with LlamaIndex \u2014 Hackathon winning app", "date": "Oct 27, 2023", "url": "https://www.llamaindex.ai/blog/newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11"}, "hash": "6660204d6aedb6fae87dc40b0fef702beb342dc3b36099e96db669bb47d8d8b0", "class_name": "RelatedNodeInfo"}}, "text": "We are confident in our mission and look forward to having you on\nboard with us. Thank you!\n\nYou can also connect us on LinkedIn: [ **Kang-Chi Ho**\n](http://www.linkedin.com/in/kangchi-ho) **,** [ **Jian-An Wang**\n](https://www.linkedin.com/in/chien-an-wang-6054b9110/)\n\nClick Here to Join Neotice [ **Neotice** ](https://www.neotice.app)\n\n", "mimetype": "text/plain", "start_char_idx": 10812, "end_char_idx": 11152, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "af751b91-a0f4-4d61-9ca1-99b542f60916": {"__data__": {"id_": "af751b91-a0f4-4d61-9ca1-99b542f60916", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-10-24-4a76204eeaa3.md", "extension": ".md", "title": "LlamaIndex newsletter 2023\u201310\u201324", "date": "Oct 24, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-24-4a76204eeaa3"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0241af91-0a6c-4c3a-8415-514c401f8eec", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-10-24-4a76204eeaa3.md", "extension": ".md", "title": "LlamaIndex newsletter 2023\u201310\u201324", "date": "Oct 24, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-24-4a76204eeaa3"}, "hash": "a4bc3dac4e667f557ffa28f85bd86efbca831de025c3c7f6452aaf558561a7b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a535a019-debf-468c-beaf-28f050bb29ca", "node_type": "1", "metadata": {}, "hash": "0d051703a4d3e409eb92c08d94f90bbf4ce1592d448345beb71880c3e4eb3185", "class_name": "RelatedNodeInfo"}}, "text": "Hello Llama Fans !\n\nWelcome back to our newsletter covering new features, guides, integrations,\nwebinars, tutorials, and more. Got a project, blog, or video you\u2019re proud of?\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 174, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a535a019-debf-468c-beaf-28f050bb29ca": {"__data__": {"id_": "a535a019-debf-468c-beaf-28f050bb29ca", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-10-24-4a76204eeaa3.md", "extension": ".md", "title": "LlamaIndex newsletter 2023\u201310\u201324", "date": "Oct 24, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-24-4a76204eeaa3"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0241af91-0a6c-4c3a-8415-514c401f8eec", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-10-24-4a76204eeaa3.md", "extension": ".md", "title": "LlamaIndex newsletter 2023\u201310\u201324", "date": "Oct 24, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-24-4a76204eeaa3"}, "hash": "a4bc3dac4e667f557ffa28f85bd86efbca831de025c3c7f6452aaf558561a7b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af751b91-a0f4-4d61-9ca1-99b542f60916", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-10-24-4a76204eeaa3.md", "extension": ".md", "title": "LlamaIndex newsletter 2023\u201310\u201324", "date": "Oct 24, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-24-4a76204eeaa3"}, "hash": "aafd9e9b314eb5beaaa3513eaf4e64f1da762952223c7ac09e6fd0b92de66a10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee02cec5-bf9f-46bc-97b1-beea8709e4a7", "node_type": "1", "metadata": {}, "hash": "8d6ea61bc6f8c76f9ea2a3bd8eec41675563441422b18535006bc0e8679b6130", "class_name": "RelatedNodeInfo"}}, "text": "Let\u2019s spotlight it! Contact us at [ news@llamaindex.ai\n](mailto:news@llamaindex.ai) .\n\nPlus, for direct updates in your email, just head to [ our homepage\n](https://www.llamaindex.ai/) and subscribe to our newsletter.\n\n**First, the highlights:**\n\n  1. ` **QueryFusionRetriever** ` **Launch:** Inspired by [ Adrian Raudaschl\u2019s ](https://twitter.com/Raudaschl) RAG-Fusion, enhancing multiple query generation with LLMs. [ Tweet ](https://twitter.com/jerryjliu0/status/1713573483228356733?s=20) , [ Docs ](https://docs.llamaindex.ai/en/latest/examples/retrievers/simple_fusion.html) . \n  2. **Router Fine-Tuning:** Our innovative router fine-tuning approach has achieved an outstanding 99% match rate, outpacing both the gpt-3.5\u2019s 65% and the base model\u2019s 12%. [ Tweet ](https://twitter.com/jerryjliu0/status/1714668623510618346?s=20) , [ Docs ](https://docs.llamaindex.ai/en/latest/examples/finetuning/router/router_finetune.html) . \n  ", "mimetype": "text/plain", "start_char_idx": 174, "end_char_idx": 1108, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee02cec5-bf9f-46bc-97b1-beea8709e4a7": {"__data__": {"id_": "ee02cec5-bf9f-46bc-97b1-beea8709e4a7", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-10-24-4a76204eeaa3.md", "extension": ".md", "title": "LlamaIndex newsletter 2023\u201310\u201324", "date": "Oct 24, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-24-4a76204eeaa3"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0241af91-0a6c-4c3a-8415-514c401f8eec", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-10-24-4a76204eeaa3.md", "extension": ".md", "title": "LlamaIndex newsletter 2023\u201310\u201324", "date": "Oct 24, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-24-4a76204eeaa3"}, "hash": "a4bc3dac4e667f557ffa28f85bd86efbca831de025c3c7f6452aaf558561a7b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a535a019-debf-468c-beaf-28f050bb29ca", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-10-24-4a76204eeaa3.md", "extension": ".md", "title": "LlamaIndex newsletter 2023\u201310\u201324", "date": "Oct 24, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-24-4a76204eeaa3"}, "hash": "31a701a8722b2ba1b5f2510c96a0a9f814bc71b2f2213ac406adb48fee7bbcd2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be92b2de-809b-4da8-824d-cb203657c46c", "node_type": "1", "metadata": {}, "hash": "2d892a29434a58a4365dd0bf4a9fc8adb62f6841c29450a60230caeb22226c3a", "class_name": "RelatedNodeInfo"}}, "text": "3. **Fusion Retriever Guide:** Guide on building an advanced Fusion Retriever from scratch. [ Docs ](https://docs.llamaindex.ai/en/latest/examples/low_level/fusion_retriever.html)\n  4. **Amazon Bedrock LLMs and AI21 Labs LLMs:** We have expanded our LLM compatibility, now seamlessly integrating with both Amazon Bedrock and AI21 Labs models. \n\n**Feature Releases and Enhancements:**\n\n  * ` **QueryFusionRetriever** ` : We introduced the ` QueryFusionRetriever ` , inspired by [ Adrian Raudaschl\u2019s ](https://twitter.com/Raudaschl) work on RAG-Fusion. This retriever allows users to generate multiple queries with LLMs, run various retrieval methods, and apply reciprocal rank fusion for improved results. [ Tweet ](https://twitter.com/jerryjliu0/status/1713573483228356733?s=20) , [ Docs ](https://docs.llamaindex.ai/en/latest/examples/retrievers/simple_fusion.html) . \n  * **Router Fine-Tuning:** We introduced router fine-tuning (V0) for improved LLM automated decision-making. Our approach achieved a 99% match rate, outperforming gpt-3.5\u2019s 65% and the base model\u2019s 12%. [ Tweet ](https://twitter.com/jerryjliu0/status/1714668623510618346?s=20) , [ Docs ](https://docs.llamaindex.ai/en/latest/examples/finetuning/router/router_finetune.html) . \n  * **SQLRetriever:** We introduce SQLRetriever, merging Text-to-SQL and RAG, enabling a RAG pipeline setup over SQL databases for structured table node retrieval and response synthesis. [ Tweet ](https://twitter.com/llama_index/status/1715518806012092497?s=20) , [ Docs ](https://docs.llamaindex.ai/en/latest/examples/index_structs/struct_indices/SQLIndexDemo.html) . \n\n**Guides:**\n\n  * [ Tutorial ](https://docs.llamaindex.ai/en/latest/examples/low_level/fusion_retriever.html) guide on **Building an Advanced Fusion Retriever from Scratch.**\n\n**Tutorials:**\n\n  * [ Saurav Joshi ](https://www.linkedin.com/in/sauravjoshi23/) \u2019s [ tutorial ](https://medium.com/@sauravjoshi23/complex-query-resolution-through-llamaindex-utilizing-recursive-retrieval-document-agents-and-sub-d4861ecd54e6) on Complex Query Resolution through LlamaIndex Utilizing Recursive Retrieval, Document Agents, and Sub Question Query Decomposition. \n  * [ Greg Loughnane ](https://twitter.com/GregOnLock) and [ Chris Alexiuk ](https://twitter.com/llm_wizard) [ tutorial ](https://www.youtube.com/watch?v=0QaUqoICNBo) on tackling domain-specific fine tuning using LlamaIndex. \n  * [ Vishwas Gowda ](https://twitter.com/VishwasAiTech) \u2019s [ blog post ](/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0) on Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex. \n  * [ Emanuel Ferreira ](https://twitter.com/manelferreira_) \u2019s blog post on the RA-DIT paper and its implementation in LlamaIndex. \n  * Yujian Tang\u2019s [ blog post ](https://zilliz.com/blog/chat-with-towards-data-science-using-llamaindex?utm_source=twitter&utm_medium=social&utm_term=zilliz) on Chat with Towards Data Science using LlamaIndex. \n  * [ Sudarshan Koirala ](https://twitter.com/mesudarshan) [ tutorial ](https://www.youtube.com/watch?v=4kwAhzzaW4A) on Chat with documents with Pinecone and LlamaIndex. \n  * [ Sudarshan Koirala ](https://twitter.com/mesudarshan) [ tutorial ](https://www.youtube.com/watch?v=BngaodT1q_4) on Combined Text-TO-SQL + Semantic Search with LlamaIndex. \n  * [ PromptEngineer ](https://twitter.com/engineerrprompt) [ tutorial ](https://www.youtube.com/watch?v=JeruKKuMxCg) on building LLM-powered financial analyst with LlamaIndex. \n\n**Integrations & Collaborations: **\n\n  * **Gradient AI:** We introduce a collaboration with Gradient AI to easily integrate fine-tuned LLMs into your LlamaIndex RAG pipeline. [ Tweet ](https://twitter.com/llama_index/status/1713970425422856477?s=20) , [ Blogpost ](https://gradient.ai/blog/introducing-the-llamindex-integration) . \n  * **PrivateGPT:** [ PrivateGPT ](https://twitter.com/PrivateGPT_AI) partners with LlamaIndex allowing private document interactions using default or custom integrations. [ Tweet ](https://twitter.com/PrivateGPT_AI/status/1715331924644815274) . \n  * **VectorFlow & LlamaHub Collaboration: ** VectorFlow\u2019s open-source vector-embedding pipeline now leverages LlamaHub for data connectors to streamline code and reduce maintenance. [ Tweet ](https://twitter.com/llama_index/status/1714446321078137015) . \n  * **Amazon Bedrock & AI21 Labs LLMs: ** We\u2019ve broadened our LLM compatibility range by integrating with Amazon Bedrock LLMs and AI21 Labs LLMs. \n  * **DashVector** : We have introduced an integration with DashVector, a robust, fully-managed vectorDB service. \n  * **Tencent Cloud:** We\u2019ve integrated with Tencent Cloud VectorDB. \n  * PGVectorStore within LlamaIndex has been enhanced to support custom Postgres schemas. This facilitates better index management and promotes easy schema-based versioning. \n  ", "mimetype": "text/plain", "start_char_idx": 1108, "end_char_idx": 5956, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be92b2de-809b-4da8-824d-cb203657c46c": {"__data__": {"id_": "be92b2de-809b-4da8-824d-cb203657c46c", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-10-24-4a76204eeaa3.md", "extension": ".md", "title": "LlamaIndex newsletter 2023\u201310\u201324", "date": "Oct 24, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-24-4a76204eeaa3"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0241af91-0a6c-4c3a-8415-514c401f8eec", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-10-24-4a76204eeaa3.md", "extension": ".md", "title": "LlamaIndex newsletter 2023\u201310\u201324", "date": "Oct 24, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-24-4a76204eeaa3"}, "hash": "a4bc3dac4e667f557ffa28f85bd86efbca831de025c3c7f6452aaf558561a7b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee02cec5-bf9f-46bc-97b1-beea8709e4a7", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-10-24-4a76204eeaa3.md", "extension": ".md", "title": "LlamaIndex newsletter 2023\u201310\u201324", "date": "Oct 24, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-24-4a76204eeaa3"}, "hash": "ac67e1bd16cbaa3e14dc85f95f39480a2c726df89a624fb741f50271f7d29277", "class_name": "RelatedNodeInfo"}}, "text": "* We now accommodate custom models that align with the OpenAI-compatible API. \n\n**Webinars:**\n\n  * [ Wenqi Glantz ](https://www.linkedin.com/in/wenqi-glantz-b5448a5a/) workshop webinar on Evaluation-Driven Development (EDD). \n  * [ Webinar ](https://www.youtube.com/watch?v=mzb6WNSaLXQ) showcasing the winning projects from the recent AGI House hackathon: \u201cBuild, Test, and Launch LLM Apps\u201d. This event was co-sponsored by LlamaIndex, TruEra, and Pinecone. \n\n", "mimetype": "text/plain", "start_char_idx": 5956, "end_char_idx": 6415, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "df20729e-935e-42c6-a041-62105847891e": {"__data__": {"id_": "df20729e-935e-42c6-a041-62105847891e", "embedding": null, "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3f0176f8-5129-4bbb-8c61-038dc7513060", "node_type": "4", "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "e2993ef729906ce47697cc74b4c536035ef113ec10650503e6789e1ae38e795e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61a59674-815e-41d6-a897-95f2b48ff29b", "node_type": "1", "metadata": {}, "hash": "7e7690fc6d4e132b8e4c67a3255712ff143e6c13dd378b9027afbfd2ca97c244", "class_name": "RelatedNodeInfo"}}, "text": "#  Introduction\n\n##  Why Long Context Matters and How Retrieval Augmentation Steps In:\n\nIn the dynamic landscape of LLMs, two methods have gained traction and seem to\nbe taking center stage: expanding the context window of Large Language Models\n(LLMs) and enhancing these models with retrieval capabilities. The continued\nevolution of GPU technology, coupled with breakthroughs in attention\nmechanisms, has given rise to long-context LLMs. Simultaneously, the concept\nof retrieval \u2014 where LLMs pick up only the most relevant context from a\nstandalone retriever \u2014 promises a revolution in efficiency and speed.\n\nIn the midst of these evolving narratives, some interesting questions emerge:\n\n  1. Retrieval-augmentation versus long context window, which one is better for downstream tasks? \n  2. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 794, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "61a59674-815e-41d6-a897-95f2b48ff29b": {"__data__": {"id_": "61a59674-815e-41d6-a897-95f2b48ff29b", "embedding": null, "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3f0176f8-5129-4bbb-8c61-038dc7513060", "node_type": "4", "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "e2993ef729906ce47697cc74b4c536035ef113ec10650503e6789e1ae38e795e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df20729e-935e-42c6-a041-62105847891e", "node_type": "1", "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "c35ecc4acb8fe4ab76b245825af9f32fd171755b8dc456a848d603d273271bc8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7f44c74-38db-4fec-91ca-4aa9de208d0d", "node_type": "1", "metadata": {}, "hash": "52a3f29c9d8e0aa5dfdcbbacfb965822ecbfba203a7456991333e18bbbe25af8", "class_name": "RelatedNodeInfo"}}, "text": "Can both methods be combined to get the best of both worlds? \n\nTo dissect these questions, in this blog post we turn to [ NVIDIA\u2019s recent\nstudy ](https://arxiv.org/pdf/2310.03025v1.pdf) , which harnesses the power of\ntwo powerful LLMs: the proprietary GPT \u2014 43B and LLaMA2\u201370B, the research\nstrives to provide actionable insights for AI practitioners.\n\n##  Prior Research and the NVIDIA Divergence:\n\nInterestingly, while NVIDIA\u2019s findings are interesting in many respects,\nAnother recent work by [ Bai et al. (2023) ](https://arxiv.org/abs/2308.14508)\nalso ventured into similar territory, although with differing outcomes.\n\nTheir work explored the impact of retrieval on long context LLMs, evaluating\nmodels like GPT-3.5-Turbo-16k and Llama2\u20137B-chat-4k. However, their findings\ndiverge from NVIDIA\u2019s in crucial ways. [ Bai et al.\n](https://arxiv.org/abs/2308.14508) discerned that retrieval was beneficial\nonly for the Llama2\u20137B-chat-4k with a 4K context window, but not for extended\ncontext models like GPT-3.5-Turbo-16k. One hypothesis for this difference\ncenters on the challenges tied to experiments using black-box APIs and the\nsmaller white-box LLMs they employed, which potentially had limited capability\nto integrate context through retrieval.\n\nNVIDIA\u2019s work distinguishes itself by tapping into much larger LLMs, yielding\nresults that not only match top-tier models like ChatGPT-3.5 but even indicate\nfurther enhancements when incorporating retrieval methods.\n\n#  Models, Datasets, and Evaluation Metrics\n\n##  Large Language Models (LLMs) Explored:\n\nThe researchers delved deep into the potential of large language models for\ntasks like generative QA and summarization. Specifically, two models were the\nprimary focus:\n\n  * **Nemo GPT-43B:** A proprietary 43 billion parameter model trained on 1.1T tokens, 70% of which were in English. This model was fed a rich diet of web archives, Wikipedia, Reddit, books, and more. It contains 48 layers and is trained using RoPE embeddings. \n  * **LLaMA2\u201370B:** A publicly available 70B parameter model trained on 2T tokens, primarily in English. It\u2019s structured with 80 layers and also utilizes RoPE embeddings. \n\n##  Context Window Extension:\n\nTo enhance the models\u2019 capability to process longer contexts, their initial 4K\ncontext window length was augmented. The GPT-43B was modified to handle 16K,\nwhile the LLaMA2\u201370B was expanded to both 16K and 32K, employing the position\ninterpolation method.\n\n##  Instruction Tuning:\n\nTo optimize the LLMs for the tasks at hand, instruction tuning was\nimplemented. A diverse dataset blend, comprising sources like Soda, ELI5,\nFLAN, and others, was created. ", "mimetype": "text/plain", "start_char_idx": 794, "end_char_idx": 3444, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a7f44c74-38db-4fec-91ca-4aa9de208d0d": {"__data__": {"id_": "a7f44c74-38db-4fec-91ca-4aa9de208d0d", "embedding": null, "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3f0176f8-5129-4bbb-8c61-038dc7513060", "node_type": "4", "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "e2993ef729906ce47697cc74b4c536035ef113ec10650503e6789e1ae38e795e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61a59674-815e-41d6-a897-95f2b48ff29b", "node_type": "1", "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "b530dcbdc9d6d2306ec25fd5b9fc3aef1c5ae7a30016a935d83e2c64811f426a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24ebc2b3-20dc-4ec6-ab48-7704567ecf90", "node_type": "1", "metadata": {}, "hash": "b1dfeff9ee1767b4d7c059e81d0a36277c8d85500815e7e424c210e53ab771e2", "class_name": "RelatedNodeInfo"}}, "text": "A consistent format template was adopted for\nmulti-turn dialogue training, and the models were meticulously fine-tuned to\naccentuate the answer segment.\n\n##  Retrieval Models Tested:\n\nThree retrieval systems were put to the test:\n\n  * **Dragon:** A state-of-the-art dual encoder model for both supervised and zero-shot information retrieval. \n  * **Contriever:** Utilizes a basic contrastive learning framework and operates unsupervised. \n  * **OpenAI embedding:** The latest version was used, accepting a maximum input of 8,191 tokens. \n\nThe retrieval approach entailed segmenting each document into 300-word\nsections, encoding both questions and these chunks, and then merging the most\npertinent chunks for response generation.\n\n##  Datasets Used for Evaluation:\n\nThe study employed seven diverse datasets, sourced from the Scroll benchmark\nand LongBench.\n\nA snapshot of these datasets includes:\n\n  * **QMSum:** A query-based summarization dataset, QMSum consists of transcripts from diverse meetings and their corresponding summaries, built upon contextual queries. \n  * **Qasper:** A question-answering dataset centered on NLP papers, Qasper offers a mix of abstractive, extractive, yes/no, and unanswerable questions from the Semantic Scholar Open Research Corpus. \n  * **NarrativeQA:** Aimed at question-answering over entire books and movie scripts, NarrativeQA provides question-answer pairs created from summaries of these extensive sources. \n  * **QuALITY:** A multiple-choice question answering set based on stories and articles, QuALITY emphasizes thorough reading, with half the questions designed to be challenging and require careful consideration. \n  * **MuSiQue:** Designed for multi-hop reasoning in question answering, MuSiQue creates multi-hop questions from single-hop ones, emphasizing connected reasoning and minimizing shortcuts. \n  * **HotpotQA:** Based on Wikipedia, HotpotQA requires reading multiple supporting documents for reasoning. It features diverse questions and provides sentence-level support for answers. \n  * **MultiFieldQA-en:** Curated to test long-context understanding across fields, MFQA uses sources like legal documents and academic papers, with annotations done by Ph.D. students. \n\n", "mimetype": "text/plain", "start_char_idx": 3444, "end_char_idx": 5674, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24ebc2b3-20dc-4ec6-ab48-7704567ecf90": {"__data__": {"id_": "24ebc2b3-20dc-4ec6-ab48-7704567ecf90", "embedding": null, "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3f0176f8-5129-4bbb-8c61-038dc7513060", "node_type": "4", "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "e2993ef729906ce47697cc74b4c536035ef113ec10650503e6789e1ae38e795e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a7f44c74-38db-4fec-91ca-4aa9de208d0d", "node_type": "1", "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "996d9e7e0a0747ff0249983869c921083f67b508bb8de415352f01a726062701", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad77265a-ed10-4356-bd52-4265ff1070e3", "node_type": "1", "metadata": {}, "hash": "0f68928231069ae32cfcfcfd5f77b0ecef4175c1f2fe47168a26cff820c89c04", "class_name": "RelatedNodeInfo"}}, "text": "##  Evaluation Metrics:\n\nThe research team used a wide range of metrics suited to each dataset. The\ngeometric mean of ROUGE scores for QM, the exact matching (EM) score for QLTY,\nand F1 scores for others were the primary metrics.\n\n#  Results\n\n  * Baseline models without retrieval, having a 4K sequence length, performed poorly since valuable texts get truncated. \n  * With retrieval, performance for 4K models like LLaMA2\u201370B-4K and GPT-43B-4K significantly improved. \n  * HotpotQA, a multi-hop dataset, particularly benefits from longer sequence models. \n  * Models with longer contexts (16K, 32K) outperform their 4K counterparts even when fed the same evidence chunks. \n  * There exists a unique \u201cU-shaped\u201d performance curve for LLMs due to the ` lost in the middle ` phenomenon, making them better at utilizing information at the beginning or end of the input. \n  * The study presents a contrasting perspective to LongBench\u2019s findings, emphasizing that retrieval is beneficial for models regardless of their context window size. \n\n##  Comparing to OpenAI Models:\n\n  * The LLaMA2\u201370B-32k model with retrieval surpasses the performance of GPT-3.5-turbo variants and is competitive with Davinci-003, underscoring its robustness in handling long context tasks. \n\n##  Comparison of Different Retrievers:\n\n  * Retrieval consistently enhances the performance across different retrievers. \n  * Public retrievers outperformed proprietary ones like OpenAI embeddings. \n\n##  Comparing with the number of retrieved chunks:\n\n  * The best performance is achieved by retrieving the top 5 or 10 chunks. Retrieving more, up to 20 chunks, doesn\u2019t offer additional benefits and can even degrade performance. \n  * The deterioration in performance when adding more chunks could be due to the ` lost-in-the-middle ` phenomenon or the model being sidetracked by non-relevant information. \n\n#  Conclusion\n\nAs we delved deep into understanding how retrieval augmentation and long-\ncontext extension interact when applied to leading language models fine-tuned\nfor long-context question-answering and summarization tasks. Here are some\nthings to be noted:\n\n  1. **Boost in Performance with Retrieval** : Implementing retrieval techniques significantly enhances the performance of both shorter 4K context language models and their longer 16K/32K context counterparts. \n  2. **Efficiency of 4K Models with Retrieval** : 4K context language models, when combined with retrieval augmentation, can achieve performance levels similar to 16K long context models. Plus, they have the added advantage of being faster during the inference process. \n  3. **Best Model Performance** : After enhancing with both context window extension and retrieval augmentation, the standout model, LLaMA2\u201370B-32k-ret (LLaMA2\u201370B-32k with retrieval), surpasses well-known models like GPT-3.5-turbo-16k and davinci-003. \n\n#  References:\n\n  1. ", "mimetype": "text/plain", "start_char_idx": 5674, "end_char_idx": 8567, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad77265a-ed10-4356-bd52-4265ff1070e3": {"__data__": {"id_": "ad77265a-ed10-4356-bd52-4265ff1070e3", "embedding": null, "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3f0176f8-5129-4bbb-8c61-038dc7513060", "node_type": "4", "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "e2993ef729906ce47697cc74b4c536035ef113ec10650503e6789e1ae38e795e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24ebc2b3-20dc-4ec6-ab48-7704567ecf90", "node_type": "1", "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}, "hash": "5cb3820bacd28123afda4987bba3b15fc94c2a00da5e19c21104fa2ffb977b22", "class_name": "RelatedNodeInfo"}}, "text": "[ Retrieval meets long context, large language models. ](https://arxiv.org/pdf/2310.03025v1.pdf)\n  2. [ Longbench: A bilingual, multitask benchmark for long context understanding. ](https://arxiv.org/abs/2308.14508)\n\nWe trust that this blog post on the review of the paper on retrieval\naugmentation with long-context LLMs has furnished you with meaningful\ninsights. We\u2019re keen to hear if your experiments align with our findings or\npresent new perspectives \u2014 divergent results always make for interesting\ndiscussions and further exploration.\n\n", "mimetype": "text/plain", "start_char_idx": 8567, "end_char_idx": 9110, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84a8c831-c6dc-4ea5-a62b-69cd31631f93": {"__data__": {"id_": "84a8c831-c6dc-4ea5-a62b-69cd31631f93", "embedding": null, "metadata": {"filename": "mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.md", "extension": ".md", "title": "Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1788f98a-120e-4cf7-ab78-b49e3b4383fe", "node_type": "4", "metadata": {"filename": "mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.md", "extension": ".md", "title": "Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125"}, "hash": "ab0941a38c19d9218d1144547e08ea9e71d5d6dede820dcf68dd0f15381951ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41b82351-bf95-40c6-84ab-dd6d1d6bd769", "node_type": "1", "metadata": {}, "hash": "eb904c6ed871265f22a9f9faab21126d8ae13352b68058382fecdad8b147218a", "class_name": "RelatedNodeInfo"}}, "text": "Despite recent motivation to utilize NLP for wider range of real world\napplications, most NLP papers, tasks and pipelines assume raw, clean texts.\nHowever, many texts we encounter in the wild, including a vast majority of\nlegal documents (e.g., contracts and legal codes), are not so clean, with many\nof them being visually structured documents (VSDs) such as PDFs. PDFs are\nversatile, preserving the visual integrity of documents, but they often pose a\nsignificant challenge when it comes to extracting and manipulating their\ncontents.\n\nIn this discussion, our focus will primarily be on text-only layered PDFs, a\ncategory often regarded by many as a resolved issue.\n\n#  Complexity of Parsing PDFs\n\n  1. **Layout Complexity** : PDFs can contain complex layouts, such as multi-column text, tables, images, and intricate formatting. This layout diversity complicates the extraction of structured data. \n  2. **Font encoding issue** s: PDFs use a variety of font encoding systems, and some of these systems do not map directly to Unicode. This can make it difficult to extract the text accurately. \n  3. **Non-linear text storage:** PDFs do not store text in the order it appears on the page. Instead, they store text in objects that can be placed anywhere on the page. This means that the order of the text in the underlying code may not match the order of the text as it appears visually. \n  4. **Inconsistent use of spaces** : In some PDFs, spaces are not used consistently or are not used at all between words. This can make it difficult to even identify word boundaries. \n\n#  Do we need an efficient parser?\n\n> In the Age of LLMs, is an Efficient Parser Still Essential When LLMs Can\n> Process Entire PDFs?\n\nThis question gains relevance if the answer to this next question is \u201cYes\u201d.\n\n> Do we need Retrieval-Augmented Generation (RAG)?\n\nWhile LLMs are powerful, they have certain limitations in terms of the amount\nof text they can process at once and the scope of information they can\nreference. Further recent research have suggested LLM performance is often\nhighest when relevant information occurs at the beginning or end of the input\ncontext, and significantly degrades when models must access relevant\ninformation in the middle of long contexts. Techniques like RAG help overcome\nthese limitations, enabling more effective and efficient processing of large\ndocuments and broader information retrieval.\n\n> Still Skeptical? Let\u2019s ask an LLM for confirmation.\n\nNow that we\u2019ve established the importance of an efficient parser, it becomes\ninstrumental in constructing an effective Retrieval-Augmented Generation (RAG)\npipeline to address the limitations of an LLM. Let\u2019s explore how we are\nachieving this today. It\u2019s crucial to remember that the quality of the context\nfed to an LLM is the cornerstone of an effective RAG, as the saying goes, \u2018\n**_Garbage In \u2014 Garbage Out_ ** .\u2019\n\nIn the context of building LLM-related applications, **chunking** is the\nprocess of breaking down large pieces of text into smaller segments. It\u2019s an\nessential technique that helps optimize the relevance of the content we get\nback from a database once we use the LLM to embed content. Some of the\nstrategies involved are\n\n  1. **Fixed-size chunking** . This is the most common and straightforward approach to chunking: we simply decide the number of tokens in our chunk and, optionally, whether there should be any overlap between them. Easy to implement & most commonly used, but never makes it to a production setting because the output is satisfactory in a Proof of Concept (POC) setup, but its accuracy degrades as we conduct further testing. \n  2. **\u201cContent-aware\u201d chunking** . Set of methods for taking advantage of the nature of the content we\u2019re chunking and applying more sophisticated chunking to it. Challenging to implement due to the reasons mentioned above, but if tackled correctly, it could be the most ideal building block for a production-grade Information Retrieval (IR) engine. \n\n#  Where\u2019s This Article Headed, Anyway?\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4028, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "41b82351-bf95-40c6-84ab-dd6d1d6bd769": {"__data__": {"id_": "41b82351-bf95-40c6-84ab-dd6d1d6bd769", "embedding": null, "metadata": {"filename": "mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.md", "extension": ".md", "title": "Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1788f98a-120e-4cf7-ab78-b49e3b4383fe", "node_type": "4", "metadata": {"filename": "mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.md", "extension": ".md", "title": "Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125"}, "hash": "ab0941a38c19d9218d1144547e08ea9e71d5d6dede820dcf68dd0f15381951ec", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84a8c831-c6dc-4ea5-a62b-69cd31631f93", "node_type": "1", "metadata": {"filename": "mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.md", "extension": ".md", "title": "Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125"}, "hash": "e7b0b57bf67b1c3744d8bf7a48b8b9b469aa8d570d8a9f7521db790d1e3c6738", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "775ec11e-d34f-494d-9736-a124b5b79b9c", "node_type": "1", "metadata": {}, "hash": "7918573688b0237aadd878075997fc54312d4aab222e9e798109b324d7d645da", "class_name": "RelatedNodeInfo"}}, "text": "Certainly, let\u2019s put an end to the historical and background details, shall\nwe?\n\nIntroducing [ **LayoutPDFReader**\n](https://github.com/nlmatics/llmsherpa#layoutpdfreader) for \u201c _Context-aware_\n\u201d chunking. [ LayoutPDFReader ](https://github.com/nlmatics/llmsherpa) can act\nas the most important tool in your RAG arsenal by parsing PDFs along with\nhierarchical layout information such as:\n\n  1. Identifying sections and subsections, along with their respective hierarchy levels. \n  ", "mimetype": "text/plain", "start_char_idx": 4028, "end_char_idx": 4509, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "775ec11e-d34f-494d-9736-a124b5b79b9c": {"__data__": {"id_": "775ec11e-d34f-494d-9736-a124b5b79b9c", "embedding": null, "metadata": {"filename": "mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.md", "extension": ".md", "title": "Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1788f98a-120e-4cf7-ab78-b49e3b4383fe", "node_type": "4", "metadata": {"filename": "mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.md", "extension": ".md", "title": "Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125"}, "hash": "ab0941a38c19d9218d1144547e08ea9e71d5d6dede820dcf68dd0f15381951ec", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41b82351-bf95-40c6-84ab-dd6d1d6bd769", "node_type": "1", "metadata": {"filename": "mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.md", "extension": ".md", "title": "Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125"}, "hash": "ce7b279e8b088c7518e74fca708e263993a13ac6a547c665811c047e5177dbdd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e026835-2536-4eba-8a03-e49b770da1b8", "node_type": "1", "metadata": {}, "hash": "19bd20840796c838acd2b9657d7a6769f68e27284bdefcb32be85d4a495d1129", "class_name": "RelatedNodeInfo"}}, "text": "2. Merging lines into coherent paragraphs. \n  3. Establishing connections between sections and paragraphs. \n  4. Recognizing tables and associating them with their corresponding sections. \n  5. Handling lists and nested list structures with precision. \n\nThe first step in using the [ LayoutPDFReader\n](https://github.com/nlmatics/llmsherpa) is to provide a URL or file path to\nit (assuming it\u2019s already been installed) and get back a document object.\n\n    \n    \n    from llmsherpa.readers import LayoutPDFReaderllmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"pdf_url = \"https://arxiv.org/pdf/1910.13461.pdf\" # also allowed is a file path e.g. /home/downloads/xyz.pdfpdf_reader = LayoutPDFReader(llmsherpa_api_url)doc = pdf_reader.read_pdf(pdf_url)\n\n##  Vector search and RAG with Smart Chunking\n\nLayoutPDFReader employs intelligent chunking to maintain the cohesion of\nrelated text:\n\n  * It groups all list items together, along with the preceding paragraph. \n  * Items within a table are chunked together. \n  * It incorporates contextual information from section headers and nested section headers. \n\nAs a quick example, the following code snippet generates a [ LlamaIndex\n](https://github.com/run-llama/llama_index) query engine from the document\nchunks produced by LayoutPDFReader.\n\n    \n    \n    from llama_index.readers.schema.base import Documentfrom llama_index import VectorStoreIndexindex = VectorStoreIndex([])for chunk in doc.chunks():    index.insert(Document(text=chunk.to_context_text(), extra_info={}))query_engine = index.as_query_engine()# Let's run one queryresponse = query_engine.query(\"list all the tasks that work with bart\")print(response)\n\nWe get the following response:\n\n    \n    \n    BART works well for text generation, comprehension tasks, abstractive dialogue, question answering, and summarization tasks.\n\nKey Considerations:\n\n  1. ", "mimetype": "text/plain", "start_char_idx": 4509, "end_char_idx": 6428, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8e026835-2536-4eba-8a03-e49b770da1b8": {"__data__": {"id_": "8e026835-2536-4eba-8a03-e49b770da1b8", "embedding": null, "metadata": {"filename": "mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.md", "extension": ".md", "title": "Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1788f98a-120e-4cf7-ab78-b49e3b4383fe", "node_type": "4", "metadata": {"filename": "mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.md", "extension": ".md", "title": "Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125"}, "hash": "ab0941a38c19d9218d1144547e08ea9e71d5d6dede820dcf68dd0f15381951ec", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "775ec11e-d34f-494d-9736-a124b5b79b9c", "node_type": "1", "metadata": {"filename": "mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.md", "extension": ".md", "title": "Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125"}, "hash": "7538a9fd513321ff9c585f04ac90a48233dc3b3f8b47ef84d13718b7a74c5b16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6468bcb2-4947-46fa-bd76-75e7c41defd0", "node_type": "1", "metadata": {}, "hash": "211d2268664696d28f6265c2d5b71d56926c6a06b32027aed34d5f731d4d5b89", "class_name": "RelatedNodeInfo"}}, "text": "LLMSherpa leverages a cost-free and open API server. Your PDFs are not retained beyond temporary storage during the parsing process. \n  ", "mimetype": "text/plain", "start_char_idx": 6428, "end_char_idx": 6564, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6468bcb2-4947-46fa-bd76-75e7c41defd0": {"__data__": {"id_": "6468bcb2-4947-46fa-bd76-75e7c41defd0", "embedding": null, "metadata": {"filename": "mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.md", "extension": ".md", "title": "Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1788f98a-120e-4cf7-ab78-b49e3b4383fe", "node_type": "4", "metadata": {"filename": "mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.md", "extension": ".md", "title": "Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125"}, "hash": "ab0941a38c19d9218d1144547e08ea9e71d5d6dede820dcf68dd0f15381951ec", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e026835-2536-4eba-8a03-e49b770da1b8", "node_type": "1", "metadata": {"filename": "mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.md", "extension": ".md", "title": "Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125"}, "hash": "d7a7ad746c6bb44045d85b8858d447633b816a9409e7b1a6f060e6d97658f8a7", "class_name": "RelatedNodeInfo"}}, "text": "2. LayoutPDFReader has undergone extensive testing with a diverse range of PDFs. However, achieving flawless parsing for every PDF remains a challenging task. \n  3. Please note that OCR (Optical Character Recognition) functionality is presently unavailable. The tool exclusively supports PDFs equipped with a text layer. \n  4. For inquiries regarding private hosting options, OCR support, or tailored assistance with particular PDF-related concerns, feel free to reach out to [ contact@nlmatics.com ](mailto:contact@nlmatics.com) or to [ me ](mailto: kiran@nlmatics.com) directly. \n\nIf you have any questions, please leave them in the comments section, and I\nwill try to respond ASAP.\n\n**_Connect?_ **\n\nIf you want to get in touch, feel free to shoot me a message on [ LinkedIn\n](https://www.linkedin.com/in/kirannpanicker/) or via [ email ](mailto:\nkirankurup@gmail.com) .\n\n#  References\n\n[ https://github.com/nlmatics/llmsherpa\n](https://github.com/nlmatics/llmsherpa#layoutpdfreader)\n\n[ Capturing Logical Structure of Visually Structured Documents with Multimodal\nTransition Parser ](https://arxiv.org/abs/2105.00150)\n\n[ Lost in the Middle: How Language Models Use Long Contexts\n](https://arxiv.org/abs/2307.03172)\n\n", "mimetype": "text/plain", "start_char_idx": 6564, "end_char_idx": 7783, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fcd8e912-0195-4e24-9c21-8509b225fbbf": {"__data__": {"id_": "fcd8e912-0195-4e24-9c21-8509b225fbbf", "embedding": null, "metadata": {"filename": "improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.md", "extension": ".md", "title": "Improving RAG effectiveness with Retrieval-Augmented Dual Instruction Tuning (RA-DIT)", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb3697bd-299a-4dfa-90d5-53dd8c4e0aa4", "node_type": "4", "metadata": {"filename": "improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.md", "extension": ".md", "title": "Improving RAG effectiveness with Retrieval-Augmented Dual Instruction Tuning (RA-DIT)", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d"}, "hash": "6a1e1e964e41610eb27eff5e2cfd4311aa3c3e947aa49646936b0020d0b5f53f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d363544b-f39e-4422-9265-f88b47679c6d", "node_type": "1", "metadata": {}, "hash": "63ce1335a8097f7618a6ae43c19914b365d52ab016ce5aab8f3e30bbc2488409", "class_name": "RelatedNodeInfo"}}, "text": "##  Introduction\n\nLarge Language Models (LLMs) improve performance by accessing external data\nfor background knowledge tasks related. However, existing approaches require\ncostly modifications during LM\u2019s pre-training or integrating the data store\nafter the model has been trained. On the downside, both strategies lead to\nsuboptimal performance.\n\nTo address this problem an **AI Research team at Meta** has proposed a method\ncalled [ **RA-DIT: RETRIEVAL-AUGMENTED DUAL INSTRUCTION TUNING**\n](https://arxiv.org/pdf/2310.01352.pdf) that allows any LLM to be upgraded to\ninclude retrieval features.\n\nIn this blog post, we will explore RA-DIT capabilities to have better\nperformance on Retrieval Augmentation Generation (RAG) through building the\ndataset and fine-tuning the models.\n\nThe RA-DIT approach involves two distinct fine-tuning steps:\n\n  1. Update a pre-trained LM to better use retrieved information. \n  2. Update the retriever to return more relevant results \n\n##  How it works\n\nThe RA-DIT approach separately fine-tunes the LLM and the retriever. The LLM\nis updated to maximize the probability of the correct answer given the\nretrieval-augmented instructions, while the retriever is updated to minimize\nhow much the document is semantically similar (relevant) to the query.\n\nBelow we are going through each step from generating the fine-tuning dataset,\nfine-tuning the language model for better predictions, and refining the\nretrieval search process.\n\n##  Fine-tuning Dataset\n\nThe fine-tuning dataset is tailored to enhance the language model\u2019s ability to\nleverage knowledge and boost its contextual awareness during prediction\ngeneration. Generating Q/A pairs, summarizing data, and incorporating chain-\nof-thought reasoning can lead to improved results when integrated with the\nmodels.\n\nFollowing our [ LamaIndex implementation\n](https://docs.llamaindex.ai/en/stable/examples/finetuning/knowledge/finetune_retrieval_aug.html#fine-\ntuning-with-retrieval-augmentation) , we retrieve the top_k nodes, generate\nQ/A pairs from the documents, and then augment the data. We use the Q/A pairs\nthrough the QueryResponseDataset module, which returns a (query, response)\npair for the fine-tuning dataset. While the retrieval fine-tuning data set is\ncreated on Q/A pairs data.\n\n##  Language Model Fine-tuning\n\nWith our fine-tuning dataset in hand, we can refine our LLM to achieve two\nmain benefits: Adapt the LLM to better utilization of relevant background\nknowledge and train the LLM to produce accurate predictions even with\nincorrectly retrieved chunks, empowering the model to rely on its own\nknowledge.\n\n##  Retriever Fine-tuning\n\nThe retriever is fine-tuned using the LM-Supervised Retrieval (LSR) method. In\nthis approach, the LLM assesses the information fetched by the retriever. If\nthe LLM finds the information misaligned with the given query, it sends\nfeedback to the retriever. Using this feedback, the retriever refines its\nsearch process, ensuring it fetches data that the LLM can effectively use.\nThis collaboration enhances the overall quality of the answers provided.\n\n##  Evaluation\n\nTo assess the suggested method, the authors employed specific datasets and\nmetrics. Let\u2019s delve into each of these to grasp the experimental results\nbetter.\n\n##  Metrics\n\nAn \u201cexact match\u201d (EM) metric was used to measure how closely the model\u2019s\nprediction matches the ground truth answer.\n\n##  Dataset\n\nThe methodology was tested on two distinct tasks:\n\n  1. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3462, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d363544b-f39e-4422-9265-f88b47679c6d": {"__data__": {"id_": "d363544b-f39e-4422-9265-f88b47679c6d", "embedding": null, "metadata": {"filename": "improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.md", "extension": ".md", "title": "Improving RAG effectiveness with Retrieval-Augmented Dual Instruction Tuning (RA-DIT)", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb3697bd-299a-4dfa-90d5-53dd8c4e0aa4", "node_type": "4", "metadata": {"filename": "improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.md", "extension": ".md", "title": "Improving RAG effectiveness with Retrieval-Augmented Dual Instruction Tuning (RA-DIT)", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d"}, "hash": "6a1e1e964e41610eb27eff5e2cfd4311aa3c3e947aa49646936b0020d0b5f53f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fcd8e912-0195-4e24-9c21-8509b225fbbf", "node_type": "1", "metadata": {"filename": "improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.md", "extension": ".md", "title": "Improving RAG effectiveness with Retrieval-Augmented Dual Instruction Tuning (RA-DIT)", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d"}, "hash": "a509c4dc2b4f2358486de369786b45a3eb4a2a91d4b77812248fc042812ccd8e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60102647-dcc7-41b6-bbf6-fa829bfc87ec", "node_type": "1", "metadata": {}, "hash": "ea7f636a3cd76d05f5c32b42d9e4842941e094030f22754d0a8ded2432286d3b", "class_name": "RelatedNodeInfo"}}, "text": "Knowledge-intensive tasks. \n  ", "mimetype": "text/plain", "start_char_idx": 3462, "end_char_idx": 3492, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60102647-dcc7-41b6-bbf6-fa829bfc87ec": {"__data__": {"id_": "60102647-dcc7-41b6-bbf6-fa829bfc87ec", "embedding": null, "metadata": {"filename": "improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.md", "extension": ".md", "title": "Improving RAG effectiveness with Retrieval-Augmented Dual Instruction Tuning (RA-DIT)", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb3697bd-299a-4dfa-90d5-53dd8c4e0aa4", "node_type": "4", "metadata": {"filename": "improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.md", "extension": ".md", "title": "Improving RAG effectiveness with Retrieval-Augmented Dual Instruction Tuning (RA-DIT)", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d"}, "hash": "6a1e1e964e41610eb27eff5e2cfd4311aa3c3e947aa49646936b0020d0b5f53f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d363544b-f39e-4422-9265-f88b47679c6d", "node_type": "1", "metadata": {"filename": "improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.md", "extension": ".md", "title": "Improving RAG effectiveness with Retrieval-Augmented Dual Instruction Tuning (RA-DIT)", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d"}, "hash": "0d511909af07b842ecf6827ea681a77149ebf74811bec75fd381e9af21eee526", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28c624cc-f041-4233-bb90-d46080f91ebb", "node_type": "1", "metadata": {}, "hash": "facaf24d3d5118e16f417db30d977ff835c98c25b3daa9356935b503a702668a", "class_name": "RelatedNodeInfo"}}, "text": "2. Commonsense reasoning. \n\nLet\u2019s explore the datasets utilized for both of these tasks.\n\n##  Knowledge-intensive dataset\n\nFor knowledge-intensive tasks the selected datasets predominantly focus on the\nmodel\u2019s capacity to access, understand, and relay deep and specific knowledge.\nThey encompass questions rooted in facts, general trivia, and complex domain-\nspecific queries;\n\nThe datasets used are MMLU, Natural Questions (NQ), TriviaQA, and a subset of\ntasks from the KILT benchmark.\n\n##  **Commonsense reasoning dataset**\n\nCommonsense reasoning datasets challenge the model\u2019s ability to reason and\ninfer based on general knowledge and everyday scenarios. They contain\nquestions and scenarios that typically don\u2019t rely on deep domain knowledge but\nrather on intuitive and general world understanding.\n\nThe datasets used are BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC-E, ARC-C,\nOBQA.\n\nFor a better understanding of how these datasets were utilized you can check\nthe [ paper ](https://arxiv.org/pdf/2310.01352.pdf) for better understanding.\n\n##  Results\n\nIn a comparative analysis of model performance on knowledge-intensive (Table 2\nbelow) and commonsense reasoning tasks (Table 3 below), three models were\nconsidered:\n\n  * LLAMA 65B \n  * LLAMA 65B REPLUG (only retrieval augmentation) \n  * RA-DIT 65B \n\n##  Knowledge Intensive Tasks\n\nEvaluations are conducted in 0-shot, 5-shot, and 64-shot fine-tuning settings.\n\n**0-shot Analysis:**\n\n  * RA-DIT 65B demonstrated superior performance with an average EM score of 50.5 across all tasks. \n  ", "mimetype": "text/plain", "start_char_idx": 3492, "end_char_idx": 5037, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28c624cc-f041-4233-bb90-d46080f91ebb": {"__data__": {"id_": "28c624cc-f041-4233-bb90-d46080f91ebb", "embedding": null, "metadata": {"filename": "improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.md", "extension": ".md", "title": "Improving RAG effectiveness with Retrieval-Augmented Dual Instruction Tuning (RA-DIT)", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb3697bd-299a-4dfa-90d5-53dd8c4e0aa4", "node_type": "4", "metadata": {"filename": "improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.md", "extension": ".md", "title": "Improving RAG effectiveness with Retrieval-Augmented Dual Instruction Tuning (RA-DIT)", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d"}, "hash": "6a1e1e964e41610eb27eff5e2cfd4311aa3c3e947aa49646936b0020d0b5f53f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60102647-dcc7-41b6-bbf6-fa829bfc87ec", "node_type": "1", "metadata": {"filename": "improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.md", "extension": ".md", "title": "Improving RAG effectiveness with Retrieval-Augmented Dual Instruction Tuning (RA-DIT)", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d"}, "hash": "9c6cbad3493d48f434ca7053157e76bbbc685abd4ede101d374667b222e87582", "class_name": "RelatedNodeInfo"}}, "text": "* It outperformed LLAMA 65B REPlug (43.1 average) and significantly surpassed LLAMA 65B (32.9 average). \n\n**5-shot Analysis:**\n\n  * RA-DIT 65B maintained its lead with an average EM score of 55.2. \n  * LLAMA 65B REPlug followed closely with 52.7, while LLAMA 65B achieved an average of 45.0. \n\nIn a separate evaluation for 64-shot fine-tuning, two models were analyzed:\nATLAS and RA-DIT 65B.\n\n**64-shot Fine-tuning:**\n\n  * RA-DIT 65B achieved an average performance of 60.9 across all tasks, slightly surpassing ATLAS, which obtained an average score of 56.8 \n\n##  Commonsense reasoning\n\n_RA-DIT 65B_ was benchmarked in order to evaluate the impact of retrieval-\naugmented instruction tuning on the LLMs parametric knowledge and reasoning\ncapabilities.\n\nIn this experiment without retrieval augmentation, _RA-DIT_ showed\nimprovements over base _LLAMA_ 65B models on 7 of 8 evaluation datasets,\nindicating that the parametric knowledge and reasoning capabilities of the LLM\ncomponent are in general preserved.\n\nIn summary, RA-DIT 65B consistently delivered great results, surpassing its\ncompetitors in multiple scenarios, underscoring its proficiency and aptitude\nin knowledge-intensive tasks while showing that the parametric knowledge and\nreasoning capabilities of the LLM are still preserved.\n\n##  Conclusion\n\nThe RA-DIT approach provides a structured method to enhance how Large Language\nModels utilize external data. Through the dual fine-tuning of both the model\nand the retriever, we target better accuracy and context-awareness in\nresponses.\n\nThe incorporation of the LSR technique fosters a more efficient data retrieval\nprocess, ensuring that the generated answers are both relevant and informed,\nthe final results show that RA-DIT surpasses un-tuned RALM approaches like\nREPLUG showing competitive results.\n\nYou can explore more about the LLamaIndex implementation at: [\nhttps://docs.llamaindex.ai/en/stable/examples/finetuning/knowledge/finetune_retrieval_aug.html#fine-\ntuning-with-retrieval-augmentation\n](https://docs.llamaindex.ai/en/stable/examples/finetuning/knowledge/finetune_retrieval_aug.html#fine-\ntuning-with-retrieval-augmentation)\n\n##  References\n\nRA-DIT paper: [ https://arxiv.org/abs/2310.01352\n](https://arxiv.org/abs/2310.01352)\n\nConnect with me on [ Twitter ](https://twitter.com/manelferreira_) and [\nLinkedin ](https://www.linkedin.com/in/emanuelcferreira/)\n\n", "mimetype": "text/plain", "start_char_idx": 5037, "end_char_idx": 7428, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "511c6606-3983-4f53-aeb2-382f61a2184b": {"__data__": {"id_": "511c6606-3983-4f53-aeb2-382f61a2184b", "embedding": null, "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "364aebea-6a85-4c68-bd51-9a0c1fe3ff8d", "node_type": "4", "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "d6379eb20ccdbee08bc4bc9291fd9103b7bce96980f9770d4c8746ebcbf3816f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4adfc69-a069-48a4-b7b0-08dd3a729406", "node_type": "1", "metadata": {}, "hash": "55e620853d9ebddd114fbdccf3c9c91e4ca68837bd6feafc8f5067c9613a9e52", "class_name": "RelatedNodeInfo"}}, "text": "In this article, we\u2019ll dive deep into the world of LLM app development and\ntake a closer look at my journey of building the Streamlit LLM hackathon-\nwinning app [ FinSight \u2014 Financial Insights At Your Fingertips\n](https://finsight-report.streamlit.app/) . This article covers the entire\nprocess from ideation to execution, along with code snippets and snapshots.\n\n#  Introduction\n\n##  A use case for LLMs in finance\n\nOne fascinating use case for LLMs in finance is to use them on company annual\nreports (10-K form). These reports are publicly available information that\npretty much every portfolio manager, financial analyst, and shareholder uses\nregularly to make informed decisions.\n\nHowever reading, understanding, and assessing these reports, especially for\nmultiple companies can be tedious and time-consuming. Hence, using LLMs on\nannual reports to extract insights and summarize would solve a lot of problems\nand save valuable time.\n\nWhen the [ Streamlit LLM Hackathon\n](https://www.linkedin.com/posts/vishwasgowda217_llm-hackathon-streamlit-\nactivity-7115398433573666816-1y72?utm_source=share&utm_medium=member_desktop)\nwas, announced I thought this was the best time to explore this idea. And\nthat\u2019s how [ FinSight ](https://finsight-report.streamlit.app/) came into\nexistence.\n\n##  How does FinSight work?\n\nA small Demonstration\n\nFinSight has two main features called Annual Report Analyzer and Finance\nMetric Review, but for this blog post, we will be concentrating on the former.\n\nAnnual Report Analyzer is a RAG(Retrieval Augmented Generation) based feature,\nwhich means that the LLM will be generating insights based on the information\nin a knowledge base (which in this case is a company\u2019s annual report). Here\u2019s\nhow it works behind the scenes:\n\nRAG pipeline for Annual Report Analyzer\n\nWhile this is a basic representation of the architecture, we will be doing a\ndeep dive into the importance of each of these components and how they work.\n\n#  Setup\n\nIn case you want to refer the code to the app: [ Repo\n](https://github.com/vishwasg217/finsight)\n\nWe will use [ LlamaIndex ](https://www.llamaindex.ai/) to build the knowledge\nbase and to query it using an LLM (gpt-4 is the best suited). LlamaIndex is a\nsimple, flexible data framework for connecting custom data sources to large\nlanguage models.\n\nFor the front end, [ Streamlit ](https://streamlit.io/) is the most convenient\ntool to build and share web apps.\n\n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2431, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e4adfc69-a069-48a4-b7b0-08dd3a729406": {"__data__": {"id_": "e4adfc69-a069-48a4-b7b0-08dd3a729406", "embedding": null, "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "364aebea-6a85-4c68-bd51-9a0c1fe3ff8d", "node_type": "4", "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "d6379eb20ccdbee08bc4bc9291fd9103b7bce96980f9770d4c8746ebcbf3816f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "511c6606-3983-4f53-aeb2-382f61a2184b", "node_type": "1", "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "a57715edfe2718df03d30416eb3639a45633490f410fe42cff7735ac6242b328", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea37959c-36ec-4424-b249-d509ab852e41", "node_type": "1", "metadata": {}, "hash": "6ab3353dff00966bb292b680498098aa22a3a767bb7e5ffd791f976138900374", "class_name": "RelatedNodeInfo"}}, "text": "1. ", "mimetype": "text/plain", "start_char_idx": 2431, "end_char_idx": 2434, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea37959c-36ec-4424-b249-d509ab852e41": {"__data__": {"id_": "ea37959c-36ec-4424-b249-d509ab852e41", "embedding": null, "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "364aebea-6a85-4c68-bd51-9a0c1fe3ff8d", "node_type": "4", "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "d6379eb20ccdbee08bc4bc9291fd9103b7bce96980f9770d4c8746ebcbf3816f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4adfc69-a069-48a4-b7b0-08dd3a729406", "node_type": "1", "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "dba189fb7d13f2d5a873630c60c9c16c58e2e15ce6a589d83e5d4237fa22258a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b83b813-8a9f-4100-a4dd-355d4d3acf8d", "node_type": "1", "metadata": {}, "hash": "d82e96e4071c7a8494a33c848213135bd3d71dc7d9d3aafce1233abb51e67bb6", "class_name": "RelatedNodeInfo"}}, "text": "Clone Repository \n\n    \n    \n    git clone https://github.com/vishwasg217/finsight.gitcd finsight\n\n2\\. Setup Virtual Environment\n\n    \n    \n    # For macOS and Linux:python3 -m venv venv# For Windows:python -m venv venv\n\n3\\. Activate Virtual Environment\n\n    \n    \n    # For macOS and Linux:source venv/bin/activate# For Windows:.\\venv\\Scripts\\activate\n\n4\\. Install Required Dependencies:\n\n    \n    \n    pip install -r requirements.txt\n\n5\\. Set up the Environment Variables:\n\n    \n    \n    # create directorymkdir .streamlit# create toml filetouch .streamlit/secrets.toml\n\nYou can get your API keys here: [ AlphaVantage\n](https://www.alphavantage.co/support/#api-key) , [ OpenAI\n](https://openai.com/blog/openai-api) ,\n\n    \n    \n    # Add the following API keysav_api_key = \"ALPHA_VANTAGE API KEY\"openai_api_key = \"OPEN AI API KEY\"\n\n#  Document Loading, Indexing, and Storage\n\nAlthough LlamaIndex has its own set of data connectors to read PDFs, we still\nneed to write a small function ` process_pdf() ` to load the PDFs since we are\ndoing it through Streamlit.\n\n    \n    \n    from pypdf import PdfReaderfrom llama_index.schema import Documentdef process_pdf(pdf):    file = PdfReader(pdf)    text = \"\"    for page in file.pages:        text += str(page.extract_text())            doc = Document(text=text)    return [doc]\n\nThe next step is to ingest, index, and store this document in a vector\ndatabase. In this case, we will use FAISS DB, as we require in an in-memory\nvector database. FAISS is also very convenient to use. Hence, we write a\nfunction called ` get_vector_index() ` to do exactly that.\n\nIn case you\u2019re interested in checking out other vector DB options, you read\ncan [ this ](https://gpt-\nindex.readthedocs.io/en/stable/core_modules/data_modules/storage/vector_stores.html)\n.\n\n    \n    \n    from llama_index.llms import OpenAIfrom llama_index import VectorStoreIndex, ServiceContext, StorageContextfrom llama_index.vector_stores import FaissVectorStoredef get_vector_index(documents):    llm = OpenAI(OPENAI_API_KEY)    faiss_index = faiss.IndexFlatL2(d)    vector_store = FaissVectorStore(faiss_index=faiss_index)    storage_context = StorageContext.from_defaults(vector_store=vector_store)    service_context = ServiceContext.from_defaults(llm=llm)     index = VectorStoreIndex.from_documents(documents,         service_context=service_context,        storage_context=storage_context    )       return index\n\n` ServiceContext() ` and ` StorageContext() ` are used to set the\nconfigurations for the vector store. Using ` VectorStoreIndex.from_documents()\n` we ingest, index, and store the document as vector embeddings in the FAISS\nDB.\n\n    \n    \n    # Calling the functions through streamlit frontendimport streamlit as stif \"index\" not in st.session_state:  st.session_state.index = Noneif \"process_doc\" not in st.session_state:        st.session_state.process_doc = Falseif st.sidebar.button(\"Process Document\"):        with st.spinner(\"Processing Document...\"):            documents = process_pdf(pdfs)            st.session_state.index = get_vector_index(documents)            st.session_state.process_doc = True  st.toast(\"Document Processsed!\")\n\n#  Query Tools and Engines\n\nNow that we have our knowledge base ready, it\u2019s time to build a mechanism to\nquery it.\n\n    \n    \n    index = get_vector_index(documents)engine = index.as_query_engine()query = \"How has Microsoft performed in this fiscal year?\"response = engine(query)\n\nIdeally, the above code should have been enough to query and synthesize a\nresponse from the information in the vector DB. However, the response wouldn't\nbe comprehensive and detailed enough, especially for such open-ended\nquestions. We need to develop a better mechanism that allows us to break down\na query into more detailed questions and retrieve context from multiple parts\nof the vector DB.\n\n    \n    \n    def get_query_engine(engine):    query_engine_tools = [        QueryEngineTool(            query_engine=engine,            metadata=ToolMetadata(                name=\"Annual Report\",                description=f\"Provides information about the company from its annual report.\",            ),        ),    ]    s_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools)    return s_engineindex = get_vector_index(documents)engine = index.as_query_engine()s_engine = get_query_engine(engine)\n\nLet\u2019s break the above function down. The ` QueryEngineTool ` module wraps\naround the ` engine ` and helps provide context and metadata to the engine.\nThis is especially useful when you have more than one engine and you want to\nprovide context to the LLM as to which one to use for a given query.\n\nHere\u2019s what that would look like:\n\n    \n    \n    # example for multiple query engine toolsquery_engine_tools = [    QueryEngineTool(        query_engine=sept_engine,        metadata=ToolMetadata(            name=\"sept_22\",            description=\"Provides information about Uber quarterly financials ending September 2022\",        ),    ),    QueryEngineTool(        query_engine=june_engine,        metadata=ToolMetadata(            name=\"june_22\",            description=\"Provides information about Uber quarterly financials ending June 2022\",        ),    )]\n\nYou can read more about the tools available in LlamaIndex [ here\n](https://docs.llamaindex.ai/en/stable/core_modules/agent_modules/tools/root.html)\n.\n\nHowever, we\u2019re currently sticking to just one QueryEnginerTool for now.\n\nThe ` SubQuestionQueryEngine ` module breaks down a complex query into many\nsub-questions and their target query engine for execution. After executing all\nsub-questions, all responses are gathered and sent to a response synthesizer\nto produce the final response. Using this module is essential because\ngenerating insights from annual reports requires complex queries that need to\nretrieve information from multiple nodes within the vector DB.\n\nSubQuestionQueryEngine at work\n\n#  Prompt Engineering\n\nPrompt engineering is essential to the entire process mainly for two reasons:\n\n  1. To provide clarity to the agent as to what it needs to retrieve from the vector DB by writing precise and relevant queries \n  2. And then control the quality of the output generated from the retrieved context by providing a structure and description for the output to be generated. \n\nBoth these points are handled by using ` PromptTemplate ` and `\nPydanticOutputParser ` module in ` langchain ` .\n\nUsing the ` PydanticOutputParser ` we write the description for the different\nsections of the insights to be generated. After having a few conversations\nwith finance experts, I concluded generating insights for these 4 sections:\ndifferent sections: Fiscal Year Highlights, Strategic Outlook and Future\nDirection, Risk Management, Innovation and R&D. Now let\u2019s write the ` pydantic\n` class for these sections:\n\n    \n    \n    from pydantic import BaseModel, Fieldclass FiscalYearHighlights(BaseModel):    performance_highlights: str = Field(..., description=\"Key performance metrics and financial stats over the fiscal year.\")    major_events: str = Field(..., description=\"Highlight of significant events, acquisitions, or strategic shifts that occurred during the year.\")    challenges_encountered: str = Field(..., description=\"Challenges the company faced during the year and, if and how they managed or overcame them.\")class StrategyOutlookFutureDirection(BaseModel):    strategic_initiatives: str = Field(..., description=\"The company's primary objectives and growth strategies for the upcoming years.\")    market_outlook: str = Field(..., description=\"Insights into the broader market, competitive landscape, and industry trends the company anticipates.\")class RiskManagement(BaseModel):    risk_factors: str = Field(..., description=\"Primary risks the company acknowledges.\")    risk_mitigation: str = Field(..., description=\"Strategies for managing these risks.\")class InnovationRnD(BaseModel):    r_and_d_activities: str = Field(..., description=\"Overview of the company's focus on research and development, major achievements, or breakthroughs.\")    innovation_focus: str = Field(..., description=\"Mention of new technologies, patents, or areas of research the company is diving into.\")\n\n**Note: These sections and their description are for generic use cases. ", "mimetype": "text/plain", "start_char_idx": 2434, "end_char_idx": 10750, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b83b813-8a9f-4100-a4dd-355d4d3acf8d": {"__data__": {"id_": "5b83b813-8a9f-4100-a4dd-355d4d3acf8d", "embedding": null, "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "364aebea-6a85-4c68-bd51-9a0c1fe3ff8d", "node_type": "4", "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "d6379eb20ccdbee08bc4bc9291fd9103b7bce96980f9770d4c8746ebcbf3816f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea37959c-36ec-4424-b249-d509ab852e41", "node_type": "1", "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "1880c7d64d675d71479513b7d9718d2720e75c4a8b1ecd7892e5f7da721d5239", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb1ceaf2-d210-45ec-8739-d19d19229471", "node_type": "1", "metadata": {}, "hash": "e9f2186b31941a68698d1ad426f66b25cf8e1415e5565c9b98b857f4049737b7", "class_name": "RelatedNodeInfo"}}, "text": "They\ncan be changed to suit your particular needs.**\n\nThese pydantic classes will provide the format and description for each\nsection to the prompt. So let\u2019s write a function that allows us to plug in any\npydantic class to a prompt:\n\n    \n    \n    from langchain.prompts import PromptTemplatefrom langchain.output_parsers import PydanticOutputParserprompt_template = \"\"\"You are given the task of generating insights for {section} from the annual report of the company. Given below is the output format, which has the subsections.Must use bullet points.Always use $ symbol for money values, and round it off to millions or billions accordinglyIncase you don't have enough info you can just write: No information available---{output_format}---\"\"\"def report_insights(engine, section, pydantic_model):    parser = PydanticOutputParser(pydantic_object=pydantic_model)    prompt_template = PromptTemplate(        template=prompt_template,        input_variables=[\"section\"],        partial_variables={\"output_format\": parser.get_format_instructions()}    )    formatted_input = prompt_template.format(section=section)    response = engine.query(formatted_input)    parsed_response = parser.parse(response.response)    return parsed_response\n\n` PromptTemplate ` plugs in all the values such as ` section ` and `\noutput_format ` into the prompt template. ` PydanticOutputParser ` converts\nthe pydantic class into a format that is readable to the LLM. The response\ngenerated will be in string format, hence we use the ` parser.parse() `\nfunction to parse the response and get a structured output.\n\n    \n    \n    # calling the function in streamlit frontendif st.session_state.process_doc:    if st.button(\"Analyze Report\"):        engine = get_query_engine(st.session_state.index.as_query_engine(similarity_top_k=3))        with st.status(\"**Analyzing Report...**\"):            st.write(\"Fiscal Year Highlights...\")            st.session_state.fiscal_year_highlights = report_insights(engine, \"Fiscal Year Highlights\", FiscalYearHighlights)            st.write(\"Strategy Outlook and Future Direction...\")            st.session_state.strategy_outlook_future_direction = report_insights(engine, \"Strategy Outlook and Future Direction\", StrategyOutlookFutureDirection)            st.write(\"Risk Management...\")            st.session_state.risk_management = report_insights(engine, \"Risk Management\", RiskManagement)                        st.write(\"Innovation and R&D...\")            st.session_state.innovation_and_rd = report_insights(engine, \"Innovation and R&D\", InnovationRnD)# displaying the generated insights  if st.session_state.fiscal_year_highlights:                with tab1:            st.write(\"## Fiscal Year Highlights\")            st.write(\"### Performance Highlights\")            st.write(st.session_state.fiscal_year_highlights.performance_highlights)            st.write(\"### Major Events\")            st.write(st.session_state.fiscal_year_highlights.major_events)            st.write(\"### Challenges Encountered\")            st.write(st.session_state.fiscal_year_highlights.challenges_encountered)            st.write(\"### Milestone Achievements\")            st.write(str(st.session_state.fiscal_year_highlights.milestone_achievements))    if st.session_state.strategy_outlook_future_direction:        with tab2:            st.write(\"## Strategy Outlook and Future Direction\")            st.write(\"### Strategic Initiatives\")            st.write(st.session_state.strategy_outlook_future_direction.strategic_initiatives)            st.write(\"### Market Outlook\")            st.write(st.session_state.strategy_outlook_future_direction.market_outlook)            st.write(\"### Product Roadmap\")            st.write(st.session_state.strategy_outlook_future_direction.product_roadmap)    if st.session_state.risk_management:        with tab3:            st.write(\"## Risk Management\")            st.write(\"### Risk Factors\")            st.write(st.session_state.risk_management.risk_factors)            st.write(\"### Risk Mitigation\")            st.write(st.session_state.risk_management.risk_mitigation)    if st.session_state.innovation_and_rd:        with tab4:            st.write(\"## Innovation and R&D\")            st.write(\"### R&D Activities\")            st.write(st.session_state.innovation_and_rd.r_and_d_activities)            st.write(\"### Innovation Focus\")            st.write(st.session_state.innovation_and_rd.innovation_focus)\n\nYou can find the complete code Annual Report Analyzer [ here\n](https://github.com/vishwasg217/finsight/blob/main/src/pages/2_%F0%9F%97%82%EF%B8%8F_Annual_Report_Analyzer.py)\n\n#  Upcoming Features\n\n  1. Select and Store Insights: I\u2019ve been working on a feature that allows the user to select any insight needed and also save it into the user\u2019s account \n  2. ", "mimetype": "text/plain", "start_char_idx": 10750, "end_char_idx": 15556, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb1ceaf2-d210-45ec-8739-d19d19229471": {"__data__": {"id_": "eb1ceaf2-d210-45ec-8739-d19d19229471", "embedding": null, "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "364aebea-6a85-4c68-bd51-9a0c1fe3ff8d", "node_type": "4", "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "d6379eb20ccdbee08bc4bc9291fd9103b7bce96980f9770d4c8746ebcbf3816f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b83b813-8a9f-4100-a4dd-355d4d3acf8d", "node_type": "1", "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}, "hash": "00fcb96c5ed61dd1ee09dd8db5b7ef628605c159a7f7bd59de2c04da38904964", "class_name": "RelatedNodeInfo"}}, "text": "Adding more profession-specific insights: Currently, the insight works well for generic purposes. However, different professions use annual reports differently, so naturally I need to create a different set of insights based on the user\u2019s use case. \n  3. ` PandasQueryEngine ` Module for querying financial statements: Using this module, the LLM will be able to extract better insights from financial statements which are typically in a structured format. \n\n#  Conclusion\n\nIn summary, FinSight\u2019s Annual Report Analyzer makes financial analysis easier\nand more insightful by harnessing the power of LLMs. It\u2019s a valuable tool for\nportfolio managers, financial analysts, and shareholders, saving time and\nimproving decision-making. While the core pipeline remains consistent, note\nthat our deployed app code might evolve to incorporate upgrades and enhanced\nfeatures, ensuring ongoing improvements.\n\nBig thanks to [ LlamaIndex ](https://www.llamaindex.ai/) for helping me make\nFinSight a reality. No other framework is as advanced in making RAG-based\ntools.\n\nIf you like what you\u2019ve read, please do leave a clap for me, and also show\nsome love to [ FinSight ](https://finsight-report.streamlit.app/) . You can\ncheck out the GitHub repo [ here ](https://github.com/vishwasg217/finsight) .\n\nYou connect with me on [ LinkedIn ](https://www.linkedin.com/feed/) and [\nTwitter ](https://twitter.com/VishwasAiTech)\n\n", "mimetype": "text/plain", "start_char_idx": 15556, "end_char_idx": 16963, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a373ce28-04de-4522-9d1c-143baaf09faa": {"__data__": {"id_": "a373ce28-04de-4522-9d1c-143baaf09faa", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-10-17-33514cbc04a2.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201317", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-17-33514cbc04a2"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "198fece2-5bff-4144-8779-547f01b4661c", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-10-17-33514cbc04a2.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201317", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-17-33514cbc04a2"}, "hash": "7f850106e083f4c3193ec5d2d4c0ce857c9eb578900ed77bc93990d4fd0af09e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "945c0b0d-da50-4ead-b037-de677f0acd9b", "node_type": "1", "metadata": {}, "hash": "21cc3850ca14907d17702238bb192490c9efa26c4245a9bad0a91a5c5efb658a", "class_name": "RelatedNodeInfo"}}, "text": "Hello Llama Enthusiasts !\n\nAnother week has flown by, and we\u2019re back with a jam-packed newsletter filled\nwith updates on hackathons, guides, integrations, features, webinars,\ntutorials, blogs, and demos. If you have a project, blog post, or video that\ndeserves a spotlight, we\u2019d love to feature it! Just reach out to us at [\nnews@llamaindex.ai ](mailto:news@llamaindex.ai) .\n\nBonus: You can now get all these updates straight to your inbox! Simply visit\nour [ homepage ](https://www.llamaindex.ai/) and sign up for our email\nupdates.\n\n**First, the highlights:**\n\n  1. **AI.Engineer Summit** : At the AI.Engineer Summit, Jerry Liu discussed RAG applications, while Simon led a workshop on RAG app optimization (Jerry\u2019s [ slides ](https://docs.google.com/presentation/d/1v7T6ejrSo87ndGeGC7tt6zeq-cftu03WWw7WL8Jskug/edit#slide=id.p) , Simon\u2019s [ slides ](https://github.com/run-llama/ai-engineer-workshop/blob/main/presentation.pdf) ) \n  2. **Text to pgVector** : we launched PGVectorSQLQueryEngine for combined SQL and vector queries on PostgreSQL. ( [ Docs ](https://t.co/4h3nTzzJ5E) , [ Tweet ](https://x.com/jerryjliu0/status/1712496323742851188?s=20) ) \n  3. **Hugging Face Integration** : Integrated with HuggingFace\u2019s text-embeddings-inference server for high-speed, large-scale BERT model serving. ( [ Docs ](https://docs.llamaindex.ai/en/latest/examples/embeddings/text_embedding_inference.html) , [ Tweet ](https://x.com/jerryjliu0/status/1712943016590381554?s=20) ) \n  4. **Multi-Document Agents** : New V1 agents support advanced multi-document retrieval and async query planning. ( [ Docs ](https://t.co/bWYv0R7J2B) , [ Tweet ](https://x.com/llama_index/status/1712129914386993295?s=20) ) \n  5. **Unstructured Parsing** : Unveiled UnstructuredElementNodeParser, a hierarchical parser for embedded tables/text using UnstructuredIO. ( [ Docs ](https://docs.llamaindex.ai/en/stable/examples/query_engine/sec_tables/tesla_10q_table.html) , [ Tweet ](https://x.com/llama_index/status/1711768906866864403?s=20) ) \n  6. **LLM Compatibility** : We have charted LLM performances on various tasks and found that the Zephyr-7b-alpha model stands out as the top-performing 7B model in advanced RAG tasks. ( [ Docs ](https://docs.llamaindex.ai/en/latest/core_modules/model_modules/llms/root.html#llm-compatibility-tracking) ) \n\n#  Congratulations to our AGI House Hackathon Winners!\n\nWe love seeing people build amazing things with LlamaIndex!\n\n**Build:**\n\n  1. [ Demostify ](https://drive.google.com/file/d/18Ru1FCchVpMi8jzjr2dHdDuZtCG83zOJ/view)\n  2. [ Stick with Fit ](https://docs.google.com/presentation/d/1pOa8AppiKpuF-aQvsD5vKebeL6Gf9lmP505FavrFOm4/edit#slide=id.g2899cea0752_0_15) , [ SafeQuery ](https://github.com/chisler/safequery) , Cherry \n\n**Break:**\n\n  * [ Fuzzy Access ](https://github.com/jeremy-brouillet/agi-hackathon)\n\n**Test:**\n\n  * X-Ray Insight \n\n**Honorable Mentions:**\n\n  * [ KindleGPT ](https://glasp.co/know-thyself/)\n  * PenTest \n\n#  LlamaIndex at [ AI.Engineer Summit ](https://twitter.com/aiDotEngineer) :\n\n  1. [ Jerry Liu ](https://twitter.com/jerryjliu0) gave a talk on Building production-ready RAG applications. [ Slides ](https://docs.google.com/presentation/d/1v7T6ejrSo87ndGeGC7tt6zeq-cftu03WWw7WL8Jskug/edit#slide=id.p) . \n  2. [ Simon ](https://twitter.com/disiok) conducted a workshop on Building, Evaluating, and Optimizing your RAG App for Production with LlamaIndex. [ Slides ](https://github.com/run-llama/ai-engineer-workshop/blob/main/presentation.pdf) , [ Code ](https://github.com/run-llama/ai-engineer-workshop/tree/main) . \n\n#  Guides:\n\n  1. **LLM Compatibility Tracking:** We\u2019ve charted LLM performances on various tasks, revealing zephyr-7b-alpha as the only current 7B model excelling in advanced RAG/ Agentic tasks. [ Docs ](https://docs.llamaindex.ai/en/latest/core_modules/model_modules/llms/root.html#llm-compatibility-tracking) . \n  2. **Evaluations:** Adjusting chunk size is essential for RAG apps. Having more chunks isn\u2019t necessarily better, and re-ranking might be counterproductive. To fine-tune, experiment with different chunk sizes and top-k values. The Arize AI team has provided a guide to help you evaluate using Arize AI Phoenix and Llama Index. [ Slides ](https://docs.google.com/presentation/d/18Z7H3WSncPzLOTHKZAj36w0E7HSGY78VkDooSzvvySE/edit) , [ Notebook ](https://colab.research.google.com/drive/1Siufl13rLI-kII1liaNfvf-NniBdwUpS?usp=sharing) . \n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4421, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "945c0b0d-da50-4ead-b037-de677f0acd9b": {"__data__": {"id_": "945c0b0d-da50-4ead-b037-de677f0acd9b", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-10-17-33514cbc04a2.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201317", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-17-33514cbc04a2"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "198fece2-5bff-4144-8779-547f01b4661c", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-10-17-33514cbc04a2.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201317", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-17-33514cbc04a2"}, "hash": "7f850106e083f4c3193ec5d2d4c0ce857c9eb578900ed77bc93990d4fd0af09e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a373ce28-04de-4522-9d1c-143baaf09faa", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-10-17-33514cbc04a2.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201317", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-17-33514cbc04a2"}, "hash": "5ef89d9df9621309ace5eabf3829d40e2f786fc7368df5488b9b04391dd7a8d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43845168-c598-4d15-aaea-3442fb59738c", "node_type": "1", "metadata": {}, "hash": "63b02dc2243860e34740108ed5544094ffd88018fba22227df13d47e820bfecf", "class_name": "RelatedNodeInfo"}}, "text": "#  Tutorials:\n\n  1. [ Shahul\u2019s ](https://twitter.com/Shahules786) [ tutorial ](https://t.co/oTA2O8sE21) demonstrates how to choose the best embeddings for your data, emphasizing that retriever performance and embedding quality are crucial for a RAG system\u2019s efficacy using the LlamaIndex and RAGAS libraries. \n  2. [ Wenqi Glantz ](https://twitter.com/wenqi_glantz) \u2019s [ tutorial ](https://levelup.gitconnected.com/evaluation-driven-development-the-swiss-army-knife-for-rag-pipelines-dba24218d47e) on Evaluation Driven Development for RAG Pipelines. \n  3. [ Wenqi Glantz ](https://twitter.com/wenqi_glantz) \u2019s [ tutorial ](https://betterprogramming.pub/masking-pii-data-in-rag-pipeline-326d2d330336) on Masking PII Data in the RAG Pipeline. \n  4. Ofer Mendelevitch\u2019s from [ Vectara ](https://twitter.com/vectara) has a [ tutorial ](https://vectara.com/retrieval-augmented-generation-rag-done-right-retrieval/) on Retrieval Augmented Generation with LlamaIndex on comparing Vectara\u2019s new Boomerang model to OpenAI and Cohere. \n  5. [ Patrick Loeber ](https://twitter.com/patloeber) from AssemblyAI has a [ tutorial ](https://www.youtube.com/watch?v=alT-0mNRF-c) on Build LlamaIndex Audio Apps. \n  6. [ Pradip Nichite ](https://www.linkedin.com/in/pradipnichite/) made a [ tutorial ](https://www.youtube.com/watch?v=ZRSI8LHpqBA) on NL2SQL with LlamaIndex: Querying Databases Using Natural Language. \n  7. [ Mayo Oshin ](https://twitter.com/mayowaoshin) has a [ tutorial ](https://www.youtube.com/watch?v=UmvqMscxwoc) on How to Compare Multiple Large PDF Files. \n  8. [ Sudarshan Koirala ](https://twitter.com/mesudarshan) made a [ tutorial ](https://www.youtube.com/watch?v=4kwAhzzaW4A) on Chat With Documents with LlamaIndex and Pinecone. \n\n#  Demos:\n\n  * [ Siva Surendira ](https://twitter.com/siva_1gc) built [ YC Bot ](https://www.theycbot.com/) to get instant startup advice from your favorite YC mentors. \n\n#  Feature Releases and Enhancements:\n\n  1. **Text to pgVector:** We introduced the PGVectorSQLQueryEngine, which allows you to query a PostgreSQL database using both full SQL and vector search simultaneously. [ Docs ](https://t.co/4h3nTzzJ5E) , [ Tweet ](https://x.com/jerryjliu0/status/1712496323742851188?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 4421, "end_char_idx": 6651, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43845168-c598-4d15-aaea-3442fb59738c": {"__data__": {"id_": "43845168-c598-4d15-aaea-3442fb59738c", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-10-17-33514cbc04a2.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201317", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-17-33514cbc04a2"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "198fece2-5bff-4144-8779-547f01b4661c", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-10-17-33514cbc04a2.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201317", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-17-33514cbc04a2"}, "hash": "7f850106e083f4c3193ec5d2d4c0ce857c9eb578900ed77bc93990d4fd0af09e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "945c0b0d-da50-4ead-b037-de677f0acd9b", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-10-17-33514cbc04a2.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201317", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-17-33514cbc04a2"}, "hash": "60c3d3f462df43b38640eb6f1e99067273693e82695f2a0d796248e66e0d5f39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "964b65b3-b31b-4859-9c5c-e03364bddfec", "node_type": "1", "metadata": {}, "hash": "5f9fd849d7e59d5820b9f804a58ed1eb6189b34658ae6735448360abb1ea268b", "class_name": "RelatedNodeInfo"}}, "text": "2. **Multi-Document Agents:** We introduce Multi-Document Agents (V1) that can now retrieve across multiple docs and plan queries asynchronously, offering a superior analysis compared to standard RAG. [ Docs ](https://t.co/bWYv0R7J2B) , [ Tweet ](https://x.com/llama_index/status/1712129914386993295?s=20) . \n  3. **UnstructuredIO:** We\u2019ve partnered with UnstructuredIO to enhance LLM/RAG applications. By extracting tables from PDFs, we\u2019ve improved query methods beyond basic vector indexing, enabling hybrid queries and cross-document comparisons, especially for tabular questions. [ Docs ](https://t.co/Ezts2C9Rpw) , [ Tweet ](https://x.com/jerryjliu0/status/1710685292913668595?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 6651, "end_char_idx": 7344, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "964b65b3-b31b-4859-9c5c-e03364bddfec": {"__data__": {"id_": "964b65b3-b31b-4859-9c5c-e03364bddfec", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-10-17-33514cbc04a2.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201317", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-17-33514cbc04a2"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "198fece2-5bff-4144-8779-547f01b4661c", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-10-17-33514cbc04a2.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201317", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-17-33514cbc04a2"}, "hash": "7f850106e083f4c3193ec5d2d4c0ce857c9eb578900ed77bc93990d4fd0af09e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "43845168-c598-4d15-aaea-3442fb59738c", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-10-17-33514cbc04a2.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201317", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-17-33514cbc04a2"}, "hash": "a8da699bc3d29cb85d99519f6a10f342fbc1d8b86cdc23c21132848e757dfd26", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf8120ab-67e5-46b2-85ce-99d3a7f94c11", "node_type": "1", "metadata": {}, "hash": "aef58bf2fe2fdc538f56f62f927262c5ab54287a15bc37d00aee9b29d08a5a95", "class_name": "RelatedNodeInfo"}}, "text": "4. **UnstructuredElementNodeParser:** Going beyond basic text-splitting, we introduce the UnstructuredElementNodeParser. It models embedded tables/text hierarchically in a data graph using UnstructuredIO. [ Docs ](https://docs.llamaindex.ai/en/stable/examples/query_engine/sec_tables/tesla_10q_table.html) , [ Tweet ](https://x.com/llama_index/status/1711768906866864403?s=20) . \n  5. **Cross-Encoder Fine-Tuning:** Cross-encoders enhance RAG by refining post-embedding search results. With LlamaIndex, you can now fine-tune cross-encoders on any document, boosting performance. [ Docs ](https://t.co/vAyv94dFk2) , [ Tweet ](https://x.com/jerryjliu0/status/1712856457413370110?s=20) . \n\n", "mimetype": "text/plain", "start_char_idx": 7344, "end_char_idx": 8031, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf8120ab-67e5-46b2-85ce-99d3a7f94c11": {"__data__": {"id_": "bf8120ab-67e5-46b2-85ce-99d3a7f94c11", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-10-17-33514cbc04a2.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201317", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-17-33514cbc04a2"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "198fece2-5bff-4144-8779-547f01b4661c", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-10-17-33514cbc04a2.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201317", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-17-33514cbc04a2"}, "hash": "7f850106e083f4c3193ec5d2d4c0ce857c9eb578900ed77bc93990d4fd0af09e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "964b65b3-b31b-4859-9c5c-e03364bddfec", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-10-17-33514cbc04a2.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201317", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-17-33514cbc04a2"}, "hash": "ee93d8893f6b13f1ddcc2266062106bd61e47d5672fa666f6120292933adb981", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc613e71-6d0f-4670-b40f-af0174adf3a6", "node_type": "1", "metadata": {}, "hash": "a657f954aa10a31615ed3adb037716ece8fde19e5b79a7255791da86979c6e34", "class_name": "RelatedNodeInfo"}}, "text": "#  Integrations & Collaborations:\n\n  1. **Assembly AI:** We introduced a new data reader for audio data integration with AssemblyAI. This integration allows effortless audio loading and facilitates building vector store indices and query engines for inquiries. [ Docs ](https://llamahub.ai/l/assemblyai) , [ Tweet ](https://x.com/llama_index/status/1711156989106299249?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 8031, "end_char_idx": 8411, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc613e71-6d0f-4670-b40f-af0174adf3a6": {"__data__": {"id_": "fc613e71-6d0f-4670-b40f-af0174adf3a6", "embedding": null, "metadata": {"filename": "llamaindex-newsletter-2023-10-17-33514cbc04a2.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201317", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-17-33514cbc04a2"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "198fece2-5bff-4144-8779-547f01b4661c", "node_type": "4", "metadata": {"filename": "llamaindex-newsletter-2023-10-17-33514cbc04a2.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201317", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-17-33514cbc04a2"}, "hash": "7f850106e083f4c3193ec5d2d4c0ce857c9eb578900ed77bc93990d4fd0af09e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf8120ab-67e5-46b2-85ce-99d3a7f94c11", "node_type": "1", "metadata": {"filename": "llamaindex-newsletter-2023-10-17-33514cbc04a2.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201317", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-17-33514cbc04a2"}, "hash": "5cee712813b0c96a4121698808f80cd0feb7b0a60413c4d83e7cdd66729d1c28", "class_name": "RelatedNodeInfo"}}, "text": "2. **Nougat \u2014 MetaAI:** We integrated Nougat, an exceptional OCR tool from Meta, that excels in interpreting scientific papers, notably mathematical notations, and LaTeX as a loader in LlamaHub, allowing streamlined processing of ArXiv papers within the RAG pipeline. [ Docs ](https://llamahub.ai/l/nougat_ocr) , [ Tweet ](https://x.com/llama_index/status/1711896904928292976?s=20) . \n  3. **Hugging Face-Text Embeddings Inference:** We integrated with the new text-embeddings-inference server from HuggingFace offering production-scale serving with distributed tracing for all BERT models at impressive speeds. [ Docs ](https://docs.llamaindex.ai/en/latest/examples/embeddings/text_embedding_inference.html) , [ Tweet ](https://x.com/jerryjliu0/status/1712943016590381554?s=20) . \n\n#  Webinars And Podcast:\n\n  1. [ Webinar ](https://www.youtube.com/watch?v=EYMZVfKcRzM) with Timescale on Time-based retrieval for RAG. \n  2. [ Webinar ](https://www.youtube.com/watch?v=POBcYr0sbcg) with Omar Khattab and Thomas Joshi on DSPy \u2014 a framework for LLMs that emphasizes programming over prompting. \n  3. Jerry Liu\u2019s [ podcast ](https://www.latent.space/p/llamaindex#details) with Latent Space on LlamaIndex\u2019s origin story, fine-tuning, and more. \n\n", "mimetype": "text/plain", "start_char_idx": 8411, "end_char_idx": 9653, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "61e3a267-bbed-4df5-8c56-93b233d3d696": {"__data__": {"id_": "61e3a267-bbed-4df5-8c56-93b233d3d696", "embedding": null, "metadata": {"filename": "llamaindex-update-2023-10-10-3718a3d19fb9.md", "extension": ".md", "title": "LlamaIndex update 2023\u201310\u201310", "date": "Oct 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4960151-cc26-4057-8056-565439ffd414", "node_type": "4", "metadata": {"filename": "llamaindex-update-2023-10-10-3718a3d19fb9.md", "extension": ".md", "title": "LlamaIndex update 2023\u201310\u201310", "date": "Oct 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9"}, "hash": "89a0de0998b55c732e8bb4755211bcd7011c79db3fbf5ce275a89c1f423169c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05405b3d-281a-4186-9d2c-9b81ffba9eaa", "node_type": "1", "metadata": {}, "hash": "39255f7752bb172a87fe815c6d9651da9ca147270dc9bf90f8d62772faf37190", "class_name": "RelatedNodeInfo"}}, "text": "Here\u2019s our weekly look at developments across the LLM space and RAG (Retrieval\nAugmented Generation) in particular, as well as the latest news and features\nfrom your favorite open source library. If you\u2019ve got a project (or a blog\npost, or a video) that you think people should hear about, we\u2019re happy to\nfeature it in here! Drop us a line at [ news@llamaindex.ai\n](mailto:news@llamaindex.ai) .\n\nThis update is now available in handy email form! Just head to our [ home page\n](https://llamaindex.ai) and enter your email to sign up.\n\n**First, the highlights:**\n\n  1. **Full observability with Arize AI Phoenix** : we launched a one-code-line integration with Arize AI for comprehensive tracing and observability in all RAG/agent pipelines. Enjoy local data storage, track LLM input/output prompts, monitor token usage, timing, retrieval visualizations, and agent loops. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 870, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05405b3d-281a-4186-9d2c-9b81ffba9eaa": {"__data__": {"id_": "05405b3d-281a-4186-9d2c-9b81ffba9eaa", "embedding": null, "metadata": {"filename": "llamaindex-update-2023-10-10-3718a3d19fb9.md", "extension": ".md", "title": "LlamaIndex update 2023\u201310\u201310", "date": "Oct 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4960151-cc26-4057-8056-565439ffd414", "node_type": "4", "metadata": {"filename": "llamaindex-update-2023-10-10-3718a3d19fb9.md", "extension": ".md", "title": "LlamaIndex update 2023\u201310\u201310", "date": "Oct 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9"}, "hash": "89a0de0998b55c732e8bb4755211bcd7011c79db3fbf5ce275a89c1f423169c9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61e3a267-bbed-4df5-8c56-93b233d3d696", "node_type": "1", "metadata": {"filename": "llamaindex-update-2023-10-10-3718a3d19fb9.md", "extension": ".md", "title": "LlamaIndex update 2023\u201310\u201310", "date": "Oct 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9"}, "hash": "d0982e014c0df1fdd7c7af6d9df1f1283ba32b32fb23dd5d945170f41c3ba349", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d89deec-c3b6-4ed3-8676-3c8d80cfba25", "node_type": "1", "metadata": {}, "hash": "08892f93906010efce91b7ba0a4f28a16b3ff630118e86af3edde28e85c86af2", "class_name": "RelatedNodeInfo"}}, "text": "Additionally, export traces for evaluations and data analysis. All while ensuring your data stays local. [ Notebook ](https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_tracing_tutorial.ipynb) , [ Tweet ](https://twitter.com/llama_index/status/1708866998149816540?s=20) . \n  2. **RetrieverEvaluator** : new in the library, \u201cRetrieverEvaluator\u201d allows enhanced retrieval evaluations, complementing LLM generation tests. The module supports benchmarking, standard ranking metrics, and synthetic dataset creation for comprehensive retrieval assessments. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/evaluation/retrieval/retriever_eval.html) , [ Tweet ](https://twitter.com/llama_index/status/1704884477552660612?s=20) . \n  3. **HuggingFace Embeddings** : we added native support for three more Hugging Face embedding models, including the base embeddings wrapper, instructor embeddings, and optimum embeddings in ONNX format. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/embeddings/huggingface.html) , [ Tweet ](https://x.com/llama_index/status/1706096762933653962?s=20) . \n  4. **Multi-Document Agents** : we\u2019ve introduced v0 experimental support for multi-document agents for advanced QA, beyond typical top-k RAG. It supports diverse queries from single to multiple docs. This foundational version sets the stage for future enhancements like parallel query planning and reduced latency. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/agent/multi_document_agents.html) , [ Tweet ](https://twitter.com/jerryjliu0/status/1708523212366393403?s=20) . \n\n**Congratulations to our Streamlit Hackathon Winners!**\n\nWe love seeing people build amazing things with LlamaIndex!\n\n  1. ", "mimetype": "text/plain", "start_char_idx": 870, "end_char_idx": 2634, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d89deec-c3b6-4ed3-8676-3c8d80cfba25": {"__data__": {"id_": "0d89deec-c3b6-4ed3-8676-3c8d80cfba25", "embedding": null, "metadata": {"filename": "llamaindex-update-2023-10-10-3718a3d19fb9.md", "extension": ".md", "title": "LlamaIndex update 2023\u201310\u201310", "date": "Oct 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4960151-cc26-4057-8056-565439ffd414", "node_type": "4", "metadata": {"filename": "llamaindex-update-2023-10-10-3718a3d19fb9.md", "extension": ".md", "title": "LlamaIndex update 2023\u201310\u201310", "date": "Oct 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9"}, "hash": "89a0de0998b55c732e8bb4755211bcd7011c79db3fbf5ce275a89c1f423169c9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05405b3d-281a-4186-9d2c-9b81ffba9eaa", "node_type": "1", "metadata": {"filename": "llamaindex-update-2023-10-10-3718a3d19fb9.md", "extension": ".md", "title": "LlamaIndex update 2023\u201310\u201310", "date": "Oct 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9"}, "hash": "4e60de22702d2c2a31004ecbb9078edf8d1169d38a5860b03e1a48a42cacc933", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "091c4290-f2e2-4bfd-bc40-b9fc6f17d198", "node_type": "1", "metadata": {}, "hash": "1ec312553a84592ff30d045d178c1b337b4940208c257163151adbfc4c4e1ed0", "class_name": "RelatedNodeInfo"}}, "text": "NewsGPT by Kang-Chi Ho: [ https://buff.ly/46jkutx ](https://buff.ly/46jkutx)\n  2. FinSight by Vishwas Gowda: [ https://buff.ly/3PzOnyC ](https://buff.ly/3PzOnyC)\n\n**Feature Releases** **and Enhancements:**\n\n  1. **Multi-Document Agents** : we introduced multi-document agents (V0) for advanced QA, beyond typical top-k RAG. They support diverse queries from single to multiple docs. This foundational version sets the stage for future enhancements like parallel query planning and reduced latency. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/agent/multi_document_agents.html) , [ Tweet ](https://twitter.com/jerryjliu0/status/1708523212366393403?s=20) . \n  2. **Ensemble Retriever:** we\u2019re addressing the RAG challenge of determining chunk size by experimenting with diverse document chunking and ensembling for retrieval. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/retrievers/ensemble_retrieval.html) , [ Tweet ](https://twitter.com/jerryjliu0/status/1707541270934212737?s=20) . \n  3. **HuggingFace Embeddings** : we added native support for three more Hugging Face embedding models, including the base embeddings wrapper, instructor embeddings, and optimum embeddings in ONNX format. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/embeddings/huggingface.html) , [ Tweet ](https://x.com/llama_index/status/1706096762933653962?s=20) . \n  4. **OpenAI Function Calling fine-tuning:** we\u2019re using OpenAI\u2019s latest function calling fine-tuning which enhanced structured data extraction, optimizing gpt-3.5-turbo for improved extraction in RAG. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/finetuning/openai_fine_tuning_functions.html) , [ Tweet ](https://twitter.com/llama_index/status/1709986951661818185?s=20) . \n  5. **Metadata Extraction** : we\u2019re making metadata extraction efficient by extracting a complete Pydantic object from a document with just one LLM call. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/metadata_extraction/PydanticExtractor.html) , [ Tweet ](https://twitter.com/llama_index/status/1705302359038202101?s=20) . \n  6. **Structured RAG Outputs** : we now efficiently structure RAG pipeline outputs with native Pydantic outputs from all queries without the need for an additional LLM parsing call. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/query_engine/pydantic_query_engine.html) , [ Tweet ](https://twitter.com/llama_index/status/1709229624709025972?s=20) . \n  7. **Streamlined** [ **secinsights.ai** ](http://secinsights.ai) **deployment** : Our open-sourced [ secinsights.ai ](https://secinsights.ai) offers a RAG app template, now enhanced with GitHub Codespaces and Docker for swift cloud deployment without setup hassles. [ Tweet ](https://twitter.com/llama_index/status/1707773681605419296?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 2634, "end_char_idx": 5462, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "091c4290-f2e2-4bfd-bc40-b9fc6f17d198": {"__data__": {"id_": "091c4290-f2e2-4bfd-bc40-b9fc6f17d198", "embedding": null, "metadata": {"filename": "llamaindex-update-2023-10-10-3718a3d19fb9.md", "extension": ".md", "title": "LlamaIndex update 2023\u201310\u201310", "date": "Oct 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4960151-cc26-4057-8056-565439ffd414", "node_type": "4", "metadata": {"filename": "llamaindex-update-2023-10-10-3718a3d19fb9.md", "extension": ".md", "title": "LlamaIndex update 2023\u201310\u201310", "date": "Oct 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9"}, "hash": "89a0de0998b55c732e8bb4755211bcd7011c79db3fbf5ce275a89c1f423169c9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d89deec-c3b6-4ed3-8676-3c8d80cfba25", "node_type": "1", "metadata": {"filename": "llamaindex-update-2023-10-10-3718a3d19fb9.md", "extension": ".md", "title": "LlamaIndex update 2023\u201310\u201310", "date": "Oct 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9"}, "hash": "4f85c6589240cccd70fa74d628548e9ee675cd6edaf9547c8dadb03cacd1ec09", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5875733-82e8-4baf-8b09-2c911cf02b4c", "node_type": "1", "metadata": {}, "hash": "9365df811d4f158f65050e99d324487f6acc1db0cdbfc2cb3b99ada94a4b49d2", "class_name": "RelatedNodeInfo"}}, "text": "8. **LongContextReorder:** We introduced LongContextReorder****,**** Zeneto\u2019s approach to reposition vital context in RAG systems, addressing the challenge of over-retrieving which can obscure essential details. [ Docs ](https://gpt-index.readthedocs.io/en/latest/core_modules/query_modules/node_postprocessors/modules.html#longcontextreorder) , [ Tweet ](https://twitter.com/llama_index/status/1705009024050270456?s=20) . \n  9. **RA-DIT:** We drew inspiration from the RA-DIT paper, which introduced LLM fine-tuning for retrieval-augmented input prompts to improve RAG systems. This method fosters enhanced utilization of context and more effective answer synthesis, even in the presence of suboptimal context. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/finetuning/knowledge/finetune_retrieval_aug.html) , [ Tweet ](https://twitter.com/jerryjliu0/status/1709646787076935818?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 5462, "end_char_idx": 6366, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5875733-82e8-4baf-8b09-2c911cf02b4c": {"__data__": {"id_": "c5875733-82e8-4baf-8b09-2c911cf02b4c", "embedding": null, "metadata": {"filename": "llamaindex-update-2023-10-10-3718a3d19fb9.md", "extension": ".md", "title": "LlamaIndex update 2023\u201310\u201310", "date": "Oct 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4960151-cc26-4057-8056-565439ffd414", "node_type": "4", "metadata": {"filename": "llamaindex-update-2023-10-10-3718a3d19fb9.md", "extension": ".md", "title": "LlamaIndex update 2023\u201310\u201310", "date": "Oct 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9"}, "hash": "89a0de0998b55c732e8bb4755211bcd7011c79db3fbf5ce275a89c1f423169c9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "091c4290-f2e2-4bfd-bc40-b9fc6f17d198", "node_type": "1", "metadata": {"filename": "llamaindex-update-2023-10-10-3718a3d19fb9.md", "extension": ".md", "title": "LlamaIndex update 2023\u201310\u201310", "date": "Oct 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9"}, "hash": "bf39028256f42718ef03db72b49a4ebc61ec4bd1a0148aae07844dc8b15804c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0da4b06c-6cde-477e-a489-bc841281c89f", "node_type": "1", "metadata": {}, "hash": "104d38854dabace0442d0813c65daecd2e51933771cdb8e3ccce7ed4d421fdef", "class_name": "RelatedNodeInfo"}}, "text": "10. **Blockchain:** LlamaIndex data agents can be now used to analyze any blockchain subgraph using natural language queries. [ Tweet ](https://twitter.com/llama_index/status/1707417880420294741?s=20) . \n\n", "mimetype": "text/plain", "start_char_idx": 6366, "end_char_idx": 6571, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0da4b06c-6cde-477e-a489-bc841281c89f": {"__data__": {"id_": "0da4b06c-6cde-477e-a489-bc841281c89f", "embedding": null, "metadata": {"filename": "llamaindex-update-2023-10-10-3718a3d19fb9.md", "extension": ".md", "title": "LlamaIndex update 2023\u201310\u201310", "date": "Oct 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4960151-cc26-4057-8056-565439ffd414", "node_type": "4", "metadata": {"filename": "llamaindex-update-2023-10-10-3718a3d19fb9.md", "extension": ".md", "title": "LlamaIndex update 2023\u201310\u201310", "date": "Oct 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9"}, "hash": "89a0de0998b55c732e8bb4755211bcd7011c79db3fbf5ce275a89c1f423169c9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5875733-82e8-4baf-8b09-2c911cf02b4c", "node_type": "1", "metadata": {"filename": "llamaindex-update-2023-10-10-3718a3d19fb9.md", "extension": ".md", "title": "LlamaIndex update 2023\u201310\u201310", "date": "Oct 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9"}, "hash": "431c2e48060942f95222c837301c72b0fc32bdf62316dd9c908492908e5de776", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bec9f5fb-2778-4e3e-a321-ff2c79883c45", "node_type": "1", "metadata": {}, "hash": "ed8ba1d30deab733d05d8c9633984a0c1646de90a93eb29b9b05346ea4b5ac1f", "class_name": "RelatedNodeInfo"}}, "text": "**RAG Evaluation Enhancements:**\n\n  1. **RetrieverEvaluator** : We introduced \u201cRetrieverEvaluator\u201d for enhanced retrieval evaluations, complementing LLM generation tests. The module supports benchmarking, standard ranking metrics, and synthetic dataset creation for comprehensive retrieval assessments. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/evaluation/retrieval/retriever_eval.html) , [ Tweet ](https://twitter.com/llama_index/status/1704884477552660612?s=20) . \n  2. **SemanticSimilarityEvaluator** : We introduced a new semantic similarity evaluator \u2014 SemanticSimilarityEvaluator for LLM/RAG outputs, comparing embedding similarity between reference and generated answers. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/evaluation/semantic_similarity_eval.html) , [ Tweet ](https://twitter.com/llama_index/status/1705614101601509630?s=20) . \n\n**Tutorials:**\n\n  1. [ Guide ](https://gpt-index.readthedocs.io/en/latest/examples/low_level/oss_ingestion_retrieval.html) on building RAG from scratch with open-source modules. \n  2. [ Dstack ](https://twitter.com/dstackai) [ tutorial ](https://dstack.ai/examples/llama-index-weaviate/) on implementing RAG with OSS LLMs using LlamaIndex and Weaviate. \n  3. [ Wenqi Glantz ](https://twitter.com/wenqi_glantz) [ turorial ](https://betterprogramming.pub/exploring-react-agent-for-better-prompting-in-rag-pipeline-b231aae0ca7c) on Exploring ReAct Agent for Better Prompting in RAG Pipeline. \n  4. [ Javier Torres ](https://twitter.com/Javtorr_) [ tutorial ](https://gpt-index.readthedocs.io/en/stable/end_to_end_tutorials/chatbots/building_a_chatbot.html) on building a multi-document chatbot. \n  5. [ Erika Cardenas ](https://twitter.com/ecardenas300) [ tutorial ](https://www.youtube.com/watch?v=Su-ROQMaiaw) on RAG techniques in LlamaIndex covering SQL Router Query Engine, Sub Question Query Engine, Recursive Retriever Query Engine, Self-Correcting Query Engine. \n  6. [ Wenqi Glantz ](https://twitter.com/wenqi_glantz) [ tutorial ](https://betterprogramming.pub/7-query-strategies-for-navigating-knowledge-graphs-with-llamaindex-ed551863d416) on 7 Query Strategies for Navigating Knowledge Graphs With LlamaIndex. \n  7. [ Ravi Theja ](https://twitter.com/ravithejads) [ tutorial ](/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5) on Evaluating the Ideal Chunk Size for RAG using LlamaIndex. \n\n**Integrations & Collaborations: **\n\n  1. **Arize AI Phoenix** : We launched a one-code-line integration with Arize AI for comprehensive tracing and observability in all RAG/agent pipelines. Enjoy local data storage, track LLM input/output prompts, monitor token usage, timing, retrieval visualizations, and agent loops. ", "mimetype": "text/plain", "start_char_idx": 6571, "end_char_idx": 9303, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bec9f5fb-2778-4e3e-a321-ff2c79883c45": {"__data__": {"id_": "bec9f5fb-2778-4e3e-a321-ff2c79883c45", "embedding": null, "metadata": {"filename": "llamaindex-update-2023-10-10-3718a3d19fb9.md", "extension": ".md", "title": "LlamaIndex update 2023\u201310\u201310", "date": "Oct 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4960151-cc26-4057-8056-565439ffd414", "node_type": "4", "metadata": {"filename": "llamaindex-update-2023-10-10-3718a3d19fb9.md", "extension": ".md", "title": "LlamaIndex update 2023\u201310\u201310", "date": "Oct 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9"}, "hash": "89a0de0998b55c732e8bb4755211bcd7011c79db3fbf5ce275a89c1f423169c9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0da4b06c-6cde-477e-a489-bc841281c89f", "node_type": "1", "metadata": {"filename": "llamaindex-update-2023-10-10-3718a3d19fb9.md", "extension": ".md", "title": "LlamaIndex update 2023\u201310\u201310", "date": "Oct 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9"}, "hash": "f69cd87d6d4e32b57e70a9bb2ac793500400e78f6f19a1f91d2352b4451e5d6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f1131c9-1525-4199-955c-66e020f1c690", "node_type": "1", "metadata": {}, "hash": "9fe604e308c88f14cf8713531f446d3da8b3b0b0c0997dcbb7cf2e18da142157", "class_name": "RelatedNodeInfo"}}, "text": "Additionally, export traces for evaluations and data analysis. All while ensuring your data stays local. [ Notebook ](https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_tracing_tutorial.ipynb) , [ Tweet ](https://twitter.com/llama_index/status/1708866998149816540?s=20) . \n  2. **Neo4j** : We introduced an API spec for LLM-agent interaction with Neo4j, offering beyond just \u201ctext-to-cypher\u201d with full agent reasoning. [ Docs ](https://llamahub.ai/l/tools-neo4j_db) , [ Tweet ](https://twitter.com/llama_index/status/1706049423644774910?s=20) . \n  3. **TimescaleDB** : We integrated with TimescaleDB for enhanced time-based retrieval in RAG systems, offering time filters and cost-effective storage solutions. [ Blogpost ](/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0) , [ Tweet ](https://twitter.com/llama_index/status/1707057039950991798?s=20) . \n  4. **BraintrustData** : We integrated with BraintrustData, enabling seamless RAG pipeline construction, evaluations, and easy public URL sharing for results. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/retrievers/recurisve_retriever_nodes_braintrust.ipynb) , [ Tweet ](https://twitter.com/llama_index/status/1707485040018632776?s=20) . \n  5. **LocalAI** : We integrated LocalAI_API LLM support for on-prem runs or as an alternative to OpenAI LLM. [ Tweet ](https://x.com/llama_index/status/1708227476684734555?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 9303, "end_char_idx": 10817, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f1131c9-1525-4199-955c-66e020f1c690": {"__data__": {"id_": "8f1131c9-1525-4199-955c-66e020f1c690", "embedding": null, "metadata": {"filename": "llamaindex-update-2023-10-10-3718a3d19fb9.md", "extension": ".md", "title": "LlamaIndex update 2023\u201310\u201310", "date": "Oct 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4960151-cc26-4057-8056-565439ffd414", "node_type": "4", "metadata": {"filename": "llamaindex-update-2023-10-10-3718a3d19fb9.md", "extension": ".md", "title": "LlamaIndex update 2023\u201310\u201310", "date": "Oct 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9"}, "hash": "89a0de0998b55c732e8bb4755211bcd7011c79db3fbf5ce275a89c1f423169c9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bec9f5fb-2778-4e3e-a321-ff2c79883c45", "node_type": "1", "metadata": {"filename": "llamaindex-update-2023-10-10-3718a3d19fb9.md", "extension": ".md", "title": "LlamaIndex update 2023\u201310\u201310", "date": "Oct 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9"}, "hash": "13ddffb6bdbe115718e6596f5c4a7ad6335836638fe784b6196d3b43d301bb3c", "class_name": "RelatedNodeInfo"}}, "text": "6. **HoneyHiveAI** : We integrated with HoneyHiveAI for enhanced multi-step RAG/agent pipeline monitoring. Log traces, gather user feedback, and utilize it for precise fine-tuning and evaluations. [ Docs ](https://docs.honeyhive.ai/quickstart/llamaindex) , [ Tweet ](https://x.com/llama_index/status/1709311769947357425?s=20) . \n  7. **UnstructuredIO** : We integrated with UnstructuredIO to tackle the RAG challenge of querying embedded tables in 10-K filings. Now, seamlessly query any tabular data or text within a 10-K document. [ Notebook ](https://github.com/run-llama/llama_index/blob/main/docs/examples/query_engine/sec_tables/tesla_10q_table.ipynb) , [ Tweet ](https://twitter.com/jerryjliu0/status/1709352476456132702?s=20) . \n  8. **Clarifai** : We integrated with Clarifai, offering access to 40+ LLMs and various embedding models. [ Tweet ](https://twitter.com/llama_index/status/1710065802672476342?s=20) . \n\n**Webinars:**\n\n  1. [ Webinar ](https://www.singlestore.com/resources/webinar-how-to-build-a-genai-app-with-llama-index/?utm_source=kunal-kushwaha&utm_medium=influencer&utm_campaign=How-to-Build-a-GenAI-App-with-LlamaIndex&campaignid=7014X0000029YtwQAE) by SingleStoreDB on How to Build a GenAI App with LlamaIndex. \n  2. [ Webinar ](https://www.youtube.com/watch?v=C5NhoMBkaQU) on projects built during the SuperAGI Autonomous Agents Hackathon featuring evo.ninja, RicAI, Atlas and MunichAI. \n\n**Events:**\n\n  1. Jerry Liu and Simon [ conducted ](https://github.com/anyscale/ray-summit-2023-training/tree/main/Ray-LlamaIndex/notebooks) a workshop on RAG + Evaluation at RaySummit. \n  2. [ Yi Ding ](https://twitter.com/yi_ding) spoke on \u2018LLM Quirks Mode\u2019 at MLOps community event. \n  3. Jerry Liu spoke on Evals/ Benchmarking [ and Advanced RAG techniques ](https://docs.google.com/presentation/d/1v7T6ejrSo87ndGeGC7tt6zeq-cftu03WWw7WL8Jskug/edit?usp=sharing) at AIConf 2023. \n  4. [ Ravi Theja ](https://twitter.com/ravithejads) conducted a workshop on Mastering RAG with LlamaIndex at PyCon India, 2023. \n  5. [ Ravi Theja ](https://twitter.com/ravithejads) presented a [ poster ](https://x.com/ravithejads/status/1709431323989856348?s=20) on Automatic Knowledge Transfer(KT) Video generation on code bases using LlamaIndex at PyCon India, 2023. \n\n", "mimetype": "text/plain", "start_char_idx": 10817, "end_char_idx": 13090, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9862ca5c-3166-48b0-828c-21b8b61ffb53": {"__data__": {"id_": "9862ca5c-3166-48b0-828c-21b8b61ffb53", "embedding": null, "metadata": {"filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "834eaff1-0a04-4c4b-a7fe-b3859be73a69", "node_type": "4", "metadata": {"filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "5f1ead524c7a5ea5c2b8bb1f5ae83cf915628d1032ab2cd4bc701b369b8b0b57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed8d5543-956c-4f20-9f13-d9bd3d42c116", "node_type": "1", "metadata": {}, "hash": "b09cced75eeec949dfa33d73d4e73b58eef8fad21dc9ab683a173655d2208cd2", "class_name": "RelatedNodeInfo"}}, "text": "#  Introduction\n\nRetrieval-augmented generation (RAG) has introduced an innovative approach\nthat fuses the extensive retrieval capabilities of search systems with the\nLLM. When implementing a RAG system, one critical parameter that governs the\nsystem\u2019s efficiency and performance is the ` chunk_size ` . How does one\ndiscern the optimal chunk size for seamless retrieval? This is where\nLlamaIndex ` Response Evaluation ` comes in handy. In this blog post, we'll\nguide you through the steps to determine the best ` chunk size ` using\nLlamaIndex\u2019s ` Response Evaluation ` module. If you're unfamiliar with the `\nResponse ` Evaluation module, we recommend reviewing its [ documentation\n](https://docs.llamaindex.ai/en/latest/core_modules/supporting_modules/evaluation/modules.html)\nbefore proceeding.\n\n#  Why Chunk Size Matters\n\nChoosing the right ` chunk_size ` is a critical decision that can influence\nthe efficiency and accuracy of a RAG system in several ways:\n\n  1. **Relevance and Granularity** : A small ` chunk_size ` , like 128, yields more granular chunks. This granularity, however, presents a risk: vital information might not be among the top retrieved chunks, especially if the ` similarity_top_k ` setting is as restrictive as 2. Conversely, a chunk size of 512 is likely to encompass all necessary information within the top chunks, ensuring that answers to queries are readily available. To navigate this, we employ the Faithfulness and Relevancy metrics. These measure the absence of \u2018hallucinations\u2019 and the \u2018relevancy\u2019 of responses based on the query and the retrieved contexts respectively. \n  2. **Response Generation Time** : As the ` chunk_size ` increases, so does the volume of information directed into the LLM to generate an answer. While this can ensure a more comprehensive context, it might also slow down the system. Ensuring that the added depth doesn't compromise the system's responsiveness is crucial. \n\nIn essence, determining the optimal ` chunk_size ` is about striking a\nbalance: capturing all essential information without sacrificing speed. It's\nvital to undergo thorough testing with various sizes to find a configuration\nthat suits the specific use case and dataset.\n\nFor a practical evaluation in choosing the right ` chunk_size ` , you can\naccess and run the following setup on this [ **Google Colab Notebook**\n](https://colab.research.google.com/drive/1LPvJyEON6btMpubYdwySfNs0FuNR9nza?usp=sharing)\n.\n\n#  Setup\n\nBefore embarking on the experiment, we need to ensure all requisite modules\nare imported:\n\n    \n    \n    import nest_asyncio\n    \n    nest_asyncio.apply()\n    \n    from llama_index import (\n        SimpleDirectoryReader,\n        VectorStoreIndex,\n        ServiceContext,\n    )\n    from llama_index.evaluation import (\n        DatasetGenerator,\n        FaithfulnessEvaluator,\n        RelevancyEvaluator\n    )\n    from llama_index.llms import OpenAI\n    \n    import openai\n    import time\n    openai.api_key = 'OPENAI-API-KEY'\n\n#  Download Data\n\nWe\u2019ll be using the Uber 10K SEC Filings for 2021 for this experiment.\n\n    \n    \n    !mkdir -p 'data/10k/'\n    !wget 'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'\n\n#  Load Data\n\nLet\u2019s load our document.\n\n    \n    \n    documents = SimpleDirectoryReader(\"./data/10k/\").load_data()\n\n#  Question Generation\n\nTo select the right ` chunk_size ` , we'll compute metrics like Average\nResponse time, Faithfulness, and Relevancy for various ` chunk_sizes ` . The `\nDatasetGenerator ` will help us generate questions from the documents.\n\n    \n    \n    data_generator = DatasetGenerator.from_documents(documents)\n    eval_questions = data_generator.generate_questions_from_nodes()\n\n#  Setting Up Evaluators\n\nWe are setting up the GPT-4 model to serve as the backbone for evaluating the\nresponses generated during the experiment. Two evaluators, `\nFaithfulnessEvaluator ` and ` RelevancyEvaluator ` , are initialised with the\n` service_context ` .\n\n  1. **Faithfulness Evaluator** \u2014 It is useful for measuring if the response was hallucinated and measures if the response from a query engine matches any source nodes. \n  2. **Relevancy Evaluator** \u2014 It is useful for measuring if the query was actually answered by the response and measures if the response + source nodes match the query. \n\n    \n    \n    # We will use GPT-4 for evaluating the responses\n    gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n    \n    # Define service context for GPT-4 for evaluation\n    service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n    \n    # Define Faithfulness and Relevancy Evaluators which are based on GPT-4\n    faithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\n    relevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4)\n\n#  Response Evaluation For A Chunk Size\n\nWe evaluate each chunk_size based on 3 metrics.\n\n  1. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4936, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed8d5543-956c-4f20-9f13-d9bd3d42c116": {"__data__": {"id_": "ed8d5543-956c-4f20-9f13-d9bd3d42c116", "embedding": null, "metadata": {"filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "834eaff1-0a04-4c4b-a7fe-b3859be73a69", "node_type": "4", "metadata": {"filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "5f1ead524c7a5ea5c2b8bb1f5ae83cf915628d1032ab2cd4bc701b369b8b0b57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9862ca5c-3166-48b0-828c-21b8b61ffb53", "node_type": "1", "metadata": {"filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "1a06d75d1dca85c00f1dbe5e0802734c79b13016ce53074bce9916e508930c42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2151e821-2644-4e0f-8783-f09f84eea08c", "node_type": "1", "metadata": {}, "hash": "44b5bad1804bf6a62c7fbc6b9b3df60099fdcf98d93eef67ee202b0d56696161", "class_name": "RelatedNodeInfo"}}, "text": "Average Response Time. \n  2. Average Faithfulness. \n  3. Average Relevancy. \n\nHere\u2019s a function, ` evaluate_response_time_and_accuracy ` , that does just\nthat which has:\n\n  1. VectorIndex Creation. \n  2. Building the Query Engine. \n  3. ", "mimetype": "text/plain", "start_char_idx": 4936, "end_char_idx": 5173, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2151e821-2644-4e0f-8783-f09f84eea08c": {"__data__": {"id_": "2151e821-2644-4e0f-8783-f09f84eea08c", "embedding": null, "metadata": {"filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "834eaff1-0a04-4c4b-a7fe-b3859be73a69", "node_type": "4", "metadata": {"filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "5f1ead524c7a5ea5c2b8bb1f5ae83cf915628d1032ab2cd4bc701b369b8b0b57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed8d5543-956c-4f20-9f13-d9bd3d42c116", "node_type": "1", "metadata": {"filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "663f92a9058c031f18df3009f8a4c2d720969b1460dae8a107d45dd428cd032f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f87720ff-a172-48a9-9d3d-c82aed6981d6", "node_type": "1", "metadata": {}, "hash": "9dc20c05411aa28d9de79a146fa326872c3279e96e51c8f6d1c6a65fc5123616", "class_name": "RelatedNodeInfo"}}, "text": "Metrics Calculation. \n\n    \n    \n    # Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size\n    # We use GPT-3.5-Turbo to generate response and GPT-4 to evaluate it.\n    def evaluate_response_time_and_accuracy(chunk_size, eval_questions):\n        \"\"\"\n        Evaluate the average response time, faithfulness, and relevancy of responses generated by GPT-3.5-turbo for a given chunk size.\n        \n        Parameters:\n        chunk_size (int): The size of data chunks being processed.\n        \n        Returns:\n        tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.\n        \"\"\"\n    \n        total_response_time = 0\n        total_faithfulness = 0\n        total_relevancy = 0\n    \n        # create vector index\n        llm = OpenAI(model=\"gpt-3.5-turbo\")\n        service_context = ServiceContext.from_defaults(llm=llm, chunk_size=chunk_size)\n        vector_index = VectorStoreIndex.from_documents(\n            eval_documents, service_context=service_context\n        )\n        # build query engine\n        query_engine = vector_index.as_query_engine()\n        num_questions = len(eval_questions)\n    \n        # Iterate over each question in eval_questions to compute metrics.\n        # While BatchEvalRunner can be used for faster evaluations (see: https://docs.llamaindex.ai/en/latest/examples/evaluation/batch_eval.html),\n        # we're using a loop here to specifically measure response time for different chunk sizes.\n        for question in eval_questions:\n            start_time = time.time()\n            response_vector = query_engine.query(question)\n            elapsed_time = time.time() - start_time\n            \n            faithfulness_result = faithfulness_gpt4.evaluate_response(\n                response=response_vector\n            ).passing\n            \n            relevancy_result = relevancy_gpt4.evaluate_response(\n                query=question, response=response_vector\n            ).passing\n    \n            total_response_time += elapsed_time\n            total_faithfulness += faithfulness_result\n            total_relevancy += relevancy_result\n    \n        average_response_time = total_response_time / num_questions\n        average_faithfulness = total_faithfulness / num_questions\n        average_relevancy = total_relevancy / num_questions\n    \n        return average_response_time, average_faithfulness, average_relevancy\n\n#  Testing Across Different Chunk Sizes\n\nWe\u2019ll evaluate a range of chunk sizes to identify which offers the most\npromising metrics.\n\n    \n    \n    chunk_sizes = [128, 256, 512, 1024, 2048]\n    \n    for chunk_size in chunk_sizes:\n      avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size, eval_questions)\n      print(f\"Chunk size {chunk_size} - Average Response time: {avg_response_time:.2f}s, Average Faithfulness: {avg_faithfulness:.2f}, Average Relevancy: {avg_relevancy:.2f}\")\n\n#  Bringing It All Together\n\nLet\u2019s compile the processes:\n\n    \n    \n    import nest_asyncio\n    \n    nest_asyncio.apply()\n    \n    from llama_index import (\n        SimpleDirectoryReader,\n        VectorStoreIndex,\n        ServiceContext,\n    )\n    from llama_index.evaluation import (\n        DatasetGenerator,\n        FaithfulnessEvaluator,\n        RelevancyEvaluator\n    )\n    from llama_index.llms import OpenAI\n    \n    import openai\n    import time\n    \n    openai.api_key = 'OPENAI-API-KEY'\n    \n    # Download Data\n    !mkdir -p 'data/10k/'\n    !wget 'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'\n    \n    # Load Data\n    reader = SimpleDirectoryReader(\"./data/10k/\")\n    documents = reader.load_data()\n    \n    # To evaluate for each chunk size, we will first generate a set of 40 questions from first 20 pages.\n    eval_documents = documents[:20]\n    data_generator = DatasetGenerator.from_documents()\n    eval_questions = data_generator.generate_questions_from_nodes(num = 20)\n    \n    # We will use GPT-4 for evaluating the responses\n    gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n    \n    # Define service context for GPT-4 for evaluation\n    service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n    \n    # Define Faithfulness and Relevancy Evaluators which are based on GPT-4\n    faithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\n    relevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4)\n    \n    # Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size\n    def evaluate_response_time_and_accuracy(chunk_size):\n        total_response_time = 0\n        total_faithfulness = 0\n        total_relevancy = 0\n    \n        # create vector index\n        llm = OpenAI(model=\"gpt-3.5-turbo\")\n        service_context = ServiceContext.from_defaults(llm=llm, chunk_size=chunk_size)\n        vector_index = VectorStoreIndex.from_documents(\n            eval_documents, service_context=service_context\n        )\n    \n        query_engine = vector_index.as_query_engine()\n        num_questions = len(eval_questions)\n    \n        for question in eval_questions:\n            start_time = time.time()\n            response_vector = query_engine.query(question)\n            elapsed_time = time.time() - start_time\n            \n            faithfulness_result = faithfulness_gpt4.evaluate_response(\n                response=response_vector\n            ).passing\n            \n            relevancy_result = relevancy_gpt4.evaluate_response(\n                query=question, response=response_vector\n            ).passing\n    \n            total_response_time += elapsed_time\n            total_faithfulness += faithfulness_result\n            total_relevancy += relevancy_result\n    \n        average_response_time = total_response_time / num_questions\n        average_faithfulness = total_faithfulness / num_questions\n        average_relevancy = total_relevancy / num_questions\n    \n        return average_response_time, average_faithfulness, average_relevancy\n    \n    # Iterate over different chunk sizes to evaluate the metrics to help fix the chunk size.\n    for chunk_size in [128, 256, 512, 1024, 2048]\n      avg_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size)\n      print(f\"Chunk size {chunk_size} - Average Response time: {avg_time:.2f}s, Average Faithfulness: {avg_faithfulness:.2f}, Average Relevancy: {avg_relevancy:.2f}\")\n\n#  Result\n\nThe above table illustrates that as the chunk size increases, there is a minor\nuptick in the Average Response Time. ", "mimetype": "text/plain", "start_char_idx": 5173, "end_char_idx": 11861, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f87720ff-a172-48a9-9d3d-c82aed6981d6": {"__data__": {"id_": "f87720ff-a172-48a9-9d3d-c82aed6981d6", "embedding": null, "metadata": {"filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "834eaff1-0a04-4c4b-a7fe-b3859be73a69", "node_type": "4", "metadata": {"filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "5f1ead524c7a5ea5c2b8bb1f5ae83cf915628d1032ab2cd4bc701b369b8b0b57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2151e821-2644-4e0f-8783-f09f84eea08c", "node_type": "1", "metadata": {"filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}, "hash": "af74ca055e7427150f905da00f2c6f565e9202869852b8aa076a05e52c0b42b2", "class_name": "RelatedNodeInfo"}}, "text": "Interestingly, the Average Faithfulness\nseems to reach its zenith at ` chunk_size ` of 1024, whereas Average Relevancy\nshows a consistent improvement with larger chunk sizes, also peaking at 1024.\nThis suggests that a chunk size of 1024 might strike an optimal balance\nbetween response time and the quality of the responses, measured in terms of\nfaithfulness and relevancy.\n\n#  Conclusion\n\nIdentifying the best chunk size for a RAG system is as much about intuition as\nit is empirical evidence. With LlamaIndex\u2019s ` Response Evaluation ` module,\nyou can experiment with various sizes and base your decisions on concrete\ndata. When building a RAG system, always remember that ` chunk_size ` is a\npivotal parameter. Invest the time to meticulously evaluate and adjust your\nchunk size for unmatched results.\n\n", "mimetype": "text/plain", "start_char_idx": 11861, "end_char_idx": 12666, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d027f33-c6a8-44ac-8e7d-6c3b018395fd": {"__data__": {"id_": "6d027f33-c6a8-44ac-8e7d-6c3b018395fd", "embedding": null, "metadata": {"filename": "llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff.md", "extension": ".md", "title": "LlamaIndex + Laurie Voss: an alpaca joins the llamas", "date": "Oct 2, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92c32a37-38b7-47aa-bc5f-5b3b3abdc47e", "node_type": "4", "metadata": {"filename": "llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff.md", "extension": ".md", "title": "LlamaIndex + Laurie Voss: an alpaca joins the llamas", "date": "Oct 2, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff"}, "hash": "95746d4576ef2c3acaefa0ebd5fc63676b47e1933bbdc913dac49fe9b9d8f94f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19a8b33e-e0e7-4055-8619-0616ae29e740", "node_type": "1", "metadata": {}, "hash": "5d6933ff35d56dcd5ec9a717f6fa7df7d14f9061510548a8c9caef66fd7e225e", "class_name": "RelatedNodeInfo"}}, "text": "Hi, I\u2019m Laurie, and today is my first day as VP of Developer Relations at\nLlamaIndex!\n\nQuick background on me: I started my career 27 years ago as a web developer,\nfounded a couple of companies including [ npm Inc. ](https://www.npmjs.com/) ,\nand have always been about talking to developers about the state of the\ndevelopment world and how we fit into it, whether that\u2019s on my [ personal site\n](https://seldo.com/) , in [ conference talks\n](https://www.youtube.com/watch?v=gChULw-uEjY&ab_channel=JSConf) , or in big [\ncommunity surveys ](https://jamstack.org/survey/2022/) .\n\nI wrote last week about why [ LLMs are the future of software\n](https://seldo.com/posts/ai-ml-llms-and-the-future-of-software) . To\nsummarize that post: until now computers have been very good at ingesting,\nsorting, and transmitting data, but understanding what they were working with\nwas beyond them. Very recently, with tools like [ GPT-4\n](https://openai.com/research/gpt-4) and [ Llama 2\n](https://ai.meta.com/llama/) that threshold has been crossed. Software can\nnow read, summarize, and make novel connections within arbitrarily large sets\nof data. It can write software, it can use tools, it can generate text, images\nand music. A huge new set of capabilities have been unlocked.\n\nThe last time I saw a shift this big in the technological landscape was\nprobably the original iPhone. Suddenly, everybody who was doing everything\nneeded to also do it for mobile devices. You sell real estate? Now you do it\nwith an app. You run a dating site? Now you do it with an app. There was a\ngigantic rush as everyone who did everything in software suddenly saw huge\nbenefits from moving onto a new platform.\n\nBut the most interesting applications that platform shift enabled were the\nones that couldn\u2019t have existed before. Uber relies on everyone \u2014 drivers as\nwell as customers \u2014 already owning a piece of mobile hardware that can connect\nto the Internet and use GPS to locate them. Prior to the mobile revolution,\nthat would have been an absurd business plan. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2035, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19a8b33e-e0e7-4055-8619-0616ae29e740": {"__data__": {"id_": "19a8b33e-e0e7-4055-8619-0616ae29e740", "embedding": null, "metadata": {"filename": "llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff.md", "extension": ".md", "title": "LlamaIndex + Laurie Voss: an alpaca joins the llamas", "date": "Oct 2, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92c32a37-38b7-47aa-bc5f-5b3b3abdc47e", "node_type": "4", "metadata": {"filename": "llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff.md", "extension": ".md", "title": "LlamaIndex + Laurie Voss: an alpaca joins the llamas", "date": "Oct 2, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff"}, "hash": "95746d4576ef2c3acaefa0ebd5fc63676b47e1933bbdc913dac49fe9b9d8f94f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d027f33-c6a8-44ac-8e7d-6c3b018395fd", "node_type": "1", "metadata": {"filename": "llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff.md", "extension": ".md", "title": "LlamaIndex + Laurie Voss: an alpaca joins the llamas", "date": "Oct 2, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff"}, "hash": "e937bec90e482b4682f1fda27647d8bd31075aeb8dee4c289e795c3232edd9ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b589868-02c2-4f87-88e7-866141f66d52", "node_type": "1", "metadata": {}, "hash": "53d8dcc07f5c1b914b09165e4c6ea880140870aba1a0cb8cd98a6b346d789b45", "class_name": "RelatedNodeInfo"}}, "text": "After it happened, it was taken\nfor granted. So in the same way, I\u2019m especially interested in seeing what the\napplications of LLMs are that were simply impossible before.\n\nAll of which is why I\u2019m delighted to be at a company at the center of the LLM\nuniverse. If you\u2019re new to LlamaIndex, it\u2019s a Python and JavaScript framework\nthat lets you quickly put together totally customizable, production-class\napplications that use LLMs. We let you ingest data from dozens of sources and\nuse any model you want, and we make it simple to link everything together.\n\nThe team at LlamaIndex is absolutely amazing and I am looking forward to\nworking with Jerry, Simon and Yi. Yi will be focusing more on partnerships.\n\n", "mimetype": "text/plain", "start_char_idx": 2035, "end_char_idx": 2741, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b589868-02c2-4f87-88e7-866141f66d52": {"__data__": {"id_": "6b589868-02c2-4f87-88e7-866141f66d52", "embedding": null, "metadata": {"filename": "llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff.md", "extension": ".md", "title": "LlamaIndex + Laurie Voss: an alpaca joins the llamas", "date": "Oct 2, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92c32a37-38b7-47aa-bc5f-5b3b3abdc47e", "node_type": "4", "metadata": {"filename": "llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff.md", "extension": ".md", "title": "LlamaIndex + Laurie Voss: an alpaca joins the llamas", "date": "Oct 2, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff"}, "hash": "95746d4576ef2c3acaefa0ebd5fc63676b47e1933bbdc913dac49fe9b9d8f94f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19a8b33e-e0e7-4055-8619-0616ae29e740", "node_type": "1", "metadata": {"filename": "llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff.md", "extension": ".md", "title": "LlamaIndex + Laurie Voss: an alpaca joins the llamas", "date": "Oct 2, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff"}, "hash": "75494f646d110e2b2ef8e643a89a330ee502d9e98f81af5641b3e1641f0d4bf8", "class_name": "RelatedNodeInfo"}}, "text": "Want to see a demo? Our [ SEC Insights ](https://www.secinsights.ai/) app lets\nyou ingest regulatory documents from major corporations and then ask questions\nabout them. Want to dive and build your own? Get started in [ Python\n](https://gpt-\nindex.readthedocs.io/en/latest/getting_started/installation.html) or [\nJavaScript ](https://ts.llamaindex.ai/) !\n\nP.S. If you\u2019re wondering \u201cwhy an alpaca?\u201d it\u2019s because my [ personal mascot\n](https://seldo.com/images/processed/floof-with-border.svg) is an alpaca and\nhas been for years. Alpacas and llamas are a great match!\n\n", "mimetype": "text/plain", "start_char_idx": 2741, "end_char_idx": 3309, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "224b5f28-ffa6-48f9-bb42-da5a5721d422": {"__data__": {"id_": "224b5f28-ffa6-48f9-bb42-da5a5721d422", "embedding": null, "metadata": {"filename": "timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.md", "extension": ".md", "title": "Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications", "date": "Sep 27, 2023", "url": "https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1bce5740-226b-40a0-8fea-8fb8455ea975", "node_type": "4", "metadata": {"filename": "timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.md", "extension": ".md", "title": "Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications", "date": "Sep 27, 2023", "url": "https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0"}, "hash": "3cbeb17257d6b045cdd71bcd517d510e3d408d64c6949e7163fc6a7e4aad699d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2ee521d-9790-4922-b86d-2e1f3d122dda", "node_type": "1", "metadata": {}, "hash": "3618cf876611c24381464790ef4a892244d4e7d16540e554254c1b202bc6616e", "class_name": "RelatedNodeInfo"}}, "text": "Authors: Avthar Sewrathan, Matvey Arye, Jerry Liu, Yi Ding\n\nIntroducing the [ Timescale Vector ](https://www.timescale.com/ai) integration\nfor LlamaIndex. Timescale Vector enables LlamaIndex developers to build better\nAI applications with PostgreSQL as their vector database: with faster vector\nsimilarity search, efficient time-based search filtering, and the operational\nsimplicity of a single, easy-to-use cloud PostgreSQL database for not only\nvector embeddings but an AI application\u2019s relational and time-series data too.\n\nPostgreSQL is the world\u2019s most loved database, according to the [ Stack\nOverflow 2023 Developer Survey\n](https://survey.stackoverflow.co/2023/#section-most-popular-technologies-\ndatabases) . And for a good reason: it\u2019s been battle-hardened by production\nuse for over three decades, it\u2019s robust and reliable, and it has a rich\necosystem of tools, drivers, and connectors.\n\nAnd while pgvector, the open-source extension for vector data on PostgreSQL,\nis a wonderful extension (and all its features are offered as part of\nTimescale Vector), it is just one piece of the puzzle in providing a\nproduction-grade experience for AI application developers on PostgreSQL. After\nspeaking with numerous developers at nimble startups and established industry\ngiants, we saw the need to enhance pgvector to cater to the performance and\noperational needs of developers building AI applications.\n\nHere\u2019s the TL;DR on how Timescale Vector helps you build better AI\napplications with LlamaIndex:\n\n  * **Faster similarity search on millions of vectors:** Thanks to the introduction of a new search index inspired by the DiskANN algorithm, [ Timescale Vector achieves 3X faster search speed at ~99% recall than a specialized database ](https://www.timescale.com/blog/how-we-made-postgresql-the-best-vector-database/?utm_campaign=vectorlaunch&utm_source=llamaindex&utm_medium=referral) and outperforms all existing PostgreSQL search indexes by between 39.39% and 1,590.33% on a dataset of one million OpenAI embeddings. Plus, enabling product quantization yields a [ 10x index space savings compared to pgvector ](https://www.timescale.com/blog/how-we-made-postgresql-the-best-vector-database/?utm_campaign=vectorlaunch&utm_source=llamaindex&utm_medium=referral) . Timescale Vector also offers pgvector\u2019s Hierarchical Navigable Small Worlds (HNSW) and Inverted File Flat (IVFFlat) indexing algorithms. \n  * **Efficient similarity search with time-based filtering:** Timescale Vector optimizes time-based vector search queries, leveraging the automatic time-based partitioning and indexing of [ Timescale\u2019s hypertables ](https://docs.timescale.com/use-timescale/latest/hypertables/) to efficiently find recent embeddings, constrain vector search by a time range or document age, and store and retrieve large language model (LLM) response and chat history with ease. Time-based semantic search also enables you to use Retrieval Augmented Generation (RAG) with time-based context retrieval to give users more useful LLM responses. \n  * **Simplified AI infra stack:** By combining vector embeddings, relational data, and time-series data in one PostgreSQL database, Timescale Vector eliminates the operational complexity that comes with managing multiple database systems at scale. \n  * **Simplified metadata handling and multi-attribute filtering:** Developers can leverage all PostgreSQL data types to store and filter metadata and JOIN vector search results with relational data for more contextually relevant responses. In future releases, Timescale Vector will further optimize rich multi-attribute filtering, enabling even faster similarity searches when filtering on metadata. \n\nOn top of these innovations for vector workloads, Timescale Vector provides a\nrobust, production-ready cloud PostgreSQL platform with flexible pricing,\nenterprise-grade security, and free expert support.\n\nIn the rest of this post, we\u2019ll dive deeper (with code!) into the unique\ncapabilities Timescale Vector enables for developers wanting to use PostgreSQL\nas their vector database with LlamaIndex:\n\n  * Faster similarity search with DiskANN, HNSW and IVFFlat index types. \n  * Efficient similarity search when filtering vectors by time. \n  * Retrieval Augmented Generation (RAG) with time-based context retrieval. \n\n(If you want to jump straight to the code, explore [ this tutorial\n](https://gpt-\nindex.readthedocs.io/en/stable/examples/vector_stores/Timescalevector.html) ).\n\n**** [ **LlamaIndex Users Get 3 Months of Timescale Vector for Free**\n](https://console.cloud.timescale.com/signup?utm_campaign=vectorlaunch&utm_source=llamaindex&utm_medium=referral)\n\nWe\u2019re giving LlamaIndex users an extended 90-day trial of Timescale Vector.\nThis makes it easy to test and develop your applications on Timescale Vector,\nas you won\u2019t be charged for any cloud PostgreSQL databases you spin up during\nyour trial period. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4903, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2ee521d-9790-4922-b86d-2e1f3d122dda": {"__data__": {"id_": "d2ee521d-9790-4922-b86d-2e1f3d122dda", "embedding": null, "metadata": {"filename": "timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.md", "extension": ".md", "title": "Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications", "date": "Sep 27, 2023", "url": "https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1bce5740-226b-40a0-8fea-8fb8455ea975", "node_type": "4", "metadata": {"filename": "timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.md", "extension": ".md", "title": "Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications", "date": "Sep 27, 2023", "url": "https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0"}, "hash": "3cbeb17257d6b045cdd71bcd517d510e3d408d64c6949e7163fc6a7e4aad699d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "224b5f28-ffa6-48f9-bb42-da5a5721d422", "node_type": "1", "metadata": {"filename": "timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.md", "extension": ".md", "title": "Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications", "date": "Sep 27, 2023", "url": "https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0"}, "hash": "36e45ec7503d5c1003be37e9aa11b302ea63bb806d2941147685b2b29a2e0504", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04f27749-44bf-4da4-931f-97f785159d4f", "node_type": "1", "metadata": {}, "hash": "a7c10fdd2173ceb51df3502fd5a41c324f5c66e675c5b037a6d842035562c271", "class_name": "RelatedNodeInfo"}}, "text": "[ Try Timescale Vector for free today\n](https://console.cloud.timescale.com/signup?utm_campaign=vectorlaunch&utm_source=llamaindex&utm_medium=referral)\n.\n\n#  Faster Vector Similarity Search in PostgreSQL\n\nTimescale Vector speeds up Approximate Nearest Neighbor (ANN) search on large-\nscale vector datasets, enhancing pgvector with a state-of-the-art ANN index\ninspired by the [ DiskANN ](https://www.microsoft.com/en-\nus/research/publication/diskann-fast-accurate-billion-point-nearest-neighbor-\nsearch-on-a-single-node/) algorithm. Timescale Vector also offers pgvector\u2019s\nHNSW and IVFFlat indexing algorithms, giving developers the flexibility to\nchoose the right index for their use case.\n\nOur performance benchmarks using the [ ANN benchmarks\n](https://github.com/erikbern/ann-benchmarks/) suite show that Timescale\nVector achieves between 39.43% and 1,590.33% faster search speed at ~99%\nrecall than all existing PostgreSQL search indexes, and 3X faster search speed\nat ~99% recall than specialized vector databases on a dataset of one million\nOpenAI embeddings. You can [ read more about the performance benchmark\nmethodology, the databases compared and results here\n](https://www.timescale.com/blog/how-we-made-postgresql-the-best-vector-\ndatabase/?utm_campaign=vectorlaunch&utm_source=llamaindex&utm_medium=referral)\n.\n\n_Timescale Vector\u2019s new DiskANN-inspired index outperforms all existing\nPostgreSQL index types when performing approximate nearest neighbor searches\nat 99 % recall on 1 million OpenAI embeddings._\n\nUsing Timescale Vector\u2019s DiskANN, HNSW, or IVFFLAT indexes in LlamaIndex is\nincredibly straightforward.\n\nSimply create a Timescale Vector vector store and add the [ data nodes\n](https://gpt-\nindex.readthedocs.io/en/latest/core_modules/data_modules/documents_and_nodes/usage_nodes.html)\nyou want to query as shown below:\n\n    \n    \n    from llama_index.vector_stores import TimescaleVectorStore\n    \n    # Create a timescale vector store with specified params\n    ts_vector_store = TimescaleVectorStore.from_params(\n       service_url=TIMESCALE_SERVICE_URL,\n       table_name=\"your_table_name\",\n       time_partition_interval= timedelta(days=7),\n    )\n    ts_vector_store.add(nodes)\n\nThen run:\n\n    \n    \n    # Create a timescale vector index (DiskANN)\n    ts_vector_store.create_index()\n\nThis will create a timescale-vector index with the default parameters.\n\nWe should point out that the term \u201cindex\u201d is a bit overloaded. For many\nVectorStores, an index is the thing that stores your data (in relational\ndatabases this is often called a table), but in the PostgreSQL world an index\nis something that speeds up search, and we are using the latter meaning here.\n\nWe can also specify the exact parameters for index creation in the `\ncreate_index ` command as follows:\n\n    \n    \n    # create new timescale vector index (DiskANN) with specified parameters\n    ts_vector_store.create_index(\"tsv\", max_alpha=1.0, num_neighbors=50)\n\nAdvantages to this Timescale Vector\u2019s new DiskANN-inspired vector search index\ninclude the following:\n\n  * Faster vector search at 99% accuracy in PostgreSQL. \n  * Optimized for running on disks, not only in memory use. \n  * Quantization optimization compatible with PostgreSQL, reducing the vector size and consequently shrinking the index size ( [ by 10x in some cases ](https://www.timescale.com/blog/how-we-made-postgresql-the-best-vector-database/?utm_campaign=vectorlaunch&utm_source=llamaindex&utm_medium=referral) !) and expediting searches. \n  * Efficient hybrid search or filtering additional dimensions. \n\nFor more on how Timescale Vector\u2019s new index works, [ see this blog post\n](https://www.timescale.com/blog/how-we-made-postgresql-the-best-vector-\ndatabase/?utm_campaign=vectorlaunch&utm_source=llamaindex&utm_medium=referral)\n.\n\nPgvector is packaged as part of Timescale Vector, so you can also access\npgvector\u2019s HNSW and IVFFLAT indexing algorithms in your LlamaIndex\napplications. The ability to conveniently create ANN search indexes from your\nLlamaIndex application code makes it easy to create different indexes and\ncompare their performance:\n\n    \n    \n    # Create an HNSW index\n    # Note: You don't need to specify m and ef_construction parameters as we set smart defaults.\n    ts_vector_store.create_index(\"hnsw\", m=16, ef_construction=64)\n    \n    # Create an IVFFLAT index\n    # Note: You don't need to specify num_lists and num_records parameters as we set smart defaults.\n    ts_vector_store.create_index(\"ivfflat\", num_lists=20, num_records=1000)\n\n#  Add Efficient Time-Based Search Functionality to Your LlamaIndex AI\nApplication\n\nTimescale Vector optimizes time-based vector search, leveraging the automatic\ntime-based partitioning and indexing of [ Timescale\u2019s hypertables\n](https://docs.timescale.com/use-timescale/latest/hypertables/) to efficiently\nsearch vectors by time and similarity.\n\nTime is often an important metadata component for vector embeddings. Sources\nof embeddings, like documents, images, and web pages, often have a timestamp\nassociated with them, for example, their creation date, publishing date, or\nthe date they were last updated, to name but a few.\n\nWe can take advantage of this time metadata in our collections of vector\nembeddings to enrich the quality and applicability of search results by\nretrieving vectors that are not just semantically similar but also pertinent\nto a specific time frame.\n\nHere are some examples where time-based retrieval of vectors can improve your\nLlamaIndex applications:\n\n  * **Finding recent embeddings:** Finding the most recent embeddings that are semantically similar to a query vector. For example, finding the most recent news, documents, or social media posts related to elections. \n  * **Search within a time-range:** Constraining similarity search to only vectors within a relevant time range. For example, asking time-based questions about a knowledge base (\u201cWhat new features were added between January and March 2023?\u201d). \n  * **Chat history:** Storing and retrieving LLM response history. For example, chatbot chat history. \n\nLet\u2019s take a look at an example of performing time-based searches on a [ git\nlog dataset\n](https://chat.openai.com/share/0612295b-a408-4e02-9ac2-3dc231fa89d1) . In a\ngit log, each entry has a timestamp, an author, and some information about the\ncommit.\n\nTo illustrate how to use TimescaleVector\u2019s time-based vector search\nfunctionality, we\u2019ll ask questions about the git log history for TimescaleDB.\nEach git commit entry has a timestamp associated with it, as well as a message\nand other metadata (e.g., author).\n\nWe\u2019ll illustrate how to create nodes with a time-based UUID and how to run\nsimilarity searches with time range filters using the Timescale Vector vector\nstore..\n\n#  Create nodes from each commit in the gitlog\n\nFirst, we load the git log entries from the [ demo CSV file\n](https://s3.amazonaws.com/assets.timescale.com/ai/commit_history.csv) using\nPandas:\n\n    \n    \n    import pandas as pd\n    from pathlib import Path\n    \n    \n    # Read the CSV file into a DataFrame\n    file_path = Path(\"../data/csv/commit_history.csv\")\n    df = pd.read_csv(file_path)\n\nNext, we\u2019ll create nodes of type `TextNode` for each commit in our git log\ndataset, extracting the relevant information and assigning it to the node\u2019s\ntext and metadata, respectively.\n\n    \n    \n    from llama_index.schema import TextNode, NodeRelationship, RelatedNodeInfo\n    # Create a Node object from a single row of data\n    def create_node(row):\n       record = row.to_dict()\n       record_name = split_name(record[\"author\"])\n       record_content = str(record[\"date\"]) + \" \" + record_name + \" \" + str(record[\"change summary\"]) + \" \" + str(record[\"change details\"])\n       node = TextNode(\n           id_=create_uuid(record[\"date\"]),\n           text= record_content,\n           metadata={\n               'commit': record[\"commit\"],\n               'author': record_name,\n               'date': create_date(record[\"date\"]),\n           }\n       )\n       return node\n    \n    nodes = [create_node(row) for _, row in df.iterrows()]\n\n**Note:** The code above references two helper functions to get things in the\nright format (`split_name()` and `create_date()`), which we\u2019ve omitted for\nbrevity. The full code is included in the tutorial linked in the Resources\nsection at the end of this post.\n\n", "mimetype": "text/plain", "start_char_idx": 4903, "end_char_idx": 13253, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04f27749-44bf-4da4-931f-97f785159d4f": {"__data__": {"id_": "04f27749-44bf-4da4-931f-97f785159d4f", "embedding": null, "metadata": {"filename": "timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.md", "extension": ".md", "title": "Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications", "date": "Sep 27, 2023", "url": "https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1bce5740-226b-40a0-8fea-8fb8455ea975", "node_type": "4", "metadata": {"filename": "timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.md", "extension": ".md", "title": "Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications", "date": "Sep 27, 2023", "url": "https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0"}, "hash": "3cbeb17257d6b045cdd71bcd517d510e3d408d64c6949e7163fc6a7e4aad699d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2ee521d-9790-4922-b86d-2e1f3d122dda", "node_type": "1", "metadata": {"filename": "timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.md", "extension": ".md", "title": "Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications", "date": "Sep 27, 2023", "url": "https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0"}, "hash": "9ad1a8c635f7970f5b40dc873fb64d333240c510c8e7eb8548985c45d9e29dd9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff83150b-a794-4297-a9bd-5d149494424f", "node_type": "1", "metadata": {}, "hash": "5727fa9273d8d153f142790f4495612ee763a6ff0127f48e653b4994f9362795", "class_name": "RelatedNodeInfo"}}, "text": "#  Create UUIDs for each node based on the date of each git commit\n\nWe will take a closer look at a helper function we use to create each node\u2019s `\nid_ ` . For time-based search in LlamaIndex, Timescale Vector uses the\n\u2018datetime\u2019 portion of a UUID v1 to place vectors in the correct time\npartition. [ Timescale Vector\u2019s Python client library\n](https://github.com/timescale/python-vector) provides a simple-to-use\nfunction named `uuid_from_time` to create a UUID v1 from a Python DateTime\nobject, which we\u2019ll then use as our `ids` for the TextNodes.\n\n    \n    \n    from timescale_vector import client\n    # Function to take in a date string in the past and return a uuid v1\n    def create_uuid(date_string: str):\n       if date_string is None:\n           return None\n       time_format = '%a %b %d %H:%M:%S %Y %z'\n       datetime_obj = datetime.strptime(date_string, time_format)\n       uuid = client.uuid_from_time(datetime_obj)\n       return str(uuid)\n\nSince we are dealing with timestamps in the past, we take advantage of the\n`uuid_from_time` function to help generate the correct UUIDs for each node. If\nyou want the current date and time associated with your Nodes (or Documents)\nfor time-based search, you can skip this step. A UUID associated with the\ncurrent date and time will be automatically generated as the nodes are added\nto the table in Timescale Vector by default.\n\nLet\u2019s take a look at the contents of a node:\n\n    \n    \n    print(nodes[0].get_content(metadata_mode=\"all\"))\n    \n    \n    commit: 44e41c12ab25e36c202f58e068ced262eadc8d16\n    author: Lakshmi Narayanan Sreethar\n    date: 2023-09-5 21:03:21+0850\n    \n    Tue Sep 5 21:03:21 2023 +0530 Lakshmi Narayanan Sreethar Fix segfault in set_integer_now_func When an invalid function oid is passed to set_integer_now_func, it finds out that the function oid is invalid but before throwing the error, it calls ReleaseSysCache on an invalid tuple causing a segfault. Fixed that by removing the invalid call to ReleaseSysCache.  ", "mimetype": "text/plain", "start_char_idx": 13253, "end_char_idx": 15249, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff83150b-a794-4297-a9bd-5d149494424f": {"__data__": {"id_": "ff83150b-a794-4297-a9bd-5d149494424f", "embedding": null, "metadata": {"filename": "timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.md", "extension": ".md", "title": "Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications", "date": "Sep 27, 2023", "url": "https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1bce5740-226b-40a0-8fea-8fb8455ea975", "node_type": "4", "metadata": {"filename": "timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.md", "extension": ".md", "title": "Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications", "date": "Sep 27, 2023", "url": "https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0"}, "hash": "3cbeb17257d6b045cdd71bcd517d510e3d408d64c6949e7163fc6a7e4aad699d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04f27749-44bf-4da4-931f-97f785159d4f", "node_type": "1", "metadata": {"filename": "timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.md", "extension": ".md", "title": "Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications", "date": "Sep 27, 2023", "url": "https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0"}, "hash": "c2740538d457bc328669272bf890ee3bc8029d85ac7db12622f734ec0c2b09d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80f4fc6a-c426-4c80-9b00-9a8b2dac6e78", "node_type": "1", "metadata": {}, "hash": "a53ab4541cd0274d3130da34d7e4a8974063bdea16b56356c4a241009e20c69f", "class_name": "RelatedNodeInfo"}}, "text": "Fixes #6037\n\n#  Create vector embeddings for the text of each node\n\nNext, we\u2019ll create vector embeddings of the content of each node so that we\ncan perform similarity searches on the text associated with each node. We\u2019ll\nuse the `OpenAIEmbedding` model to create the embeddings.\n\n    \n    \n    # Create embeddings for nodes\n    from llama_index.embeddings import OpenAIEmbedding\n    embedding_model = OpenAIEmbedding()\n    \n    for node in nodes:\n       node_embedding = embedding_model.get_text_embedding(\n           node.get_content(metadata_mode=\"all\")\n       )\n       node.embedding = node_embedding\n\n#  Load nodes into Timescale Vector vector store\n\nNext, we\u2019ll create a `TimescaleVectorStore` instance and add the nodes we\ncreated to it.\n\n    \n    \n    # Create a timescale vector store and add the newly created nodes to it\n    ts_vector_store = TimescaleVectorStore.from_params(\n       service_url=TIMESCALE_SERVICE_URL,\n       table_name=\"li_commit_history\",\n       time_partition_interval= timedelta(days=7),\n    )\n    ts_vector_store.add(nodes)\n\nTo take advantage of Timescale Vector\u2019s efficient time-based search, we need\nto specify the `time_partition_interval` argument when instantiating a\nTimescale Vector vector store. This argument represents the length of each\ninterval for partitioning the data by time. ", "mimetype": "text/plain", "start_char_idx": 15249, "end_char_idx": 16573, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80f4fc6a-c426-4c80-9b00-9a8b2dac6e78": {"__data__": {"id_": "80f4fc6a-c426-4c80-9b00-9a8b2dac6e78", "embedding": null, "metadata": {"filename": "timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.md", "extension": ".md", "title": "Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications", "date": "Sep 27, 2023", "url": "https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1bce5740-226b-40a0-8fea-8fb8455ea975", "node_type": "4", "metadata": {"filename": "timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.md", "extension": ".md", "title": "Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications", "date": "Sep 27, 2023", "url": "https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0"}, "hash": "3cbeb17257d6b045cdd71bcd517d510e3d408d64c6949e7163fc6a7e4aad699d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff83150b-a794-4297-a9bd-5d149494424f", "node_type": "1", "metadata": {"filename": "timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.md", "extension": ".md", "title": "Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications", "date": "Sep 27, 2023", "url": "https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0"}, "hash": "faa227cda94cc156b1b6879c04d87c39378dd699f6cf68ae7318c6c2bbfb41f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "032e89bb-be00-474a-861c-fbd9d2648674", "node_type": "1", "metadata": {}, "hash": "ac779f7b15105bc0606b2a696bd5907810d459c4bb2c91dd4d81737ae4d73384", "class_name": "RelatedNodeInfo"}}, "text": "Each partition will consist of\ndata that falls within the specified length of time.\n\nIn the example above, we use seven days for simplicity, but you can pick\nwhatever value makes sense for the queries used by your application \u2014 for\nexample, if you query recent vectors frequently, you might want to use a\nsmaller time delta like one day, or if you query vectors over a decade-long\ntime period, then you might want to use a larger time delta like six months or\none year. As a rule of thumb, common queries should touch only a couple of\npartitions and at the same time your full dataset should fit within a 1000\npartitions, but don\u2019t stress too much \u2014 the system is not very sensitive to\nthis value.\n\n#  Similarity search with time filters\n\nNow that we\u2019ve loaded our nodes that contain vector embeddings data and\nmetadata into a Timescale Vector vector store, and enabled automatic time-\nbased partitioning on the table our vectors and metadata are stored in, we can\nquery our vector store with time-based filters as follows:\n\n    \n    \n    # Query the vector database\n    vector_store_query = VectorStoreQuery(query_embedding = query_embedding, similarity_top_k=5)\n    \n    # Time filter variables for query\n    start_dt = datetime(2023, 8, 1, 22, 10, 35) # Start date = 1 August 2023, 22:10:35\n    end_dt = datetime(2023, 8, 30, 22, 10, 35) # End date = 30 August 2023, 22:10:35\n    \n    # return most similar vectors to query between start date and end date date range\n    # returns a VectorStoreQueryResult object\n    query_result = ts_vector_store.query(vector_store_query, start_date = start_dt, end_date = end_dt)\n\nLet\u2019s take a look at the date and contents of the nodes returned by our query:\n\n    \n    \n    # for each node in the query result, print the node metadata date\n    for node in query_result.nodes:\n       print(\"-\" * 80)\n       print(node.metadata[\"date\"])\n       print(node.get_content(metadata_mode=\"all\"))\n    \n    \n    --------------------------------------------------------------------------------\n    2023-08-3 14:30:23+0500\n    commit:  7aeed663b9c0f337b530fd6cad47704a51a9b2ec\n    author: Dmitry Simonenko\n    date: 2023-08-3 14:30:23+0500\n    \n    Thu Aug 3 14:30:23 2023 +0300 Dmitry Simonenko Feature flags for TimescaleDB features This PR adds..\n    --------------------------------------------------------------------------------\n    2023-08-29 18:13:24+0320\n    commit:  e4facda540286b0affba47ccc63959fefe2a7b26\n    author: Sven Klemm\n    date: 2023-08-29 18:13:24+0320\n    \n    Tue Aug 29 18:13:24 2023 +0200 Sven Klemm Add compatibility layer for _timescaledb_internal functions With timescaledb 2.12 all the functions present in _timescaledb_internal were\u2026\n    --------------------------------------------------------------------------------\n    2023-08-22 12:01:19+0320\n    commit:  cf04496e4b4237440274eb25e4e02472fc4e06fc\n    author: Sven Klemm\n    date: 2023-08-22 12:01:19+0320\n    \n    Tue Aug 22 12:01:19 2023 +0200 Sven Klemm Move utility functions to _timescaledb_functions schema To increase schema security we do not want to mix\u2026\n    --------------------------------------------------------------------------------\n    2023-08-29 10:49:47+0320\n    commit:  a9751ccd5eb030026d7b975d22753f5964972389\n    author: Sven Klemm\n    date: 2023-08-29 10:49:47+0320\n    \n    Tue Aug 29 10:49:47 2023 +0200 Sven Klemm Move partitioning functions to _timescaledb_functions schema To increase schema security\u2026\n    --------------------------------------------------------------------------------\n    2023-08-9 15:26:03+0500\n    commit:  44eab9cf9bef34274c88efd37a750eaa74cd8044\n    author: Konstantina Skovola\n    date: 2023-08-9 15:26:03+0500\n    \n    Wed Aug 9 15:26:03 2023 +0300 Konstantina Skovola Release 2.11.2 This release contains bug fixes since the 2.11.1 release\u2026\n\nSuccess! Notice how only vectors with timestamps within the specified start\nand end date ranges of 1 August, 2023, and 30 August, 2023, are included in\nthe results.\n\nHere\u2019s some intuition for why Timescale Vector\u2019s time-based partitioning\nspeeds up ANN queries with time-based filters.\n\nTimescale Vector partitions the data by time and creates ANN indexes on each\npartition individually. Then, during search, we perform a three-step process:\n\n  * Step 1: filter our partitions that don\u2019t match the time predicate. \n  * Step 2: perform the similarity search on all matching partitions. \n  * Step 3: combine all the results from each partition in step 2, rerank, and filter out results by time. \n\nTimescale Vector leverages [ TimescaleDB\u2019s hypertables\n](https://docs.timescale.com/use-timescale/latest/hypertables/about-\nhypertables/) , which automatically partition vectors and associated metadata\nby a timestamp. This enables efficient querying on vectors by both similarity\nto a query vector and time, as partitions not in the time window of the query\nare ignored, making the search a lot more efficient by filtering out whole\nswaths of data in one go.\n\nWhen performing a vector similarity search on `TimescaleVectorStore`, rather\nthan specifying the start and end dates for our search, we can also specify a\ntime filter with a provided start date and time delta later:\n\n    \n    \n    # return most similar vectors to query from start date and a time delta later\n    query_result = ts_vector_store.query(vector_store_query, start_date = start_dt, time_delta = td)\n\nAnd we can specify a time filter within a provided end_date and time delta\nearlier. This syntax is very useful for filtering your search results to\ncontain vectors before a certain date cutoff.\n\n    \n    \n    # return most similar vectors to query from end date and a time delta earlier\n    query_result = ts_vector_store.query(vector_store_query, end_date = end_dt, time_delta = td)\n\n#  Powering Retrieval Augmented Generation With Time-Based Context Retrieval\nin LlamaIndex Applications With Timescale Vector\n\nLet\u2019s put everything together and look at how to use the TimescaleVectorStore\nto power RAG on the git log dataset we examined above.\n\nTo do this, we can use the TimescaleVectorStore as a [ QueryEngine\n](https://gpt-\nindex.readthedocs.io/en/latest/api_reference/query/query_engines.html) . When\ncreating the query engine, we use TimescaleVector\u2019s time filters to constrain\nthe search to a relevant time range by passing our time filter parameters as\n`vector_strore_kwargs`.\n\n    \n    \n    from llama_index import VectorStoreIndex\n    from llama_index.storage import StorageContext\n    \n    index = VectorStoreIndex.from_vector_store(ts_vector_store)\n    query_engine = index.as_query_engine(vector_store_kwargs = ({\"start_date\": start_dt, \"end_date\":end_dt}))\n    \n    query_str = \"What's new with TimescaleDB functions? When were these changes made and by whom?\"\n    response = query_engine.query(query_str)\n    print(str(response))\n\nWe asked the LLM a question about our gitlog, namely, \u201cWhat\u2019s new with\nTimescaleDB functions. When were these changes made and by whom?\u201d\n\nHere\u2019s the response we get, which synthesizes the nodes returned from semantic\nsearch with time-based filtering on the Timescale VectorStore:\n\n    \n    \n    TimescaleDB functions have undergone changes recently. These changes include the addition of several GUCs (Global User Configuration) that allow for enabling or disabling major TimescaleDB features. Additionally, a compatibility layer has been added for the \"_timescaledb_internal\" functions, which were moved into the \"_timescaledb_functions\" schema to enhance schema security. These changes were made by Dmitry Simonenko and Sven Klemm. ", "mimetype": "text/plain", "start_char_idx": 16573, "end_char_idx": 24129, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "032e89bb-be00-474a-861c-fbd9d2648674": {"__data__": {"id_": "032e89bb-be00-474a-861c-fbd9d2648674", "embedding": null, "metadata": {"filename": "timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.md", "extension": ".md", "title": "Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications", "date": "Sep 27, 2023", "url": "https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1bce5740-226b-40a0-8fea-8fb8455ea975", "node_type": "4", "metadata": {"filename": "timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.md", "extension": ".md", "title": "Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications", "date": "Sep 27, 2023", "url": "https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0"}, "hash": "3cbeb17257d6b045cdd71bcd517d510e3d408d64c6949e7163fc6a7e4aad699d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80f4fc6a-c426-4c80-9b00-9a8b2dac6e78", "node_type": "1", "metadata": {"filename": "timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.md", "extension": ".md", "title": "Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications", "date": "Sep 27, 2023", "url": "https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0"}, "hash": "334fa3d3795b168ecf69a22c3458699a987d3916309b47ef07fbe8cbdad215f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62f3aebd-f97f-4b2f-9b3f-86fb3c83978c", "node_type": "1", "metadata": {}, "hash": "4d90b8e01fcfca5dcd1162999ee79cc44846929a43c6b7c86739be0a93889dba", "class_name": "RelatedNodeInfo"}}, "text": "The specific dates of these changes are August 3, 2023, and August 29, 2023, respectively.\n\nThis is a simple example of a powerful concept \u2014 using time-based context\nretrieval in your RAG applications can help provide more relevant answers to\nyour users. ", "mimetype": "text/plain", "start_char_idx": 24129, "end_char_idx": 24384, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62f3aebd-f97f-4b2f-9b3f-86fb3c83978c": {"__data__": {"id_": "62f3aebd-f97f-4b2f-9b3f-86fb3c83978c", "embedding": null, "metadata": {"filename": "timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.md", "extension": ".md", "title": "Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications", "date": "Sep 27, 2023", "url": "https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1bce5740-226b-40a0-8fea-8fb8455ea975", "node_type": "4", "metadata": {"filename": "timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.md", "extension": ".md", "title": "Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications", "date": "Sep 27, 2023", "url": "https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0"}, "hash": "3cbeb17257d6b045cdd71bcd517d510e3d408d64c6949e7163fc6a7e4aad699d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "032e89bb-be00-474a-861c-fbd9d2648674", "node_type": "1", "metadata": {"filename": "timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.md", "extension": ".md", "title": "Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications", "date": "Sep 27, 2023", "url": "https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0"}, "hash": "b30d9a3bfa3a333512a2ab6aebb8ac56c64502107d9633848b13ff687403da76", "class_name": "RelatedNodeInfo"}}, "text": "This time-based context retrieval can be helpful to any dataset\nwith a natural language and time component. Timescale Vector uniquely enables\nthis thanks to its efficient time-based similarity search capabilities, and\ntaking advantage of it in your LlamaIndex application is easy thanks to the\nTimescale Vector integration.\n\n#  Resources and next steps\n\nNow that you\u2019ve learned how Timescale Vector can help you power better AI\napplications with PostgreSQL, it\u2019s your turn to dive in. Take the next step in\nyour learning journey by following one of the tutorials or reading one of the\nblog posts in the resource set below:\n\n  * [ **Up and Running Tutorial** ](https://gpt-index.readthedocs.io/en/stable/examples/vector_stores/Timescalevector.html) **:** learn how to use Timescale Vector in LlamaIndex using a real-world dataset. You\u2019ll learn how to use Timescale Vector as a Vectorstore, Retriever, and QueryEngine and perform time-based similarity search on vectors. \n  * [ **Timescale Vector explainer** ](https://www.timescale.com/blog/how-we-made-postgresql-the-best-vector-database/?utm_campaign=vectorlaunch&utm_source=llamaindex&utm_medium=referral) : learn more about the internals of Timescale Vector. \n  * [ **Timescale Vector website** ](https://www.timescale.com/ai) **:** learn more about Timescale Vector and Timescale\u2019s AI Launch Week. \n\n**And a reminder:** [ **LlamaIndex Users get Timescale Vector free for 90\ndays**\n](https://console.cloud.timescale.com/signup?utm_campaign=vectorlaunch&utm_source=llamaindex&utm_medium=referral)\n\nWe\u2019re giving LlamaIndex users an extended 90-day trial of Timescale Vector.\nThis makes it easy to test and develop your applications on Timescale Vector,\nas you won\u2019t be charged for any cloud PostgreSQL databases you spin up during\nyour trial period. [ Try Timescale Vector for free today\n](https://console.cloud.timescale.com/signup?utm_campaign=vectorlaunch&utm_source=llamaindex&utm_medium=referral)\n.\n\n", "mimetype": "text/plain", "start_char_idx": 24384, "end_char_idx": 26340, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "673f9672-867b-4fc9-be46-3d59b6527fa4": {"__data__": {"id_": "673f9672-867b-4fc9-be46-3d59b6527fa4", "embedding": null, "metadata": {"filename": "llamaindex-update-20-09-2023-86ed66f78bac.md", "extension": ".md", "title": "LlamaIndex Update \u2014 20/09/2023", "date": "Sep 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-20-09-2023-86ed66f78bac"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c03101fd-c33e-428e-98cc-850dc59baa6f", "node_type": "4", "metadata": {"filename": "llamaindex-update-20-09-2023-86ed66f78bac.md", "extension": ".md", "title": "LlamaIndex Update \u2014 20/09/2023", "date": "Sep 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-20-09-2023-86ed66f78bac"}, "hash": "0aa021b7c246bdd4ad7b22a9c8130d2bcd85d918140a9fa64d3d621c6ad61269", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cfa0f7cc-794e-42bd-8df1-e591cbd48a3f", "node_type": "1", "metadata": {}, "hash": "b3b41c48cf2da6a61070c55f4e4a555cb2423a6520798ca3e511230eb869618a", "class_name": "RelatedNodeInfo"}}, "text": "**Hello LlamaIndex Enthusiasts!**\n\nWelcome to the fifth edition of our LlamaIndex Update series.\n\n##  **Most Important Takeaways:**\n\n  1. We\u2019ve open-sourced [ **SECInsights.ai** ](http://secinsights.ai/) \u2014 your gateway to the production RAG framework. \n  2. Replit templates \u2014 kickstart your projects with zero environment setup hassles. \n  3. Build RAG from scratch and get hands-on with our processes. \n\nBut wait, there\u2019s more!\n\n  * Feature Releases and Enhancements \n  * Fine-Tuning Guides \n  * Retrieval Tips for RAG \n  * Building RAG from Scratch Guides \n  * Tutorials \n  * Integration with External Platforms \n  * Events \n  * Webinars \n\nSo, let\u2019s embark on this journey together. Dive in and explore the offerings\nof the fifth edition of the LlamaIndex Update series!\n\n#  **Feature Releases and Enhancements**\n\n  1. **Open-Sourced RAG Platform** : LlamaIndex open-sourced [ **http://secinsights.ai** ](http://secinsights.ai) , accelerating RAG app development with chat-based Q&A features. [ Tweet ](https://twitter.com/llama_index/status/1699116440056651976?s=20)\n  2. **Linear Adapter Fine-Tuning** : LlamaIndex enables efficient fine-tuning of linear adapters on any embedding without re-embedding, enhancing retrieval/RAG across various models. [ Tweet ](https://twitter.com/llama_index/status/1699566421506671043?s=20) , [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding_adapter.html) , [ BlogPost ](https://medium.com/llamaindex-blog/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383)\n  3. **Hierarchical Agents** : By structuring LLM agents in a parent-child hierarchy, we enhance complex search and retrieval tasks across diverse data, offering more reliability than a standalone agent. [ Tweet ](https://twitter.com/llama_index/status/1699929022027718729?s=20)\n  4. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1849, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cfa0f7cc-794e-42bd-8df1-e591cbd48a3f": {"__data__": {"id_": "cfa0f7cc-794e-42bd-8df1-e591cbd48a3f", "embedding": null, "metadata": {"filename": "llamaindex-update-20-09-2023-86ed66f78bac.md", "extension": ".md", "title": "LlamaIndex Update \u2014 20/09/2023", "date": "Sep 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-20-09-2023-86ed66f78bac"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c03101fd-c33e-428e-98cc-850dc59baa6f", "node_type": "4", "metadata": {"filename": "llamaindex-update-20-09-2023-86ed66f78bac.md", "extension": ".md", "title": "LlamaIndex Update \u2014 20/09/2023", "date": "Sep 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-20-09-2023-86ed66f78bac"}, "hash": "0aa021b7c246bdd4ad7b22a9c8130d2bcd85d918140a9fa64d3d621c6ad61269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "673f9672-867b-4fc9-be46-3d59b6527fa4", "node_type": "1", "metadata": {"filename": "llamaindex-update-20-09-2023-86ed66f78bac.md", "extension": ".md", "title": "LlamaIndex Update \u2014 20/09/2023", "date": "Sep 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-20-09-2023-86ed66f78bac"}, "hash": "5ca7269443217a698daec65c9ed1770142d18bc2917195401cc2fd642269a931", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f21144d7-2e27-400e-9214-6c0e6d00803f", "node_type": "1", "metadata": {}, "hash": "d7ebd48bd5e4a47e508801a2e7971db02d6c8118cf29a4fdb462fdce562a228c", "class_name": "RelatedNodeInfo"}}, "text": "**SummaryIndex** : We\u2019ve renamed ListIndex to SummaryIndex to make it clearer what its main functionality is. Backward compatibility is maintained for existing code using ListIndex. [ Tweet ](https://twitter.com/llama_index/status/1698728395948081247?s=20)\n  5. **Evaluation:** LlamaIndex\u2019s new RAG evaluation toolkit offers async capabilities, diverse assessment criteria, and a centralized BaseEvaluator for easier developer integrations. [ Tweet ](https://x.com/llama_index/status/1703074307763818775?s=20) , [ Docs ](https://gpt-index.readthedocs.io/en/latest/core_modules/supporting_modules/evaluation/modules.html) . \n  ", "mimetype": "text/plain", "start_char_idx": 1849, "end_char_idx": 2475, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f21144d7-2e27-400e-9214-6c0e6d00803f": {"__data__": {"id_": "f21144d7-2e27-400e-9214-6c0e6d00803f", "embedding": null, "metadata": {"filename": "llamaindex-update-20-09-2023-86ed66f78bac.md", "extension": ".md", "title": "LlamaIndex Update \u2014 20/09/2023", "date": "Sep 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-20-09-2023-86ed66f78bac"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c03101fd-c33e-428e-98cc-850dc59baa6f", "node_type": "4", "metadata": {"filename": "llamaindex-update-20-09-2023-86ed66f78bac.md", "extension": ".md", "title": "LlamaIndex Update \u2014 20/09/2023", "date": "Sep 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-20-09-2023-86ed66f78bac"}, "hash": "0aa021b7c246bdd4ad7b22a9c8130d2bcd85d918140a9fa64d3d621c6ad61269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cfa0f7cc-794e-42bd-8df1-e591cbd48a3f", "node_type": "1", "metadata": {"filename": "llamaindex-update-20-09-2023-86ed66f78bac.md", "extension": ".md", "title": "LlamaIndex Update \u2014 20/09/2023", "date": "Sep 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-20-09-2023-86ed66f78bac"}, "hash": "78e63d785fb6b4ef217e4038aa39890a8e6ca3bc4ea0e81f1cc36c578559a55c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "590e9080-8837-454a-89ab-eaadd605821d", "node_type": "1", "metadata": {}, "hash": "f52ae22a6d73fa2be67668bc1c84063405dc9201bb905292c0c0ce9748f98962", "class_name": "RelatedNodeInfo"}}, "text": "6. **Hybrid Search for Postgres/pgvector** : LlamaIndex introduces a hybrid search for Postgres/pgvector. [ Tweet ](https://twitter.com/llama_index/status/1700892425592696915?s=20) , [ Docs ](https://gpt-index.readthedocs.io/en/stable/examples/vector_stores/postgres.html#hybrid-search) . \n  7. **Replit Templates:** LlamaIndex partners with Replit for easy LLM app templates, including ready-to-use Streamlit apps and full Typescript templates. [ Tweet ](https://x.com/llama_index/status/1702847763183235278?s=20) , [ Replit Templates ](https://replit.com/@LlamaIndex) . \n\n##  **LlamaIndex.TS:**\n\n  1. Launches with MongoDBReader and type-safe metadata. [ Tweet ](https://x.com/llama_index/status/1702382520631959721?s=20) . \n  2. Launches with chat history, enhanced keyword index, and Notion DB support. [ Tweet ](https://twitter.com/llama_index/status/1701292211764338898?s=20) . \n\n#  Fine-Tuning Guides:\n\n  1. **OpenAI Fine-Tuning:** LlamaIndex unveils a fresh guide on harnessing OpenAI fine-tuning to embed knowledge from any text corpus. In short: generate QA pairs with GPT-4, format them into a training dataset, and proceed to fine-tuning. [ Tweet ](https://twitter.com/llama_index/status/1701264116311322937?s=20) , [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/finetuning/knowledge/finetune_knowledge.html) . \n  ", "mimetype": "text/plain", "start_char_idx": 2475, "end_char_idx": 3815, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "590e9080-8837-454a-89ab-eaadd605821d": {"__data__": {"id_": "590e9080-8837-454a-89ab-eaadd605821d", "embedding": null, "metadata": {"filename": "llamaindex-update-20-09-2023-86ed66f78bac.md", "extension": ".md", "title": "LlamaIndex Update \u2014 20/09/2023", "date": "Sep 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-20-09-2023-86ed66f78bac"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c03101fd-c33e-428e-98cc-850dc59baa6f", "node_type": "4", "metadata": {"filename": "llamaindex-update-20-09-2023-86ed66f78bac.md", "extension": ".md", "title": "LlamaIndex Update \u2014 20/09/2023", "date": "Sep 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-20-09-2023-86ed66f78bac"}, "hash": "0aa021b7c246bdd4ad7b22a9c8130d2bcd85d918140a9fa64d3d621c6ad61269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f21144d7-2e27-400e-9214-6c0e6d00803f", "node_type": "1", "metadata": {"filename": "llamaindex-update-20-09-2023-86ed66f78bac.md", "extension": ".md", "title": "LlamaIndex Update \u2014 20/09/2023", "date": "Sep 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-20-09-2023-86ed66f78bac"}, "hash": "9ad8ef5875431589c8313db0749014904ddda24cc206bae96f81892de4853c73", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6cb8cf0-23e9-4d62-bef3-91f26346153d", "node_type": "1", "metadata": {}, "hash": "f5fd4ef8e534bdc81f1abd45727e5abc4b604c7567df8e2bf7df87ee0d235304", "class_name": "RelatedNodeInfo"}}, "text": "2. **Embedding Fine-Tuning:** LlamaIndex has a more advanced embedding fine-tuning feature, enabling complex NN query transformations on any embedding, including custom ones, and offering the ability to save intermediate checkpoints for enhanced model control. [ Tweet ](https://twitter.com/llama_index/status/1701983207946965285?s=20) , [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding_adapter.html) . \n\n#  Retrieval Tips For RAG:\n\n  * Use references (smaller chunks or summaries) instead of embedding full text. \n  * Results in 10\u201320 % improvement. \n  ", "mimetype": "text/plain", "start_char_idx": 3815, "end_char_idx": 4422, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6cb8cf0-23e9-4d62-bef3-91f26346153d": {"__data__": {"id_": "e6cb8cf0-23e9-4d62-bef3-91f26346153d", "embedding": null, "metadata": {"filename": "llamaindex-update-20-09-2023-86ed66f78bac.md", "extension": ".md", "title": "LlamaIndex Update \u2014 20/09/2023", "date": "Sep 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-20-09-2023-86ed66f78bac"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c03101fd-c33e-428e-98cc-850dc59baa6f", "node_type": "4", "metadata": {"filename": "llamaindex-update-20-09-2023-86ed66f78bac.md", "extension": ".md", "title": "LlamaIndex Update \u2014 20/09/2023", "date": "Sep 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-20-09-2023-86ed66f78bac"}, "hash": "0aa021b7c246bdd4ad7b22a9c8130d2bcd85d918140a9fa64d3d621c6ad61269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "590e9080-8837-454a-89ab-eaadd605821d", "node_type": "1", "metadata": {"filename": "llamaindex-update-20-09-2023-86ed66f78bac.md", "extension": ".md", "title": "LlamaIndex Update \u2014 20/09/2023", "date": "Sep 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-20-09-2023-86ed66f78bac"}, "hash": "b6b36742205ad2006f0ed554184d4503b2185f3ee6ab9a96b1524456b81a9de0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0260805-3129-406f-8391-abe3f258a4b2", "node_type": "1", "metadata": {}, "hash": "1f4376c9ac73d254b2743e6fbb218ebbc82dc4cfcbd5a455052ad05f978c981e", "class_name": "RelatedNodeInfo"}}, "text": "* Embeddings decoupled from main text chunks. \n  * Smaller references allow efficient LLM synthesis. \n  * Deduplication applied for repetitive references. \n  * Evaluated using synthetic dataset; 20\u201325% MRR boost. \n\n[ Tweet ](https://twitter.com/jerryjliu0/status/1698727872721285282?s=20)\n\n#  Building RAG from Scratch Guides:\n\n  1. Build Data Ingestion from scratch. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/low_level/ingestion.html) . \n  2. Build Retrieval from scratch. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/low_level/retrieval.html) . \n  3. Build Vector Store from scratch. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/low_level/vector_store.html) . \n  4. Build Response Synthesis from scratch. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/low_level/response_synthesis.html) . \n  5. Build Router from scratch. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/low_level/router.html) . \n  6. Build Evaluation from scratch. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/low_level/evaluation.html) . \n\n", "mimetype": "text/plain", "start_char_idx": 4422, "end_char_idx": 5526, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0260805-3129-406f-8391-abe3f258a4b2": {"__data__": {"id_": "f0260805-3129-406f-8391-abe3f258a4b2", "embedding": null, "metadata": {"filename": "llamaindex-update-20-09-2023-86ed66f78bac.md", "extension": ".md", "title": "LlamaIndex Update \u2014 20/09/2023", "date": "Sep 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-20-09-2023-86ed66f78bac"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c03101fd-c33e-428e-98cc-850dc59baa6f", "node_type": "4", "metadata": {"filename": "llamaindex-update-20-09-2023-86ed66f78bac.md", "extension": ".md", "title": "LlamaIndex Update \u2014 20/09/2023", "date": "Sep 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-20-09-2023-86ed66f78bac"}, "hash": "0aa021b7c246bdd4ad7b22a9c8130d2bcd85d918140a9fa64d3d621c6ad61269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6cb8cf0-23e9-4d62-bef3-91f26346153d", "node_type": "1", "metadata": {"filename": "llamaindex-update-20-09-2023-86ed66f78bac.md", "extension": ".md", "title": "LlamaIndex Update \u2014 20/09/2023", "date": "Sep 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-20-09-2023-86ed66f78bac"}, "hash": "f1828826cc097540288d39c05e453efa9518228aa2564af90e7010281fa00b2e", "class_name": "RelatedNodeInfo"}}, "text": "#  Tutorials:\n\n  1. [ Wenqi Glantz ](https://twitter.com/wenqi_glantz) [ tutorial ](https://betterprogramming.pub/fine-tuning-gpt-3-5-rag-pipeline-with-gpt-4-training-data-49ac0c099919) on Fine-Tuning GPT-3.5 RAG Pipeline with GPT-4 Training Data with LlamaIndex fine-tuning abstractions. \n  2. [ Wenqi Glantz ](https://twitter.com/wenqi_glantz) [ tutorial ](https://betterprogramming.pub/fine-tuning-gpt-3-5-rag-pipeline-with-gpt-4-training-data-49ac0c099919) on Fine-Tuning Your Embedding Model to Maximize Relevance Retrieval in RAG Pipeline with LlamaIndex. \n\nTutorials from the LlamaIndex Team.\n\n  * [ Sourabh ](https://twitter.com/thesourabhd) [ tutorial ](https://www.youtube.com/watch?v=2O52Tfj79T4) on SEC Insights, End-to-End Guide on [ secinsights.ai ](https://t.co/VY9we1zhip)\n  * [ Adam\u2019s ](https://twitter.com/ajhofmann18) [ tutorial ](https://www.youtube.com/watch?v=lcuL6Gqw_-g) on Custom Tools for Data Agents. \n  * [ Logan ](https://twitter.com/LoganMarkewich) [ tutorial ](https://www.youtube.com/watch?v=mIyZ_9gqakE) on retrieval/reranking, covering Node Parsing, AutoMergingRetriever, HierarchicalNodeParser, node post-processors, and the setup of a RouterQueryEngine. \n\n#  **Integrations with External Platforms**\n\n  1. **Integration with PortkeyAI** : LlamaIndex integrates with PortkeyAI, boosting LLM providers like OpenAI with features like auto fallbacks and load balancing. [ Tweet, ](https://x.com/llama_index/status/1699087716183638256?s=20) [ Documentation ](https://gpt-index.readthedocs.io/en/latest/examples/llm/portkey.html)\n  2. **Collaboration with Anyscale** : LlamaIndex collaborates with anyscalecompute, enabling easy tuning of open-source LLMs using Ray Serve/Train. [ Tweet, ](https://twitter.com/llama_index/status/1699444987627466986?s=20) [ Documentation ](https://gpt-index.readthedocs.io/en/latest/examples/llm/anyscale.html)\n  3. **Integration with Elastic** : LlamaIndex integrates with Elastic, enhancing capabilities such as vector search, text search, hybrid search models, enhanced metadata handling, and es_filters. [ Tweet, ](https://twitter.com/llama_index/status/1700195709041954929?s=20) [ Documentation ](https://gpt-index.readthedocs.io/en/stable/examples/vector_stores/ElasticsearchIndexDemo.html)\n  4. **Integration with MultiOn** : LlamaIndex integrates with MultiOn, enabling data agents to navigate the web and handle tasks via an LLM-designed browser. [ Tweet, ](https://twitter.com/llama_index/status/1700221470427754610?s=20) [ Documentation ](https://llamahub.ai/l/tools-multion)\n  5. **Integration with Vectara** : LlamaIndex collaborates with Vectara to streamline RAG processes from loaders to databases. [ Tweet, ](https://twitter.com/llama_index/status/1701673229675552876?s=20) [ Blog Post ](https://medium.com/llamaindex-blog/llamaindex-vectara-7a3889cd34cb)\n  6. **Integration with LiteLLM** : LlamaIndex integrates with LiteLLM, offering access to over 100 LLM APIs and features like chat, streaming, and async operations. [ Tweet, ](https://x.com/llama_index/status/1703188185323561432?s=20) [ Documentation ](https://gpt-index.readthedocs.io/en/stable/examples/llm/litellm.html)\n  7. **Integration with MonsterAPI** : LlamaIndex integrates with MonsterAPI, allowing users to query data using LLMs like Llama 2 and Falcon. [ Tweet, ](https://x.com/monsterapis/status/1702252516061372595?s=20) [ Blog Post ](https://blog.monsterapi.ai/llama-index-monsterapi-integration-llm-rag/)\n\n#  **Events:**\n\n  1. [ Jerry Liu ](https://twitter.com/jerryjliu0) spoke on [ Production Ready LLM Applications ](https://docs.google.com/presentation/d/1uzhz1aFWbyXSrWBzQ1FPQWtVjMgJqAYGoGoVzEnNmAg/edit#slide=id.p) at the Arize AI event. \n  2. [ Ravi Theja ](https://twitter.com/ravithejads) conducted a [ workshop ](https://x.com/ravithejads/status/1699644440002826350?s=20) at LlamaIndex + Replit Pune Generative AI meetup. \n  3. [ Jerry Liu ](https://twitter.com/jerryjliu0) [ session ](https://www.youtube.com/watch?v=wlKe9U8hmi0) on Building a Lending Criteria Chatbot in Production with Stelios from MQube. \n\n#  **Webinars** :\n\n  1. [ Webinar ](https://www.youtube.com/watch?v=l-SGgWRe58A) on How to Win an LLM Hackathon by Alex Reibman, Rahul Parundekar, Caroline Frasca, and Yi Ding. \n  2. [ Webinar ](https://www.youtube.com/watch?v=eGC7m8_SgDk) on LLM Challenges in Production with Mayo Oshin, AI Jason, and Dylan. \n\n", "mimetype": "text/plain", "start_char_idx": 5526, "end_char_idx": 9913, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "066f374f-9302-4926-8162-d2396b7bfc01": {"__data__": {"id_": "066f374f-9302-4926-8162-d2396b7bfc01", "embedding": null, "metadata": {"filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "14687a53-0578-47a9-ac10-048904a69276", "node_type": "4", "metadata": {"filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "hash": "d0187bf2ee948a24b388233c78e9ba256d6c7274030e1d4f63b92de761860d7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0cdbc951-b42b-43a5-b22c-68ecd6243377", "node_type": "1", "metadata": {}, "hash": "c920f742f9578087d53e122d9c399197098a5b0f2e2d09fdf023ae6af9fedf28", "class_name": "RelatedNodeInfo"}}, "text": "(co-authored by Ofer Mendelevitch, head of Developer Relations at Vectara, and\nLogan Markewich, founding engineer at LlamaIndex)\n\n#  Introduction\n\n[ Vectara ](https://vectara.com) is a trusted GenAI platform. Exposing a set\nof easy to use [ APIs ](https://docs.vectara.com/docs/) , Vectara\u2019s platform\nreduces the complexity involved in developing [ Grounded Generation\n](https://vectara.com/grounded-generation-making-generative-ai-safe-\ntrustworthy-more-relevant/) (aka retrieval-augmented-generation) applications,\nand managing the LLM infrastructure that\u2019s required to deploy them at scale in\nproduction.\n\nToday we\u2019re happy to announce Vectara\u2019s integration with [ LlamaIndex\n](https://github.com/jerryjliu/llama_index) via a new type of Index: the\n_Managed Index_ . In this blog post, we\u2019ll dig deeper into how a [\nManagedIndex ](https://gpt-\nindex.readthedocs.io/en/stable/community/integrations/managed_indices.html)\nworks, and show examples of using Vectara as a Managed Index.\n\n#  What is Vectara?\n\nVectara is an end-to-end platform that offers powerful generative AI\ncapabilities for developers, including:\n\n**Data processing.** Vectara supports various file types for ingestion\nincluding markdown, PDF, PPT, DOC, HTML and many others. At ingestion time,\nthe text is automatically extracted from the files, and chunked into\nsentences. Then a vector embedding is computed for each chunk, so you don\u2019t\nneed to call any additional service for that.\n\n**Vector and text storage.** Vectara hosts and manages the vector store (where\nthe document embeddings are stored) as well as the associated text. Developers\ndon\u2019t need to go through a long and expensive process of evaluation and choice\nof vector databases. Nor do they have to worry about setting up that Vector\ndatabase, managing it in their production environment, re-indexing, and many\nother DevOps considerations that become important when you scale your\napplication beyond a simple prototype.\n\n**Query flow.** When issuing a query, calculating the embedding vector for\nthat query and retrieving the resulting text segments (based on similarity\nmatch) is fully managed by Vectara. Vectara also provides a robust\nimplementation of hybrid search and re-ranking out of the box, which together\nwith a state of the art embedding model ensures the most relevant text\nsegments are returned in the retrieval step.\n\n**Security and Privacy.** Vectara\u2019s API is fully encrypted in transit and at\nrest, and supports customer-managed-keys (CMK). We never train on your data,\nso you can be sure your data is safe from privacy leaks.\n\n**Figure 1:** Vectara\u2019s API platform for \u201cGrounded Generation\u201d\n\nThe nice thing is that all this complexity is fully managed by Vectara, taking\na lot of the heavy lifting off of the developer\u2019s shoulders, so that they\ndon\u2019t have to specialize in the constantly evolving skills of large language\nmodels, embedding models, vector stores and MLOps.\n\n#  From VectorStoreIndex to ManagedIndex\n\nLlamaIndex is a data framework for building LLM applications. It provides a\nset of composable modules for users to define a data pipeline for their\napplication. This consists of data loaders, text splitters, metadata\nextractors, and vector store integrations.\n\nA popular abstraction that users use is the VectorStoreIndex, providing\nintegrations with different vector databases.\n\nHowever, a challenge here is that users still need to carefully define how to\nload data, parse it, as well as choose an embedding model and a vector DB to\nuse. Since Vectara abstracts away this complexity, the Vectara and LlamaIndex\nteams jointly came up with a new abstraction: The ManagedIndex.\n\nAs shown in figure 2, when ingesting data into a VectorStoreIndex, data is\nprocessed locally taking advantage of multiple components like Data Connectors\nand Node parsers.\n\n**Figure 2:** typical flow of document processing in LlamaIndex for a\nVectorStoreIndex\n\nWith Vectara (figure 3), this whole flow is replaced by a single \u201cindexing\u201d\nAPI call , and all this processing is instead performed in the backend by the\nVectara platform.\n\n**Figure 3:** pre-processing with the VectaraIndex simplifies the complex\ningest flow to a single step.\n\n#  How does the VectaraIndex work?\n\nLet\u2019s take a look at a simple question-answering example using VectaraIndex,\nin this case asking questions from one of Paul Graham\u2019s Essays.\n\n**Step 1: Setup your Vectara account and Index**\n\nTo get started, follow our [ quickstart\n](https://docs.vectara.com/docs/quickstart) guide: [ signup\n](https://console.vectara.com/signup) for a free Vectara account, create a\ncorpus (index), and generate your API key.\n\nThen setup your Vectara customer_id, corpus_id and api_key as environment\nvariables, so that the VectaraIndex can access those easily, for example:\n\n    \n    \n    VECTARA_CUSTOMER_ID=<YOUR_CUSTOMER_ID>\n    VECTARA_CORPUS_ID=<YOUR_CORPUS_ID>\n    VECTARA_API_KEY=\"zwt_RbZfGT\u2026\"\n\n**Step 2: Create a VectaraIndex instance with LlamaIndex**\n\nBuilding the Vectara Index is extremely simple:\n\n    \n    \n    from llama_index import SimpleDirectoryReader\n    from llama_index.indices import VectaraIndex\n    From pprint Import pprint\n    \n    documents = SimpleDirectoryReader(\"paul_graham\").load_data()\n    index = VectaraIndex.from_documents(documents)\n\nHere we load Paul Graham\u2019s Essay using LlamaIndex\u2019s SimpleDirectoryReader into\na single document. The from_documents() constructor is then used to generate\nthe VectaraIndex instance.\n\nUnlike the common flow that uses LlamaIndex tools like data connectors,\nparsers and embedding models to process the input data, with VectaraIndex the\ndocuments are sent directly to Vectara via the [ Indexing API\n](https://docs.vectara.com/docs/api-reference/indexing-apis/indexing) .\nVectara\u2019s platform then processes, chunks, encodes and stores the text and\nembeddings into a Vectara corpus, making it available instantly for querying.\n\n**Step 3: Query**\n\nAfter the data is fully ingested, you can take advantage of the rich set of\nquery constructs built into LlamaIndex. For example let\u2019s use the index to\nretrieve the top-k most relevant nodes:\n\n    \n    \n    retriever = index.as_retriever(similarity_top_k=7)\n    # docs should contain the 7 most relevant documents for the query\n    docs = retriever.retrieve(\u201cWhat is the IBM 1401?\u201d)\n    pprint(docs[0].node.text)\n\n> (\u2018My stories were awful. They had hardly any plot, just characters with\n> strong feelings, which I imagined made them deep. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 6458, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0cdbc951-b42b-43a5-b22c-68ecd6243377": {"__data__": {"id_": "0cdbc951-b42b-43a5-b22c-68ecd6243377", "embedding": null, "metadata": {"filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "14687a53-0578-47a9-ac10-048904a69276", "node_type": "4", "metadata": {"filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "hash": "d0187bf2ee948a24b388233c78e9ba256d6c7274030e1d4f63b92de761860d7a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "066f374f-9302-4926-8162-d2396b7bfc01", "node_type": "1", "metadata": {"filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "hash": "55247cfaacf77e2ed828b2d01475c1f411cdc9c4006289b432cc7f17f11d150f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ab76f83-3dc3-4ee1-aa23-6f3d5b67b56a", "node_type": "1", "metadata": {}, "hash": "66d01769b7931c97dce5f80cba8b322f5c03fd397262a986af1710576579dff3", "class_name": "RelatedNodeInfo"}}, "text": "The first programs I tried\n> writing were on the IBM 1401 that our school district used for what was then\n> called \u201cdata processing.\u201d This was in 9th grade, so I was 13 or 14. The\n> school district\u2019s 1401 happened to be in the basement of our junior high\n> school, and my friend Rich Draves and I got permission to use it.\u2019)\n\nHere we printed out the top matching Node given the query \u201cwhat is the IBM\n1401?\u201d This in turn results in a call to Vectara\u2019s [ Search API\n](https://docs.vectara.com/docs/api-reference/search-apis/search) that returns\nthe top-k matching document segments.\n\nThose are transformed into NodeWithScore objects and thus can be used as usual\nwith the rest of the LlamaIndex querying tools. For example we can use\nLlamaIndex\u2019s query_engine() to convert the retrieved matching document\nsegments (nodes) into a comprehensive response to our question:\n\n    \n    \n    # Get an answer to the query based on the content of the essay\n    response = index.as_query_engine().query(\"What can the 1401 do?\")\n    print(response)\n\n> The 1401 was used for \u201cdata processing\u201d and could load programs into memory\n> and run them. ", "mimetype": "text/plain", "start_char_idx": 6458, "end_char_idx": 7589, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ab76f83-3dc3-4ee1-aa23-6f3d5b67b56a": {"__data__": {"id_": "2ab76f83-3dc3-4ee1-aa23-6f3d5b67b56a", "embedding": null, "metadata": {"filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "14687a53-0578-47a9-ac10-048904a69276", "node_type": "4", "metadata": {"filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "hash": "d0187bf2ee948a24b388233c78e9ba256d6c7274030e1d4f63b92de761860d7a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0cdbc951-b42b-43a5-b22c-68ecd6243377", "node_type": "1", "metadata": {"filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "hash": "bb04cfc4ca8e9b3a5c85d1fbe44d4a220779d82c616338b9a899055e95ee8a93", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f78586b-37bc-4bcf-8979-a8e08e9a7dc8", "node_type": "1", "metadata": {}, "hash": "bb301918940df8c55129e8af4bb0f4f9917bd6f2d7c01fb59f91e113d49a2a6b", "class_name": "RelatedNodeInfo"}}, "text": "It had a card reader, printer, CPU, disk drives, and used an\n> early version of Fortran as the programming language. ", "mimetype": "text/plain", "start_char_idx": 7589, "end_char_idx": 7706, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f78586b-37bc-4bcf-8979-a8e08e9a7dc8": {"__data__": {"id_": "8f78586b-37bc-4bcf-8979-a8e08e9a7dc8", "embedding": null, "metadata": {"filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "14687a53-0578-47a9-ac10-048904a69276", "node_type": "4", "metadata": {"filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "hash": "d0187bf2ee948a24b388233c78e9ba256d6c7274030e1d4f63b92de761860d7a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ab76f83-3dc3-4ee1-aa23-6f3d5b67b56a", "node_type": "1", "metadata": {"filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}, "hash": "f4023c5ff5bd665b1384d365ecf920c23e8f1bb99f02124286681c70e548cb46", "class_name": "RelatedNodeInfo"}}, "text": "The only form of input\n> to programs was data stored on punched cards.\n\n#  Why Use VectaraIndex with LlamaIndex?\n\nBy adding the concept of a \u201cManaged Index\u201d and the VectaraIndex to LlamaIndex,\nusers can continue to take advantage of the tools and capabilities offered by\nthe LlamaIndex library while integrating with a generative AI platform like\nVectara.\n\nRetrievers and Query Engines are just the tip of the iceberg. Using a managed\nindex with Vectara, developers have full access to advanced utilities like\nrouters, advanced query engines, data agents, chat engines, and more! Being\nable to retrieve context using Vectara empowers developers to build these\ncomplex applications using LlamaIndex components.\n\nFor example, in the following code we use the chat engine in LlamaIndex to\nquickly create a chat interaction using our VectaraIndex:\n\n    \n    \n    chat = index.as_chat_engine(chat_mode='context')\n    res = chat.chat(\"When did the author learn Lisp?\")\n    print(res.response)\n\n> \u201cThe author learned Lisp in college.\u201d\n\nA follow up question retains the chat history for context, as you might\nexpect:\n\n    \n    \n    chat.chat(\"and was it helpful for projects?\").response\n\n> \u201cYes, learning Lisp was helpful for the author\u2019s projects. They used Lisp in\n> both Viaweb and Y Combinator, indicating its usefulness in their work.\u201d\n    \n    \n    chat.chat(\"what was a distinctive characteristic of that programming language?\").response\n\n> \u201cA distinctive characteristic of Lisp is that its core is a language defined\n> by writing an interpreter in itself. It was originally designed as a formal\n> model of computation and an alternative to the Turing machine. This self-\n> referential nature of Lisp sets it apart from other programming languages.\u201d\n\nFor more information on how to use chat-engines, check out the [ documentation\n](https://gpt-\nindex.readthedocs.io/en/latest/core_modules/query_modules/chat_engines/usage_pattern.html)\n, and for more information on other query capabilities with LlamaIndex, check\nout the full documentation [ here ](https://gpt-\nindex.readthedocs.io/en/latest/index.html) .\n\n#  Summary\n\nLlamaIndex makes it super easy to populate VectaraIndex with content from any\ndocument or data source, while utilizing the Vectara service for managing the\ndocument processing, chunking, embedding and making all of this data available\nfor advanced retrieval in query time using the LlamaIndex library.\n\nVectaraIndex is based on the new LlamaIndex Managed Index abstraction, which\nbetter supports GenAI platforms like Vectara, and enables additional vendors\nwho also provide end-to-end platforms to join in.\n\nTo get started with Vectara and LlamaIndex you can follow the Vectara\nquickstart [ guide ](https://docs.vectara.com/docs/quickstart) to setup your\naccount, and the examples above with your own data.\n\n", "mimetype": "text/plain", "start_char_idx": 7706, "end_char_idx": 10534, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d5af6f3-eb70-4cb5-b0d0-5a3d4b147618": {"__data__": {"id_": "5d5af6f3-eb70-4cb5-b0d0-5a3d4b147618", "embedding": null, "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36c588af-23f5-402f-b06b-dc8285afc728", "node_type": "4", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "17f37ed6f67d2a57e53770adeb46a22d0962da911663e7e148819c62e8d4d4b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2eb2835-9ba6-4a5c-a98f-ea5e485429d4", "node_type": "1", "metadata": {}, "hash": "4980e9f6db31f2fddf57ff498564f02d6a640265b3df7a34e98419f32d9d262d", "class_name": "RelatedNodeInfo"}}, "text": "Hello LlamaIndex Community!\n\nWe\u2019re thrilled to bring you the latest edition of our LlamaIndex Update\nseries. Whether you\u2019ve been a part of our journey from the start or have just\nrecently joined us, your engagement and input are invaluable to us.\n\nIn this update, we\u2019re excited to unveil some significant advancements. We\u2019ve\ngot comprehensive updates on new features for both the Python and TypeScript\nversions of LlamaIndex. In addition, we\u2019re offering some expert insights on\nRAG tips that you won\u2019t want to miss. To keep you ahead of the curve, we\u2019ve\nalso curated a selection of webinars, tutorials, events, and demos.\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 623, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2eb2835-9ba6-4a5c-a98f-ea5e485429d4": {"__data__": {"id_": "c2eb2835-9ba6-4a5c-a98f-ea5e485429d4", "embedding": null, "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36c588af-23f5-402f-b06b-dc8285afc728", "node_type": "4", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "17f37ed6f67d2a57e53770adeb46a22d0962da911663e7e148819c62e8d4d4b0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d5af6f3-eb70-4cb5-b0d0-5a3d4b147618", "node_type": "1", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "fc3710f24c77085e148dcf22d41ff55ac5e108f75ed09377ac58d599d1a5660b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b83794da-0574-4954-b8c7-5c8d2294337b", "node_type": "1", "metadata": {}, "hash": "b5061e8d93575460da3ca8ef67cc659fa9821a3ac95f319eabfbe472553f1803", "class_name": "RelatedNodeInfo"}}, "text": "So without further ado, let\u2019s delve into the latest developments.\n\n#  **New Features:**\n\n##  LlamaIndex\n\n  1. LlamaIndex introduces the Sweep AI code splitter for RAG apps, addressing the challenges of traditional code splitting. This tool features recursive splitting combined with CSTs across 100+ languages, enhancing the LlamaIndex experience. [ BlogPost ](https://docs.sweep.dev/blogs/chunking-2m-files) , [ Tweet ](https://twitter.com/jerryjliu0/status/1686413452988878849?s=20) . \n  2. LlamaIndex now supports streaming data ETL, enhancing structured data extraction with the OpenAI Function API. By inputting a Pydantic object class in LlamaIndex, users can receive streamed data objects from OpenAI individually. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/output_parsing/openai_pydantic_program.html#extraction-into-album-streaming) , [ Tweet ](https://twitter.com/jerryjliu0/status/1686752090197008387?s=20) . \n  3. LlamaIndex has teamed up with Neo4j to amplify knowledge graph capabilities with LLM\u2019s. This integration not only allows for storing any knowledge graph created in LlamaIndex directly in Neo4j but also introduces a specialized text-to-cypher prompt for Neo4j users. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/Neo4jKGIndexDemo.html) , [ Tweet ](https://twitter.com/llama_index/status/1687224679340064768?s=20) . \n  4. LlamaIndex, in collaboration with Mendable AI and Nomic AI, unveils a Nomic Atlas visual map detailing user questions from the Mendable AI bot. This innovative tool groups similar questions, providing insights for improved app deployment, prompt control, language support, and documentation. New users can find the helpful Mendable AI bot on LlamaIndex\u2019s documentation site. [ Tweet ](https://twitter.com/llama_index/status/1687485785228906496?s=20) . \n  5. LlamaIndex, in collaboration with Predibase, offers an optimal way to operationalize LLMs. Experience top-tier RAG by privately hosting open-source LLMs on managed infrastructure right within your VPC. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/llm/predibase.html) , [ Tweet ](https://twitter.com/predibase/status/1687493843619598336?s=20) . \n  6. The LlamaIndex playground [ app ](https://llama-playground.vercel.app/) enhances the RAG experience. Updates include new Temperature and Top P options, along with intuitive tooltips offering plain language explanations. \n  7. LlamaIndex Tip: Boost your RAG systems by adding structured data to raw text. This allows for easier metadata filtering and optimal embedding biases. Dive into our guide on harnessing the HuggingFace span marker for targeted entity extraction. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/metadata_extraction/EntityExtractionClimate.html) , [ Tweet ](https://twitter.com/llama_index/status/1688711638537682944?s=20) . \n  8. LlamaIndex now has the Semantic Scholar Loader. With it, users can swiftly set up citation-based Q&A systems. [ Docs ](https://llamahub.ai/l/semanticscholar) , [ Tweet ](https://twitter.com/shauryr/status/1687858252481236993?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 623, "end_char_idx": 3758, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b83794da-0574-4954-b8c7-5c8d2294337b": {"__data__": {"id_": "b83794da-0574-4954-b8c7-5c8d2294337b", "embedding": null, "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36c588af-23f5-402f-b06b-dc8285afc728", "node_type": "4", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "17f37ed6f67d2a57e53770adeb46a22d0962da911663e7e148819c62e8d4d4b0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2eb2835-9ba6-4a5c-a98f-ea5e485429d4", "node_type": "1", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "158a4aaaef0ae772016abf7e6e78c84090696bc5129a8e5b8aa2d743519c61ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c15036c5-c315-489c-b2a3-a17d41798fc9", "node_type": "1", "metadata": {}, "hash": "5b927f92fc27aa7a05911878a04d2713c788da0f0c9d83d943d7b3ee372b4ff7", "class_name": "RelatedNodeInfo"}}, "text": "9. LlamaIndex highlights the significance of text chunk size in LLM QA systems. To determine the best chunk size without human intervention, we suggest ensembling different sizes and using a reranker for context relevance during queries. This method involves simultaneous queries across retrievers of various sizes and consolidating results for reranking. Though experimental, this approach aims to discern the optimal chunk size strategy. [ Docs ](https://gpt-index.readthedocs.io/en/stable/examples/retrievers/ensemble_retrieval.html) , [ Tweet ](https://twitter.com/jerryjliu0/status/1688948298781249536?s=20) . \n  10. LlamaIndex\u2019s customer support bot seamlessly interfaces with Shopify\u2019s 50k-line GraphQL API Spec. Through smart tools and LlamaIndex features, it offers quick insights like ` refunded orders ` despite the vast spec size. Efficient indexing ensures precise user query responses. [ Docs ](https://llamahub.ai/l/tools-shopify) , [ Tweet ](https://twitter.com/jerryjliu0/status/1689295239822069760?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 3758, "end_char_idx": 4785, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c15036c5-c315-489c-b2a3-a17d41798fc9": {"__data__": {"id_": "c15036c5-c315-489c-b2a3-a17d41798fc9", "embedding": null, "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36c588af-23f5-402f-b06b-dc8285afc728", "node_type": "4", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "17f37ed6f67d2a57e53770adeb46a22d0962da911663e7e148819c62e8d4d4b0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b83794da-0574-4954-b8c7-5c8d2294337b", "node_type": "1", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "021676ddc9720c36e406fd73255d4a1d60773ec46cfd925da2efc200869bec18", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35eae915-917c-4612-91e8-ee556488c079", "node_type": "1", "metadata": {}, "hash": "2898a0594296bcb00c94e05f570f9eca52f4cde36525660a1e297a929c56666a", "class_name": "RelatedNodeInfo"}}, "text": "11. LlamaIndex\u2019s integration with Xinference enables users to effortlessly expand models like llama 2, chatglm, and vicuna to incorporate RAG and agents. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/llm/XinferenceLocalDeployment.html) , [ Tweet ](https://twitter.com/llama_index/status/1689426281015005184?s=20) . \n  12. LlamaIndex introduces ` One-click Observability ` . With just a single code line, integrate LlamaIndex with advanced observability tools from partners like Weights & Biases, ArizeAI, and TruEra, simplifying LLM app debugging for production. [ Docs ](https://gpt-index.readthedocs.io/en/latest/end_to_end_tutorials/one_click_observability.html) , [ Tweet ](https://twitter.com/llama_index/status/1689659395465191424?s=20) . \n  13. LlamaIndex has updated the LLM default temperature value to 0.1. [ Tweet ](https://twitter.com/yi_ding/status/1689692197871042561?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 4785, "end_char_idx": 5693, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35eae915-917c-4612-91e8-ee556488c079": {"__data__": {"id_": "35eae915-917c-4612-91e8-ee556488c079", "embedding": null, "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36c588af-23f5-402f-b06b-dc8285afc728", "node_type": "4", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "17f37ed6f67d2a57e53770adeb46a22d0962da911663e7e148819c62e8d4d4b0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c15036c5-c315-489c-b2a3-a17d41798fc9", "node_type": "1", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "929b70e51cda15c8c88c7c483d5888db887f6fb3f1e86ab93a5400cf6a736749", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be94974d-5da9-401f-afc1-05865e901ac5", "node_type": "1", "metadata": {}, "hash": "2f48ad71e62a81f544f61dbacc88adff234f3c6fe49b729903c9fd8fa6546c62", "class_name": "RelatedNodeInfo"}}, "text": "14. LlamaIndex integration with Zep, enhancing the memory layer of LLM apps. It\u2019s not just about storage but also enriching data with summaries, metadata, and more. [ BlogPost ](https://medium.com/llamaindex-blog/zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc) , [ Tweet ](https://twitter.com/jerryjliu0/status/1690018390059225088?s=20) . \n  15. LlamaIndex has revamped its defaults! Now, gpt-3.5-turbo is the go-to LLM, with enhanced prompts and a superior text splitter. Additionally, if OpenAI\u2019s key isn\u2019t set, it has backup options with llama.cpp. New embedding features have also been added. [ Tweet ](https://twitter.com/llama_index/status/1690081661453803520?s=20) . \n  16. LlamaIndex now seamlessly integrates with FastChat by [ lmsysorg ](https://twitter.com/lmsysorg) . Elevate your LLM deployments like Vicuna and Llama 2, serving as an alternative to OpenAI. [ Tweet ](https://twitter.com/jerryjliu0/status/1691114369705533440?s=20) . \n  17. LlamaIndex provides a seamless integration with Azure AI Services. Dive into a richer ecosystem of AI tools from Computer Vision, Translation, and speech enhancing your multi-modal AI interactions. [ Docs1 ](https://llamahub.ai/l/tools-azure_translate) , [ Docs2 ](https://llamahub.ai/l/tools-azure_speech) , [ Docs3 ](https://llamahub.ai/l/tools-azure_cv) , [ Tweet ](https://twitter.com/llama_index/status/1691605500079800674?s=20) . \n  18. LlamaIndex unveils ` Graph RAG ` \u2014 an approach to enhance LLMs with context from graph databases. Extract valuable subgraphs from any knowledge graph for superior question-answering capabilities. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/query_engine/knowledge_graph_rag_query_engine.html) , [ Tweet ](https://twitter.com/jerryjliu0/status/1691835187519459338?s=20) . \n  19. LlamaIndex has expanded native async support, enhancing the scalability of full-stack LLM apps. We now offer async agents, tool execution, and callback support, and have introduced async methods in vector stores. [ Tweet ](https://twitter.com/llama_index/status/1691965149840908642?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 5693, "end_char_idx": 7789, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be94974d-5da9-401f-afc1-05865e901ac5": {"__data__": {"id_": "be94974d-5da9-401f-afc1-05865e901ac5", "embedding": null, "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36c588af-23f5-402f-b06b-dc8285afc728", "node_type": "4", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "17f37ed6f67d2a57e53770adeb46a22d0962da911663e7e148819c62e8d4d4b0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35eae915-917c-4612-91e8-ee556488c079", "node_type": "1", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "2ad64422f2548159b693633869d75aa903aec31e51c8d41b15fa5bcd36f6fc23", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a67ab9b-8d25-4987-9fa9-3e915769a0ee", "node_type": "1", "metadata": {}, "hash": "3046dd120f7eac83a1206b06f1556e3a3d9633782ea0e149afe4bc4255ade5af", "class_name": "RelatedNodeInfo"}}, "text": "20. LlamaIndex enhances debugging with data agent trace observability. Additionally, system prompts can now be added to any query engine and we have begun the transition of LLM and embedding modules to Pydantic. [ Docs ](https://twitter.com/llama_index/status/1692696993900974399?s=20) , [ Tweet ](https://gpt-index.readthedocs.io/en/latest/examples/callbacks/LlamaDebugHandler.html#see-traces-events-for-agents) . \n  ", "mimetype": "text/plain", "start_char_idx": 7789, "end_char_idx": 8207, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a67ab9b-8d25-4987-9fa9-3e915769a0ee": {"__data__": {"id_": "6a67ab9b-8d25-4987-9fa9-3e915769a0ee", "embedding": null, "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36c588af-23f5-402f-b06b-dc8285afc728", "node_type": "4", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "17f37ed6f67d2a57e53770adeb46a22d0962da911663e7e148819c62e8d4d4b0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be94974d-5da9-401f-afc1-05865e901ac5", "node_type": "1", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "30c40219fe3771266132fe2a2d220fa58fcfb30b0912f43d636c7c8804d3f6f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e7afcca-75e4-4309-a9fa-54bc294f4918", "node_type": "1", "metadata": {}, "hash": "a698540d3d3efaa7f112e1f054b92473dc2b2b17c88cdd9cb181ee706bd1328a", "class_name": "RelatedNodeInfo"}}, "text": "21. LlamaIndex\u2019s ` Recursive Document Agents ` enhance RAG by retrieving based on summaries and adjusting chunk retrieval per need. This boosts querying across varied documents, offering both question-answering and summarization within a document. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/query_engine/recursive_retriever_agents.html) , [ Tweet ](https://twitter.com/jerryjliu0/status/1693421308674289822?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 8207, "end_char_idx": 8643, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e7afcca-75e4-4309-a9fa-54bc294f4918": {"__data__": {"id_": "1e7afcca-75e4-4309-a9fa-54bc294f4918", "embedding": null, "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36c588af-23f5-402f-b06b-dc8285afc728", "node_type": "4", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "17f37ed6f67d2a57e53770adeb46a22d0962da911663e7e148819c62e8d4d4b0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a67ab9b-8d25-4987-9fa9-3e915769a0ee", "node_type": "1", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "1fec55092c1cc2f3a6df2ab6881d54d48af0080713d6d80ff16114f9182f8c88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aea65467-b9c4-4e50-8832-a2dd2be570d5", "node_type": "1", "metadata": {}, "hash": "2a641615b758858de80b8e4261426e68b79a9fa61c386872426c66c46ad91cbe", "class_name": "RelatedNodeInfo"}}, "text": "22. LlamaIndex integrates with Metaphor to supercharge data agents. This integration offers a specialized search engine tailored for LLMs, allowing dynamic data lookup beyond just RAG, and answering a broader range of questions. [ BlogPost ](https://medium.com/llamaindex-blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f) , [ Tweet ](https://twitter.com/llama_index/status/1693649115983618278?s=20) . \n  23. LlamaIndex now supports integration with OpenAI\u2019s fine-tuned models via their new endpoint. Seamlessly integrate these models into your RAG pipeline. [ Docs ](https://gpt-index.readthedocs.io/en/latest/core_modules/model_modules/llms/usage_custom.html) , [ Tweet ](https://twitter.com/llama_index/status/1694116968008401201?s=20) . \n  24. LlamaIndex introduces the ` OpenAIFineTuningHandler ` to streamline data collection for fine-tuning gpt-3.5-turbo with GPT-4 outputs. Run RAG with GPT-4 and effortlessly generate a dataset to train a more cost-effective model. [ Notebook ](https://github.com/jerryjliu/llama_index/blob/main/experimental/openai_fine_tuning/openai_fine_tuning.ipynb) , [ Tweet ](https://twitter.com/llama_index/status/1694395355725746397?s=20) . \n  25. LlamaIndex presents the ` Principled Development Practices ` guide, detailing best practices for LLM app development Observability, Evaluation, and Monitoring. [ Docs ](https://gpt-index.readthedocs.io/en/latest/end_to_end_tutorials/principled_dev_practices.html) , [ Tweet ](https://twitter.com/llama_index/status/1694736328276271248?s=20) . \n  26. LlamaIndex introduces a refined Prompt system. With just three core classes: ` PromptTemplate ` , ` ChatPromptTemplate ` , and ` SelectorPromptTemplate ` , users can effortlessly format as chat messages or text and tailor prompts based on model conditions. [ Docs ](https://gpt-index.readthedocs.io/en/latest/core_modules/model_modules/prompts.html#) , [ Tweet ](https://twitter.com/llama_index/status/1695093392378880324?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 8643, "end_char_idx": 10641, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aea65467-b9c4-4e50-8832-a2dd2be570d5": {"__data__": {"id_": "aea65467-b9c4-4e50-8832-a2dd2be570d5", "embedding": null, "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36c588af-23f5-402f-b06b-dc8285afc728", "node_type": "4", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "17f37ed6f67d2a57e53770adeb46a22d0962da911663e7e148819c62e8d4d4b0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e7afcca-75e4-4309-a9fa-54bc294f4918", "node_type": "1", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "a8a1780f9deac8c3bb8ddf7a970181344172b39143890329cf86d0cefaa38991", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "909e284a-6c8f-4290-b3c9-a8aeb832eae1", "node_type": "1", "metadata": {}, "hash": "b1499935cc4fc64a2c6fd7ab2395384e00e0f41c663d871d95cdfe2d576e5541", "class_name": "RelatedNodeInfo"}}, "text": "27. LlamaIndex delves into ` chunk dreaming ` a concept inspired by [ Thomas H. Chapin IV ](https://twitter.com/tomchapin) . By auto-extracting metadata from a text chunk, it can identify potential questions and provide summaries over neighboring nodes. This enriched context boosts RAG\u2019s performance. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/metadata_extraction/MetadataExtraction_LLMSurvey.html) , [ Tweet ](https://twitter.com/llama_index/status/1695233836983144764?s=20) . \n  28. LlamaIndex is integrated with BagelDB, enabling developers to effortlessly tap into vector data stored on BagelDB. [ Tweet ](https://twitter.com/BagelDB_ai/status/1695158701387059319?s=20) . \n  29. LlamaIndex now lets the LLM choose between vector search for semantic queries or our BM25 retriever for keyword-specific ones. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/retrievers/bm25_retriever.html) , [ Tweet ](https://twitter.com/llama_index/status/1695590257054630149?s=20) . \n  30. LlamaIndex introduces the ` AutoMergingRetriever ` , crafted with insights from [ Jason ](https://twitter.com/jxnlco) and ChatGPT. This technique fetches precise context chunks and seamlessly merges them, optimizing LLM responses. Using the HierarchicalNodeParser, we ensure interconnected chunks for enhanced context clarity. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/retrievers/auto_merging_retriever.html) , [ Tweet ](https://twitter.com/llama_index/status/1695832757560356871?s=20) . \n  31. LlamaIndex introduces embedding finetuning for optimized retrieval performance. Beyond enhancing RAG, we\u2019ve simplified retrieval evaluations with automatic QA dataset generation from text, streamlining both finetuning and evaluation processes. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding.html) , [ Tweet ](https://twitter.com/llama_index/status/1696583119539966070?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 10641, "end_char_idx": 12597, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "909e284a-6c8f-4290-b3c9-a8aeb832eae1": {"__data__": {"id_": "909e284a-6c8f-4290-b3c9-a8aeb832eae1", "embedding": null, "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36c588af-23f5-402f-b06b-dc8285afc728", "node_type": "4", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "17f37ed6f67d2a57e53770adeb46a22d0962da911663e7e148819c62e8d4d4b0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aea65467-b9c4-4e50-8832-a2dd2be570d5", "node_type": "1", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "f1dadb4c74f4ce7e519b5ef959ac2af85c7567dd8464f3224787aa1c2fe1b265", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71891c40-c446-424d-93c2-9e516c34a890", "node_type": "1", "metadata": {}, "hash": "2afdaebb50cf7626f07ce8a5ef9a6a575ea05f4909c7f165389881ab61239879", "class_name": "RelatedNodeInfo"}}, "text": "32. LlamaIndex now integrates directly with Airbyte sources including Gong, Hubspot, Salesforce, Shopify, Stripe, Typeform, and Zendesk Support. Easily enhance your LlamaIndex application with these platforms implemented as data loaders. [ BlogPost ](https://airbyte.com/blog/introducing-airbyte-sources-within-llamaindex) , [ Tweet ](https://twitter.com/AirbyteHQ/status/1696633858316243046?s=20) . \n  33. LlamaIndex integrates with DeepEval, a comprehensive library to evaluate LLM and RAG apps. Assess on four key metrics: Relevance, Factual Consistency, Answer Similarity, and Bias/Toxicity. [ Docs ](https://gpt-index.readthedocs.io/en/latest/community/integrations/deepeval.html) , [ Tweet ](https://twitter.com/llama_index/status/1696674470566764846?s=20) . \n  34. LlamaIndex recommends evaluating LLM + RAG step-by-step, especially retrieval. Create synthetic retrieval datasets from text chunks using LLMs. This method not only evaluates retrieval but also fine-tunes embeddings. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding.html#generate-corpus) , [ Tweet ](https://twitter.com/jerryjliu0/status/1696675525442609166?s=20) . \n  35. LlamaIndex unveils a managed index abstraction simplifying RAG\u2019s ingestion and storage processes with Vectara. [ Docs ](https://gpt-index.readthedocs.io/en/latest/community/integrations/managed_indices.html) , [ Tweet ](https://twitter.com/llama_index/status/1696919525151899671?s=20) . \n  36. LlamaIndex has significantly enhanced its callback handling support, encompassing features like tracebacks, LLM token counts, templates, and detailed agent tool information. These advancements pave the way for smoother integrations with evaluation and observability applications. [ Tweet ](https://twitter.com/llama_index/status/1697407787154997754?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 12597, "end_char_idx": 14449, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71891c40-c446-424d-93c2-9e516c34a890": {"__data__": {"id_": "71891c40-c446-424d-93c2-9e516c34a890", "embedding": null, "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36c588af-23f5-402f-b06b-dc8285afc728", "node_type": "4", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "17f37ed6f67d2a57e53770adeb46a22d0962da911663e7e148819c62e8d4d4b0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "909e284a-6c8f-4290-b3c9-a8aeb832eae1", "node_type": "1", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "ae45aebe02ca4ed6ed5aa1db50fcdb5ff7c28b06a9378ae699171e9ced111ed6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a025fe32-a6aa-448c-ad69-099fa95e1b35", "node_type": "1", "metadata": {}, "hash": "3d4f3c54736a057b2e90d0668746f4f3c824fadd76704218e10c1d96edfc525b", "class_name": "RelatedNodeInfo"}}, "text": "37. LlamaIndex has integrated with AskMarvinAI, enabling automated metadata extraction from text corpora. Just annotate a Pydantic model and effortlessly log metadata from all associated text chunks. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/metadata_extraction/MarvinMetadataExtractorDemo.html) , [ Tweet ](https://twitter.com/llama_index/status/1697632035186372745?s=20) . \n  38. LlamaIndex is integrated with RunGPT by JinaAI, an outstanding framework for one-click deployment of various open-source models such as Llama, Vicuna, Pythia, and more. Coupled with LlamaIndex\u2019s innate chat/streaming capabilities, users can now deploy and utilize powerhouse models like Llama-7B seamlessly. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/llm/rungpt.html) , [ Tweet ](https://twitter.com/llama_index/status/1698001332563837165?s=20) . \n\n##  LlamaIndex.TS\n\n  1. LITS has Full Azure OpenAI integration. [ Tweet ](https://twitter.com/yi_ding/status/1688564790007087104) . \n  ", "mimetype": "text/plain", "start_char_idx": 14449, "end_char_idx": 15452, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a025fe32-a6aa-448c-ad69-099fa95e1b35": {"__data__": {"id_": "a025fe32-a6aa-448c-ad69-099fa95e1b35", "embedding": null, "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36c588af-23f5-402f-b06b-dc8285afc728", "node_type": "4", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "17f37ed6f67d2a57e53770adeb46a22d0962da911663e7e148819c62e8d4d4b0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71891c40-c446-424d-93c2-9e516c34a890", "node_type": "1", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "49c76d31956670d760073e267c2ab187724a977377a898e0c12348ac31bde6c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b26a0614-6bf1-4cb2-aa5f-d7b11ad254d7", "node_type": "1", "metadata": {}, "hash": "54356ea02644e7b0b44763b9f23ae1729fdf1bc8dc661f97ddf14ab1dd67bab3", "class_name": "RelatedNodeInfo"}}, "text": "2. LITS Enhanced Llama2 support, new default temperature (0.1), and GPT chat integration. [ Tweet ](https://twitter.com/llama_index/status/1689086106036547584?s=20) . \n  3. LITS helps to use ` fromDocuments ` without repeat checks; auto SHA256 comparison. [ Tweet ](https://twitter.com/llama_index/status/1691502243286228993?s=20) . \n  4. LITS now supports OpenAI v4, Anthropic 0.6, & Replicate 0.16.1., CSV loader, Merged NodeWithEmbeddings & BaseNode. [ Tweet ](https://twitter.com/llama_index/status/1691984600506257462?s=20) . \n  5. LITS now supports PapaCSVLoader for math. [ Tweet ](https://twitter.com/yi_ding/status/1691991221974217104?s=20) . \n  6. LITS is now integrated with LiteLLM. [ Tweet ](https://twitter.com/yi_ding/status/1692408213340340328) . \n  7. LITS now has additional session options for proxy server support, Default timeout reset to 60 seconds for OpenAI. [ Tweet ](https://twitter.com/llama_index/status/1693072438404276460?s=20) . \n  ", "mimetype": "text/plain", "start_char_idx": 15452, "end_char_idx": 16415, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b26a0614-6bf1-4cb2-aa5f-d7b11ad254d7": {"__data__": {"id_": "b26a0614-6bf1-4cb2-aa5f-d7b11ad254d7", "embedding": null, "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36c588af-23f5-402f-b06b-dc8285afc728", "node_type": "4", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "17f37ed6f67d2a57e53770adeb46a22d0962da911663e7e148819c62e8d4d4b0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a025fe32-a6aa-448c-ad69-099fa95e1b35", "node_type": "1", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "9a5f6a2ce5231cad37b026cba6002c7e2dde9f89b909fc6de219a3c144e675b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76569759-779b-4f28-9338-f12cc3cd07cb", "node_type": "1", "metadata": {}, "hash": "e6bd0a131fc5ce214b02b232e27a7dc845f64e1dcc82b3a554fa957fdc27b7db", "class_name": "RelatedNodeInfo"}}, "text": "8. LITS now has Pinecone integration. [ Tweet ](https://twitter.com/yi_ding/status/1693275444848840745?s=20) . \n  9. LITS has Optimized ChatGPT prompts, fixed metadata rehydration issues, and OpenAI Node v4.1.0 with fine-tuned model support. [ Tweet ](https://twitter.com/llama_index/status/1694382741218005153?s=20) . \n  10. LITS has introduced enhanced text-splitting features, including a specialized tokenizer for Chinese, Japanese, and Korean, and refinements to the SentenceSplitter for handling decimal numbers. [ Tweet ](https://twitter.com/llama_index/status/1694719208217588070?s=20) . \n  11. LITS has a Markdown loader and metadata support in the response synthesizer. [ Tweet ](https://twitter.com/llama_index/status/1695156772783395255?s=20) . \n  12. LITS revamped usability: ` ListIndex ` is now ` SummaryIndex ` for clarity, and prompts have been made typed and customizable to enhance user control and experience. [ Tweet ](https://twitter.com/llama_index/status/1696626780277481491?s=20) . \n  13. LITS has Notion Reader. Now, users can effortlessly import their documents directly into their RAG or Data Agent application in LITS. [ Tweet ](https://twitter.com/llama_index/status/1698053712521146389?s=20) . \n\n#  RAG Tips:\n\nLlamaIndex shares [ four tactics ](https://gpt-\nindex.readthedocs.io/en/latest/end_to_end_tutorials/dev_practices/production_rag.html)\nto boost your RAG pipeline:\n\n1\u20e3 Use summaries for retrieval, and a broader context for synthesis.\n\n2\u20e3 Use metadata for structured retrieval over large docs.\n\n3\u20e3 Deploy LLMs for dynamic retrieval based on tasks.\n\n4\u20e3 Fine-tune embeddings for better retrieval.\n\n#  Tutorials:\n\n  1. [ Jason's ](https://twitter.com/jasonzhou1993) [ tutorial ](https://www.youtube.com/watch?v=qKtM2AlDTs8&t=475s) on adding Image Responses to GPT knowledge retrieval apps. \n  2. [ Wenqi Glantz ](https://twitter.com/wenqi_glantz) [ tutorial ](https://betterprogramming.pub/building-production-ready-llm-apps-with-llamaindex-document-metadata-for-higher-accuracy-retrieval-a8ceca641fb5) on Building Production-Ready LLM Apps with LlamaIndex: Document Metadata for Higher Accuracy Retrieval \n  3. Streamlit [ tutorial ](https://blog.streamlit.io/build-a-chatbot-with-custom-data-sources-powered-by-llamaindex/) on Building a chatbot with custom data sources, powered by LlamaIndex. \n  4. [ Wenqi Glantz ](https://twitter.com/wenqi_glantz) [ tutorial ](https://betterprogramming.pub/building-production-ready-llm-apps-with-llamaindex-recursive-document-agents-for-dynamic-retrieval-1f4b25287918) on Building Production-Ready LLM Apps With LlamaIndex: Recursive Document Agents for Dynamic Retrieval. \n  5. [ Erika Cardenas ](https://twitter.com/ecardenas300) covers the usage of [ LlamaIndex in building an RAG app ](https://twitter.com/ecardenas300/status/1695816617207153016?s=20) . \n  6. [ Argilla ](https://argilla.io/) blog post on [ Fine-tuning and evaluating GPT-3.5 with human feedback for RAG using LlamaIndex ](https://docs.argilla.io/en/latest/guides/llms/examples/fine-tuning-openai-rag-feedback.html#Evaluating-base-vs-fine-tuned-with-human-preference-data) . \n  7. [ KDNuggests ](https://www.kdnuggets.com/) blog post on [ Build Your Own PandasAI with LlamaIndex ](https://www.kdnuggets.com/build-your-own-pandasai-with-llamaindex) . \n\nFrom the LlamaIndex team:\n\n  1. [ Jerry Liu ](https://twitter.com/jerryjliu0) \u2019s [ tutorial ](https://medium.com/llamaindex-blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d) on fine-tuning Llama 2 for Text-to-SQL Applications. \n  2. [ Jerry Liu's ](https://twitter.com/jerryjliu0) [ tutorial ](https://medium.com/llamaindex-blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971) on Fine-Tuning Embeddings for RAG with Synthetic Data. \n  3. [ Ravi Theja\u2019s ](https://twitter.com/ravithejads) [ tutorial ](https://medium.com/llamaindex-blog/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b) on combining Text2SQL and RAG with LlamaIndex to analyze product reviews. \n  4. [ Ravi Theja\u2019s ](https://twitter.com/ravithejads) tutorial on different [ Indicies, Storage Context, and Service Context of LlamaIndex. ](https://www.youtube.com/watch?v=gQXXeLHTxkI)\n  5. [ Ravi Theja\u2019s ](https://twitter.com/ravithejads) tutorial on [ Custom Retrievers and Hybrid Search in LlamaIndex. ](https://www.youtube.com/watch?v=hsEWohYtg0I)\n  6. [ Adam's ](https://twitter.com/ajhofmann18) tutorial on [ Introduction to Data Agents for Developers ](https://www.youtube.com/watch?v=GkIEEdIErm8) . \n  7. [ Ravi Theja\u2019s ](https://twitter.com/ravithejads) tutorial on creating [ Automatic Knowledge Transfer (KT) Generation for Code Bases using LlamaIndex. ](https://medium.com/llamaindex-blog/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af)\n\n#  Webinars:\n\n  1. [ Webinar ](https://www.youtube.com/watch?v=njzB6fm0U8g) with members from Docugami on Document Metadata and Local Models for Better, Faster Retrieval. \n  2. [ Webinar ](https://www.youtube.com/watch?v=CwdAi1tts9c) with Shaun and Piaoyang on building Personalized AI Characters with RealChar. \n  3. [ Webinar ](https://www.youtube.com/watch?v=Zj5RCweUHIk) with Bob (Weaviet), Max (sid.ai), and Tuana (HayStack) on making RAG Production-Ready. \n  4. [ Workshop ](https://www.youtube.com/watch?v=hb8uT-VBEwQ) by Wey Gu on Building RAG with Knowledge Graphs. \n  5. [ Webinar ](https://www.youtube.com/watch?v=mndiDJ5k26A) with Jo Bergum and Shishir Patil on fine-tuning and RAG. \n\n#  Events:\n\n  1. [ Jerry Liu ](https://twitter.com/jerryjliu0) spoke about LlamaIndex at the [ NYSE Floor Talk ](https://www.youtube.com/watch?v=QtYL4Cm-pjE) . \n  2. [ Ravi Theja ](https://twitter.com/ravithejads) spoke about LlamaIndex at the [ Fifth Elephant conference ](https://twitter.com/ravithejads/status/1689491855543599104?s=20) in Bengaluru, India. \n  3. [ Ravi Theja ](https://twitter.com/ravithejads) conducted a [ workshop ](https://twitter.com/fifthel/status/1692785973283656052?s=20) on LlamaIndex in Bengaluru, India. \n\n#  Demos And Papers:\n\n  1. The paper titled [ Performance of ChatGPT, human radiologists, and context-aware ChatGPT in identifying AO codes from radiology reports ](https://www.nature.com/articles/s41598-023-41512-8) is an intriguing medical research. It leverages both LlamaIndex and ChatGPT to pinpoint AO codes within radiology reports, enhancing fracture classification. A fantastic fusion of tech and medicine! \n  ", "mimetype": "text/plain", "start_char_idx": 16415, "end_char_idx": 22904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76569759-779b-4f28-9338-f12cc3cd07cb": {"__data__": {"id_": "76569759-779b-4f28-9338-f12cc3cd07cb", "embedding": null, "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36c588af-23f5-402f-b06b-dc8285afc728", "node_type": "4", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "17f37ed6f67d2a57e53770adeb46a22d0962da911663e7e148819c62e8d4d4b0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b26a0614-6bf1-4cb2-aa5f-d7b11ad254d7", "node_type": "1", "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}, "hash": "80332e1fd8af56d35493898d24ba641a12c2d073d5567a9622b98463fdd177fe", "class_name": "RelatedNodeInfo"}}, "text": "2. [ SEC Insights AI ](https://www.secinsights.ai/) does SEC document analysis using LlamaIndex is on Product Hunt as the 5th product of the day. \n  3. [ RentEarth ](https://lablab.ai/event/autonomous-agents-hackathon/kbve/atlas) : an agent to build your own startup with an amazing 3D interface and LlamaIndex. \n\nIn wrapping up this edition of our LlamaIndex Update series, we\u2019re reminded of\nthe power of collaboration and innovation. From new features to integrations\nand tutorials, our mission to revolutionize the AI realm marches forward. To\nevery member of our community, thank you for your unwavering support and\nenthusiasm. Let\u2019s continue to elevate the world of AI together!\n\n", "mimetype": "text/plain", "start_char_idx": 22904, "end_char_idx": 23589, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "55887fe7-0029-444d-b5e8-de80cd66f4a5": {"__data__": {"id_": "55887fe7-0029-444d-b5e8-de80cd66f4a5", "embedding": null, "metadata": {"filename": "fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.md", "extension": ".md", "title": "Fine-Tuning a Linear Adapter for Any Embedding Model", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2f25b56e-7640-4445-adf3-27a80a5e62b2", "node_type": "4", "metadata": {"filename": "fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.md", "extension": ".md", "title": "Fine-Tuning a Linear Adapter for Any Embedding Model", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383"}, "hash": "c4917863fa86a62af2a02e0254bd75cf2bf169b6fdc8c51524e182cede9c9015", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e92d4bb3-fb4e-4147-beb1-a2e3fdaffa0d", "node_type": "1", "metadata": {}, "hash": "4c5a4e3e4792f1a143180f39b2dbd19f9f1acf0d844a02c9ac9eb5d1fc25b89a", "class_name": "RelatedNodeInfo"}}, "text": "We\u2019ve added capabilities in LlamaIndex allowing you to fine-tune a linear\nadapter on top of embeddings produced from _any_ model ( `\nsentence_transformers ` , OpenAI, and more).\n\nThis allows you to transform your embedding representations into a new latent\nspace that\u2019s optimized for retrieval over your specific data and queries. This\ncan lead to small increases in retrieval performance that in turn translate to\nbetter performing RAG systems.\n\nA nice bonus: you do _not_ need to re-embed your documents by using this\nadapter! Simply transform the query instead.\n\nWe have a [ full end-to-end guide ](https://gpt-\nindex.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding_adapter.html)\nshowing how you can generate a synthetic dataset, fine-tune the linear\nadapter, and evaluate its performance.\n\n#  Context\n\nThe concept of fine-tuning your embedding model is powerful. In fact, we were\ninspired to both add a [ full example repository ](https://github.com/run-\nllama/finetune-embedding) / [ blog post ](https://medium.com/llamaindex-\nblog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971) as well\nas [ native abstractions in LlamaIndex ](https://gpt-\nindex.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding.html)\nshowing how you can fine-tune a sentence_transformers model over any\nunstructured text corpus (with our ` SentenceTransformersFinetuneEngine ` ).\n\nHowever, this approach has some limitations:\n\n  * The ` SentenceTransformersFinetuneEngine ` is limited to fine-tuning ` sentence_transformers ` models. \n  * After finetuning the embedding model, you will need to re-embed your document corpus. \n\nDuring our [ Finetuning + RAG webinar\n](https://www.youtube.com/watch?v=mndiDJ5k26A) last Friday, Jo (Vespa)\nmentioned the exact same problem: fine-tuning the embeddings model requires\nyou to reindex your documents. However, his work with Vespa [ explored the\nconcept of \u201cfreezing\u201d document embeddings using a foundation model\n](https://blog.vespa.ai/tailoring-frozen-embeddings-with-vespa/) , and instead\ntraining a transformation on the query embedding.\n\nThis inspired us to explore a similar embedding fine-tuning approach that was\nsimultaneously more general but also allowed us to freeze existing document\nembeddings.\n\n#  Approach\n\nOur brand-new ` EmbeddingAdapterFinetuneEngine ` fine-tunes a **linear\nadapter** on top of query embeddings produced by any model. The **linear\nadapter** is simply a linear transformation that specifically transforms the\nquery embedding _while keeping document embeddings fixed_ .\n\nThe linear adapter can be used on top of any existing embeddings model: SBERT\nembeddings, OpenAI embeddings, Cohere embeddings, and more. As a result you\ncan just plug this in on top of any embedding model that you\u2019re already using!\n\nSince document embeddings are unchanged, this means that you can always fine-\ntune this linear adapter _after_ you\u2019ve generated embeddings for your\ndocuments. You can choose to arbitrarily re-train this adapter on top of\nchanging data distributions, without needing to re-embed all your documents.\n\n##  Technical Details\n\nAs mentioned above, the linear adapter simply performs a linear transformation\non top of the query embedding while keeping the Document embeddings fixed\n(with a weight matrix W + bias term b):\n\nAnd that\u2019s it! If document embeddings can be represented as a (n x d) matrix\nD, where n is number of documents and d is the embedding dimension, then\nembedding similarity is just measured by\n\nThe linear adapter is trained using a similar loss term as the `\nMultipleNegativesRankingLoss ` function in ` sentence_transformers ` \u2014 given a\nbatch of positive (question, context) examples, the function uses cross-\nentropy loss under the hood to penalize the ground-truth (question, context)\npairs for being far apart and swapped pairs for being too close.\n\n**Additional Notes:** We ended up writing the bulk of this fine-tuning logic\nin plain PyTorch, but taking heavy inspiration from the `\nsentence_transformers ` [ source code ](https://github.com/UKPLab/sentence-\ntransformers) . We couldn\u2019t use sentence_transformers directly since we take\nin embeddings as inputs rather than raw text. You can take a look at some of\nour training code here.\n\n#  Notebook Walkthrough\n\nIn this notebook walkthrough, we follow a similar set of steps as our [\nprevious blog post on embedding fine-tuning ](https://medium.com/llamaindex-\nblog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971) :\n\n  1. Generate a synthetic question-context dataset for both training and evaluation. \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4617, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e92d4bb3-fb4e-4147-beb1-a2e3fdaffa0d": {"__data__": {"id_": "e92d4bb3-fb4e-4147-beb1-a2e3fdaffa0d", "embedding": null, "metadata": {"filename": "fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.md", "extension": ".md", "title": "Fine-Tuning a Linear Adapter for Any Embedding Model", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2f25b56e-7640-4445-adf3-27a80a5e62b2", "node_type": "4", "metadata": {"filename": "fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.md", "extension": ".md", "title": "Fine-Tuning a Linear Adapter for Any Embedding Model", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383"}, "hash": "c4917863fa86a62af2a02e0254bd75cf2bf169b6fdc8c51524e182cede9c9015", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "55887fe7-0029-444d-b5e8-de80cd66f4a5", "node_type": "1", "metadata": {"filename": "fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.md", "extension": ".md", "title": "Fine-Tuning a Linear Adapter for Any Embedding Model", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383"}, "hash": "5e3289f5c59317a5a105bb5107e88564f9b27bb25da58c237ffa944567dc892d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f140c749-e4c0-4fe7-b5fa-0b816c2edb20", "node_type": "1", "metadata": {}, "hash": "91d6d5cf2e1ffc610c5abc363282468dc32c79e0c8ddf73aed6e25d2ab72b2e7", "class_name": "RelatedNodeInfo"}}, "text": "2. ", "mimetype": "text/plain", "start_char_idx": 4617, "end_char_idx": 4620, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f140c749-e4c0-4fe7-b5fa-0b816c2edb20": {"__data__": {"id_": "f140c749-e4c0-4fe7-b5fa-0b816c2edb20", "embedding": null, "metadata": {"filename": "fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.md", "extension": ".md", "title": "Fine-Tuning a Linear Adapter for Any Embedding Model", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2f25b56e-7640-4445-adf3-27a80a5e62b2", "node_type": "4", "metadata": {"filename": "fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.md", "extension": ".md", "title": "Fine-Tuning a Linear Adapter for Any Embedding Model", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383"}, "hash": "c4917863fa86a62af2a02e0254bd75cf2bf169b6fdc8c51524e182cede9c9015", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e92d4bb3-fb4e-4147-beb1-a2e3fdaffa0d", "node_type": "1", "metadata": {"filename": "fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.md", "extension": ".md", "title": "Fine-Tuning a Linear Adapter for Any Embedding Model", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383"}, "hash": "18875806598ec17122f5fae1bb290c870e6a30390904c41c5ca0db3d3516414d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a153adf2-d711-4f8a-a736-f80c61e80676", "node_type": "1", "metadata": {}, "hash": "5395149900876bbc536ecf9197a2875d533cba2d3e0d158bc31e56402fe6cb66", "class_name": "RelatedNodeInfo"}}, "text": "Fine-tuning our linear adapter on top of an existing model (e.g. SBERT) \n  3. ", "mimetype": "text/plain", "start_char_idx": 4620, "end_char_idx": 4698, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a153adf2-d711-4f8a-a736-f80c61e80676": {"__data__": {"id_": "a153adf2-d711-4f8a-a736-f80c61e80676", "embedding": null, "metadata": {"filename": "fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.md", "extension": ".md", "title": "Fine-Tuning a Linear Adapter for Any Embedding Model", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2f25b56e-7640-4445-adf3-27a80a5e62b2", "node_type": "4", "metadata": {"filename": "fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.md", "extension": ".md", "title": "Fine-Tuning a Linear Adapter for Any Embedding Model", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383"}, "hash": "c4917863fa86a62af2a02e0254bd75cf2bf169b6fdc8c51524e182cede9c9015", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f140c749-e4c0-4fe7-b5fa-0b816c2edb20", "node_type": "1", "metadata": {"filename": "fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.md", "extension": ".md", "title": "Fine-Tuning a Linear Adapter for Any Embedding Model", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383"}, "hash": "bb44a92c7a22e3e73e0dea7728d31aef2be3dc9fe2c6f8d0f9b1d3060681552d", "class_name": "RelatedNodeInfo"}}, "text": "Getting the embedding model, and evaluating it. \n\nAs with the previous post, we use the UBER and LYFT 10K as example data. We\nuse Lyft to generate our training dataset and Uber to generate our evaluation\ndataset.\n\nThe full guide is here: [ https://gpt-\nindex.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding_adapter.html\n](https://gpt-\nindex.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding_adapter.html)\n\n##  Generate a Synthetic Dataset for Trraining and Evaluation\n\nWe use our helper abstractions, ` generate_qa_embedding_pairs ` , to generate\nour training and evaluation dataset. This function takes in any set of text\nnodes (chunks) and generates a structured dataset containing (question,\ncontext) pairs.\n\n    \n    \n    from llama_index.finetuning import (\n        generate_qa_embedding_pairs,\n        EmbeddingQAFinetuneDataset,\n    )\n    \n    # generate\n    train_dataset = generate_qa_embedding_pairs(train_nodes)\n    val_dataset = generate_qa_embedding_pairs(val_nodes)\n    \n    # save\n    train_dataset.save_json(\"train_dataset.json\")\n    val_dataset.save_json(\"val_dataset.json\")\n    \n    # load \n    train_dataset = EmbeddingQAFinetuneDataset.from_json(\"train_dataset.json\")\n    val_dataset = EmbeddingQAFinetuneDataset.from_json(\"val_dataset.json\")\n\n##  Fine-tuning our Linear Adapter\n\nWe then fine-tune our linear adapter on top of an existing embedding model. We\nimport our new ` EmbeddingAdapterFinetuneEngine ` abstraction, which takes in\nan existing embedding model and a set of training parameters.\n\nIn this example we use the ` bge-small-en ` sentence-transformers model, but\nwe can also use any embedding model in LlamaIndex/LangChain.\n\n    \n    \n    from llama_index.finetuning import EmbeddingAdapterFinetuneEngine\n    from llama_index.embeddings import resolve_embed_model\n    import torch\n    \n    base_embed_model = resolve_embed_model(\"local:BAAI/bge-small-en\")\n    # alternative: use OpenAI\n    # from llama_index.embeddings import OpenAIEmbedding\n    # openai = OpenAIEmbedding()\n    \n    finetune_engine = EmbeddingAdapterFinetuneEngine(\n        train_dataset,\n        base_embed_model,\n        model_output_path=\"&lt;model_output_path&gt;\",\n        epochs=4,\n        verbose=True,\n        # can optionally pass along any parameters that go into `train_model`\n        # optimizer_class=torch.optim.SGD,\n        # optimizer_params={\"lr\": 0.01}\n    )\n\nWe can then call fine-tune to kick off the fine-tuning job. Training a linear\nmodel is quite straightforward and doesn\u2019t require heavy machinery \u2014 this can\neasily run on a Macbook.\n\n    \n    \n    finetune_engine.finetune()\n\n##  Getting the Embedding Model, and Evaluating it\n\nOnce the fine-tuning job is then, we can then fetch our embedding model.\n\nWe can either directly fetch it from our ` finetune_engine ` , or import our\nnew ` LinearAdapterEmbeddingModel ` and construct it in a more manual fashion.\n\nOption 1:\n\n    \n    \n    embed_model = finetune_engine.get_finetuned_model()\n\nOption 2:\n\n    \n    \n    from llama_index.embeddings import LinearAdapterEmbeddingModel\n    \n    embed_model = LinearAdapterEmbeddingModel(base_embed_model, \"&lt;model_output_path&gt;\")\n\nThe next step is to evaluate it. We compare the fine-tuned model against the\nbase model, as well as against ` text-embedding-ada-002 ` .\n\nWe evaluate with two ranking metrics:\n\n  * **Hit-rate metric:** For each (query, context) pair, we retrieve the top-k documents with the query. It\u2019s a _hit_ if the results contain the ground-truth context. \n  * **Mean Reciprocal Rank** : A slightly more granular ranking metric that looks at the \u201creciprocal rank\u201d of the ground-truth context in the top-k retrieved set. The reciprocal rank is defined as 1/rank. Of course, if the results don\u2019t contain the context, then the reciprocal rank is 0. \n\nSome additional comments:\n\n  * We ran with 4 epochs over the Lyft documents \n  * We used Adam as an optimizer with the default learning rate (we tried SGD and it didn\u2019t work as well) \n\n**Results**\n\nQuantiative metrics (hit-rate and MRR) for ada, bge, and our fine-tuned model\n\nIn terms of hit-rate, the base model gets 78.7% hit-rate on the validation\ndataset, and the fine-tuned model gets 79.8%. In the meantime ` text-\nembedding-ada-002 ` gets 87.0%.\n\nIn terms of MRR, the base model gets 64.3%, and the fine-tuned model gets 66%.\n` text-embedding-ada-002 ` gets 68.4%.\n\nThere is some performance bump from the fine-tuned model, though admittedly it\nis small \u2014 it is smaller than the performance bump gained through fine-tuning\nsentence_transformers directly on the latest dataset.\n\nThat said, a performance bump is still a performance bump, and it\u2019s very cheap\nfor you to spin up and try yourself! So you can decide whether or not this\nwould make sense for you.\n\n#  Conclusion\n\nWe created a brand-new module in LlamaIndex that allows you fine-tune a linear\nadapter on top of any embedding model.\n\nIt can help you eke out some marginal improvement in retrieval metrics;\nimportantly, it allows you to keep document embeddings fixed and only\ntransform the query.\n\n##  Resources\n\nGuide: [ https://gpt-\nindex.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding_adapter.html\n](https://gpt-\nindex.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding_adapter.html)\n\nTraining code (if you want to take a look for yourself): [\nhttps://github.com/jerryjliu/llama_index/blob/main/llama_index/finetuning/embeddings/adapter_utils.py\n](https://github.com/jerryjliu/llama_index/blob/main/llama_index/finetuning/embeddings/adapter_utils.py)\n\n", "mimetype": "text/plain", "start_char_idx": 4698, "end_char_idx": 10313, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8421e132-1b64-4199-be5e-ca2e7fa4f2fc": {"__data__": {"id_": "8421e132-1b64-4199-be5e-ca2e7fa4f2fc", "embedding": null, "metadata": {"filename": "chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c.md", "extension": ".md", "title": "ChatGPT\u2019s Knowledge is Two Years Old: What to do if you\u2019re building applications?", "date": "Sep 1, 2023", "url": "https://www.llamaindex.ai/blog/chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "03d5b93b-ec0f-4c92-9e23-4d7185e9e703", "node_type": "4", "metadata": {"filename": "chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c.md", "extension": ".md", "title": "ChatGPT\u2019s Knowledge is Two Years Old: What to do if you\u2019re building applications?", "date": "Sep 1, 2023", "url": "https://www.llamaindex.ai/blog/chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c"}, "hash": "d911d30092d8fdf0b279feb9700112d786c538afdf0255cdfdf8ebec40b49cf9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c6f5af0-d4ed-41b9-a34d-9aa87fd60111", "node_type": "1", "metadata": {}, "hash": "41f04fa8342121af13ad6a8fdbcd25290157d2a49f0d9e86bb011bd3a1a06087", "class_name": "RelatedNodeInfo"}}, "text": "It\u2019s official: as of today, ChatGPT\u2019s knowledge cutoff is 2 years old.\n\n> Happy 2nd birthday to ChatGPT's knowledge cutoff! [\n> pic.twitter.com/O1cgRPSP3l ](https://t.co/O1cgRPSP3l)\n>\n> \u2014 Yi Ding -- prod/acc (@yi_ding) [ September 1, 2023\n> ](https://twitter.com/yi_ding/status/1697589370222711081?ref_src=twsrc%5Etfw)\n\n#  Why doesn\u2019t OpenAI just update it?\n\nThere are some fundamental reasons for this: training new LLMs is an expensive\n\u2014 at least tens of millions of dollars \u2014 and not guaranteed process. Cleaning\nnew data sets for training is also expensive.\n\n#  What should I do if I\u2019m building an application that needs more recent\ndata?\n\nYou may be tempted to just send ChatGPT the entire wikipedia pages for 2022\nand 2023: [ https://en.wikipedia.org/wiki/2022\n](https://en.wikipedia.org/wiki/2022) You\u2019ll soon run into two limits: 1.\nthere is a limit on the number of words you can send to a large language model\n(LLM). This is called the \u201ccontext window.\u201d 2. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 967, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c6f5af0-d4ed-41b9-a34d-9aa87fd60111": {"__data__": {"id_": "0c6f5af0-d4ed-41b9-a34d-9aa87fd60111", "embedding": null, "metadata": {"filename": "chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c.md", "extension": ".md", "title": "ChatGPT\u2019s Knowledge is Two Years Old: What to do if you\u2019re building applications?", "date": "Sep 1, 2023", "url": "https://www.llamaindex.ai/blog/chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "03d5b93b-ec0f-4c92-9e23-4d7185e9e703", "node_type": "4", "metadata": {"filename": "chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c.md", "extension": ".md", "title": "ChatGPT\u2019s Knowledge is Two Years Old: What to do if you\u2019re building applications?", "date": "Sep 1, 2023", "url": "https://www.llamaindex.ai/blog/chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c"}, "hash": "d911d30092d8fdf0b279feb9700112d786c538afdf0255cdfdf8ebec40b49cf9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8421e132-1b64-4199-be5e-ca2e7fa4f2fc", "node_type": "1", "metadata": {"filename": "chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c.md", "extension": ".md", "title": "ChatGPT\u2019s Knowledge is Two Years Old: What to do if you\u2019re building applications?", "date": "Sep 1, 2023", "url": "https://www.llamaindex.ai/blog/chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c"}, "hash": "bc19a32d0c3bc47219f03b9650fe9899c0ceb96b0eea7b20d1cc1eb54cc45134", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72f51578-3ec9-49bf-a46e-8968f52b7951", "node_type": "1", "metadata": {}, "hash": "9068aacc085e7920290c91a773b02f9429d1a0ade2cf61a04156a305fe4d14cf", "class_name": "RelatedNodeInfo"}}, "text": "LLM APIs charge you by the\nword, so the more you send it, the more expensive your API calls become.\n\nThe standard technique is one called \u201cRetrieval Augmented Generation\u201d or RAG.\nWhat it is, boiled down very simply, is a process of searching for the right\ncontext, giving that context to the LLM, and then getting better results back.\n\n> What\u2019s Retrieval Augmented Generation? ", "mimetype": "text/plain", "start_char_idx": 967, "end_char_idx": 1344, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72f51578-3ec9-49bf-a46e-8968f52b7951": {"__data__": {"id_": "72f51578-3ec9-49bf-a46e-8968f52b7951", "embedding": null, "metadata": {"filename": "chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c.md", "extension": ".md", "title": "ChatGPT\u2019s Knowledge is Two Years Old: What to do if you\u2019re building applications?", "date": "Sep 1, 2023", "url": "https://www.llamaindex.ai/blog/chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "03d5b93b-ec0f-4c92-9e23-4d7185e9e703", "node_type": "4", "metadata": {"filename": "chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c.md", "extension": ".md", "title": "ChatGPT\u2019s Knowledge is Two Years Old: What to do if you\u2019re building applications?", "date": "Sep 1, 2023", "url": "https://www.llamaindex.ai/blog/chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c"}, "hash": "d911d30092d8fdf0b279feb9700112d786c538afdf0255cdfdf8ebec40b49cf9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c6f5af0-d4ed-41b9-a34d-9aa87fd60111", "node_type": "1", "metadata": {"filename": "chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c.md", "extension": ".md", "title": "ChatGPT\u2019s Knowledge is Two Years Old: What to do if you\u2019re building applications?", "date": "Sep 1, 2023", "url": "https://www.llamaindex.ai/blog/chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c"}, "hash": "e466e31c92ba06d05968d5d1f519eefa28ed7f5cc823a8a7c6a48fe2eac96cd0", "class_name": "RelatedNodeInfo"}}, "text": "Search, Give, Get. For those of us\n> coming from a traditional software development background RAG can sound\n> intimidating, but it really is a simple concept: Search for the relevant\n> data Give the data to GPT Get a better response Of course,\u2026\n>\n> \u2014 Yi Ding -- prod/acc (@yi_ding) [ July 28, 2023\n> ](https://twitter.com/yi_ding/status/1684765549929332736?ref_src=twsrc%5Etfw)\n\nAt [ LlamaIndex ](https://llamaindex.ai) we are the RAG experts, but there is\na whole community of open source projects that are tackling this problem. We\nhave integrated with over 20 open source vector databases and there are other\nopen source tools like LangChain, Semantic Kernel, DSPy, Axilla and others\n(put your favorites in the comments!) that are attacking the problem in\ndifferent ways.\n\nAnother technique is called fine tuning. Here, you essentially create a new\ncustom model on top of an existing LLM. While LlamaIndex does support fine\ntuning, it often requires much more work and data:\n\n> We are big fans of fine tuning and custom models but knowing when to use RAG\n> and when to use fine tuning, and how to use them in combination, is\n> essential. Watch this space! [ https://t.co/vTpWauhj3C\n> ](https://t.co/vTpWauhj3C)\n>\n> \u2014 LlamaIndex (@llama_index) [ August 18, 2023\n> ](https://twitter.com/llama_index/status/1692570383201812710?ref_src=twsrc%5Etfw)\n\n#  What if I don\u2019t need more recent data?\n\nThat\u2019s totally OK! Not every application needs data that\u2019s more recent than\n2021. Before LlamaIndex, I worked on an open source reading education tool,\nand phonics have definitely not changed in the last two years. If you\u2019re\nbuilding something to write bedtime stories ( Kidgeni [ https://kidgeni.com/\n](https://kidgeni.com/) ) or raps (check out TextFX! [\nhttps://textfx.withgoogle.com/ ](https://textfx.withgoogle.com/) ) your\napplication\n\n#  What if I just want to use ChatGPT with more recent information?\n\nThere are a lot of chatbots that use Retrieval Augmented Generation currently.\nA few of the ones I\u2019ve personally tried are Metaphor [\nhttps://metaphor.systems/ ](https://metaphor.systems/) , Perplexity [\nhttps://www.perplexity.ai/ ](https://www.perplexity.ai/) and Medisearch [\nhttps://medisearch.io/ ](https://medisearch.io/) , and of course Google Bard\nand BingGPT.\n\n", "mimetype": "text/plain", "start_char_idx": 1344, "end_char_idx": 3617, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79e6915a-78a2-48b0-a216-2cbd9e0ae05f": {"__data__": {"id_": "79e6915a-78a2-48b0-a216-2cbd9e0ae05f", "embedding": null, "metadata": {"filename": "introducing-airbyte-sources-within-llamaindex-42209071722f.md", "extension": ".md", "title": "Introducing Airbyte sources within LlamaIndex", "date": "Aug 29, 2023", "url": "https://www.llamaindex.ai/blog/introducing-airbyte-sources-within-llamaindex-42209071722f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f6e5d8be-352a-484e-af22-cdf2841640b5", "node_type": "4", "metadata": {"filename": "introducing-airbyte-sources-within-llamaindex-42209071722f.md", "extension": ".md", "title": "Introducing Airbyte sources within LlamaIndex", "date": "Aug 29, 2023", "url": "https://www.llamaindex.ai/blog/introducing-airbyte-sources-within-llamaindex-42209071722f"}, "hash": "09d1d1d15c2e4aeee687ac917b73d9dcafc02f97d373b1ac174dca0993a834b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ddc887d-82d7-437a-9f2d-9595ab84a39f", "node_type": "1", "metadata": {}, "hash": "ea78781d4bff779d97a6944dab91ae412f6e9be38d899a2f55aa3536c85e7175", "class_name": "RelatedNodeInfo"}}, "text": "Authored by Joe Reuter, Software Engineer at Airbyte\n\n(cross-posted from the Airbyte blog; check it out [ here\n](https://airbyte.com/blog/introducing-airbyte-sources-within-llamaindex) !)\n\n#  Content\n\nIt\u2019s now possible to utilize the Airbyte sources for [ Gong\n](https://llamahub.ai/l/airbyte_gong) , [ Hubspot\n](https://llamahub.ai/l/airbyte_hubspot) , [ Salesforce\n](https://llamahub.ai/l/airbyte_salesforce) , [ Shopify\n](https://llamahub.ai/l/airbyte_shopify) , [ Stripe\n](https://llamahub.ai/l/airbyte_stripe) , [ Typeform\n](https://llamahub.ai/l/airbyte_typeform) and [ Zendesk Support\n](https://llamahub.ai/l/airbyte_zendesk_support) directly within your\nLlamaIndex-based application, implemented as [ data loaders ](https://gpt-\nindex.readthedocs.io/en/latest/core_modules/data_modules/connector/usage_pattern.html)\n.\n\nFor example, to load the Stripe invoices for a user, you can use the\nAirbyteStripeLoader. Installing it is super simple, when you have LlamaIndex\ninstalled locally you only need to install the source you are interested in,\nand you are ready to go:\n\n    \n    \n    pip install airbyte-source-stripe\n    pip install llama-hub\n\nAfter that, simply download the loader and pass in configuration and the\nstream you want to load:\n\n    \n    \n    from llama_hub.airbyte_stripe.base import AirbyteStripeReader\n    \n    config = {\n      \"client_secret\": \"&lt;secret key&gt;\",\n      \"account_id\": \"&lt;account id&gt;\",\n      \"start_date\": \"&lt;date from which to start retrieving records from in ISO format, e.g. 2020\u201310\u201320T00:00:00Z&gt;\"\n    }\n    reader = AirbyteStripeReader(config=config)\n    documents = reader.load_data(stream_name=\"invoices\")\n\n#  Why does this matter?\n\nThis is the beginning of making Airbyte\u2019s [ 300+ sources\n](http://docs.airbyte.com/integrations) available as data loaders in LlamaHub.\n\nAirbyte can move data from just about any source to your warehouse or vector\ndatabase to power your LLM use case (check out this [ tutorial\n](https://airbyte.com/tutorials/chat-with-your-data-using-openai-pinecone-\nairbyte-and-langchain) for setting up such a data pipeline!). This is normally\ndone by using Airbyte Cloud or a local Airbyte instance, setting up a\nconnection, and running it on a schedule (or via API trigger) to make sure\nyour data stays fresh.\n\nBut if you are just getting started and are running everything locally, using\na full Airbyte instance (including the UI, scheduling service, scale-out\ncapabilities, etc..) may be overkill.\n\nWith this release, it\u2019s easier than ever to run any Python-based source in\nLlamaIndex directly within your Python runtime \u2014 no need to spin up an Airbyte\ninstance or make API calls to Airbyte Cloud.\n\n#  Moving between hosted and embedded Airbyte\n\nSince the same code is running under the hood, every Airbyte-built loader is\ncompatible with the respective source in the Airbyte service. This means it\u2019s\ntrivial to lift your embedded loading pipeline into your self-hosted Airbyte\ninstallation or your Airbyte Cloud instance. The schema of the loader\nconfiguration object and that of the output records is 100% compatible.\n\nRunning syncs on hosted Airbyte means:\n\n  * UI to keep track of running pipelines \n  * Event notifications including alerting on failing syncs or running post-sync operations \n  * Easily running pipelines on a schedule \n  * Scale-out capabilities \n  * API to power programmatic use cases \n  * Out-of-the-box state management of your connections \n  * Support \n  * And more \n\nRunning syncs with LlamaIndex loaders means:\n\n  * No overhead for running yet another service \n  * Full control over timing and pipeline execution \n\n#  Combining Airbyte loaders with indices and query engines\n\nAs Airbyte loaders are behaving like regular loaders, they can easily be\ncombined with all LlamaIndex utilities to build powerful LLM-based\napplications:\n\n    \n    \n    relevant_keys = [\"customer_name\", \"total\", \"currency\"]\n    reader = AirbyteStripeReader(\n        config=strip_config,\n        record_handler=lambda record, id: Document(\n            doc_id=id,\n            text=record.data[\"description\"] or \"\",\n            extra_info={\n                key: record.data[key] for key in relevant_keys if key in record.data\n            },\n        ),\n    )\n    \n    index = ListIndex.from_documents(reader.load_data(stream_name=\"invoices\"))\n    query_engine = index.as_query_engine()\n    question = input(\"What do you want to know about your customers?\")\n    print(query_engine.query(question))\n\n#  Incremental loads\n\nSince your python application is basically acting as the Airbyte platform, you\nhave full control over how the \u201csync\u201d is executed. For example you can still\nbenefit from [ incremental syncs\n](https://glossary.airbyte.com/term/incremental-synchronization/) if your\nstream supports it by accessing the \u201clast_state\u201d property of the loader. This\nallows you to load only documents that changed since the last time you loaded,\nallowing you to update an existing vector database effectively:\n\n    \n    \n    import airbyte_cdk.models.airbyte_protocol import AirbyteMessage\n    with open('stripe_sync_checkpoint.json', 'w') as file:\n      file.write(reader.last_state.json())\n    \n    # later\n    with open('stripe_sync_checkpoint.json', 'r') as file:\n      current_state = AirbyteStateMessage.parse_raw(file.read())\n    new_docs = reader.load_data(stream_name=\"invoices\", state=current_state)\n\n#  Mapping Airbyte records to LlamaIndex documents\n\nBy default, each record gets mapped to a Document as part of the loader, with\nall the various fields in the record becoming a part of the `extra_info`\nproperty of the Document (the `extra_info` represents structured metadata for\neach document) . The text portion of the document is set to the JSON\nrepresentation of the record. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5768, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ddc887d-82d7-437a-9f2d-9595ab84a39f": {"__data__": {"id_": "4ddc887d-82d7-437a-9f2d-9595ab84a39f", "embedding": null, "metadata": {"filename": "introducing-airbyte-sources-within-llamaindex-42209071722f.md", "extension": ".md", "title": "Introducing Airbyte sources within LlamaIndex", "date": "Aug 29, 2023", "url": "https://www.llamaindex.ai/blog/introducing-airbyte-sources-within-llamaindex-42209071722f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f6e5d8be-352a-484e-af22-cdf2841640b5", "node_type": "4", "metadata": {"filename": "introducing-airbyte-sources-within-llamaindex-42209071722f.md", "extension": ".md", "title": "Introducing Airbyte sources within LlamaIndex", "date": "Aug 29, 2023", "url": "https://www.llamaindex.ai/blog/introducing-airbyte-sources-within-llamaindex-42209071722f"}, "hash": "09d1d1d15c2e4aeee687ac917b73d9dcafc02f97d373b1ac174dca0993a834b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79e6915a-78a2-48b0-a216-2cbd9e0ae05f", "node_type": "1", "metadata": {"filename": "introducing-airbyte-sources-within-llamaindex-42209071722f.md", "extension": ".md", "title": "Introducing Airbyte sources within LlamaIndex", "date": "Aug 29, 2023", "url": "https://www.llamaindex.ai/blog/introducing-airbyte-sources-within-llamaindex-42209071722f"}, "hash": "919e2c156d0857482e03490ff68f7d80fbde0ae109010c33103252e59cd495a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e1821c9-0c49-4c8b-85a2-09ca1777f1bd", "node_type": "1", "metadata": {}, "hash": "a1b7f8e051bc9365afcf2cf431c6b7de5e24c9a0461e2ca71389f083b12ae231", "class_name": "RelatedNodeInfo"}}, "text": "By default, any metadata defined on the Document\nwill be concatenated with the text in downstream modules, so all the fields in\nthe record will be used for embedding and synthesis purposes within a\nLlamaIndex app. You can pass in a record handler to customize this behavior to\nbuild the text part of a record depending on the data:\n\n    \n    \n    def handle_record(record, id):\n      return Document(doc_id=id, text=record.data[\"title\"], extra_info=record.data)\n    reader = AirbyteGongReader(config=gong_config, record_handler=handle_record)\n\n#  Custom sources\n\nFor now, the following Airbyte sources are available as pip packages (with\nmore to come):\n\n  * [ Gong ](https://llamahub.ai/l/airbyte_gong) pip install airbyte-source-gong \n  * [ Hubspot ](https://llamahub.ai/l/airbyte_hubspot) pip install airbyte-source-hubspot \n  * [ Salesforce ](https://llamahub.ai/l/airbyte_salesforce) pip install airbyte-source-salesforce \n  * [ Shopify ](https://llamahub.ai/l/airbyte_shopify) pip install airbyte-source-shopify \n  * [ Stripe ](https://llamahub.ai/l/airbyte_stripe) pip install airbyte-source-stripe \n  * [ Typeform ](https://llamahub.ai/l/airbyte_typeform) pip install airbyte-source-typeform \n  * [ Zendesk Support ](https://llamahub.ai/l/airbyte_zendesk_support) pip install airbyte-source-zendesk-support \n\nHowever, if you have implemented your own custom Airbyte sources, it\u2019s also\npossible to integrate them by using the AirbyteCDKReader base class that works\nwith the Source interface of the Airbyte CDK:\n\n    \n    \n    from llama_index import download_loader\n    from my_source.source import MyCustomSource # plug in your own source here\n    AirbyteCDKReader = download_loader(AirbyteCDKReader)\n    config = {\n      # your custom configuration\n    }\n    reader = AirbyteCDKReader(source_class=MyCustomSource, config=config)\n    documents = reader.load_data(stream_name=\"my-stream\")\n\nYou can also install sources from the main Airbyte repository by installing\ndirectly via git \u2014 for example, to fetch the Github source, simply run\n\n    \n    \n    pip install \"source_github@git+https://github.com/airbytehq/airbyte.git@master#subdirectory=airbyte-integrations/connectors/source-github\"\n\nAfter that, the source is available to be plucked into the AirbyteCDKLoader:\n\n    \n    \n    from source_github.source import SourceGithub\n    issues_loader = AirbyteCDKReader(source_class=SourceGithub, config=config)\n    documents = reader.load_data(stream_name=\"issues\")\n\nCheck out [ the connector development documentation\n](https://docs.airbyte.com/connector-development/) for how to get started\nwriting your own sources \u2014 it\u2019s easy to get started with them and will allow\nyou to move from local embedded loaders to using a hosted Airbyte instance\nseamlessly depending on your needs.\n\n#  Any questions? ", "mimetype": "text/plain", "start_char_idx": 5768, "end_char_idx": 8572, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e1821c9-0c49-4c8b-85a2-09ca1777f1bd": {"__data__": {"id_": "5e1821c9-0c49-4c8b-85a2-09ca1777f1bd", "embedding": null, "metadata": {"filename": "introducing-airbyte-sources-within-llamaindex-42209071722f.md", "extension": ".md", "title": "Introducing Airbyte sources within LlamaIndex", "date": "Aug 29, 2023", "url": "https://www.llamaindex.ai/blog/introducing-airbyte-sources-within-llamaindex-42209071722f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f6e5d8be-352a-484e-af22-cdf2841640b5", "node_type": "4", "metadata": {"filename": "introducing-airbyte-sources-within-llamaindex-42209071722f.md", "extension": ".md", "title": "Introducing Airbyte sources within LlamaIndex", "date": "Aug 29, 2023", "url": "https://www.llamaindex.ai/blog/introducing-airbyte-sources-within-llamaindex-42209071722f"}, "hash": "09d1d1d15c2e4aeee687ac917b73d9dcafc02f97d373b1ac174dca0993a834b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ddc887d-82d7-437a-9f2d-9595ab84a39f", "node_type": "1", "metadata": {"filename": "introducing-airbyte-sources-within-llamaindex-42209071722f.md", "extension": ".md", "title": "Introducing Airbyte sources within LlamaIndex", "date": "Aug 29, 2023", "url": "https://www.llamaindex.ai/blog/introducing-airbyte-sources-within-llamaindex-42209071722f"}, "hash": "5fd22dcfd32db88a4762cf504954d87a92cfffa6be939341030a3f22cc02af12", "class_name": "RelatedNodeInfo"}}, "text": "We would love to hear from you\n\nIf you are interested in leveraging Airbyte to ship data to your LLM-based\napplications, [ please take a moment\n](https://docs.google.com/forms/d/e/1FAIpQLSduobMZwbqiFlPxsWDG-\nhrBw6NLYMDu_7zRfo4j7AsaO1QtfQ/viewform?usp=sf_link&_gl=1*m4v6ic*_ga*MTM4ODAyNjg4NS4xNjY5ODkyNDQ1*_ga_HDBMVFQGBH*MTY5MjM2MzY0Ni45NS4xLjE2OTIzNjU2NDUuMC4wLjA.)\nto fill out our survey so we can make sure to prioritize the most important\nfeatures.\n\nIf you have questions or are interested in other existing sources being\nexposed as loaders this way, do not hesitate to reach out on our [ community\nslack channel ](https://airbyte.com/community/community) or in the [\nintegrations channel\n](https://discord.com/channels/1059199217496772688/1100459663696334968) on the\nLlamaIndex discord server.\n\n", "mimetype": "text/plain", "start_char_idx": 8572, "end_char_idx": 9371, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e4bbdf3b-8778-4a14-a285-40d6e24868e3": {"__data__": {"id_": "e4bbdf3b-8778-4a14-a285-40d6e24868e3", "embedding": null, "metadata": {"filename": "llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.md", "extension": ".md", "title": "LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases", "date": "Aug 29, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3035a753-bc13-4060-b9c2-2b453fe5b319", "node_type": "4", "metadata": {"filename": "llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.md", "extension": ".md", "title": "LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases", "date": "Aug 29, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af"}, "hash": "c445031e9546f5e380d22ec3d938e2b2ce467663d5267b9748a392daaccc6349", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3b040b3-4724-4cc0-b9f7-70ae0b3b7813", "node_type": "1", "metadata": {}, "hash": "ed3b2e75bdf77a99223e2d5dc0a362307ce732447e7526d98eb589b51ec93c58", "class_name": "RelatedNodeInfo"}}, "text": "#  Introduction:\n\nIn the world of IT and Software Development, knowledge transfer (KT) stands\nout as a big challenge. Whether it\u2019s new hires trying to understand their\nroles, folks on their notice periods aiming for a smooth handover, or the\ndaily tasks of developers and product specialists adapting to ever-changing\nprojects \u2014 the KT process often leads to stress and worry.\n\nThis gets more complicated with information spread out everywhere, the mix of\nnew and old tech, and the fast pace of IT and Software Development projects.\nIn this situation, broken bits of knowledge become the norm, causing delays,\nmisunderstandings, and making learning harder.\n\nBut amidst these challenges, might there be a beacon of optimism shining\nthrough?\n\n[ Vibhav ](https://www.linkedin.com/in/vibhavagarwal5/) and I have developed a\nsystem that seamlessly organizes KT sessions. By leveraging personal images,\nwe generate video explanations that are paired with individual code snippets,\nmaking the code far more comprehensible. Our innovative approach was\nrecognized when we secured the First Prize at the Google Cloud, Searce, and\nLifeSight hackathon. With the combined strengths of LlamaIndex and D-ID, our\naim is not just to consolidate information but also to simplify tasks and\nelevate the KT process. In doing so, we\u2019re transforming a daunting industry\nchallenge into a straightforward and manageable endeavor.\n\nWant to see how LlamaIndex plays a key role in this change?\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1467, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d3b040b3-4724-4cc0-b9f7-70ae0b3b7813": {"__data__": {"id_": "d3b040b3-4724-4cc0-b9f7-70ae0b3b7813", "embedding": null, "metadata": {"filename": "llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.md", "extension": ".md", "title": "LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases", "date": "Aug 29, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3035a753-bc13-4060-b9c2-2b453fe5b319", "node_type": "4", "metadata": {"filename": "llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.md", "extension": ".md", "title": "LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases", "date": "Aug 29, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af"}, "hash": "c445031e9546f5e380d22ec3d938e2b2ce467663d5267b9748a392daaccc6349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4bbdf3b-8778-4a14-a285-40d6e24868e3", "node_type": "1", "metadata": {"filename": "llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.md", "extension": ".md", "title": "LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases", "date": "Aug 29, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af"}, "hash": "c3e562f3bd408f2f1163a9b4a0cac8371b446a8fbe8fc69aa3273ed10789071b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa7e0b9d-4c2e-4914-9ba2-cc381ce1bfbe", "node_type": "1", "metadata": {}, "hash": "8323e02365fa6d0c376d7c2b8f28eb86935ee09412c9c98a7524bd41fd146363", "class_name": "RelatedNodeInfo"}}, "text": "Let\u2019s dive in together!\n\n#  Solution:\n\nThe solution has four stages:\n\n##  Code Parsing:\n\n  * Break down the code base into individual code snippets or blocks. \n\n##  Summary and Explanation Generation with LlamaIndex:\n\n  * Produce a comprehensive summary of the entire code base. \n  * Create detailed explanations for each individual code block using LlamaIndex. \n\n##  Video Creation with D-ID:\n\n  * Generate videos using text-to-speech capabilities provided by D-ID. \n\n##  Video-Code Integration:\n\n  * Seamlessly stitch together the individual code blocks with their corresponding generated videos. \n\nLet\u2019s dive into each stage in detail.\n\n", "mimetype": "text/plain", "start_char_idx": 1467, "end_char_idx": 2107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa7e0b9d-4c2e-4914-9ba2-cc381ce1bfbe": {"__data__": {"id_": "aa7e0b9d-4c2e-4914-9ba2-cc381ce1bfbe", "embedding": null, "metadata": {"filename": "llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.md", "extension": ".md", "title": "LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases", "date": "Aug 29, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3035a753-bc13-4060-b9c2-2b453fe5b319", "node_type": "4", "metadata": {"filename": "llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.md", "extension": ".md", "title": "LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases", "date": "Aug 29, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af"}, "hash": "c445031e9546f5e380d22ec3d938e2b2ce467663d5267b9748a392daaccc6349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3b040b3-4724-4cc0-b9f7-70ae0b3b7813", "node_type": "1", "metadata": {"filename": "llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.md", "extension": ".md", "title": "LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases", "date": "Aug 29, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af"}, "hash": "2cb76c0896290422889de5e51739daf2f0f4360389521e07452e95d1bf7283b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "715a4591-4278-4d67-8ab7-1d9ddaee2340", "node_type": "1", "metadata": {}, "hash": "d04db0e3c8151a5239b27fc87d06f7d3ab9c5a86348221243f034a49dc361bc2", "class_name": "RelatedNodeInfo"}}, "text": "##  1\\. Code Parsing: Breaking Down the Code\n\nCode Parser\n\nUnderstanding a code base starts with a high-level summary, but the true depth\nlies in individual snippets or blocks. However, using entire code bases for\nexplanations can overwhelm language models like LLMs, causing them to either\nexceed token limits or miss key details.\n\nOur approach is simple yet efficient: break the code into digestible sections\nlike import statements, classes, initializer functions, and methods without\nlosing the code\u2019s flow. This segmentation is done through a dependency graph\napproach, utilizing Python\u2019s ` ast ` library. By analyzing the code's\nstructure, we can extract classes, their docstrings, initializers, and other\nmethods. This method not only captures the essence of each segment but is also\nflexible, allowing for further rules to extract additional code components.\n\nThe ` code_parser ` class embodies this strategy. It navigates the code,\ndistinguishing module-level functions from class-nested ones, and arranges\nthem systematically. The result? A granular yet comprehensive view of the\ncode, paving the way for precise and context-rich explanations.\n\n##  2\\. Summary and Explanation Generation with LlamaIndex\n\n**Producing a Comprehensive Summary:**\n\nThe initial step in understanding a code base is to grasp its overall essence.\nThis is achieved by generating a concise summary that gives a bird\u2019s-eye view\nof the entire code. LlamaIndex\u2019s Summary [ Index ](https://gpt-\nindex.readthedocs.io/en/latest/core_modules/data_modules/index/index_guide.html)\nhas been tailored for this exact task. In SummaryIndex, each block of code is\ntreated as a node. By inputting the structured blocks obtained from our code\nparsing phase into SummaryIndex, we can produce a comprehensive snapshot that\nserves as a summary of the entire code base.\n\n**Detailed Explanations for Individual Code Blocks:**\n\nWith a general understanding established, the next step is to delve into the\nfiner details. Starting from import statements, progressing to functions, and\neventually diving into classes and initializer functions, every block gets its\ndue attention. Here, LlamaIndex\u2019s ` accumulate ` response mode is a valuable\nasset, providing in-depth explanations for each block.\n\nHowever, a challenge arises. While ` accumulate ` provides in-depth insights\ninto each block, it can occasionally miss the broader context offered by\npreceding blocks. To address this limitation, we\u2019ve adopted a two-pronged\napproach. As depicted in the subsequent architecture, we employ two\nSummaryIndices for this endeavor.\n\n  ", "mimetype": "text/plain", "start_char_idx": 2107, "end_char_idx": 4693, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "715a4591-4278-4d67-8ab7-1d9ddaee2340": {"__data__": {"id_": "715a4591-4278-4d67-8ab7-1d9ddaee2340", "embedding": null, "metadata": {"filename": "llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.md", "extension": ".md", "title": "LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases", "date": "Aug 29, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3035a753-bc13-4060-b9c2-2b453fe5b319", "node_type": "4", "metadata": {"filename": "llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.md", "extension": ".md", "title": "LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases", "date": "Aug 29, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af"}, "hash": "c445031e9546f5e380d22ec3d938e2b2ce467663d5267b9748a392daaccc6349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa7e0b9d-4c2e-4914-9ba2-cc381ce1bfbe", "node_type": "1", "metadata": {"filename": "llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.md", "extension": ".md", "title": "LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases", "date": "Aug 29, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af"}, "hash": "50a188478fdeb842eed1b42c00f1ca56a18324ccc35bd6107298198fd791f022", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58ac2940-929a-4289-9e94-c4c603682483", "node_type": "1", "metadata": {}, "hash": "e60032dd6e9ab2b4c1f3566838d8937c3587c60e83e332ea526d9b779d7521ae", "class_name": "RelatedNodeInfo"}}, "text": "1. We utilize the first SummaryIndex to generate a concise summary for each block, treating each block as a Node in SummaryIndex. \n  2. For the second SummaaryIndex in the stack, we feed the summarized context from one node into the next. This ensures every node benefits from the context of its predecessor. We then harness the ` accumulate ` mode to provide detailed explanations, making certain that every segment of the code is explained comprehensively, preserving the broader perspective. The outcome? A deep, contextually rich interpretation of each code section. \n\nNote: We utilized Google\u2019s PaLM API in conjunction with LlamaIndex to generate\nsummaries and explanations. Alternatively, models like GPT-3.5, GPT-4, or\nother LLM\u2019s can be employed for this purpose.\n\n", "mimetype": "text/plain", "start_char_idx": 4693, "end_char_idx": 5466, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58ac2940-929a-4289-9e94-c4c603682483": {"__data__": {"id_": "58ac2940-929a-4289-9e94-c4c603682483", "embedding": null, "metadata": {"filename": "llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.md", "extension": ".md", "title": "LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases", "date": "Aug 29, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3035a753-bc13-4060-b9c2-2b453fe5b319", "node_type": "4", "metadata": {"filename": "llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.md", "extension": ".md", "title": "LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases", "date": "Aug 29, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af"}, "hash": "c445031e9546f5e380d22ec3d938e2b2ce467663d5267b9748a392daaccc6349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "715a4591-4278-4d67-8ab7-1d9ddaee2340", "node_type": "1", "metadata": {"filename": "llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.md", "extension": ".md", "title": "LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases", "date": "Aug 29, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af"}, "hash": "56f42a70e97ab6c2c3f3f0cbe6b0e2c827367147820c545d57a05e38d46f877f", "class_name": "RelatedNodeInfo"}}, "text": "##  3\\. Video Creation with D-ID:\n\nAfter carefully crafting summaries and detailed explanations for each code\nblock, it\u2019s essential to convey this information in a captivating and\naccessible manner. Videos, given their dynamic appeal, have the power to make\nintricate code explanations clearer and more engaging. This is where D-ID\ncomes into play.\n\nWith the prowess of D-ID\u2019s cutting-edge technology, we\u2019re able to create\nrealistic videos where avatars \u2014 whether they\u2019re of us or another chosen\nfigure \u2014 articulate each code block. Now, what brings these avatars to life?\nThe answer lies in Microsoft\u2019s text-to-speech synthesizer. This tool takes our\ndetailed textual explanations and transforms them into natural, fluent speech.\nThus, with D-ID, we\u2019re not just generating video but also integrating audio,\nculminating in a comprehensive and fluid video explanation.\n\nTo see this in action, let\u2019s take a look at a sample output.\n\n##  4\\. Video-Code Integration:\n\nAfter generating insightful videos with avatars elucidating the code and\nhaving our individual code snippets ready, the next crucial step is to marry\nthese two elements. This fusion ensures that viewers receive an immersive\nvisual experience, where they can simultaneously watch the explanation and\nobserve the related code.\n\nTo achieve this, we employed the ` carbon ` library, which transforms our code\nsnippets into visually appealing images. These images, when presented side-by-\nside with our explanatory videos, offer a clearer understanding of the code in\nfocus. The final touch is added with the ` moviepy ` library, which seamlessly\nstitches the video and code images together, ensuring a smooth and integrated\nvisual flow. Below, you'll find a sample illustrating this compelling\ncombination.\n\n#  Final Automatic Knowledge Transfer (KT) Generated Video\n\nFollowing our detailed process, we\u2019ve crafted a KT video where Jerry explains\nthe ChatEngine code base of LlamaIndex. Watch the video below to see it all\ncome together!\n\nCode Repository: [ https://github.com/ravi03071991/KT_Generator\n](https://github.com/ravi03071991/KT_Generator)\n\n#  Conclusion\n\nThrough this post, we\u2019ve showcased the transformative potential of LlamaIndex\nin creating Knowledge Transfer (KT) Videos for code bases. It\u2019s genuinely\nremarkable to envision the advancements we\u2019re making in this space. The\nmethodology we\u2019ve adopted is language-neutral, allowing flexibility in\nadapting to various code bases. With some tweaks to the code parsing phase, we\nbelieve it\u2019s feasible to scale this to cover expansive code repositories\nwithin organizations. Imagine a platform akin to YouTube, perhaps\n**KodeTube(KT)** , where an organization\u2019s entire codebase is cataloged\nthrough explanatory videos. The horizon is bright with the opportunities\nLlamaIndex brings, and we\u2019re thrilled about the journey ahead.\n\n", "mimetype": "text/plain", "start_char_idx": 5466, "end_char_idx": 8314, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2dcb4780-e9c1-456a-8ba1-a41033e5772d": {"__data__": {"id_": "2dcb4780-e9c1-456a-8ba1-a41033e5772d", "embedding": null, "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ffc1e5ca-3684-4aba-b4bb-8e22550bbbd3", "node_type": "4", "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "f733d11d3ca9cc9e059982e27f234bfd7c3adc087650da7b664397e36dd130ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "87fcb361-7153-4ff3-9919-035be55520e9", "node_type": "1", "metadata": {}, "hash": "9ff7b831e33c7201dea93a7f950fbab8bd2b707006b0583910c6d99a322cf549", "class_name": "RelatedNodeInfo"}}, "text": "**UPDATE 9/10/2023:** We\u2019ve included embedding finetuning abstractions into\nthe LlamaIndex repo, so this repo is technically outdated! Please check out\nour [ embedding fine-tuning guides ](https://gpt-\nindex.readthedocs.io/en/latest/end_to_end_tutorials/finetuning.html#finetuning-\nembeddings-for-better-retrieval-performance) in the core documentation.\n\nWe\u2019ve created a [ comprehensive, end-to-end guide ](https://github.com/run-\nllama/finetune-embedding) showing you how to fine-tune an embedding model to\nimprove performance of Retrieval Augmented Generation (RAG) systems over any\nunstructured text corpus (no labels required!).\n\nThe result is a [ **5\u201310% performance increase in retrieval evaluation\nmetrics** ](https://github.com/run-llama/finetune-\nembedding/blob/main/evaluate.ipynb) \u2014 our finetuned ` bge ` model almost\nreaches ` text-embedding-ada-002 ` levels of retrieval performance in terms of\nhit rate. This enables more accurate retrieval which leads to better RAG\nsystems as a whole.\n\nThis tutorial is helpful to _anyone_ building RAG systems:\n\n  * If you\u2019re new to finetuning, no problem! We have [ step by step notebooks ](https://github.com/run-llama/finetune-embedding#steps-for-running) walking through the key steps. Simply substitute the file links for your own data, and just run every cell. \n  * Finetuning embedding models is lightweight and doesn\u2019t require a GPU. These notebooks were tested on an M2 Macbook Pro. \n\n**Resources**\n\n  * Repo: [ https://github.com/run-llama/finetune-embedding ](https://github.com/run-llama/finetune-embedding)\n  * Notebooks: [ Dataset Generation ](https://github.com/run-llama/finetune-embedding/blob/main/generate_dataset.ipynb) , [ Finetuning ](https://github.com/run-llama/finetune-embedding/blob/main/finetune.ipynb) , [ Evaluation ](https://github.com/run-llama/finetune-embedding/blob/main/evaluate.ipynb)\n\n#  Background/Context\n\n##  **The Current RAG Stack**\n\nRAG is a popular paradigm for connecting Large Language Models (LLMs) with an\nexternal source of data that was not present in its training corpus. It pairs\na **retrieval model** over a knowledge bank with the LLM through its input\nprompt space. RAG stacks typically look like the following:\n\n  * **Indexing** : Prepare a corpus of unstructured text, parse/chunk it. Then _embed_ each chunk and put in a vector database. \n  * **Query-time:** _Retrieve_ context from the vector db using top-k embedding similarity lookup, and stuff context into the LLM input space. \n\n(Of course RAG can be much more advanced than this, and LlamaIndex provides\ntools for both [ simple and advanced RAG ](https://gpt-\nindex.readthedocs.io/en/latest/getting_started/concepts.html) )\n\nUnfortunately RAG is easy to prototype by cobbling together the different\ncomponents, but hard to productionize. The simple stack has many failure modes\nand oftentimes the issue lies with bad retrieval \u2014 if the returned context is\nirrelevant to the query, then the capability of the LLM is irrelevant; the\nanswer will always be bad.\n\n##  **How Can We Make Retrieval Better?**\n\nWe can try more sophisticated retrieval algorithms (e.g. hybrid search,\nreranking).\n\nAn [ insight ](https://twitter.com/jerryjliu0/status/1692931028963221929?s=20)\nfrom our recent [ production RAG webinar\n](https://www.youtube.com/watch?v=Zj5RCweUHIk) , however, is that the\nembeddings themselves may not live in an optimal latent space for your data.\nEmbeddings generated by pre-trained models may be close/far from each other\nbased on the pre-training objective, but may not completely align with your\nown retrieval objective. For instance, if you\u2019re building search over ML ArXiv\npapers, you may want the embeddings to align semantically with specific ML\nconcepts (e.g. \u201cLLMs\u201d, \u201cNLP\u201d) and not filler words \u201cThis paper is\u2026\u201d).\n\nFinetuning is a way to solve that. The concept of finetuning has become\nincreasingly popular in the LLM space, with [ technological advancements\n](https://github.com/artidoro/qlora) as well as [ easy-to-use services\n](https://platform.openai.com/docs/guides/fine-tuning) .\n\nIn this tutorial, we focus on **finetuning the embedding model.** We show how\nfinetuning the embedding model can lead to better retrieval performance.\n\n##  Challenges/Considerations\n\nWhen you finetune embeddings, you need training examples. In the case of\nembeddings, this typically means that you have both \u201cpositive\u201d and \u201cnegative\u201d\nexamples \u2014 pairs of texts that should be close to each other and far from each\nother.\n\nAn issue is that we don\u2019t have these positive or negative examples apriori.\nGiven a dataset of unstructured text, is it possible to **automatically**\ngenerate these example pairs?\n\nWith LlamaIndex you can! We use LlamaIndex modules to automatically generate a\nset of questions from unstructured text chunks. These (question, chunk) pairs\nare then used as positive examples as training signals for the model (negative\nexamples are randomly sampled across other chunks).\n\nThe next section shows a full walkthrough across all of our modules.\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5031, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "87fcb361-7153-4ff3-9919-035be55520e9": {"__data__": {"id_": "87fcb361-7153-4ff3-9919-035be55520e9", "embedding": null, "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ffc1e5ca-3684-4aba-b4bb-8e22550bbbd3", "node_type": "4", "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "f733d11d3ca9cc9e059982e27f234bfd7c3adc087650da7b664397e36dd130ee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2dcb4780-e9c1-456a-8ba1-a41033e5772d", "node_type": "1", "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "1aaa096160283b9024096bddb8c5fa827e6fb1e666ce194b7b2853d39c8126fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "613b6bc4-d901-4cd4-8b4a-6acde642f3bc", "node_type": "1", "metadata": {}, "hash": "b878131a1a57edc74805f3f2cc61034018042ff52e03fe0644754398d5ca0c21", "class_name": "RelatedNodeInfo"}}, "text": "#  Walkthrough\n\nAt a high-level, we do the following:\n\n  1. Generating synthetic dataset for training and evaluation ( [ Notebook ](https://github.com/run-llama/finetune-embedding/blob/main/generate_dataset.ipynb) ) \n  2. Finetuning an opensource embedding model ( [ Notebook ](https://github.com/run-llama/finetune-embedding/blob/main/finetune.ipynb) ) \n  3. Evaluating the embedding model ( [ Notebook ](https://github.com/run-llama/finetune-embedding/blob/main/evaluate.ipynb) ) \n\n##  Generating synthetic dataset for training and evaluation\n\nThe key idea here is that we can leverage an LLM to generate hypothetical\nquestions that are best answered by a given piece of context. This allows us\nto generate synthetic positive pairs of (query, relevant documents) in a\nscalable way without requiring human labellers.\n\nMore concretely, we first process the given documents into a corpus of text\nchunks. We do this with the ` SimpleNodeParser ` module in LlamaIndex:\n\n    \n    \n    parser = SimpleNodeParser()\n    nodes = parser.get_nodes_from_documents(docs, show_progress=verbose)\n    corpus = {\n      node.node_id: node.get_content(metadata_mode=MetadataMode.NONE) \n      for node in nodes\n    }\n\nThen for each text chunk, we use LLM to generate a few hypothetical questions\nthat can be answered with information form that text chunk. The example prompt\nis shown below as well.\n\n    \n    \n    ", "mimetype": "text/plain", "start_char_idx": 5031, "end_char_idx": 6426, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "613b6bc4-d901-4cd4-8b4a-6acde642f3bc": {"__data__": {"id_": "613b6bc4-d901-4cd4-8b4a-6acde642f3bc", "embedding": null, "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ffc1e5ca-3684-4aba-b4bb-8e22550bbbd3", "node_type": "4", "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "f733d11d3ca9cc9e059982e27f234bfd7c3adc087650da7b664397e36dd130ee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "87fcb361-7153-4ff3-9919-035be55520e9", "node_type": "1", "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "b64f5ebf4a7a55a4bd81ac7701492f82f324b50119efd6526c7f648a36b03d2f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d846d79-1450-4e8e-952d-12fcfa1d2e8e", "node_type": "1", "metadata": {}, "hash": "b1d21552df13a31e34ce61663e0a583dd518f78c0ba1145806344c7dd6fd403a", "class_name": "RelatedNodeInfo"}}, "text": "prompt_template = prompt_template or \"\"\"\\\n      Context information is below.\n      \n      ---------------------\n      {context_str}\n      ---------------------\n      \n      Given the context information and not prior knowledge.\n      generate only questions based on the below query.\n      \n      ", "mimetype": "text/plain", "start_char_idx": 6426, "end_char_idx": 6724, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d846d79-1450-4e8e-952d-12fcfa1d2e8e": {"__data__": {"id_": "5d846d79-1450-4e8e-952d-12fcfa1d2e8e", "embedding": null, "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ffc1e5ca-3684-4aba-b4bb-8e22550bbbd3", "node_type": "4", "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "f733d11d3ca9cc9e059982e27f234bfd7c3adc087650da7b664397e36dd130ee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "613b6bc4-d901-4cd4-8b4a-6acde642f3bc", "node_type": "1", "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "655795fd3fcc596e775194b4275e57c83cd927891bffaa315f2ecc1809493ab9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2bf542ad-c282-49a7-b2f5-913b01325080", "node_type": "1", "metadata": {}, "hash": "c719e31859ecebeb0a068ab04e27921b268fe1dfe361f0c0826bfbe1c5437b34", "class_name": "RelatedNodeInfo"}}, "text": "You are a Teacher/ Professor. Your task is to setup \\\n      {num_questions_per_chunk} questions for an upcoming \\\n      quiz/examination. The questions should be diverse in nature \\\n      across the document. Restrict the questions to the \\\n      context information provided.\"\n      \"\"\"\n    \n    # for a given node, extract questions (do this over all nodes in outer loop)\n    query = prompt_template.format(context_str=text, num_questions_per_chunk=num_questions_per_chunk)\n    response = llm.complete(query)\n    \n    result = str(response).strip().split(\"\\n\")\n    questions = [\n        re.sub(r\"^\\d+[\\).\\s]\", \"\", question).strip() for question in result\n    ]\n    questions = [question for question in questions if len(question) &gt; 0]\n    \n\nFinally, we collect all pairs of questions and text chunks as the dataset.\nExample query, chunk, and mapping is shown below.\n\n    \n    \n    # example query\n    f331640a-b407-4028-8db8-4b8db691dd34: \"What is the market value of Lyft's common stock held by non-affiliates as of June 30, 2021, based on the closing sales price of the Class A common stock on that date?\"\n    \n    # example corpus\n    d5554f3e-cdaf-41d7-ac49-8f0ffe3f5759:\"UNITED STATESSECURITIES AND...\"\n    \n    # example mapping\n    f331640a-b407-4028-8db8-4b8db691dd34: d5554f3e-cdaf-41d7-ac49-8f0ffe3f5759\n\n##  Finetuning an opensource embedding model\n\nWe leverage the high-level model fitting API from ` sentencetransformers ` to\nvery easily setup a training process.\n\nWe use ` MultipleNegativesRankingLoss ` as the training object and `\nInformationRetrievalEvaluator ` as the evaluator during training. Also, we use\n` [ BAAI/bge-small-en ](https://huggingface.co/BAAI/bge-small-en) ` on Hugging\nFace as the base model and train for a small number of epochs.\n\n    \n    \n    # define model\n    model_id = \"BAAI/bge-small-en\"\n    model = SentenceTransformer(model_id)\n    \n    ...\n    \n    # define loss\n    from sentence_transformers import losses\n    loss = losses.MultipleNegativesRankingLoss(model)\n    \n    # define evaluator\n    from sentence_transformers.evaluation import InformationRetrievalEvaluator\n    # define over validation dataset\n    ...\n    evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs)\n    \n    # run training\n    ...\n    model.fit(\n        train_objectives=[(loader, loss)],\n        epochs=EPOCHS,\n        warmup_steps=warmup_steps,\n        output_path='exp_finetune',\n        show_progress_bar=True,\n        evaluator=evaluator, \n        evaluation_steps=50,\n    )\n\n##  Evaluating the embedding model\n\nWe compare the finetuned model against the base model, as well as the OpenAI\nembedding model ` text-embedding-ada-002 ` .\n\nWe evaluate with two main metrics:\n\n  * **Hit-rate metric:** For each (query, relevant_doc) pair, we retrieve the top-k documents with the query. ", "mimetype": "text/plain", "start_char_idx": 6724, "end_char_idx": 9556, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2bf542ad-c282-49a7-b2f5-913b01325080": {"__data__": {"id_": "2bf542ad-c282-49a7-b2f5-913b01325080", "embedding": null, "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ffc1e5ca-3684-4aba-b4bb-8e22550bbbd3", "node_type": "4", "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "f733d11d3ca9cc9e059982e27f234bfd7c3adc087650da7b664397e36dd130ee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d846d79-1450-4e8e-952d-12fcfa1d2e8e", "node_type": "1", "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}, "hash": "a82eb9ef25cbd5d23e616247129c3ef5af6238121e1eaa3a3d50c384c1cc6529", "class_name": "RelatedNodeInfo"}}, "text": "It\u2019s a _hit_ if the results contain relevant_doc. \n  * ` InformationRetrievalEvaluator ` from sentence_transformers. This provides a comprehensive suite of metrics such as cosine similarity accuracy, precision, recall at different top-k values. \n\n**Results**\n\nIn terms of hit-rate metric, the base model gets 78% hit-rate on the\nvalidation dataset, and the fine-tuned model gets 84%. ` text-embedding-\nada-002 ` gets 87%, which means that our fine-tuned model is only 3% off!\n\nHit-rate for `text-embedding-ada-002`, base model, finetuned model\n\nThe InformationRetrievalEvaluator shows a similar improvement across an entire\nsuite of metrics. The fine-tuned model increases evaluation metrics by 5\u201310%\ncompared to the base-model.\n\nEvaluation suite from `InformationRetrievalEvaluator`\n\n#  Conclusion\n\nWe successfully finetuned an embedding model over unlabeled, unstructured data\nto give better retrieval performance for downstream RAG systems. We show a\n5\u201310% improvement across all metrics!\n\n**Resources**\n\n(copied from intro)\n\n  * Repo: [ https://github.com/run-llama/finetune-embedding ](https://github.com/run-llama/finetune-embedding)\n  * Notebooks: [ Dataset Generation ](https://github.com/run-llama/finetune-embedding/blob/main/generate_dataset.ipynb) , [ Finetuning ](https://github.com/run-llama/finetune-embedding/blob/main/finetune.ipynb) , [ Evaluation ](https://github.com/run-llama/finetune-embedding/blob/main/evaluate.ipynb)\n\n", "mimetype": "text/plain", "start_char_idx": 9556, "end_char_idx": 10999, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0b81199-539a-4f24-805d-d5b08fbc3b05": {"__data__": {"id_": "c0b81199-539a-4f24-805d-d5b08fbc3b05", "embedding": null, "metadata": {"filename": "llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.md", "extension": ".md", "title": "LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs", "date": "Aug 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "340f46ae-5714-42b4-84ab-42bf54612094", "node_type": "4", "metadata": {"filename": "llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.md", "extension": ".md", "title": "LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs", "date": "Aug 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f"}, "hash": "b444e080a00a0f47afc74296304211d3fd007cc0dd92217f1c6934c2818a87ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50681b0d-1c8a-4207-9e45-475b92178e91", "node_type": "1", "metadata": {}, "hash": "6aa3340997ca41e546cb0eacb7cefee3c182aed3f5b230b5836b5480082b6087", "class_name": "RelatedNodeInfo"}}, "text": "(co-authored by Jerry Liu, CEO of LlamaIndex, Jeffrey Wang, co-founder at\nMetaphor, and Adam Hoffman, Software Engineer at Hypotenuse Labs)\n\nWe\u2019re incredibly excited to launch an [ integration\n](https://llamahub.ai/l/tools-metaphor) between LlamaIndex and [ Metaphor\n](https://platform.metaphor.systems/) : combine the capabilities of LlamaIndex\ndata agents with Metaphor as a _native_ LLM search tool to enable knowledge\nworkers capable of answering any question over any data, no matter how recent\nor complex.\n\nWe provide a deeper overview of Metaphor and the LlamaIndex integration below.\nWe also walk through our [ example notebook\n](https://github.com/emptycrown/llama-\nhub/blob/main/llama_hub/tools/notebooks/metaphor.ipynb) to showcase how they\ncan be combined.\n\n#  Background/Context\n\nState-of-the art large language models (LLMs) such as ChatGPT, GPT-4, Claude 2\nhave incredible reasoning capabilities that unlock a wide variety of use cases\n\u2014 from insight extraction to question-answering to general workflow\nautomation. Yet they are limited in their abilities to retrieve contextually\nrelevant information. A popular stack that has emerged is to setup a\nretrieval-augmented generation (RAG) system, which combines LLMs with external\nstorage solutions over a static knowledge source. Frameworks such as\nLlamaIndex provide a variety of tools to setup both simple and complex RAG\nsystems.\n\nYet even this is not the complete picture. LLMs should ideally be able to\ndynamically search and retrieve information from the external world, not just\ndepend on a static source of knowledge. This would allow them to fulfill a\nmore general set of tasks and not only perform search/retrieval, but perform\nactions as well.\n\nTo do this well, we need two core components:\n\n  * General abstractions that allow LLMs to intelligently perform various tasks over your data, in both a \u201cread\u201d and \u201cwrite\u201d fashion \n  * A good search engine tailored for LLM use \n\nLlamaIndex [ data agent abstractions ](https://medium.com/llamaindex-\nblog/data-agents-eed797d7972f) help to satisfy the first core component. A\ncomplete data agent consists of both a reasoning loop as well as a set of\nTools. These tools can be interfaces for search/retrieval or more generally\nany external API. Given a query, the agent will execute its reasoning loop and\ndynamically figure out the set of Tools it will need to fulfill the task at\nhand.\n\nData agents have access to a rich set of Tools offered on [ LlamaHub\n](https://llamahub.ai/) \u2014 these range from Gmail API, to a SQL db API, to a\nbasic tool in the form of Bing search. We\u2019ve shown that they are capable of\ne2e tasks from [ sending emails ](https://github.com/emptycrown/llama-\nhub/blob/main/llama_hub/tools/notebooks/gmail.ipynb) , [ scheduling meetings\n](https://github.com/emptycrown/llama-\nhub/blob/main/llama_hub/tools/notebooks/google_calendar.ipynb) , to automating\n[ custom support insight extraction ](https://github.com/emptycrown/llama-\nhub/blob/main/llama_hub/tools/notebooks/shopify.ipynb) . Yet there has never\nbeen a tool tailored for LLM use.\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3080, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50681b0d-1c8a-4207-9e45-475b92178e91": {"__data__": {"id_": "50681b0d-1c8a-4207-9e45-475b92178e91", "embedding": null, "metadata": {"filename": "llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.md", "extension": ".md", "title": "LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs", "date": "Aug 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "340f46ae-5714-42b4-84ab-42bf54612094", "node_type": "4", "metadata": {"filename": "llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.md", "extension": ".md", "title": "LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs", "date": "Aug 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f"}, "hash": "b444e080a00a0f47afc74296304211d3fd007cc0dd92217f1c6934c2818a87ef", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0b81199-539a-4f24-805d-d5b08fbc3b05", "node_type": "1", "metadata": {"filename": "llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.md", "extension": ".md", "title": "LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs", "date": "Aug 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f"}, "hash": "03005969805187d5b47e8eb197fe00bc1c0dc92b427c68326a1373c6ee5df0e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c4ae5c2-1a3c-4798-b4e8-fe81ab38cf1b", "node_type": "1", "metadata": {}, "hash": "78e0688f0fa1707a2b06294d498ceaac8b692854813ad718d9cd335f881bf2b7", "class_name": "RelatedNodeInfo"}}, "text": "#  Overview of Metaphor\n\nThe Metaphor API is designed to connect your LLM to the internet. It allows\nyou to perform fully neural, highly semantic searches over the Internet and\nalso get clean, HTML content from the results.\n\nMetaphor was trained to predict links on the internet, given how people talk\nabout things on the Internet. For example, someone might post about a great\narticle they read like this:\n\n> Found an amazing article I read about the history of Rome\u2019s architecture:\n> [LINK]\n\nBy training a model to predict these links given how people talk about them,\nthe end result is a totally different way to search the internet \u2014 _search as\nif you\u2019re about to share the link you want_ . While a little unintuitive at\nfirst, searching this way can return extremely high quality results. But for\nthe purposes of LlamaIndex, you won\u2019t need to worry about this because by\ndefault, queries will be converted into Metaphor prompts.\n\nWhy would you use Metaphor Search over Bing/Google? There are 3 main reasons:\n\n  * You can search fully semantically, for instance with feelings or complex descriptors. \n  * You can search only for the type of entity that you want. Companies, articles, people. \n  * You can find content that Google simply doesn\u2019t surface well, maybe because keywords aren\u2019t the right tool or maybe just because Google doesn\u2019t care about returning good results for that type of content. \n\nTo learn more, you can read the full Metaphor API [ blog post\n](https://platform.metaphor.systems/blog/building-search-for-the-post-chatgpt-\nworld) .\n\n#  Integration Details\n\nThe [ Metaphor Tool Spec in LlamaHub ](https://llamahub.ai/l/tools-metaphor)\nis an API interface that consists of 5 tools that an agent can use.\n\n  * **Search:** The entrypoint to Metaphor \u2014 allows an agent to pass a natural language query that will then be passed to the Metaphor search engine. This endpoint also contains some additional parameters, such as the number of results, domains to include/exclude, and a date filter. \n  * **Retrieve Documents:** This will retrieve the content of a set of documents given IDs. These ids are returned as part of the results from the search endpoint above. \n  * **Search and Retrieve Documents:** This is a convenience endpoint that combines the functionality of `search` and `retrieve_documents`. \n  * **Find Similar:** This directly calls an endpoint offered by Metaphor, which will return a list of documents similar to a given URL. \n  * **Current Date:** This is a convenience function that returns the current date. On its own it is unrelated to Metaphor\u2019s API, but may be called beforehand to figure out the right date filters to pass to some of Metaphor\u2019s endpoints. \n\nIn the next section, let\u2019s walk through how a data agent can make use of these\nendpoints through various use cases.\n\n#  Example Walkthrough\n\nLet\u2019s walk through our example notebook showing how LlamaIndex data agents can\nbe used with Metaphor.\n\n**Testing the Metaphor Tools**\n\nThe first step is to import the Metaphor tool spec.\n\n    \n    \n    # Set up Metaphor tool\n    from llama_hub.tools.metaphor.base import MetaphorToolSpec\n    metaphor_tool = MetaphorToolSpec(\n    api_key='your-key',\n    )\n    # convert tool spec to a list of tools\n    metaphor_tool_list = metaphor_tool.to_tool_list()\n    for tool in metaphor_tool_list:\n    print(tool.metadata.name)\n\nIn this walkthrough, we make use of all of the tools. But you\u2019re free to pick\nand choose to use specific tools if you want to define a more custom workflow\nand restrict the agent action space.\n\nWe can play around with the set of tools before defining our agent. All of our\nMetaphor tools make use of the `AutoPrompt` option where Metaphor will pass a\nquery through an LLM to refine and improve the query.\n\nExample input:\n\n    \n    \n    metaphor_tool.search('machine learning transformers', num_results=3)\n\nExample output:\n\n    \n    \n    [{'title': 'On the potential of Transformers in Reinforcement Learning',\n    'url': 'https://lorenzopieri.com/rl_transformers/',\n    'id': 'ysJlYSgeGW3l4zyOBoSGcg'},\n    {'title': 'Transformers: Attention in Disguise',\n    'url': 'https://www.mihaileric.com/posts/transformers-attention-in-disguise/',\n    'id': 'iEYMai5rS9k0hN5_BH0VZg'},\n    {'title': 'Transformers in Computer Vision: Farewell Convolutions!',\n    'url': 'https://towardsdatascience.com/transformers-in-computer-vision-farewell-convolutions-f083da6ef8ab?gi=a1d0a9a2896c',\n    'id': 'kX1Z89DdjSvBrH1S1XLvwg'}]\n\nThe notebook also contains examples of us playing around with the other\nendpoints: ` retrieve_documents ` , ` find_similar ` , `\nsearch_and_retrieve_documents ` .\n\n**Setting up an OpenAI Function Calling Agent with Metaphor**\n\nWe can create an agent with access to all of the above tools and start testing\nit out:\n\n    \n    \n    from llama_index.agent import OpenAIAgent\n    # We don't give the Agent our unwrapped retrieve document tools, instead passing the wrapped tools\n    agent = OpenAIAgent.from_tools(\n      metaphor_tool_list,\n      verbose=True,\n    )\n\nThat\u2019s it in terms of setup! Let\u2019s try giving an example query:\n\n    \n    \n    print(agent.chat('What are the best restaurants in toronto?\"))\n\n", "mimetype": "text/plain", "start_char_idx": 3080, "end_char_idx": 8264, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c4ae5c2-1a3c-4798-b4e8-fe81ab38cf1b": {"__data__": {"id_": "8c4ae5c2-1a3c-4798-b4e8-fe81ab38cf1b", "embedding": null, "metadata": {"filename": "llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.md", "extension": ".md", "title": "LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs", "date": "Aug 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "340f46ae-5714-42b4-84ab-42bf54612094", "node_type": "4", "metadata": {"filename": "llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.md", "extension": ".md", "title": "LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs", "date": "Aug 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f"}, "hash": "b444e080a00a0f47afc74296304211d3fd007cc0dd92217f1c6934c2818a87ef", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50681b0d-1c8a-4207-9e45-475b92178e91", "node_type": "1", "metadata": {"filename": "llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.md", "extension": ".md", "title": "LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs", "date": "Aug 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f"}, "hash": "892b7c5ad4fd495f001bf74c80be6021aec390be107b4daa0b28c129f3df20e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d5bf640-e9ca-4b1f-ac08-281aeb32efcb", "node_type": "1", "metadata": {}, "hash": "069c1857253d1b13e231a44fe9a1eecaab43883a4fe9ffe1b360e9e7cb48430f", "class_name": "RelatedNodeInfo"}}, "text": "We walk through the execution trace of this agent to see how it is interacting\nwith the Metaphor tool.\n\n    \n    \n    === Calling Function ===\n    Calling function: search with args: {\n      \"query\": \"best restaurants in Toronto\"\n    }\n    [Metaphor Tool] Autoprompt string: Here's a link to the best restaurant in Toronto:\n    Got output: [{'title': 'Via Allegro Ristorante - Toronto Fine Dining Restaurant', 'url': 'https://viaallegroristorante.com/', 'id': 'EVlexzJh-lzkVr4tb2y_qw'}, {'title': 'The Senator \u2013 Home', 'url': 'https://thesenator.com/', 'id': 'dA3HVr5P8E0Bs7nH2gH7ZQ'}, {'title': 'Home - The Rushton', 'url': 'https://therushton.com/', 'id': '6Je-igG-i-ApqISC5XXmGQ'}, {'title': 'Location', 'url': 'https://osteriagiulia.ca/', 'id': 'HjP5c54vqb3n3UNa3HevSA'}, {'title': 'StockYards | Stockyards Toronto', 'url': 'https://www.thestockyards.ca/', 'id': 'Pffz-DQlOepqVgKQDmW5Ig'}, {'title': 'Select A Restaurant', 'url': 'https://www.torontopho.com/', 'id': 'DiQ1hU1gmrIzpKnOaVvZmw'}, {'title': 'Home | Kit Kat Italian Bar &amp; Grill', 'url': 'http://www.kitkattoronto.com/', 'id': 'kdAcLioBgnwzuHyd0rWS1w'}, {'title': 'La Fenice', 'url': 'https://www.lafenice.ca/', 'id': 'M-LHQZP6V40V81fqLFAQxQ'}, {'title': 'Le Ph\u00e9nix', 'url': 'https://www.lephenixto.com/', 'id': 'spCTcFr0GHlFUTzyngfRVw'}, {'title': 'ITALIAN, INSPIRED.', 'url': 'https://figotoronto.com/', 'id': 'OvBcTqEo1tCSywr4ATptCg'}]\n    ========================\n    Here are some of the best restaurants in Toronto:\n    \n    1. ", "mimetype": "text/plain", "start_char_idx": 8264, "end_char_idx": 9767, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d5bf640-e9ca-4b1f-ac08-281aeb32efcb": {"__data__": {"id_": "0d5bf640-e9ca-4b1f-ac08-281aeb32efcb", "embedding": null, "metadata": {"filename": "llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.md", "extension": ".md", "title": "LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs", "date": "Aug 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "340f46ae-5714-42b4-84ab-42bf54612094", "node_type": "4", "metadata": {"filename": "llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.md", "extension": ".md", "title": "LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs", "date": "Aug 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f"}, "hash": "b444e080a00a0f47afc74296304211d3fd007cc0dd92217f1c6934c2818a87ef", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c4ae5c2-1a3c-4798-b4e8-fe81ab38cf1b", "node_type": "1", "metadata": {"filename": "llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.md", "extension": ".md", "title": "LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs", "date": "Aug 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f"}, "hash": "4ce3a9df6b125e57fe12ba0ff22bc6b3f35ebeeeb5b8da4c5143edbb09b7e322", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2bfffa7-5a3b-424d-a87a-f5948453de81", "node_type": "1", "metadata": {}, "hash": "dcdd85e5ffebbc279be041bfb0b749cf163c286258e81d2530674ebf4b6d60ad", "class_name": "RelatedNodeInfo"}}, "text": "[Via Allegro Ristorante](https://viaallegroristorante.com/)\n    2. ", "mimetype": "text/plain", "start_char_idx": 9767, "end_char_idx": 9834, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2bfffa7-5a3b-424d-a87a-f5948453de81": {"__data__": {"id_": "c2bfffa7-5a3b-424d-a87a-f5948453de81", "embedding": null, "metadata": {"filename": "llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.md", "extension": ".md", "title": "LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs", "date": "Aug 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "340f46ae-5714-42b4-84ab-42bf54612094", "node_type": "4", "metadata": {"filename": "llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.md", "extension": ".md", "title": "LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs", "date": "Aug 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f"}, "hash": "b444e080a00a0f47afc74296304211d3fd007cc0dd92217f1c6934c2818a87ef", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d5bf640-e9ca-4b1f-ac08-281aeb32efcb", "node_type": "1", "metadata": {"filename": "llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.md", "extension": ".md", "title": "LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs", "date": "Aug 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f"}, "hash": "707fd2adb1a260304aa53c028817af5104f898422d9fcc4c1b06f604fc2a7288", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49fbcc0a-9ef7-406e-b580-8e0b77b30268", "node_type": "1", "metadata": {}, "hash": "7e1c8cf03cedfe5bce2f9881a28fb44ba124bf82d2740ea5981d5a354b71eb78", "class_name": "RelatedNodeInfo"}}, "text": "[The Senator](https://thesenator.com/)\n    3. [The Rushton](https://therushton.com/)\n    4. [Osteria Giulia](https://osteriagiulia.ca/)\n    5. [Stockyards](https://www.thestockyards.ca/)\n    6. [Toronto Pho](https://www.torontopho.com/)\n    7. [Kit Kat Italian Bar &amp; Grill](http://www.kitkattoronto.com/)\n    8. [La Fenice](https://www.lafenice.ca/)\n    9. [Le Ph\u00e9nix](https://www.lephenixto.com/)\n    10. [Figo](https://figotoronto.com/)\n    \n    You can visit their websites for more information. Enjoy your dining experience in Toronto!\n\nThe execution trace shows that the agent is simply calling the `search`\nendpoint with \u201cbest restaurants in Toronto\u201d, and returning that as a list of\ndictionaries representing the search results.\n\nNote that we can ask a followup question as well:\n\n    \n    \n    print(agent.chat('tell me more about Osteria Giulia'))\n\nAnd we get the following result (note: we truncate some of the intermediate\noutput):\n\n    \n    \n    === Calling Function ===\n    Calling function: retrieve_documents with args: {\n    \"ids\": [\"HjP5c54vqb3n3UNa3HevSA\"]\n    }\n    Got output: [\u2026]\n    ========================\n    Osteria Giulia is a restaurant located at 134 Avenue Road in Toronto, Ontario. You can contact them at 416.964.8686 or via email at info@osteriagiulia.ca (for general inquiries only, no reservation requests via email).\n    The restaurant's operating hours are from Monday to Saturday, from 5:00pm to 11:00pm. On Sundays, the restaurant is available for private bookings.\n    Parking is available on Avenue Road and Davenport Road.\n    You can follow Osteria Giulia on Instagram [@osteriagiulia](https://www.instagram.com/osteriagiulia). They also have a sister restaurant called Giulietta, which you can visit at [giu.ca](https://giu.ca) or on Instagram [@giulietta972](https://www.instagram.com/giulietta972).\n    Please note that the information provided is based on the available document and may be subject to change. ", "mimetype": "text/plain", "start_char_idx": 9834, "end_char_idx": 11794, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49fbcc0a-9ef7-406e-b580-8e0b77b30268": {"__data__": {"id_": "49fbcc0a-9ef7-406e-b580-8e0b77b30268", "embedding": null, "metadata": {"filename": "llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.md", "extension": ".md", "title": "LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs", "date": "Aug 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "340f46ae-5714-42b4-84ab-42bf54612094", "node_type": "4", "metadata": {"filename": "llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.md", "extension": ".md", "title": "LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs", "date": "Aug 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f"}, "hash": "b444e080a00a0f47afc74296304211d3fd007cc0dd92217f1c6934c2818a87ef", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2bfffa7-5a3b-424d-a87a-f5948453de81", "node_type": "1", "metadata": {"filename": "llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.md", "extension": ".md", "title": "LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs", "date": "Aug 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f"}, "hash": "21a517b7746fb8d6aed18672563a485891a97601ed8279023235d91ce29af3e8", "class_name": "RelatedNodeInfo"}}, "text": "It is recommended to visit their official website or contact them directly for the most up-to-date information.\n\nSince \u201cOsteria Giulia\u201d is in the agent conversation history, the agent now\nknows to call the `retrieve` endpoint to return more information about the\nrelevant search result.\n\n**Advanced: Avoiding Context Window Issues**\n\nOne issue with using ` retrieve ` is that the content can be quite long. If\nthe content is naively appended to the conversation history and dumped into\nthe LLM context window, then we may run into context window limitations.\n\nLlamaIndex offers tool abstractions to help deal with this. Our `\nLoadAndSearchToolSpec ` wraps any given tool that may return a large amount of\ndata, and it splits it into two tools: a load tool that will dynamically store\nthe data in an index, and a search tool that allows for search over that\nindex.\n\nOn the Metaphor side, this is also where we define a `\nsearch_and_retrieve_documents ` endpoint that combines ` search ` and `\nretrieve ` . This allows the agent to make a single query to retrieve a large\nnumber of documents, which when combined with the ` LoadAndSearchToolSpec `\nwill get directly stored within an index. If the agent were to call ` search `\nand ` retrieve ` separately, then it would both take longer and consume more\ntokens to write the search results to conversation history, and then passing\nthat into the prompt again to call ` retrieve ` over all document IDs.\n\nCreating the ` LoadAndSearchToolSpec ` :\n\n    \n    \n    from llama_index.tools.tool_spec.load_and_search.base import LoadAndSearchToolSpec\n    # The search_and_retrieve_documents tool is the third in the tool list, as seen above\n    wrapped_retrieve = LoadAndSearchToolSpec.from_defaults(\n      metaphor_tool_list[2],\n    )\n\nNow let\u2019s walk through a full execution example:\n\n    \n    \n    # Just pass the wrapped tools and the get_date utility\n    agent = OpenAIAgent.from_tools(\n      [*wrapped_retrieve.to_tool_list(), metaphor_tool_list[4]],\n      verbose=True,\n    )\n    print(agent.chat('Can you summarize everything published in the last month regarding news on superconductors'))\n\nThe output here shows that the agent calls multiple tools in succession to get\nthe right answer.\n\n    \n    \n    === Calling Function ===\n    Calling function: current_date with args: {}\n    Got output: 2023-08-20\n    ========================\n    === Calling Function ===\n    Calling function: search_and_retrieve_documents with args: {\n      \"query\": \"superconductors\",\n      \"start_published_date\": \"2023-07-20\",\n      \"end_published_date\": \"2023-08-20\"\n    }\n    [Metaphor Tool] Autoprompt: \"Here is an interesting article about superconductors:\n    Got output: Content loaded! You can now search the information using read_search_and_retrieve_documents\n    ========================\n    === Calling Function ===\n    Calling function: read_search_and_retrieve_documents with args: {\n      \"query\": \"superconductors\"\n    }\n    Got output: \n    Superconductors are materials that can perfectly conduct electricity. They are used in a variety of applications, such as particle accelerators, nuclear fusion devices, MRI machines, and maglev trains. However, so far, no superconductor has been proven to work at ambient pressures and temperatures. On July 22, scientists in South Korea published research claiming to have solved this problem with a material called LK-99, which has an electrical resistivity that drops to near zero at 30 degrees Celsius (86 degrees Fahrenheit).\n    ========================\n    In the last month, there have been developments in the field of superconductors. Scientists in South Korea have published research on a material called LK-99, which has the ability to conduct electricity with near-zero resistance at a temperature of 30 degrees Celsius (86 degrees Fahrenheit). This breakthrough could potentially lead to the development of superconductors that work at ambient pressures and temperatures, opening up new possibilities for various applications such as particle accelerators, nuclear fusion devices, MRI machines, and maglev trains.\n\nThe agent used the ` get_date ` tool to determine the current month, and then\napplied the filters in Metaphor based on publication date when calling `\nsearch ` . It then loaded the documents using ` retrieve_documents ` and read\nthem using ` read_retrieve_documents ` .\n\n#  Conclusion\n\nAs shown above, the integration between LlamaIndex data agents + Metaphor\nsearch has the potential to bypass existing limitations with LLMs and even RAG\nsystems. We\u2019re excited to continue exploring this further in future blog\nposts.\n\nWe encourage you to play around with the notebook \u2014 make sure to check it out!\n\n**Resources:**\n\n  * Notebook: [ https://github.com/emptycrown/llama-hub/blob/main/llama_hub/tools/notebooks/metaphor.ipynb ](https://github.com/emptycrown/llama-hub/blob/main/llama_hub/tools/notebooks/metaphor.ipynb)\n  * LlamaHub: [ https://llamahub.ai/l/tools-metaphor ](https://llamahub.ai/l/tools-metaphor)\n  * Metaphor: [ https://platform.metaphor.systems/ ](https://platform.metaphor.systems/)\n  * Metaphor API Docs: [ https://docs.metaphor.systems/ ](https://docs.metaphor.systems/)\n\n", "mimetype": "text/plain", "start_char_idx": 11794, "end_char_idx": 16996, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f63b1f8-0880-488c-b11c-5da0f987ff32": {"__data__": {"id_": "0f63b1f8-0880-488c-b11c-5da0f987ff32", "embedding": null, "metadata": {"filename": "easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.md", "extension": ".md", "title": "Easily Finetune Llama 2 for Your Text-to-SQL Applications", "date": "Aug 17, 2023", "url": "https://www.llamaindex.ai/blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bab77fe8-f4b9-466e-95ea-cf9ad876baf4", "node_type": "4", "metadata": {"filename": "easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.md", "extension": ".md", "title": "Easily Finetune Llama 2 for Your Text-to-SQL Applications", "date": "Aug 17, 2023", "url": "https://www.llamaindex.ai/blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d"}, "hash": "8f07bd4121d226435365b3fbb47f1c14641cb90eb089de88843541dcc3b449c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c5bff10-f3b9-4caf-9453-080d6531f289", "node_type": "1", "metadata": {}, "hash": "5f9ca1b682f3098fc4fd124ab16808d09c399ca8567e0fee00d1bc21e621f327", "class_name": "RelatedNodeInfo"}}, "text": "[ Llama 2 ](https://ai.meta.com/llama/) is a huge milestone in the advancement\nof open-source LLMs. The biggest model and its finetuned variants sit at the\ntop of the [ Hugging Face Open LLM Leaderboard\n](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) . Multiple\nbenchmarks show that it is approaching GPT-3.5 (or in some cases even\nsurpassing it) in terms of performance. All of this means that open-source\nLLMs are an increasingly viable and reliable option for use in complex LLM\napplications, from RAG systems to agents.\n\n#  Context: Llama-2\u20137B is Not Good at Text-to-SQL\n\nA downside of the smallest Llama 2 model (7B parameters), however, is that\nit\u2019s not very good at generating SQL, making it impractical for structured\nanalytics use cases. As an example, we tried prompting Llama 2 to generate the\ncorrect SQL statement given the following prompt template:\n\n    \n    \n    You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. \n    \n    You must output the SQL query that answers the question.\n    \n    ### Input:\n    {input}\n    \n    ### Context:\n    {context}\n    \n    ### Response:\n\nHere we plugged in a sample entry from the [ sql-create-context dataset\n](https://huggingface.co/datasets/b-mc2/sql-create-context) .\n\n    \n    \n    input: In 1981 which team picked overall 148?\n    context: CREATE TABLE table_name_8 (team VARCHAR, year VARCHAR, overall_pick VARCHAR)\n\nMeanwhile, here is the generated output vs. correct output:\n\n    \n    \n    Generated output: SELECT * FROM `table_name_8` WHERE '1980' = YEAR AND TEAM = \"Boston Celtics\" ORDER BY OVERALL_PICK DESC LIMIT 1;\n    \n    Correct output: SELECT team FROM table_name_8 WHERE year = 1981 AND overall_pick = \"148\"\n\nThis is clearly not ideal. Unlike ChatGPT and GPT-4, Llama 2 does not reliably\nproduce well-formatted and correct SQL outputs.\n\nThis is exactly where fine-tuning comes in \u2014 given a proper corpus of text-to-\nSQL data, we can teach Llama 2 to be better at generating SQL outputs from\nnatural language. At a high-level, fine-tuning involves modifying the weights\nof the model in some capacity. There are different ways to finetune models,\nfrom updating all parameters of the network, to a subset of the parameters, to\nonly finetuning additional parameters (e.g. [ how LoRA works\n](https://arxiv.org/abs/2106.09685) ).\n\nOnce the model is finetuned, it can still be plugged into a downstream LLM\napplication. That is exactly what this tutorial aims to show. It is a step\nmore involved than our existing tutorials which have primarily focused on \u201cin-\ncontext learning\u201d and \u201cretrieval-augmentation\u201d use cases \u2014 freezing the model\nitself but focusing on the orchestration of data into the input prompt.\nFinetuning can have a high learning curve and also require a lot of compute.\nThis tutorial makes it as easy as possible to get started.\n\n#  Tutorial Overview\n\nIn this tutorial, we show you how you can finetune Llama 2 on a text-to-SQL\ndataset, and then use it for structured analytics against any SQL database\nusing the capabilities of [ LlamaIndex\n](https://github.com/jerryjliu/llama_index) .\n\nHere is the stack that we use:\n\n  * ` [ b-mc2/sql-create-context ](https://huggingface.co/datasets/b-mc2/sql-create-context) ` [ from Hugging Face datasets ](https://huggingface.co/datasets/b-mc2/sql-create-context) as the training dataset \n  * [ OpenLLaMa ](https://github.com/openlm-research/open_llama) ` open_llama_7b_v2 ` as the base model \n  * [ PEFT for efficient finetuning ](https://github.com/huggingface/peft)\n  * [ Modal ](https://modal.com/) for handling all cloud compute/orchestration for finetuning. And also for the excellent reference [ doppel-bot repo ](https://github.com/modal-labs/doppel-bot) . \n  * [ LlamaIndex ](https://www.llamaindex.ai/) for text-to-SQL inference against any SQL database. \n\nSpecial mention to the awesome [ Llama 2 tutorial from Anyscale that helped to\ninspire this project ](https://www.anyscale.com/blog/fine-tuning-\nllama-2-a-comprehensive-case-study-for-tailoring-models-to-unique-\napplications) .\n\nAll of our materials can be found in our Github repo: [\nhttps://github.com/run-llama/modal_finetune_sql ](https://github.com/run-\nllama/modal_finetune_sql) (again emphasizing that this is adapted from [\ndoppel-bot ](https://github.com/modal-labs/doppel-bot) ). Also, the full\ntutorial can be found in our [ Jupyter notebook guide\n](https://github.com/run-llama/modal_finetune_sql/blob/main/tutorial.ipynb) .\nMake sure to check it out!\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4579, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c5bff10-f3b9-4caf-9453-080d6531f289": {"__data__": {"id_": "1c5bff10-f3b9-4caf-9453-080d6531f289", "embedding": null, "metadata": {"filename": "easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.md", "extension": ".md", "title": "Easily Finetune Llama 2 for Your Text-to-SQL Applications", "date": "Aug 17, 2023", "url": "https://www.llamaindex.ai/blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bab77fe8-f4b9-466e-95ea-cf9ad876baf4", "node_type": "4", "metadata": {"filename": "easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.md", "extension": ".md", "title": "Easily Finetune Llama 2 for Your Text-to-SQL Applications", "date": "Aug 17, 2023", "url": "https://www.llamaindex.ai/blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d"}, "hash": "8f07bd4121d226435365b3fbb47f1c14641cb90eb089de88843541dcc3b449c1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f63b1f8-0880-488c-b11c-5da0f987ff32", "node_type": "1", "metadata": {"filename": "easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.md", "extension": ".md", "title": "Easily Finetune Llama 2 for Your Text-to-SQL Applications", "date": "Aug 17, 2023", "url": "https://www.llamaindex.ai/blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d"}, "hash": "f09a89bc5cdbc597359d3508d46302c5281b51a7c88702f2bbc6aefdcdbe57ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dff0714c-ca56-4a5d-8071-f090cdb580ce", "node_type": "1", "metadata": {}, "hash": "30df6d11ffedda8a961108af08a1dff3281c2d2c3a8cb735766eb4e1f49f07fe", "class_name": "RelatedNodeInfo"}}, "text": "As mentioned above, performing finetuning does require quite a few steps. Our\ngoal is to make this as straightforward as possible to follow and use out of\nthe box. We don\u2019t cover all the nitty gritty detailsof Modal, PEFT, the\nfinetuning procedure itself, etc. but we do give a rough overview.\n\nThere are also certainly higher-level APIs that we could\u2019ve used (e.g. OpenAI,\nLamini) in order to achieve this task. ", "mimetype": "text/plain", "start_char_idx": 4579, "end_char_idx": 4992, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dff0714c-ca56-4a5d-8071-f090cdb580ce": {"__data__": {"id_": "dff0714c-ca56-4a5d-8071-f090cdb580ce", "embedding": null, "metadata": {"filename": "easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.md", "extension": ".md", "title": "Easily Finetune Llama 2 for Your Text-to-SQL Applications", "date": "Aug 17, 2023", "url": "https://www.llamaindex.ai/blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bab77fe8-f4b9-466e-95ea-cf9ad876baf4", "node_type": "4", "metadata": {"filename": "easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.md", "extension": ".md", "title": "Easily Finetune Llama 2 for Your Text-to-SQL Applications", "date": "Aug 17, 2023", "url": "https://www.llamaindex.ai/blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d"}, "hash": "8f07bd4121d226435365b3fbb47f1c14641cb90eb089de88843541dcc3b449c1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c5bff10-f3b9-4caf-9453-080d6531f289", "node_type": "1", "metadata": {"filename": "easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.md", "extension": ".md", "title": "Easily Finetune Llama 2 for Your Text-to-SQL Applications", "date": "Aug 17, 2023", "url": "https://www.llamaindex.ai/blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d"}, "hash": "dc3f18e741459a6e62b70385c9d0cdc3755c42ed7e1d055a7a076ce5cf7d7537", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c98a4e4-1786-4f92-80ff-ba018be5ae7d", "node_type": "1", "metadata": {}, "hash": "188028071b2089c09e0256849942d5230454b6a9eb8a920a093f822d9c840d6d", "class_name": "RelatedNodeInfo"}}, "text": "There\u2019s plenty of room for followup\ntutorials to cover these topics!\n\n##  Step 1: Loading Training Data for Finetuning LLaMa\n\nThe first step here is to open up the [ Jupyter notebook\n](https://github.com/run-llama/modal_finetune_sql/blob/main/tutorial.ipynb) .\nThe notebook is organized into a series of runnable scripts that each perform\nthe steps needed to load data.\n\nOur code uses Modal for every step of the orchestration, and Modal is best\nused on top of the Python scripts themselves. That is why a lot of these cells\ndon\u2019t contain Python blocks of their own.\n\nFirst we use Modal to load in the ` b-mc2/sql-create-context ` dataset. This\nis a simple task that just loads in the dataset and formats it into a ` .jsonl\n` file.\n\n    \n    \n    modal run src.load_data_sql --data-dir \"data_sql\"\n\nAs we can see, under the hood the task is quite straightforward:\n\n    \n    \n    # Modal stubs allow our function to run remotely\n    @stub.function(\n        retries=Retries(\n            max_retries=3,\n            initial_delay=5.0,\n            backoff_coefficient=2.0,\n        ),\n        timeout=60 * 60 * 2,\n        network_file_systems={VOL_MOUNT_PATH.as_posix(): output_vol},\n        cloud=\"gcp\",\n    )\n    def load_data_sql(data_dir: str = \"data_sql\"):\n        from datasets import load_dataset\n    \n        dataset = load_dataset(\"b-mc2/sql-create-context\")\n    \n        dataset_splits = {\"train\": dataset[\"train\"]}\n        out_path = get_data_path(data_dir)\n    \n        out_path.parent.mkdir(parents=True, exist_ok=True)\n    \n        for key, ds in dataset_splits.items():\n            with open(out_path, \"w\") as f:\n                for item in ds:\n                    newitem = {\n                        \"input\": item[\"question\"],\n                        \"context\": item[\"context\"],\n                        \"output\": item[\"answer\"],\n                    }\n                    f.write(json.dumps(newitem) + \"\\n\")\n\n##  Step 2: Run Finetuning Script\n\nThe next step is to run our finetuning script on the parsed dataset.\n\n    \n    \n    modal run src.finetune_sql --data-dir \"data_sql\" --model-dir \"model_sql\"\n\nThe finetuning script performs the following steps.\n\n**Splits the dataset into training and validation splits**\n\n    \n    \n    train_val = data[\"train\"].train_test_split(test_size=val_set_size, shuffle=True, seed=42)\n    train_data = train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n    val_data = train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n\n**Formats each split into tuples of (input prompt, label):** The input query\nand context are formatted into the same input prompt. The input prompt is then\ntokenized, and the labels are set to the exact same as the input prompt \u2014 this\nallows the model to train on next-token prediction.\n\n    \n    \n    def generate_and_tokenize_prompt(data_point):\n      full_prompt = generate_prompt_sql(\n          data_point[\"input\"],\n          data_point[\"context\"],\n          data_point[\"output\"],\n      )\n      tokenized_full_prompt = tokenize(full_prompt)\n      if not train_on_inputs:\n          raise NotImplementedError(\"not implemented yet\")\n      return tokenized_full_prompt\n\nThe input prompt is the exact same as what was given at the top of this blog.\n\nWhen the finetuning script is run, the model is saved in the remote cloud\ndirectory specified by model_dir (which is set to a default value if not\nspecified).\n\n", "mimetype": "text/plain", "start_char_idx": 4992, "end_char_idx": 8386, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c98a4e4-1786-4f92-80ff-ba018be5ae7d": {"__data__": {"id_": "3c98a4e4-1786-4f92-80ff-ba018be5ae7d", "embedding": null, "metadata": {"filename": "easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.md", "extension": ".md", "title": "Easily Finetune Llama 2 for Your Text-to-SQL Applications", "date": "Aug 17, 2023", "url": "https://www.llamaindex.ai/blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bab77fe8-f4b9-466e-95ea-cf9ad876baf4", "node_type": "4", "metadata": {"filename": "easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.md", "extension": ".md", "title": "Easily Finetune Llama 2 for Your Text-to-SQL Applications", "date": "Aug 17, 2023", "url": "https://www.llamaindex.ai/blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d"}, "hash": "8f07bd4121d226435365b3fbb47f1c14641cb90eb089de88843541dcc3b449c1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dff0714c-ca56-4a5d-8071-f090cdb580ce", "node_type": "1", "metadata": {"filename": "easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.md", "extension": ".md", "title": "Easily Finetune Llama 2 for Your Text-to-SQL Applications", "date": "Aug 17, 2023", "url": "https://www.llamaindex.ai/blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d"}, "hash": "fe7116fdf3f85fee93b2cdcc7294885a613241e24a3b263afa490edbb462fda6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c18ad5d-bbcf-442e-8d17-f32b288c853e", "node_type": "1", "metadata": {}, "hash": "a1218746b0fea93afefc99ad93d262190969c845c9e2d2cdf60062e869bf3ce3", "class_name": "RelatedNodeInfo"}}, "text": "##  Step 3: Evaluation\n\nThe model has been finetuned and can be served from the cloud. We can run some\nbasic evaluations using sample data from sql-create-context to compare the\nperformance of the finetuned model vs. the baseline Llama 2 model.\n\n    \n    \n    modal run src.eval_sql::main\n\nThe results demonstrate a massive improvement for the finetuned model:\n\n    \n    \n    Input 1: {'input': 'Which region (year) has Abigail at number 7, Sophia at number 1 and Aaliyah at number 5?', 'context': 'CREATE TABLE table_name_12 (region__year_ VARCHAR, no_5 VARCHAR, no_7 VARCHAR, no_1 VARCHAR)', 'output': 'SELECT region__year_ FROM table_name_12 WHERE no_7 = \"abigail\" AND no_1 = \"sophia\" AND\n    no_5 = \"aaliyah\"'}\n    Output 1 (finetuned model): SELECT region__year_ FROM table_name_12 WHERE no_7 = \"abigail\" AND no_1 = \"aaliyah\" AND no_5 = \"sophia\"\n    Output 1 (base model): SELECT * FROM table_name_12 WHERE region__year = '2018' AND no_5 = 'Abigail' AND no_7 = 'Sophia' AND no_1 = 'Aaliyah';\n    \n    \n    Input 2: {'input': 'Name the result/games for 54741', 'context': 'CREATE TABLE table_21436373_11 (result_games VARCHAR, attendance VARCHAR)', 'output': 'SELECT result_games FROM table_21436373_11 WHERE attendance = 54741'}\n    Output 2 (finetuned model): SELECT result_games FROM table_21436373_11 WHERE attendance = \"54741\"\n    Output 2 (base model): SELECT * FROM table_21436373_11 WHERE result_games = 'name' AND attendance &gt; 0;\n\nWhereas the base model produces wrongly formatted outputs, or incorrect SQL\nstatements,\n\nthe finetuned model is able to produce outputs that are much closer to that of\nthe expected output.\n\n##  Step 4: Integrating the Finetuned Model with LlamaIndex\n\nWe can now use this model in LlamaIndex for text-to-SQL over any database.\n\nWe first define a test SQL database that we can then use to test the inference\ncapabilities of the model.\n\n", "mimetype": "text/plain", "start_char_idx": 8386, "end_char_idx": 10267, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c18ad5d-bbcf-442e-8d17-f32b288c853e": {"__data__": {"id_": "4c18ad5d-bbcf-442e-8d17-f32b288c853e", "embedding": null, "metadata": {"filename": "easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.md", "extension": ".md", "title": "Easily Finetune Llama 2 for Your Text-to-SQL Applications", "date": "Aug 17, 2023", "url": "https://www.llamaindex.ai/blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bab77fe8-f4b9-466e-95ea-cf9ad876baf4", "node_type": "4", "metadata": {"filename": "easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.md", "extension": ".md", "title": "Easily Finetune Llama 2 for Your Text-to-SQL Applications", "date": "Aug 17, 2023", "url": "https://www.llamaindex.ai/blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d"}, "hash": "8f07bd4121d226435365b3fbb47f1c14641cb90eb089de88843541dcc3b449c1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c98a4e4-1786-4f92-80ff-ba018be5ae7d", "node_type": "1", "metadata": {"filename": "easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.md", "extension": ".md", "title": "Easily Finetune Llama 2 for Your Text-to-SQL Applications", "date": "Aug 17, 2023", "url": "https://www.llamaindex.ai/blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d"}, "hash": "2b3ab2e5f7115c827f8b7f274a07c865069da54746a3fde5460d771ba6dcfdc6", "class_name": "RelatedNodeInfo"}}, "text": "We create a toy ` city_stats ` table that contains city name, population, and\ncountry information, and populate it with a few sample cities.\n\n    \n    \n    db_file = \"cities.db\"\n    engine = create_engine(f\"sqlite:///{db_file}\")\n    metadata_obj = MetaData()\n    # create city SQL table\n    table_name = \"city_stats\"\n    city_stats_table = Table(\n        table_name,\n        metadata_obj,\n        Column(\"city_name\", String(16), primary_key=True),\n        Column(\"population\", Integer),\n        Column(\"country\", String(16), nullable=False),\n    )\n    metadata_obj.create_all(engine)\n\nThis is stored in a ` cities.db ` file.\n\nWe can then use Modal to load both the finetuned model and this database file\ninto the ` NLSQLTableQueryEngine ` in LlamaIndex - this query engine allows\nusers easily start performing text-to-SQL over a given database.\n\n    \n    \n    modal run src.inference_sql_llamaindex::main --query \"Which city has the highest population?\" --sqlite-file-path \"nbs/cities.db\" --model-dir \"model_sql\" --use-finetuned-model True\n\nWe get a response like the following:\n\n    \n    \n    SQL Query: SELECT MAX(population) FROM city_stats WHERE country = \"United States\"\n    Response: [(2679000,)]\n\n#  Conclusion\n\nAnd that\u2019s basically it! This tutorial provides a very high-level way for you\nto get started finetuning a Llama 2 model on generating SQL statements, and\nshowcases end-to-end how you can plug it into your text-to-SQL workflows with\nLlamaIndex.\n\n**Resources**\n\nFor the sake of completeness we\u2019re linking all of our resources again here.\n\nTutorial repo: [ https://github.com/run-llama/modal_finetune_sql\n](https://github.com/run-llama/modal_finetune_sql) (adapted from [ doppel-bot\n](https://modal.com/) ).\n\n[ Jupyter notebook guide ](https://github.com/run-\nllama/modal_finetune_sql/blob/main/tutorial.ipynb) .\n\nStack:\n\n  * ` [b-mc2/sql-create-context ` from Hugging Face datasets]( [ https://huggingface.co/datasets/b-mc2/sql-create-context ](https://huggingface.co/datasets/b-mc2/sql-create-context) ) \n  * [ OpenLLaMa ](https://github.com/openlm-research/open_llama)\n  * [ PEFT ](https://github.com/huggingface/peft)\n  * [ Modal ](https://modal.com/) (+ [ doppel-bot repo ](https://github.com/modal-labs/doppel-bot) ). \n  * [ LlamaIndex ](https://www.llamaindex.ai/)\n\nSpecial mention: [ Llama 2 tutorial from Anyscale\n](https://www.anyscale.com/blog/fine-tuning-llama-2-a-comprehensive-case-\nstudy-for-tailoring-models-to-unique-applications) .\n\n", "mimetype": "text/plain", "start_char_idx": 10267, "end_char_idx": 12734, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b701c90b-3bde-4a5b-a5eb-886e47f1e748": {"__data__": {"id_": "b701c90b-3bde-4a5b-a5eb-886e47f1e748", "embedding": null, "metadata": {"filename": "llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.md", "extension": ".md", "title": "LlamaIndex: Harnessing the Power of Text2SQL and RAG to Analyze Product Reviews", "date": "Aug 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9f5e7985-169b-42e2-af2c-99878578fd00", "node_type": "4", "metadata": {"filename": "llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.md", "extension": ".md", "title": "LlamaIndex: Harnessing the Power of Text2SQL and RAG to Analyze Product Reviews", "date": "Aug 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b"}, "hash": "ff11ebb3d7de32b43dc883b6272a9ff5d0f95926e8f0e37dbe9041b25ea90f9d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f96fdff-5315-43bf-a356-c63c7149ac4e", "node_type": "1", "metadata": {}, "hash": "9dc021f4ccae6e2a98d99f49487c8a0da9622f7f1b28666fb0755998b50b00d9", "class_name": "RelatedNodeInfo"}}, "text": "#  Introduction\n\nE-commerce platforms, such as Amazon and Walmart, are teeming with products\nthat attract a multitude of reviews every single day. These reviews are\ncrucial touchpoints that reflect consumer sentiments about products. But how\ncan businesses sift through vast databases to derive meaningful insights from\nthese reviews?\n\nThe answer lies in combining SQL with RAG (Retrieval Augmented Generation)\nthrough LlamaIndex.\n\nLet\u2019s deep dive into this!\n\n#  Sample Dataset of Product Reviews\n\nFor the purpose of this demonstration, we\u2019ve generated a sample dataset using\nGPT-4 that comprises reviews for three products: iPhone13, SamsungTV, and an\nErgonomic Chair. Here\u2019s a sneak peek:\n\n  * iPhone13: \u201cAmazing battery life and camera quality. Best iPhone yet.\u201d \n  * SamsungTV: \u201cImpressive picture clarity and vibrant colors. A top-notch TV.\u201d \n  * Ergonomic Chair: \u201cFeels really comfortable even after long hours.\u201d \n\nHere is a sample dataset.\n\n    \n    \n    rows = [\n        # iPhone13 Reviews\n        {\"category\": \"Phone\", \"product_name\": \"Iphone13\", \"review\": \"The iPhone13 is a stellar leap forward. From its sleek design to the crystal-clear display, it screams luxury and functionality. Coupled with the enhanced battery life and an A15 chip, it's clear Apple has once again raised the bar in the smartphone industry.\"},\n        {\"category\": \"Phone\", \"product_name\": \"Iphone13\", \"review\": \"This model brings the brilliance of the ProMotion display, changing the dynamics of screen interaction. The rich colors, smooth transitions, and lag-free experience make daily tasks and gaming absolutely delightful.\"},\n        {\"category\": \"Phone\", \"product_name\": \"Iphone13\", \"review\": \"The 5G capabilities are the true game-changer. Streaming, downloading, or even regular browsing feels like a breeze. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1804, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f96fdff-5315-43bf-a356-c63c7149ac4e": {"__data__": {"id_": "9f96fdff-5315-43bf-a356-c63c7149ac4e", "embedding": null, "metadata": {"filename": "llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.md", "extension": ".md", "title": "LlamaIndex: Harnessing the Power of Text2SQL and RAG to Analyze Product Reviews", "date": "Aug 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9f5e7985-169b-42e2-af2c-99878578fd00", "node_type": "4", "metadata": {"filename": "llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.md", "extension": ".md", "title": "LlamaIndex: Harnessing the Power of Text2SQL and RAG to Analyze Product Reviews", "date": "Aug 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b"}, "hash": "ff11ebb3d7de32b43dc883b6272a9ff5d0f95926e8f0e37dbe9041b25ea90f9d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b701c90b-3bde-4a5b-a5eb-886e47f1e748", "node_type": "1", "metadata": {"filename": "llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.md", "extension": ".md", "title": "LlamaIndex: Harnessing the Power of Text2SQL and RAG to Analyze Product Reviews", "date": "Aug 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b"}, "hash": "a254a74e94a456313b9297325c4c94aeb7ba9a963d721001a5c50d7a353565bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d5b3a3d-a8bf-4b1a-9c24-1143b12ad18f", "node_type": "1", "metadata": {}, "hash": "7bff40e08b1a7d739aef67aaecf24dc0c4204c6daaf30507b57f996aceb69d82", "class_name": "RelatedNodeInfo"}}, "text": "It's remarkable how seamless the integration feels, and it's obvious that Apple has invested a lot in refining the experience.\"},\n    \n        # SamsungTV Reviews\n        {\"category\": \"TV\", \"product_name\": \"SamsungTV\", \"review\": \"Samsung's display technology has always been at the forefront, but with this TV, they've outdone themselves. Every visual is crisp, the colors are vibrant, and the depth of the blacks is simply mesmerizing. The smart features only add to the luxurious viewing experience.\"},\n        {\"category\": \"TV\", \"product_name\": \"SamsungTV\", \"review\": \"This isn't just a TV; it's a centerpiece for the living room. The ultra-slim bezels and the sleek design make it a visual treat even when it's turned off. And when it's on, the 4K resolution delivers a cinematic experience right at home.\"},\n        {\"category\": \"TV\", \"product_name\": \"SamsungTV\", \"review\": \"The sound quality, often an oversight in many TVs, matches the visual prowess. It creates an enveloping atmosphere that's hard to get without an external sound system. ", "mimetype": "text/plain", "start_char_idx": 1804, "end_char_idx": 2852, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d5b3a3d-a8bf-4b1a-9c24-1143b12ad18f": {"__data__": {"id_": "2d5b3a3d-a8bf-4b1a-9c24-1143b12ad18f", "embedding": null, "metadata": {"filename": "llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.md", "extension": ".md", "title": "LlamaIndex: Harnessing the Power of Text2SQL and RAG to Analyze Product Reviews", "date": "Aug 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9f5e7985-169b-42e2-af2c-99878578fd00", "node_type": "4", "metadata": {"filename": "llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.md", "extension": ".md", "title": "LlamaIndex: Harnessing the Power of Text2SQL and RAG to Analyze Product Reviews", "date": "Aug 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b"}, "hash": "ff11ebb3d7de32b43dc883b6272a9ff5d0f95926e8f0e37dbe9041b25ea90f9d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f96fdff-5315-43bf-a356-c63c7149ac4e", "node_type": "1", "metadata": {"filename": "llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.md", "extension": ".md", "title": "LlamaIndex: Harnessing the Power of Text2SQL and RAG to Analyze Product Reviews", "date": "Aug 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b"}, "hash": "9a0e793bd6164992edef5e7833919763f2344aa4778d1a18f07c1b926cdf0e4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0212bcbf-721c-42a7-909e-c5a1632be4b6", "node_type": "1", "metadata": {}, "hash": "430b42344a2647db6e262888aacf36048ef7a748fd343fa20da3d23c8dd3a153", "class_name": "RelatedNodeInfo"}}, "text": "Combined with its user-friendly interface, it's the TV I've always dreamt of.\"},\n    \n        # Ergonomic Chair Reviews\n        {\"category\": \"Furniture\", \"product_name\": \"Ergonomic Chair\", \"review\": \"Shifting to this ergonomic chair was a decision I wish I'd made earlier. Not only does it look sophisticated in its design, but the level of comfort is unparalleled. Long hours at the desk now feel less daunting, and my back is definitely grateful.\"},\n        {\"category\": \"Furniture\", \"product_name\": \"Ergonomic Chair\", \"review\": \"The meticulous craftsmanship of this chair is evident. Every component, from the armrests to the wheels, feels premium. The adjustability features mean I can tailor it to my needs, ensuring optimal posture and comfort throughout the day.\"},\n        {\"category\": \"Furniture\", \"product_name\": \"Ergonomic Chair\", \"review\": \"I was initially drawn to its aesthetic appeal, but the functional benefits have been profound. The breathable material ensures no discomfort even after prolonged use, and the robust build gives me confidence that it's a chair built to last.\"},\n    ]\n\n#  Setting up an In-Memory Database\n\nTo process our data, we\u2019re using an in-memory SQLite database. ", "mimetype": "text/plain", "start_char_idx": 2852, "end_char_idx": 4056, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0212bcbf-721c-42a7-909e-c5a1632be4b6": {"__data__": {"id_": "0212bcbf-721c-42a7-909e-c5a1632be4b6", "embedding": null, "metadata": {"filename": "llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.md", "extension": ".md", "title": "LlamaIndex: Harnessing the Power of Text2SQL and RAG to Analyze Product Reviews", "date": "Aug 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9f5e7985-169b-42e2-af2c-99878578fd00", "node_type": "4", "metadata": {"filename": "llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.md", "extension": ".md", "title": "LlamaIndex: Harnessing the Power of Text2SQL and RAG to Analyze Product Reviews", "date": "Aug 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b"}, "hash": "ff11ebb3d7de32b43dc883b6272a9ff5d0f95926e8f0e37dbe9041b25ea90f9d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2d5b3a3d-a8bf-4b1a-9c24-1143b12ad18f", "node_type": "1", "metadata": {"filename": "llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.md", "extension": ".md", "title": "LlamaIndex: Harnessing the Power of Text2SQL and RAG to Analyze Product Reviews", "date": "Aug 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b"}, "hash": "1af2c4022dfb3195e83d8bb1f4100e0a1bd5e6457c0950f38b38454e12d0e892", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "108b0c19-c33c-4faf-af99-5b35d32d708c", "node_type": "1", "metadata": {}, "hash": "ec3cfcec64d6fba4b05d2c15bd0a3f8aa59aec634f908d0e772f6bc565fa9f96", "class_name": "RelatedNodeInfo"}}, "text": "SQLAlchemy\nprovides an efficient way to model, create, and interact with this database.\nHere\u2019s how our ` product_reviews ` table structure looks:\n\n  * ` id ` (Integer, Primary Key) \n  * ` category ` (String) \n  * ` product_name ` (String) \n  * ` review ` (String, Not Null) \n\nOnce we\u2019ve defined our table structure, we populate it with our sample\ndataset.\n\n    \n    \n    engine = create_engine(\"sqlite:///:memory:\")\n    metadata_obj = MetaData()\n    \n    # create product reviews SQL table\n    table_name = \"product_reviews\"\n    city_stats_table = Table(\n        table_name,\n        metadata_obj,\n        Column(\"id\", Integer(), primary_key=True),\n        Column(\"category\", String(16), primary_key=True),\n        Column(\"product_name\", Integer),\n        Column(\"review\", String(16), nullable=False)\n    )\n    metadata_obj.create_all(engine)\n    \n    sql_database = SQLDatabase(engine, include_tables=[\"product_reviews\"])\n    \n    for row in rows:\n        stmt = insert(city_stats_table).values(**row)\n        with engine.connect() as connection:\n            cursor = connection.execute(stmt)\n            connection.commit()\n\n#  Analysing Product Reviews \u2014 Text2SQL + RAG\n\nDeriving insights from data often requires intricate questioning.\n\nSQL + RAG in LlamaIndex simplifies this by breaking it into a three-step\nprocess:\n\n  1. **Decomposition of the Question:**\n\n  * Primary Query Formation: Frame the main question in natural language to extract preliminary data from the SQL table. \n  * Secondary Query Formation: Construct an auxiliary question to refine or interpret the results of the primary query. \n\n**2\\. Data Retrieval** : Run the primary query using the Text2SQL LlamaIndex\nmodule to obtain the initial set of results.\n\n**3\\. Final Answer Generation:** Use List Index to further refine the results\nbased on the secondary question, leading to the conclusive answer.\n\nLet\u2019s start doing it step by step.\n\n#  Decomposing User Query into Two Phases\n\nWhen working with a relational database, it\u2019s often helpful to break down user\nqueries into more manageable parts. This makes it easier to retrieve accurate\ndata from our database and subsequently process or interpret this data to meet\nthe user\u2019s needs. We\u2019ve designed an approach to decompose queries into two\ndistinct questions by giving an example to ` gpt-3.5-turbo ` model to generate\ntwo distinct questions.\n\nLet\u2019s apply this to the query \u201cGet the summary of reviews of Iphone13\u201d and our\nsystem would generate:\n\n  * Database Query: \u201cRetrieve reviews related to iPhone13 from the table.\u201d \n  * Interpretation Query: \u201cSummarize the retrieved reviews.\u201d \n\nThis approach ensures that we cater to both the data retrieval and data\ninterpretation needs, resulting in more accurate and tailored responses to\nuser queries.\n\n    \n    \n    def generate_questions(user_query: str) -&gt; List[str]:\n      system_message = '''\n      You are given with Postgres table with the following columns.\n    \n      city_name, population, country, reviews.\n    \n      Your task is to decompose the given question into the following two questions.\n    \n      1. Question in natural language that needs to be asked to retrieve results from the table.\n      2. Question that needs to be asked on the top of the result from the first question to provide the final answer.\n    \n      Example:\n    \n      Input:\n      How is the culture of countries whose population is more than 5000000\n    \n      Output:\n      1. Get the reviews of countries whose population is more than 5000000\n      2. Provide the culture of countries\n      '''\n    \n      messages = [\n          ChatMessage(role=\"system\", content=system_message),\n          ChatMessage(role=\"user\", content=user_query),\n      ]\n      generated_questions = llm.chat(messages).message.content.split('\\n')\n    \n      return generated_questions\n    \n    user_query = \"Get the summary of reviews of Iphone13\"\n    \n    text_to_sql_query, rag_query = generate_questions(user_query)\n\n#  Data Retrieval \u2014 Executing the Primary Query\n\nWhen we decompose a user\u2019s question into its constituent parts, the first step\nis to convert the \u201cDatabase Query in Natural Language\u201d into an actual SQL\nquery that can be run against our database. In this section, we\u2019ll use the\nLlamaIndex\u2019s ` NLSQLTableQueryEngine ` to handle the conversion and execution\nof this SQL query.\n\n**Setting up the NLSQLTableQueryEngine:**\n\nThe ` NLSQLTableQueryEngine ` is a powerful tool that takes natural language\nqueries and converts them into SQL queries. We initiate this by providing the\nnecessary details:\n\n  * ` sql_database ` : This represents our SQL database connection details. \n  ", "mimetype": "text/plain", "start_char_idx": 4056, "end_char_idx": 8691, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "108b0c19-c33c-4faf-af99-5b35d32d708c": {"__data__": {"id_": "108b0c19-c33c-4faf-af99-5b35d32d708c", "embedding": null, "metadata": {"filename": "llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.md", "extension": ".md", "title": "LlamaIndex: Harnessing the Power of Text2SQL and RAG to Analyze Product Reviews", "date": "Aug 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9f5e7985-169b-42e2-af2c-99878578fd00", "node_type": "4", "metadata": {"filename": "llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.md", "extension": ".md", "title": "LlamaIndex: Harnessing the Power of Text2SQL and RAG to Analyze Product Reviews", "date": "Aug 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b"}, "hash": "ff11ebb3d7de32b43dc883b6272a9ff5d0f95926e8f0e37dbe9041b25ea90f9d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0212bcbf-721c-42a7-909e-c5a1632be4b6", "node_type": "1", "metadata": {"filename": "llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.md", "extension": ".md", "title": "LlamaIndex: Harnessing the Power of Text2SQL and RAG to Analyze Product Reviews", "date": "Aug 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b"}, "hash": "a48fb0a419a362b15fe82c0589e6a600de8657ec968558f9eb3d4ae625330b8a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee032901-d0bb-4bf8-b84b-7c51d379c548", "node_type": "1", "metadata": {}, "hash": "4f758df001eb7a05e4fa054724fe4a935fddde30f0d698183abab7299537d66e", "class_name": "RelatedNodeInfo"}}, "text": "* ` tables ` : We specify which table(s) our query will be run against. In this scenario, we're targeting the ` product_reviews ` table. \n  * ` synthesize_response ` : When set to ` False ` , this ensures we receive raw SQL responses without additional synthesis. \n  * ` service_context ` : This is an optional parameter, which could be used to provide service-specific settings or plugins. \n\n    \n    \n    sql_query_engine = NLSQLTableQueryEngine(\n        sql_database=sql_database,\n        tables=[\"product_reviews\"],\n        synthesize_response=False,\n        service_context=service_context\n    )\n\n**Executing the natural language Query:**\n\nAfter setting up the engine, the next step is executing our natural language\nquery against it. The engine\u2019s ` query() ` method is used for this purpose.\n\n    \n    \n    sql_response = sql_query_engine.query(text_to_sql_query)\n\n**Processing the SQL Response:**\n\nThe result of our SQL query is usually a list of rows (with each row\nrepresented as a list of reviews). To make it more readable and usable for the\nthird step of processing summarizing reviews, we convert this result into a\nsingle string.\n\n    \n    \n    sql_response_list = ast.literal_eval(sql_response.response)\n    text = [' '.join(t) for t in sql_response_list]\n    text = ' '.join(text)\n\nYou can check the generated SQL query in ` sql_response.metadata[\"sql_query\"].\n`\n\nBy following this process, we\u2019re able to seamlessly integrate natural language\nprocessing with SQL query execution. Let\u2019s go with the last step in this\nprocess for getting a summary of the reviews.\n\n#  Refining and Interpreting the reviews with ListIndex:\n\nAfter obtaining the primary set of results from the SQL query, there are often\nsituations where further refinement or interpretation is required. This is\nwhere ` ListIndex ` from LlamaIndex plays a crucial role. It allows us to\nexecute the secondary question on our obtained text data to get a refined\nanswer.\n\n    \n    \n    listindex = ListIndex([Document(text=text)])\n    list_query_engine = listindex.as_query_engine()\n    \n    response = list_query_engine.query(rag_query)\n    \n    print(response.response)\n\nNow let\u2019s wrap everything under a function and try out a few interesting\nexamples:\n\n    \n    \n    \"\"\"Function to perform SQL+RAG\"\"\"\n    \n    def sql_rag(user_query: str) -&gt; str:\n      text_to_sql_query, rag_query = generate_questions(user_query)\n    \n      sql_response = sql_query_engine.query(text_to_sql_query)\n    \n      sql_response_list = ast.literal_eval(sql_response.response)\n    \n      text = [' '.join(t) for t in sql_response_list]\n      text = ' '.join(text)\n    \n      listindex = ListIndex([Document(text=text)])\n      list_query_engine = listindex.as_query_engine()\n    \n      summary = list_query_engine.query(rag_query)\n    \n      return summary.response\n\n##  Examples:\n\n    \n    \n    sql_rag(\"How is the sentiment of SamsungTV product?\")\n\n_The sentiment of the reviews for the Samsung TV product is generally\npositive. ", "mimetype": "text/plain", "start_char_idx": 8691, "end_char_idx": 11681, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee032901-d0bb-4bf8-b84b-7c51d379c548": {"__data__": {"id_": "ee032901-d0bb-4bf8-b84b-7c51d379c548", "embedding": null, "metadata": {"filename": "llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.md", "extension": ".md", "title": "LlamaIndex: Harnessing the Power of Text2SQL and RAG to Analyze Product Reviews", "date": "Aug 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9f5e7985-169b-42e2-af2c-99878578fd00", "node_type": "4", "metadata": {"filename": "llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.md", "extension": ".md", "title": "LlamaIndex: Harnessing the Power of Text2SQL and RAG to Analyze Product Reviews", "date": "Aug 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b"}, "hash": "ff11ebb3d7de32b43dc883b6272a9ff5d0f95926e8f0e37dbe9041b25ea90f9d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "108b0c19-c33c-4faf-af99-5b35d32d708c", "node_type": "1", "metadata": {"filename": "llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.md", "extension": ".md", "title": "LlamaIndex: Harnessing the Power of Text2SQL and RAG to Analyze Product Reviews", "date": "Aug 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b"}, "hash": "988225120d7847b9c428bf8b4d18c4115bedcbf612febe985f116f23bb45c6c1", "class_name": "RelatedNodeInfo"}}, "text": "Users express satisfaction with the picture clarity, vibrant colors,\nand stunning picture quality. They appreciate the smart features, user-\nfriendly interface, and easy connectivity options. The sleek design and wall-\nmounting capability are also praised. The ambient mode, gaming mode, and HDR\ncontent are mentioned as standout features. Users find the remote control with\nvoice command convenient and appreciate the regular software updates. However,\nsome users mention that the sound quality could be better and suggest using an\nexternal audio system. Overall, the reviews indicate that the Samsung TV is\nconsidered a solid investment for quality viewing._\n\n    \n    \n    sql_rag(\"Are people happy with Ergonomic Chair?\")\n\n_The overall satisfaction of people with the Ergonomic Chair is high._\n\nYou can play around with the approach and dataset in the Google Colab Notebook\n\u2014 [ here ](https://colab.research.google.com/drive/13le_rgEo-\nwaW5ZWjWDEyUf64R6n_4Cez?usp=sharing) .\n\n#  Conclusion\n\nIn the era of e-commerce, where user reviews dictate the success or failure of\nproducts, the ability to rapidly analyze and interpret vast swaths of textual\ndata is paramount. LlamaIndex, through its ingenious integration of SQL and\nRAG, offers businesses a powerful tool to glean actionable insights from such\ndatasets. By seamlessly blending structured SQL queries with the abstraction\nof natural language processing, we\u2019ve showcased a streamlined approach to\ntransform vague user queries into precise, informative answers.\n\nWith this approach, businesses can now efficiently sift through mountains of\nreviews, extract the essence of user sentiments, and make informed decisions.\nWhether it\u2019s about gauging the overall sentiment for a product, understanding\nspecific feature feedback, or even tracking the evolution of reviews over\ntime, the Text2SQL+RAG methodology in LlamaIndex is the harbinger of a new age\nof data analytics.\n\n", "mimetype": "text/plain", "start_char_idx": 11681, "end_char_idx": 13609, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5284a101-cc2e-4e81-b5c3-c7d442b4d288": {"__data__": {"id_": "5284a101-cc2e-4e81-b5c3-c7d442b4d288", "embedding": null, "metadata": {"filename": "zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc.md", "extension": ".md", "title": "Zep and LlamaIndex: A Vector Store Walkthrough", "date": "Aug 11, 2023", "url": "https://www.llamaindex.ai/blog/zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "635b2115-d0c1-4c5f-9b24-cebc3f4db1ec", "node_type": "4", "metadata": {"filename": "zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc.md", "extension": ".md", "title": "Zep and LlamaIndex: A Vector Store Walkthrough", "date": "Aug 11, 2023", "url": "https://www.llamaindex.ai/blog/zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc"}, "hash": "6ba237ea991d4c238ba961f51b699d73bcb50bc13de505b99c8cac22ae6cc875", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b554c31-66ed-47fd-98bd-b7168cdbe402", "node_type": "1", "metadata": {}, "hash": "14a069e3258ce7e7d0a0bd7dcd6b5b104f7391754ba93d879aec870a55e0f5a8", "class_name": "RelatedNodeInfo"}}, "text": "Editor\u2019s Note: This article was written by [ Daniel\n](https://twitter.com/danielchalef) at Zep for the LlamaIndex blog.\n\nZep is a [ long-term memory store for LLM applications\n](https://www.getzep.com/) . With Zep, developers can easily add relevant\ndocuments, chat history memory & rich user data to LLM app prompts. Document\nand chat history storage, embedding, enrichment, and more are taken care of by\nthe Zep service.\n\nIn this article, we demonstrate how to use Zep\u2019s new Document Vector Store\nwith the (also new) ZepVectorStore for LlamaIndex.\n\n#  Installing Zep and some important concepts\n\n[ Zep is open source ](https://github.com/getzep/zep) and may be [ installed\nvia Docker ](https://docs.getzep.com/deployment/quickstart/) , or to\nKubernetes and hosting platforms such as Render. SDKs are available for Python\nand TypeScript, and frameworks such as LangChain and LlamaIndex ship with\nsupport for Zep.\n\nZep stores documents in Collections, with the document text, embeddings, and\nmetadata all colocated. This enables hybrid semantic search over a collection,\nwith results filtered by JSONPath queries against document metadata. When\nusing Zep with LlamaIndex, LlamaIndex filters are translated for use by Zep.\n\nA document or document chunk is equivalent to a LlamaIndex TextNode or\nNodeWithEmbedding.\n\nCollections can be optionally set to automatically embed texts using a service\nsuch as OpenAI or locally using an embedding model of your choice. However,\nwhen using Zep with LlamaIndex, we rely on LlamaIndex\u2019s integrations with\nembedded services and libraries.\n\n#  Creating a ZepVectorStore and Document Collection\n\nYou will need to have [ installed Zep ](https://docs.getzep.com/) and have\nyour API URL and, optionally, authentication key handy.\n\n    \n    \n    from llama_index.vector_stores import ZepVectorStore\n    zep_api_url = \"http://localhost:8000\"\n    zep_api_key = \"&lt;optional_jwt_token&gt;\"\n    collection_name = \"babbage\" # The name of a new or existing collection\n    embedding_dimensions = 1536 # the dimensions of the embedding model you intend to use\n    vector_store = ZepVectorStore(\n      api_url=zep_api_url,\n      api_key=zep_api_key,\n      collection_name=collection_name,\n      embedding_dimensions=embedding_dimensions\n    )\n\nThe collection name is a unique identifier for your vector index and should\nonly contain alphanumeric characters. If the collection does not exist, Zep\nwill automatically create one for you.\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2459, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b554c31-66ed-47fd-98bd-b7168cdbe402": {"__data__": {"id_": "7b554c31-66ed-47fd-98bd-b7168cdbe402", "embedding": null, "metadata": {"filename": "zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc.md", "extension": ".md", "title": "Zep and LlamaIndex: A Vector Store Walkthrough", "date": "Aug 11, 2023", "url": "https://www.llamaindex.ai/blog/zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "635b2115-d0c1-4c5f-9b24-cebc3f4db1ec", "node_type": "4", "metadata": {"filename": "zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc.md", "extension": ".md", "title": "Zep and LlamaIndex: A Vector Store Walkthrough", "date": "Aug 11, 2023", "url": "https://www.llamaindex.ai/blog/zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc"}, "hash": "6ba237ea991d4c238ba961f51b699d73bcb50bc13de505b99c8cac22ae6cc875", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5284a101-cc2e-4e81-b5c3-c7d442b4d288", "node_type": "1", "metadata": {"filename": "zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc.md", "extension": ".md", "title": "Zep and LlamaIndex: A Vector Store Walkthrough", "date": "Aug 11, 2023", "url": "https://www.llamaindex.ai/blog/zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc"}, "hash": "c41aabe3ae81daec3b793934298f379561916bd3501b233fbffabc7b29e06f0e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d107517-6473-4bb7-9fa1-0416c06bb656", "node_type": "1", "metadata": {}, "hash": "e2a1cadb156cb4849571b308914c8ab88c8feb1b3ecaadeed06a5328e0a725c4", "class_name": "RelatedNodeInfo"}}, "text": "#  Creating and populating an Index\n\nBelow we\u2019ll use a common LlamaIndex pattern for loading content and adding it\nto an index. After loading the text data, we create a StorageContext backed by\nthe ZepVectorStore.\n\nWe then create the index using our loaded documents and Zep-backed storage\ncontext.\n\n    \n    \n    from llama_index import VectorStoreIndex, SimpleDirectoryReader\n    from llama_index.storage.storage_context import StorageContext\n    \n    documents = SimpleDirectoryReader(\"./babbages_calculating_engine/\").load_data()\n    \n    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n    index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n    \n    query = \"the sun and stars\"\n    \n    query_engine = index.as_query_engine()\n    response = query_engine.query(query)\n    \n    \n    print(str(response))\n    \n    \n    But one of the most signal examples of this kind, of which we are aware, is related by Mr Baily. The catalogue of stars published by the Astronomical Society was computed by two separate and independent persons, and was afterwards compared and examined with great care and attention by Mr Stratford. ", "mimetype": "text/plain", "start_char_idx": 2459, "end_char_idx": 3639, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d107517-6473-4bb7-9fa1-0416c06bb656": {"__data__": {"id_": "8d107517-6473-4bb7-9fa1-0416c06bb656", "embedding": null, "metadata": {"filename": "zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc.md", "extension": ".md", "title": "Zep and LlamaIndex: A Vector Store Walkthrough", "date": "Aug 11, 2023", "url": "https://www.llamaindex.ai/blog/zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "635b2115-d0c1-4c5f-9b24-cebc3f4db1ec", "node_type": "4", "metadata": {"filename": "zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc.md", "extension": ".md", "title": "Zep and LlamaIndex: A Vector Store Walkthrough", "date": "Aug 11, 2023", "url": "https://www.llamaindex.ai/blog/zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc"}, "hash": "6ba237ea991d4c238ba961f51b699d73bcb50bc13de505b99c8cac22ae6cc875", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b554c31-66ed-47fd-98bd-b7168cdbe402", "node_type": "1", "metadata": {"filename": "zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc.md", "extension": ".md", "title": "Zep and LlamaIndex: A Vector Store Walkthrough", "date": "Aug 11, 2023", "url": "https://www.llamaindex.ai/blog/zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc"}, "hash": "3a667773a80ff9de75f88491eb172b8c1e0b0efc25ca7f13e85115df00e63ea0", "class_name": "RelatedNodeInfo"}}, "text": "On examining this catalogue, and recalculating a portion of it, Mr Baily discovered an error in the case of the star\n\nFinally, we run a simple text query against the index and print the resulting\nnode\u2019s text.\n\n#  Hybrid search with metadata filters\n\nAs mentioned above, Zep also supports associating rich metadata with\ndocuments. This metadata can be an arbitrarily deep JSON structure. When\nworking with LlamaIndex, we currently support filtering on top-level keys in\nthe map.\n\nThe code below demonstrates running a vector search over an index and\nfiltering on metadata using LlamaIndex\u2019s MetadataFilters. We print the result\nand the normalized cosine similarity for the matching result.\n\n    \n    \n    from llama_index.schema import TextNode\n    from llama_index.vector_stores.types import ExactMatchFilter, MetadataFilters\n    \n    nodes = [\n       TextNode(\n           text=\"Not aware that tables of these squares existed, Bouvard, who calculated the tides for Laplace, underwent the labour of calculating the square of each individual sine in every case in which it occurred.\",\n           metadata={\n               \"topic\": \"math\",\n               \"entities\": \"laplace\",\n           },\n       ),\n       TextNode(\n           text=\"Within the limits of the lunar orbit there are not less than one thousand stars, which are so situated as to be in the moon's path, and therefore to exhibit, at some period or other, those desirable occultations.\",\n           metadata={\n               \"topic\": \"astronomy\",\n               \"entities\": \"moon\",\n           },\n       ),\n    ]\n    \n    \n    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n    index = VectorStoreIndex(nodes, storage_context=storage_context)\n    \n    \n    filters = MetadataFilters(filters=[ExactMatchFilter(key=\"topic\", value=\"astronomy\")])\n    \n    \n    retriever = index.as_retriever(filters=filters)\n    result = retriever.retrieve(\"What is the structure of our galaxy?\")\n    \n    \n    for r in result:\n       print(\"\\n\", r.node.text, r.score)\n    \n    \n    Within the limits of the lunar orbit there are not less than one thousand stars, which are so situated as to be in the moon's path, and therefore to exhibit, at some period or other, those desirable occultations.  0.6456785674\n\n#  Summing it up\n\nZep offers a single API for vector search over documents and chat history,\nallowing developers to populate prompts with both forms of long-term memory.\nLlamaIndex makes it extremely easy to populate Zep with content from a broad\nset of documents and data sources and query these sources when building\nprompts and other functionality for LLM apps.\n\n#  Next Steps\n\n  * Read the [ Zep Quick Start Guide ](https://docs.getzep.com/deployment/quickstart/)\n  * [ Zep and LlamaIndex Walkthrough Notebook ](https://github.com/jerryjliu/llama_index/tree/main/docs/examples/vector_stores/ZepIndexDemo.ipynb)\n  * [ Getting Started with LllamaIndex ](https://gpt-index.readthedocs.io/en/stable/index.html)\n\n", "mimetype": "text/plain", "start_char_idx": 3639, "end_char_idx": 6628, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9e14192-8020-4c06-bbd7-3189c19e7a11": {"__data__": {"id_": "a9e14192-8020-4c06-bbd7-3189c19e7a11", "embedding": null, "metadata": {"filename": "llamaindex-update-08-01-2023-185514d9b897.md", "extension": ".md", "title": "LlamaIndex Update \u2014 08/01/2023", "date": "Aug 1, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-08-01-2023-185514d9b897"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc2828e9-fbd2-4fe5-96df-1001a43afd6c", "node_type": "4", "metadata": {"filename": "llamaindex-update-08-01-2023-185514d9b897.md", "extension": ".md", "title": "LlamaIndex Update \u2014 08/01/2023", "date": "Aug 1, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-08-01-2023-185514d9b897"}, "hash": "1435c943c96b45e2ac54d2279465fe55c6ba707fcc3c80888352e715abf38857", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1ff3678-dd54-4c4c-86af-14dd23a06df3", "node_type": "1", "metadata": {}, "hash": "4089101a340aa052171051159cdb7d844f6bb9546975774079a754555967911c", "class_name": "RelatedNodeInfo"}}, "text": "Greetings once again, LlamaIndex community!\n\nWelcome to the third installment of our LlamaIndex Update series. Your active\nparticipation continues to drive our open-source community forward. We\nappreciate every contribution whether you\u2019re an experienced LlamaIndex\ncontributor or a newcomer!\n\nIn our latest edition, we\u2019ve prepared an assortment of updates for you. From\nadvancements in Data Agents and LlamaIndex TS, benchmarking, and a host of\ninspiring events, webinars, blog posts, and demos, we\u2019ve got plenty in store.\n\nWithout more ado, let\u2019s dive into these updates.\n\n#  New Features:\n\n  1. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 597, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1ff3678-dd54-4c4c-86af-14dd23a06df3": {"__data__": {"id_": "a1ff3678-dd54-4c4c-86af-14dd23a06df3", "embedding": null, "metadata": {"filename": "llamaindex-update-08-01-2023-185514d9b897.md", "extension": ".md", "title": "LlamaIndex Update \u2014 08/01/2023", "date": "Aug 1, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-08-01-2023-185514d9b897"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc2828e9-fbd2-4fe5-96df-1001a43afd6c", "node_type": "4", "metadata": {"filename": "llamaindex-update-08-01-2023-185514d9b897.md", "extension": ".md", "title": "LlamaIndex Update \u2014 08/01/2023", "date": "Aug 1, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-08-01-2023-185514d9b897"}, "hash": "1435c943c96b45e2ac54d2279465fe55c6ba707fcc3c80888352e715abf38857", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a9e14192-8020-4c06-bbd7-3189c19e7a11", "node_type": "1", "metadata": {"filename": "llamaindex-update-08-01-2023-185514d9b897.md", "extension": ".md", "title": "LlamaIndex Update \u2014 08/01/2023", "date": "Aug 1, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-08-01-2023-185514d9b897"}, "hash": "c832920399a466885cafcdf62068097dd08579bbe69dc78ac1755f72c5c1bc8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d7d4754-97cb-42e2-a69f-1f5143e7178c", "node_type": "1", "metadata": {}, "hash": "379496796cfa772a6b723a51644495e175c6ac4bea0b8a2f64e2babaecfe1a82", "class_name": "RelatedNodeInfo"}}, "text": "We heard you! LlamaIndex has completely revamped our documentation. The update includes new clearer documents on high-level concepts, detailed module guides, comprehensive tutorials, and an all-inclusive API reference. [ Docs ](https://gpt-index.readthedocs.io/en/latest/index.html) , [ Tweet ](https://twitter.com/llama_index/status/1678558820552040448?s=20)\n  2. LlamaIndex launched Data Agents, an innovative feature that combines AI agents with data. This launch introduces components like an agent reasoning loop and tool abstractions. Accompanied by an extensive upgrade to LlamaHub, the new feature offers more than 15 tool specs for easy integration. Data Agents enhance query capabilities and are designed to handle varied data applications. [ Docs ](https://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/agents/root.html) , [ Tweet ](https://twitter.com/jerryjliu0/status/1679185930287222784?s=20) , [ Blog Post ](https://medium.com/llamaindex-blog/data-agents-eed797d7972f)\n  3. LlamaIndex launched LlamaIndex.TS, a lean Typescript package for building robust Retrieval Augmented Generation (RAG) systems. It simplifies tasks like document parsing and tackling context window limitations. LlamaIndex.TS is ideal for quickly building apps like using frameworks like Next.JS to chat over your data. [ Docs ](https://ts.llamaindex.ai/) , [ Tweet ](https://twitter.com/llama_index/status/1683556970945736704?s=20) , [ Blogpost ](https://medium.com/llamaindex-blog/introducing-llamaindex-ts-89f41a1f24ab)\n  4. LlamaIndex teams up with Zapier Natural Language API (NLA), reducing the cognitive load on the data agent when handling APIs with multiple parameters. Zapier NLA translates complex third-party APIs into simpler interfaces using a single natural language parameter: instruction. This helps the data agent concentrate on tool selection and action orchestration. [ Tweet ](https://twitter.com/llama_index/status/1683880312173129728?s=20) , [ Blogpost ](https://medium.com/llamaindex-blog/data-agents-zapier-nla-67146395ce1)\n  5. LlamaIndex\u2019s ` ContextChatEngine ` addresses the issue of conversational agents hallucinating information by ensuring retrieval of context with every user interaction. This feature, compatible with all ReAct and OpenAI Function agent types, prepends retrieved-context as a system message. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/chat_engine/chat_engine_context.html) , [ Tweet ](https://twitter.com/llama_index/status/1685690590527430656?s=20)\n  6. ", "mimetype": "text/plain", "start_char_idx": 597, "end_char_idx": 3125, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d7d4754-97cb-42e2-a69f-1f5143e7178c": {"__data__": {"id_": "3d7d4754-97cb-42e2-a69f-1f5143e7178c", "embedding": null, "metadata": {"filename": "llamaindex-update-08-01-2023-185514d9b897.md", "extension": ".md", "title": "LlamaIndex Update \u2014 08/01/2023", "date": "Aug 1, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-08-01-2023-185514d9b897"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc2828e9-fbd2-4fe5-96df-1001a43afd6c", "node_type": "4", "metadata": {"filename": "llamaindex-update-08-01-2023-185514d9b897.md", "extension": ".md", "title": "LlamaIndex Update \u2014 08/01/2023", "date": "Aug 1, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-08-01-2023-185514d9b897"}, "hash": "1435c943c96b45e2ac54d2279465fe55c6ba707fcc3c80888352e715abf38857", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1ff3678-dd54-4c4c-86af-14dd23a06df3", "node_type": "1", "metadata": {"filename": "llamaindex-update-08-01-2023-185514d9b897.md", "extension": ".md", "title": "LlamaIndex Update \u2014 08/01/2023", "date": "Aug 1, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-08-01-2023-185514d9b897"}, "hash": "6e851d92e570928a4f32b064847568593bf3f86b41490bed4e00ea493400b6c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca64912f-dfa1-4351-b428-8e45b105672f", "node_type": "1", "metadata": {}, "hash": "48eec396255d99891252525efe6df82c45722959eec20b1534d73599006cabcc", "class_name": "RelatedNodeInfo"}}, "text": "This month marked the launch of two new exciting LLMs. First off was Anthropic Claude 2.0. We launched with day 1 support of the new model. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/llm/anthropic.html) , [ Tweet ](https://twitter.com/jerryjliu0/status/1678944607965708288?s=20) . The other one was Llama2, and LlamaIndex now offers best-in-class integration with the Llama2 model on Replicate. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/llm/llama_2.html) , [ Tweet ](https://twitter.com/llama_index/status/1681438906296991749?s=20)\n  7. LlamaIndex is day one compatible with Chroma v0.4.0, enhancing support for in-memory, persisted, and self-hosted databases. This upgrade simplifies the use of Chroma within LlamaIndex, making database handling easier and more efficient. [ Tweet ](https://twitter.com/llama_index/status/1681167979176665088?s=20)\n  8. LlamaIndex\u2019s newly launched Data Agents can automatically interact with any API defined via an OpenAPI spec. It handles indexing/loading of large data from API specs and facilitates easy integration of the OpenAPI Tool, enhancing the ability to call web services. [ Docs ](https://llamahub.ai/l/tools-openapi) , [ Tweet ](https://twitter.com/llama_index/status/1679522417558040577?s=20)\n  9. LlamaIndex now utilizes the ` rebel-large ` model for high-speed relation extraction. Combined with CUDA, you can generate knowledge graphs from your text data. [ Tweet ](https://twitter.com/jerryjliu0/status/1685078740555169793?s=20)\n  10. ", "mimetype": "text/plain", "start_char_idx": 3125, "end_char_idx": 4649, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca64912f-dfa1-4351-b428-8e45b105672f": {"__data__": {"id_": "ca64912f-dfa1-4351-b428-8e45b105672f", "embedding": null, "metadata": {"filename": "llamaindex-update-08-01-2023-185514d9b897.md", "extension": ".md", "title": "LlamaIndex Update \u2014 08/01/2023", "date": "Aug 1, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-08-01-2023-185514d9b897"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc2828e9-fbd2-4fe5-96df-1001a43afd6c", "node_type": "4", "metadata": {"filename": "llamaindex-update-08-01-2023-185514d9b897.md", "extension": ".md", "title": "LlamaIndex Update \u2014 08/01/2023", "date": "Aug 1, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-08-01-2023-185514d9b897"}, "hash": "1435c943c96b45e2ac54d2279465fe55c6ba707fcc3c80888352e715abf38857", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d7d4754-97cb-42e2-a69f-1f5143e7178c", "node_type": "1", "metadata": {"filename": "llamaindex-update-08-01-2023-185514d9b897.md", "extension": ".md", "title": "LlamaIndex Update \u2014 08/01/2023", "date": "Aug 1, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-08-01-2023-185514d9b897"}, "hash": "e23de6975a3d1ad0908929bef65409dbe87dc89383a1d37dda8e5a4be170e4aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d8ef2d0-1688-4a68-8c71-ff04421feb49", "node_type": "1", "metadata": {}, "hash": "0c6273659eda384dea74a3f0be20e405c691babe7ede5c9b11dd8e5de5dd14a0", "class_name": "RelatedNodeInfo"}}, "text": "LlamaIndex introduced a code interpreter tool. This feature equips any LLM with the ability to analyze data and generate visualizations, expanding their capabilities similar to those of ChatGPT. [ Tweet ](https://twitter.com/jerryjliu0/status/1681304143930212357?s=20)\n  11. LlamaIndex now integrates with Eduardo Reis\u2019s Llama 2 functions API at [ llama-api.com ](http://llama-api.com) . [ Tweet ](https://twitter.com/llama_index/status/1683231608038641664?s=20)\n  12. LlamaIndex TS now supports integration with OpenAI Whisper. [ Docs ](https://www.npmjs.com/package/llamaindex-whisper) , [ Tweet ](https://twitter.com/yi_ding/status/1683990169815502848?s=20)\n  13. LlamaIndex now seamlessly integrates with [ K\u00f9zudb ](https://twitter.com/kuzudb) , allowing users to directly store extracted knowledge graphs/triples for advanced processing, querying, and visualization. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/KuzuGraphDemo.html) , [ Tweet ](https://twitter.com/kuzudb/status/1685010132277530624?s=20)\n  14. LlamaIndex combines data agents with text-to-image models enhancing user prompts with relevant context from a knowledge base. This integration allows for more advanced multimodal reasoning by merging LLM RAG systems with text-to-image tools. [ Docs ](https://llamahub.ai/l/tools-text_to_image) , [ Tweet ](https://twitter.com/jerryjliu0/status/1686026926442315778?s=20)\n\n#  Benchmarking:\n\n  1. LlamaIndex now supports BEIR, an Information Retrieval benchmark. Users can define custom retrievers within LlamaIndex, apply the vector index, or implement reranking steps, and then easily evaluate their methods using any dataset from BEIR. [ Tweet ](https://twitter.com/llama_index/status/1680569394198372352?s=20)\n  2. LlamaIndex\u2019s Llama2 agents have shown promising performance in our agent task benchmark. Especially notable is their capability to appropriately use tools within a ReACT loop. However, the tasks\u2019 difficulty varies, with both 13B and 70B models notably refraining from dialing a phone number, underlining the AI\u2019s limitations. [ Tweet ](https://twitter.com/llama_index/status/1681724356764872704?s=20)\n  3. LlamaIndex now has integration with the HotpotQA benchmark! This enables rigorous testing of LLM\u2019s multi-hop reasoning capabilities by providing the full context to the models, helping you evaluate their performance more accurately. Perfect for stress-testing LLMs like ChatGPT, Claude 2, PaLM, and more. Plus, explore how context reordering can simplify tasks for your LLMs. ", "mimetype": "text/plain", "start_char_idx": 4649, "end_char_idx": 7208, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d8ef2d0-1688-4a68-8c71-ff04421feb49": {"__data__": {"id_": "3d8ef2d0-1688-4a68-8c71-ff04421feb49", "embedding": null, "metadata": {"filename": "llamaindex-update-08-01-2023-185514d9b897.md", "extension": ".md", "title": "LlamaIndex Update \u2014 08/01/2023", "date": "Aug 1, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-08-01-2023-185514d9b897"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc2828e9-fbd2-4fe5-96df-1001a43afd6c", "node_type": "4", "metadata": {"filename": "llamaindex-update-08-01-2023-185514d9b897.md", "extension": ".md", "title": "LlamaIndex Update \u2014 08/01/2023", "date": "Aug 1, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-08-01-2023-185514d9b897"}, "hash": "1435c943c96b45e2ac54d2279465fe55c6ba707fcc3c80888352e715abf38857", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca64912f-dfa1-4351-b428-8e45b105672f", "node_type": "1", "metadata": {"filename": "llamaindex-update-08-01-2023-185514d9b897.md", "extension": ".md", "title": "LlamaIndex Update \u2014 08/01/2023", "date": "Aug 1, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-08-01-2023-185514d9b897"}, "hash": "4bdeb8cb6cfa1919684fa41dd3b56e2926157956f0bd6f58afe9659cdc4673ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7285fb7d-92db-4838-9e6b-0e1a58a6d3c6", "node_type": "1", "metadata": {}, "hash": "d2606539e99402d6b137fcfcc3a410a8f7c5c7122bba62663e7a28de9a89c750", "class_name": "RelatedNodeInfo"}}, "text": "[ Tweet ](https://twitter.com/jerryjliu0/status/1684589377614413825?s=20)\n  4. LlamaIndex now supports over 20 vector databases, each with unique features and capabilities. To help understand their differences, we have compiled a comprehensive comparison table, guiding the choice of the optimal database for the use case. [ Tweet ](https://twitter.com/llama_index/status/1685326422175535104?s=20)\n\n#  Tutorials:\n\nWe were excited to see so many people making tutorials for LlamaIndex this\nmonth!\n\n  ", "mimetype": "text/plain", "start_char_idx": 7208, "end_char_idx": 7707, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7285fb7d-92db-4838-9e6b-0e1a58a6d3c6": {"__data__": {"id_": "7285fb7d-92db-4838-9e6b-0e1a58a6d3c6", "embedding": null, "metadata": {"filename": "llamaindex-update-08-01-2023-185514d9b897.md", "extension": ".md", "title": "LlamaIndex Update \u2014 08/01/2023", "date": "Aug 1, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-08-01-2023-185514d9b897"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc2828e9-fbd2-4fe5-96df-1001a43afd6c", "node_type": "4", "metadata": {"filename": "llamaindex-update-08-01-2023-185514d9b897.md", "extension": ".md", "title": "LlamaIndex Update \u2014 08/01/2023", "date": "Aug 1, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-08-01-2023-185514d9b897"}, "hash": "1435c943c96b45e2ac54d2279465fe55c6ba707fcc3c80888352e715abf38857", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d8ef2d0-1688-4a68-8c71-ff04421feb49", "node_type": "1", "metadata": {"filename": "llamaindex-update-08-01-2023-185514d9b897.md", "extension": ".md", "title": "LlamaIndex Update \u2014 08/01/2023", "date": "Aug 1, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-08-01-2023-185514d9b897"}, "hash": "a94d9b46058881d9ae9949ba959da45890f0d8a0a2c14c045f0610eaa7e9e414", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f3c29b3-abde-4e89-9a55-b59a6619bec6", "node_type": "1", "metadata": {}, "hash": "5bc8d3d9c0bb005bd94fa34162073a22110089b9753f0c43a5dfb3a53abb9c36", "class_name": "RelatedNodeInfo"}}, "text": "1. [ Adam Hofmann ](https://medium.com/@adam.hofmann) \u2019s blog post on [ Building Better Tools for LLM Agents ](https://medium.com/llamaindex-blog/building-better-tools-for-llm-agents-f8c5a6714f11) . \n  2. [ Weav ](https://weaviate.io/) iate\u2019s [ tutorial ](https://github.com/weaviate/recipes/blob/main/integrations/llama2-demo/notebook.ipynb) on using the Llama2 model with LlamaIndex and Weaviet on external data. \n  3. [ Erika ](https://twitter.com/ecardenas300) \u2019s [ tutorial ](https://twitter.com/ecardenas300/status/1681669892741775361?s=20) on VectorStore Index, List Index, and Tree Index. \n  4. [ James Maslek ](https://www.linkedin.com/in/james-maslek/) \u2019s [ tutorial ](https://openbb.co/blog/breaking-barriers-with-openbb-and-llamaIndex) on Breaking Barriers with OpenBB and LlamaIndex: Simplifying data access to 100+ trusted sources. \n  5. [ Ayush Thakur ](https://wandb.ai/ayush-thakur) \u2019s tutorial on [ Building Advanced Query Engine and Evaluation with LlamaIndex and W&B ](https://wandb.ai/ayush-thakur/llama-index-report/reports/Building-Advanced-Query-Engine-and-Evaluation-with-LlamaIndex-and-W-B--Vmlldzo0OTIzMjMy) . \n  6. [ Trulens ](https://www.trulens.org/) \u2019s [ tutorial ](https://github.com/truera/trulens/blob/main/trulens_eval/examples/frameworks/llama_index/llamaindex-yelp-agent.ipynb) on using LlamaIndex Yelp agent to answer queries using Yelp data, and evaluate it for definitiveness and accuracy using custom feedback functions, compare its performance against a standalone LLM. \n  7. [ Airbyte ](https://twitter.com/AirbyteHQ) \u2019s [ tutorial ](https://airbyte.com/tutorials/airbyte-and-llamaindex-elt-and-chat-with-your-data-warehouse-without-writing-sql) on Chat with your data warehouse without writing SQL. \n  8. [ Anil Chandra Naidu ](https://twitter.com/matchaman11) \u2019s tutorial on [ Retrievers ](https://github.com/SamurAIGPT/LlamaIndex-course/blob/main/retrievers/Retrievers.ipynb) and [ QueryEngines ](https://github.com/SamurAIGPT/LlamaIndex-course/blob/main/query_engines/Query_Engines.ipynb) . \n  9. [ Wenqi Glantz ](https://medium.com/@wenqiglantz) \u2019s tutorial on [ Exploring Snowflake and Streamlit With LlamaIndex Text-to-SQL ](https://betterprogramming.pub/exploring-snowflake-and-streamlit-with-llamaindex-text-to-sql-f66fec6e321b) . \n\nAnd from the LlamaIndex team:\n\n  1. [ Logan ](https://twitter.com/LoganMarkewich) \u2019s [ tutorial ](https://www.youtube.com/watch?v=2c64G-iDJKQ) on a comprehensive understanding of embedding models, their benchmarking, and their implementation in LlamaIndex, with a focus on OpenAI and Instructor embeddings, enabling semantic search through numerical text representations. \n  2. [ Logan ](https://twitter.com/LoganMarkewich) \u2019s [ tutorial ](https://www.youtube.com/watch?v=LQy8iHOJE2A) on the evaluation of query engines using LlamaIndex, learn to handle uncontrolled outputs and runtime costs while measuring performance with GPT-4. \n  3. ", "mimetype": "text/plain", "start_char_idx": 7707, "end_char_idx": 10631, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f3c29b3-abde-4e89-9a55-b59a6619bec6": {"__data__": {"id_": "3f3c29b3-abde-4e89-9a55-b59a6619bec6", "embedding": null, "metadata": {"filename": "llamaindex-update-08-01-2023-185514d9b897.md", "extension": ".md", "title": "LlamaIndex Update \u2014 08/01/2023", "date": "Aug 1, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-08-01-2023-185514d9b897"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc2828e9-fbd2-4fe5-96df-1001a43afd6c", "node_type": "4", "metadata": {"filename": "llamaindex-update-08-01-2023-185514d9b897.md", "extension": ".md", "title": "LlamaIndex Update \u2014 08/01/2023", "date": "Aug 1, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-08-01-2023-185514d9b897"}, "hash": "1435c943c96b45e2ac54d2279465fe55c6ba707fcc3c80888352e715abf38857", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7285fb7d-92db-4838-9e6b-0e1a58a6d3c6", "node_type": "1", "metadata": {"filename": "llamaindex-update-08-01-2023-185514d9b897.md", "extension": ".md", "title": "LlamaIndex Update \u2014 08/01/2023", "date": "Aug 1, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-08-01-2023-185514d9b897"}, "hash": "178b97245c1b20d2fe5adfb743642e4f8843f6616d412b6b37a90e00ac403d03", "class_name": "RelatedNodeInfo"}}, "text": "[ Ravi Theja ](https://twitter.com/ravithejads) \u2019s [ tutorial ](https://www.youtube.com/watch?v=A3iqOJHBQhM) on Key Components to build QA Systems. \n\n#  Webinars:\n\n  1. [ Webinar ](https://www.youtube.com/watch?v=s8ZNLqi9hzc) with Didier Lopes, CEO/Co-Founder at OpenBB on LLMs for Investment Research. \n  2. [ Webinar ](https://llamaindex-and.wandb.events/) on Building & Evaluating an Advanced Query Engine Over Your Data with Weights and Biases. \n  3. [ Webinar ](https://www.youtube.com/watch?v=TdVbH7uJR_Y) with [ Jason ](https://twitter.com/jxnlco) Liu on From Prompt to Schema Engineering with Pydantic. \n\n#  Events:\n\n  1. LlamaIndex and Arize [ workshop ](https://arize.com/resource/llm-search-retrieval-systems-with-arize-and-llamaindex-powering-llms-on-your-proprietary-data/) on LLM Search & Retrieval Systems with Arize and LlamaIndex: Powering LLMs on Your Proprietary Data. \n  2. LlamaIndex and TruLens [ workshop ](https://go.truera.com/event-llm-app-workshop-with-llamaindex-and-trulens?utm_campaign=event-2023-07-27-san-francisco&utm_source=twitter&utm_medium=social) on building an LLM App. \n  3. [ TPF ](https://twitter.com/TheProductfolks) (The Product Folks) [ workshop session ](https://www.youtube.com/watch?v=2ul5XQXp-YI) on Building QA Systems With LlamaIndex by [ Ravi Theja ](https://twitter.com/ravithejads) . \n  4. [ Ravi Theja ](https://twitter.com/ravithejads) [ talk ](https://twitter.com/ravithejads/status/1684768609111801856?s=20) at the [ Speciale VC ](https://twitter.com/specialeinvest?lang=en) GenAI meetup in Chennai on Beyond the Basics: Leveraging LlamaIndex from Concept to Production. \n  5. Data Agents session at TPF X Nexus VC [ Buildathon ](https://twitter.com/TheProductfolks/status/1685167361060737024?s=20) by [ Ravi Theja ](https://twitter.com/ravithejads) . \n\n#  Demos:\n\n  1. [ Tali.AI ](https://twitter.com/TryTaliAI) at the Augment hackathon dove into the future of support roles by developing an Autonomous Support Bot using LlamaIndex. [ Tweet ](https://twitter.com/TryTaliAI/status/1683960220702371845?s=20)\n  2. [ SuperAGI ](https://superagi.com/) integrated with LlamaIndex which enables AI agents to process a wide variety of data from both structured and unstructured sources including Docx, PDF, CSV files, videos, and images. [ Tweet ](https://twitter.com/_superAGI/status/1679058876023603201?s=20)\n\n", "mimetype": "text/plain", "start_char_idx": 10631, "end_char_idx": 12994, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f92f2dc-465c-4a3a-ba2c-cf29d9b4755f": {"__data__": {"id_": "8f92f2dc-465c-4a3a-ba2c-cf29d9b4755f", "embedding": null, "metadata": {"filename": "data-agents-zapier-nla-67146395ce1.md", "extension": ".md", "title": "Data Agents + Zapier NLA", "date": "Jul 25, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-zapier-nla-67146395ce1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0160dfb-5f5f-4596-9b02-10e879ce4929", "node_type": "4", "metadata": {"filename": "data-agents-zapier-nla-67146395ce1.md", "extension": ".md", "title": "Data Agents + Zapier NLA", "date": "Jul 25, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-zapier-nla-67146395ce1"}, "hash": "41e4a974ea1d454fbb7405cb91e87906c8cf3da41b7c5eaaae6a421c7546a792", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d506bfd-465f-44a6-a811-0c8830fe9323", "node_type": "1", "metadata": {}, "hash": "3261f1bcd0723c218c5a20cfdd34a7e8cdfb353846d5f202fac56c715b7b59c8", "class_name": "RelatedNodeInfo"}}, "text": "> Joint blog by LlamaIndex team & Zapier NLA team\n\nWouldn\u2019t it be great to have a personal assistant that can **access your\ndata** and **perform tasks for you** ?\n\nIntroducing LlamaIndex data agents, now more powerful with Zapier NLA.\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 235, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d506bfd-465f-44a6-a811-0c8830fe9323": {"__data__": {"id_": "3d506bfd-465f-44a6-a811-0c8830fe9323", "embedding": null, "metadata": {"filename": "data-agents-zapier-nla-67146395ce1.md", "extension": ".md", "title": "Data Agents + Zapier NLA", "date": "Jul 25, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-zapier-nla-67146395ce1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0160dfb-5f5f-4596-9b02-10e879ce4929", "node_type": "4", "metadata": {"filename": "data-agents-zapier-nla-67146395ce1.md", "extension": ".md", "title": "Data Agents + Zapier NLA", "date": "Jul 25, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-zapier-nla-67146395ce1"}, "hash": "41e4a974ea1d454fbb7405cb91e87906c8cf3da41b7c5eaaae6a421c7546a792", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f92f2dc-465c-4a3a-ba2c-cf29d9b4755f", "node_type": "1", "metadata": {"filename": "data-agents-zapier-nla-67146395ce1.md", "extension": ".md", "title": "Data Agents + Zapier NLA", "date": "Jul 25, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-zapier-nla-67146395ce1"}, "hash": "85ac946e76d5c998b1eb52d91cc2ae26d536d9599c46d521fd52ad47f735d0a8", "class_name": "RelatedNodeInfo"}}, "text": "**Within 5 lines of code** , you can access the 5,000+ third party apps and\nover 30,000 actions on Zapier.\n\n    \n    \n    from llama_hub.tools.zapier.base import ZapierToolSpec\n    from llama_index.agent import OpenAIAgent\n    \n    zapier_spec = ZapierToolSpec(api_key=\"sk-ak-your-key\")\n    agent = OpenAIAgent.from_tools(zapier_spec.to_tool_list(), verbose=True)\n    \n    agent.chat('Can you summarize the unread emails and send it to me on Slack?')\n\n", "mimetype": "text/plain", "start_char_idx": 235, "end_char_idx": 687, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ddcd0fb-65db-41c3-8bf8-766a5beb6f9f": {"__data__": {"id_": "1ddcd0fb-65db-41c3-8bf8-766a5beb6f9f", "embedding": null, "metadata": {"filename": "introducing-llamaindex-ts-89f41a1f24ab.md", "extension": ".md", "title": "Introducing LlamaIndex.TS", "date": "Jul 24, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llamaindex-ts-89f41a1f24ab"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "301b8db7-e100-4df2-966e-691d2ebcffb3", "node_type": "4", "metadata": {"filename": "introducing-llamaindex-ts-89f41a1f24ab.md", "extension": ".md", "title": "Introducing LlamaIndex.TS", "date": "Jul 24, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llamaindex-ts-89f41a1f24ab"}, "hash": "034d71e1cde166a3b50c3fb5c2a265bd08d767a875d2f9497b86e99e9b29eaf4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "746aa0c5-e71f-4898-ab77-7726792b00f2", "node_type": "1", "metadata": {}, "hash": "43a6035f2f66c2ea029a8f7e36e0945822bb4412697aecc652e9fb9c61e6e297", "class_name": "RelatedNodeInfo"}}, "text": "We are beyond excited to announce v0.0.1 of [ **LlamaIndex.TS**\n](https://github.com/run-llama/LlamaIndexTS/) , a Typescript first library\nfocused on helping you use your private data with large language models.\n\n##  **What is LlamaIndex?**\n\nOur core goal for LlamaIndex is to help developers easily integrate their data\nwith Large Language Models (LLMs). LLMs, like ChatGPT, have been a revolution\nin the way we think about handling textual input and data, but all of them\nhave the limitation in what data they have access to. In addition to the\n\u201cknowledge cutoff\u201d (we are nearing the 2 year anniversary for when ChatGPT\u2019s\nlatest data was trained) LLMs can\u2019t access data from your companies, from your\npersonal analyses, or from the data your users generate.\n\nWith LlamaIndex.TS, we look to achieve that goal by meeting developers at\ntheir (my) language of choice, in this case Typescript. We are committed to\nmaking this library the easiest to use, most robust solution out there for\nusing data with LLMs.\n\n##  **Backstory**\n\nIt was at the Emergency ChatGPT Hackathon hosted by Pete Huang and Rachel\nWoods that I met Jerry. Having worked in the JS world for the last 8 years, my\nfirst question was \u201cwhy don\u2019t you build this in Javascript?\u201d After he\ndemurred, he very patiently guided me through setting up the Python dev\nenvironment. (I think it took us 20 minutes before we figured it all out!) So,\nwhen Jerry offered to let me build LlamaIndex.TS I obviously couldn\u2019t turn him\ndown. Can\u2019t wait to see what you build with it!\n\n##  **Design**\n\nAt a high level, LlamaIndex.TS first takes the file inputs, loads them into a\nstandardized format, and creates an Index (knowledge base).\n\nWe then retrieve the relevant information from the index and use that in our\nquery to the LLM to generate more a grounded response.\n\nCheck out [ our docs ](https://ts.llamaindex.ai/) for a more in depth\nexplanation!\n\n##  Playground\n\nWe are building an open source playground for LlamaIndex.TS. Please check it\nout at [ https://llama-playground.vercel.app/ ](https://llama-\nplayground.vercel.app/) PRs are welcome here! [ https://github.com/run-\nllama/ts-playground ](https://github.com/run-llama/ts-playground)\n\n##  **Main Differences from LlamaIndex Python**\n\n  * All function names are camel cased. \n  * The prompt interface is much simpler and uses native javascript template literals. \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2377, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "746aa0c5-e71f-4898-ab77-7726792b00f2": {"__data__": {"id_": "746aa0c5-e71f-4898-ab77-7726792b00f2", "embedding": null, "metadata": {"filename": "introducing-llamaindex-ts-89f41a1f24ab.md", "extension": ".md", "title": "Introducing LlamaIndex.TS", "date": "Jul 24, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llamaindex-ts-89f41a1f24ab"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "301b8db7-e100-4df2-966e-691d2ebcffb3", "node_type": "4", "metadata": {"filename": "introducing-llamaindex-ts-89f41a1f24ab.md", "extension": ".md", "title": "Introducing LlamaIndex.TS", "date": "Jul 24, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llamaindex-ts-89f41a1f24ab"}, "hash": "034d71e1cde166a3b50c3fb5c2a265bd08d767a875d2f9497b86e99e9b29eaf4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ddcd0fb-65db-41c3-8bf8-766a5beb6f9f", "node_type": "1", "metadata": {"filename": "introducing-llamaindex-ts-89f41a1f24ab.md", "extension": ".md", "title": "Introducing LlamaIndex.TS", "date": "Jul 24, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llamaindex-ts-89f41a1f24ab"}, "hash": "b4317e6d9bdba0ac57657a331f6d6355912ae04572c3b1586952602440180b7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c832198-4c34-4caa-a5a8-cfb19960a642", "node_type": "1", "metadata": {}, "hash": "73a0337172f143111d0aa7ded0a403ba4ce51c98cf8cd9c4182f8ffb291d83c0", "class_name": "RelatedNodeInfo"}}, "text": "* We do not ship non-async versions of functions. Please use await or .then callbacks. \n  * We use interfaces and POJOs in lieu of classes where it makes sense. For example, ChatEngine, a base class in Python is an interface in JS. ServiceContext, a class in Python is an interface/POJO in JS. \n\n##  **Runtimes**\n\nCurrently, we support NodeJS v18 and up. ", "mimetype": "text/plain", "start_char_idx": 2377, "end_char_idx": 2732, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c832198-4c34-4caa-a5a8-cfb19960a642": {"__data__": {"id_": "5c832198-4c34-4caa-a5a8-cfb19960a642", "embedding": null, "metadata": {"filename": "introducing-llamaindex-ts-89f41a1f24ab.md", "extension": ".md", "title": "Introducing LlamaIndex.TS", "date": "Jul 24, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llamaindex-ts-89f41a1f24ab"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "301b8db7-e100-4df2-966e-691d2ebcffb3", "node_type": "4", "metadata": {"filename": "introducing-llamaindex-ts-89f41a1f24ab.md", "extension": ".md", "title": "Introducing LlamaIndex.TS", "date": "Jul 24, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llamaindex-ts-89f41a1f24ab"}, "hash": "034d71e1cde166a3b50c3fb5c2a265bd08d767a875d2f9497b86e99e9b29eaf4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "746aa0c5-e71f-4898-ab77-7726792b00f2", "node_type": "1", "metadata": {"filename": "introducing-llamaindex-ts-89f41a1f24ab.md", "extension": ".md", "title": "Introducing LlamaIndex.TS", "date": "Jul 24, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llamaindex-ts-89f41a1f24ab"}, "hash": "27a2a2d73e5b63e458b680274914475d63065792d1a238983d0358ef29269316", "class_name": "RelatedNodeInfo"}}, "text": "Lots of plans on this front though.\nStay tuned!\n\n##  **Contributing**\n\nOnly the core features are built out so far, so there is a lot of work that\nneeds to be done on the loader and integration side. If you\u2019re interested in\ncontributing, please send us a message or even better a PR!\n\n[ https://github.com/run-llama/LlamaIndexTS ](https://github.com/run-\nllama/LlamaIndexTS)\n\n", "mimetype": "text/plain", "start_char_idx": 2732, "end_char_idx": 3108, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b54db65-c237-46e0-9812-87b37dc6bfa8": {"__data__": {"id_": "7b54db65-c237-46e0-9812-87b37dc6bfa8", "embedding": null, "metadata": {"filename": "building-better-tools-for-llm-agents-f8c5a6714f11.md", "extension": ".md", "title": "Building Better Tools for LLM Agents", "date": "Jul 17, 2023", "url": "https://www.llamaindex.ai/blog/building-better-tools-for-llm-agents-f8c5a6714f11"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8078c614-37b9-4aa0-8f9d-96fdb8e4c9a8", "node_type": "4", "metadata": {"filename": "building-better-tools-for-llm-agents-f8c5a6714f11.md", "extension": ".md", "title": "Building Better Tools for LLM Agents", "date": "Jul 17, 2023", "url": "https://www.llamaindex.ai/blog/building-better-tools-for-llm-agents-f8c5a6714f11"}, "hash": "4bba3d2eaf26665ed83c98bd1b80247119a34975f2e360140d22bf6c706ba553", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7dfc7af-5a42-4b39-b54c-8a0370e39ab1", "node_type": "1", "metadata": {}, "hash": "827e44a1225cb9d0b6e0bb8d1def0b2a286ff8fbf85fb5e3e54a99d909b9f2dd", "class_name": "RelatedNodeInfo"}}, "text": "Over the past month I\u2019ve been diving into the world of Large Language Model\n(LLM) Agents and building out LlamaIndex\u2019s library of tools for use with\nagents. I helped to lead the LlamaHub Tools effort as part of broader [ Data\nAgents launch ](https://medium.com/llamaindex-blog/data-agents-eed797d7972f)\nlast week.\n\nIn the process of building out LlamaHub Tools I\u2019ve collected some techniques\nfor creating effective and easy to use tools, and want to share some of my\nthoughts.\n\n#  Context on LlamaHub Tools\n\n[ LlamaHub Tools ](https://llamahub.ai/) allow LLMs like ChatGPT to connect to\nAPIs and act on a user\u2019s behalf to create, read, update and delete data.\nExamples of tools that we\u2019ve put together include [ drafting and sending\nemails ](https://llamahub.ai/l/tools-gmail) , [ reading and creating Google\nCalendar invite ](https://llamahub.ai/l/tools-google_calendar) s, [ searching\nWikipedia ](https://llamahub.ai/l/tools-wikipedia) , and that\u2019s just a few of\nthe 15 tools we are releasing on launch.\n\n##  Overview of tool abstractions\n\nSo how exactly do LlamaHub Tools work? The LlamaHub tool abstractions allow\nyou to easily write Python functions that can be understood and called by\nAgents. Instead of trying to make an Agent do complicated mathematics for\nexample, we can provide the Agent with a Tool that calls Wolfram Alpha and\nprovides the result to the Agent:\n\n    \n    \n    from llama_index.tools.base import BaseToolSpec\n    \n    QUERY_URL_TMPL = \"http://api.wolframalpha.com/v1/result?appid={app_id}&amp;i={query}\"\n    \n    # Inherit from the LlamaIndex BaseToolSpec abstraction\n    class WolframAlphaToolSpec(BaseToolSpec):\n    \n      # Define the functions that we export to the LLM\n        spec_functions = [\"wolfram_alpha_query\"]\n    \n      # Initialize with our wolfram alpha API key\n        def __init__(self, app_id: Optional[str] = None) -&gt; None:\n            \"\"\"Initialize with parameters.\"\"\"\n            self.token = app_id\n      \n      # Our function to be called by the Agent\n      def wolfram_alpha_query(self, query: str):\n              \"\"\"\n              Make a query to wolfram alpha about a mathematical or scientific problem.\n      \n              Example inputs:\n                  \"(7 * 12 ^ 10) / 321\"\n                  \"How many calories are there in a pound of strawberries\"\n      \n              Args:\n                  query (str): The query to be passed to wolfram alpha.\n      \n              \"\"\"\n              response = requests.get(QUERY_URL_TMPL.format(app_id=self.token, query=urllib.parse.quote_plus(query)))\n              return response.text\n\nThe above code is enough to define a LlamaIndex Tool that allows the Agent to\nquery to Wolfram Alpha. No more incorrect guesses at math problems! We can\ninitialize an instance of the Tool Spec like this:\n\n    \n    \n    # Initialize an instance of the Tool\n    wolfram_spec = WolframAlphaToolSpec(app_id=\"your-key\")\n    # Convert the Tool Spec to a list of tools. In this case we just have one tool.\n    tools = wolfram_spec.to_tool_list()\n    # Convert the tool to an OpenAI function and inspect\n    print(tools[0].metadata.to_openai_function())\n\nHere\u2019s the cleaned up output of the print statement:\n\n    \n    \n    {\n      'description': '\n        Make a query to wolfram alpha about a mathematical or scientific problem.\n      \n              Example inputs:\n                  \"(7 * 12 ^ 10) / 321\"\n                  \"How many calories are there in a pound of strawberries\"\n      \n              Args:\n                  query (str): The query to be passed to wolfram alpha.',\n      'name': 'wolfram_alpha_query',\n      'parameters': {\n        'properties': {'query': {'title': 'Query', 'type': 'string'}},\n        'title': 'wolfram_alpha_query',\n        'type': 'object'\n      }\n    }\n\nWe can see that the [ docstring ](https://en.wikipedia.org/wiki/Docstring)\ndescribing how to use the Tool get passed to the Agent. Additionally, the\nparameters, type info and function name are passed along to give the Agent a\nstrong idea on how it can use this function. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4049, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7dfc7af-5a42-4b39-b54c-8a0370e39ab1": {"__data__": {"id_": "e7dfc7af-5a42-4b39-b54c-8a0370e39ab1", "embedding": null, "metadata": {"filename": "building-better-tools-for-llm-agents-f8c5a6714f11.md", "extension": ".md", "title": "Building Better Tools for LLM Agents", "date": "Jul 17, 2023", "url": "https://www.llamaindex.ai/blog/building-better-tools-for-llm-agents-f8c5a6714f11"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8078c614-37b9-4aa0-8f9d-96fdb8e4c9a8", "node_type": "4", "metadata": {"filename": "building-better-tools-for-llm-agents-f8c5a6714f11.md", "extension": ".md", "title": "Building Better Tools for LLM Agents", "date": "Jul 17, 2023", "url": "https://www.llamaindex.ai/blog/building-better-tools-for-llm-agents-f8c5a6714f11"}, "hash": "4bba3d2eaf26665ed83c98bd1b80247119a34975f2e360140d22bf6c706ba553", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b54db65-c237-46e0-9812-87b37dc6bfa8", "node_type": "1", "metadata": {"filename": "building-better-tools-for-llm-agents-f8c5a6714f11.md", "extension": ".md", "title": "Building Better Tools for LLM Agents", "date": "Jul 17, 2023", "url": "https://www.llamaindex.ai/blog/building-better-tools-for-llm-agents-f8c5a6714f11"}, "hash": "e4afc61654d27d69a7e60de25c2957b2ba854e2cdabdcf10223b5ed5ce808066", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c9c07e8-dd35-4f66-af42-fca1e791030f", "node_type": "1", "metadata": {}, "hash": "cab651323437e6b53b3540ce671542ad22dd697155aa4c15758ae3bd1355d8c4", "class_name": "RelatedNodeInfo"}}, "text": "All of this information is\nessentially acting as the prompt for how the agent understands the tool.\n\nInheriting from the BaseToolSpec class means it\u2019s very simple to write Tools\nfor Agents to use. In fact, the above tool definition is only 9 lines of code,\nignoring white space, imports and comments. We can easily get the function\nready for Agents to use without any heavy boilerplate or modifications. Let\u2019s\nlook at loading the Tool into an OpenAI Agent:\n\n    \n    \n    agent = OpenAIAgent.from_tools(tools, verbose=True)\n    agent.chat('What is (7 * 12 ^ 10) / 321')\n    \"\"\" OUTPUT:\n    === Calling Function ===\n    Calling function: wolfram_alpha_query with args: {\n      \"query\": \"(7 * 12 ^ 10) / 14\"\n    }\n    Got output: 30958682112\n    ========================\n    Response(response='The result of the expression (7 * 12 ^ 10) / 14 is 30,958,682,112.', source_nodes=[], metadata=None)\n    \"\"\"\n\nAnd we can test out passing this query to ChatGPT without the tools:\n\n    \n    \n    &gt; 'What is (7 * 12 ^ 10) / 321'\n    \"\"\"\n    To calculate the expression (7 * 12^10) / 14, you need to follow the order of operations, which is parentheses, exponents, multiplication, and division (from left to right).\n    \n    Step 1: Calculate the exponent 12^10.\n    12^10 = 619,173,642,24.\n    \n    Step 2: Multiply 7 by the result from Step 1.\n    7 * 619,173,642,24 = 4,333,215,496,68.\n    \n    Step 3: Divide the result from Step 2 by 14.\n    4,333,215,496,68 / 14 = 309,515,392,62.\n    \n    Therefore, the result of the expression (7 * 12^10) / 14 is 309,515,392,62.\n    \"\"\"\n\nThis example should show how easily you can write new Tools for use with\nAgents. ", "mimetype": "text/plain", "start_char_idx": 4049, "end_char_idx": 5702, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c9c07e8-dd35-4f66-af42-fca1e791030f": {"__data__": {"id_": "5c9c07e8-dd35-4f66-af42-fca1e791030f", "embedding": null, "metadata": {"filename": "building-better-tools-for-llm-agents-f8c5a6714f11.md", "extension": ".md", "title": "Building Better Tools for LLM Agents", "date": "Jul 17, 2023", "url": "https://www.llamaindex.ai/blog/building-better-tools-for-llm-agents-f8c5a6714f11"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8078c614-37b9-4aa0-8f9d-96fdb8e4c9a8", "node_type": "4", "metadata": {"filename": "building-better-tools-for-llm-agents-f8c5a6714f11.md", "extension": ".md", "title": "Building Better Tools for LLM Agents", "date": "Jul 17, 2023", "url": "https://www.llamaindex.ai/blog/building-better-tools-for-llm-agents-f8c5a6714f11"}, "hash": "4bba3d2eaf26665ed83c98bd1b80247119a34975f2e360140d22bf6c706ba553", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7dfc7af-5a42-4b39-b54c-8a0370e39ab1", "node_type": "1", "metadata": {"filename": "building-better-tools-for-llm-agents-f8c5a6714f11.md", "extension": ".md", "title": "Building Better Tools for LLM Agents", "date": "Jul 17, 2023", "url": "https://www.llamaindex.ai/blog/building-better-tools-for-llm-agents-f8c5a6714f11"}, "hash": "2c417f81879d5c3ba094271746725db6bd0e13e38801591bb516d626a78d5b97", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff9c2fa0-8fbb-4a96-a3da-dd92dded1103", "node_type": "1", "metadata": {}, "hash": "09da73579b6aa328a5989aac87b66b20a3dd78448292de374a7ecddf6f3fb6bf", "class_name": "RelatedNodeInfo"}}, "text": "For the rest of the blog post I\u2019ll be talking about tips and tricks\nI\u2019ve found to write more functional and effective tools. Hopefully by the end\nof the blog post you are excited to write and contribute some Tools of your\nown!\n\n#  Techniques for building better tools\n\nBelow are a variety of tactics for writing more usable and functional tools to\nminimize friction when interfacing with the Agent. Not all of the tactics\napply to every tool, but usually at least a few of the techniques below will\nprove valuable.\n\n##  Writing useful tool prompts\n\nHere\u2019s an example of the function signature and docstring for a tool that an\nAgent can call to create a draft email.\n\n    \n    \n    def create_draft(\n            self,\n            to: List[str],\n            subject: str,\n            message: str\n        ) -&gt; str:\n            \"\"\"Create and insert a draft email.\n               Print the returned draft's message and id.\n               Returns: Draft object, including draft id and message meta data.\n    \n            Args:\n                to (List[str]): The email addresses to send the message to, eg ['adam@example.com']\n                subject (str): The subject for the event\n                message (str): The message for the event\n            \"\"\"\n\nThis prompt takes advantage of a few different patterns to ensure that the\nagent can use the tool effectively:\n\n  * Give a concise description of the function and its purpose \n  * Inform the Agent on what data will be returned from this function \n  * List the arguments that the function accepts, with descriptions and type information \n  * Give example values for arguments with a specific format, eg adam@example.com \n\nTool prompts should be concise as to not take up too much length in context,\nbut also informative enough that the agent can use the tool without making\nmistakes.\n\n##  Making tools tolerant of partial inputs\n\nOne way to help Agents make fewer mistakes is to write tools that are more\n_tolerant_ of their inputs, for example by making inputs optional when the\nvalue can be inferred from somewhere else. Take the example of drafting an\nemail, but this time let\u2019s consider a tool that updates a draft email:\n\n    \n    \n    def update_draft(\n            self,\n            draft_id: str,\n            to: Optional[List[str]] = None,\n            subject: Optional[str] = None,\n            message: Optional[str] = None,\n        ) -&gt; str:\n            \"\"\"Update a draft email.\n               Print the returned draft's message and id.\n               This function is required to be passed a draft_id that is obtained when creating messages\n               Returns: Draft object, including draft id and message meta data.\n    \n            Args:\n                draft_id (str): the id of the draft to be updated\n                to (Optional[str]): The email addresses to send the message to\n                subject (Optional[str]): The subject for the event\n                message (Optional[str]): The message for the event\n            \"\"\"\n\nThe Gmail API **requires** all of the above values when updating a draft,\nhowever using just the ` draft_id ` we can fetch the current content of the\ndraft and use the existing values as defaults if the Agent did not provide the\nvalues when updating the draft:\n\n    \n    \n    def update_draft(...):\n      ...\n      draft = self.get_draft(draft_id)\n      headers = draft['message']['payload']['headers']\n      for header in headers:\n          if header['name'] == 'To' and not to:\n              to = header['value']\n          elif header['name'] == 'Subject' and not subject:\n              subject = header['value']\n        elif header['name'] == 'Message' and not message:\n          message = header['values']\n      ...\n\nBy providing the above logic in the ` update_draft ` function, the Agent can\ninvoke ` update_draft ` with only one of the fields (and the ` draft_id ` ),\nand we can update the draft as the user expects. This means that in more\ncircumstances the Agent can complete the task successfully, instead of\nreturning an error or needing to ask for more information.\n\n", "mimetype": "text/plain", "start_char_idx": 5702, "end_char_idx": 9790, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff9c2fa0-8fbb-4a96-a3da-dd92dded1103": {"__data__": {"id_": "ff9c2fa0-8fbb-4a96-a3da-dd92dded1103", "embedding": null, "metadata": {"filename": "building-better-tools-for-llm-agents-f8c5a6714f11.md", "extension": ".md", "title": "Building Better Tools for LLM Agents", "date": "Jul 17, 2023", "url": "https://www.llamaindex.ai/blog/building-better-tools-for-llm-agents-f8c5a6714f11"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8078c614-37b9-4aa0-8f9d-96fdb8e4c9a8", "node_type": "4", "metadata": {"filename": "building-better-tools-for-llm-agents-f8c5a6714f11.md", "extension": ".md", "title": "Building Better Tools for LLM Agents", "date": "Jul 17, 2023", "url": "https://www.llamaindex.ai/blog/building-better-tools-for-llm-agents-f8c5a6714f11"}, "hash": "4bba3d2eaf26665ed83c98bd1b80247119a34975f2e360140d22bf6c706ba553", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c9c07e8-dd35-4f66-af42-fca1e791030f", "node_type": "1", "metadata": {"filename": "building-better-tools-for-llm-agents-f8c5a6714f11.md", "extension": ".md", "title": "Building Better Tools for LLM Agents", "date": "Jul 17, 2023", "url": "https://www.llamaindex.ai/blog/building-better-tools-for-llm-agents-f8c5a6714f11"}, "hash": "78be71c3e9ef60df4adf77a878726699c1ee78faef9c42c37f4fe201849f639a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8cc38866-a386-4f5c-8adf-0de3ac9be560", "node_type": "1", "metadata": {}, "hash": "1dd958c31080be16473a2547b71e92d51dbb351896403fa255157e3601a08eb2", "class_name": "RelatedNodeInfo"}}, "text": "##  Validating input and Agent error handling\n\nDespite best efforts at prompting and tolerance, we can end up in\ncircumstances where the Agent invokes a tool in a way that it can\u2019t complete\nthe task at hand. However, we can detect this and prompt the Agent to recover\nthe error on its own.\n\nFor example, in the ` update_draft ` example above, what do we do if the agent\ncalls the function without a ` draft_id ` ? We could simply pass along the\nnull value and return an error from the Gmail API library, but we could also\ndetect that a null ` draft_id ` will invariably cause an error, and return a\nprompt for the agent instead:\n\n    \n    \n    def update_draft(...):\n      if draft_id == None:\n        return \"You did not provide a draft id when calling this function. If you previously created or retrieved the draft, the id is available in context\"\n\nNow, if the Agent invokes ` update_draft ` without a ` draft_id ` , it is made\naware of the exact mistake it made and given instructions on how it can\ncorrect the issue.\n\nIn my experience working with this tool, the Agent will often immediately call\nthe ` update_draft ` function in the correct way when receiving this prompt,\nor if there is no ` draft_id ` available, it will inform the user of the issue\nand ask the user for a ` draft_id ` . Either scenario is much better than\ncrashing or returning an opaque error from a library to the user.\n\n##  Providing simple functions related to the tool\n\nAgents can struggle at what would otherwise be simple functions for a computer\nto calculate. For example, when building a tool for creating events in Google\nCalendar, a user may prompt the Agent with something like this:\n\n> _Create an event on my Calendar to discuss the Tools PR with_ [\n> _adam@example.com_ ](mailto:adam@example.com) _tomorrow at 4pm_\n\nCan you see the problem? If we try asking ChatGPT what day it is:\n\n    \n    \n    agent.chat('what day is it?')\n    # > I apologize for the confusion. As an AI language model, I don't have real-time data or access to the current date. My responses are based on the information I was last trained on, which is up until September 2021. To find out the current day, I recommend checking your device's clock, referring to a calendar, or checking an online source for the current date.\n\nAgents won\u2019t know what the current date is, and so the Agent would either call\nthe function incorrectly, providing a string like ` tomorrow ` for the date,\nhallucinate a date sometime in the past based on when it was trained, or put\nthe burden on the user to tell it the date. All of the above actions cause\nfriction and frustration for the user.\n\nInstead, in the Google Calendar Tool Spec we provide a simple deterministic\nfunction for the agent to call if it needs to fetch the date:\n\n    \n    \n    def get_date(self):\n            \"\"\"\n            A function to return todays date.\n            Call this before any other functions if you are unaware of the current date\n            \"\"\"\n            return datetime.date.today()\n\nNow, when the Agent tries to handle the prompt above, it can first call the\nfunction to get the date and then create the event as the user requested,\ninferring the date for \u201ctomorrow\u201d or \u201ca week from now\u201d. No errors, no guesses\nand no need for further user interaction!\n\n##  Returning prompts from functions that perform mutations\n\nSome functions perform mutations to data in a way that it isn\u2019t clear what\nuseful data can be returned from the function, back to the agent. For example,\nin the Google Calendar tool if an event is successfully created it doesn\u2019t\nmake sense to return the content of the event back to the Agent, as the agent\njust passed in all of the information and thus has it in context.\n\nGenerally with functions that are focused on mutations (create, update,\ndelete) we can help the Agent understand its actions better by using the\nreturn value of these functions to further prompt the agent. For example, from\nthe Google Calendar ` create_event ` tool we could do the following:\n\n    \n    \n    def create_event(...):\n      ...\n      return 'Event created succesfully! You can move onto the next step.'  \n\nThis helps the agent register that the action succeeded and encourages it to\ncomplete the action it was prompted for, especially if creating the google\ncalendar event is only a single step in a multiple step instruction. We can\nstill return ids as part of these prompts as well:\n\n    \n    \n    def create_event(...):\n      ...\n      event = service.events().insert(...).execute()\n      return 'Event created with id {event.id}! You can move onto the next step.'\n\n", "mimetype": "text/plain", "start_char_idx": 9790, "end_char_idx": 14394, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8cc38866-a386-4f5c-8adf-0de3ac9be560": {"__data__": {"id_": "8cc38866-a386-4f5c-8adf-0de3ac9be560", "embedding": null, "metadata": {"filename": "building-better-tools-for-llm-agents-f8c5a6714f11.md", "extension": ".md", "title": "Building Better Tools for LLM Agents", "date": "Jul 17, 2023", "url": "https://www.llamaindex.ai/blog/building-better-tools-for-llm-agents-f8c5a6714f11"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8078c614-37b9-4aa0-8f9d-96fdb8e4c9a8", "node_type": "4", "metadata": {"filename": "building-better-tools-for-llm-agents-f8c5a6714f11.md", "extension": ".md", "title": "Building Better Tools for LLM Agents", "date": "Jul 17, 2023", "url": "https://www.llamaindex.ai/blog/building-better-tools-for-llm-agents-f8c5a6714f11"}, "hash": "4bba3d2eaf26665ed83c98bd1b80247119a34975f2e360140d22bf6c706ba553", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff9c2fa0-8fbb-4a96-a3da-dd92dded1103", "node_type": "1", "metadata": {"filename": "building-better-tools-for-llm-agents-f8c5a6714f11.md", "extension": ".md", "title": "Building Better Tools for LLM Agents", "date": "Jul 17, 2023", "url": "https://www.llamaindex.ai/blog/building-better-tools-for-llm-agents-f8c5a6714f11"}, "hash": "62cae2e5cb60addeaf7c3f7caceeb7d75ba495e4b9fb4cec5047511f6356d654", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b1943b98-0bb0-4114-aa96-fbe54df31b7e", "node_type": "1", "metadata": {}, "hash": "50293e6fa3bebaf0682f8eb61ff6b347316c854f384b37b2e77cf987e80ef987", "class_name": "RelatedNodeInfo"}}, "text": "##  Storing large responses in indices for the Agent to read\n\nOne consideration when building tools that has been mentioned already is the\nsize of the context window the Agent has. Currently, LLMs tend to have context\nwindows from 4k-16k tokens, however it can certainly be larger or smaller. If\nthe size of the data that a tool would return is larger than the context\nwindow, the Agent will be unable to process the data and error out.\n\nOne consideration when building tools that has been mentioned already is the\nsize of the context window the Agent has. Currently, LLMs tend to have context\nwindows from 4k-16k tokens, however it can certainly be larger or smaller. If\nthe size of the data that a tool would return is larger than the context\nwindow, the Agent will be unable to process the data and error out.\n\nThe only consideration that needs to be made when creating tools that might\nneed to be wrapped by the LoadAndSearchTool, is they need to return a list of\nLlamaIndex documents. For a tool that returns a string, the only modification\nyou need to make to have it be compatible with the LoadAndSearchTool is\nwrapping it in a document and an array:\n\n    \n    \n    from llama_index.readers.schema.base import Document\n    \n    # Not compatible\n    def large_text_response_function():\n      ...\n      return result\n    \n    # LoadAndSearch compatible\n    def large_text_response_function():\n      ...\n      return [Document(text=result)]\n\n##  Verify how the Agent understands the tool\n\nA useful technique for debugging tools in development is to **ask the Agent\nabout its own tools** : the tools it has available, what arguments the tools\naccept, what those arguments represent, and what the tool is used for. The\nresponses of the Agent are useful in determining where your prompts might be\nlacking or helping pinpoint why an Agent is failing to successfully use a tool\nthat you are developing.\n\nAn example conversation debugging the Google Calendar Tool Spec, assuming you\nhave setup an Agent similar to the example notebook:\n\n    \n    \n    agent.chat('what tools do you have available')\n    # I have the following tools available\n    #   1. `load_data`: This tool allows me to load data from your calendar. It can retrieve a specified number of events starting from a given date.\n    #   2. `create_event`: This tool allows me to create an event on your calendar. You can provide details such as the title, description, location, start datetime, end datetime, and attendees.\n    #   3. `get_date`: This tool returns today's date. It can be used to ensure that the correct date is used when working with other tools.\n    ", "mimetype": "text/plain", "start_char_idx": 14394, "end_char_idx": 17023, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1943b98-0bb0-4114-aa96-fbe54df31b7e": {"__data__": {"id_": "b1943b98-0bb0-4114-aa96-fbe54df31b7e", "embedding": null, "metadata": {"filename": "building-better-tools-for-llm-agents-f8c5a6714f11.md", "extension": ".md", "title": "Building Better Tools for LLM Agents", "date": "Jul 17, 2023", "url": "https://www.llamaindex.ai/blog/building-better-tools-for-llm-agents-f8c5a6714f11"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8078c614-37b9-4aa0-8f9d-96fdb8e4c9a8", "node_type": "4", "metadata": {"filename": "building-better-tools-for-llm-agents-f8c5a6714f11.md", "extension": ".md", "title": "Building Better Tools for LLM Agents", "date": "Jul 17, 2023", "url": "https://www.llamaindex.ai/blog/building-better-tools-for-llm-agents-f8c5a6714f11"}, "hash": "4bba3d2eaf26665ed83c98bd1b80247119a34975f2e360140d22bf6c706ba553", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8cc38866-a386-4f5c-8adf-0de3ac9be560", "node_type": "1", "metadata": {"filename": "building-better-tools-for-llm-agents-f8c5a6714f11.md", "extension": ".md", "title": "Building Better Tools for LLM Agents", "date": "Jul 17, 2023", "url": "https://www.llamaindex.ai/blog/building-better-tools-for-llm-agents-f8c5a6714f11"}, "hash": "168708f8c7ccd31f9040719237fa4d0eeb31b54270f783497a0d5f7e8e3b9fd0", "class_name": "RelatedNodeInfo"}}, "text": "# Please let me know if you would like more information about any of these tools or if you have any specific requests.\n    agent.chat('what arguments does the load_data tool take')\n    # 'The `load_data` tool takes the following arguments:\n    #   1. `number_of_results` (optional, default: 100): Specifies the number of events to return from your calendar.\n    #   2. `start_date` (optional): Specifies the start date from which to retrieve events. It can be provided as a string in ISO format (e.g., \"2023-07-05\") or as a `datetime.date` object.\n    # You can use these arguments to customize the number of events and the start date for retrieving data from your calendar.\n\n#  Concluding thoughts\n\nBuilding tools for Agents requires you to think critically about how users\nwill interact with the Agent, and to try and anticipate any possible usage\npatterns. Building well thought out tools that can be resilient to errors and\nissues requires careful consideration and testing of possible use cases. I\nhope that these reflections and techniques assist you in creating new tools\nfor Agents, and don\u2019t forget to share your tools on [ LlamaHub\n](https://llama-hub-ui.vercel.app/) .\n\n", "mimetype": "text/plain", "start_char_idx": 17023, "end_char_idx": 18204, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23c95b9b-6e4e-4077-8c06-05543351b4d8": {"__data__": {"id_": "23c95b9b-6e4e-4077-8c06-05543351b4d8", "embedding": null, "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bf175024-92ea-4126-a6fb-9f1bb834913c", "node_type": "4", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "1beba26d53a7fbe5d5daca8c63bb5f086f199996b63573ec2ef86e96a8574283", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "524770c4-6f60-48d2-ae21-e92a87622539", "node_type": "1", "metadata": {}, "hash": "de7fa74b117ddaf6b91428a258624e2d5782ee30460a328a09856ad726315434", "class_name": "RelatedNodeInfo"}}, "text": "Today we\u2019re incredibly excited to announce the launch of a big new capability\nwithin LlamaIndex: **Data Agents** .\n\nData Agents are LLM-powered knowledge workers that can intelligently perform\nvarious tasks over your data, in both a \u201cread\u201d and \u201cwrite\u201d function. They are\ncapable of the following:\n\n  * Perform automated search and retrieval over different types of data \u2014 unstructured, semi-structured, and structured. \n  ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 422, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "524770c4-6f60-48d2-ae21-e92a87622539": {"__data__": {"id_": "524770c4-6f60-48d2-ae21-e92a87622539", "embedding": null, "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bf175024-92ea-4126-a6fb-9f1bb834913c", "node_type": "4", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "1beba26d53a7fbe5d5daca8c63bb5f086f199996b63573ec2ef86e96a8574283", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23c95b9b-6e4e-4077-8c06-05543351b4d8", "node_type": "1", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "abaa23731c354bba4a54d37b9867da8c106ae8165dc8a9bd783f204ee2b8ef50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da9ce0f0-722d-4454-a6ab-c47cbce1b94d", "node_type": "1", "metadata": {}, "hash": "41a8499603a55324c5b80e39b02972679d5f427acd2122fd59587338251e6ea1", "class_name": "RelatedNodeInfo"}}, "text": "* Calling any external service API in a structured fashion. They can either process the response immediately, or index/cache this data for future use. \n  * Storing conversation history. \n  * Using all of the above to fulfill both simple and complex data tasks. \n\n", "mimetype": "text/plain", "start_char_idx": 422, "end_char_idx": 685, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da9ce0f0-722d-4454-a6ab-c47cbce1b94d": {"__data__": {"id_": "da9ce0f0-722d-4454-a6ab-c47cbce1b94d", "embedding": null, "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bf175024-92ea-4126-a6fb-9f1bb834913c", "node_type": "4", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "1beba26d53a7fbe5d5daca8c63bb5f086f199996b63573ec2ef86e96a8574283", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "524770c4-6f60-48d2-ae21-e92a87622539", "node_type": "1", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "f2f0724f5ac89b73b448b8c1f873ca3917a879eef4001ac0a816c4291789cca9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "557d5334-b2dd-4b26-b03d-8e221515ddb9", "node_type": "1", "metadata": {}, "hash": "b713f50f043e8b686569d985f87e716ffd0b96f8a5d0c1bfb72585be0f3fdec3", "class_name": "RelatedNodeInfo"}}, "text": "We\u2019ve worked hard to provide abstractions, services, and guides on both the\nagents side and tools side in order to build data agents. Today\u2019s launch\nconsists of the following key components:\n\n  * [ **General Agent/Tool Abstractions** ](https://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/agents/root.html) **:** a set of abstractions to build agent loops, and to have those loops interact with tools according to a structured API definition. \n  * [ **LlamaHub Tool Repository** ](https://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/tools/root.html) **:** A [ brand-new section within LlamaHub ](https://llamahub.ai/) that consists of 15+ Tools (e.g. Google Calendar, Notion, SQL, OpenAPI) that can be connected. Opening to [ community contributions ](https://github.com/emptycrown/llama-hub) ! \n\n", "mimetype": "text/plain", "start_char_idx": 685, "end_char_idx": 1517, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "557d5334-b2dd-4b26-b03d-8e221515ddb9": {"__data__": {"id_": "557d5334-b2dd-4b26-b03d-8e221515ddb9", "embedding": null, "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bf175024-92ea-4126-a6fb-9f1bb834913c", "node_type": "4", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "1beba26d53a7fbe5d5daca8c63bb5f086f199996b63573ec2ef86e96a8574283", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da9ce0f0-722d-4454-a6ab-c47cbce1b94d", "node_type": "1", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "bd00894df1667e5c529ffddffbf44daf9698ab65bc620a0ec0cd60de981489cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "abb120b3-9231-42a5-b3a8-cfdb0b14584a", "node_type": "1", "metadata": {}, "hash": "155a13ce72287f3ef0ece275eded9f3b85be06fb48123b64324cbd8e367f3c66", "class_name": "RelatedNodeInfo"}}, "text": "See below for full details. **We show you how to build a Gmail agent that\u2019s\nable to automatically create/send emails in <10 lines of code! **\n\n#  Context\n\nOur core mission at LlamaIndex is to unlock the full capabilities of LLMs over\nyour external sources of data. It provides a set of tools to both define\n\u201cstate\u201d (how to parse/structure your data), and \u201ccompute\u201d (how to query your\ndata). ", "mimetype": "text/plain", "start_char_idx": 1517, "end_char_idx": 1908, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abb120b3-9231-42a5-b3a8-cfdb0b14584a": {"__data__": {"id_": "abb120b3-9231-42a5-b3a8-cfdb0b14584a", "embedding": null, "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bf175024-92ea-4126-a6fb-9f1bb834913c", "node_type": "4", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "1beba26d53a7fbe5d5daca8c63bb5f086f199996b63573ec2ef86e96a8574283", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "557d5334-b2dd-4b26-b03d-8e221515ddb9", "node_type": "1", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "deb9ed7ee962b671cabe77da29ca73c07a6b339ac7c81a812cd84faa86f43b86", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c62be553-1e07-49b3-9578-92cf8a9b2648", "node_type": "1", "metadata": {}, "hash": "79969d662ce55e29d3250f2ca022e7db2784f7a35345577e8a7f254cb2787919", "class_name": "RelatedNodeInfo"}}, "text": "Up until now, our framework has primarily focused on search and\nretrieval use case. We have an incredible suite of tools and capabilities that\nnot only allow you to create the basic RAG stack around a vector database +\ntop-k retrieval, but also offer much greater functionality [ beyond that\n](https://gpt-\nindex.readthedocs.io/en/latest/core_modules/query_modules/query_engine/root.html)\n.\n\nA lot of that technology used to lie in our query engines. Our goal was to\nincrease the capability of query engines to answer a wide range of different\nqueries. In order to do this, we had to improve the \u201creasoning\u201d capabilities\nof these query engines. As a result some of our existing query capabilities\ncontain \u201cagent-like\u201d components: we have query engines capable of chain-of-\nthought reasoning, query decomposition, and routing. In the process, users had\nthe option of choosing from a spectrum of query engines that had more\nconstrained reasoning capabilities to less constrained capabilities.\n\nBut there was a huge opportunity for LLMs to have an even richer set of\ninteractions with data; they should be capable of general reasoning over any\nset of tools, whether from a database or an API. They should also be capable\nof both \u201cread\u201d and \u201cwrite\u201d capabilities \u2014 the ability to not only understand\nstate but also modify it. As a result they should be able to do more than\nsearch and retrieval from a static knowledge source.\n\nSome existing [ services ](https://openai.com/blog/chatgpt-plugins) , [\ntoolkits ](https://python.langchain.com/docs/modules/agents/) , and [ research\n](https://arxiv.org/abs/2302.04761) [ papers\n](https://arxiv.org/abs/2210.03629) have already demonstrated the\npossibilities of LLM-powered \u201cagents\u201d that can interact with the external\nenvironment. Using these existing approaches as inspiration, we saw an\nopportunity to build a principled series of abstractions enabling anyone to\nbuild knowledge workers over their data.\n\n#  Core Components of Data Agents\n\nBuilding a data agent requires the following core components:\n\n  * A reasoning loop \n  * Tool abstractions \n\nAt a high-level, a data agent is provided with a set of APIs, or Tools, to\ninteract with. These APIs can return information about the world, or perform\nan action that modifies state. ", "mimetype": "text/plain", "start_char_idx": 1908, "end_char_idx": 4183, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c62be553-1e07-49b3-9578-92cf8a9b2648": {"__data__": {"id_": "c62be553-1e07-49b3-9578-92cf8a9b2648", "embedding": null, "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bf175024-92ea-4126-a6fb-9f1bb834913c", "node_type": "4", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "1beba26d53a7fbe5d5daca8c63bb5f086f199996b63573ec2ef86e96a8574283", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "abb120b3-9231-42a5-b3a8-cfdb0b14584a", "node_type": "1", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "479ab06a65580908759d40a95af5d170de740e3d935bcfd4ed53b1eb92b5edf3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a4f716e-2a19-4aeb-b981-92c4a0e8c156", "node_type": "1", "metadata": {}, "hash": "ea9e41c1e1f6f7ba54f7b345cb17f0aac0a24fd109b87202026520aef135b608", "class_name": "RelatedNodeInfo"}}, "text": "Each Tool exposes a request/response interface.\nThe request is a set of structured parameters, and the response can be any\nformat (at least conceptually, in most cases the response here is a text\nstring of some form).\n\nGiven an input task, the data agent uses a **reasoning loop** to decide which\ntools to use, in which sequence, and the parameters to call each tool. The\n\u201cloop\u201d can conceptually be very simple (a one-step tool selection process), or\ncomplex (a multi-step selection process, where a multitude of tools are picked\nat each step).\n\nThese components are described in more detail below.\n\n##  Agent Abstraction + Reasoning Loop\n\nWe have support for the following agents:\n\n  * OpenAI Function agent (built on top of the OpenAI Function API) \n  * a ReAct agent (which works across any chat/text completion endpoint). \n\nYou can use them as the following:\n\n    \n    \n    from llama_index.agent import OpenAIAgent, ReActAgent\n    from llama_index.llms import OpenAI\n    \n    # import and define tools\n    ...\n    # initialize llm\n    llm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n    # initialize openai agent\n    agent = OpenAIAgent.from_tools(tools, llm=llm, verbose=True)\n    # initialize ReAct agent\n    agent = ReActAgent.from_tools(tools, llm=llm, verbose=True)\n    # use agent\n    response = agent.chat(\"What is (121 * 3) + 42?\")\n\nEach agent takes in a set of Tools. ", "mimetype": "text/plain", "start_char_idx": 4183, "end_char_idx": 5558, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a4f716e-2a19-4aeb-b981-92c4a0e8c156": {"__data__": {"id_": "0a4f716e-2a19-4aeb-b981-92c4a0e8c156", "embedding": null, "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bf175024-92ea-4126-a6fb-9f1bb834913c", "node_type": "4", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "1beba26d53a7fbe5d5daca8c63bb5f086f199996b63573ec2ef86e96a8574283", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c62be553-1e07-49b3-9578-92cf8a9b2648", "node_type": "1", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "eb79b2e902e3da3745ea286521450747894dfd55909502c7df092e245d71dbf9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "55d15888-53e7-4b17-b896-10889f03d2d8", "node_type": "1", "metadata": {}, "hash": "4d47ab0f02ad54ad2e2d2a3cd7f6ee04ef80d989b1a4e8bd994bda4200d216d6", "class_name": "RelatedNodeInfo"}}, "text": "The details behind our tool abstractions\nare provided below. Each agent also supports two main methods for taking in an\ninput task \u2014 ` chat ` and ` query ` . Note that these are the core methods\nused in our ` ChatEngine ` and ` QueryEngine ` respectively. In fact that our\nbase agent class ( ` BaseAgent ` ) simply inherits from ` BaseChatEngine ` and\n` BaseQueryEngine ` . ` chat ` allows the agent to utilize previously stored\nconversation history, whereas ` query ` is a stateless call - history/state is\nnot preserved over time.\n\nThe reasoning loop depends on the type of agent. The OpenAI agent calls the\nOpenAI function API in a while loop, since the tool decision logic is baked\ninto the function API. Given an input prompt and previous chat history (which\nincludes previous function calls), the function API will decide whether to\nmake another function call (pick a Tool), or return an assistant message. If\nthe API returns a function call, then we are responsible for executing the\nfunction and passing in a function message in the chat history. If the API\nreturns an assistant message, then the loop is complete (we assume the task is\nsolved).\n\nThe ReAct agent uses general text completion endpoints, so it can be used with\nany LLM. A text completion endpoint has a simple input str \u2192 output str\nformat, which means that the reasoning logic must be encoded in the prompt.\nThe ReAct agent uses an input prompt inspired by the ReAct paper (and adapted\ninto other versions), in order to decide which tool to pick. It looks\nsomething like this:\n\n    \n    \n    ...\n    You have access to the following tools:\n    {tool_desc}\n    \n    To answer the question, please use the following format.\n    \n    ```\n    Thought: I need to use a tool to help me answer the question.\n    Action: tool name (one of {tool_names})\n    Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"text\": \"hello world\", \"num_beams\": 5}})\n    ```\n    Please use a valid JSON format for the action input. Do NOT do this {{'text': 'hello world', 'num_beams': 5}}.\n    \n    If this format is used, you will receive a response in the following format:\n    \n    ```\n    Observation: tool response\n    ```\n    ...\n\nWe implement ReAct natively over chat prompts; the reasoning loop is\nimplemented as an alternating series of assistant and user messages. The\nThought/Action/Action Input section is represented as an assistant message,\nand the Observation section is implemented as a user message.\n\n**Note:** the ReAct prompt expects not only the name of the tool to pick, but\nalso the parameters to fill in the tool in a JSON format. This makes the\noutput not dissimilar from the output of the OpenAI Function API \u2014 the main\ndifference is that in the case of the function API, the tool-picking logic is\nbaked into the API itself (through a finetuned model), whereas here it is\nelicited through explicit prompting.\n\n##  Tool Abstractions\n\nHaving proper tool abstractions is at the core of building data agents.\nDefining a set of Tools is similar to defining any API interface, with the\nexception that these Tools are meant for agent rather than human use. We allow\nusers to define both a single Tool as well as a \u201cToolSpec\u201d containing a series\nof functions under the hood.\n\nWe describe the base tool abstraction, as well as how you can easily define\ntools over existing query engines, other tools.\n\n**Base Tool Abstraction**\n\nThe base tool defines a very generic interface. The ` __call__ ` function can\ntake in any series of arguments, and return a generic ` ToolOutput ` container\nthat can capture any response. A tool also has metadata containing its name,\ndescription, and function schema.\n\n    \n    \n    @dataclass\n    class ToolMetadata:\n        description: str\n        name: Optional[str] = None\n        fn_schema: Optional[Type[BaseModel]] = DefaultToolFnSchema\n    \n    class BaseTool:\n        @property\n        @abstractmethod\n        def metadata(self) -&gt; ToolMetadata:\n            pass\n        @abstractmethod\n        def __call__(self, input: Any) -&gt; ToolOutput:\n            pass\n\n**Function Tool**\n\nA function tool allows users to easily convert any function into a Tool. It\ntakes in a user-defined function (that can take in any inputs/outputs), and\nwraps it into a tool interface. It can also \u201cauto-infer\u201d the function schema\nif it isn\u2019t specified beforehand.\n\nOur ` ToolSpec ` classes make use of this ` FunctionTool ` abstraction to\nconvert functions defined in the tool spec into a set of agent tools (see\nbelow).\n\nHere\u2019s a trivial example of defining a FunctionTool.\n\n    \n    \n    from llama_index.tools.function_tool import FunctionTool\n    \n    def multiply(a: int, b: int) -&gt; int:\n        \"\"\"Multiple two integers and returns the result integer\"\"\"\n        return a * b\n    multiply_tool = FunctionTool.from_defaults(fn=multiply)\n\n**QueryEngineTool**\n\nOf course, we also provide Tool abstractions to wrap our existing query\nengines. This provides a seamless transition from working on query engines to\nworking on agents. ", "mimetype": "text/plain", "start_char_idx": 5558, "end_char_idx": 10614, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "55d15888-53e7-4b17-b896-10889f03d2d8": {"__data__": {"id_": "55d15888-53e7-4b17-b896-10889f03d2d8", "embedding": null, "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bf175024-92ea-4126-a6fb-9f1bb834913c", "node_type": "4", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "1beba26d53a7fbe5d5daca8c63bb5f086f199996b63573ec2ef86e96a8574283", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a4f716e-2a19-4aeb-b981-92c4a0e8c156", "node_type": "1", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "919c14026979611f6054a3019c32b5027a659e121dd9e80697bc5ccc335a5020", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3021bfef-2e95-4411-8eaf-049192a50eab", "node_type": "1", "metadata": {}, "hash": "6e6bcb6b117942cbb8a690636b82f9be28d0c6eb94fcfd90294455be31c162d8", "class_name": "RelatedNodeInfo"}}, "text": "Our query engines can be thought of \u201cconstrained\u201d agents\nmeant for the read/write setting and centered around retrieval purposes. These\nquery engines can be used in an overall agent setting.\n\n    \n    \n    from llama_index.tools import QueryEngineTool\n    \n    query_engine_tools = [\n        QueryEngineTool(\n            query_engine=query_engine, \n            metadata=ToolMetadata(\n                name='&lt;tool_name&gt;', \n                description=\"Queries over X data source.\"\n            )\n        ),\n     ...\n    ]\n\n**Tool Specs**\n\nA **tool spec** is a Python class that represents a full API specification\nthat an agent can interact with, and a tool spec can be converted into a list\nof tools that an agent can be initialized with.\n\nThis class allows users to define entire services, not just single tools that\nperform individual tasks. Each tool spec may contain read/write endpoints that\nallow an agent to interact with a service in meaningful ways. For instance, a\nSlack tool spec could allow the user to both read existing messages and\nchannels ( ` load_data ` , ` fetch_channels ` ) as well as write messages ( `\nsend_message ` ). It would be roughly defined as the following:\n\n    \n    \n    class SlackToolSpec(BaseToolSpec):\n        \"\"\"Slack tool spec.\"\"\"\n        spec_functions = [\"load_data\", \"send_message\", \"fetch_channels\"]\n    \n        def load_data(\n              self,\n              channel_ids: List[str],\n              reverse_chronological: bool = True,\n          ) -&gt; List[Document]:\n              \"\"\"Load data from the input directory.\"\"\"\n              ...\n          def send_message(\n              self,\n              channel_id: str,\n              message: str,\n          ) -&gt; None:\n              \"\"\"Send a message to a channel given the channel ID.\"\"\"\n              ...\n          def fetch_channels(\n              self,\n          ) -&gt; List[str]:\n              \"\"\"Fetch a list of relevant channels.\"\"\"\n              ...\n\nIf a tool spec is initialized, it can be converted into a list of tools that\ncan be fed into an agent with ` to_tool_list ` . For instance,\n\n    \n    \n    tool_spec = SlackToolSpec()\n    # initialize openai agent\n    agent = OpenAIAgent.from_tools(tool_spec.to_tool_list(), llm=llm, verbose=True)\n\nDefining a tool spec is not that different than defining a Python class. Each\nfunction becomes converted into a tool, and by default the docstring for each\nfunction gets used as the tool description (though you can customize\nnames/description in ` to_tool_list(func_to_metadata_mapping=...) ` .\n\nWe also made the intentional choice that the input arguments and return types\ncan be anything. The primary reason is to preserve the generality of the tool\ninterface for subsequent iterations of agents. Even if current iterations of\nagents expect tool outputs to be in string format, that may change in the\nfuture, and we didn\u2019t want to arbitrarily restrict the types of tool\ninterface.\n\n#  LlamaHub Tool Repository\n\nA huge component of our launch is a brand-new addition to [ LlamaHub\n](https://llamahub.ai/) : a Tool Repository. The Tool Repository consists of\n**15+ Tool Specs** that an agent can use. These tool specs represent an\ninitial curated list of services that an agent can interact with and enrich\nits capability to perform different actions.\n\nAmong others, they include the following specs:\n\n  * Gmail Spec \n  * Zapier Spec \n  * Google Calendar Spec \n  * OpenAPI Spec \n  * SQL + Vector Database Spec \n\nWe also provide a list of **utility tools** that help to abstract away pain\npoints when designing agents to interact with different API services that\nreturn large amounts of data.\n\nFor instance, our Gmail Tool Spec allows an agent to search existing emails,\ncreate drafts, update drafts, and send emails. Our Zapier Spec allows an agent\nto perform any natural language query to Zapier through their [ Natural\nLanguage Actions ](https://nla.zapier.com/start/) interface.\n\nBest of all, you don\u2019t need to spend a lot of time figuring out how to use\nthese tools \u2014 we have [ **10+ notebooks**\n](https://github.com/emptycrown/llama-hub/tree/main/llama_hub/tools/notebooks)\nshowing how you can build agents for each service, or even build agents that\nuse a combination of services (e.g. Gmail, Google Calendar, and Search).\n\n", "mimetype": "text/plain", "start_char_idx": 10614, "end_char_idx": 14905, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3021bfef-2e95-4411-8eaf-049192a50eab": {"__data__": {"id_": "3021bfef-2e95-4411-8eaf-049192a50eab", "embedding": null, "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bf175024-92ea-4126-a6fb-9f1bb834913c", "node_type": "4", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "1beba26d53a7fbe5d5daca8c63bb5f086f199996b63573ec2ef86e96a8574283", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "55d15888-53e7-4b17-b896-10889f03d2d8", "node_type": "1", "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}, "hash": "5480b36b5e76a5313ee3eae3e4829f0be0c38dfdf2f9243b90ea320562c95563", "class_name": "RelatedNodeInfo"}}, "text": "##  Example Walkthrough\n\nLet\u2019s take a look at a few examples! We initialize an OpenAIAgent with the\nGmail Spec. As mentioned above, the spec consists of tools to search emails,\ncreate/update drafts, and send emails.\n\nNow let\u2019s give the agent a sequence of commands so that it can create an email\ndraft, make a few edits to it, and then send it off.\n\nFirst, let\u2019s create an initial email draft. Note that the agent chooses the `\ncreate_draft ` tool, which takes in the \u201cto\u201d, \u201csubject\u201d, and \u201cmessage\u201d\nparameters. The agent is able to infer the parameters simultaneously while\npicking the tool.\n\nNext, let\u2019s update the draft with a slight modification:\n\nNext, let\u2019s show the current state of the draft.\n\nFinally, let\u2019s send the email!\n\nThis is a good start, but this is just the beginning. We are actively working\non contributing more tools to this repository, and we\u2019re also opening this up\nto community contributions. If you\u2019re interested in contributing a Tool to\nLlamaHub, please feel free to open a PR in this repo.\n\n##  Utility Tools\n\nOftentimes, directly querying an API can return a massive volume of data,\nwhich on its own may overflow the context window of the LLM (or at the very\nleast unnecessarily increase the number of tokens that you are using).\n\nTo tackle this, we\u2019ve provided an initial set of \u201cutility tools\u201d in the core\nLlamaIndex repo \u2014 utility tools are not conceptually tied to a given service\n(e.g. Gmail, Notion), but rather can augment the capabilities of existing\nTools. In this particular case, utility tools help to abstract away common\npatterns of needing to cache/index and query data that\u2019s returned from any API\nrequest.\n\nLet\u2019s walk through our two main utility tools below.\n\n**OnDemandLoaderTool**\n\nThis tool turns any existing LlamaIndex data loader ( ` BaseReader ` class)\ninto a tool that an agent can use. The tool can be called with all the\nparameters needed to trigger ` load_data ` from the data loader, along with a\nnatural language query string. During execution, we first load data from the\ndata loader, index it (for instance with a vector store), and then query it\n\u201con-demand\u201d. All three of these steps happen in a single tool call.\n\nOftentimes this can be preferable to figuring out how to load and index API\ndata yourself. While this may allow for data reusability, oftentimes users\njust need an ad-hoc index to abstract away prompt window limitations for any\nAPI call.\n\nA usage example is given below:\n\n    \n    \n    from llama_hub.wikipedia.base import WikipediaReader\n    from llama_index.tools.on_demand_loader_tool import OnDemandLoaderTool\n    \n    tool = OnDemandLoaderTool.from_defaults(\n     reader,\n     name=\"Wikipedia Tool\",\n     description=\"A tool for loading data and querying articles from Wikipedia\"\n    )\n\n**LoadAndSearchToolSpec**\n\nThe LoadAndSearchToolSpec takes in any existing Tool as input. As a tool spec,\nit implements ` to_tool_list ` , and when that function is called, two tools\nare returned: a ` load ` tool and then a ` search ` tool.\n\nThe ` load ` Tool execution would call the underlying Tool, and the index the\noutput (by default with a vector index). The ` search ` Tool execution would\ntake in a query string as input and call the underlying index.\n\nThis is helpful for any API endpoint that will by default return large volumes\nof data \u2014 for instance our WikipediaToolSpec will by default return entire\nWikipedia pages, which will easily overflow most LLM context windows.\n\nExample usage is shown below:\n\n    \n    \n    from llama_hub.tools.wikipedia.base import WikipediaToolSpec\n    from llama_index.tools.tool_spec.load_and_search.base import LoadAndSearchToolSpec\n    \n    wiki_spec = WikipediaToolSpec()\n    # Get the search wikipedia tool\n    tool = wiki_spec.to_tool_list()[1]\n    # Create the Agent with load/search tools\n    agent = OpenAIAgent.from_tools(\n     LoadAndSearchToolSpec.from_defaults(\n        tool\n     ).to_tool_list(), verbose=True\n    )\n\nThis is the output when we run an input prompt\n\n    \n    \n    agent.chat('what is the capital of poland')\n\nOutput:\n\n    \n    \n    === Calling Function ===\n    Calling function: search_data with args: {\n      \"query\": \"capital of Poland\"\n    }\n    Got output: Content loaded! You can now search the information using read_search_data\n    ========================\n    === Calling Function ===\n    Calling function: read_search_data with args: {\n      \"query\": \"What is the capital of Poland?\"\n    }\n    Got output: \n    The capital of Poland is Warsaw.\n    ========================\n    AgentChatResponse(response='The capital of Poland is Warsaw.', sources=[])\n\nNote that the agent figures out that it first needs to first call the \u201cload\u201d\ntool (denoted by the original name of the tool, \u201csearch_data\u201d). This load tool\nwill load the Wikipedia page and index under the hood. The output just\nmentions that the \u201ccontent is loaded, and tells the agent that the next step\nis to use ` read_search_data ` . The agent then reasons that it needs to call\nthe ` read_search_data ` tool, which will query the index for the right\nanswer.\n\n#  FAQ\n\n**Should I use Data Agents for search and retrieval, or continue to use Query\nEngines?**\n\nShort answer: both are possible. Query engines give you the ability to define\nyour own workflows over your data, in both a constrained reasoning fashion as\nwell as unconstrained fashion. For instance, you may want to define a specific\nworkflow over text-to-SQL with our ` NLStructStoreQueryEngine ` (constrained),\nor a router module to decide between semantic search or summarization (less\nconstrained), or use our ` SubQuestionQueryEngine ` to decompose a question\namong sub-documents (even less constrained).\n\nBy default, agent loops are unconstrained, and can theoretically reason over\nany set of tools that you give them. This means that you can get out-of-the-\nbox advanced search/retrieval capabilities \u2014 for instance, in our OpenAI\ncookbook we show that you can get joint text-to-SQL capabilities by simply\nproviding a SQL query engine and Vector Store Query engine as tools. But on\nthe other hand, agents built in this fashion can be quite unreliable (see our\nblog post for more insights). If you are using agents for search/retrieval, be\nmindful of the 1) LLM you pick, and the 2) set of tools you pick too.\n\n**How are LlamaIndex data agents different than existing agent frameworks\n(LangChain, Hugging Face, etc.)?**\n\nMost of these core concepts are not new. Our overall design has taken\ninspiration from popular tools and frameworks for building agents. But in our\n\u201cdata agents\u201d design, we\u2019ve tried our best to answer the following key\nquestions well:\n\n  * How do we effectively index/query and retrieve data beforehand? \n  * How do we effectively index/query and retrieve data on the fly? \n  * How do we design API interfaces for read/writes that are simultaneously rich (can take in structured inputs), but also easy for agents to understand? \n  * How do we properly get sources in citations? \n\nOur goal with data agents is to create automated knowledge workers that can\nreason over and interact with data. Our core toolkit provides the foundations\nfor properly indexing, retrieving, and querying data \u2014 these can be easily\nintegrated as tools. We provide some additional tool abstractions to handle\nthe cases where you want to \u201ccache\u201d API outputs on the fly (see above).\nFinally, we provide principled tool abstractions and design principles so that\nagents can interface with external services in a structured manner.\n\n**Can I use Tools with LangChain agents?** You can easily use any of our tools\nwith LangChain agents as well.\n\n    \n    \n    tools = tool_spec.to_tool_list()\n    langchain_tools = [t.to_langchain_tool() for t in tools]\n\nSee our [ tools usage guide ](https://gpt-\nindex.readthedocs.io/en/latest/core_modules/agent_modules/tools/usage_pattern.html#using-\nwith-langchain) for more details!\n\n#  Conclusion\n\nIn summary, today we launched two key items: Data Agent components (incl.\nagent reasoning loop and tool abstractions) and the LlamaHub Tool repository.\n\n##  Resources\n\nWe\u2019ve written a comprehensive section in the docs \u2014 take a look here: [\nhttps://gpt-\nindex.readthedocs.io/en/latest/core_modules/agent_modules/agents/root.html\n](https://gpt-\nindex.readthedocs.io/en/latest/core_modules/agent_modules/agents/root.html)\n\nTake a look at our LlamaHub Tools section: [ https://llamahub.ai/\n](https://llamahub.ai/)\n\nNotebook Tutorials for LlamaHub Tools: [ https://github.com/emptycrown/llama-\nhub/tree/main/llama_hub/tools/notebooks ](https://github.com/emptycrown/llama-\nhub/tree/main/llama_hub/tools/notebooks)\n\nIf you have questions, please hop on our Discord: [\nhttps://discord.gg/dGcwcsnxhU ](https://discord.gg/dGcwcsnxhU)\n\n", "mimetype": "text/plain", "start_char_idx": 14905, "end_char_idx": 23632, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93236a20-3f7f-4d21-ab91-4214fd5cf2bb": {"__data__": {"id_": "93236a20-3f7f-4d21-ab91-4214fd5cf2bb", "embedding": null, "metadata": {"filename": "llamaindex-update-07-10-2023-4ceebdab96cb.md", "extension": ".md", "title": "LlamaIndex Update \u2014 07/11/2023", "date": "Jul 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-07-10-2023-4ceebdab96cb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c33628f-7781-4a91-912c-dc524ca918db", "node_type": "4", "metadata": {"filename": "llamaindex-update-07-10-2023-4ceebdab96cb.md", "extension": ".md", "title": "LlamaIndex Update \u2014 07/11/2023", "date": "Jul 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-07-10-2023-4ceebdab96cb"}, "hash": "2f489319d3393953bf80c831a53006aa96e01a721225ed90a565927b31c9a74b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6c3eeb1-ffb5-4869-b225-1aa0c0ac70af", "node_type": "1", "metadata": {}, "hash": "c39c48c15bdcaf898961a5ae0bb6226e197fd82a4627159c6b83318847478cfb", "class_name": "RelatedNodeInfo"}}, "text": "Greetings once again, LlamaIndex community!\n\nWelcome back to our second installment in the LlamaIndex Update series. In our\nongoing commitment to keep you informed and engaged with our rapidly evolving\nopen-source project, this blog post brings you more exciting updates on\nfeatures, webinars, hackathons, and community events.\n\nBuilding on the foundation of our inaugural post, we will continue to strive\nto keep both our long-standing contributors and fresh faces synced with our\nprogress. We aim to not just inform but also inspire you to partake in our\ncollective journey towards growth and innovation.\n\nWithout further delay, let\u2019s delve into the latest happenings in this edition\nof the LlamaIndex Update.\n\n##  Features And Integrations:\n\n  1. LlamaIndex\u2019s partnership with Anyscale uses the Ray platform to boost performance and deployment. It accelerates LlamaIndex\u2019s operations by a factor of ten and streamlines deployment to production servers. The core Ray Distributed Toolkit aids in efficient task parallelization, while Ray Serve ensures easy deployment of query engines to production. [ Blogpost ](https://www.anyscale.com/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-ray) , [ Tweet ](https://twitter.com/llama_index/status/1673451316398653440?s=20)\n  2. LlamaIndex enhanced metadata representation in documents. The ` extra_info ` and ` node_info ` fields are now replaced with a ` metadata ` dictionary. This facilitates precise control over data and allows users to exclude metadata keys during embedding or LLM prediction. This boosts LLM and retrieval performance and offers customizable metadata injection, formatting, and template creation. [ Docs ](https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_documents.html) , [ Tweet ](https://twitter.com/llama_index/status/1673721486757199872?s=20)\n  3. LlamaIndex supports both Text Completion API, involving output parsing and input prompt modification, and Structured API, requiring input function signatures and output conversion. Despite Structured API being easier to use, its limited availability keeps Text Completion API relevant. Both are supported by LlamaIndex\u2019s ` PydanticProgram ` . [ Docs ](https://gpt-index.readthedocs.io/en/latest/how_to/structured_outputs/root.html) , [ Tweet ](https://twitter.com/llama_index/status/1674075533548871681?s=20)\n  4. LlamaIndex now collaborates with Chainlit.io, facilitating swift construction of advanced chat UIs for any LLM app. This integration, beyond providing a basic chat interface, also logs intermediate results and sources. [ Blogpost ](https://docs.chainlit.io/integrations/llama-index) , [ Tweet ](https://twitter.com/jerryjliu0/status/1674107773758611456?s=20)\n  5. LlamaIndex now incorporates the DePlot model for interpreting charts and plots in QA/chatbot applications. Primarily effective for simple charts, such as bar charts and time series, DePlot converts these visuals into text format for easy embedding, indexing, and usage in downstream applications. This functionality is now accessible via the LlamaHub data loader, expanding LlamaIndex\u2019s capabilities for diverse applications. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3162, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6c3eeb1-ffb5-4869-b225-1aa0c0ac70af": {"__data__": {"id_": "e6c3eeb1-ffb5-4869-b225-1aa0c0ac70af", "embedding": null, "metadata": {"filename": "llamaindex-update-07-10-2023-4ceebdab96cb.md", "extension": ".md", "title": "LlamaIndex Update \u2014 07/11/2023", "date": "Jul 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-07-10-2023-4ceebdab96cb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c33628f-7781-4a91-912c-dc524ca918db", "node_type": "4", "metadata": {"filename": "llamaindex-update-07-10-2023-4ceebdab96cb.md", "extension": ".md", "title": "LlamaIndex Update \u2014 07/11/2023", "date": "Jul 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-07-10-2023-4ceebdab96cb"}, "hash": "2f489319d3393953bf80c831a53006aa96e01a721225ed90a565927b31c9a74b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93236a20-3f7f-4d21-ab91-4214fd5cf2bb", "node_type": "1", "metadata": {"filename": "llamaindex-update-07-10-2023-4ceebdab96cb.md", "extension": ".md", "title": "LlamaIndex Update \u2014 07/11/2023", "date": "Jul 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-07-10-2023-4ceebdab96cb"}, "hash": "d6f981fd523618d9d1bc5965e9d52be19c8f6b4d258f2cb808896fdf8dbb203f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac2036b1-502c-4d11-821c-b194483aaffa", "node_type": "1", "metadata": {}, "hash": "bd2a4fa3ec17d2cb91d3c49f4eb52fa74f8623dd184b76086e8e246d6ba81be2", "class_name": "RelatedNodeInfo"}}, "text": "[ Docs ](https://llama-hub-ui.vercel.app/l/file-image_deplot) , [ Tweet ](https://twitter.com/jerryjliu0/status/1674442367087316992?s=20)\n  6. LlamaIndex now incorporates the Github Issues reader, which allows for comprehensive loading and querying of issues from any GitHub repository. Additionally, the Sitemap Loader reader enables users to read all webpages from a specified sitemap. [ Tweet ](https://twitter.com/llama_index/status/1674443061118791680?s=20)\n  7. LlamaIndex introduces the ` ContextRetrieverOpenAIAgent ` feature, which enhances tool picking by incorporating more context from user messages. It performs a retrieval step before the LLM call, ensuring increased reliability and better mapping of queries to the right tools, especially in the presence of domain-specific terms. Unlike a \u201cretrieval tool\u201d, this feature guarantees retrieval before any action is taken. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/agent/openai_agent_context_retrieval.html) , [ Tweet ](https://twitter.com/jerryjliu0/status/1674807074918928385?s=20)\n  8. LlamaIndex now features code-based extraction for efficient data extraction from arbitrary text. This feature includes a \u201cFit\u201d step to generate functions based on training data, and an \u201cInference\u201d step to run these functions on new data. It offers two versions: DFEvaporateProgram for extracting one value per field from a text, and MultiValueEvaporateProgram for extracting multiple values per field. This feature can be used to extract structured data from raw HTML sources and also offers the ability to identify salient fields in a text given a topic. ", "mimetype": "text/plain", "start_char_idx": 3162, "end_char_idx": 4789, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac2036b1-502c-4d11-821c-b194483aaffa": {"__data__": {"id_": "ac2036b1-502c-4d11-821c-b194483aaffa", "embedding": null, "metadata": {"filename": "llamaindex-update-07-10-2023-4ceebdab96cb.md", "extension": ".md", "title": "LlamaIndex Update \u2014 07/11/2023", "date": "Jul 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-07-10-2023-4ceebdab96cb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c33628f-7781-4a91-912c-dc524ca918db", "node_type": "4", "metadata": {"filename": "llamaindex-update-07-10-2023-4ceebdab96cb.md", "extension": ".md", "title": "LlamaIndex Update \u2014 07/11/2023", "date": "Jul 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-07-10-2023-4ceebdab96cb"}, "hash": "2f489319d3393953bf80c831a53006aa96e01a721225ed90a565927b31c9a74b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6c3eeb1-ffb5-4869-b225-1aa0c0ac70af", "node_type": "1", "metadata": {"filename": "llamaindex-update-07-10-2023-4ceebdab96cb.md", "extension": ".md", "title": "LlamaIndex Update \u2014 07/11/2023", "date": "Jul 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-07-10-2023-4ceebdab96cb"}, "hash": "ef25bc6f0cad18c085fb062a5e5d3785a5bcce3157f971f7ecf3413a3b865409", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4debc68-bd48-4223-ba34-e592fdbe37d0", "node_type": "1", "metadata": {}, "hash": "9ac232219990f9349326038651e7e36e50c6206d77a3c8d1fa0c72ae208be3ea", "class_name": "RelatedNodeInfo"}}, "text": "[ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/output_parsing/evaporate_program.html) , [ Tweet ](https://twitter.com/jerryjliu0/status/1675901084840390656?s=20)\n  9. LlamaIndex has significantly improved its text-to-SQL capabilities, offering a \u201cDefault\u201d SQL query engine and an SQL query engine with an object index for handling large table schemas. These upgrades simplify the process, requiring only a SQL database for the default engine and enabling indexing of large table schemas with the ObjectIndex. Additionally, LlamaIndex now also integrates with [ duckdb ](http://twitter.com/duckdb) , further enhancing the SQL querying process. [ Docs_SQL ](https://gpt-index.readthedocs.io/en/latest/guides/tutorials/sql_guide.html) , [ Docs_duckdb ](https://gpt-index.readthedocs.io/en/latest/examples/index_structs/struct_indices/duckdb_sql_query.html) , [ Tweet ](https://twitter.com/llama_index/status/1676002583381692421?s=20)\n  10. LlamaIndex 0.7.0 enhances modularity for LLM app development. It includes native LLM abstractions for platforms like OpenAI and Hugging Face, a standalone Response Synthesis module, and improved Document Metadata Management. These abstractions can be used independently or integrated into indices/query engines. The Response Synthesis module abstracts away context window limitations, while the Document Metadata Management feature allows deep customization of metadata, potentially boosting retrieval performance. [ Blogpost ](https://medium.com/llamaindex-blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024) , [ Tweet ](https://twitter.com/llama_index/status/1676255154662969345?s=20)\n  11. LlamaIndex introduces Recursive Retrieval, a concept that utilizes the hierarchical nature of knowledge. A Node in LlamaIndex can contain references to other retrievers or query engines. This process starts with a retriever and recursively explores links to others. For instance, structured tables from a PDF can be extracted, each represented as a data frame. These tables can be referenced by ` IndexNode ` objects embedded with other Nodes. During a query, if an IndexNode is among the top-k nodes, it triggers another retriever or query engine, allowing sophisticated querying overall data. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/query_engine/pdf_tables/recursive_retriever.html) , [ Tweet ](https://twitter.com/jerryjliu0/status/1676606169002004481?s=20)\n  12. LlamaIndex introduces OpenAI agent streaming for efficient function calling and enhances user experience by providing progress bars during index creation for a real-time understanding of the process duration. [ Tweet ](https://twitter.com/llama_index/status/1676742253669408768?s=20)\n  13. LlamaIndex introduces personalized data interaction through system prompts, callback events for SubQuestionQueryEngine, and a streamlined process for Azure OpenAI integration. [ Docs_AOI ](https://gpt-index.readthedocs.io/en/latest/examples/customization/llms/AzureOpenAI.html) , [ Notebook_personality ](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/chat_engine/chat_engine_personality.ipynb) , [ Tweet ](https://twitter.com/llama_index/status/1676981157513265153?s=20)\n  14. LlamaIndex leverages LLM\u2019s to automatically extract metadata, significantly enhancing the relevance and precision of information retrieval. This is achieved through five key MetadataExtractor modules (SummaryExtractor, QuestionsAnsweredExtractor, TitleExtractor, MetadataFeatureExtractor) that augment text with rich, context-specific details. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/metadata_extraction/MetadataExtractionSEC.html) , [ Tweet ](https://twitter.com/jerryjliu0/status/1677706208017518593?s=20)\n\n##  **Tutorials:**\n\n  1. ", "mimetype": "text/plain", "start_char_idx": 4789, "end_char_idx": 8601, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4debc68-bd48-4223-ba34-e592fdbe37d0": {"__data__": {"id_": "c4debc68-bd48-4223-ba34-e592fdbe37d0", "embedding": null, "metadata": {"filename": "llamaindex-update-07-10-2023-4ceebdab96cb.md", "extension": ".md", "title": "LlamaIndex Update \u2014 07/11/2023", "date": "Jul 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-07-10-2023-4ceebdab96cb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c33628f-7781-4a91-912c-dc524ca918db", "node_type": "4", "metadata": {"filename": "llamaindex-update-07-10-2023-4ceebdab96cb.md", "extension": ".md", "title": "LlamaIndex Update \u2014 07/11/2023", "date": "Jul 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-07-10-2023-4ceebdab96cb"}, "hash": "2f489319d3393953bf80c831a53006aa96e01a721225ed90a565927b31c9a74b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac2036b1-502c-4d11-821c-b194483aaffa", "node_type": "1", "metadata": {"filename": "llamaindex-update-07-10-2023-4ceebdab96cb.md", "extension": ".md", "title": "LlamaIndex Update \u2014 07/11/2023", "date": "Jul 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-07-10-2023-4ceebdab96cb"}, "hash": "85c87790784e297ccb9a3ff90d12336217616665807a2a0d71a92135df412fcc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f66691de-b78e-47a0-b07f-105f7c5821c5", "node_type": "1", "metadata": {}, "hash": "d29d146e116422b3262ecbe53765a7c2a56fccdda1fc7d35b80afdeddc020433", "class_name": "RelatedNodeInfo"}}, "text": "[ Anyscale tutorial ](https://www.youtube.com/watch?v=Vd_8lS1iDBg) on \u201cHow to Build an LLM Query Engine in 10 Minutes using LlamaIndex.\u201d \n  2. [ Erika Cardenas tutorial ](https://www.youtube.com/watch?v=Bu9skgCrJY8) on how to load data into Weaviate and how to connect LlamaIndex to a Weaviate instance using LlamaIndex. \n  3. [ Wenqi Glantz tutorial ](https://betterprogramming.pub/refreshing-private-data-sources-with-llamaindex-document-management-1d1f1529f5eb) on Refreshing Private Data Sources with LlamaIndex Document Management. \n  4. [ Michael Hunger tutorial ](https://medium.com/llamaindex-blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7) on Load in data from [ neo4j ](https://twitter.com/neo4j) , [ NebulaGraph ](https://twitter.com/NebulaGraph) , and index/query with LlamaIndex using GraphDB Cypher and GraphQL data loaders. \n  5. [ Pradip Nichite video tutorial ](https://www.youtube.com/watch?v=XGBQ_f-Yy48) and [ blogpost ](https://blog.futuresmart.ai/mastering-llamaindex-create-save-load-indexes-customize-llms-prompts-embeddings) on Mastering LlamaIndex: Create, Save & Load Indexes, Customize LLMs, Prompts & Embeddings. \n\n##  Webinars And Podcasts:\n\n  1. [ Webinar ](https://www.youtube.com/watch?v=bPoNCkjDmco) on Graph Databases, Knowledge Graphs, and RAG with Wey (NebulaGraph). \n  2. [ Webinar ](https://www.youtube.com/watch?v=gbyfXRxU0Gw) with Albus \u2014 a comprehensive Slackbot for enterprise search, [ xpress.ai ](http://Xpress.ai) \u2014 a low-code solution for building LLM workflows + agents and [ ImmigrantFirst.ai ](https://t.co/QAJyGqZPcB) \u2014 assistant to help immigrants complete their EB-1A/O1 apps more efficiently. \n  3. ", "mimetype": "text/plain", "start_char_idx": 8601, "end_char_idx": 10286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f66691de-b78e-47a0-b07f-105f7c5821c5": {"__data__": {"id_": "f66691de-b78e-47a0-b07f-105f7c5821c5", "embedding": null, "metadata": {"filename": "llamaindex-update-07-10-2023-4ceebdab96cb.md", "extension": ".md", "title": "LlamaIndex Update \u2014 07/11/2023", "date": "Jul 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-07-10-2023-4ceebdab96cb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c33628f-7781-4a91-912c-dc524ca918db", "node_type": "4", "metadata": {"filename": "llamaindex-update-07-10-2023-4ceebdab96cb.md", "extension": ".md", "title": "LlamaIndex Update \u2014 07/11/2023", "date": "Jul 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-07-10-2023-4ceebdab96cb"}, "hash": "2f489319d3393953bf80c831a53006aa96e01a721225ed90a565927b31c9a74b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4debc68-bd48-4223-ba34-e592fdbe37d0", "node_type": "1", "metadata": {"filename": "llamaindex-update-07-10-2023-4ceebdab96cb.md", "extension": ".md", "title": "LlamaIndex Update \u2014 07/11/2023", "date": "Jul 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-07-10-2023-4ceebdab96cb"}, "hash": "34bb701bb01c283e593650f90c95aef03279a8ce18ec1663e2e3528aa77d5fa5", "class_name": "RelatedNodeInfo"}}, "text": "[ Data Exchange Podcast ](https://www.youtube.com/watch?v=NAoqOJrE8rQ&list=PLnTmH22EvTFTtWJRPTNzosDIDblnSg0PD&t=1s) with Ben Lorica on LlamaIndex \n\n##  Events:\n\nRavi Theja gave talks on \u201cLlamaIndex: Basics To Production\u201d at Accel Partners\nand Together VC Fund in India.\n\n", "mimetype": "text/plain", "start_char_idx": 10286, "end_char_idx": 10557, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ec93946-a525-494a-80e9-75a247fb06ef": {"__data__": {"id_": "4ec93946-a525-494a-80e9-75a247fb06ef", "embedding": null, "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dd317f66-60d5-495b-9596-574f69d4beba", "node_type": "4", "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "0f7a5685c5883fbac0e3d3d6f269286645e361b8584df00b881cee2e53504eed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a74c9239-ce7f-4a43-a912-0988773b58f1", "node_type": "1", "metadata": {}, "hash": "cdf3694cb3ed512fc9881927246143a910f6e128dceab6345aa1b3d12549e4b8", "class_name": "RelatedNodeInfo"}}, "text": "A few months ago, we launched LlamaIndex 0.6.0, which included a massive\nrewrite of our codebase to make our library more modular, customizable, and\naccessible to both beginner and advanced users:\n\n  * We created modular storage abstractions (data, indices), and compute abstractions (retrievers, query engines). \n  * We created a lower-level API where users could use our modules (retrievers, query engines) independently and customize it as part of a larger system. \n\nToday, we\u2019re excited to launch LlamaIndex 0.7.0. Our latest release continues\nthe theme of improving modularity/customizability at the lower level to enable\n**bottoms-up development of LLM applications over your data.** You now have\neven more control over using key abstractions: the LLM, our response\nsynthesizer, and our Document and Node objects.\n\n  * We\u2019ve created **standalone LLM abstractions** (OpenAI, HuggingFace, PaLM). \n  * We\u2019ve made our **response synthesis module an independent module** you can use completely independently of the rest of our abstractions \u2014 get rid of the prompt boilerplate of trying to figure out how to fit context within a context window. \n  * We\u2019ve added **extensive metadata management capabilities** to our Document/Node objects \u2014 now you have complete control over context you decide to inject into your documents. \n\nBelow, we describe each section more in detail. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1375, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a74c9239-ce7f-4a43-a912-0988773b58f1": {"__data__": {"id_": "a74c9239-ce7f-4a43-a912-0988773b58f1", "embedding": null, "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dd317f66-60d5-495b-9596-574f69d4beba", "node_type": "4", "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "0f7a5685c5883fbac0e3d3d6f269286645e361b8584df00b881cee2e53504eed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ec93946-a525-494a-80e9-75a247fb06ef", "node_type": "1", "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "5e7299d890672bc552fdc479b752c404a20f35e2d06ff199b708c7d92939e835", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ec8044c-1ffe-429e-a8f4-8c04d465d017", "node_type": "1", "metadata": {}, "hash": "b30333b39ca57b347409ccc3f3118297c473b3abe00915d1f1d25f5ea0b1f36b", "class_name": "RelatedNodeInfo"}}, "text": "We also outline a full list of\nbreaking changes at the bottom.\n\n#  Standalone LLM Abstractions\n\nWe\u2019ve created standalone LLM abstractions for OpenAI, HuggingFace, and PaLM.\nThese abstractions can be used on their own, or as part of an existing\nLlamaIndex system (query engines, retrievers).\n\n##  High-level Motivation\n\nWe did this for multiple reasons:\n\n  * Cleaner abstractions in the codebase. Before, our ` LLMPredictor ` class had a ton of leaky abstractions with the underlying LangChain LLM class. This made our LLM abstractions hard to reason about, and hard to customize. \n  * Slightly cleaner dev UX. Before, if you wanted to customize the default LLM (for instance, use \u201ctext-davinci-003\u201d, you had to import the correct LangChain class, wrap it in our LLMPredictor, and then pass it to ServiceContext. Now it\u2019s easy to just import our LLM abstraction (which is natively documented with our docs) and plug it into ServiceContext. Of course, you can still use LangChain\u2019s LLMs if you wish. \n  * Conducive to bottoms-up development: it makes sense to play around with these LLM modules independently before plugging them in as part of a larger system. It\u2019s reflective of our bigger push in 0.7.0 to let users compose their own workflows. \n\n##  **Using on their own**\n\nOur LLM abstractions support both ` complete ` and ` chat ` endpoints. The\nmain difference is that ` complete ` is designed to take in a simple string\ninput, and output a ` CompletionResponse ` (containing text output +\nadditional fields). ` chat ` takes in a ` ChatMessage ` and outputs a `\nChatResponse ` (containing a chat message + additional fields).\n\nThese LLM endpoints also natively support streaming via ` stream_complete `\nand ` stream_chat ` .\n\nHere\u2019s on how you can use the LLM abstractions on their own:\n\n    \n    \n    from llama_index.llms import OpenAI\n    \n    # using complete endpoint\n    resp = OpenAI().complete('Paul Graham is ')\n    print(resp)\n    # get raw object\n    resp_raw = resp.raw\n    # using chat endpoint\n    from llama_index.llms import ChatMessage, OpenAI\n    messages = [\n        ChatMessage(role='system', content='You are a pirate with a colorful personality'),\n        ChatMessage(role='user', content='What is your name')\n    ]\n    resp = OpenAI().chat(messages)\n    print(resp)\n    # get raw object\n    resp_raw = resp.raw\n    # using streaming endpoint\n    from llama_index.llms import OpenAI\n    llm = OpenAI()\n    resp = llm.stream_complete('Paul Graham is ')\n    for delta in resp:\n        print(delta, end='')\n\nHere\u2019s how you can use the LLM abstractions as part of an overall LlamaIndex\nsystem.\n\n    \n    \n    from llama_index.llms import OpenAI\n    from llama_index.indices.service_context import ServiceContext\n    from llama_index import VectorStoreIndex\n    \n    llm = OpenAI(model='gpt-3.5-turbo', temperature=0)\n    service_context = ServiceContext.from_defaults(llm=llm)\n    index = VectorStoreIndex.from_documents(docs, service_context=service_context)\n    response = index.as_query_engine().query(\"&lt;question&gt;\")\n\nNote: Our top-level ` LLMPredictor ` still exists but is less user-facing (and\nwe might deprecate in the future). Also, you can still use LangChain LLMs\nthrough our ` LangChainLLM ` class.\n\n##  **Resources**\n\nAll of our notebooks have by default been updated to use our native OpenAI LLM\nintegration. Here\u2019s some resources to show both the LLM abstraction on its own\nas well as how it can be used in the overall system:\n\n  * [ OpenAI LLM ](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/llm/openai.ipynb)\n  * [ Using LLM in LLMPredictor ](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/llm/llm_predictor.ipynb)\n  * [ Changing LLM within Index/Query Engine ](https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html#example-changing-the-underlying-llm)\n  * [ Defining a custom LLM Model ](https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html#example-using-a-custom-llm-model-advanced)\n\n#  Standalone Response Synthesis Modules\n\n##  **Context**\n\nIn any RAG system, there is retrieval and there is synthesis. The\nresponsibility of the synthesis component is to take in incoming context as\ninput, and synthesize a response using the LLM.\n\nFundamentally, the synthesis module needs to synthesize a response over\n**any** context list, regardless of how long that context list is. This is\nessentially \u201cboilerplate\u201d that an LLM developer / [ \u201cAI engineer\u201d\n](https://www.latent.space/p/ai-engineer) must write.\n\nWe had this as an internal abstraction in LlamaIndex before (as a `\nResponseSynthesizer ` ), but the external-facing UX was unfriendly to users.\nThe actual piece that gathered responses (the ` ResponseBuilder ` ) was hard\nto customize, and the ` ResponseSynthesizer ` itself was adding an extra\nunnecessary layer.\n\nNow we have a set of standalone modules that you can easily import.\nPreviously, when you set the ` response_mode ` in the query engine, these were\nbeing setup for you. Now they are more directly available and user-facing.\n\nHere\u2019s a list of all the new ` Response Synthesiszer ` modules available from\n` llama_index.response_synthesizer ` :\n\n  * ` Refine ` \\- Query an LLM, sending each text chunk individually. After the first LLM call, the existing answer is also sent to the LLM for updating and refinement using the next text chunk. \n  * ` Accumulate ` \\- Query an LLM with the same prompt across multiple text chunks, and return a formatted list of responses \n  * ` Compact ` \\- The same as ` Refine ` , but puts as much text as possible into each LLM call \n  * ` CompactAndAccumulate ` \\- The same as ` Accumulate ` , but puts as much text as possible \n  * ` TreeSummarize ` \\- Create a bottom-up summary from the provided text chunks, and return the root summary \n  * ` SimpleSummarize ` \\- Combine and truncate all text chunks, and summarize in a single LLM call \n\n##  **Usage**\n\nAs detailed above, you can directly set a response synthesizer in a query\nengine, or let the ` response_mode ` fetch the relevant response synthesizer.\n\nFurthermore though, you can directly call and use these synthesizers as low\nlevel modules. Here\u2019s a small example:\n\n    \n    \n    from llama_index import ServiceContext\n    from llama_index.response_synthesizers import CompactAndRefine\n    \n    # you can also configure the text_qa_template, refine_template, \n    # and streaming toggle from here\n    response_synthesizer = CompactAndRefine(\n      service_context=service_context.from_defaults()\n    )\n    response = response_synthesizer.get_response(\n     \"What skills does Bob have?\",\n      text_chunks=[\" ...\"]  # here would be text, hopefully about Bob's skills\n    )\n\n##  Resources\n\nHere are some additional notebooks showing how to use `\nget_response_synthesizer ` :\n\n  * [ Low-level API Usage Pattern ](https://gpt-index.readthedocs.io/en/latest/guides/primer/usage_pattern.html#low-level-api)\n  * [ Custom Retrievers ](https://gpt-index.readthedocs.io/en/latest/examples/query_engine/CustomRetrievers.html#plugin-retriever-into-query-engine)\n\n#  Metadata Management Capabilities\n\nIf you want to have good performance in any LLM application over your data\n(including a RAG pipeline), you need to make sure that your documents actually\ncontain relevant context for the query. One way to do this is to add proper\nmetadata, both at the document-level and after the documents have been parsed\ninto text chunks (into Nodes).\n\nWe allow you to define metadata fields within a Document, customize the ID,\nand also customize the metadata text/format for LLM usage and embedding usage.\n\n**Defining Metadata Fields**\n\n    \n    \n    document = Document(\n        text='text', \n        metadata={\n            'filename': '&lt;doc_file_name&gt;', \n            'category': '&lt;category&gt;'\n        }\n    )\n\n**Customizing the ID**\n\nThe ID of each document can be set multiple ways\n\n  * Within the constructor: ` document = Document(text=\"text\", doc_id_=\"id\") `\n  * After constructing the object: ` document.doc_id = \"id\" `\n  * Automatically using the ` SimpleDirectoryReader ` : ` SimpleDirectoryReader(filename_as_id=True).load_data() `\n\n**Customizing the Metadata Text for LLMs and Embeddings**\n\nAs seen above, you can set metadata containing useful information. By default,\nall the metadata will be seen by the embedding model and the LLM. However,\nsometimes you may want to only include data to bias embeddings, or only\ninclude data as extra information for the LLM!\n\nWith the new ` Document ` objects, you can configure what each metadata field\nis used for:\n\n    \n    \n    document = Document(\n        text='text', \n        metadata={\n            'filename': '&lt;doc_file_name&gt;', \n            'category': '&lt;category&gt;'\n        },\n        excluded_llm_metadata_keys=['filename', 'category'],\n        excluded_embed_metadata_keys=['filename']\n    )\n\n**Customizing the Metadata Format Template**\n\nWhen the metadata is inserted into the text, it follows a very specific\nformat. This format is configurable at multiple levels:\n\n    \n    \n    from llama_index.schema import MetadataMode\n    \n    document = Document(\n      text='text',\n      metadata={\"key\": \"val\"},\n      metadata_seperator=\"::\",\n        metadata_template=\"{key}=&gt;{value}\",\n        text_template=\"Metadata: {metadata_str}\\\\n-----\\\\nContent: {content}\"\n    )\n    # available modes are ALL, NONE, LLM, and EMBED\n    print(document.get_content(metadata_mode=MetadataMode.ALL))\n    # output:\n    # Metadata: key=&gt;val\n    # -----\n    # text\n\nPlease check out this guide for more [ details ](https://gpt-\nindex.readthedocs.io/en/latest/how_to/customization/custom_documents.html) !\n\n#  Full List of Breaking Changes\n\n##  Response Synthesis + Node Postprocessors\n\nThe ` ResponseSynthesizer ` object class has been removed, and replaced with `\nget_response_synthesizer ` . ", "mimetype": "text/plain", "start_char_idx": 1375, "end_char_idx": 11259, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ec8044c-1ffe-429e-a8f4-8c04d465d017": {"__data__": {"id_": "8ec8044c-1ffe-429e-a8f4-8c04d465d017", "embedding": null, "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dd317f66-60d5-495b-9596-574f69d4beba", "node_type": "4", "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "0f7a5685c5883fbac0e3d3d6f269286645e361b8584df00b881cee2e53504eed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a74c9239-ce7f-4a43-a912-0988773b58f1", "node_type": "1", "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "eeb35b14c0e5c171628ee6702c71e22459c041195bc0065833d2a59969599623", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c16d9f7e-93c8-41a5-9579-d21f19e20176", "node_type": "1", "metadata": {}, "hash": "8213cf21ea9e23a77cf72967b23ad2988c9df5d98e499614f9d186cf882b44b3", "class_name": "RelatedNodeInfo"}}, "text": "In addition to this, node post processors are now\nhandled by the query engine directly, and the old ` SentenceEmbeddingOptimizer\n` has been switched to become a node post processor instance itself.\n\nHere is an example of the required migration to use all moved features.\n\n**Old**\n\n    \n    \n    from llama_index import (\n        VectorStoreIndex,\n        ResponseSynthesizer,\n    )\n    from llama_index.indices.postprocessor import SimilarityPostprocessor\n    from llama_index.optimizers import SentenceEmbeddingOptimizer\n    from llama_index.query_engine import RetrieverQueryEngine\n    \n    documents = ...\n    # build index\n    index = VectorStoreIndex.from_documents(documents)\n    # configure retriever\n    retriever = index.as_retriever(\n       similarity_top_k=3\n    )\n    # configure response synthesizer\n    response_synthesizer = ResponseSynthesizer.from_args(\n       response_mode=\"tree_summarize\",\n        node_postprocessors=[\n            SimilarityPostprocessor(similarity_cutoff=0.7),\n            SentenceEmbeddingOptimizer(percentile_cutoff=0.5)\n        ]\n    )\n    # assemble query engine\n    query_engine = RetrieverQueryEngine(\n        retriever=retriever,\n        response_synthesizer=response_synthesizer,\n    )\n\n**New**\n\n    \n    \n    from llama_index import (\n        VectorStoreIndex,\n        get_response_synthesizer,\n    )\n    from llama_index.indices.postprocessor import (\n        SimilarityPostprocessor,\n        SentenceEmbeddingOptimizer\n    )\n    \n    documents = ...\n    # build index\n    index = VectorStoreIndex.from_documents(documents)\n    # configure response synthesizer\n    response_synthesizer = get_response_synthesizer(\n       response_mode=\"tree_summarize\",\n    )\n    # assemble query engine\n    query_engine = index.as_query_engine(\n      similarity_top_k=3,\n        response_synthesizer=response_synthesizer,\n        node_postprocessors=[\n            SimilarityPostprocessor(similarity_cutoff=0.7),\n            SentenceEmbeddingOptimizer(percentile_cutoff=0.5)\n        ]\n    )\n\n##  LLM Predictor\n\nWhile introducing a new LLM abstraction, we cleaned up the LLM Predictor and\nremoved several deprecated functionalities:\n\n  1. Remove ` ChatGPTLLMPredictor ` and ` HuggingFaceLLMPredictor ` (use ` OpenAI ` and ` HuggingFaceLLM ` instead, see [ migration guide ](https://gpt-index.readthedocs.io/en/latest/how_to/customization/llms_migration_guide.html) ) \n  2. ", "mimetype": "text/plain", "start_char_idx": 11259, "end_char_idx": 13663, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c16d9f7e-93c8-41a5-9579-d21f19e20176": {"__data__": {"id_": "c16d9f7e-93c8-41a5-9579-d21f19e20176", "embedding": null, "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dd317f66-60d5-495b-9596-574f69d4beba", "node_type": "4", "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "0f7a5685c5883fbac0e3d3d6f269286645e361b8584df00b881cee2e53504eed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ec8044c-1ffe-429e-a8f4-8c04d465d017", "node_type": "1", "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "82c92f3ed0ea89d1d612034f1862e31f84d7946c1ba73ea07bcee841a8ec9ecc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33fededd-4739-4e68-b7fb-f8032c872cae", "node_type": "1", "metadata": {}, "hash": "6870ec3795caddc36c73ab4b0b10891a6561b21fe99a9f8be23fa8a1c630655a", "class_name": "RelatedNodeInfo"}}, "text": "Remove support for setting ` cache ` via ` LLMPredictor ` constructor. \n  3. Removed ` llama_index.token_counter.token_counter ` module (see [ migration guide ](https://gpt-index.readthedocs.io/en/latest/how_to/callbacks/token_counting_migration.html) ). \n\nNow, the LLM Predictor class is mostly a lightweight wrapper on top of the `\nLLM ` abstraction that handles:\n\n  * conversion of prompts to the string or chat message input format expected by the LLM \n  * logging of prompts and responses to a callback manager \n\nWe advice users to configure the ` llm ` argument in ` ServiceContext `\ndirectly (instead of creating LLM Predictor).\n\n##  Chat Engine\n\nWe updated the ` BaseChatEngine ` interface to take in a ` List[ChatMessage]]\n` for the ` chat_history ` instead of tuple of strings. This makes the data\nmodel consistent with the input/output of the ` LLM ` , also more flexibility\nto specify consecutive messages with the same role.\n\n**Old**\n\n    \n    \n    engine = SimpleChatEngine.from_defaults(\n    \tchat_history=[(\"human message\", \"assistant message\")],\n    )\n    response = engine.chat(\"new human message\")\n\n**New**\n\n    \n    \n    engine = SimpleChatEngine.from_defaults(\n        service_context=mock_service_context,\n        chat_history=[\n            ChatMessage(role=MessageRole.USER, content=\"human message\"),\n            ChatMessage(role=MessageRole.ASSISTANT, content=\"assistant message\"),\n        ],\n    )\n    response = engine.chat(\"new human message\")\n\nWe also exposed ` chat_history ` state as a property and supported overriding\n` chat_history ` in ` chat ` and ` achat ` endpoints.\n\n##  Prompt Helper\n\nWe removed some previously deprecated arguments: ` max_input_size ` , `\nembedding_limit ` , ` max_chunk_overlap `\n\n#  Conclusion\n\nAt a high-level, we hope that these changes continue to enable bottoms-up\ndevelopment of LLM applications over your data. ", "mimetype": "text/plain", "start_char_idx": 13663, "end_char_idx": 15539, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33fededd-4739-4e68-b7fb-f8032c872cae": {"__data__": {"id_": "33fededd-4739-4e68-b7fb-f8032c872cae", "embedding": null, "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dd317f66-60d5-495b-9596-574f69d4beba", "node_type": "4", "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "0f7a5685c5883fbac0e3d3d6f269286645e361b8584df00b881cee2e53504eed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c16d9f7e-93c8-41a5-9579-d21f19e20176", "node_type": "1", "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}, "hash": "9f5c8c3e55916fef9d05cc2026bf0862a0a6f66432bce0328613150fda557894", "class_name": "RelatedNodeInfo"}}, "text": "We first encourage you to play\naround with our new modules on their own to get a sense what they do and where\nthey can be used. Once you\u2019re ready to use them in more advanced workflows,\nthen you can figure out how to use our outer components to setup a\nsophisticated RAG pipeline.\n\nAs always, our [ repo ](https://github.com/jerryjliu/llama_index) is here and\nour [ docs ](https://gpt-index.readthedocs.io/en/latest/) are here. If you\nhave thoughts/comments, don\u2019t hesitate to hop in our [ Discord\n](https://discord.gg/dGcwcsnxhU) !\n\n", "mimetype": "text/plain", "start_char_idx": 15539, "end_char_idx": 16073, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "811b5dee-0ded-41a2-a1f5-1d2dc7166ee2": {"__data__": {"id_": "811b5dee-0ded-41a2-a1f5-1d2dc7166ee2", "embedding": null, "metadata": {"filename": "special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.md", "extension": ".md", "title": "Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "22603646-039a-4d97-bb34-9b49c6a6ac9a", "node_type": "4", "metadata": {"filename": "special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.md", "extension": ".md", "title": "Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0"}, "hash": "b1bb3077b2f54883e66dd6099e810f4b1b33f5ec05595b13511be14bfbb4fd12", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8af87fb7-5ed6-4dbc-9f95-5df3882c75a8", "node_type": "1", "metadata": {}, "hash": "917496f8c2b08f66152f11bd5d097559069627fa029f36b2c17793b167befb7f", "class_name": "RelatedNodeInfo"}}, "text": "We had an awesome time at the Berkeley Hackathon two weeks ago (6/17\u20136/18).\nThe attendance stats were impressive:\n\n  * 1200 hackers \n  * 262 submitted projects \n  * 2 real-life llamas \n\nLlamaIndex sponsored a \u201cBest Knowledge-Intensive LLM App\u201d prize series at the\nhackathon. The criteria was an app that leveraged a knowledge base of custom\ndata to build innovative new application experiences.\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 396, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8af87fb7-5ed6-4dbc-9f95-5df3882c75a8": {"__data__": {"id_": "8af87fb7-5ed6-4dbc-9f95-5df3882c75a8", "embedding": null, "metadata": {"filename": "special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.md", "extension": ".md", "title": "Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "22603646-039a-4d97-bb34-9b49c6a6ac9a", "node_type": "4", "metadata": {"filename": "special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.md", "extension": ".md", "title": "Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0"}, "hash": "b1bb3077b2f54883e66dd6099e810f4b1b33f5ec05595b13511be14bfbb4fd12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "811b5dee-0ded-41a2-a1f5-1d2dc7166ee2", "node_type": "1", "metadata": {"filename": "special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.md", "extension": ".md", "title": "Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0"}, "hash": "8988392d2d20a8ba81e002e28a32c92b3533593b11759862171f3030553c5416", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c56c18bd-c834-450a-be22-b9b21880a430", "node_type": "1", "metadata": {}, "hash": "6e3ca325dbcb111839eab0d41596506d816ae31f2b94b2702dde28c7fed8ffa7", "class_name": "RelatedNodeInfo"}}, "text": "We announced three prize winners along with an honorable mention. ", "mimetype": "text/plain", "start_char_idx": 396, "end_char_idx": 462, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c56c18bd-c834-450a-be22-b9b21880a430": {"__data__": {"id_": "c56c18bd-c834-450a-be22-b9b21880a430", "embedding": null, "metadata": {"filename": "special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.md", "extension": ".md", "title": "Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "22603646-039a-4d97-bb34-9b49c6a6ac9a", "node_type": "4", "metadata": {"filename": "special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.md", "extension": ".md", "title": "Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0"}, "hash": "b1bb3077b2f54883e66dd6099e810f4b1b33f5ec05595b13511be14bfbb4fd12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8af87fb7-5ed6-4dbc-9f95-5df3882c75a8", "node_type": "1", "metadata": {"filename": "special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.md", "extension": ".md", "title": "Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0"}, "hash": "e4d38054a21fa7f4f228ae67189e66c50e07441c5fef4a7768989ae31e610239", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b816843-31fc-4f97-8972-663c90ffac2f", "node_type": "1", "metadata": {}, "hash": "c8975f73c62509e5f951384a346cd6dea55f1ca8d6d2aef4ebdbf1a59e32a4db", "class_name": "RelatedNodeInfo"}}, "text": "We are\nexcited to feature each project in a special highlight below. In each\nhighlight, the creators describe the project mission and what it solves, the\nimplementation+tech stack, challenges, and future directions. ", "mimetype": "text/plain", "start_char_idx": 462, "end_char_idx": 678, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b816843-31fc-4f97-8972-663c90ffac2f": {"__data__": {"id_": "2b816843-31fc-4f97-8972-663c90ffac2f", "embedding": null, "metadata": {"filename": "special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.md", "extension": ".md", "title": "Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "22603646-039a-4d97-bb34-9b49c6a6ac9a", "node_type": "4", "metadata": {"filename": "special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.md", "extension": ".md", "title": "Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0"}, "hash": "b1bb3077b2f54883e66dd6099e810f4b1b33f5ec05595b13511be14bfbb4fd12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c56c18bd-c834-450a-be22-b9b21880a430", "node_type": "1", "metadata": {"filename": "special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.md", "extension": ".md", "title": "Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0"}, "hash": "474085d34781c879c4e3cb32dccdcf5d7963fac74e4e2142af05eb374cde201a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0703fb6b-99bc-4bda-ac83-fb286ce88a12", "node_type": "1", "metadata": {}, "hash": "53d543a2447f5d890d162bc1cb179cf7974df3409a94be1cf7643caca3afaa54", "class_name": "RelatedNodeInfo"}}, "text": "Check it out!\n\n#  First Prize Winner: Helmet AI\n\nCreators: Jaiveer Singh, Devin Mui, Ethan Mehta, Manav Rathod\n\nDevpost: [ https://devpost.com/software/helmet-ai\n](https://devpost.com/software/helmet-ai)\n\n##  Introduction\n\nIn today\u2019s rapidly evolving business landscape, staying ahead of the\ncompetition is paramount for success. However, the deluge of information and\nthe ever-changing market dynamics can make it challenging for business leaders\nto make informed decisions. In this blog post, we introduce Helmet AI, a\ncutting-edge market intelligence tool designed to empower leadership teams\nwith real-time insights and a competitive edge. Join us as we explore the\ncapabilities, technology stack, and future prospects of Helmet AI.\n\n##  Unveiling Helmet AI\n\nHelmet AI is an innovative market intelligence tool that harnesses the power\nof advanced technologies to provide leaders with actionable insights and an\nunparalleled understanding of the global business landscape. With its context-\naware Ingestion Engine and Insight Extractor powered by OpenAI\u2019s GPT models,\nHelmet AI offers a comprehensive solution for tracking breaking news,\nuncovering hidden relationships, and extracting valuable, personalized\ninsights from vast amounts of data. For ease of use, Helmet AI displays these\ninsights in a familiar, Twitter-like \u201cFeed\u201d interface. Additionally, Helmet AI\noffers a Chat interface for users to ask questions about a particular news\nstory to Helmet\u2019s knowledgeable chat agent.\n\n##  Key Features and Technology Stack\n\n**Context-Aware Ingestion Engine:**\n\n  * Helmet AI\u2019s Ingestion Engine continuously monitors the vast landscape of breaking news and global events. By leveraging techniques such as subscribing to RSS feeds for up to date news data and processing documents with LlamaIndex and LangChain, the engine builds a complete understanding of real-time events and their implications on various user profiles. Embeddings are stored in a Pinecone Vector Database. \n\n**Insight Extractor with OpenAI\u2019s GPT Models:**\n\n  * The Insight Extractor component of Helmet AI utilizes the power of OpenAI\u2019s GPT models to identify and concisely explain the intricate relationships between seemingly disparate topics surfaced in your feed. By transforming raw data into actionable insights with intelligent explanations, leaders can make informed decisions based on an understanding of market trends and complex dynamics. \n\n**Scalable Infrastructure:**\n\n  * Helmet AI is built on Azure\u2019s robust infrastructure, utilizing a range of services such as App Services, a PostgreSQL Database, and Github Actions for orchestrating Deployments. The implementation also incorporates GraphQL for efficient data retrieval and processing. \n\n##  Challenges Overcome and Accomplishments\n\nDuring the development of Helmet AI, our team encountered various challenges,\nincluding integrating MindsDB with Azure and overcoming limitations with Gmail\nauthentication. However, we were able to overcome these obstacles and\nsuccessfully implemented Helmet AI in just 36 hours during the Berkeley AI\nHackathon. Additionally, we established a seamless deployment process using\nGitHub Actions, automating manual service orchestration. The experience was\nparticularly rewarding for the first-time hackers on the team.\n\n##  Key Learnings\n\nThroughout the development process, our team gained valuable insights. We\ndiscovered the importance of setting up deployment flows early on to reduce\nstress during crunch time. Embracing best practices in software engineering\nproved crucial. Furthermore, we realized the potential of leveraging advanced\nlanguage models as implicit knowledge graphs, expanding their applications\nbeyond traditional embeddings.\n\n##  Future Prospects\n\nLooking ahead, Helmet AI aims to scale up the Ingestion Engine to handle the\nentirety of the web, leveraging technologies like AnyScale. The team plans to\ncollaborate with enterprise business development teams to initiate pilot\nprograms and gather feedback for further refinement. With a solid foundation\nin place, Helmet AI hopes to have an impact on the way leaders gather insights\nand make strategic decisions.\n\n##  Conclusion\n\nHelmet AI represents a solid attempt at a game-changing solution for business\nleaders seeking to stay ahead in today\u2019s fast-paced business world. By\nleveraging cutting-edge technologies, including AI-powered insight extraction\nand explanation and real-time data analysis, Helmet AI empowers leaders to\nconfidently navigate market challenges and seize emerging opportunities. As\nthe tool continues to evolve and expand its capabilities, the future of market\nintelligence looks promising. Stay tuned for more updates on Helmet AI\u2019s\njourney towards transforming the way we approach gathering information and\nstrategic decision-making.\n\n#  Winner: Split\n\nCreators: Aditya Ariyur, Nikhil Patel, Ronit Nagarapu\n\nDevpost: [ https://devpost.com/software/split-pv4hn7\n](https://devpost.com/software/split-pv4hn7)\n\n##  Background/Motivation\n\nWe wanted to develop an easy-to-use workflow that allowed users to generate\npersonalized emails with the assistance of AI, while retaining the user\u2019s\nunique writing style and emotion inflections.\n\n##  What It Is\n\nOur product learns from your previous emails and trains a custom LLM that will\ndraft emails that sound like you, not like a robot. ", "mimetype": "text/plain", "start_char_idx": 678, "end_char_idx": 6030, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0703fb6b-99bc-4bda-ac83-fb286ce88a12": {"__data__": {"id_": "0703fb6b-99bc-4bda-ac83-fb286ce88a12", "embedding": null, "metadata": {"filename": "special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.md", "extension": ".md", "title": "Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "22603646-039a-4d97-bb34-9b49c6a6ac9a", "node_type": "4", "metadata": {"filename": "special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.md", "extension": ".md", "title": "Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0"}, "hash": "b1bb3077b2f54883e66dd6099e810f4b1b33f5ec05595b13511be14bfbb4fd12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b816843-31fc-4f97-8972-663c90ffac2f", "node_type": "1", "metadata": {"filename": "special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.md", "extension": ".md", "title": "Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0"}, "hash": "3343d33dc890d71b9465654fb800128d165e5f9840b577b6a77c17c38c8fbbd5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3653c951-0fa2-4772-8383-5770180b0049", "node_type": "1", "metadata": {}, "hash": "51bc47d2b21b8ba5b36a21ed82b1b0abbe204f2a92528a3f6a85001873f87090", "class_name": "RelatedNodeInfo"}}, "text": "It learns from your\nwriting style and how you respond to specific people. Then, it generates\nemails from user prompts that match that style.\n\n##  How We Built It\n\nWe used the Google API and LlamaIndex to parse through a user\u2019s old emails and\ndevelop an LLM model built on OpenAI\u2019s text-davinci-003. Then, we use Hume to\nunderstand the user\u2019s tone and emotion in their emails, and associate it with\nspecific subjects and recipients so future emails can be fine-tuned to fit the\nuser\u2019s emailing habits. The current interface was developed using React.js for\nthe website and a Flask API to interact with the backend LLM model.\n\n##  Challenges + What We Learned\n\nIt was quite difficult to get all of the different aspects of our model\nworking together in unison, especially establishing the connection between the\nparsed emails and Hume emotion tags to the LlamaIndex model. We had to\nexperiment with many different tools and prompt styles to get an accurate\nemail generation. However, with a lot of dedication and troubleshooting, we\nwere able to develop a working model to demonstrate our concept and its\npotential functionality. We learned how rewarding it was to train our own LLM\nusing LlamaIndex. Base LLMs like ChatGPT are already so powerful, so the\nfunctionality of training a custom LLM based on your own data unlocks endless\npossibilities.\n\n##  What\u2019s Next\n\nWe hope to completely integrate the code and workflow into a Google plugin or\nextension so users can easily implement it into their daily emailing. We want\nto ensure the privacy and security of the user\u2019s data, so we want to\nexperiment with methods to reduce how much data is sent to third-party\nservices like OpenAI. We also want to dedicate further development to the\nemotion training, as this could boost the effectiveness of our product and add\nto our main value proposition of personalized, user-specific email generation.\n\n#  Winner: Prosper AI\n\nCreators: Alan Yang, Ashay Changwani, Punit Sai Arani, Vedant Tapadia\n\nDevpost: [ https://devpost.com/software/prosper-ai\n](https://devpost.com/software/prosper-ai)\n\n[ Vercel Demo ](https://prosperai.vercel.app/) / [ YouTube Video\n](https://www.youtube.com/watch?v=_-v0BhFPjAQ)\n\n##  Overview\n\nProsper AI is a trailblazer in utilizing Artificial Intelligence to unlock\nyour full financial potential. It serves as an accessible and smart virtual\nfinancial advisor, armed with precise insights and personalized advice. Our\nmission is to democratize financial expertise. By bridging the resource gap,\nProsper AI aims to level the playing field for all.\n\n##  The Genesis of Prosper AI\n\nThe spark that ignited Prosper AI was a simple observation of the wealth\ndisparity among different social classes. The rich have always had access to\nknowledge and resources that help in growing and safeguarding their wealth. ", "mimetype": "text/plain", "start_char_idx": 6030, "end_char_idx": 8854, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3653c951-0fa2-4772-8383-5770180b0049": {"__data__": {"id_": "3653c951-0fa2-4772-8383-5770180b0049", "embedding": null, "metadata": {"filename": "special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.md", "extension": ".md", "title": "Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "22603646-039a-4d97-bb34-9b49c6a6ac9a", "node_type": "4", "metadata": {"filename": "special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.md", "extension": ".md", "title": "Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0"}, "hash": "b1bb3077b2f54883e66dd6099e810f4b1b33f5ec05595b13511be14bfbb4fd12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0703fb6b-99bc-4bda-ac83-fb286ce88a12", "node_type": "1", "metadata": {"filename": "special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.md", "extension": ".md", "title": "Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0"}, "hash": "fc4f0dd0a25a985bf2d15310c7d53a99b3968419d79d03341861257000a38f3e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c887066-60a0-4e68-b36c-b9938cdbe8d6", "node_type": "1", "metadata": {}, "hash": "b7b94acd494a559fce856e72a3da07125b9aa10b5dadccef711514c0a57748a2", "class_name": "RelatedNodeInfo"}}, "text": "In\ncontrast, those from modest backgrounds often lack the necessary knowledge and\ntools to utilize what they earn effectively. Many resort to social media for\nfinancial advice, which is often generic and occasionally unreliable as it\ncomes from unqualified influencers. Hiring a financial advisor, on the other\nhand, could be exorbitant and impractical for those with a limited budget.\n\nThis is where Prosper AI steps in. We embraced the challenge to develop an\ninnovative solution utilizing state-of-the-art technology and models to help\ndigest and simplify complex financial data.\n\n##  The Prosperity Engine: How Prosper AI Works\n\nProsper AI sources your financial data from any number and type of bank or\ninvestment account to provide qualified financial advice that adheres to\nregulatory policies, to give you personalized tips, advice and explanations.\n\nProsper AI achieves this by leveraging an open finance provider such as Plaid\nto source users financial information. Then Prosper AI will ask a series of\nfinancial goal questions to help contextualize the ideal outcomes for the\nuser. Using this combination of personal financial data and goals, Prosper AI\nwill provide a set of optimal and personalized recommendations on how to\nachieve these goals.\n\nThe beauty of Prosper AI lies in its interactivity and support. Users have the\nliberty to pose questions at any juncture if they find something perplexing.\nThis is particularly invaluable for demystifying complicated charts or\nfinancial jargon. Furthermore, Prosper AI goes beyond just answering questions\nabout the current recommendations. It\u2019s like having an expert financial\nadvisor at your beck and call, ready to generate insights, charts, and\nsuggestions for any aspect of your financial landscape. Whether it\u2019s planning\nfor retirement, optimizing investments, or understanding tax liabilities,\nProsper AI stands ready to guide users with precision and personalized\ninsights to cultivate financial acumen and empower smarter financial decision-\nmaking.\n\n##  The Building Blocks of Prosper AI: A Look into Our Tech Stack\n\n**Backend:** Our backend, the engine that powers Prosper AI, is written in\nPython and based on a FastAPI server. We chose Python because of its agility\nand the vast availability of open-source libraries that expedite the\ndevelopment process. Additionally, Python\u2019s native packages provided by OpenAI\nand Plaid seamlessly integrate with our backend, ensuring both development and\nruntime efficiency.\n\nOne of the cornerstones of Prosper AI\u2019s backend is a powerful prompting\npipeline which simulates a fine-tuned model. To achieve this, we tap into the\ncapabilities of OpenAI\u2019s GPT-4, enhanced with function calling, and interlink\nit with Pinecone\u2019s vector database using additional tools like LlamaIndex.\nThis fusion forges a streamlined yet powerful interface.\n\n**Frontend:** When we started out, especially during the hackathon phase, we\ndeveloped the web application frontend using Next.js, which was our comfort\nzone. However, as we progressed and aimed for higher benchmarks, we recognized\nthe need to migrate to a more performant framework. We decided on SvelteKit,\nwhich stands out for its simplicity and performance, significantly\naccelerating the development process.\n\nOne of our key objectives is to make Prosper AI accessible and user-friendly.\nWe crafted a minimalist user interface, which declutters the screen while\nmaintaining the essence of information. Moreover, we supplemented this with\nvisualizations, which are crucial in translating complex financial data into\nunderstandable and actionable insights for the user. Through this combination\nof a robust backend and an intuitive frontend, Prosper AI is poised to\nrevolutionize personal financial management.\n\n##  Overcoming Challenges: The Journey of Prosper AI\u2019s Development\n\nThe primary challenge we encountered during the initial stages was the\ncreation of a pipeline to ingest and process years of financial data\nanalytically. The sheer volume of data was not just overwhelming to handle all\nat once, but it was also crucial to process it responsibly and meaningfully.\n\nTo tackle this, we had to design a system that dissected the vast financial\ndata into digestible segments, structuring it in an orderly manner that\nenabled logical understanding and actionable insights. Although crafting such\na system under time pressure was strenuous, it offered us a valuable insight\ninto the magnitude of data we were dealing with. It further emphasized the\nsignificance of our mission: to efficiently and comprehensively process such\nvast data for the benefit of our users.\n\nAnother demanding task was incorporating the complexities of tax code into our\nplatform. Thousands of pages of tax regulations had to be converted into\nintelligent code, capable of offering savvy financial suggestions. Despite the\nenormous effort this task required, it was crucial in creating a comprehensive\nwealth management system. The result is a platform that delivers an optimized,\npersonalized financial plan tailored to each user\u2019s specific goals and needs,\nas well as future plans. Our platform not only identifies the type of accounts\nand the cash flow strategies that would minimize tax liabilities but also\ncharts a roadmap for maximizing net worth growth over the next 30 years.\n\nThis is the essence of Prosper AI \u2014 using technology to simplify complex\nfinancial management and facilitate the path towards prosperity.\n\n##  The Road Ahead for Prosper AI\n\nAs we set our sights on the future, the Prosper AI team is more determined\nthan ever to make strides in revolutionizing personal wealth management. Our\nimmediate focus is to transition into full-time startup mode, which entails\ndelving deeper into the development of feature functionalities and solidifying\nthe foundation of our platform.\n\nA key milestone on our roadmap is engaging in pilot use cases with our initial\ngroup of customers who have eagerly joined our waitlist. This phase is\ncritical, as it allows us to validate the effectiveness and impact of Prosper\nAI in real-world scenarios. Through feedback and insights gathered from this\ninitial group, we\u2019ll be able to refine and enhance the platform to ensure it\nnot only meets but surpasses the expectations of our users.\n\nBut we won\u2019t stop there. ", "mimetype": "text/plain", "start_char_idx": 8854, "end_char_idx": 15154, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c887066-60a0-4e68-b36c-b9938cdbe8d6": {"__data__": {"id_": "7c887066-60a0-4e68-b36c-b9938cdbe8d6", "embedding": null, "metadata": {"filename": "special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.md", "extension": ".md", "title": "Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "22603646-039a-4d97-bb34-9b49c6a6ac9a", "node_type": "4", "metadata": {"filename": "special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.md", "extension": ".md", "title": "Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0"}, "hash": "b1bb3077b2f54883e66dd6099e810f4b1b33f5ec05595b13511be14bfbb4fd12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3653c951-0fa2-4772-8383-5770180b0049", "node_type": "1", "metadata": {"filename": "special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.md", "extension": ".md", "title": "Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0"}, "hash": "ff4f25182eed9c2079e6e0598d038c42fe46e1593f40774645ba2c2ae466fbd5", "class_name": "RelatedNodeInfo"}}, "text": "The learnings from the pilot phase will serve as the\nspringboard for subsequent developments and innovations. As we continue to\nharness cutting-edge technology and data analytics, Prosper AI aims to\ndemocratize access to financial knowledge and tools that can empower\nindividuals to unlock their financial potential.\n\nStay tuned as Prosper AI embarks on this exciting journey towards transforming\nthe landscape of personal finance, making it more accessible, intelligent, and\npersonalized for all.\n\nTogether with Prosper AI, let\u2019s cultivate the seeds of financial growth and\nharvest the fruits of prosperity.\n\n##  Video/screenshots/links to material.\n\nLearn more and join our waitlist for a chance to win a $50 Amazon voucher:\n\n## [ Unleash The Power of Comparison  Tap into Prosper AI, your intelligent\nsidekick for personalized & optimal financial advice.  prosperai.vercel.app\n](https://prosperai.vercel.app/?source=post_page-----\nc135681bb6f0--------------------------------)\n\n[ https://www.youtube.com/watch?v=_-v0BhFPjAQ\n](https://www.youtube.com/watch?v=_-v0BhFPjAQ)\n\n", "mimetype": "text/plain", "start_char_idx": 15154, "end_char_idx": 16229, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0958479-b601-438a-8156-1cb8f8ffde6c": {"__data__": {"id_": "c0958479-b601-438a-8156-1cb8f8ffde6c", "embedding": null, "metadata": {"filename": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.md", "extension": ".md", "title": "Enriching LlamaIndex Models with GraphQL and Graph Databases", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e31106da-d110-425b-8f6e-2b8cf866f208", "node_type": "4", "metadata": {"filename": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.md", "extension": ".md", "title": "Enriching LlamaIndex Models with GraphQL and Graph Databases", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7"}, "hash": "5b80622fb46fa8f20489b384fdf1c5af28c461beca096f5e7574d8e65c2346a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b51f70fc-9e97-4c52-bfab-9c9243f19b3e", "node_type": "1", "metadata": {}, "hash": "13737fe60454db6f38831c1fab68eacc84f6f4c209a5121ac6d0ebae3161fd5e", "class_name": "RelatedNodeInfo"}}, "text": "In this article I wanted to share the process of adding new data loaders to\nLlamaIndex. First we\u2019ll look at what LlamaIndex is and try a simple example of\nproviding additional context to an LLM query using a simple CSV loader. Then\nwe look at how easy it is to add a new loader for graph databases to\nLlamaIndex. And lastly we try that new loader and another loader for GraphQL\nAPIs that I added in practice and see how their extra context can help an LLM\nanswer questions better.\n\n#  Background/Context\n\nI was listening to the [ \"This Week in ML\" (twiml) Podcast\n](https://medium.com/llamaindex-blog/llamaindex-on-twiml-ai-a-distilled-\nsummary-using-llamaindex-de2a88551595) where [ Jerry Liu\n](https://medium.com/u/e76da1c45ef7?source=post_page-----\nbcaecec262d7--------------------------------) from LlamaIndex (previously GPT-\nIndex) explained the ideas behind the library to enrich query contexts to LLMs\nwith data from any number of sources.\n\n[ LlamaIndex ](https://gpt-index.readthedocs.io/en/latest/index.html) is a\ntoolkit to augment LLMs with your own (private) data using in-context\nlearning. It takes care of selecting the right context to retrieve from large\nknowledge bases. To achieve that it utilizes a number of connectors or loaders\n(from [ LlamaHub ](https://llamahub.ai/) ) and data structures (indices) to\nefficiently provide the pre-processed data as ` Documents ` .\n\nEach type of index stores documents in different ways, e.g via embeddings for\nvector search, as a simple list or graph or tree structure. Those indices are\nused as query interface to the LLM, transparently embedding the relevant\ncontext.\n\nBesides the higher quality response from the LLM, you get also the documents\nreturned that have been used to construct the answer. LlamaIndex also allows\nchain of thought reasoning, compare/contrast queries, and natural language\nquerying of databases.\n\nSee also this presentation from Jerry:\n\nAll the code for the blog post is available in this [ Colab Notebook\n](https://colab.research.google.com/drive/1NUrIoiOh692LaQkBHEmnD-5IuLBpBqGJ#scrollTo=JN4gqQF-\nNRwj) .\n\n#  Using a Basic CSV Loader\n\nHere is an example of using a basic CSV loader to provide documents for\nLlamaIndex.\n\nIn our Notebook we download the ` countries.csv ` via the [ Countries List\nProject ](https://annexare.github.io/Countries/) (MIT) ( [ raw source\n](https://raw.githubusercontent.com/annexare/Countries/master/dist/countries.csv)\n).\n\nOur dependencies are ` llama-index ` and ` python-dotenv ` .\n\n    \n    \n    !pip install llama-index==0.6.19 python-dotenv\n\nWe need to provide our OpenAI-api key, to avoid accidentally leaking it in the\nnotebook, I uploaded an ` openai.env ` file and use the ` dotenv ` library to\nload the contents as environment variables.\n\nIn the next step we load the env file and prepare the OpenAI `\nChatGPTLLMPredictor ` (using ` gpt-3.5-turbo ` by default) and add it to the `\nServiceContext ` .\n\n    \n    \n    import os\n    from pathlib import Path\n    from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, ServiceContext, GPTListIndex\n    from llama_index.llm_predictor.chatgpt import ChatGPTLLMPredictor\n    from dotenv import load_dotenv\n    from llama_index import download_loader\n    \n    load_dotenv(\"openai.env\")\n    \n    llm_predictor = ChatGPTLLMPredictor()\n    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\nNow we can use the loader to load the CSV and turn it into documents, create\nan an GPT Index ( ` VectorStoreIndex ` in this case), which LlamaIndex can\nthen use to retrieve the relevant information to pass along in the context to\nthe LLM.\n\nInitializing CSV Loader and GPTVectorStoreIndex\n\n    \n    \n    SimpleCSVReader = download_loader(\"SimpleCSVReader\")\n    loader = SimpleCSVReader(concat_rows=False)\n    documents = loader.load_data(file=Path('./countries.csv'))\n    \n    print(documents)\n    index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n\nDocuments from the CSV Loader\n\n    \n    \n    [Document(text='country, capital, type', doc_id='67c30c68-7d9f-4906-945b-9affc96f95d2', embedding=None, doc_hash='3a506ebea9c04655b51406d79fdf5e3a87c3d8ff5b5387aace3e5a79711a21b8', extra_info=None),\n    Document(text='Abkhazia, Sukhumi, countryCapital', doc_id='6e6be4b5-051f-48e0-8774-6d48e0444785', embedding=None, doc_hash='ea387d0eab94cc6c59f98c473ac1f0ee64093901673b43e1c0d163bbc203026e', extra_info=None),\n    ...]\n\nThe CSV loader didn\u2019t create one Document per CSV row by default, but only one\nfor the whole document, but you could configure it so that it turned the CSV\ninto one document per row.\n\nLlamaIndex supports much more involved setups of different kinds of indexes,\nallows to chain them and even conditionally select one or the other. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4775, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b51f70fc-9e97-4c52-bfab-9c9243f19b3e": {"__data__": {"id_": "b51f70fc-9e97-4c52-bfab-9c9243f19b3e", "embedding": null, "metadata": {"filename": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.md", "extension": ".md", "title": "Enriching LlamaIndex Models with GraphQL and Graph Databases", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e31106da-d110-425b-8f6e-2b8cf866f208", "node_type": "4", "metadata": {"filename": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.md", "extension": ".md", "title": "Enriching LlamaIndex Models with GraphQL and Graph Databases", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7"}, "hash": "5b80622fb46fa8f20489b384fdf1c5af28c461beca096f5e7574d8e65c2346a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0958479-b601-438a-8156-1cb8f8ffde6c", "node_type": "1", "metadata": {"filename": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.md", "extension": ".md", "title": "Enriching LlamaIndex Models with GraphQL and Graph Databases", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7"}, "hash": "98baab6ed30c5a5261856ee174c6f0a95691c1430bc5e750762bc14bf4d90eb9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8dc349f-9c21-47a1-8dc4-fc92e6923995", "node_type": "1", "metadata": {}, "hash": "542b47c15fe1ce62ef8045cb887fdff8d079b9cf4b6299a2d5ddc4ebedd51209", "class_name": "RelatedNodeInfo"}}, "text": "Here we\njust do the bare minimum to demonstrate our loaders.\n\nAfter setting up the indices with the appropriate loaders, and connected\nindexes, we now can use the index as an LLM query engine and execute our user\nquery.\n\nTo demonstrate that the LLM still is able to use its world knowledge, we can\nask in a mix of English (System), German (Question) and French (requested\nAnswer).\n\n    \n    \n    queryEngine = index.as_query_engine()\n    \n    queryEngine.query(\"\"\"\n    Provide the answer in French.\n    Question: Was ist die Hauptstadt von Albanien?\n    \"\"\")\n\nAs you can see in the response below it doesn\u2019t just answer our question\ncorrectly in French ` La capitale de l\u2019Albanie est Tirana. ` , but also\nprovides which documents it used to generate the answer.\n\n    \n    \n    Response(response=\"La capitale de l'Albanie est Tirana.\", \n    source_nodes=[NodeWithScore(node=Node(text='              &lt;td&gt;Albania&lt;/td&gt;', doc_id='3decbee1-98cc-4650-a071-ed25cd3e00d5', embedding=None, doc_hash='7d9d85082095471a9663690742d2d49fc37b2ec37cc5acf4e99e006a68a17742', extra_info=None, \n    node_info={'start': 0, 'end': 30, '_node_type': &lt;NodeType.TEXT: '1'&gt;}, \n    relationships={&lt;DocumentRelationship.SOURCE: '1'&gt;: '7b6c861f-2c2f-4905-a047-edfc25f7df19'}), score=0.7926356007369129), \n    NodeWithScore(node=Node(text='              &lt;td&gt;Algiers&lt;/td&gt;', doc_id='8111b737-9f45-4855-8cd8-f958d4eb0ccd', embedding=None, doc_hash='8570a02a057a6ebbd0aff6d3f63c9f29a0ee858a81d913298d31b025101d1e44', \n    extra_info=None, node_info={'start': 0, 'end': 30, '_node_type': &lt;NodeType.TEXT: '1'&gt;}, relationships={&lt;DocumentRelationship.SOURCE: '1'&gt;: '22e11ac6-8375-4d0c-91c6-4750fc63a375'}), score=0.7877589022795918)], extra_info={'3decbee1-98cc-4650-a071-ed25cd3e00d5': None, '8111b737-9f45-4855-8cd8-f958d4eb0ccd': None})\n\n#  LlamaIndex Loaders\n\nThe number of existing data sources in [ LlamaHub ](https://llamahub.ai/) is\nimpressive, I counted 100+ integrations in [ the repository\n](https://github.com/emptycrown/llama-hub) . You can find anything from Google\ndocs, to GitHub, to relational databases.\n\n", "mimetype": "text/plain", "start_char_idx": 4775, "end_char_idx": 6908, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8dc349f-9c21-47a1-8dc4-fc92e6923995": {"__data__": {"id_": "c8dc349f-9c21-47a1-8dc4-fc92e6923995", "embedding": null, "metadata": {"filename": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.md", "extension": ".md", "title": "Enriching LlamaIndex Models with GraphQL and Graph Databases", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e31106da-d110-425b-8f6e-2b8cf866f208", "node_type": "4", "metadata": {"filename": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.md", "extension": ".md", "title": "Enriching LlamaIndex Models with GraphQL and Graph Databases", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7"}, "hash": "5b80622fb46fa8f20489b384fdf1c5af28c461beca096f5e7574d8e65c2346a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b51f70fc-9e97-4c52-bfab-9c9243f19b3e", "node_type": "1", "metadata": {"filename": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.md", "extension": ".md", "title": "Enriching LlamaIndex Models with GraphQL and Graph Databases", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7"}, "hash": "62fd8304e78f4fe885c36c2714f09f609c64b83f4b37881635e5d5ced409d3be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bcda5d5b-bbb7-46ef-ae41-e924864e9856", "node_type": "1", "metadata": {}, "hash": "49b396a3b60a0b450ed2f1d64d689c6e20c13c789327a2e77389ac30a44754bd", "class_name": "RelatedNodeInfo"}}, "text": "LlamaHub, screenshot by Author\n\nBut I was missing two of my favorite technologies: GraphQL - the API query\nlanguage open sourced by Facebook and Graph databases like Neo4j, the best way\nto store and manage large amounts of connected data, for example in Knowledge\nGraphs.\n\n> So I thought: \"How hard can it be to add them :)\"\n\n#  Adding the new loaders\n\nAdding new loaders is really straightforward. There is a script in the llama-\nhub repository to help with adding a new loader. Running ` ./add-loader.sh\n<folder> ` added the skeleton files.\n\nTo get familiar with the existing implementations I looked at the [ Databases\n(relational) ](https://github.com/emptycrown/llama-\nhub/tree/main/llama_hub/database) and [ MongoDB integrations\n](https://github.com/emptycrown/llama-hub/tree/main/llama_hub/mongo) , the\nformer for the Graph Database and the latter for the GraphQL.\n\nIt was easy enough, we only needed the requirements for our loader, implement\nthe ` base.py ` with an straightforward API and a ` README.md` ` with an\nexplanation and a code example.\n\nThe main difference my loaders have from the existing ones, is that they don\u2019t\nuse hard-coded field names for extracting the relevant value from the query\nresult, but instead turn the result into YAML.\n\nI picked YAML not because I like it, but because it was closest to a textual\nrepresentation of a nested tree of key-value pairs that a user would write as\nnested bullet lists.\n\nBelow is the example code for the Graph Database implementation (the GraphQL\none is similar).\n\n#  Adding the Graph Database Loader\n\nI added the requirements for the ` neo4j ` dependency, a Cypher query language\nover Bolt protocol python driver, that also works with Memgraph and AWS\nNeptune.\n\nThen I added the code for ` ___init___ ` to take in a database server URI,\ndatabase name and credentials to connect and create a driver instance.\n\nThe ` load_data ` method takes in the query to run and optional parameters.\nIt\u2019s implemented by calling the driver\u2019s ` execute_query ` method.\n\nEach row of results is mapped into a LlamaIndex ` Document ` with the ` text `\nbeing the YAML representation of the results.\n\n    \n    \n    \"\"\"Graph Database Cypher Reader.\"\"\"\n    \n    from typing import Dict, List, Optional\n    \n    from llama_index.readers.base import BaseReader\n    from llama_index.readers.schema.base import Document\n    \n    import yaml\n    \n    class GraphDBCypherReader(BaseReader):\n        \"\"\"Graph database Cypher reader.\n    \n        Combines all Cypher query results into the Document type used by LlamaIndex.\n    \n        Args:\n            uri (str): Graph Database URI\n            username (str): Username\n            password (str): Password\n    \n        \"\"\"\n    \n        def __init__(\n            self,\n            uri: str,\n            username: str,\n            password: str,\n            database: str\n        ) -&gt; None:\n            \"\"\"Initialize with parameters.\"\"\"\n            try:\n                from neo4j import GraphDatabase, basic_auth\n    \n            except ImportError:\n                raise ImportError(\n                    \"`neo4j` package not found, please run `pip install neo4j`\"\n                )\n            if uri:\n                if uri is None:\n                    raise ValueError(\"`uri` must be provided.\")\n                self.client = GraphDatabase.driver(uri=uri, auth=basic_auth(username, password))\n                self.database = database\n    \n        def load_data(\n            self, query: str, parameters: Optional[Dict] = None\n        ) -&gt; List[Document]:\n            \"\"\"Run the Cypher with optional parameters and turn results into documents\n    \n            Args:\n                query (str): Graph Cypher query string.\n                parameters (Optional[Dict]): optional query parameters.\n    \n            Returns:\n                List[Document]: A list of documents.\n    \n            \"\"\"\n            if parameters is None:\n                parameters = {}\n    \n            records, summary, keys = self.client.execute_query(query, parameters, database_ = self.database)\n    \n            documents = [Document(yaml.dump(entry.data())) for entry in records]\n    \n            return documents\n\nYou\u2019re now ready to start using the data loader. If you want to start using\nthis in your code, simply import `GraphDBCypherReader` from the relevant file\nand follow the steps below.\n\nIf you wish to submit the loader on LlamaHub, the process is fairly\nstraightforward. After adding an example to the readme which uses an always-on\ndemo server with StackOverflow data, I was ready to create a [ pull request\n](https://github.com/emptycrown/llama-hub/pull/266) . After a short discussion\nthe PR was quickly merged.\n\nThanks a lot Jerry for the smooth experience.\n\n", "mimetype": "text/plain", "start_char_idx": 6908, "end_char_idx": 11660, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bcda5d5b-bbb7-46ef-ae41-e924864e9856": {"__data__": {"id_": "bcda5d5b-bbb7-46ef-ae41-e924864e9856", "embedding": null, "metadata": {"filename": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.md", "extension": ".md", "title": "Enriching LlamaIndex Models with GraphQL and Graph Databases", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e31106da-d110-425b-8f6e-2b8cf866f208", "node_type": "4", "metadata": {"filename": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.md", "extension": ".md", "title": "Enriching LlamaIndex Models with GraphQL and Graph Databases", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7"}, "hash": "5b80622fb46fa8f20489b384fdf1c5af28c461beca096f5e7574d8e65c2346a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8dc349f-9c21-47a1-8dc4-fc92e6923995", "node_type": "1", "metadata": {"filename": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.md", "extension": ".md", "title": "Enriching LlamaIndex Models with GraphQL and Graph Databases", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7"}, "hash": "61c3b6154861489f470deab38c9e6f357b044a04d05867b1b2e07a6bb2a4d270", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "274164e0-5a9c-4f3c-9d5b-844dfc3612bf", "node_type": "1", "metadata": {}, "hash": "31de5d7bdb8c59105f29b60c351d9b2b1c3326378ff01a14f5d79b7036c3a9ab", "class_name": "RelatedNodeInfo"}}, "text": "Now let\u2019s see how to use our two loaders.\n\n#  Using the Graph Database Loader\n\nThe GraphDB Cypher loader, connects to graph databases, which are specialized\ndatabases that store data not in tables but in entities ( _Nodes_ ) and their\n_Relationships_ . Because they are schema free, you can store real-world\nknowledge without compromising on richness.\n\nImage for \u201cNetwork Graph\u201d generated by Midjourney by Author\n\nRelationships can also hold attributes, which can represent time, weights,\ncosts or whatever defines the concrete relationship. Any node can have as many\nor as few attributes or relationships as needed.\n\n> To query a graph database you can use the ` _Cypher_ ` query language, a\n> pattern based language that expresses those relationships in visual ascii-\n> art patterns. You encircle nodes in parentheses ` _()_ ` and draw\n> relationships as arrows ` _-- > _ ` with additional constraints put in\n> square brackets. Otherwise Cypher provides many features known from SQL and\n> also supports many graph operations as well as handling data structures like\n> nested documents, of lists and dicts.\n\nLet\u2019s use a movie graph database and ask the LLM a question about **_common\naction movie plots_ ** .\n\nSetting up the ` ServiceContext ` and the ` ChatGPTLLMPredictor ` is the same\nas before.\n\nThen we get the ` GraphDBCypherReader ` and connect it to our database (with\nan small example movie graph from [ TheMovieDB ](https://themoviedb.org) with\npermission).\n\n    \n    \n    GraphDBCypherReader = download_loader('GraphDBCypherReader')\n    \n    reader = GraphDBCypherReader(uri = \"neo4j+s://demo.neo4jlabs.com\", \\\n        username = \"recommendations\", password = \"recommendations\", database = \"recommendations\")\n\nThen we define our query to the graph database with a parameter of year that\nallows us to pick more recent movies. When loading the data, each row of\nresults should turn into one ` Document ` where the ` text ` property of the\ndocument is the YAML representation of the row.\n\n    \n    \n    query = \"\"\"\n        MATCH (m:Movie)-[rel:ACTED_IN|DIRECTED|IN_GENRE]-(other)\n        WHERE $year &lt; m.year and m.imdbRating &gt; $rating\n        WITH m, type(rel) as relation, collect(other.name) as names\n        RETURN m.title as title, m.year as year, m.plot as plot, relation, names\n        ORDER BY m.year ASC\n    \"\"\"\n    \n    documents = reader.load_data(query, parameters = {\"year\":1990,\"rating\":8})\n    index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n    \n    print(len(documents))\n    print(documents[0:5])\n\nThe output will look similar to the following:\n\n    \n    \n    829\n    [Document(text='names:\\n- Saifei He\\n- Li Gong\\n- Jingwu Ma\\n- Cuifen Cao\\nplot: A young woman becomes the fourth wife of a wealthy lord, and must learn to live\\n  with the strict rules and tensions within the household.\\nrelation: ACTED_IN\\ntitle: Raise the Red Lantern (Da hong deng long gao gao gua)\\nyear: 1991\\n', doc_id='782d9a63-251b-4bb8-aa3d-5d8f6d1fb5d2', embedding=None, doc_hash='f9fd966bc5f2234e94d09efebd3be008db8c891f8666c1a364abf7812f5d7a1c', extra_info=None), Document(text='names:\\n- Yimou Zhang\\nplot: A young woman becomes the fourth wife of a wealthy lord, and must learn to live\\n  with the strict rules and tensions within the household.\\nrelation: DIRECTED\\ntitle: Raise the Red Lantern (Da hong deng long gao gao gua)\\nyear: 1991\\n', doc_id='2e13caf6-b9cf-4263-a264-7121bc77d1ee', embedding=None, doc_hash='e1f340ed1fac2f1b8d6076cfc2c9e9cb0109d5d11e5dcdbf3a467332f5995cb1', extra_info=None), ...]\n\nNow we can use our ` index ` to run a LLM query to answer the questions we\nwanted to pose.\n\n    \n    \n    queryEngine= index.as_query_engine()\n    \n    queryEngine.query(\"\"\"\n    What are the most common plots in action movies?\n    \"\"\")\n\n", "mimetype": "text/plain", "start_char_idx": 11660, "end_char_idx": 15459, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "274164e0-5a9c-4f3c-9d5b-844dfc3612bf": {"__data__": {"id_": "274164e0-5a9c-4f3c-9d5b-844dfc3612bf", "embedding": null, "metadata": {"filename": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.md", "extension": ".md", "title": "Enriching LlamaIndex Models with GraphQL and Graph Databases", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e31106da-d110-425b-8f6e-2b8cf866f208", "node_type": "4", "metadata": {"filename": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.md", "extension": ".md", "title": "Enriching LlamaIndex Models with GraphQL and Graph Databases", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7"}, "hash": "5b80622fb46fa8f20489b384fdf1c5af28c461beca096f5e7574d8e65c2346a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bcda5d5b-bbb7-46ef-ae41-e924864e9856", "node_type": "1", "metadata": {"filename": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.md", "extension": ".md", "title": "Enriching LlamaIndex Models with GraphQL and Graph Databases", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7"}, "hash": "c72b541191c028ea4e7066b1619ec34c3c1081e41d7444f71f7ae316dc80233d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ba408ab-1c68-49f0-a9d5-5dda3acea107", "node_type": "1", "metadata": {}, "hash": "3f4b1283dcadf667504dfcb5f365001f6ce7c09d6dee375fa28b511989c5a2dc", "class_name": "RelatedNodeInfo"}}, "text": "The answer shows that the LLM can utilize the inputs, understands the genre\n\"action movies\" and can summarize their plots. Here is its answer.\n\n> Based on the given context information, it appears that the most common\n> plots in action movies are heists and battles against controlling forces.\n> However, it is important to note that this conclusion is based on a limited\n> sample size and may not be representative of all action movies.\n    \n    \n    Response(response='Based on the given context information, it appears that the most common plots in action movies are heists and battles against controlling forces. However, it is important to note that this conclusion is based on a limited sample size and may not be representative of all action movies.',\n    \n    \n    source_nodes=[NodeWithScore(node=Node(text='names:\\n- Action\\n- Crime\\n- Thriller\\nplot: A group of professional bank robbers start to feel the heat from police when\\n  they unknowingly leave a clue at their latest heist.\\nrelation: IN_GENRE\\ntitle: Heat\\nyear: 1995\\n', doc_id='bb117618-1cce-4cec-bd9b-8645ab0b50a3', embedding=None, doc_hash='4d493a9f33eb7a1c071756f61e1975ae5c313ecd42243f81a8827919a618468b', extra_info=None, node_info={'start': 0, 'end': 215, '_node_type': &lt;NodeType.TEXT: '1'&gt;}, relationships={&lt;DocumentRelationship.SOURCE: '1'&gt;: 'dbfffdae-d88c-49e2-9d6b-83dad427a3f3'}), score=0.8247381316731472), NodeWithScore(node=Node(text='names:\\n- Thriller\\n- Sci-Fi\\n- Action\\nplot: A computer hacker learns from mysterious rebels about the true nature of his\\n  reality and his role in the war against its controllers.\\nrelation: IN_GENRE\\ntitle: Matrix, The\\nyear: 1999\\n', doc_id='c4893c61-32ee-4d05-b559-1f65a5197e5e', embedding=None, doc_hash='0b6a080bf712548099c5c8c1b033884a38742c73dc23d420ac2e677e7ece82f4', extra_info=None, node_info={'start': 0, 'end': 227, '_node_type': &lt;NodeType.TEXT: '1'&gt;}, relationships={&lt;DocumentRelationship.SOURCE: '1'&gt;: '6c8dea11-1371-4f5a-a1a1-7f517f027008'}), score=0.8220633045996049)], extra_info={'bb117618-1cce-4cec-bd9b-8645ab0b50a3': None, 'c4893c61-32ee-4d05-b559-1f65a5197e5e': None})\n\n#  Using the GraphQL Loader\n\nThe GraphQL loader is similarly easy to use.\n\n[ GraphQL ](https://graphql.org) is not a database query language, but an API\nquery language that is based on strict schema expressed in \"type definitions\".\n", "mimetype": "text/plain", "start_char_idx": 15459, "end_char_idx": 17833, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ba408ab-1c68-49f0-a9d5-5dda3acea107": {"__data__": {"id_": "3ba408ab-1c68-49f0-a9d5-5dda3acea107", "embedding": null, "metadata": {"filename": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.md", "extension": ".md", "title": "Enriching LlamaIndex Models with GraphQL and Graph Databases", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e31106da-d110-425b-8f6e-2b8cf866f208", "node_type": "4", "metadata": {"filename": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.md", "extension": ".md", "title": "Enriching LlamaIndex Models with GraphQL and Graph Databases", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7"}, "hash": "5b80622fb46fa8f20489b384fdf1c5af28c461beca096f5e7574d8e65c2346a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "274164e0-5a9c-4f3c-9d5b-844dfc3612bf", "node_type": "1", "metadata": {"filename": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.md", "extension": ".md", "title": "Enriching LlamaIndex Models with GraphQL and Graph Databases", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7"}, "hash": "a844eed2bfcd557c9103a29649b0f1219536746816c4d624d2ccae1368886788", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c3d3104-b82d-4c3b-b12b-dd1e73aed651", "node_type": "1", "metadata": {}, "hash": "f5de8b19ea086eaf4d37492be5e2d817642645b35800c95a06c6f219a82a0799", "class_name": "RelatedNodeInfo"}}, "text": "There you express your entities, their attributes (fields) both for scalar\ndatatypes as well as object datatypes pointing to other entities.\n\nWhat is GraphQL from GraphQL.org, Screenshot by Author\n\nGraphQL itself is a tree based query language, that expresses a nested\nstructure of data that you want to fetch starting from a root query. The\nfields of every entity returned from that query can be selected and for object\nfields you can further select fields from the referred entity and so on,\nalmost ad-infinitum (API-Limits apply).\n\nThere are a number of GraphQL libraries, most notably the JavaScript reference\nimplementation, but also ` gql ` for python, and also integrations with\ndatabases like Hasura, Prisma or the [ Neo4j-GraphQL-Library\n](https://neo4j.com/product/graphql-library/) . Several larger projects now\nprovide GraphQL APIs including GitHub, Spotify, Twitter.\n\nThe demo is similar to our first one. We use a public GraphQL endpoint ( [\nhttps://countries.trevorblades.com/ ](https://countries.trevorblades.com/) ),\nthat provides a structure of continent\u2192country\u2192capital. ( [ Licensed under MIT\n](https://github.com/trevorblades/countries) )\n\nA subset of the type-definition is here.\n\n    \n    \n    type Query {\n        continent(code: ID!): Continent\n        continents(filter: ContinentFilterInput = {}): [Continent!]!\n        countries(filter: CountryFilterInput = {}): [Country!]!\n        country(code: ID!): Country\n        language(code: ID!): Language\n        languages(filter: LanguageFilterInput = {}): [Language!]!\n    }\n    \n    type Continent {\n        code: ID!\n        countries: [Country!]!\n        name: String!\n    }\n    \n    type Country {\n        awsRegion: String!\n        capital: String\n        code: ID!\n        continent: Continent!\n        currencies: [String!]!\n        currency: String\n        emoji: String!\n        emojiU: String!\n        languages: [Language!]!\n        name(lang: String): String!\n        native: String!\n        phone: String!\n        phones: [String!]!\n        states: [State!]!\n        subdivisions: [Subdivision!]!\n    }\n    ...\n\nIn our demo, we again define the ` ServiceContext ` with the `\nChatGPTLLMPredictor ` as before. Then we get the ` GraphQLReader ` loader and\npoint it to the URL of the endpoint. You can also provide additional HTTP-\nHeaders, e.g. ", "mimetype": "text/plain", "start_char_idx": 17833, "end_char_idx": 20162, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c3d3104-b82d-4c3b-b12b-dd1e73aed651": {"__data__": {"id_": "6c3d3104-b82d-4c3b-b12b-dd1e73aed651", "embedding": null, "metadata": {"filename": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.md", "extension": ".md", "title": "Enriching LlamaIndex Models with GraphQL and Graph Databases", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e31106da-d110-425b-8f6e-2b8cf866f208", "node_type": "4", "metadata": {"filename": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.md", "extension": ".md", "title": "Enriching LlamaIndex Models with GraphQL and Graph Databases", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7"}, "hash": "5b80622fb46fa8f20489b384fdf1c5af28c461beca096f5e7574d8e65c2346a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ba408ab-1c68-49f0-a9d5-5dda3acea107", "node_type": "1", "metadata": {"filename": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.md", "extension": ".md", "title": "Enriching LlamaIndex Models with GraphQL and Graph Databases", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7"}, "hash": "3ebdf77939bc6a8f66e9eab369e68ba544db7395c2ed337272c8f3625016340f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26a5ed24-7606-4cb3-bb79-5a33c3ce85ca", "node_type": "1", "metadata": {}, "hash": "706e1165a8047d2699a7ce9dad7ee0b390d999cb954dd8ae8b80893fea2c5a83", "class_name": "RelatedNodeInfo"}}, "text": "for authentication.\n\n    \n    \n    GraphQLReader = download_loader('GraphQLReader')\n    reader = GraphQLReader(uri = \"https://countries.trevorblades.com/\", headers = {})\n    \n    \n    query = \"\"\"\n    query getContinents {\n      continents {\n        name\n        countries {\n          name\n          capital\n        }\n      }\n    }\n    \"\"\"\n    documents = reader.load_data(query, variables = {})\n    print(len(documents))\n    print(documents)\n\nWe see that it finds 7 continents with countries and capitals, each of the\nroot results (continent) is turned into a document\n\n    \n    \n    7\n    [Document(text='countries:\\n- capital: Luanda\\n  name: Angola\\n- capital: Ouagadougou\\n  name: Burkina Faso\\n- capital: Bujumbura\\n  name: Burundi\\n- capital: Porto-Novo\\n  name: Benin\\n- capital: Gaborone\\n  name: Botswana\\n- capital: Kinshasa\\n  name: Democratic Republic of the Congo\\n- capital: Bangui\\n  name: Central African Republic\\n....',doc_id='b82fec36-5e82-4246-b7ab-f590bf6741ab', embedding=None, doc_hash='a4caa760423d6ca861b9332f386add3c449f1683168391ae10f7f73a691a2240', extra_info=None)]\n\nAgain we stress the LLM only a little bit by asking it in German, \"Which\ncapitals are in North America\".\n\n    \n    \n    index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n    queryEngine= index.as_query_engine()\n    \n    response = queryEngine.query(\"\"\"\n    Question: Welche Hauptst\u00e4dte liegen in Nordamerika?\n    Answer:\n    \"\"\")\n    \n    response.response\n\nI was surprised, as I had only expected a hand-full of countries and cities.\nBut we get 27 countries that are in North America. This shows how our\nperception is skewed by the western worldview.\n\n    \n    \n    Die Hauptst\u00e4dte, die in Nordamerika liegen, sind Ottawa, San Jos\\xE9, Havana, Willemstad, Roseau, Santo Domingo, St. George's, Nuuk, Guatemala City, Tegucigalpa, Port-au-Prince, Kingston, Basseterre, George Town, Castries, Marigot, Fort-de-France, Plymouth, Mexico City, Managua, Panama City, Saint-Pierre, San Juan, San Salvador, Philipsburg, Cockburn Town, Port of Spain, Washington D.C., Kingstown und Road Town.\n\nWe could also flip the GraphQL query around and then get 250 countries with\ntheir respective capitals and continents.\n\n    \n    \n    query = \"\"\"\n    query getCountries {\n      countries {\n        name\n        capital\n        continent {\n            name\n        }\n      }\n    }\n    \"\"\"\n    documents = reader.load_data(query, variables = {})\n    print(len(documents))\n    print(documents)\n\nBoth document lists should work equally well, but let\u2019s see.\n\nThis time the answer from the LLM was much more limited. I\u2019m not sure if that\nwas because the index fed the LLM fewer documents to pick from.\n\n    \n    \n    index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n    queryEngine= index.as_query_engine()\n    \n    response = queryEngine.query(\"\"\"\n    Question: Which capitals are in North America?\n    Answer:\n    \"\"\")\n    \n    response.response\n    \n    \n    Washington D.C. ", "mimetype": "text/plain", "start_char_idx": 20162, "end_char_idx": 23187, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26a5ed24-7606-4cb3-bb79-5a33c3ce85ca": {"__data__": {"id_": "26a5ed24-7606-4cb3-bb79-5a33c3ce85ca", "embedding": null, "metadata": {"filename": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.md", "extension": ".md", "title": "Enriching LlamaIndex Models with GraphQL and Graph Databases", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e31106da-d110-425b-8f6e-2b8cf866f208", "node_type": "4", "metadata": {"filename": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.md", "extension": ".md", "title": "Enriching LlamaIndex Models with GraphQL and Graph Databases", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7"}, "hash": "5b80622fb46fa8f20489b384fdf1c5af28c461beca096f5e7574d8e65c2346a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c3d3104-b82d-4c3b-b12b-dd1e73aed651", "node_type": "1", "metadata": {"filename": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.md", "extension": ".md", "title": "Enriching LlamaIndex Models with GraphQL and Graph Databases", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7"}, "hash": "a4247431ccbd74f481cb3cd37f1437e4d602b4ac88a14afe069782ff37fec236", "class_name": "RelatedNodeInfo"}}, "text": "and Mexico City are in North America.\n\n#  Conclusion\n\nIt was really smooth to add new data loaders to LlamaHub, thanks a lot to [\nJerry Liu ](https://medium.com/u/e76da1c45ef7?source=post_page-----\nbcaecec262d7--------------------------------) for making it so easy. Please\nlet me know what you\u2019re doing with these loaders and if you have any feedback.\n\nIf I find time in the next weeks I also want to look into the `\nKnowledgeGraphIndex ` and see if my graph database loader can nicely populate\nthat one.\n\n", "mimetype": "text/plain", "start_char_idx": 23187, "end_char_idx": 23694, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47ae2bf1-83d6-483a-a1ba-f7f8e7b5989d": {"__data__": {"id_": "47ae2bf1-83d6-483a-a1ba-f7f8e7b5989d", "embedding": null, "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68669e3a-bedb-4584-a56b-d99aecc96570", "node_type": "4", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "7681e64ffab777e24150ac9a12ce2f5605fd961b8c339fe201dda2d5a3c2e1e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a3d307c-eb1d-4dd3-a45b-d84f57c81a32", "node_type": "1", "metadata": {}, "hash": "c5d8307b65dcc8e2b4c1d72daf8e50cc6a2a38a5624b24a892cd97933764b3ea", "class_name": "RelatedNodeInfo"}}, "text": "Co-authors: Jerry Liu (CEO at LlamaIndex), Amog Kamsetty (Software Engineer at\nAnyscale)\n\n( **note:** this is cross-posted from the original blog post on Anyscale\u2019s\nwebsite. [ Check it out here ](https://www.anyscale.com/blog/build-and-scale-\na-powerful-query-engine-with-llamaindex-ray) !)\n\nIn this blog, we showcase how you can use LlamaIndex and Ray to build a query\nengine to answer questions and generate insights about Ray itself, given its\ndocumentation and blog posts.\n\nWe\u2019ll give a quick introduction of LlamaIndex + Ray, and then walk through a\nstep-by-step tutorial on building and deploying this query engine. We make use\nof both Ray Datasets to parallelize building indices as well as Ray Serve to\nbuild deployments.\n\n#  Introduction\n\nLarge Language Models (LLMs) offer the promise of allowing users to extract\ncomplex insights from their unstructured text data. Retrieval-augmented\ngeneration pipelines have emerged as a common pattern for developing LLM\napplications allowing users to effectively perform semantic search over a\ncollection of documents.\n\n_Example of retrieval augmented generation. Relevant context is pulled from a\nset of documents and included in the LLM input prompt._\n\nHowever, when productionizing these applications over many different data\nsources, there are a few challenges:\n\n  1. Tooling for indexing data from many different data sources \n  2. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1386, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a3d307c-eb1d-4dd3-a45b-d84f57c81a32": {"__data__": {"id_": "1a3d307c-eb1d-4dd3-a45b-d84f57c81a32", "embedding": null, "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68669e3a-bedb-4584-a56b-d99aecc96570", "node_type": "4", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "7681e64ffab777e24150ac9a12ce2f5605fd961b8c339fe201dda2d5a3c2e1e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47ae2bf1-83d6-483a-a1ba-f7f8e7b5989d", "node_type": "1", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "473ba57e09d46d9de04f780743274d13032845faf03e579c7ae55326a47905f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c92f247c-40d0-4d97-87f1-c49959c72819", "node_type": "1", "metadata": {}, "hash": "f4e8f3adec4f1425450593fe81a7672dec48c6ccc324398d3e5a3c6470bef366", "class_name": "RelatedNodeInfo"}}, "text": "Handling complex queries over different data sources \n  3. Scaling indexing to thousands or millions of documents \n  4. Deploying a scalable LLM application into production \n\nHere, we showcase how [ LlamaIndex ](https://gpt-\nindex.readthedocs.io/en/latest/) and [ Ray ](https://docs.ray.io/en/latest/)\nare the perfect setup for this task.\n\nLlamaIndex is a data framework for building LLM applications, and solves\nChallenges #1 and #2. It also provides a comprehensive toolkit allowing users\nto connect their private data with a language model. ", "mimetype": "text/plain", "start_char_idx": 1386, "end_char_idx": 1930, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c92f247c-40d0-4d97-87f1-c49959c72819": {"__data__": {"id_": "c92f247c-40d0-4d97-87f1-c49959c72819", "embedding": null, "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68669e3a-bedb-4584-a56b-d99aecc96570", "node_type": "4", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "7681e64ffab777e24150ac9a12ce2f5605fd961b8c339fe201dda2d5a3c2e1e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a3d307c-eb1d-4dd3-a45b-d84f57c81a32", "node_type": "1", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "7900d001356e683586ca2babf5b56233a86a127a86a22771d45f5a6234be3d97", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b391dba6-8c48-4f5f-94f3-74b4bd694f3c", "node_type": "1", "metadata": {}, "hash": "68752fc1cd29e5036262c7fb7e5d3915419d6ccac9d9e481175b7ff4c8bffcd4", "class_name": "RelatedNodeInfo"}}, "text": "It offers a variety of\ntools to help users first ingest and index their data \u2014 convert different\nformats of unstructured and structured data into a format that the language\nmodel can use, and query their private data.\n\nRay is a powerful framework for scalable AI that solves Challenges #3 and #4.\nWe can use it to dramatically accelerate ingest, inference, pretraining, and\nalso effortlessly deploy and scale the query capabilities of LlamaIndex into\nthe cloud.\n\nMore specifically, we showcase a very relevant use case \u2014 highlighting Ray\nfeatures that are present in both the documentation as well as the Ray blog\nposts!\n\n#  Data Ingestion and Embedding Pipeline\n\nWe use LlamaIndex + Ray to ingest, parse, embed and store Ray docs and blog\nposts in a parallel fashion. For the most part, these steps are duplicated\nacross the two data sources, so we show the steps for just the documentation\nbelow.\n\nCode for this part of the blog is [ available here\n](https://github.com/amogkam/llama_index_ray/blob/main/create_vector_index.py)\n.\n\n_Sequential pipeline with \u201cingest\u201d, \u201cparse\u201d and \u201cembed\u201d stages. Files are\nprocessed sequentially resulting in poor hardware utilization and long\ncomputation time._ _Parallel pipeline. Thanks to Ray we can process multiple\ninput files simultaneously. Parallel processing has much better performance,\nbecause hardware is better utilized._\n\n#  Load Data\n\nWe start by ingesting these two sources of data. We first fetch both data\nsources and download the HTML files.\n\n", "mimetype": "text/plain", "start_char_idx": 1930, "end_char_idx": 3427, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b391dba6-8c48-4f5f-94f3-74b4bd694f3c": {"__data__": {"id_": "b391dba6-8c48-4f5f-94f3-74b4bd694f3c", "embedding": null, "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68669e3a-bedb-4584-a56b-d99aecc96570", "node_type": "4", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "7681e64ffab777e24150ac9a12ce2f5605fd961b8c339fe201dda2d5a3c2e1e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c92f247c-40d0-4d97-87f1-c49959c72819", "node_type": "1", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "18757e93542611667116c9685c2d7872a1793e5d2ae017dba4975d19f15f4bea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbdaeaa5-5218-43ea-a3c6-c1b7f2b5d00e", "node_type": "1", "metadata": {}, "hash": "3c1a378adb7b4eebe6814c883f14b4d0d95e6591833c871627e50f1cafba8e80", "class_name": "RelatedNodeInfo"}}, "text": "We then need to load and parse these files. We can do this with the help of\nLlamaHub, our community-driven repository of 100+ data loaders from various\nAPI\u2019s, file formats (.pdf, .html, .docx), and databases. We use an HTML data\nloader offered by [ Unstructured ](https://github.com/Unstructured-\nIO/unstructured) .\n\n    \n    \n    from typing import Dict, List\n    from pathlib import Path\n    \n    from llama_index import download_loader\n    from llama_index import Document\n    \n    # Step 1: Logic for loading and parsing the files into llama_index documents.\n    UnstructuredReader = download_loader(\"UnstructuredReader\")\n    loader = UnstructuredReader()\n    \n    def load_and_parse_files(file_row: Dict[str, Path]) -&gt; Dict[str, Document]:\n        documents = []\n        file = file_row[\"path\"]\n        if file.is_dir():\n            return []\n        # Skip all non-html files like png, jpg, etc.\n        if file.suffix.lower() == \".html\":\n            loaded_doc = loader.load_data(file=file, split_documents=False)\n            loaded_doc[0].extra_info = {\"path\": str(file)}\n            documents.extend(loaded_doc)\n        return [{\"doc\": doc} for doc in documents]\n\nUnstructured offers a robust suite of parsing tools on top of various files.\nIt is able to help sanitize HTML documents by stripping out information like\ntags and formatting the text accordingly.\n\n##  Scaling Data Ingest\n\nSince we have many HTML documents to process, loading/processing each one\nserially is inefficient and slow. This is an opportunity to use Ray and\ndistribute execution of the `load_and_parse_files` method across multiple CPUs\nor GPUs.\n\n    \n    \n    import ray\n    \n    # Get the paths for the locally downloaded documentation.\n    all_docs_gen = Path(\"./docs.ray.io/\").rglob(\"*\")\n    all_docs = [{\"path\": doc.resolve()} for doc in all_docs_gen]\n    \n    # Create the Ray Dataset pipeline\n    ds = ray.data.from_items(all_docs)\n    \n    # Use `flat_map` since there is a 1:N relationship.\n    # Each filepath returns multiple documents.\n    loaded_docs = ds.flat_map(load_and_parse_files)\n\n#  Parse Files\n\nNow that we\u2019ve loaded the documents, the next step is to parse them into Node\nobjects \u2014 a \u201cNode\u201d object represents a more granular chunk of text, derived\nfrom the source documents. Node objects can be used in the input prompt as\ncontext; by setting a small enough chunk size, we can make sure that inserting\nNode objects do not overflow the context limits.\n\nWe define a function called `convert_documents_into_nodes` which converts\ndocuments into nodes using a simple text splitting strategy.\n\n    \n    \n    # Step 2: Convert the loaded documents into llama_index Nodes. This will split the documents into chunks.\n    from llama_index.node_parser import SimpleNodeParser\n    from llama_index.data_structs import Node\n    \n    def convert_documents_into_nodes(documents: Dict[str, Document]) -&gt; Dict[str, Node]:\n        parser = SimpleNodeParser()\n        document = documents[\"doc\"]\n        nodes = parser.get_nodes_from_documents([document]) \n        return [{\"node\": node} for node in nodes]\n\n##  Run Parsing in Parallel\n\nSince we have many documents, processing each document into nodes serially is\ninefficient and slow. We use Ray `flat_map` method to process documents into\nnodes in parallel:\n\n    \n    \n    # Use `flat_map` since there is a 1:N relationship. Each document returns multiple nodes.\n    nodes = loaded_docs.flat_map(convert_documents_into_nodes)\n\n#  Generate Embeddings\n\nWe then generate embeddings for each Node using a Hugging Face Sentence\nTransformers model. We can do this with the help of LangChain\u2019s embedding\nabstraction.\n\nSimilar to document loading/parsing, embedding generation can similarly be\nparallelized with Ray. We wrap these embedding operations into a helper class,\ncalled `EmbedNodes`, to take advantage of Ray abstractions.\n\n    \n    \n    # Step 3: Embed each node using a local embedding model.\n    from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n    \n    class EmbedNodes:\n        def __init__(self):\n            self.embedding_model = HuggingFaceEmbeddings(\n                # Use all-mpnet-base-v2 Sentence_transformer.\n                # This is the default embedding model for LlamaIndex/Langchain.\n                model_name=\"sentence-transformers/all-mpnet-base-v2\", \n                model_kwargs={\"device\": \"cuda\"},\n                # Use GPU for embedding and specify a large enough batch size to maximize GPU utilization.\n                # Remove the \"device\": \"cuda\" to use CPU instead.\n                encode_kwargs={\"device\": \"cuda\", \"batch_size\": 100}\n                )\n    \n        def __call__(self, node_batch: Dict[str, List[Node]]) -&gt; Dict[str, List[Node]]:\n            nodes = node_batch[\"node\"]\n            text = [node.text for node in nodes]\n            embeddings = self.embedding_model.embed_documents(text)\n            assert len(nodes) == len(embeddings)\n    \n            for node, embedding in zip(nodes, embeddings):\n                node.embedding = embedding\n            return {\"embedded_nodes\": nodes}\n\nAfterwards, generating an embedding for each node is as simple as calling the\nfollowing operation in Ray:\n\n    \n    \n    # Use `map_batches` to specify a batch size to maximize GPU utilization.\n    # We define `EmbedNodes` as a class instead of a function so we only initialize the embedding model once. \n    \n    # This state can be reused for multiple batches.\n    embedded_nodes = nodes.map_batches(\n        EmbedNodes, \n        batch_size=100, \n        # Use 1 GPU per actor.\n        num_gpus=1,\n        # There are 4 GPUs in the cluster. Each actor uses 1 GPU. So we want 4 total actors.\n        compute=ActorPoolStrategy(size=4))\n    \n    # Step 5: Trigger execution and collect all the embedded nodes.\n    ray_docs_nodes = []\n    for row in embedded_nodes.iter_rows():\n        node = row[\"embedded_nodes\"]\n        assert node.embedding is not None\n        ray_docs_nodes.append(node)\n\n#  Data Indexing\n\nThe next step is to store these nodes within an \u201cindex\u201d in LlamaIndex. An\nindex is a core abstraction in LlamaIndex to \u201cstructure\u201d your data in a\ncertain way \u2014 this structure can then be used for downstream LLM retrieval +\nquerying. An index can interface with a storage or vector store abstraction.\n\nThe most commonly used index abstraction within LlamaIndex is our vector\nindex, where each node is stored along with an embedding. In this example, we\nuse a simple in-memory vector store, but you can also choose to specify any\none of LlamaIndex\u2019s 10+ vector store integrations as the storage provider\n(e.g. Pinecone, Weaviate, Chroma).\n\nWe build two vector indices: one over the documentation nodes, and another\nover the blog post nodes and persist them to disk. Code is [ available here\n](https://github.com/amogkam/llama_index_ray/blob/main/create_vector_index.py#L102:L131)\n.\n\n    \n    \n    from llama_index import GPTVectorStoreIndex\n    \n    # Store Ray Documentation embeddings\n    ray_docs_index = GPTVectorStoreIndex(nodes=ray_docs_nodes)\n    ray_docs_index.storage_context.persist(persist_dir=\"/tmp/ray_docs_index\")\n    \n    # Store Anyscale blog post embeddings\n    ray_blogs_index = GPTVectorStoreIndex(nodes=ray_blogs_nodes)\n    ray_blogs_index.storage_context.persist(persist_dir=\"/tmp/ray_blogs_index\")\n\n**That\u2019s it in terms of building a data pipeline using LlamaIndex + Ray Data**\n!\n\nYour data is now ready to be used within your LLM application. ", "mimetype": "text/plain", "start_char_idx": 3427, "end_char_idx": 10903, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fbdaeaa5-5218-43ea-a3c6-c1b7f2b5d00e": {"__data__": {"id_": "fbdaeaa5-5218-43ea-a3c6-c1b7f2b5d00e", "embedding": null, "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68669e3a-bedb-4584-a56b-d99aecc96570", "node_type": "4", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "7681e64ffab777e24150ac9a12ce2f5605fd961b8c339fe201dda2d5a3c2e1e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b391dba6-8c48-4f5f-94f3-74b4bd694f3c", "node_type": "1", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "ddab60211fd9c10399a1fc1b98ab7fe5d3e46d0c04a01a99b249be0d513c6785", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8339e2c-af41-4400-8bf4-c550b5917a4e", "node_type": "1", "metadata": {}, "hash": "4cd2c9db5078791eb46bb045a4984771180c766c54e3a8a70b91a599a7cd3ae0", "class_name": "RelatedNodeInfo"}}, "text": "Check out our\nnext section for how to use advanced LlamaIndex query capabilities on top of\nyour data.\n\n#  Data Querying\n\nLlamaIndex provides both simple and advanced query capabilities on top of your\ndata + indices. The central abstraction within LlamaIndex is called a \u201cquery\nengine.\u201d A query engine takes in a natural language query input and returns a\nnatural language \u201coutput\u201d. Each index has a \u201cdefault\u201d corresponding query\nengine. For instance, the default query engine for a vector index first\nperforms top-k retrieval over the vector store to fetch the most relevant\ndocuments.\n\nThese query engines can be easily derived from each index:\n\n    \n    \n    ray_docs_engine = ray_docs_index.as_query_engine(similarity_top_k=5, service_context=service_context)\n    \n    ray_blogs_engine = ray_blogs_index.as_query_engine(similarity_top_k=5, service_context=service_context)\n\nLlamaIndex also provides more advanced query engines for multi-document use\ncases \u2014 for instance, we may want to ask how a given feature in Ray is\nhighlighted in both the documentation and blog. `SubQuestionQueryEngine` can\ntake in other query engines as input. Given an existing question, it can\ndecide to break down the question into simpler questions over any subset of\nquery engines; it will execute the simpler questions and combine results at\nthe top-level.\n\nThis abstraction is quite powerful; it can perform semantic search over one\ndocument, or combine results across multiple documents.\n\nFor instance, given the following question \u201cWhat is Ray?\u201d, we can break this\ninto sub-questions \u201cWhat is Ray according to the documentation\u201d, and \u201cWhat is\nRay according to the blog posts\u201d over the document query engine and blog query\nengine respectively.\n\n    \n    \n    # Define a sub-question query engine, that can use the individual query engines as tools.\n            query_engine_tools = [\n                QueryEngineTool(\n                    query_engine=self.ray_docs_engine,\n                    metadata=ToolMetadata(name=\"ray_docs_engine\", description=\"Provides information about the Ray documentation\")\n                ),\n                QueryEngineTool(\n                    query_engine=self.ray_blogs_engine, \n                    metadata=ToolMetadata(name=\"ray_blogs_engine\", description=\"Provides information about Ray blog posts\")\n                ),\n            ]\n    \n    sub_query_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools, service_context=service_context, use_async=False)\n\nHave a look at [ deploy_app.py\n](https://github.com/amogkam/llama_index_ray/blob/main/deploy_app.py#L22:L56)\nto review the full implementation.\n\n#  Deploying with Ray Serve\n\nWe\u2019ve now created an incredibly powerful query module over your data. As a\nnext step, what if we could seamlessly deploy this function to production and\nserve users? Ray Serve makes this incredibly easy to do. Ray Serve is a\nscalable compute layer for serving ML models and LLMs that enables serving\nindividual models or creating composite model pipelines where you can\nindependently deploy, update, and scale individual components.\n\nTo do this, you just need to do the following steps:\n\n  1. ", "mimetype": "text/plain", "start_char_idx": 10903, "end_char_idx": 14079, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8339e2c-af41-4400-8bf4-c550b5917a4e": {"__data__": {"id_": "f8339e2c-af41-4400-8bf4-c550b5917a4e", "embedding": null, "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68669e3a-bedb-4584-a56b-d99aecc96570", "node_type": "4", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "7681e64ffab777e24150ac9a12ce2f5605fd961b8c339fe201dda2d5a3c2e1e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fbdaeaa5-5218-43ea-a3c6-c1b7f2b5d00e", "node_type": "1", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "022d186db53db786f5bf51e3251de8ca5a4c189436e9b593485008ef52e9b763", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c26dc2fd-5628-4e57-b782-4cd3c2a4f9c1", "node_type": "1", "metadata": {}, "hash": "e43cb4a6a68cdc86d9e5dcb3ac0211a1b253352b51155c08d5930b9190081cba", "class_name": "RelatedNodeInfo"}}, "text": "Define an outer class that can \u201cwrap\u201d a query engine, and expose a \u201cquery\u201d endpoint \n  2. Add a `@ray.serve.deployment` decorator on this class \n  3. Deploy the Ray Serve application \n\nIt will look something like the following:\n\n    \n    \n    from ray import serve\n    \n    @serve.deployment\n    class QADeployment:\n        def __init__(self):\n     self.query_engine = ...\n    \n        def query(self, query: str):\n                response =  self.query_engine.query(query)\n                source_nodes = response.source_nodes\n                source_str = \"\"\n                for i in range(len(source_nodes)):\n                    node = source_nodes[i]\n                    source_str += f\"Sub-question {i+1}:\\n\"\n                    source_str += node.node.text\n                    source_str += \"\\n\\n\"\n                return f\"Response: {str(response)} \\n\\n\\n {source_str}\\n\"\n    \n        async def __call__(self, request: Request):\n            query = request.query_params[\"query\"]\n            return str(self.query(query))\n    \n    # Deploy the Ray Serve application.\n    deployment = QADeployment.bind()\n\nHave a look at the [ deploy_app.py\n](https://github.com/amogkam/llama_index_ray/blob/main/deploy_app.py) for full\nimplementation.\n\n#  Example Queries\n\nOnce we\u2019ve deployed the application, we can query it with questions about Ray.\n\nWe can query just one of the data sources:\n\n    \n    \n    Q: \"What is Ray Serve?\"\n    \n    Ray Serve is a system for deploying and managing applications on a Ray\n    cluster. It provides APIs for deploying applications, managing replicas, and\n    making requests to applications. ", "mimetype": "text/plain", "start_char_idx": 14079, "end_char_idx": 15698, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c26dc2fd-5628-4e57-b782-4cd3c2a4f9c1": {"__data__": {"id_": "c26dc2fd-5628-4e57-b782-4cd3c2a4f9c1", "embedding": null, "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68669e3a-bedb-4584-a56b-d99aecc96570", "node_type": "4", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "7681e64ffab777e24150ac9a12ce2f5605fd961b8c339fe201dda2d5a3c2e1e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8339e2c-af41-4400-8bf4-c550b5917a4e", "node_type": "1", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "09e0f078f84f584c1267cfe75e1aefd1e8ba21f134573aa89940eddc4dd848a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ef2f201-8c12-4519-ae96-4eabb8f4f3a6", "node_type": "1", "metadata": {}, "hash": "47bfc07dae3f64ae7052782cca6e4d530982d44422de297a7b248626d6124bc1", "class_name": "RelatedNodeInfo"}}, "text": "It also provides a command line interface\n    (CLI) for managing applications and a dashboard for monitoring applications.\n\nBut, we can also provide complex queries that require synthesis across both\nthe documentation and the blog posts. These complex queries are easily handled\nby the subquestion-query engine that we defined.\n\n    \n    \n    Q: \"Compare and contrast how the Ray docs and the Ray blogs present Ray Serve\"\n    \n    Response: \n    The Ray docs and the Ray blogs both present Ray Serve as a web interface\n    that provides metrics, charts, and other features to help Ray users\n    understand and debug Ray applications. However, the Ray docs provide more\n    detailed information, such as a Quick Start guide, user guide, production\n    guide, performance tuning guide, development workflow guide, API reference,\n    experimental Java API, and experimental gRPC support. Additionally, the Ray\n    docs provide a guide for migrating from 1.x to 2.x. On the other hand, the\n    Ray blogs provide a Quick Start guide, a User Guide, and Advanced Guides to\n    help users get started and understand the features of Ray Serve.\n    Additionally, the Ray blogs provide examples and use cases to help users\n    understand how to use Ray Serve in their own projects.\n    \n    ---\n    \n    Sub-question 1\n    \n    Sub question: How does the Ray docs present Ray Serve\n    \n    Response: \n    The Ray docs present Ray Serve as a web interface that provides metrics,\n    charts, and other features to help Ray users understand and debug Ray\n    applications. It provides a Quick Start guide, user guide, production guide,\n    performance tuning guide, and development workflow guide. ", "mimetype": "text/plain", "start_char_idx": 15698, "end_char_idx": 17383, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ef2f201-8c12-4519-ae96-4eabb8f4f3a6": {"__data__": {"id_": "4ef2f201-8c12-4519-ae96-4eabb8f4f3a6", "embedding": null, "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68669e3a-bedb-4584-a56b-d99aecc96570", "node_type": "4", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "7681e64ffab777e24150ac9a12ce2f5605fd961b8c339fe201dda2d5a3c2e1e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c26dc2fd-5628-4e57-b782-4cd3c2a4f9c1", "node_type": "1", "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}, "hash": "3b75196427682ac6b02d040997967bb15e984a7989764cb080b20a0047d0b026", "class_name": "RelatedNodeInfo"}}, "text": "It also provides\n    an API reference, experimental Java API, and experimental gRPC support.\n    Finally, it provides a guide for migrating from 1.x to 2.x.\n    \n    ---\n    \n    Sub-question 2\n    \n    Sub question: How does the Ray blogs present Ray Serve\n    \n    Response: \n    The Ray blog presents Ray Serve as a framework for distributed applications\n    that enables users to handle HTTP requests, scale and allocate resources,\n    compose models, and more. It provides a Quick Start guide, a User Guide, and\n    Advanced Guides to help users get started and understand the features of Ray\n    Serve. Additionally, it provides examples and use cases to help users\n    understand how to use Ray Serve in their own projects.\n\n#  Conclusion\n\nIn this example, we showed how you can build a scalable data pipeline and a\npowerful query engine using LlamaIndex + Ray. We also demonstrated how to\ndeploy LlamaIndex applications using Ray Serve. This allows you to\neffortlessly ask questions and synthesize insights about Ray across disparate\ndata sources!\n\nWe used LlamaIndex \u2014 a data framework for building LLM applications \u2014 to load,\nparse, embed and index the data. We ensured efficient and fast parallel\nexecution by using Ray. Then, we used LlamaIndex querying capabilities to\nperform semantic search over a single document, or combine results across\nmultiple documents. Finally, we used Ray Serve to package the application for\nproduction use.\n\nImplementation in open source, code is available on GitHub: [ LlamaIndex-Ray-\napp ](https://github.com/amogkam/llama_index_ray)\n\n#  What\u2019s next?\n\nVisit LlamaIndex [ site ](https://www.llamaindex.ai/) and [ docs\n](https://gpt-index.readthedocs.io/en/latest/) to learn more about this data\nframework for building LLM applications.\n\nVisit [ Ray docs ](https://docs.ray.io/en/latest/ray-overview/use-\ncases.html#llms-and-gen-ai) to learn more about how to build and deploy\nscalable LLM apps.\n\nJoin our communities!\n\n  * [ Join Ray community ](https://forms.gle/9TSdDYUgxYs8SA9e8) on Slack and Ray #LLM channel. \n  * You can also join the LlamaIndex [ community on discord ](https://discord.gg/UB58qbeq) . \n\nWe have our [ Ray Summit 2023 ](https://raysummit.anyscale.com/) early-bird\nregistration open until 6/30. Secure your spot, save some money, savor the\ncommunity camaraderie at the summit.\n\n", "mimetype": "text/plain", "start_char_idx": 17383, "end_char_idx": 19726, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eba5dc32-49af-4d25-b19b-d2fbbff1abe1": {"__data__": {"id_": "eba5dc32-49af-4d25-b19b-d2fbbff1abe1", "embedding": null, "metadata": {"filename": "llamaindex-update-6-26-2023-ed30a9d45f84.md", "extension": ".md", "title": "LlamaIndex Update \u2014 06/26/2023", "date": "Jun 26, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-6-26-2023-ed30a9d45f84"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "29b9e75b-2597-42da-b8a7-a40eba97e76a", "node_type": "4", "metadata": {"filename": "llamaindex-update-6-26-2023-ed30a9d45f84.md", "extension": ".md", "title": "LlamaIndex Update \u2014 06/26/2023", "date": "Jun 26, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-6-26-2023-ed30a9d45f84"}, "hash": "09b7b0c11eab2b56d52d4a3135dfadfacf851b27bbcfecf650d2f53cc8ebf0d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6abb63ad-84a2-444e-a36f-b35e2c452e3a", "node_type": "1", "metadata": {}, "hash": "bec52af09ae4f2f9e7641c364cc2c174c6e0011d4d8346fcc0f828d87d51ab22", "class_name": "RelatedNodeInfo"}}, "text": "Greetings, LlamaIndex community!\n\nWe\u2019re excited to introduce our new blog series, the LlamaIndex Update.\nRecognizing the fast pace of our open-source project, this series will serve\nas your continual guide, tracking the latest advancements in features,\nwebinars, hackathons, and community events.\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 298, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6abb63ad-84a2-444e-a36f-b35e2c452e3a": {"__data__": {"id_": "6abb63ad-84a2-444e-a36f-b35e2c452e3a", "embedding": null, "metadata": {"filename": "llamaindex-update-6-26-2023-ed30a9d45f84.md", "extension": ".md", "title": "LlamaIndex Update \u2014 06/26/2023", "date": "Jun 26, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-6-26-2023-ed30a9d45f84"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "29b9e75b-2597-42da-b8a7-a40eba97e76a", "node_type": "4", "metadata": {"filename": "llamaindex-update-6-26-2023-ed30a9d45f84.md", "extension": ".md", "title": "LlamaIndex Update \u2014 06/26/2023", "date": "Jun 26, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-6-26-2023-ed30a9d45f84"}, "hash": "09b7b0c11eab2b56d52d4a3135dfadfacf851b27bbcfecf650d2f53cc8ebf0d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eba5dc32-49af-4d25-b19b-d2fbbff1abe1", "node_type": "1", "metadata": {"filename": "llamaindex-update-6-26-2023-ed30a9d45f84.md", "extension": ".md", "title": "LlamaIndex Update \u2014 06/26/2023", "date": "Jun 26, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-6-26-2023-ed30a9d45f84"}, "hash": "275b96156fed1f8cb0d295be0b402b6f74e80bd122f1ced97c1d83736f347419", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e28414a9-144b-421a-be57-4a41ba3ecb0a", "node_type": "1", "metadata": {}, "hash": "e4ec2a181f858f50f5c885b84f60c4de1a3a8656d34bd284b4d502fdb408bc60", "class_name": "RelatedNodeInfo"}}, "text": "Our goal is simple: to keep you updated, engaged, and inspired. ", "mimetype": "text/plain", "start_char_idx": 298, "end_char_idx": 362, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e28414a9-144b-421a-be57-4a41ba3ecb0a": {"__data__": {"id_": "e28414a9-144b-421a-be57-4a41ba3ecb0a", "embedding": null, "metadata": {"filename": "llamaindex-update-6-26-2023-ed30a9d45f84.md", "extension": ".md", "title": "LlamaIndex Update \u2014 06/26/2023", "date": "Jun 26, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-6-26-2023-ed30a9d45f84"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "29b9e75b-2597-42da-b8a7-a40eba97e76a", "node_type": "4", "metadata": {"filename": "llamaindex-update-6-26-2023-ed30a9d45f84.md", "extension": ".md", "title": "LlamaIndex Update \u2014 06/26/2023", "date": "Jun 26, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-6-26-2023-ed30a9d45f84"}, "hash": "09b7b0c11eab2b56d52d4a3135dfadfacf851b27bbcfecf650d2f53cc8ebf0d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6abb63ad-84a2-444e-a36f-b35e2c452e3a", "node_type": "1", "metadata": {"filename": "llamaindex-update-6-26-2023-ed30a9d45f84.md", "extension": ".md", "title": "LlamaIndex Update \u2014 06/26/2023", "date": "Jun 26, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-6-26-2023-ed30a9d45f84"}, "hash": "90eaad74c50c4d142bb2a12bbf1001a8ac4f18d1f678e08b272db3b0e6cd55c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f6b7060-177f-45d4-85ca-6cf6b95f9cef", "node_type": "1", "metadata": {}, "hash": "fc631e54fe73ae4c3982e0f3bca83d21d3394b52e055fd338501a27ae168687f", "class_name": "RelatedNodeInfo"}}, "text": "Whether you\u2019re\na long-time contributor or a new joiner, these updates will help you stay in\nsync with our progress.\n\nSo, let\u2019s explore the recent happenings in our premier edition of the\nLlamaIndex Update.\n\n##  **Features And Integrations:**\n\n  1. LLMs with Knowledge Graphs, supported by NebulaGraph. This new stack enables unique retrieval-augmented generation techniques. Our Knowledge Graph index introduces a GraphStore abstraction, complementing our existing data store types. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.html) , [ Tweet ](https://twitter.com/jerryjliu0/status/1667196231863656448)\n  2. Better LLM app UX supports in-line citations of its sources, enhancing interpretability and traceability. Our new ` CitationQueryEngine ` enables these citations and ensures they correspond with retrieved documents. This feature marks a leap towards improving transparency in LlamaIndex applications. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/query_engine/citation_query_engine.html) , [ Tweet ](https://twitter.com/jerryjliu0/status/1667563694472175616?s=20)\n  3. LlamaIndex integrates with Microsoft Guidance to ensure structured outputs from LLMs. It allows direct prompting of JSON keys and facilitates the conversion of Pydantic objects into the Guidance format, enhancing structured interactions. It can be used independently or with the SubQuestionQueryEngine. [ Docs ](https://gpt-index.readthedocs.io/en/latest/how_to/integrations/guidance.html) , [ Tweet ](https://twitter.com/llama_index/status/1668281830347530242)\n  4. The GuidelineEvaluator module allows users to set text guidelines, thereby aiding in the evaluation of LLM-generated text responses. This paves the way toward automated error correction capabilities. [ Notebook ](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/evaluation/RetryQuery.ipynb) , [ Tweet ](https://twitter.com/llama_index/status/1667920234500751361?s=20)\n  5. We now include a simple ` OpenAIAgent ` , offering an agent interface capable of sequential tool use and async callbacks. This integration was made possible with the help of the OpenAI function API and the LangChain abstractions. [ Tweet ](https://twitter.com/llama_index/status/1668995630356725762)\n  6. ` OpenAIPydanticProgram ` in LlamaIndex enhances structured output extraction. This standalone module allows any LLM input to be converted into a Pydantic object, providing a streamlined approach to data structuring. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/output_parsing/openai_pydantic_program.html) , [ Tweet ](https://twitter.com/llama_index/status/1668995632873234435)\n  7. We now incorporate the FLARE technique for a knowledge-augmented long-form generation. FLARE uses iterative retrieval to construct extended content, deciding to perform retrieval with each sentence. Unlike conventional vector index methods, our FLARE implementation builds a template iteratively, filling gaps with retrieval for more pertinent responses. Please note, this is a beta feature and works best with GPT-4. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/query_engine/flare_query_engine.html) , [ Tweet ](https://twitter.com/jerryjliu0/status/1669719509987643392?s=20)\n  8. We now employ the Maximal Marginal Relevance (MMR) algorithm to enhance diversity and minimize redundancy in retrieved results. This technique measures the similarity between a candidate document and the query while minimizing similarity with previous documents, depending on a user-specified threshold. Please note that careful calibration is necessary to ensure that increased diversity doesn\u2019t introduce irrelevant context. The threshold value is key to balancing diversity and relevance. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/SimpleIndexDemoMMR.html) , [ Tweet ](https://twitter.com/llama_index/status/1669801174109925377?s=20)\n  9. We now support recursive Pydantic objects for complex schema extraction. This enhancement, inspired by parsing directory trees, employs a mix of recursive (Node) and non-recursive (DirectoryTree) Pydantic models, facilitating more sophisticated agent-tool interactions. [ Tweet ](https://twitter.com/jerryjliu0/status/1670823521801621505?s=20)\n  10. We have developed agents that can perform advanced query planning over data using the Function API and Pydantic. These agents input a full Pydantic graph in the function signature of a query plan tool, which is then executed. This system can work with any tool and has the potential to construct complex query plans. However, it has limitations like difficulty in producing deep nesting and the possibility of outputting invalid responses. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/agent/openai_agent_query_plan.html) , [ Tweet ](https://twitter.com/jerryjliu0/status/1671183584072470529?s=20)\n  11. `OpenAIAgent` is capable of advanced data retrieval and analysis, such as auto-vector database retrieval and joint text-to-SQL and semantic search. We have also built a query plan tool interface that allows the agent to generate structured/nested query plans, which can then be executed against any set of tools, enabling advanced reasoning and analysis. Docs: [ OpenAI Agent + Query Engine ](https://gpt-index.readthedocs.io/en/latest/examples/agent/openai_agent_query_cookbook.html) , [ Retrieval Augmented OpenAI Agent ](https://gpt-index.readthedocs.io/en/latest/examples/agent/openai_agent_retrieval.html) , [ OpenAI Agent Query Planning ](https://gpt-index.readthedocs.io/en/latest/examples/agent/openai_agent_query_plan.html) . [ Tweet ](https://twitter.com/llama_index/status/1671185213538578433?s=20)\n  12. The new multi-router feature allows for QA over complex data collections, where answers may be spread across multiple sources. It uses a \u201cMultiSelector\u201d object to select relevant choices given a query. The router can pick up to a maximum number of choices. It can use either a raw LLM completion API or the OpenAI Function API. If the Function API is used, schema validity can be enforced. A simple usage example involves a RouterQueryEngine, where the PydanticMultiSelector selects the relevant vector and keyword index to synthesize an answer. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/query_engine/RouterQueryEngine.html#define-router-query-engine) , [ Tweet ](https://twitter.com/jerryjliu0/status/1671536412498477057?s=20)\n  13. We have made a significant upgrade to our token tracking feature. Users can now easily track prompt, completion, and embedding tokens through the platform\u2019s callback handler. The upgrade aims to make token counting more efficient and user-friendly. [ Docs ](https://gpt-index.readthedocs.io/en/latest/how_to/callbacks/token_counting_migration.html) , [ Tweet ](https://twitter.com/llama_index/status/1671893230412247042?s=20)\n  14. We released a guide that demonstrates how to build a custom retriever that combines vector similarity search with knowledge graphs in LLM RAG systems. It involves constructing a vector index and a knowledge graph index and combining the results from both during query time. This method can improve results by providing additional context for entities. However, it may lead to a slight increase in latency. [ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.html) , [ Tweet ](https://twitter.com/jerryjliu0/status/1671895098270031872?s=20)\n  15. In an LLM workflow, managing large amounts of data, including PDFs, agent Tools, SQL table schemas, etc., requires efficient indexing. To handle this, we introduce our Object Index, a wrapper over our existing index data structures. This allows any object to be converted into an indexable text format, providing a unified interface that enhances the functionality of our indices over various data types. [ Tweet ](https://twitter.com/jerryjliu0/status/1672263302628646912?s=20)\n  16. The OpenBB Finance Terminal is a great platform for investment research and is completely open-source. It now includes a feature called AskOBB, powered by Llama Index, which allows users to easily access any financial data through natural language. [ Tweet ](https://twitter.com/jerryjliu0/status/1672637698136489989?s=20)\n  17. The TruLens team has introduced tracing for LlamaIndex-based LLM applications in its latest release. This new feature allows developers to evaluate and track their experiments more efficiently. It automatically evaluates various components of the application stack, including app inputs and outputs, LLM calls, retrieved-context chunks from an index, and latency. This is part of an ongoing collaboration between the LlamaIndex and TruLens teams to improve the development, evaluation, and iteration of LLM apps. [ Notebook ](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.3.0/trulens_eval/examples/vector-dbs/llama_index/quickstart.ipynb) , [ Blogpost ](https://medium.com/llamaindex-blog/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c)\n  18. Prem App has successfully integrated with Llama Index, enhancing privacy in AI development. This union allows developers to connect custom data sources to large language models easily, simplifying data ingestion, indexing, and querying. To use this integration, download the Prem App and connect your data sources through the Llama Index platform. This allows for efficient data management and boosts AI application development, providing developers with more control and flexibility. [ Notebook ](https://github.com/premAI-io/prem-daemon/blob/main/resources/notebooks/llama_index.ipynb) , [ Blogpost ](https://medium.com/llamaindex-blog/llama-index-prem-ai-join-forces-51702fecedec)\n  19. We now enable the extraction of tabular data frames from unstructured text. This feature, powered by the OpenAI Function API and Pydantic models, simplifies text-to-SQL or text-to-DF conversions within structured data workflows. Note that effective use may require significant prompt optimization. ", "mimetype": "text/plain", "start_char_idx": 362, "end_char_idx": 10605, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2f6b7060-177f-45d4-85ca-6cf6b95f9cef": {"__data__": {"id_": "2f6b7060-177f-45d4-85ca-6cf6b95f9cef", "embedding": null, "metadata": {"filename": "llamaindex-update-6-26-2023-ed30a9d45f84.md", "extension": ".md", "title": "LlamaIndex Update \u2014 06/26/2023", "date": "Jun 26, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-6-26-2023-ed30a9d45f84"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "29b9e75b-2597-42da-b8a7-a40eba97e76a", "node_type": "4", "metadata": {"filename": "llamaindex-update-6-26-2023-ed30a9d45f84.md", "extension": ".md", "title": "LlamaIndex Update \u2014 06/26/2023", "date": "Jun 26, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-6-26-2023-ed30a9d45f84"}, "hash": "09b7b0c11eab2b56d52d4a3135dfadfacf851b27bbcfecf650d2f53cc8ebf0d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e28414a9-144b-421a-be57-4a41ba3ecb0a", "node_type": "1", "metadata": {"filename": "llamaindex-update-6-26-2023-ed30a9d45f84.md", "extension": ".md", "title": "LlamaIndex Update \u2014 06/26/2023", "date": "Jun 26, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-6-26-2023-ed30a9d45f84"}, "hash": "2e5482aaedac72e29ad5ec5ac60f38d4fb981e82c132811b4e8e9fb6a7ee1570", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "473b6047-54ee-4838-98e3-335210a020e8", "node_type": "1", "metadata": {}, "hash": "cf4665319a49d37caaf038734243699575c6af902bd0c23365bd625c6c389214", "class_name": "RelatedNodeInfo"}}, "text": "[ Docs ](https://gpt-index.readthedocs.io/en/latest/examples/output_parsing/df_output_parser.html) , [ Tweet ](https://twitter.com/jerryjliu0/status/1673004155227750401?s=20)\n\n**Tutorials:**\n\n  1. [ James Brigg\u2019s tutorial ](https://www.youtube.com/watch?v=WKvAWub8VCU&t=2s) on using LlamaIndex with Pinecone. \n  2. [ Jerry Liu's tutorial ](https://weaviate.io/blog/llamaindex-and-weaviate) on using LlamaIndex with Weaviate. \n  3. [ Sophia Yang tutorial ](https://www.youtube.com/watch?v=cNMYeW2mpBs) on LlamaIndex overview, Use cases, and integration with LangChain. \n  4. Anil Chandra Naidu is building a [ course ](https://github.com/SamurAIGPT/LlamaIndex-course) on LlamaIndex. The course presently covers topics such as introduction, fundamentals, and data connectors. \n  5. [ OpenAI cookbook by Simon ](https://github.com/openai/openai-cookbook/blob/main/examples/third_party_examples/financial_document_analysis_with_llamaindex.ipynb) on how to perform financial analysis with LlamaIndex. \n\n##  **Webinars And Podcasts:**\n\n  1. ", "mimetype": "text/plain", "start_char_idx": 10605, "end_char_idx": 11640, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "473b6047-54ee-4838-98e3-335210a020e8": {"__data__": {"id_": "473b6047-54ee-4838-98e3-335210a020e8", "embedding": null, "metadata": {"filename": "llamaindex-update-6-26-2023-ed30a9d45f84.md", "extension": ".md", "title": "LlamaIndex Update \u2014 06/26/2023", "date": "Jun 26, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-6-26-2023-ed30a9d45f84"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "29b9e75b-2597-42da-b8a7-a40eba97e76a", "node_type": "4", "metadata": {"filename": "llamaindex-update-6-26-2023-ed30a9d45f84.md", "extension": ".md", "title": "LlamaIndex Update \u2014 06/26/2023", "date": "Jun 26, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-6-26-2023-ed30a9d45f84"}, "hash": "09b7b0c11eab2b56d52d4a3135dfadfacf851b27bbcfecf650d2f53cc8ebf0d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f6b7060-177f-45d4-85ca-6cf6b95f9cef", "node_type": "1", "metadata": {"filename": "llamaindex-update-6-26-2023-ed30a9d45f84.md", "extension": ".md", "title": "LlamaIndex Update \u2014 06/26/2023", "date": "Jun 26, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-6-26-2023-ed30a9d45f84"}, "hash": "3f36ff43f2146aa0c712c599124364b5b006870321d18a6f6b4f6dae31dec080", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6145080-3f9e-4d56-8aad-088f97b2b2d7", "node_type": "1", "metadata": {}, "hash": "28b23759d79815f1f6622e4c0f86b7e5552ccbdfa6152e772542a90d9b62c1e6", "class_name": "RelatedNodeInfo"}}, "text": "[ Webinar ](https://www.youtube.com/watch?v=6ot9io-brzI&t=2s) on Demonstrate-Search-Predict (DSP) with Omar Khattab. \n  2. [ Webinar ](https://www.youtube.com/watch?v=7aIzxFyJP-A&t=22s) on Practical challenges of building a Legal Chatbot over your PDFs with Sam Yu \n  3. [ MaML podcast ](https://podcasters.spotify.com/pod/show/maml-podcast/episodes/Jerry-Liu---Building-LlamaIndex--the-Data-Framework-for-LLMs-e25u3ga) with Jerry Liu. \n\n##  **Hackathons:**\n\nThe LlamaIndex team has presented at the UC Berkeley Hackathon and the\nStellaris VP Hackathon in India. The community has warmly welcomed LlamaIndex,\nand teams at these hackathons have developed intriguing use cases \u2014 Customer\nsupport during emergency cases, Understanding Legal documents.\n\n##  **Events:**\n\n  1. ", "mimetype": "text/plain", "start_char_idx": 11640, "end_char_idx": 12412, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6145080-3f9e-4d56-8aad-088f97b2b2d7": {"__data__": {"id_": "a6145080-3f9e-4d56-8aad-088f97b2b2d7", "embedding": null, "metadata": {"filename": "llamaindex-update-6-26-2023-ed30a9d45f84.md", "extension": ".md", "title": "LlamaIndex Update \u2014 06/26/2023", "date": "Jun 26, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-6-26-2023-ed30a9d45f84"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "29b9e75b-2597-42da-b8a7-a40eba97e76a", "node_type": "4", "metadata": {"filename": "llamaindex-update-6-26-2023-ed30a9d45f84.md", "extension": ".md", "title": "LlamaIndex Update \u2014 06/26/2023", "date": "Jun 26, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-6-26-2023-ed30a9d45f84"}, "hash": "09b7b0c11eab2b56d52d4a3135dfadfacf851b27bbcfecf650d2f53cc8ebf0d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "473b6047-54ee-4838-98e3-335210a020e8", "node_type": "1", "metadata": {"filename": "llamaindex-update-6-26-2023-ed30a9d45f84.md", "extension": ".md", "title": "LlamaIndex Update \u2014 06/26/2023", "date": "Jun 26, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-6-26-2023-ed30a9d45f84"}, "hash": "3bcaa1519fc9734b50ee1b6640a274bc7f89beebaf3127e74fda551e8d18fcea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f940bd71-f40e-45bf-a65c-92a855fdbb14", "node_type": "1", "metadata": {}, "hash": "c8dbddc1541d211a02b07a9a36d84fa58ed0bc9d08090fc0a10fad7d7dec1ef4", "class_name": "RelatedNodeInfo"}}, "text": "Jerry Liu spoke on Building and troubleshooting an AI Search & Retrieval System at Arize \u2014 LlamaIndex event. \n  2. Ravi Theja presented about LlamaIndex and its applications at Together in India. \n\nThat\u2019s all for this edition of the LlamaIndex Update. We hope you found this\ninformation useful and are as excited as we are about the progress we\u2019re\nmaking. ", "mimetype": "text/plain", "start_char_idx": 12412, "end_char_idx": 12768, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f940bd71-f40e-45bf-a65c-92a855fdbb14": {"__data__": {"id_": "f940bd71-f40e-45bf-a65c-92a855fdbb14", "embedding": null, "metadata": {"filename": "llamaindex-update-6-26-2023-ed30a9d45f84.md", "extension": ".md", "title": "LlamaIndex Update \u2014 06/26/2023", "date": "Jun 26, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-6-26-2023-ed30a9d45f84"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "29b9e75b-2597-42da-b8a7-a40eba97e76a", "node_type": "4", "metadata": {"filename": "llamaindex-update-6-26-2023-ed30a9d45f84.md", "extension": ".md", "title": "LlamaIndex Update \u2014 06/26/2023", "date": "Jun 26, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-6-26-2023-ed30a9d45f84"}, "hash": "09b7b0c11eab2b56d52d4a3135dfadfacf851b27bbcfecf650d2f53cc8ebf0d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6145080-3f9e-4d56-8aad-088f97b2b2d7", "node_type": "1", "metadata": {"filename": "llamaindex-update-6-26-2023-ed30a9d45f84.md", "extension": ".md", "title": "LlamaIndex Update \u2014 06/26/2023", "date": "Jun 26, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-6-26-2023-ed30a9d45f84"}, "hash": "1206c66775230c4a51b2258318ebd0ca82d8c401f5205d59a2e0750367614348", "class_name": "RelatedNodeInfo"}}, "text": "We\u2019re grateful for the continued support and contributions from our\ncommunity. Remember, your feedback and suggestions are invaluable to us, so\ndon\u2019t hesitate to reach out.\n\nStay tuned for our next update, where we\u2019ll share more exciting developments\nfrom the LlamaIndex project. Until then, happy indexing!\n\n", "mimetype": "text/plain", "start_char_idx": 12768, "end_char_idx": 13077, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81774e96-39f8-4062-8529-771906d1b28d": {"__data__": {"id_": "81774e96-39f8-4062-8529-771906d1b28d", "embedding": null, "metadata": {"filename": "build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.md", "extension": ".md", "title": "Build and Evaluate LLM Apps with LlamaIndex and TruLens", "date": "Jun 23, 2023", "url": "https://www.llamaindex.ai/blog/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65437f97-2e48-4ffb-a733-840a4165e5da", "node_type": "4", "metadata": {"filename": "build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.md", "extension": ".md", "title": "Build and Evaluate LLM Apps with LlamaIndex and TruLens", "date": "Jun 23, 2023", "url": "https://www.llamaindex.ai/blog/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c"}, "hash": "409c03c9cd4a855f46a81c8f2da97891a73e5082cf7a403a9cbb1b4cf383f62f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91f8a5a2-095f-4b83-a757-a744f8b6f815", "node_type": "1", "metadata": {}, "hash": "21092ba779b3f5bfb01db70fc0669d0a68d4c96a962000aebea994c0eb445c5d", "class_name": "RelatedNodeInfo"}}, "text": "**Authors:** Anupam Datta, Shayak Sen, Jerry Liu, Simon Suo\n\n**Source Link:** [ https://truera.com/build-and-evaluate-llm-apps-with-\nllamaindex-and-trulens/ ](https://truera.com/build-and-evaluate-llm-apps-with-\nllamaindex-and-trulens/)\n\nLlamaIndex is a popular open source framework for building LLM apps. TruLens\nis an open source library for evaluating, tracking, and iterating on LLM apps\nto improve their quality. The LlamaIndex and TruLens teams are actively\ncollaborating to enable LLM app developers to rapidly build, evaluate, and\niterate on their apps.\n\nIn the latest release of TruLens, we introduce tracing for LlamaIndex based\nLLM applications that allow you to evaluate and track your experiments with\njust a few lines of code. This lets you automatically evaluate a number of\ndifferent components of the application stack including:\n\n  * App inputs and outputs \n  * LLM calls \n  * Retrieved context chunks from an index \n  * Latency \n  * Cost and Token Counts (coming soon!) \n\nCheck out this [ notebook\n](https://github.com/truera/trulens/blob/releases/rc-trulens-\neval-0.3.0/trulens_eval/examples/vector-dbs/llama_index/quickstart.ipynb) to\nget started and read along to get a step by step view.\n\n#  How do I actually use this?\n\n##  Build A LlamaIndex App\n\nLlamaIndex lets you connect your data to LLMs and rapidly build applications\nfor a number of different use cases.\n\n    \n    \n    from llama_index import VectorStoreIndex, SimpleDirectoryReader\n    \n    documents = SimpleDirectoryReader('llama_index/data').load_data()\n    index = VectorStoreIndex.from_documents(documents)\n    \n    query_engine = index.as_query_engine()\n\nOnce you build your app, you can easily query your data:\n\n    \n    \n    response = query_engine.query(\"What did the author do growing up?\")\n    print(response)\n\nAnd you get an appropriate response.\n\n    \n    \n    ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1858, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "91f8a5a2-095f-4b83-a757-a744f8b6f815": {"__data__": {"id_": "91f8a5a2-095f-4b83-a757-a744f8b6f815", "embedding": null, "metadata": {"filename": "build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.md", "extension": ".md", "title": "Build and Evaluate LLM Apps with LlamaIndex and TruLens", "date": "Jun 23, 2023", "url": "https://www.llamaindex.ai/blog/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65437f97-2e48-4ffb-a733-840a4165e5da", "node_type": "4", "metadata": {"filename": "build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.md", "extension": ".md", "title": "Build and Evaluate LLM Apps with LlamaIndex and TruLens", "date": "Jun 23, 2023", "url": "https://www.llamaindex.ai/blog/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c"}, "hash": "409c03c9cd4a855f46a81c8f2da97891a73e5082cf7a403a9cbb1b4cf383f62f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81774e96-39f8-4062-8529-771906d1b28d", "node_type": "1", "metadata": {"filename": "build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.md", "extension": ".md", "title": "Build and Evaluate LLM Apps with LlamaIndex and TruLens", "date": "Jun 23, 2023", "url": "https://www.llamaindex.ai/blog/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c"}, "hash": "078cad91b5a52f15b3fddf62ecb759764f4278b6e7fc3f3bbce9a1188ce03eef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ddad1e0-88a5-44f5-b252-49f2ab85e91e", "node_type": "1", "metadata": {}, "hash": "79a9f66e613e798cd021172397ba4bc4a7ee2846a191b61c08557f05575c433d", "class_name": "RelatedNodeInfo"}}, "text": "Growing up, the author wrote short stories, programmed on an IBM 1401, and nagged his father to buy him a TRS-80 microcomputer. He wrote simple games, a program to predict how high his model rockets would fly, and a word processor. He also studied philosophy in college, but switched to AI after becoming bored with it. ", "mimetype": "text/plain", "start_char_idx": 1858, "end_char_idx": 2178, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ddad1e0-88a5-44f5-b252-49f2ab85e91e": {"__data__": {"id_": "1ddad1e0-88a5-44f5-b252-49f2ab85e91e", "embedding": null, "metadata": {"filename": "build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.md", "extension": ".md", "title": "Build and Evaluate LLM Apps with LlamaIndex and TruLens", "date": "Jun 23, 2023", "url": "https://www.llamaindex.ai/blog/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65437f97-2e48-4ffb-a733-840a4165e5da", "node_type": "4", "metadata": {"filename": "build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.md", "extension": ".md", "title": "Build and Evaluate LLM Apps with LlamaIndex and TruLens", "date": "Jun 23, 2023", "url": "https://www.llamaindex.ai/blog/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c"}, "hash": "409c03c9cd4a855f46a81c8f2da97891a73e5082cf7a403a9cbb1b4cf383f62f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "91f8a5a2-095f-4b83-a757-a744f8b6f815", "node_type": "1", "metadata": {"filename": "build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.md", "extension": ".md", "title": "Build and Evaluate LLM Apps with LlamaIndex and TruLens", "date": "Jun 23, 2023", "url": "https://www.llamaindex.ai/blog/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c"}, "hash": "bb4ecc0ea53f2615ae18eebda72e7c0443c0683ffcf5ab05cab17a474f78bac5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a3b996c-2ed9-4789-b7ce-4610d4ebc18a", "node_type": "1", "metadata": {}, "hash": "67e7428f056062818bb02245651b223006281852cd01b91568b79871ffdcaf59", "class_name": "RelatedNodeInfo"}}, "text": "He then took art classes at Harvard and applied to art schools, eventually attending RISD.\n\n##  Wrap A LlamaIndex App with TruLens\n\nWith TruLens, you can wrap LlamaIndex query engines with a TruLlama wrapper.\nThis wrapper preserves all LlamaIndex behavior, but traces all of the\nintermediate steps so that they can be individually evaluated.\n\n    \n    \n    from trulens_eval import TruLlama\n    l = TruLlama(query_engine)\n\nThe wrapped app can now be queried in the exact same way:\n\n    \n    \n    response = l.query(\"What did the author do growing up?\")\n    print(response)\n\nExcept, now the details of the query are logged by TruLens.\n\n##  Add Feedback Functions\n\nNow to evaluate the behavior of your models, we can add feedback functions to\nyour wrapped application. Note that as a developer you only need to **add a\nfew lines of code** to start using feedback functions in your apps. You can\nalso easily add functions tailored to the needs of your application.\n\nOur goal with feedback functions is to programmatically check the app for\nquality metrics.\n\n  * The first feedback function checks for language match between the prompt and the response. It\u2019s a useful check since a natural user expectation is that the response is in the same language as the prompt. It is implemented with a call to a HuggingFace API that programmatically checks for language match. \n  * The next feedback function checks how relevant the answer is to the question by using an Open AI LLM that is prompted to produce a relevance score. \n  * Finally, the third feedback function checks how relevant individual chunks retrieved from the vector database are to the question, again using an OpenAI LLM in a similar manner. This is useful because the retrieval step from a vector database may produce chunks that are not relevant to the question and the quality of the final response would be better if these chunks are filtered out before producing the final response. \n\n    \n    \n    from trulens_eval import TruLlama, Tru, Query, Feedback, feedback\n    \n    # Initialize Huggingface-based feedback function collection class:\n    hugs = feedback.Huggingface()\n    openai = feedback.OpenAI()\n    # Define a language match feedback function using HuggingFace.\n    f_lang_match = Feedback(hugs.language_match).on_input_output()\n    # By default this will check language match on the main app input and main app\n    # output.\n    \n    # Question/answer relevance between overall question and answer.\n    f_qa_relevance = Feedback(openai.relevance).on_input_output()\n    \n    # Question/statement relevance between question and each context chunk.\n    f_qs_relevance = Feedback(openai.qs_relevance).on_input().on(\n        TruLlama.select_source_nodes().node.text\n    ).aggregate(np.min)\n    \n    \n    feedbacks = [f_lang_match, f_qa_relevance, f_qs_relevance]\n    \n    l = TruLlama(app=query_engine, feedbacks=feedbacks)\n\n##  Explore In Dashboard\n\nEvery query that is tracked can now be viewed in the TruLens dashboard. After\nrunning the feedback functions on a set of records (interactions), you can see\nthe aggregate results of the evaluation on a leaderboard; then drill down into\nan app version and examine how it is performing on individual records. These\nsteps can help you understand the quality of an app version and its failure\nmodes.\n\nIn this example, the model is doing fairly well on the relevance and language\nmatch feedback evaluations, but seems to be doing poorly on qs_relevance. This\ncan be an indicator that the retrieved chunks are often irrelevant. This can\nbe a significant source of \u201challucinations\u201d in retrieval-augmented generative\nAI apps.\n\nWe can now drill down and identify specific instances where this may be an\nissue:\n\nLet\u2019s look at a good example first. ", "mimetype": "text/plain", "start_char_idx": 2178, "end_char_idx": 5936, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a3b996c-2ed9-4789-b7ce-4610d4ebc18a": {"__data__": {"id_": "1a3b996c-2ed9-4789-b7ce-4610d4ebc18a", "embedding": null, "metadata": {"filename": "build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.md", "extension": ".md", "title": "Build and Evaluate LLM Apps with LlamaIndex and TruLens", "date": "Jun 23, 2023", "url": "https://www.llamaindex.ai/blog/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65437f97-2e48-4ffb-a733-840a4165e5da", "node_type": "4", "metadata": {"filename": "build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.md", "extension": ".md", "title": "Build and Evaluate LLM Apps with LlamaIndex and TruLens", "date": "Jun 23, 2023", "url": "https://www.llamaindex.ai/blog/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c"}, "hash": "409c03c9cd4a855f46a81c8f2da97891a73e5082cf7a403a9cbb1b4cf383f62f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ddad1e0-88a5-44f5-b252-49f2ab85e91e", "node_type": "1", "metadata": {"filename": "build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.md", "extension": ".md", "title": "Build and Evaluate LLM Apps with LlamaIndex and TruLens", "date": "Jun 23, 2023", "url": "https://www.llamaindex.ai/blog/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c"}, "hash": "798d1b617ae21cbc53e417ed263464f3d0928f690cd69ccf8a8020fd6fbd53ab", "class_name": "RelatedNodeInfo"}}, "text": "\u201cWhat did the author do growing up?\u201d\n\nIn this example, we retrieved two chunks from the index both of which were\nfairly relevant to the the question and as a result the LLM summarizes it into\na relevant and factually correct answer.\n\nOn the other hand, let\u2019s look at an example where this didn\u2019t go so well:\n\u201cWhere was the author born?\u201d. In this example, the app confidently provides an\nincorrect answer.\n\nIn this example, the two pieces of context retrieved had moderate relevance to\nthe question. Further, neither context contained the answer. Even though our\nrelevance feedback function (which doesn\u2019t check for factual correctness)\ndidn\u2019t detect an issue, because the underlying chunks were not very relevant,\nthis was a strong indicator that something was off. Indeed, this is an example\nof the model hallucinating on a question that is fairly easy to fact check.\n\n##  Iterate on your App\n\nOnce you find issues like this with your app, it can be helpful to iterate on\nyour prompts, models and chunking approaches to optimize your app. As you do\nthis, you can track the performance of each version of your model with\nTruLens. Here is an example of a dashboard with multiple iterations testing\nagainst each other.\n\n", "mimetype": "text/plain", "start_char_idx": 5936, "end_char_idx": 7154, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2fa8afd8-68bf-4c74-8234-fdd3f1417a88": {"__data__": {"id_": "2fa8afd8-68bf-4c74-8234-fdd3f1417a88", "embedding": null, "metadata": {"filename": "llama-index-prem-ai-join-forces-51702fecedec.md", "extension": ".md", "title": "Llama Index & Prem AI Join Forces", "date": "Jun 23, 2023", "url": "https://www.llamaindex.ai/blog/llama-index-prem-ai-join-forces-51702fecedec"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "da12eaf4-bd48-4e41-b6e6-a80d2aa6b208", "node_type": "4", "metadata": {"filename": "llama-index-prem-ai-join-forces-51702fecedec.md", "extension": ".md", "title": "Llama Index & Prem AI Join Forces", "date": "Jun 23, 2023", "url": "https://www.llamaindex.ai/blog/llama-index-prem-ai-join-forces-51702fecedec"}, "hash": "8626181942debbb38f83d9de516255d0cc05b6113477ff0bf09817fc5e21e375", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbd261f1-2c7f-4ed1-9615-a7258db8cc8d", "node_type": "1", "metadata": {}, "hash": "2dee510e68892e166f62066685bd9d696468a3bc8024f60f48f475576f78facb", "class_name": "RelatedNodeInfo"}}, "text": "**Co-authors:** Simone Giacomelli (co-founder at Prem), Jerry Liu (co-\nfounder/CEO at LlamaIndex)\n\nWe\u2019re pleased to share the successful integration of Prem App and Llama Index,\na union that brings a new level of privacy to AI development. Prem\u2019s self-\nhosting AI models and Llama\u2019s versatile data framework enhances the ability to\nbuild AI applications in a customizable and flexible manner.\n\n##  Integration Details\n\nBy combining Prem\u2019s self-hosting AI models with Llama Index\u2019s data framework,\ndevelopers can now connect custom data sources to large language models\neasily. This simplifies the process of data ingestion, indexing, and querying,\nstreamlining the overall AI development cycle.\n\n##  Getting Started\n\nTo leverage this integration, simply download the Prem App and connect your\ndata sources through the Llama Index platform. This allows you to self-host\nyour AI models with Prem App and utilize Llama Index\u2019s capabilities to manage\nyour data efficiently. This integration, therefore, significantly boosts AI\napplication development, giving developers greater control and flexibility\nover their projects.\n\n#  Getting Started\n\n##  Install Prem\n\nYou can run Prem in two different ways:\n\n  * MacOS: go to [ https://premai.io ](https://premai.io) and download Prem App. \n  * Server: run the installer script: ` wget -q <https://get.prem.ninja/install.sh> -O install.sh; sudo bash ./install.sh `\n\n##  Run the services in the GUI\n\nWhen the UI is up and running, you can see all the services available. With\njust one click you can download the service you are interested in. In the\nbackground, the docker image associated with the service will be downloaded\nbased on your hardware requirements.\n\nWhile waiting for the download to be completed, read more about the service,\nin the detail view. Just click on the card and you will be redirected to the\nservice page. Each service page is packaged with some general info as well as\ncomplete documentation giving more details into the model exposed. When the\ndownload has been completed, just click on Open and the service will start.\nYou can interact with the service from the playground or from APIs.\n\nYou can check the port on which the service is running from the service detail\nview.\n\n#  Start Building Your App\n\nIn this quick tutorial will show you how to build a simple Talk to your Data\nuse case using Prem landing page content.\n\nIn order to achieve that we will need to run three services:\n\n  * Redis: we will use Redis as a vector store to store the embeddings. \n  * Vicuna 7B Q4: we will use Vicuna in order to generate a proper response for the user based on the most similar document we get using Redis similarity search \n  * All MiniLM L6 V2: we will use sentence transformers in order to generate the embeddings out of our documents. \n\nIf all the services necessary are running, you will see a similar interface as\nthe one beyond.\n\n", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2899, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cbd261f1-2c7f-4ed1-9615-a7258db8cc8d": {"__data__": {"id_": "cbd261f1-2c7f-4ed1-9615-a7258db8cc8d", "embedding": null, "metadata": {"filename": "llama-index-prem-ai-join-forces-51702fecedec.md", "extension": ".md", "title": "Llama Index & Prem AI Join Forces", "date": "Jun 23, 2023", "url": "https://www.llamaindex.ai/blog/llama-index-prem-ai-join-forces-51702fecedec"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "da12eaf4-bd48-4e41-b6e6-a80d2aa6b208", "node_type": "4", "metadata": {"filename": "llama-index-prem-ai-join-forces-51702fecedec.md", "extension": ".md", "title": "Llama Index & Prem AI Join Forces", "date": "Jun 23, 2023", "url": "https://www.llamaindex.ai/blog/llama-index-prem-ai-join-forces-51702fecedec"}, "hash": "8626181942debbb38f83d9de516255d0cc05b6113477ff0bf09817fc5e21e375", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2fa8afd8-68bf-4c74-8234-fdd3f1417a88", "node_type": "1", "metadata": {"filename": "llama-index-prem-ai-join-forces-51702fecedec.md", "extension": ".md", "title": "Llama Index & Prem AI Join Forces", "date": "Jun 23, 2023", "url": "https://www.llamaindex.ai/blog/llama-index-prem-ai-join-forces-51702fecedec"}, "hash": "38c098cc29e9ad713a33942ab756029a8cc1ecc595114bcace3107990d2aebf5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4df2592-d90f-49a3-8e57-191bea270f84", "node_type": "1", "metadata": {}, "hash": "7cea40b1d6cf42f7304f9114417ab04c96e928525f30d094545e959f29838b1e", "class_name": "RelatedNodeInfo"}}, "text": "You can now start integrating the services using Llama Index library. In the\nfollowing code snippets, we will show you how you can build a simple talk to\nyour data use case using Prem and Llama Index.\n\n  1. Import all necessary dependencies and assign a random string to ` OPENAI_API_KEY ` environment variable. \n\n    \n    \n    import os\n    \n    from llama_index.vector_stores import RedisVectorStore\n    from llama_index.storage.storage_context import StorageContext\n    from llama_index import ListIndex, LLMPredictor, Document\n    \n    from langchain.chat_models import ChatOpenAI\n    from langchain.embeddings.openai import OpenAIEmbeddings\n    \n    from llama_index import LangchainEmbedding, ServiceContext\n    \n    os.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\n2\\. Load the Data / Create some Documents. In this example, I am using Prem\nlanding page content creating manually some documents.\n\n    \n    \n    doc1 = Document(text=\"Prem is an easy to use open source AI platform. With Prem you can quickly build privacy preserving AI applications.\")\n    doc2 = Document(text=\"\"\"\n    Prem App\n    \n    An intuitive desktop application designed to effortlessly deploy and self-host Open-Source AI models without exposing sensitive data to third-party.\n    \n    \"\"\")\n    doc3 = Document(text=\"\"\"\n    Prem Benefits\n    \n    Effortless Integration\n    Seamlessly implement machine learning models with the user-friendly interface of OpenAI's API.\n    \n    Ready for the Real World\n    Bypass the complexities of inference optimizations. Prem's got you covered.\n    \n    Rapid Iterations, Instant Results\n    Develop, test, and deploy your models in just minutes.\n    \n    Privacy Above All\n    Your keys, your models. We ensure end-to-end encryption.\n    \n    Comprehensive Documentation\n    Dive into our rich resources and learn how to make the most of Prem.\n    \n    Preserve Your Anonymity\n    Make payments with Bitcoin and Cryptocurrency. It's a permissionless infrastructure, designed for you.\n    \"\"\")\n\n", "mimetype": "text/plain", "start_char_idx": 2899, "end_char_idx": 4912, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4df2592-d90f-49a3-8e57-191bea270f84": {"__data__": {"id_": "b4df2592-d90f-49a3-8e57-191bea270f84", "embedding": null, "metadata": {"filename": "llama-index-prem-ai-join-forces-51702fecedec.md", "extension": ".md", "title": "Llama Index & Prem AI Join Forces", "date": "Jun 23, 2023", "url": "https://www.llamaindex.ai/blog/llama-index-prem-ai-join-forces-51702fecedec"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "da12eaf4-bd48-4e41-b6e6-a80d2aa6b208", "node_type": "4", "metadata": {"filename": "llama-index-prem-ai-join-forces-51702fecedec.md", "extension": ".md", "title": "Llama Index & Prem AI Join Forces", "date": "Jun 23, 2023", "url": "https://www.llamaindex.ai/blog/llama-index-prem-ai-join-forces-51702fecedec"}, "hash": "8626181942debbb38f83d9de516255d0cc05b6113477ff0bf09817fc5e21e375", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbd261f1-2c7f-4ed1-9615-a7258db8cc8d", "node_type": "1", "metadata": {"filename": "llama-index-prem-ai-join-forces-51702fecedec.md", "extension": ".md", "title": "Llama Index & Prem AI Join Forces", "date": "Jun 23, 2023", "url": "https://www.llamaindex.ai/blog/llama-index-prem-ai-join-forces-51702fecedec"}, "hash": "9fface649e3d7107b00d7c78d06d0cc413cfeef31d3dfc14995c5bffd4fd2277", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "424a04cb-7c54-4c64-b68b-6aecc03dcc94", "node_type": "1", "metadata": {}, "hash": "839fab781538c23a0cb436a58428584ae6c0c492ef1c2ce77dc00bdad6391502", "class_name": "RelatedNodeInfo"}}, "text": "3\\. ", "mimetype": "text/plain", "start_char_idx": 4912, "end_char_idx": 4916, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "424a04cb-7c54-4c64-b68b-6aecc03dcc94": {"__data__": {"id_": "424a04cb-7c54-4c64-b68b-6aecc03dcc94", "embedding": null, "metadata": {"filename": "llama-index-prem-ai-join-forces-51702fecedec.md", "extension": ".md", "title": "Llama Index & Prem AI Join Forces", "date": "Jun 23, 2023", "url": "https://www.llamaindex.ai/blog/llama-index-prem-ai-join-forces-51702fecedec"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "da12eaf4-bd48-4e41-b6e6-a80d2aa6b208", "node_type": "4", "metadata": {"filename": "llama-index-prem-ai-join-forces-51702fecedec.md", "extension": ".md", "title": "Llama Index & Prem AI Join Forces", "date": "Jun 23, 2023", "url": "https://www.llamaindex.ai/blog/llama-index-prem-ai-join-forces-51702fecedec"}, "hash": "8626181942debbb38f83d9de516255d0cc05b6113477ff0bf09817fc5e21e375", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b4df2592-d90f-49a3-8e57-191bea270f84", "node_type": "1", "metadata": {"filename": "llama-index-prem-ai-join-forces-51702fecedec.md", "extension": ".md", "title": "Llama Index & Prem AI Join Forces", "date": "Jun 23, 2023", "url": "https://www.llamaindex.ai/blog/llama-index-prem-ai-join-forces-51702fecedec"}, "hash": "5b94bd4b99bdd73b9f3f2b4505a86907c0c1382afd0e22624d6c47970b5fde22", "class_name": "RelatedNodeInfo"}}, "text": "Instantiate the LLMs connecting to the running services.\n\n    \n    \n    # Instantiate a llm predictor using Langchain pointing to vicuna-7b-q4 service\n    llm_predictor = LLMPredictor(llm=ChatOpenAI(openai_api_base=\"http://localhost:8111/api/v1\", max_tokens=128))\n    \n    # Instantiate the embeddings object using Langchain pointing to all-MiniLM-L6-v2 service\n    embeddings = OpenAIEmbeddings(openai_api_base=\"http://localhost:8444/api/v1\")\n    embed_model = LangchainEmbedding(embeddings)\n    \n    # define a service context using the embeddings and llm defined above.\n    service_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor=llm_predictor)\n\n4\\. Configure the Vector Store\n\n    \n    \n    # instantiate the vectorstore connecting to Redis service\n    vector_store = RedisVectorStore(\n        index_name=\"prem_landing\",\n        index_prefix=\"llama\",\n        redis_url=\"redis://localhost:6379\",\n        overwrite=True\n    )\n    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n\n5\\. Index the documents\n\n    \n    \n    index = ListIndex.from_documents([doc1, doc2, doc3], storage_context=storage_context)\n\n6\\. Perform an example query\n\n    \n    \n    query_engine = index.as_query_engine(\n        retriever_mode=\"embedding\", \n        verbose=True, \n        service_context=service_context\n    )\n    response = query_engine.query(\"What are Prem benefits?\")\n    print(response)\n    \n    \n    The benefits of using Prem include: Effortless Integration, Ready for the Real World, Rapid Iterations, Instant Results, Privacy Above All, Comprehensive Documentation, Preserve Your Anonymity, and an intuitive desktop application designed to effortlessly deploy and self-host Open-Source AI models without exposing sensitive data to third-party.\n\nAnd Done You are now using Prem with Llama Index.\n\n#  More Information\n\nCheck out our documentation at: [ https://github.com/premai-io/prem-app\n](https://github.com/premai-io/prem-app)\n\nCheck out a simple talk to your data notebook with Llama Index: [\nhttps://github.com/premAI-io/prem-\ndaemon/blob/main/resources/notebooks/llama_index.ipynb\n](https://github.com/premAI-io/prem-\ndaemon/blob/main/resources/notebooks/llama_index.ipynb)\n\nCheckout our YouTube tutorials\n\n  * Getting Started with Prem: [ https://www.youtube.com/watch?v=XixH46Ysl5A ](https://www.youtube.com/watch?v=XixH46Ysl5A)\n  * Deploy Prem in your Paperspace instance: [ https://www.youtube.com/watch?v=aW8t6wouwx0 ](https://www.youtube.com/watch?v=aW8t6wouwx0)\n\n#  Join Us\n\nOur partnership is based on a shared understanding that the future of AI is\nopen, composable, and privacy-centric.\n\n[ Join us on this journey ](https://discord.com/invite/kpKk6vYVAn) !\n\n", "mimetype": "text/plain", "start_char_idx": 4916, "end_char_idx": 7639, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3421d7be-aea9-4025-900f-b5cd128185b2": {"__data__": {"id_": "3421d7be-aea9-4025-900f-b5cd128185b2", "embedding": null, "metadata": {"filename": "llamaindex-and-weaviate-ba3ff1cbf5f4.md", "extension": ".md", "title": "LlamaIndex and Weaviate", "date": "Jun 22, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-and-weaviate-ba3ff1cbf5f4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "04fe36ff-9050-4f95-a893-a90249ba18bc", "node_type": "4", "metadata": {"filename": "llamaindex-and-weaviate-ba3ff1cbf5f4.md", "extension": ".md", "title": "LlamaIndex and Weaviate", "date": "Jun 22, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-and-weaviate-ba3ff1cbf5f4"}, "hash": "da2e947a2d099ca48cce36c15c24560aa33fb951f6155cb12b449a11e5eab594", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ee2b3ff-33e3-4b9c-a1ee-4431dcdffe8e", "node_type": "1", "metadata": {}, "hash": "fbc080e77a66f945a55f05245d2a71fa2bc858a2125627b98abe2ad5928d4474", "class_name": "RelatedNodeInfo"}}, "text": "**Co-authors:**\n\n  * Jerry Liu (co-founder/CEO of LlamaIndex) \n  * Erika Cardenas (Developer Advocate, Weaviate) \n\nWhile large language models (LLMs) like GPT-4 have impressive capabilities in\ngeneration and reasoning, they have limitations in terms of their ability to\naccess and retrieve specific facts, figures, or contextually relevant\ninformation. A popular solution to this problem is setting up a retrieval-\naugmented generation (RAG) system: combine the language model with an external\nstorage provider, and create an overall software system that can orchestrate\nthe interactions with and between these components in order to create a \u201cchat\nwith your data\u201d experience.\n\nThe combination of Weaviate and LlamaIndex provide the critical components\nneeded to easily setup a powerful and reliable RAG stack, so that you can\neasily deliver powerful LLM-enabled experiences over your data, such as search\nengines, chatbots, and more. First, we can use Weaviate as the vector database\nthat acts as the external storage provider. Next, we can use a powerful data\nframework such as LlamaIndex to help with data management and orchestration\naround Weaviate when building the LLM app.\n\nIn this blog post, we walk through an overview of LlamaIndex and some of the\ncore data management and query modules. We then go through an initial demo\nnotebook.\n\nWe\u2019re kicking off a new series to guide you on how to use LlamaIndex and\nWeaviate for your LLM applications.\n\n#  An Introduction to LlamaIndex\n\nLlamaIndex is a data framework for building LLM applications. It provides a\ncomprehensive toolkit for ingestion, management, and querying of your external\ndata so that you can use it with your LLM app.\n\n##  Data Ingestion\n\nOn data ingestion, LlamaIndex offers connectors to 100+ data sources, ranging\nfrom different file formats (.pdf, .docx, .pptx) to APIs (Notion, Slack,\nDiscord, etc.) to web scrapers (Beautiful Soup, Readability, etc.). These data\nconnectors are primarily hosted on [LlamaHub]( [ https://llamahub.ai/\n](https://llamahub.ai/) ). This makes it easy for users to integrate data from\ntheir existing files and applications.\n\n##  Data Indexing\n\nOnce the data is loaded, LlamaIndex offers the ability to index this data with\na wide variety of data structures and storage integration options (including\nWeaviate). LlamaIndex supports indexing unstructured, semi-structured, and\nstructured data. A standard way to index unstructured data is to split the\nsource documents into text \u201cchunks\u201d, embed each chunk, and store each\nchunk/embedding in a vector database.\n\n##  Data Querying\n\nOnce your data is ingested/stored, LlamaIndex provides the tools to define an\nadvanced retrieval / query \u201cengine\u201d over your data. Our retriever constructs\nallow you to retrieve data from your knowledge base given an input prompt. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2814, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ee2b3ff-33e3-4b9c-a1ee-4431dcdffe8e": {"__data__": {"id_": "9ee2b3ff-33e3-4b9c-a1ee-4431dcdffe8e", "embedding": null, "metadata": {"filename": "llamaindex-and-weaviate-ba3ff1cbf5f4.md", "extension": ".md", "title": "LlamaIndex and Weaviate", "date": "Jun 22, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-and-weaviate-ba3ff1cbf5f4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "04fe36ff-9050-4f95-a893-a90249ba18bc", "node_type": "4", "metadata": {"filename": "llamaindex-and-weaviate-ba3ff1cbf5f4.md", "extension": ".md", "title": "LlamaIndex and Weaviate", "date": "Jun 22, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-and-weaviate-ba3ff1cbf5f4"}, "hash": "da2e947a2d099ca48cce36c15c24560aa33fb951f6155cb12b449a11e5eab594", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3421d7be-aea9-4025-900f-b5cd128185b2", "node_type": "1", "metadata": {"filename": "llamaindex-and-weaviate-ba3ff1cbf5f4.md", "extension": ".md", "title": "LlamaIndex and Weaviate", "date": "Jun 22, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-and-weaviate-ba3ff1cbf5f4"}, "hash": "71d26f972c1cd41bd5c7503ce3a7751277053d7691ad5928e15bcc38f09e3e83", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c777f0fd-53d4-44b7-aceb-c450f6b069ff", "node_type": "1", "metadata": {}, "hash": "2e171c6fc7fbb399a697ed4afaa14c78b49f141ec9d87725135d2ddd9bf684eb", "class_name": "RelatedNodeInfo"}}, "text": "A\nquery engine construct allows you to define an interface that can take in an\ninput prompt, and output a knowledge-augmented response \u2014 it can use retrieval\nand synthesis (LLM) modules under the hood.\n\nSome examples of query engine \u201ctasks\u201d are given below, in rough order from\neasy to advanced:\n\n  * Semantic Search: Retrieve the top-k most similar items from the knowledge corpus by embedding similarity to the query, and synthesize a response over these contexts. \n  * Structured Analytics: Convert natural language to a SQL query that can be executed \n  * Query Decomposition over Documents: Break down a query into sub-questions, each over a subset of underlying documents. Each sub-question can be executed against its own query engine. \n\n", "mimetype": "text/plain", "start_char_idx": 2814, "end_char_idx": 3559, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c777f0fd-53d4-44b7-aceb-c450f6b069ff": {"__data__": {"id_": "c777f0fd-53d4-44b7-aceb-c450f6b069ff", "embedding": null, "metadata": {"filename": "llamaindex-and-weaviate-ba3ff1cbf5f4.md", "extension": ".md", "title": "LlamaIndex and Weaviate", "date": "Jun 22, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-and-weaviate-ba3ff1cbf5f4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "04fe36ff-9050-4f95-a893-a90249ba18bc", "node_type": "4", "metadata": {"filename": "llamaindex-and-weaviate-ba3ff1cbf5f4.md", "extension": ".md", "title": "LlamaIndex and Weaviate", "date": "Jun 22, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-and-weaviate-ba3ff1cbf5f4"}, "hash": "da2e947a2d099ca48cce36c15c24560aa33fb951f6155cb12b449a11e5eab594", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ee2b3ff-33e3-4b9c-a1ee-4431dcdffe8e", "node_type": "1", "metadata": {"filename": "llamaindex-and-weaviate-ba3ff1cbf5f4.md", "extension": ".md", "title": "LlamaIndex and Weaviate", "date": "Jun 22, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-and-weaviate-ba3ff1cbf5f4"}, "hash": "29f7dcfd2a80a6c21734ac4d1c4beb95736348c70571a33309e3f553c0e4b6fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ff15592-3757-4ae9-9456-58fef7c63622", "node_type": "1", "metadata": {}, "hash": "d79f1928e7c1be4e80e6b804d8dbcf6b230101d5ceb14bf1c87e780a041a1d56", "class_name": "RelatedNodeInfo"}}, "text": "#  Demo Notebook Walkthrough\n\nLet\u2019s walk through a simple example of how LlamaIndex can be used with\nWeaviate to build a simple Question-Answering (QA) system over the Weaviate\nblogs!\n\nThe full code can be found in the [ Weaviate recipes repo\n](https://github.com/weaviate/recipes/blob/main/integrations/llamaindex/upload.py)\n.\n\nThe first step is to setup your Weaviate client. In this example, we connect\nto a local Weaviate instance through port ` http://localhost:8080 ` :\n\n    \n    \n    import weaviate\n    # connect to your weaviate instance\n    client = weaviate.Client(\"http://localhost:8080\")\n\nThe next step is to ingest the Weaviate documentation and parse the documents\ninto chunks. You can choose to use one of our many web page readers to scrape\nany website yourself \u2014 but luckily, the downloaded files are already readily\navailable in the recipes repo.\n\n    \n    \n    from llama_index.node_parser import SimpleNodeParser\n    # load the blogs in using the reader\n    blogs = SimpleDirectoryReader('./data').load_data()\n    # chunk up the blog posts into nodes\n    parser = SimpleNodeParser()\n    nodes = parser.get_nodes_from_documents(blogs)\n\nHere, we use the SimpleDirectoryReader to load in all documents from a given\ndirectory. We then use our ` SimpleNodeParser ` to chunk up the source\ndocuments into Node objects (text chunks).\n\nThe next step is to 1) define a ` WeaviateVectorStore ` , and 2) build a\nvector index over this vector store using LlamaIndex.\n\n    \n    \n    # construct vector store\n    vector_store = WeaviateVectorStore(weaviate_client = client, index_name=\"BlogPost\", text_key=\"content\")\n    # setting up the storage for the embeddings\n    storage_context = StorageContext.from_defaults(vector_store = vector_store)\n    # set up the index\n    index = VectorStoreIndex(nodes, storage_context = storage_context)\n\nOur WeaviateVectorStore abstraction creates a central interface between our\ndata abstractions and the Weaviate service. Note that the ` VectorStoreIndex `\nis initialized from both the nodes and the storage context object containing\nthe Weaviate vector store. During the initialization phase, the nodes are\nloaded into the vector store.\n\nFinally, we can define a query engine on top of our index. ", "mimetype": "text/plain", "start_char_idx": 3559, "end_char_idx": 5801, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ff15592-3757-4ae9-9456-58fef7c63622": {"__data__": {"id_": "5ff15592-3757-4ae9-9456-58fef7c63622", "embedding": null, "metadata": {"filename": "llamaindex-and-weaviate-ba3ff1cbf5f4.md", "extension": ".md", "title": "LlamaIndex and Weaviate", "date": "Jun 22, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-and-weaviate-ba3ff1cbf5f4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "04fe36ff-9050-4f95-a893-a90249ba18bc", "node_type": "4", "metadata": {"filename": "llamaindex-and-weaviate-ba3ff1cbf5f4.md", "extension": ".md", "title": "LlamaIndex and Weaviate", "date": "Jun 22, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-and-weaviate-ba3ff1cbf5f4"}, "hash": "da2e947a2d099ca48cce36c15c24560aa33fb951f6155cb12b449a11e5eab594", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c777f0fd-53d4-44b7-aceb-c450f6b069ff", "node_type": "1", "metadata": {"filename": "llamaindex-and-weaviate-ba3ff1cbf5f4.md", "extension": ".md", "title": "LlamaIndex and Weaviate", "date": "Jun 22, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-and-weaviate-ba3ff1cbf5f4"}, "hash": "26ac57244b144091614160137c884043b361982a10eaa66a299fea71fac333cf", "class_name": "RelatedNodeInfo"}}, "text": "This query engine\nwill perform semantic search and response synthesis, and output an answer.\n\n    \n    \n    \u200b\u200bquery_engine = index.as_query_engine()\n    response = query_engine.query(\"What is the intersection between LLMs and search?\")\n    print(response)\n\nYou should get an answer like the following:\n\n    \n    \n    The intersection between LLMs and search is the ability to use LLMs to improve search capabilities, such as retrieval-augmented generation, query understanding, index construction, LLMs in re-ranking, and search result compression. LLMs can also be used to manage document updates, rank search results, and compress search results. LLMs can be used to prompt the language model to extract or formulate a question based on the prompt and then send that question to the search engine, or to prompt the model with a description of the search engine tool and how to use it with a special `[SEARCH]` token. LLMs can also be used to prompt the language model to rank search results according to their relevance with the query, and to classify the most likely answer span given a question and text passage as input.\n\n#  Next Up in this Series\n\nThis blog post shared an initial overview of the LlamaIndex and Weaviate\nintegration. We covered an introduction to the toolkits offered in LlamaIndex\nand a notebook on how to build a simple QA engine over Weaviate\u2019s blog posts.\nNow that we have a baseline understanding, we will build on this by sharing\nmore advanced guides soon. Stay tuned!\n\n##  What\u2019s next [ \u200b ](https://weaviate.io/blog/llamaindex-and-weaviate#whats-\nnext)\n\nCheck out [ Getting Started with Weaviate\n](https://weaviate.io/developers/weaviate/quickstart) , and begin building\namazing apps with Weaviate.\n\nYou can reach out to us on [ Slack ](https://weaviate.io/slack) or [ Twitter\n](https://twitter.com/weaviate_io) , or [ join the community forum\n](https://forum.weaviate.io/) .\n\nWeaviate is open source, and you can follow the project on [ GitHub\n](https://github.com/weaviate/weaviate) . Don\u2019t forget to give us a while you\nare there!\n\n", "mimetype": "text/plain", "start_char_idx": 5801, "end_char_idx": 7866, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95a7620c-16e4-47a6-a867-60604e082249": {"__data__": {"id_": "95a7620c-16e4-47a6-a867-60604e082249", "embedding": null, "metadata": {"filename": "llamaindex-and-transformers-agents-67042ee1d8d6.md", "extension": ".md", "title": "LlamaIndex and Transformers Agents", "date": "Jun 8, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-and-transformers-agents-67042ee1d8d6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d38e934e-c3a1-4a48-b1f7-8b9f89ed3c1a", "node_type": "4", "metadata": {"filename": "llamaindex-and-transformers-agents-67042ee1d8d6.md", "extension": ".md", "title": "LlamaIndex and Transformers Agents", "date": "Jun 8, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-and-transformers-agents-67042ee1d8d6"}, "hash": "e0fa6d1413a85ce85d9107470598cd7d45034c41ebb6a1ace380486ccd090c4a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26f125b2-5132-465b-acd1-6186c3b41edb", "node_type": "1", "metadata": {}, "hash": "03acc7d237734d51c8fe537a740b221c0ee31747cb25e7589d49589a5c6bcb63", "class_name": "RelatedNodeInfo"}}, "text": "#  Summary\n\nAgents are a popular use-case for Large Language Models (LLMs), typically\nprovide a structure that enables LLMs to make decisions, use tools, and\naccomplish tasks. These agents can take many forms, like the fully-autonomous\nversions seen with [ Auto-GPT ](https://github.com/Significant-Gravitas/Auto-\nGPT) , to more controlled implementations like [ Langchain\n](https://python.langchain.com/en/latest/) Agents. With the recent release of\n[ Transformers Agents\n](https://huggingface.co/docs/transformers/transformers_agents) , we showcase\nhow [ LlamaIndex ](https://www.llamaindex.ai/) continues to be a useful tool\nfor agents, by augmenting their existing image-generator tool. Using an vector\nindex created from 10K [ DiffusionDB\n](https://huggingface.co/datasets/poloclub/diffusiondb) prompts, the\nText2Image Prompt Assistant tool we created can re-write prompts to generate\nmore beautiful images. Full source code is available in the [ Hugging Face\nSpace for the tool\n](https://huggingface.co/spaces/llamaindex/text2image_prompt_assistant/tree/main)\n, and a [ colab notebook\n](https://colab.research.google.com/drive/1r0t423LTkCYi5fGLrSfTdtC0DzKuU-\nzq?usp=sharing) is available as a usage walkthrough.\n\n#  Creating the Tool\n\nTransformers Agents come with a variety of per-configured tools that leverage\nthe vast amounts of open-source models hosted on Hugging Face-Hub.\nFurthermore, additional tools can be created and shared by simply publishing a\nnew Hugging Face Space with the proper tool setup.\n\nTo create a tool, your code simply needs a ` tool_config.json ` file that\ndescribes the tool, as well as a file containing the implementation of your\ntool. While the documentation was a little fuzzy for this part, we eventually\nwere able to use [ the implementation of existing custom tools\n](https://huggingface.co/huggingface-tools) as the framework for our own.\n\nTo enable LlamaIndex to write text-to-image prompts, we need a way to show the\nLLM what examples of good prompts look like. To do this, we indexed 10K random\ntext-to-image prompts from DiffusionDB.\n\n    \n    \n    from datasets import load_dataset\n    from llama_index import VectorStoreIndex, Document\n    \n    # downloads a LOT of data\n    dataset = load_dataset('poloclub/diffusiondb', '2m_random_10k')\n    \n    documents = []\n    for sample in dataset['train']:\n        documents.append(Document(sample['prompt']))\n    \n    # create index\n    index = VectorStoreIndex.from_documents(documents)\n    \n    # store index\n    index.storage_context.persist(persist_dir=\"./storage\")\n\nTo get LlamaIndex to write prompts using examples, we need to customize the\nprompt templates a bit. You can see the final prompt templates and how to use\nthem below:\n\n    \n    \n    text_qa_template = Prompt(\n        \"Examples of text-to-image prompts are below: \\n\"\n        \"---------------------\\n\"\n        \"{context_str}\"\n        \"\\n---------------------\\n\"\n        \"Given the existing examples of text-to-image prompts, \"\n        \"write a new text-to-image prompt in the style of the examples, \"\n        \"by re-wording the following prompt to match the style of the above examples: {query_str}\\n\"\n    )\n    \n    \n    refine_template = Prompt(\n        \"The initial prompt is as follows: {query_str}\\n\"\n        \"We have provided an existing text-to-image prompt based on this query: {existing_answer}\\n\"\n        \"We have the opportunity to refine the existing prompt \"\n        \"(only if needed) with some more relevant examples of text-to-image prompts below.\\n\"\n        \"------------\\n\"\n        \"{context_msg}\\n\"\n        \"------------\\n\"\n        \"Given the new examples of text-to-image prompts, refine the existing text-to-image prompt to better \"\n        \"statisfy the required style. \"\n        \"If the context isn't useful, or the existing prompt is good enough, return the existing prompt.\"\n    )\n    \n    query_engine = index.as_query_engine(\n        text_qa_template=text_qa_template, \n        refine_template=refine_template\n    )\n    \n    response = query_engine.query(\"Draw me a picture of a happy dog\")\n\n##  Snag #1\n\nOne main drawback of Transformers Agents currently is that they will only pick\none tool to solve each prompt. So if we want to augment the image-generator\ntool, we need to replace it! In our tool implementation, we actually load the\noriginal image-generator tool and call it after running LlamaIndex to generate\na new text-to-image prompt.\n\n##  Snag #2\n\nThe next bump in our journey is how Hugging Face downloads tools from the\nspace. Initially, it only downloading the ` tool_config.json ` file and the\nsource code for the tool. But we also need to download the prompts we spent\ntime indexing!\n\nTo get around this, during the ` setup() ` of the tool, we call `\nhf_hub_download() ` to download the files we need to load the index.\n\n##  Back on Track\n\nWith the index created and the general processes figured out, the actual tool\nimplementation is fairly straightforward.\n\n    \n    \n    class Text2ImagePromptAssistant(Tool):\n        \n        inputs = ['text']\n        outputs = ['image']\n        description = PROMPT_ASSISTANT_DESCRIPTION\n        \n        def __init__(self, *args, openai_api_key='', model_name='text-davinci-003', temperature=0.3, verbose=False, **hub_kwargs):\n            super().__init__()\n            os.environ['OPENAI_API_KEY'] = openai_api_key\n            if model_name == 'text-davinci-003':\n                llm = OpenAI(model_name=model_name, temperature=temperature)\n            elif model_name in ('gpt-3.5-turbo', 'gpt-4'):\n                llm = ChatOpenAI(model_name=model_name, temperature=temperature)\n            else:\n                raise ValueError(\n                    f\"{model_name} is not supported, please choose one \"\n                    \"of 'text-davinci-003', 'gpt-3.5-turbo', or 'gpt-4'.\"\n                )\n            service_context = ServiceContext.from_defaults(llm_predictor=LLMPredictor(llm=llm))\n            set_global_service_context(service_context)\n            \n            self.storage_path = os.path.dirname(__file__)\n            self.verbose = verbose\n            self.hub_kwargs = hub_kwargs\n    \n        def setup(self):\n            hf_hub_download(repo_id=\"llamaindex/text2image_prompt_assistant\", filename=\"storage/vector_store.json\", repo_type=\"space\", local_dir=self.storage_path)\n            hf_hub_download(repo_id=\"llamaindex/text2image_prompt_assistant\", filename=\"storage/index_store.json\", repo_type=\"space\", local_dir=self.storage_path)\n            hf_hub_download(repo_id=\"llamaindex/text2image_prompt_assistant\", filename=\"storage/docstore.json\", repo_type=\"space\", local_dir=self.storage_path)\n            \n            self.index = load_index_from_storage(StorageContext.from_defaults(persist_dir=os.path.join(self.storage_path, \"storage\")))\n            self.query_engine = self.index.as_query_engine(similarity_top_k=5, text_qa_template=text_qa_template, refine_template=refine_template)\n            \n            # setup the text-to-image tool too\n            self.text2image = load_tool('huggingface-tools/text-to-image')\n            self.text2image.setup()\n    \n            self.initialized = True\n    \n        def __call__(self, prompt):\n            if not self.is_initialized:\n                self.setup()\n    \n            better_prompt = str(self.query_engine.query(prompt)).strip()\n            \n            if self.verbose:\n                print('==New prompt generated by LlamaIndex==', flush=True)\n                print(better_prompt, '\\n', flush=True)\n    \n            return self.text2image(better_prompt)\n\n#  Running the Tool\n\nWith the tool setup, we can now test it with an actual agent! For testing, we\nused an ` OpenAIAgent ` with the ` text-davinci-003 ` model. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 7743, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26f125b2-5132-465b-acd1-6186c3b41edb": {"__data__": {"id_": "26f125b2-5132-465b-acd1-6186c3b41edb", "embedding": null, "metadata": {"filename": "llamaindex-and-transformers-agents-67042ee1d8d6.md", "extension": ".md", "title": "LlamaIndex and Transformers Agents", "date": "Jun 8, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-and-transformers-agents-67042ee1d8d6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d38e934e-c3a1-4a48-b1f7-8b9f89ed3c1a", "node_type": "4", "metadata": {"filename": "llamaindex-and-transformers-agents-67042ee1d8d6.md", "extension": ".md", "title": "LlamaIndex and Transformers Agents", "date": "Jun 8, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-and-transformers-agents-67042ee1d8d6"}, "hash": "e0fa6d1413a85ce85d9107470598cd7d45034c41ebb6a1ace380486ccd090c4a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95a7620c-16e4-47a6-a867-60604e082249", "node_type": "1", "metadata": {"filename": "llamaindex-and-transformers-agents-67042ee1d8d6.md", "extension": ".md", "title": "LlamaIndex and Transformers Agents", "date": "Jun 8, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-and-transformers-agents-67042ee1d8d6"}, "hash": "87720b36fe15bc1347617a53fab59104b76faf336a9f32d010a7c1169640dbb7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57667410-a6b4-41f5-9aae-a5899ef66b80", "node_type": "1", "metadata": {}, "hash": "e915d1785f1631622c1c12bbd0a1fac5ac86822ad95194f820abcbeb8a365243", "class_name": "RelatedNodeInfo"}}, "text": "When asked to\ndraw a picture of a mountain, this is what we got:\n\n    \n    \n    from transformers import OpenAiAgent\n    agent = OpenAiAgent(model=\"text-davinci-003\", api_key=\"your_api_key\")\n    \n    agent.run(\"Draw me a picture a mountain.\")\n\nThe initial picture of mountains that the agent created.\n\nAs you can see, the picture looks alright. But, text-to-image prompts are\nsomewhat of an art.\n\nTo use our new tool, we just need to replace the existing image-generator\ntool:\n\n    \n    \n    from transformers import load_tool\n    prompt_assistant = load_tool(\n        \"llamaindex/text2image_prompt_assistant\",\n        openai_api_key=\"your_api_key\",\n        model_name='text-davinci-003',\n        temperature=0.3,  # increase or decrease this to control variation\n        verbose=True\n    )\n    \n    from transformers import OpenAiAgent\n    agent = OpenAiAgent(model=\"text-davinci-003\", api_key=\"your_api_key\")\n    \n    # replace the existing tool\n    agent.toolbox['image_generator'] = prompt_assistant\n    \n    agent.run(\"Draw me a picture a mountain.\")\n\nUsing Our new LlamaIndex Prompt Assistant tool, we get a much more stylized\nresult. In the terminal, we see the prompt was re-written as \u201ca majestic\nmountain peak, surrounded by lush greenery, with a stunning sunset in the\nbackground,\u201d which resulted in the following image:\n\nImage generated by our Text2Image Prompt Assistant tool.\n\nLooks great! ", "mimetype": "text/plain", "start_char_idx": 7743, "end_char_idx": 9147, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "57667410-a6b4-41f5-9aae-a5899ef66b80": {"__data__": {"id_": "57667410-a6b4-41f5-9aae-a5899ef66b80", "embedding": null, "metadata": {"filename": "llamaindex-and-transformers-agents-67042ee1d8d6.md", "extension": ".md", "title": "LlamaIndex and Transformers Agents", "date": "Jun 8, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-and-transformers-agents-67042ee1d8d6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d38e934e-c3a1-4a48-b1f7-8b9f89ed3c1a", "node_type": "4", "metadata": {"filename": "llamaindex-and-transformers-agents-67042ee1d8d6.md", "extension": ".md", "title": "LlamaIndex and Transformers Agents", "date": "Jun 8, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-and-transformers-agents-67042ee1d8d6"}, "hash": "e0fa6d1413a85ce85d9107470598cd7d45034c41ebb6a1ace380486ccd090c4a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26f125b2-5132-465b-acd1-6186c3b41edb", "node_type": "1", "metadata": {"filename": "llamaindex-and-transformers-agents-67042ee1d8d6.md", "extension": ".md", "title": "LlamaIndex and Transformers Agents", "date": "Jun 8, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-and-transformers-agents-67042ee1d8d6"}, "hash": "202325b35fba65cc942a671fd7ba4680bea07308116235439d3e99cdb3d4d807", "class_name": "RelatedNodeInfo"}}, "text": "With the temperature variable, we can control how varied the\ngenerated prompts become. With a temperature above zero, each prompt generated\nby LlamaIndex with the same agent prompt will be brand new!\n\n#  Conclusion\n\nIn conclusion, we have demonstrated how LlamaIndex can be used to augment LLM\nagents, by implementing a Text2Image Prompt Assistant tool with a Transformers\nAgent. Using a vector database created from DiffusionDB, LlamaIndex can\nsuggest better prompts when generating images.\n\nCustom tools in Transformers Agents are easily distributed and shared using\nHugging Face Spaces, and we are excited to see what other people build and\nshare!\n\n", "mimetype": "text/plain", "start_char_idx": 9147, "end_char_idx": 9799, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "55c4c151-a5db-4089-a361-73f4f245a637": {"__data__": {"id_": "55c4c151-a5db-4089-a361-73f4f245a637", "embedding": null, "metadata": {"filename": "building-the-data-framework-for-llms-bca068e89e0e.md", "extension": ".md", "title": "Building the data framework for LLMs", "date": "Jun 6, 2023", "url": "https://www.llamaindex.ai/blog/building-the-data-framework-for-llms-bca068e89e0e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc4358bd-2df7-4d22-af6c-f4cc87129ae4", "node_type": "4", "metadata": {"filename": "building-the-data-framework-for-llms-bca068e89e0e.md", "extension": ".md", "title": "Building the data framework for LLMs", "date": "Jun 6, 2023", "url": "https://www.llamaindex.ai/blog/building-the-data-framework-for-llms-bca068e89e0e"}, "hash": "8047afe396b912a59ae5309971a8b7ec3784b5acf5e727270855a0d8768324cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "819391e7-dbb6-4b77-9117-87a647854800", "node_type": "1", "metadata": {}, "hash": "dea7f87674c7dba79ac9d06305d186af44a1391b63c65b6ebfda78876f28e257", "class_name": "RelatedNodeInfo"}}, "text": "Today is an exciting day for LlamaIndex, and a big milestone in my personal\njourney with generative AI. I\u2019ve followed generative models for most of my\nacademic/professional career \u2014 from [ my research on GANs/sensor compression\n](https://scholar.google.com/citations?user=8JjemawAAAAJ&hl=en) to following [\nTransformers ](https://arxiv.org/abs/1706.03762) / [ GPT\n](https://openai.com/blog/gpt-3-apps) [ developments\n](https://openai.com/blog/chatgpt) . It became increasingly clear that as\nthese models got bigger/better, they were evolving from knowledge generators\nto intelligent engines that could reason/act over new information.\n\nI formalized some of these key intuitions more concretely:\n\n  * LLMs are fantastic reasoning engines, capable of question-answering, summarization, planning, and more. They had the promise of becoming the \u201cneural\u201d compute unit at the core of a new age of AI-enabled software. \n  * Yet, LLMs inherently have no awareness of your own data. \n  * No one really knew the best practices for feeding your data into the LLM. Models had limited context windows and were expensive to finetune. \n\nIf we could offer a toolkit to help set up the data architecture for LLM apps,\nthen we could enable anyone to build LLM-powered knowledge workers and\ntransform the way that software is written over private data. LLM-enabled\nsoftware requires new infrastructure tooling over your data and has\nsignificant implications for the modern software data stack.\n\nDetermined to tackle this challenge, I built GPT Index (which we later\nrebranded to LlamaIndex), an initial exploratory effort to organize and\nretrieve information using LLMs. ( [ first Tweet is here\n](https://twitter.com/jerryjliu0/status/1590192512639332353?s=20) !)\n\nIt happened at the perfect time. Since last November, there has been an\nexplosion in developer interest in building applications on top of LLMs. Most\ndevelopers were figuring out ways to leverage the reasoning capabilities of\nLLMs on top of their own private data. Just two months in, I joined forces\nwith Simon Suo, a brilliant AI technologist and my former colleague, and we\nevolved LlamaIndex from an exploratory project into a comprehensive framework\ndesigned to connect a user\u2019s private data with LLMs. It gained recognition\nwithin the AI community, captivating the attention of hackers, developers, and\nindustry experts alike. In just six months, the project garnered an impressive\nfollowing, with [ 16K Github Stars ](https://github.com/jerryjliu/llama_index)\n, [ 20K Twitter followers ](https://twitter.com/llama_index) , [ 200K monthly\ndownloads ](https://pypi.org/project/llama-index/) , and [ 6K active Discord\nusers ](https://discord.gg/dGcwcsnxhU) . Companies like Instabase, Front, and\nUber started experimenting with LlamaIndex on top of their data.\n\nSome initial stacks started to emerge \u2014 for instance a common paradigm for\nbuilding QA systems and chatbots was using a simple retrieval mechanism (top-k\nlookup from a vector database) with an LLM. LlamaIndex became viewed as a [\ncritical ](https://medium.com/cowboy-ventures/the-new-infra-stack-for-\ngenerative-ai-9db8f294dc3f) [ data ](https://www.unusual.vc/post/devtools-for-\nlanguage-models) [ orchestration ](https://www.madrona.com/foundation-models/)\n[ component ](https://foundationcapital.com/foundation-model-ops-powering-the-\nnext-wave-of-generative-ai-apps/) of the emerging LLM software landscape.\n\nYet, it became clear that there were still significant technical challenges in\nthe space of LLMs and data, and no one had the right answers. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3566, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "819391e7-dbb6-4b77-9117-87a647854800": {"__data__": {"id_": "819391e7-dbb6-4b77-9117-87a647854800", "embedding": null, "metadata": {"filename": "building-the-data-framework-for-llms-bca068e89e0e.md", "extension": ".md", "title": "Building the data framework for LLMs", "date": "Jun 6, 2023", "url": "https://www.llamaindex.ai/blog/building-the-data-framework-for-llms-bca068e89e0e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc4358bd-2df7-4d22-af6c-f4cc87129ae4", "node_type": "4", "metadata": {"filename": "building-the-data-framework-for-llms-bca068e89e0e.md", "extension": ".md", "title": "Building the data framework for LLMs", "date": "Jun 6, 2023", "url": "https://www.llamaindex.ai/blog/building-the-data-framework-for-llms-bca068e89e0e"}, "hash": "8047afe396b912a59ae5309971a8b7ec3784b5acf5e727270855a0d8768324cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "55c4c151-a5db-4089-a361-73f4f245a637", "node_type": "1", "metadata": {"filename": "building-the-data-framework-for-llms-bca068e89e0e.md", "extension": ".md", "title": "Building the data framework for LLMs", "date": "Jun 6, 2023", "url": "https://www.llamaindex.ai/blog/building-the-data-framework-for-llms-bca068e89e0e"}, "hash": "2762d2ba0711eebd8a9375f976979deb0365a67383cf18f9cb467cb60795dd53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1cc555f-fe43-431b-9fe9-aacdb09b70ee", "node_type": "1", "metadata": {}, "hash": "17598fd9fc4111f95a58adb7222551cf3b895c249eb457a5df47d29bab8cd76c", "class_name": "RelatedNodeInfo"}}, "text": "Even with the\ncapable toolkit that we\u2019ve developed, we were just starting to scratch the\nsurface on unlocking value from data.\n\nWe are thrilled to share that LlamaIndex has secured $8.5 million in seed\nfunding, led by Greylock, to help propel these efforts further. We\u2019re excited\nto work with Jerry Chen, Saam Motamedi, and Jason Risch on the Greylock team.\nJoining us in this exciting journey are Jack Altman (CEO of Lattice), Lenny\nRachitsky (Lenny\u2019s Newsletter), Mathilde Collin (CEO of Front), Raquel Urtasun\n(CEO of Waabi), Joey Gonzalez (Berkeley), and many others. Their belief in our\nvision and the impact of LlamaIndex on the future of AI fuels our passion in\nsolving these data + AI problems.\n\n#  **Why LlamaIndex?**\n\nCalling an LLM API is easy. Setting up a software system that can extract\ninsights from your private data is harder.\n\nLlamaIndex is the advanced data framework for your LLM applications. It\nencompasses essential features allowing you to both manage and query your\ndata.\n\n  ", "mimetype": "text/plain", "start_char_idx": 3566, "end_char_idx": 4567, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1cc555f-fe43-431b-9fe9-aacdb09b70ee": {"__data__": {"id_": "c1cc555f-fe43-431b-9fe9-aacdb09b70ee", "embedding": null, "metadata": {"filename": "building-the-data-framework-for-llms-bca068e89e0e.md", "extension": ".md", "title": "Building the data framework for LLMs", "date": "Jun 6, 2023", "url": "https://www.llamaindex.ai/blog/building-the-data-framework-for-llms-bca068e89e0e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc4358bd-2df7-4d22-af6c-f4cc87129ae4", "node_type": "4", "metadata": {"filename": "building-the-data-framework-for-llms-bca068e89e0e.md", "extension": ".md", "title": "Building the data framework for LLMs", "date": "Jun 6, 2023", "url": "https://www.llamaindex.ai/blog/building-the-data-framework-for-llms-bca068e89e0e"}, "hash": "8047afe396b912a59ae5309971a8b7ec3784b5acf5e727270855a0d8768324cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "819391e7-dbb6-4b77-9117-87a647854800", "node_type": "1", "metadata": {"filename": "building-the-data-framework-for-llms-bca068e89e0e.md", "extension": ".md", "title": "Building the data framework for LLMs", "date": "Jun 6, 2023", "url": "https://www.llamaindex.ai/blog/building-the-data-framework-for-llms-bca068e89e0e"}, "hash": "e90bf9c262c15b449cbaed50f1cd828407c93e5cda1b74ebaafac4f06f58c9a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0d5e85a-4568-4043-9499-c0a2c05fc739", "node_type": "1", "metadata": {}, "hash": "c6f4c9437fc26fc2c89dbd3563db45fb056ae34f938ea97f74d8209d48db00bd", "class_name": "RelatedNodeInfo"}}, "text": "* **Data Management:** Data ingestion, data parsing/slicing, data storage/indexing. \n  * **Data Querying:** Data retrieval, response synthesis, multi-step interactions over your data. \n\nLlamaIndex allows you to seamlessly integrate individual or enterprise data,\nincluding files, workplace apps, and databases, with LLM applications. We also\noffer an extensive array of integrations with other storage providers and\ndownstream applications.\n\n  * 100+ data loaders \n  * 13+ vector database providers \n  * Integrations with observability and experimentation frameworks (e.g. prompt tracking and system tracing) \n  * Integrations as a [ ChatGPT Retrieval Plugin ](https://github.com/openai/chatgpt-retrieval-plugin/blob/main/datastore/providers/llama_datastore.py) or with [ Poe ](https://github.com/poe-platform/poe-protocol/tree/main/llama_poe)\n\nThe end result is that you can build a variety of amazing knowledge-intensive\nLLM applications. This ranges from a search engine over your data, to chatbot-\nstyle interfaces, to structured analytics helpers, to autonomous knowledge\nagents.\n\n#  **What\u2019s next?**\n\nThere are _so_ many things that we want to do to more fully realize our vision\nof unlocking LLM capabilities on top of your data. We\u2019ll broadly break this\ndown into two categories: 1) our continued commitment to the open-source\ndeveloper community, and 2) solving the data problem at scale for enterprises.\n\n##  **Build the best open source data framework and developer community**\n\nAt a high-level, we want to continue iterating on our core feature\ncapabilities, improving reliability, and satisfy both the needs of beginner\nand advanced users.\n\n  * **Handle complex queries:** We want to continue advancing the idea of \u201cquerying your data\u201d, whether it\u2019s through leveraging agent-style interactions for data retrieval and synthesis or program synthesis/DSL. \n  * **Multi-modal data management:** The future of foundation models is multimodal, not just contained to LLMs. There are many types of semi-structured data (e.g. semi-structured data like JSONs, yaml files) as well as \u201ccomplex\u201d unstructured data (audio, images, video) that we\u2019d love to have native support for. \n  * **Better evaluation of LLM data systems:** Properly evaluating LLM calls is already tricky (how do you best evaluate the quality of a generated output? Some [ libraries ](https://github.com/openai/evals) for handling this). This becomes even more tricky when you chain LLM calls within an overall data system. We want to invest efforts into this area to provide greater transparency to our users. \n  * **Optimization of Latency/Cost:** Users are faced with a plethora of choices when it comes to building a data-driven LLM app: the choice of LLM model, embedding model, vector database, etc. They must choose in accordance to a variety of factors, from latency and cost to privacy. \n  * **Ease of use for both beginner users and advanced users:** Our goal is to make the utilization of LLM capabilities accessible and user-friendly for individuals at all skill levels. We will develop clear tutorials, examples, and tools to simplify the learning curve and convey the value of all of our features. \n\n##  **Solving the data problem at scale for Enterprises**\n\nAs we\u2019re iterating on the open-source project, we also want to identify the\nsurrounding pain points in being able to build and deploy data-powered LLM\napps to production. Our solution to this will build upon the success of our\nopen-source project and be a natural evolution to the enterprise setting.\n\n  * **Production-ready data ingestion and management:** We want to handle data updates, data consistency, and scalability to larger volumes of data parsing. We also want to continue expanding on the right storage abstractions for multi-modal data. \n  * **Scale to Large Data Volumes:** Enterprises will typically have orders of magnitude more data than an individual. We want to invest in hosted infrastructure/deployment solutions around our core package so that you don\u2019t have to. \n  * **Domain-specific LLM solutions:** We want to offer packaged solutions to enable users to easily build LLM apps in different domains, from healthcare to finance to legal. \n\nIf you\u2019re building LLM apps in the enterprise setting, we\u2019d love to chat and\nlearn more about pain points + desired features! ", "mimetype": "text/plain", "start_char_idx": 4567, "end_char_idx": 8900, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0d5e85a-4568-4043-9499-c0a2c05fc739": {"__data__": {"id_": "f0d5e85a-4568-4043-9499-c0a2c05fc739", "embedding": null, "metadata": {"filename": "building-the-data-framework-for-llms-bca068e89e0e.md", "extension": ".md", "title": "Building the data framework for LLMs", "date": "Jun 6, 2023", "url": "https://www.llamaindex.ai/blog/building-the-data-framework-for-llms-bca068e89e0e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc4358bd-2df7-4d22-af6c-f4cc87129ae4", "node_type": "4", "metadata": {"filename": "building-the-data-framework-for-llms-bca068e89e0e.md", "extension": ".md", "title": "Building the data framework for LLMs", "date": "Jun 6, 2023", "url": "https://www.llamaindex.ai/blog/building-the-data-framework-for-llms-bca068e89e0e"}, "hash": "8047afe396b912a59ae5309971a8b7ec3784b5acf5e727270855a0d8768324cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1cc555f-fe43-431b-9fe9-aacdb09b70ee", "node_type": "1", "metadata": {"filename": "building-the-data-framework-for-llms-bca068e89e0e.md", "extension": ".md", "title": "Building the data framework for LLMs", "date": "Jun 6, 2023", "url": "https://www.llamaindex.ai/blog/building-the-data-framework-for-llms-bca068e89e0e"}, "hash": "afddc35c68b55279305afda8b21ea7a07e89f977161d0617c7559f588afe9090", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b3ef8a6-eddb-4b14-b743-6e19c3a9022e", "node_type": "1", "metadata": {}, "hash": "d41f89e05961104d25ac58fb4147c1803b32b9004044011652b0b459adc2f9fb", "class_name": "RelatedNodeInfo"}}, "text": "Check out our [ form here\n](https://docs.google.com/forms/d/1lzXIE9G07D9eoK7MBUpRuN-\nPdBYGA7nWVqcIvNtN9mQ/edit#responses) .\n\n#  **Join the Llama Gang!**\n\nJoin the Llama(Index) gang as we embark on this journey to solve problems at\nthe intersection of LLMs and data. We are not just building tools for ML\npractitioners/researchers; the emerging LLM + data architecture stacks have\nimplications for _all_ of software development. As a result, we are operating\nat the intersection of incredibly fun and challenging problems from a variety\nof different fields:\n\n  * Foundation Model Development \n  * Information Retrieval + Recommendation Systems \n  * Data Systems \n  * MLOps \n  * DevOps \n\nInterested in checking out the project?\n\n  * Find our project on [ Github ](https://github.com/jerryjliu/llama_index) and check out our [ Docs ](https://gpt-index.readthedocs.io/en/latest/)\n  * Check out our brand new landing page: [ https://llamaindex.ai ](https://llamaindex.ai)\n  * Join our [ Discord ](https://discord.gg/dGcwcsnxhU) or Follow our [ Twitter ](https://twitter.com/llama_index)\n\nAlso, we\u2019re hiring!\n\n  * We\u2019re looking for founding engineers \u2014 experience in one or more of AI, data systems, and full-stack/front-end is nice to have but not a requirement. \n  ", "mimetype": "text/plain", "start_char_idx": 8900, "end_char_idx": 10161, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b3ef8a6-eddb-4b14-b743-6e19c3a9022e": {"__data__": {"id_": "4b3ef8a6-eddb-4b14-b743-6e19c3a9022e", "embedding": null, "metadata": {"filename": "building-the-data-framework-for-llms-bca068e89e0e.md", "extension": ".md", "title": "Building the data framework for LLMs", "date": "Jun 6, 2023", "url": "https://www.llamaindex.ai/blog/building-the-data-framework-for-llms-bca068e89e0e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc4358bd-2df7-4d22-af6c-f4cc87129ae4", "node_type": "4", "metadata": {"filename": "building-the-data-framework-for-llms-bca068e89e0e.md", "extension": ".md", "title": "Building the data framework for LLMs", "date": "Jun 6, 2023", "url": "https://www.llamaindex.ai/blog/building-the-data-framework-for-llms-bca068e89e0e"}, "hash": "8047afe396b912a59ae5309971a8b7ec3784b5acf5e727270855a0d8768324cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0d5e85a-4568-4043-9499-c0a2c05fc739", "node_type": "1", "metadata": {"filename": "building-the-data-framework-for-llms-bca068e89e0e.md", "extension": ".md", "title": "Building the data framework for LLMs", "date": "Jun 6, 2023", "url": "https://www.llamaindex.ai/blog/building-the-data-framework-for-llms-bca068e89e0e"}, "hash": "f6106d4eb9840e3e2b2f4d7ef83424bb38ceb5c663fd314f381e52f48cc9bbde", "class_name": "RelatedNodeInfo"}}, "text": "* If you\u2019re interested, [ fill out our form here ](https://docs.google.com/forms/d/e/1FAIpQLScpSqZvTincCsspY5CyY_9gAGXnQfTS7HQvsgVQccncCJ7x5w/viewform?usp=sf_link) . \n\n", "mimetype": "text/plain", "start_char_idx": 10161, "end_char_idx": 10329, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6013a694-9489-4651-85cb-be8c1c90d8e6": {"__data__": {"id_": "6013a694-9489-4651-85cb-be8c1c90d8e6", "embedding": null, "metadata": {"filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8a1f04fc-2aef-4384-8c26-74fc2938e91a", "node_type": "4", "metadata": {"filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "hash": "5d82c2b900fbabdf56c600b6fabe576eca7b25c810b27484985a87d5442478d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56bbeb99-5b37-4c3d-81d4-0fabf8003056", "node_type": "1", "metadata": {}, "hash": "4af0d575a1609e9a104951f63d79f7adf0629b5a2b1c79b0d3c6fd90d64fde51", "class_name": "RelatedNodeInfo"}}, "text": "**Co-Authors:**\n\n  * Akash Sharma, founder and CEO, Vellum \n  * Jerry Liu, co-founder and CEO, LlamaIndex \n\n#  About Us\n\nThe central mission of [ LlamaIndex\n](https://github.com/jerryjliu/llama_index) is to provide an interface between\nLarge Language Models (LLM\u2019s), and your private, external data. Over the past\nfew months, it has become one of the most popular open-source frameworks for\nLLM data augmentation (context-augmented generation), for a variety of use\ncases: question-answering, summarization, structured queries, and more.\n\n[ Vellum ](http://vellum.ai/) is a developer platform to build high quality\nLLM applications. The platform provides best-in-class tooling for prompt\nengineering, unit testing, regression testing, monitoring & versioning of in-\nproduction traffic and model fine tuning. Vellum\u2019s platform helps companies\nsave countless engineering hours to build internal tooling and instead use\nthat time to build end user facing applications.\n\n#  Why we partnered on this integration\n\nUntil recently, LlamaIndex users did not have a way to do prompt engineering\nand unit testing pre-production and versioning/monitoring the prompts post\nproduction. Prompt engineering and unit testing is key to ensure that your LLM\nfeature is producing reliable results in production. Here\u2019s an example of\nsimple prompt that produces vastly different results between GPT-3, GPT-3.5\nand GPT-4:\n\n##  Unit testing your prompts\n\nCreating a unit test bank is a proactive approach to ensure prompt reliability\n\u2014 it\u2019s best practice to run 50\u2013100 test cases before putting prompts in\nproduction. The test bank should comprise scenarios & edge cases anticipated\nin production, think of this as QAing your feature before it goes to\nproduction. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1741, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56bbeb99-5b37-4c3d-81d4-0fabf8003056": {"__data__": {"id_": "56bbeb99-5b37-4c3d-81d4-0fabf8003056", "embedding": null, "metadata": {"filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8a1f04fc-2aef-4384-8c26-74fc2938e91a", "node_type": "4", "metadata": {"filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "hash": "5d82c2b900fbabdf56c600b6fabe576eca7b25c810b27484985a87d5442478d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6013a694-9489-4651-85cb-be8c1c90d8e6", "node_type": "1", "metadata": {"filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "hash": "e43115b17efde10c14b1ae7fc48f99c3deb18e51128ce187dced1e8fa3564e13", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bac6a762-03a6-4c4c-94b8-ea4a3f078c9d", "node_type": "1", "metadata": {}, "hash": "77e9e9b9eed6edec535910abc1ff47993a67d8b80bdfaf9c1b63e9160e092d40", "class_name": "RelatedNodeInfo"}}, "text": "The prompts should \u201cpass\u201d these test cases based on your\nevaluation criteria. Use Vellum Test Suites to upload test cases in bulk via\nCSV upload.\n\n##  Regression testing in production\n\nDespite how well you test before sending a prompt in production, edge cases\ncan appear when in production. This is expected, so no stress! Through the\nVellum integration, LlamaIndex users can change prompts and get prompt\nversioning without making any code changes. While doing that, however, it\u2019s\nbest practice to run historical inputs that were sent to the prompt in\nproduction to the new prompt and confirm it doesn\u2019t break any existing\nbehavior. LLMs are sometimes unpredictable, even changing the word \u201cgood\u201d to\n\u201cgreat\u201d in a prompt can result in differing outputs!\n\n#  Best practices to leverage the integration\n\n##  How to access the integration\n\n[ This\n](https://github.com/jerryjliu/llama_index/blob/main/examples/vellum/Vellum%20Integration%20Demo.ipynb)\ndemo notebook goes into detail on how you can use Vellum to manage prompts\nwithin LlamaIndex.\n\n**Prerequisites**\n\n  1. Sign up for a free Vellum account at [ app.vellum.ai/signup ](https://app.vellum.ai/signup)\n  2. Go to [ app.vellum.ai/api-keys ](https://app.vellum.ai/api-keys) and generate a Vellum API key. Note it down somewhere. \n\n**Auto-Register Prompts & Make Predictions Through Vellum **\n\nIf you import a prompt in LlamaIndex, the VellumPredictor class will used to\nauto-register a prompt with Vellum to make predictions.\n\nBy registering a prompt with Vellum, Vellum will create:\n\n  1. A \u201cSandbox\u201d \u2014 an environment where you can iterate on the prompt, it\u2019s model, provider, params, etc.; and \n  2. A \u201cDeployment\u201d \u2014 a thin API proxy between you and LLM providers and offering prompt versioning, request monitoring, and more \n\nYou can use VellumPromptRegistry to retrieve information about the registered\nprompt and get links to open its corresponding Sandbox and Deployment in\nVellum\u2019s UI. More details about Vellum\u2019s Sandbox and Deployment features can\nbe found [ here ](https://www.notion.so/Vellum-LlamaIndex-\nintegration-096fb3f141ac49c695b7fcb6c70c0519?pvs=21)\n\n##  Prompt engineering tips in context augmented use cases\n\nThink of the Large Language Model as a smart college graduate that needs\ninstructions if the task at hand is not clear. If you\u2019re not getting good\nresults with the default prompt templates, follow these instructions:\n\n  1. ", "mimetype": "text/plain", "start_char_idx": 1741, "end_char_idx": 4150, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bac6a762-03a6-4c4c-94b8-ea4a3f078c9d": {"__data__": {"id_": "bac6a762-03a6-4c4c-94b8-ea4a3f078c9d", "embedding": null, "metadata": {"filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8a1f04fc-2aef-4384-8c26-74fc2938e91a", "node_type": "4", "metadata": {"filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "hash": "5d82c2b900fbabdf56c600b6fabe576eca7b25c810b27484985a87d5442478d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56bbeb99-5b37-4c3d-81d4-0fabf8003056", "node_type": "1", "metadata": {"filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "hash": "aca81432816bcd3bd43b2cfcde4fb1d0bb43d2dc7110ec99baaf4e6a00441e66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b334187e-9b11-4d9b-a0c9-19177d3d55bd", "node_type": "1", "metadata": {}, "hash": "c60c2abd06d33d015ee2f0d7cdc696ad80df4cbe44e35dbb20041038e791c319", "class_name": "RelatedNodeInfo"}}, "text": "Add use case specific details to the prompt to guide what the model focuses on. \n  2. Create 5\u201310 input scenarios to test performance \n  3. Iterate a few times: (i) Tweak the prompt by adding more specific instructions or examples for the scenarios with bad results, (ii) Evaluate against the target response for each scenario \n  4. In parallel, test out different foundation models and model providers using Vellum\u2019s Sandbox. Maybe Claude or PaLM does better than GPT-4 for your use case. \n  5. ", "mimetype": "text/plain", "start_char_idx": 4150, "end_char_idx": 4646, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b334187e-9b11-4d9b-a0c9-19177d3d55bd": {"__data__": {"id_": "b334187e-9b11-4d9b-a0c9-19177d3d55bd", "embedding": null, "metadata": {"filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8a1f04fc-2aef-4384-8c26-74fc2938e91a", "node_type": "4", "metadata": {"filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "hash": "5d82c2b900fbabdf56c600b6fabe576eca7b25c810b27484985a87d5442478d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bac6a762-03a6-4c4c-94b8-ea4a3f078c9d", "node_type": "1", "metadata": {"filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}, "hash": "d99651f8223d3c5820dec90c9ee0be11f99860ddd4f2b6ddf469cae7d1ab0c22", "class_name": "RelatedNodeInfo"}}, "text": "If you would like additional reasoning or explanation, use a more prescriptive approach: \n\n  * Add detailed step by step instructions to the end of the prompt and ask the LLM to walk though those steps when creating it\u2019s answer: \n  * e.g. (1) \u2026 (2) \u2026 (3) \u2026 \u2026 (6) Output a JSON with the following typescript schema \n  * This is convenient because it\u2019s simple to parse out the JSON blob from the LLM output \n  * However this causes more tokens to be generated so is slower and costs more, but it\u2019s not nearly as expensive and slow as chaining multiple calls \n\n##  Measuring prompt quality, before production\n\nOne of the common reasons why evaluating LLM model quality is hard is that\nthere\u2019s no defined framework. The evaluation metric depends on your use case.\nThis [ blog ](https://www.vellum.ai/blog/how-to-evaluate-the-quality-of-large-\nlanguage-models-for-production-use-cases) goes in more detail, but in summary,\nthe evaluation approach depends on type of use case:\n\n  * **Classification:** accuracy, recall, precision, F score and confusion matrices for a deeper evaluation \n  * **Data extraction:** Validate that the output is syntactically valid and the expected keys are present in the generated response \n  * **SQL/Code generation:** Validate that the output is syntactically valid and running it will return the expected values \n  * **Creative output:** Semantic similarity between model generated response and target response using cross-encoders \n\nVellum\u2019s Sandbox and Test Suites offer **Exact Match, Regex Match, Semantic\nSimilarity & Webhook ** as evaluation criteria. You get a clear indication of\nwhich test cases \u201cpass\u201d, given your evaluation criteria\n\n**Testing in Vellum Sandbox**\n\n**Testing in Vellum Test Suites**\n\n##  Measuring prompt quality, once in production\n\nUser feedback is the ultimate source of truth for model quality \u2014 if there\u2019s a\nway for your users to either implicitly or explicitly tell you whether they\nthe response is \u201cgood\u201d or \u201cbad,\u201d that\u2019s what you should track and improve!\n\nExplicit user feedback is collected when your users respond with something\nlike a or in your UI when interacting with the LLM output. Asking explicitly\nmay not result in enough volume of feedback to measure overall quality. If\nyour feedback collection rates are low, we suggest using implicit feedback if\npossible.\n\nImplicit feedback is based on how users react to the output generated by the\nLLM. For example, if you generate a first draft of en email for a user and\nthey send it without making edits, that\u2019s likely a good response! If they hit\nregenerate, or re-write the whole thing, that\u2019s probably not a good response.\nImplicit feedback collection may not be possible for all use cases, but it can\nbe a powerful gauge of quality.\n\nUse Vellum\u2019s Actuals endpoint to track the quality of each completion and\ntrack results in the Completions and Monitoring tabs of your Deployment.\n\n", "mimetype": "text/plain", "start_char_idx": 4646, "end_char_idx": 7549, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eae9f1f7-146a-437b-8c69-b37af3720d5f": {"__data__": {"id_": "eae9f1f7-146a-437b-8c69-b37af3720d5f", "embedding": null, "metadata": {"filename": "combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.md", "extension": ".md", "title": "Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation", "date": "May 28, 2023", "url": "https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c699067-f5df-4a3c-a513-45453dc811f2", "node_type": "4", "metadata": {"filename": "combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.md", "extension": ".md", "title": "Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation", "date": "May 28, 2023", "url": "https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b"}, "hash": "a1f1d867138ca99619507287e6a79335ca026926fb30d4b5af6bb906fbf70133", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38392555-0e09-49d2-9a09-194d4ac176fb", "node_type": "1", "metadata": {}, "hash": "f46a895e703f56b9bf58c644c06b9bbaaa9d3ce6f272b9cbdcc83561aef5ed86", "class_name": "RelatedNodeInfo"}}, "text": "#  Summary\n\nIn this article, we showcase a powerful new query engine ( `\nSQLAutoVectorQueryEngine ` ) in LlamaIndex that can leverage both a SQL\ndatabase as well as a vector store to fulfill complex natural language queries\nover a combination of structured and unstructured data. This query engine can\nleverage the expressivity of SQL over structured data, and join it with\nunstructured context from a vector database. We showcase this query engine on\na few examples and show that it can handle queries that make use of both\nstructured/unstructured data, or either.\n\nCheck out the full guide here: [ https://gpt-\nindex.readthedocs.io/en/latest/examples/query_engine/SQLAutoVectorQueryEngine.html\n](https://gpt-\nindex.readthedocs.io/en/latest/examples/query_engine/SQLAutoVectorQueryEngine.html)\n.\n\n#  Context\n\nData lakes in enterprises typically encompass both **structured** and\n**unstructured** data. Structured data is typically stored in a tabular format\nin SQL databases, organized into tables with predefined schemas and\nrelationships between entities. On the other hand, unstructured data found in\ndata lakes lacks a predefined structure and does not fit neatly into\ntraditional databases. This type of data includes text documents, but also\nother multimodal formats such as audio recordings, videos, and more.\n\nLarge Language Models (LLMs) have the ability to extract insights from both\nstructured and unstructured data. There have been some initial tooling and\nstacks that have emerged for tackling both types of data:\n\n  * **Text-to-SQL (Structured data):** Given a collection of tabular schemas, we convert natural language into a SQL statement which can then be executed against the database. \n  * **Semantic Search with a Vector Database (Unstructured Data):** Store unstructured documents along with their embeddings in a vector database (e.g. Pinecone, Chroma, Milvus, Weaviate, etc.). ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1901, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "38392555-0e09-49d2-9a09-194d4ac176fb": {"__data__": {"id_": "38392555-0e09-49d2-9a09-194d4ac176fb", "embedding": null, "metadata": {"filename": "combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.md", "extension": ".md", "title": "Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation", "date": "May 28, 2023", "url": "https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c699067-f5df-4a3c-a513-45453dc811f2", "node_type": "4", "metadata": {"filename": "combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.md", "extension": ".md", "title": "Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation", "date": "May 28, 2023", "url": "https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b"}, "hash": "a1f1d867138ca99619507287e6a79335ca026926fb30d4b5af6bb906fbf70133", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eae9f1f7-146a-437b-8c69-b37af3720d5f", "node_type": "1", "metadata": {"filename": "combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.md", "extension": ".md", "title": "Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation", "date": "May 28, 2023", "url": "https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b"}, "hash": "8ff8a8c7f3871b36f64e90635ffd00f6e2397eb6d90be6e1736cc52a7122216e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c7d9410-b6c0-4655-9966-05e18e50ebe3", "node_type": "1", "metadata": {}, "hash": "891ef506424854f940a21734cd8eb072c632ae82809768377889ca42e822fd27", "class_name": "RelatedNodeInfo"}}, "text": "During query-time, fetch the relevant documents by embedding similarity, and then put into the LLM input prompt to synthesize a response. \n\n", "mimetype": "text/plain", "start_char_idx": 1901, "end_char_idx": 2041, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c7d9410-b6c0-4655-9966-05e18e50ebe3": {"__data__": {"id_": "6c7d9410-b6c0-4655-9966-05e18e50ebe3", "embedding": null, "metadata": {"filename": "combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.md", "extension": ".md", "title": "Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation", "date": "May 28, 2023", "url": "https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c699067-f5df-4a3c-a513-45453dc811f2", "node_type": "4", "metadata": {"filename": "combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.md", "extension": ".md", "title": "Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation", "date": "May 28, 2023", "url": "https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b"}, "hash": "a1f1d867138ca99619507287e6a79335ca026926fb30d4b5af6bb906fbf70133", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38392555-0e09-49d2-9a09-194d4ac176fb", "node_type": "1", "metadata": {"filename": "combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.md", "extension": ".md", "title": "Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation", "date": "May 28, 2023", "url": "https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b"}, "hash": "43fe1a06f0aea38b0d7c12c2559eb0c36638da9521d652c60b428a9184ee03b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f76a50e8-370b-45ef-b12c-28acf5385ccd", "node_type": "1", "metadata": {}, "hash": "86a9b0794045003d4c4e8394e26d83420eef0f8dfb1e5208ff1ddedd6d25916b", "class_name": "RelatedNodeInfo"}}, "text": "Each of these stacks solves particular use cases.\n\n##  Text-to-SQL Over Structured Data\n\nIn the structured setting, SQL is an extremely expressive language for\noperating over tabular data \u2014 in the case of analytics, you can get\naggregations, join information across multiple tables, sort by timestamp, and\nmuch more. Using the LLM to convert natural language to SQL can be thought as\na program synthesis \u201ccheat code\u201d \u2014 just let the LLM compile to the right SQL\nquery, and let the SQL engine on the database handle the rest!\n\n**Use Case:** Text-to-SQL queries are well-suited for analytics use cases\nwhere the answer can be found by executing a SQL statement. They are not\nsuited for cases where you\u2019d need more detail than what is found in a\nstructured table, or if you\u2019d need more sophisticated ways of determining\nrelevance to the query beyond simple constructs like ` WHERE ` conditions.\n\n**Example queries suited for Text-to-SQL:**\n\n  * \u201cWhat is the average population of cities in North America\u201d? \n  * \u201cWhat are the largest cities and populations in each respective continent?\u201d \n\n##  Semantic Search over Unstructured Data\n\nIn the unstructured setting, the behavior for retrieval-augmented generation\nsystems is to first perform retrieval and then synthesis. During retrieval, we\nfirst look up the most relevant documents to the query by embedding\nsimilarity. ", "mimetype": "text/plain", "start_char_idx": 2041, "end_char_idx": 3406, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f76a50e8-370b-45ef-b12c-28acf5385ccd": {"__data__": {"id_": "f76a50e8-370b-45ef-b12c-28acf5385ccd", "embedding": null, "metadata": {"filename": "combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.md", "extension": ".md", "title": "Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation", "date": "May 28, 2023", "url": "https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c699067-f5df-4a3c-a513-45453dc811f2", "node_type": "4", "metadata": {"filename": "combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.md", "extension": ".md", "title": "Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation", "date": "May 28, 2023", "url": "https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b"}, "hash": "a1f1d867138ca99619507287e6a79335ca026926fb30d4b5af6bb906fbf70133", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c7d9410-b6c0-4655-9966-05e18e50ebe3", "node_type": "1", "metadata": {"filename": "combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.md", "extension": ".md", "title": "Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation", "date": "May 28, 2023", "url": "https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b"}, "hash": "b2347936c4d9a98b3dfdc6a5b08788d77597c1decef09917d93107b695a2c800", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e370bda8-5ed0-4ca3-81fc-3a80411452df", "node_type": "1", "metadata": {}, "hash": "28a5dd354a86d8a211fd701e2b565246f8c9545127c8fa84f0d4691e52ab0807", "class_name": "RelatedNodeInfo"}}, "text": "Some vector stores support being able to handle additional\nmetadata filters for retrieval. We can choose to manually specify the set of\nrequired filters, or have the LLM \u201cinfer\u201d what the query string and metadata\nfilters should be (see our [ auto-retrieval modules\n](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/vector_stores/pinecone_auto_retriever.ipynb)\nin LlamaIndex or LangChain\u2019s [ self-query module\n](https://python.langchain.com/en/latest/modules/indexes/retrievers/examples/self_query.html)\n).\n\n**Use Case:** Retrieval Augmented Generation is well suited for queries where\nthe answer can be obtained within some sections of unstructured text data.\nMost existing vector stores (e.g. Pinecone, Chroma) do not offer a SQL-like\ninterface; hence they are less suited for queries that involve aggregations,\njoins, sums, etc.\n\n**Example queries suited for Retrieval Augmented Generation**\n\n  * \u201cTell me about the historical museums in Berlin\u201d \n  * \u201cWhat does Jordan ask from Nick on behalf of Gatsby?\u201d \n\n##  Combining These Two Systems\n\nFor some queries, we may want to make use of knowledge in **both structured\ntables as well as vector databases/document stores** in order to give the best\nanswer to the query. Ideally this can give us the best of both worlds: the\nanalytics capabilities over structured data, and semantic understanding over\nunstructured data.\n\nHere\u2019s an example use case:\n\n  * You have access to a collection of articles about different cities, stored in a vector database \n  * You also have access to a structured table containing statistics for each city. \n\nGiven this data collection, let\u2019s take an example query: \u201cTell me about the\narts and culture of the city with the highest population.\u201d\n\nThe \u201cproper\u201d way to answer this question is roughly as follows:\n\n  * Query the structured table for the city with the highest population. \n\n    \n    \n    SELECT city, population FROM city_stats ORDER BY population DESC LIMIT 1\n\n  * Convert the original question into a more detailed question: \u201cTell me about the arts and culture of Tokyo.\u201d \n  * Ask the new question over your vector database. \n  * Use the original question + intermediate queries/responses to SQL db and vector db to synthesize the answer. \n\nLet\u2019s think about some of the high-level implications of such a sequence:\n\n  * Instead of doing embedding search (and optionally metadata filters) to retrieve relevant context, we want to somehow have a SQL query as a first \u201cretrieval\u201d step. \n  * We want to make sure that we can somehow \u201cjoin\u201d the results from the SQL query with the context stored in the vector database. There is no existing language to \u201cjoin\u201d information between a SQL and vector database. We will have to implement this behavior ourselves. \n  ", "mimetype": "text/plain", "start_char_idx": 3406, "end_char_idx": 6168, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e370bda8-5ed0-4ca3-81fc-3a80411452df": {"__data__": {"id_": "e370bda8-5ed0-4ca3-81fc-3a80411452df", "embedding": null, "metadata": {"filename": "combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.md", "extension": ".md", "title": "Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation", "date": "May 28, 2023", "url": "https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c699067-f5df-4a3c-a513-45453dc811f2", "node_type": "4", "metadata": {"filename": "combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.md", "extension": ".md", "title": "Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation", "date": "May 28, 2023", "url": "https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b"}, "hash": "a1f1d867138ca99619507287e6a79335ca026926fb30d4b5af6bb906fbf70133", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f76a50e8-370b-45ef-b12c-28acf5385ccd", "node_type": "1", "metadata": {"filename": "combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.md", "extension": ".md", "title": "Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation", "date": "May 28, 2023", "url": "https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b"}, "hash": "2d82f78f3ae920c103e48d5921742970fbb78b7bf3c9128ff4e6fd126c2714c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6db23bf-bd37-4899-8071-6e5facc4e15d", "node_type": "1", "metadata": {}, "hash": "4f22d6964026be4a9bff245ded20f97d8f79a465857630961e4476cb79785e11", "class_name": "RelatedNodeInfo"}}, "text": "* Neither data source can answer this question on its own. The structured table only contains population information. The vector database contains city information but no easy way to query for the city with the maximum population. \n\n#  A Query Engine to Combine Structured Analytics and Semantic Search\n\nWe have created a brand-new query engine ( ` SQLAutoVectorQueryEngine ` ) that\ncan query, join, sequence, and combine both structured data from both your SQL\ndatabase and unstructured data from your vector database in order to\nsynthesize the final answer.\n\nThe ` SQLAutoVectorQueryEngine ` is initialized through passing in a SQL query\nengine ( ` GPTNLStructStoreQueryEngine ` ) as well as a query engine that uses\nour vector store [ auto-retriever module\n](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/vector_stores/pinecone_auto_retriever.ipynb)\n( ` VectorIndexAutoRetriever ` ). Both the SQL query engine and vector query\nengines are wrapped as \u201cTool\u201d objects containing a ` name ` and ` description\n` field.\n\n> _Reminder: the_ ` _VectorIndexAutoRetriever_ ` _takes in a natural language\n> query as input. Given some knowledge of the metadata schema of the vector\n> database, the auto retriever first_ infers _the other necessary query\n> parameters to pass in (e.g. top-k value, and metadata filters), and executes\n> a query against the vector database with all the query parameters._\n\nDiagram of the flow for SQLAutoVectorQueryEngine\n\nDuring query-time, we run the following steps:\n\n  1. A selector prompt (similarly used in our ` [ RouterQueryEngine ](https://gpt-index.readthedocs.io/en/latest/reference/query/query_engines/router_query_engine.html) ` , see [ guide ](https://gpt-index.readthedocs.io/en/latest/examples/query_engine/RouterQueryEngine.html) ) first chooses whether we should query the SQL database or the vector database. If it chooses to use the vector query engine, then the rest of the function execution is the same as querying the ` RetrieverQueryEngine ` with ` VectorIndexAutoRetriever ` . \n  2. If it chooses to query the SQL database, it will execute a text-to-SQL query operation against the database, and (optionally) synthesize a natural language output. \n  3. A **query transformation** is run, to convert the original question into a more detailed question given the results from the SQL query. For instance if the original question is \u201cTell me about the arts and culture of the city with the highest population.\u201d, and the SQL query returns Tokyo as the city with the highest population, then the new query is \u201cTell me about the arts and culture of Tokyo.\u201d The one exception is if the SQL query itself is enough to answer the original question; if it is, then function execution returns with the SQL query as the response. \n  4. The new query is then run through through the vector store query engine, which performs retrieval from the vector store and then LLM response synthesis. We enforce using a ` VectorIndexAutoRetriever ` module. This allows us to automatically infer the right query parameters (query string, top k, metadata filters), given the result of the SQL query. For instance, with the example above, we may infer the query to be something like ` query_str=\"arts and culture\" ` and ` filters={\"title\": \"Tokyo\"} ` . \n  5. The original question, SQL query, SQL response, vector store query, and vector store response are combined into a prompt to synthesize the final answer. \n\nTaking a step back, here are some general comments about this approach:\n\n  * Using our auto-retrieval module is our way of _simulating_ a join between the SQL database and vector database. We effectively use the results from our SQL query to determine the parameters to query the vector database with. \n  * This also implies that there doesn\u2019t need to be an explicit mapping between the items in the SQL database and the metadata in the vector database, since we can rely on the LLM being able come up with the right query for different items. It would be interesting to model explicit relationships between structured tables and document store metadata though; that way we don\u2019t need to spend an extra LLM call in the auto-retrieval step inferring the right metadata filters. \n\n#  Experiments\n\nSo how well does this work? It works surprisingly well across a broad range of\nqueries, from queries that can leverage both structured data and unstructured\ndata to queries that are specific to a structured data collection or\nunstructured data collection.\n\n##  Setup\n\nOur experiment setup is very simple. We have a SQL table called ` city_stats `\nwhich contains the city, population, and country of three different cities:\nToronto, Tokyo, and Berlin.\n\nWe also use a Pinecone index to store Wikipedia articles corresponding to the\nthree cities. Each article is chunked up and stored as a separate \u201cNode\u201d\nobject; each chunk also contains a ` title ` metadata attribute containing the\ncity name.\n\nWe then derive the ` VectorIndexAutoRetriever ` and ` RetrieverQueryEngine `\nfrom the Pinecone vector index.\n\n    \n    \n    from llama_index.indices.vector_store.retrievers import VectorIndexAutoRetriever\n    from llama_index.vector_stores.types import MetadataInfo, VectorStoreInfo\n    from llama_index.query_engine.retriever_query_engine import RetrieverQueryEngine\n    \n    \n    vector_store_info = VectorStoreInfo(\n        content_info='articles about different cities',\n        metadata_info=[\n            MetadataInfo(\n                name='city', \n                type='str', \n                description='The name of the city'),\n        ]\n    )\n    vector_auto_retriever = VectorIndexAutoRetriever(vector_index, vector_store_info=vector_store_info)\n    \n    retriever_query_engine = RetrieverQueryEngine.from_args(\n        vector_auto_retriever, service_context=service_context\n    )\n\nYou can also get the SQL query engine as follows\n\n    \n    \n    sql_query_engine = sql_index.as_query_engine()\n\nBoth the SQL query engine and vector query engine can be wrapped as `\nQueryEngineTool ` objects.\n\n    \n    \n    sql_tool = QueryEngineTool.from_defaults(\n        query_engine=sql_query_engine,\n        description=(\n            'Useful for translating a natural language query into a SQL query over a table containing: '\n            'city_stats, containing the population/country of each city'\n        )\n    )\n    vector_tool = QueryEngineTool.from_defaults(\n        query_engine=query_engine,\n        description=f'Useful for answering semantic questions about different cities',\n    )\n\nFinally, we can define our ` SQLAutoVectorQueryEngine `\n\n    \n    \n    query_engine = SQLAutoVectorQueryEngine(\n        sql_tool,\n        vector_tool,\n        service_context=service_context\n    )\n\n##  Results\n\nWe run some example queries.\n\n**Query 1**\n\n    \n    \n    query_engine.query(\n      'Tell me about the arts and culture of the city with the highest population'\n    )\n\nIntermediate steps:\n\nFinal Response:\n\n    \n    \n    Tokyo is the city with the highest population, with 13.96 million people. ", "mimetype": "text/plain", "start_char_idx": 6168, "end_char_idx": 13205, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6db23bf-bd37-4899-8071-6e5facc4e15d": {"__data__": {"id_": "f6db23bf-bd37-4899-8071-6e5facc4e15d", "embedding": null, "metadata": {"filename": "combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.md", "extension": ".md", "title": "Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation", "date": "May 28, 2023", "url": "https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c699067-f5df-4a3c-a513-45453dc811f2", "node_type": "4", "metadata": {"filename": "combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.md", "extension": ".md", "title": "Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation", "date": "May 28, 2023", "url": "https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b"}, "hash": "a1f1d867138ca99619507287e6a79335ca026926fb30d4b5af6bb906fbf70133", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e370bda8-5ed0-4ca3-81fc-3a80411452df", "node_type": "1", "metadata": {"filename": "combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.md", "extension": ".md", "title": "Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation", "date": "May 28, 2023", "url": "https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b"}, "hash": "4fd5d27c4bc870c7fefe577db717bd99d79dfe74848781e1db45d786a7cc6b5e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39af304a-cf03-4a1f-a482-d3a93f5e89e1", "node_type": "1", "metadata": {}, "hash": "6bccc921aa00c541a324fe8e58fa0719a9443c55b6916fed0bcdcb6849b61409", "class_name": "RelatedNodeInfo"}}, "text": "It is a vibrant city with a rich culture and a wide variety of art forms. ", "mimetype": "text/plain", "start_char_idx": 13205, "end_char_idx": 13279, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "39af304a-cf03-4a1f-a482-d3a93f5e89e1": {"__data__": {"id_": "39af304a-cf03-4a1f-a482-d3a93f5e89e1", "embedding": null, "metadata": {"filename": "combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.md", "extension": ".md", "title": "Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation", "date": "May 28, 2023", "url": "https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c699067-f5df-4a3c-a513-45453dc811f2", "node_type": "4", "metadata": {"filename": "combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.md", "extension": ".md", "title": "Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation", "date": "May 28, 2023", "url": "https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b"}, "hash": "a1f1d867138ca99619507287e6a79335ca026926fb30d4b5af6bb906fbf70133", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f6db23bf-bd37-4899-8071-6e5facc4e15d", "node_type": "1", "metadata": {"filename": "combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.md", "extension": ".md", "title": "Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation", "date": "May 28, 2023", "url": "https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b"}, "hash": "cb7ff9495e038a261ab2054ab383401a72424558fe11a61992c0871a36d45e2c", "class_name": "RelatedNodeInfo"}}, "text": "From traditional Japanese art such as calligraphy and woodblock prints to modern art galleries and museums, Tokyo has something for everyone. There are also many festivals and events throughout the year that celebrate the city's culture and art. Some popular festivals and events in Tokyo include the Sann\u014d Festival at Hie Shrine, the Sanja Festival at Asakusa Shrine, and the biennial Kanda Festivals. These events often feature parades with elaborately decorated floats and thousands of people. Additionally, an enormous fireworks display over the Sumida River takes place annually on the last Saturday of July, attracting over a million viewers. During spring, when cherry blossoms bloom, many residents gather in Ueno Park, Inokashira Park, and the Shinjuku Gyoen National Garden for picnics under the blossoms.\n\nThis query runs through the full flow of the ` SQLAutoVectorQueryEngine ` . It\nfirst queries the SQL database for the city with the highest population\n(\u201cTokyo\u201d), and then queries the vector database with the new query. The\nresults are combined into a final response.\n\n**Query 2**\n\n    \n    \n    query_engine.query(\"Tell me about the history of Berlin\")\n\nIntermediate Steps:\n\nFinal Response:\n\n    \n    \n    Berlin's history dates back to the early 13th century when it was founded as a small settlement. In 1618, the Margraviate of Brandenburg entered into a personal union with the Duchy of Prussia, and in 1701, they formed the Kingdom of Prussia with Berlin as its capital. The city grew and merged with neighboring cities, becoming a center of the Enlightenment under the rule of Frederick the Great in the 18th century.\n    \n    The Industrial Revolution in the 19th century transformed Berlin, expanding its economy, population, and infrastructure. In 1871, it became the capital of the newly founded German Empire. The early 20th century saw Berlin as a hub for the German Expressionist movement and a major world capital known for its contributions to science, technology, arts, and other fields.\n    \n    In 1933, Adolf Hitler and the Nazi Party came to power, leading to a decline in Berlin's Jewish community and the city's involvement in World War II. After the war, Berlin was divided into East and West Berlin, with the former under Soviet control and the latter under the control of the United States, United Kingdom, and France. The Berlin Wall was built in 1961, physically and ideologically dividing the city until its fall in 1989. Following the reunification of Germany in 1990, Berlin once again became the capital of a unified Germany and has since continued to grow and develop as a major global city.\n\nThis query only requires the vector database and not the SQL database. The\ninitial selector correctly identifies that we should just query the vector\ndatabase and return the result.\n\n**Query 3**\n\n    \n    \n    query_engine.query('Can you give me the country corresponding to each city?')\n\nIntermediate Steps\n\nFinal Response\n\n    \n    \n     Toronto is in Canada, Tokyo is in Japan, and Berlin is in Germany.\n\nThis query can be answered by just querying the SQL database, it does not need\nadditional information from the vector database. The query transform step\ncorrectly identifies \u201cNone\u201d as the followup question, indicating that the\noriginal question has been answered.\n\n#  Conclusion\n\nSo far, the stacks around LLMs + unstructured data and LLMs + structured data\nhave largely been separate. We\u2019re excited about how combining LLMs on top of\nboth structured and unstructured data can unlock new retrieval/query\ncapabilities in novel and interesting ways!\n\nWe\u2019d love for you to try out the ` SQLAutoVectorQueryEngine ` and let us know\nwhat you think.\n\nThe full notebook walkthrough can be found [ in this guide ](https://gpt-\nindex.readthedocs.io/en/latest/examples/query_engine/SQLAutoVectorQueryEngine.html)\n( [ associated notebook\n](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/query_engine/SQLAutoVectorQueryEngine.ipynb)\n).\n\n", "mimetype": "text/plain", "start_char_idx": 13279, "end_char_idx": 17271, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6f03331a-a943-4a29-aa3e-943022f4a6ad": {"__data__": {"id_": "6f03331a-a943-4a29-aa3e-943022f4a6ad", "embedding": null, "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "50ab3a10-e38e-4b1c-b2dc-eb1cd03df774", "node_type": "4", "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "hash": "089f9620e42064ae0cb2f5aea08a73dd5248413650d1318182ba309da86db390", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66a3d680-a36e-4eba-83fa-4c0571f01cbc", "node_type": "1", "metadata": {}, "hash": "5058c51b62e4a0abcfe713ca121285a80cd33d1205a2a72995ab99dfbce8e367", "class_name": "RelatedNodeInfo"}}, "text": "#  Summary\n\nIn this article, we compare how well LLM-powered agents with different degrees\nof complexity perform over practical data tasks (financial analysis). We\ncompare the performance of agents with more _complex, unrestrained_\ninteraction behavior (ReAct) with agents that contain _simpler, more\nconstrained_ interactions (routing). We specifically analyze how much\ncomplexity can be added to the agent layer vs. the tool layer.\n\nWe find that the choice of the language model matters a lot. ReAct agents that\nare powered by \u201cdumber\u201d models (in a tongue-in-cheek fashion we are referring\nto any non GPT-4 model as \u201cdumb\u201d) struggle to return relevant results over\ndata. We find that constraining agent interaction behavior, and giving them\naccess to more tools that can more explicitly perform complex actions, can\nhelp improve query performance over these less sophisticated LLMs. In\ncontrast, more sophisticated models (GPT-4) can more reliably utilize the\nReAct loop to execute a variety of complex data queries.\n\nThis blog post is quite detailed; we provide a _lot_ of experiments and\nresults below. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66a3d680-a36e-4eba-83fa-4c0571f01cbc": {"__data__": {"id_": "66a3d680-a36e-4eba-83fa-4c0571f01cbc", "embedding": null, "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "50ab3a10-e38e-4b1c-b2dc-eb1cd03df774", "node_type": "4", "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "hash": "089f9620e42064ae0cb2f5aea08a73dd5248413650d1318182ba309da86db390", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f03331a-a943-4a29-aa3e-943022f4a6ad", "node_type": "1", "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "hash": "5793d92d9a51fa28eade863d5d770cf737f3124f06e9efb94ed957d53262f493", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "36a99eb7-2155-4c9c-a153-6277c93f7ef5", "node_type": "1", "metadata": {}, "hash": "6bad38b6e18f0bb66a1fcdabad24c7c2751351af42568a0de4a590d29afa1242", "class_name": "RelatedNodeInfo"}}, "text": "Best of all, you can run this all yourself with our [ example\nnotebook\n](https://colab.research.google.com/drive/1uP38k4nr8OPmXbY4dLoKKQW0F29WtNuY?usp=sharing)\n!\n\n#  Overview of Agents\n\nBuilding LLM-powered agents have gotten increasingly popular in the past few\nmonths. Frameworks like [ LangChain ](https://github.com/hwchase17/langchain)\nhave made it much easier to create these agents according to a set of common\nabstractions.\n\nAt a high-level, an \u201cagent\u201d is essentially an automated decision engine, that\ncan be used to interact with an external environment. The core agent loop\nlooks something like the following:\n\n  1. The agent has access to a set of \u201ctools\u201d, which are generic functions that it can perform. It has an awareness of each tool through some attached metadata, and it can call each tool (either as a function call or structured API). \n  2. ", "mimetype": "text/plain", "start_char_idx": 1107, "end_char_idx": 1969, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "36a99eb7-2155-4c9c-a153-6277c93f7ef5": {"__data__": {"id_": "36a99eb7-2155-4c9c-a153-6277c93f7ef5", "embedding": null, "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "50ab3a10-e38e-4b1c-b2dc-eb1cd03df774", "node_type": "4", "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "hash": "089f9620e42064ae0cb2f5aea08a73dd5248413650d1318182ba309da86db390", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66a3d680-a36e-4eba-83fa-4c0571f01cbc", "node_type": "1", "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "hash": "2a56b4dc103a06c350573ef6698b7930a721f208c46128921056371e54566658", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cba26f53-2cee-4563-bc1d-d4242dcc2437", "node_type": "1", "metadata": {}, "hash": "7ecd3c9a48dba590095555c5589afa026956724a0823be95a008d1d857b04be2", "class_name": "RelatedNodeInfo"}}, "text": "User feeds in a natural language input to the agent. \n  3. Given the input, the agent **interacts with the set of tools** in some fashion and returns the response. \n\nThere\u2019s a variety of ways to perform **agent-tool interaction.**\n\n  * The most popular is probably [ ReAct ](https://arxiv.org/abs/2210.03629) : the agent reasons over the next action, constructs an action command, executes the action. It repeats these steps in an iterative loop until the task is complete. \n  ", "mimetype": "text/plain", "start_char_idx": 1969, "end_char_idx": 2446, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cba26f53-2cee-4563-bc1d-d4242dcc2437": {"__data__": {"id_": "cba26f53-2cee-4563-bc1d-d4242dcc2437", "embedding": null, "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "50ab3a10-e38e-4b1c-b2dc-eb1cd03df774", "node_type": "4", "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "hash": "089f9620e42064ae0cb2f5aea08a73dd5248413650d1318182ba309da86db390", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "36a99eb7-2155-4c9c-a153-6277c93f7ef5", "node_type": "1", "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "hash": "7282189f1b4c8e3ee93133e0095cd16d0e93bc5f954d05df797e810a52899a89", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3955352-61f1-4c45-92f2-b2c5f2638bf7", "node_type": "1", "metadata": {}, "hash": "494fd52d674fdda4b8b66647cb2932cebee8c388b16a3e0a422f5fa8f3857f47", "class_name": "RelatedNodeInfo"}}, "text": "* There are other interaction modes too. Recently there was a paper on [ Plan-and-solve Prompting ](https://arxiv.org/pdf/2305.04091.pdf) , which generates a plan beforehand (to decompose a complex task into simpler ones). Before ReAct there have also been related techniques on [ Self-Ask ](https://arxiv.org/abs/2210.03350) and [ Chain of Thought Prompting ](https://arxiv.org/abs/2201.11903) . \n\n##  \u201cComplex\u201d vs. \u201cSimple\u201d Agent Interaction Techniques\n\nWe classify techniques like ReAct are more _complex and unconstrained:_ this\nis because they perform iterative reasoning and also break the input into\nsmaller steps. Complicated agent interaction loops allow for more _freedom of\nbehavior,_ and create an increased burden on the LLM being used. The pro of\ncomplex interaction frameworks is that they can be more general and handle a\nbroader class of queries over simple tools. The con is that if the LLM is not\nup to par, then these frameworks are prone to making mistakes; unconstrained\nbehavior can lead to unexpected results.\n\nOn the other end of the spectrum, you can imagine a _simple and constrained_\nagent interaction mechanism, where the agent does one-step selection of the\nunderlying tool to use, and returns the response from the tool. The agent\nessentially just acts as a router from the query to Tool. There are no steps\nto break down the question into smaller ones, and no iterative chain-of-\nthought loops. The pro here is that the model will likely make fewer errors.\nThe con here is that the interaction technique allows for less freedom and\nimposes more constraints on behavior.\n\n##  Investigating Agent Interaction Techniques for Data Querying\n\nWe at LlamaIndex are interested in how agents can help augment data tasks.\nMore specifically, we are interested in how agents can help perform complex\nuser queries over a diverse range of data sources. This includes not only\nasking questions over a single document, but being able to synthesize insights\nacross multiple documents and return that to the user.\n\nLlamaIndex query engines can be used as Tools within an agent construct to\nquery your data (we provide [ seamless integrations with LangChain\n](https://gpt-\nindex.readthedocs.io/en/latest/how_to/integrations/using_with_langchain.html)\n). These Tools can vary in complexity. For instance, a _simple_ Tool could be\nour [ vector store query engine ](https://gpt-\nindex.readthedocs.io/en/latest/how_to/integrations/vector_stores.html) , which\ndoes top-k embedding retrieval from a vector store. A more _advanced_ tool\ncould be a query engine over our graph data structure, which can be setup to [\nexplicitly provide compare/contrast capabilities ](https://gpt-\nindex.readthedocs.io/en/latest/use_cases/queries.html#compare-contrast-\nqueries) over any subset of documents. The tool itself can contain \u201cagent-\nlike\u201d decision-making capabilities under the hood. LlamaIndex provides a\nvariety of modules around [ routing ](https://gpt-\nindex.readthedocs.io/en/latest/examples/query_engine/RouterQueryEngine.html) ,\n[ query decomposition ](https://gpt-\nindex.readthedocs.io/en/latest/how_to/query/query_transformations.html#single-\nstep-query-decomposition) , and [ multi-part query planning ](https://gpt-\nindex.readthedocs.io/en/latest/examples/query_engine/sub_question_query_engine.html)\n.\n\nIn this blog post, we are interested in comparing the following approaches to\ndesigning agents and tools to see which approach can provide good answers to\ndifferent user queries in a robust fashion:\n\n  * more _complex and unconstrained_ agent interaction (ReAct) over a set of _simple_ Tools \n  * more _simple and constrained_ agent interaction (simple routing) that uses more _complex_ Tools \n\nComplex Agents with Simple Tools, Simple Agents with Complex Tools\n\nEssentially what we are interested in is how much complexity can be pushed to\nthe agent interaction layer vs. being left in the Tool layer. We explore the\nfollowing concrete example: let\u2019s say the user query is to compare/contrast\ntwo different documents (a relatively complex query). If the set of Tools are\nall just vector indices over different documents, could the agent interaction\nloop figure out how to execute that query reliably against the vector indices?\nOn the other hand, if we push the complexity down to the Tool layer, then we\ncould _explicitly_ have a Tool that can perform \u201ccompare/contrast\u201d over your\nDocuments. Then the burden on the agent is to simply call this Tool instead of\ninteracting with a set of other tools in a more complex fashion.\n\n##  High-Level Findings\n\nThe high-level finding is that **less sophisticated agents need more\nconstraints.** More specifically, we found that using a GPT-3 powered agent in\na ReAct loop did not provide good results over complex queries; it was not\nable to figure out the proper interaction pattern over the provided set of\nTools in order to surface the results. Instead, by adding more constraints to\nthe agent behavior and providing more sophistication in the Tool itself, we\nwere able to get a GPT-3 agent to produce better results.\n\n**Smarter agents require fewer constraints.** We did find that GPT-4 agents\nwith ReAct were able to provide better query results than GPT-3 agents when\npresented with a set of simple Tools over the data. This implies that more\npowerful agents may not need as many tools to \u201cexplicitly\u201d perform tasks when\nmuch of that logic can be handled in the agent interaction loop.\n\n#  Setup\n\nOur data consists of three Uber 10-Q filings (quarterly financial reports) in\n2022: March, June, and September. We wish to execute different queries over\nthis data; the bulk of these queries are around comparing different bits of\ninformation between these documents.\n\n    \n    \n    march_2022 = SimpleDirectoryReader(input_files=[\"../data/10q/uber_10q_march_2022.pdf\"]).load_data()\n    june_2022 = SimpleDirectoryReader(input_files=[\"../data/10q/uber_10q_june_2022.pdf\"]).load_data()\n    sept_2022 = SimpleDirectoryReader(input_files=[\"../data/10q/uber_10q_sept_2022.pdf\"]).load_data()\n\nWe use LlamaIndex to define a vector index over each document, which just\nstores the document chunks + embeddings in a vector store. We can then query\neach vector index using a simple ` QueryEngine ` . ", "mimetype": "text/plain", "start_char_idx": 2446, "end_char_idx": 8695, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3955352-61f1-4c45-92f2-b2c5f2638bf7": {"__data__": {"id_": "e3955352-61f1-4c45-92f2-b2c5f2638bf7", "embedding": null, "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "50ab3a10-e38e-4b1c-b2dc-eb1cd03df774", "node_type": "4", "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "hash": "089f9620e42064ae0cb2f5aea08a73dd5248413650d1318182ba309da86db390", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cba26f53-2cee-4563-bc1d-d4242dcc2437", "node_type": "1", "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "hash": "f984c0e806c0eb12a819eda8079e5a785d6401679df32ab2feaf4fe080165a91", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "249765ad-5eaa-4f4e-ace6-f144ca1a24f9", "node_type": "1", "metadata": {}, "hash": "ac09a3de12e9914dd575d0e9cafb2839876ba7d723b3b176106cea1d721d5db3", "class_name": "RelatedNodeInfo"}}, "text": "We create a tool for each\nof these ` QueryEngine ` objects.\n\n    \n    \n    # define indices\n    march_index = GPTVectorStoreIndex.from_documents(march_2022)\n    june_index = GPTVectorStoreIndex.from_documents(june_2022)\n    sept_index = GPTVectorStoreIndex.from_documents(sept_2022)\n    \n    # define query engine\n    march_engine = march_index.as_query_engine(similarity_top_k=3)\n    june_engine = june_index.as_query_engine(similarity_top_k=3)\n    sept_engine = sept_index.as_query_engine(similarity_top_k=3)\n\nWe also define a ` ComposableGraph ` over these three documents. The\ncomposable graph roughly follows the [ guide described here ](https://gpt-\nindex.readthedocs.io/en/latest/use_cases/queries.html#compare-contrast-\nqueries) . This graph is explicitly setup to perform compare/contrast queries\nover these three documents.\n\n    \n    \n    graph = ComposableGraph.from_indices(\n        GPTListIndex,\n        children_indices=[march_index, june_index, sept_index],\n        index_summaries=[\n            \"Provides information about Uber quarterly financials ending March 2022\",\n            \"Provides information about Uber quarterly financials ending June 2022\",\n            \"Provides information about Uber quarterly financials ending September 2022\"\n        ]\n    )\n\nThe graph can be queried with a ` ComposableGraphQueryEngine ` :\n\n    \n    \n    # define decompose_transform\n    decompose_transform = DecomposeQueryTransform(verbose=True)\n    \n    # define custom query engines\n    custom_query_engines = {}\n    for index in [march_index, june_index, sept_index]:\n        query_engine = index.as_query_engine(service_context=service_context)\n        query_engine = TransformQueryEngine(\n            query_engine,\n            query_transform=decompose_transform,\n            transform_extra_info={'index_summary': index.index_struct.summary},\n        )\n        custom_query_engines[index.index_id] = query_engine\n    \n    custom_query_engines[graph.root_id] = graph.root_index.as_query_engine(\n        service_context=service_context,\n        streaming=True,\n    )\n    \n    # define graph\n    g_engine = graph.as_query_engine(\n        custom_query_engines=custom_query_engines\n    )\n\nWe try the following agent setups:\n\n  * **GPT-3 ReAct agent:** A zero-shot GPT-3 ReAct agent with three Tools: each Tool corresponds to the vector index over a 10-Q filing. \n  * **GPT-4 ReAct agent:** Same as above but using GPT-4 instead. \n  ", "mimetype": "text/plain", "start_char_idx": 8695, "end_char_idx": 11131, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "249765ad-5eaa-4f4e-ace6-f144ca1a24f9": {"__data__": {"id_": "249765ad-5eaa-4f4e-ace6-f144ca1a24f9", "embedding": null, "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "50ab3a10-e38e-4b1c-b2dc-eb1cd03df774", "node_type": "4", "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "hash": "089f9620e42064ae0cb2f5aea08a73dd5248413650d1318182ba309da86db390", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3955352-61f1-4c45-92f2-b2c5f2638bf7", "node_type": "1", "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "hash": "27a8355ffef2e7a06395cd130664d3ecc8aa71057b210a2f0a55a02e1d5d0190", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "914af9df-8885-4724-b233-f1e8982aed9f", "node_type": "1", "metadata": {}, "hash": "626c842461e442cdefebd519c8060a8a35d75393dfbd570406e432aacf9d56dd", "class_name": "RelatedNodeInfo"}}, "text": "* **Simple Router agent:** A simple router \u201cagent\u201d with four Tools: the three Tools listed above + the ` ComposableGraphQueryEngine ` explicitly setup to perform compare/contrast queries. \n\nThe code snippets for initializing these agents are below. For the simple\nrouter agent, we use the native ` RouterQueryEngine ` within LlamaIndex,\nthough you should also be able to achieve similar results in LangChain through\neither the zero-shot agent (with tweaked settings) or the router chain.\n\n##  **GPT-3/GPT-4 ReAct Agent Setup**\n\n    \n    \n    # initializing zero-shot ReAct agent\n    \n    uber_config_sept = IndexToolConfig(\n        query_engine=sept_engine, \n        name=f\"Uber 10Q September 2022\",\n        description=f\"Provides information about Uber quarterly financials ending September 2022\",\n        tool_kwargs={\"return_direct\": False}\n    )\n    uber_config_june = IndexToolConfig(\n        query_engine=june_engine, \n        name=f\"Uber 10Q June 2022\",\n        description=f\"Provides information about Uber quarterly financials ending June 2022\",\n        tool_kwargs={\"return_direct\": False}\n    )\n    uber_config_march = IndexToolConfig(\n        query_engine=march_engine, \n        name=f\"Uber 10Q March 2022\",\n        description=f\"Provides information about Uber quarterly financials ending March 2022\",\n        tool_kwargs={\"return_direct\": False}\n    )\n    \n    toolkit = LlamaToolkit(\n        index_configs=[uber_config_sept, uber_config_june, uber_config_march],\n    )\n    \n    # this is a light wrapper around `initialize_agent` in langchain (which defaults to zero-shot)\n    agent_chain = create_llama_agent(\n        toolkit,\n        llm, # can be GPT-3 or GPT-4 \n        verbose=True\n    )\n\n##  Simple Router Agent Setup\n\n    \n    \n    query_tool_sept = QueryEngineTool.from_defaults(\n        query_engine=sept_engine,\n        description=f\"Provides information about Uber quarterly financials ending September 2022\",\n    )\n    query_tool_june = QueryEngineTool.from_defaults(\n        query_engine=june_engine,\n        description=f\"Provides information about Uber quarterly financials ending June 2022\",\n    )\n    query_tool_march = QueryEngineTool.from_defaults(\n        query_engine=march_engine,\n        description=f\"Provides information about Uber quarterly financials ending March 2022\",\n    )\n    query_tool_graph = QueryEngineTool.from_defaults(\n        query_engine=g_engine,\n        description=f\"Provides comparisons between Uber financials across quarters in 2022. Can be used to answer \"\n                     \"any questions that require analysis across multiple quarters.", "mimetype": "text/plain", "start_char_idx": 11131, "end_char_idx": 13734, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "914af9df-8885-4724-b233-f1e8982aed9f": {"__data__": {"id_": "914af9df-8885-4724-b233-f1e8982aed9f", "embedding": null, "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "50ab3a10-e38e-4b1c-b2dc-eb1cd03df774", "node_type": "4", "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "hash": "089f9620e42064ae0cb2f5aea08a73dd5248413650d1318182ba309da86db390", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "249765ad-5eaa-4f4e-ace6-f144ca1a24f9", "node_type": "1", "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "hash": "993e5882338c2debec633cef720e6ca4e1ccdf9aa018f19fdb76c8b4af371e2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af188508-8db4-452a-ba79-32646a92725a", "node_type": "1", "metadata": {}, "hash": "266410aea8f5ad93f4d369e505c8f710e536b6ceca451c4f6ee60099761f345d", "class_name": "RelatedNodeInfo"}}, "text": "\",\n    )\n    \n    # our \"router\" query engine is effectively a simple agent that can only perform routing\n    query_engine = RouterQueryEngine(\n        selector=LLMSingleSelector.from_defaults(),\n        query_engine_tools=[\n            query_tool_sept,\n            query_tool_june,\n            query_tool_march,\n            query_tool_graph\n        ]\n    )\n\nNow that we\u2019ve described the setup, let\u2019s take a look at the results below!\n\n#  Findings and Experiments\n\nAt a high-level, we find using GPT-3 in ReAct agents produces suboptimal\nresults over these queries. They tend to exhibit the following\ncharacteristics:\n\n  * **Unpredictability in the set of chosen tools:** The set of tools chosen can differ even if the questions are semantically similar, leading to variability in the responses. \n  * **Lack of coverage in the set of chosen tools:** Oftentimes we expect that a given question is able to make use of all three 10-Q statements, but only a subset of them are picked. \n  * **Erroneous chain-of-thought processing:** Sometimes the agent uses tools throughout the CoT process that are irrelevant to the question. \n\nIn contrast, we find that GPT-4 ReAct agents provide answers that are more\nrelevant, predictable, and exhibit fewer errors in intermediate results.\n\nFinally, we find that using a simpler routing-only GPT-3 agent with access to\nan explicit \u201ccompare/contrast\u201d tool allows the agent to perform better.\n\nAs a reminder, full results are in the notebook: [\nhttps://colab.research.google.com/drive/1uP38k4nr8OPmXbY4dLoKKQW0F29WtNuY?usp=sharing\n](https://colab.research.google.com/drive/1uP38k4nr8OPmXbY4dLoKKQW0F29WtNuY?usp=sharing)\n\n##  GPT-3 ReAct Agent Results\n\n**Query 1**\n\n    \n    \n    agent_chain.run(input=\"Analyze Uber revenue growth over the last few quarters\")\n\nResponse:\n\nWe see that only the September 10-Q filing is chosen to answer the question.\nThe September 10-Q does contain some information about revenue growth compared\nto the same period in 2021, but that doesn\u2019t explicitly answer the question,\nwhich is about revenue growth the past few quarters.\n\n**Query 2**\n\n    \n    \n    agent_chain.run(input=\"Analyze changes in risk factors for Uber\")\n\nResponse:\n\nThe September and June 10-Q filings are chosen, but not March. Moreover, the\nanswer is vague and doesn\u2019t provide much detail regarding concrete risk\nfactors for Uber (and also mentions that the risk factors \u201chave changed over\nthe past three quarters\u201d even though it\u2019s only using two Tools).\n\n**Query 3**\n\nIn this query, we more explicitly showcase how slight changes in prompts can\ninduce different chain-of-thought paths through different Tools, and as a\nresult produce different answers.\n\n    \n    \n    # Prompt variation 1 \n    agent_chain.run(input=\"Analyze Uber revenue growth and risk factors over time\")\n\nResponse:\n\n    \n    \n    # Prompt variation 2\n    agent_chain.run(input=\"Analyze Uber revenue growth and risk factors over quarters\")\n\nThe main difference between these two queries is \u201cover time\u201d versus \u201cover\nquarters.\u201d As we can see, not only are the selected Tools different between\nthe two variations, but the inputs are different as well \u2014 in the first it\u2019s\n\u201cfinancials\u201d, and in the second it\u2019s \u201cRevenue growth and risk factors.\u201d\n\nSince the Tool input in the first variant is unrelated to the question, the\nanswer is similarly vague: \u201cUber\u2019s revenue growth and risk factors can be\nanalyzed by comparing the financials\u2026\u201d\n\n**Query 4:**\n\nHere instead of asking a compare/contrast question let\u2019s just ask a question\nabout a given statement.\n\n    \n    \n    agent_chain.run(input=\"How much cash did Uber have in sept 2022?\")\n\nWe see that the agent makes two errors 1) it is not able to supply an action\ninput to each Tool, and 2) ends up looking through the June and March filings\nwhich are irrelevant to the question.\n\n##  GPT-4 ReAct Agent Results\n\nGPT-4 ReAct agents perform a lot better than GPT-3 agents. They\ncomprehensively go through the set of available Tools, and provide much more\ndetailed observation extraction and response synthesis.\n\nWe won\u2019t go through all of these examples, but they can be found in the\nexample notebook!\n\n**Query 1:**\n\n    \n    \n    agent_chain_gpt4.run(input=\"Analyze Uber revenue growth over the last few quarters\")\n\nResponse:\n\nUnlike the GPT-3 agent, here the GPT-4 agent at least goes through every\nfiling and synthesizes the result.\n\n**Query 2**\n\n    \n    \n    agent_chain_gpt4.run(input=\"Analyze changes in risk factors for Uber\")\n\nResponse:\n\nHere the GPT-4 agent still only looks at September and June (and skips March),\nbut the response is way more detailed, and references concrete facts within\nthe report.\n\n##  GPT-3 Router Agent Results\n\nReminder: the router agent doesn\u2019t do any CoT and has additional access to our\nComposableGraph query engine, which can explicitly perform compare/contrast\nqueries. Let\u2019s take a look at how this agent does.\n\n**Query 1**\n\n    \n    \n    response = query_engine.query(\"Analyze Uber revenue growth over the last few quarters\")\n\nResponse (intermediate steps):\n\n    \n    \n    Selecting query engine 3: Provides comparisons between Uber financials across quarters in 2022. Can be used to answer any questions that require analysis across multiple quarters..\n    > Current query: Analyze Uber revenue growth over the last few quarters\n    > New query:  What was Uber's revenue growth from the last quarter ending March 2022 compared to the previous quarter?\n    > Current query: Analyze Uber revenue growth over the last few quarters\n    > New query:  What was Uber's revenue growth from the last quarter ending June 2022 compared to the previous quarter?\n    > Current query: Analyze Uber revenue growth over the last few quarters\n    > New query:  What was Uber's revenue growth from the last few quarters ending September 2022?\n    \n\nFinal Response\n\n    \n    \n    Based on the limited context information provided, we can only analyze Uber's revenue growth for two specific quarters:\n    \n    1. For the quarter ending March 2022, Uber's revenue growth increased compared to the first quarter of 2021. However, the exact percentage of growth is not provided.\n    \n    2. For the quarter ending September 2022, Uber's revenue growth was 72%.\n    \n    Unfortunately, we do not have enough data to analyze the revenue growth for the quarters ending June 2022 or to compare it to the previous quarter. To provide a more comprehensive analysis, additional data on Uber's revenue growth for other quarters would be needed.\n\nTo be fair, this response does not cover the month of June. But the fault of\nthat lies with the implementation of the Tool itself rather than the agent\ninteraction step. The agent\u2019s only job is to pick the right Tool to solve the\ntask at hand, and it does that job correctly; it decides to choose the Tool\ncorresponding to our ` ComposableGraphQueryEngine ` abstraction (which can\nperform compare/contrast queries).\n\n**Query 2**\n\n    \n    \n    response = query_engine.query(\"Analyze changes in risk factors for Uber\")\n\nResponse (intermediate steps):\n\n    \n    \n    INFO:llama_index.query_engine.router_query_engine:Selecting query engine 3: Provides comparisons between Uber financials across quarters in 2022. Can be used to answer any questions that require analysis across multiple quarters..\n    Selecting query engine 3: Provides comparisons between Uber financials across quarters in 2022. Can be used to answer any questions that require analysis across multiple quarters..\n    &gt; Current query: Analyze changes in risk factors for Uber\n    &gt; New query:  What are the risk factors for Uber in the quarter ending March 2022?\n    &gt; Current query: Analyze changes in risk factors for Uber\n    &gt; New query:  What are the risk factors for Uber in the quarter ending June 2022?\n    &gt; Current query: Analyze changes in risk factors for Uber\n    &gt; New query:  What are the risk factors for Uber in the quarter ending September 2022?\n\nFinal Response\n\n    \n    \n    From the context information provided, we can analyze the changes in risk factors for Uber across the three quarters as follows:\n    \n    1. COVID-19 pandemic: The ongoing pandemic remains a constant risk factor across all quarters, with its impact on driver and courier supply constraints, consumer demand, and government restrictions. However, the specific concerns related to the pandemic may evolve, such as the emergence of new virus variants or changes in vaccination rates.\n    \n    2. Driver and courier supply constraints: This risk factor is present in the quarter ending March 2022 but is not explicitly mentioned in the subsequent quarters. However, it may still be an underlying concern, especially if the pandemic continues to impact driver availability.\n    \n    3. Changes in consumer demand: This risk factor is mentioned in the quarter ending March 2022, with an imbalance between Mobility and Delivery services. It is not explicitly mentioned in the subsequent quarters, but fluctuations in revenue due to changes in Gross Bookings and Mobility Trip volumes in the quarter ending June 2022 could be related to changes in consumer demand.\n    \n    4. Government restrictions and regulations: This risk factor is present in the quarter ending March 2022, with the impact of restrictions on Mobility offerings. In the quarter ending June 2022, the focus shifts to the potential impact of regulatory changes and legal claims, such as driver classification in the UK. In the quarter ending September 2022, the risk factor of driver classification as employees, workers, or quasi-employees becomes more prominent.\n    \n    5. ", "mimetype": "text/plain", "start_char_idx": 13734, "end_char_idx": 23354, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "af188508-8db4-452a-ba79-32646a92725a": {"__data__": {"id_": "af188508-8db4-452a-ba79-32646a92725a", "embedding": null, "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "50ab3a10-e38e-4b1c-b2dc-eb1cd03df774", "node_type": "4", "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "hash": "089f9620e42064ae0cb2f5aea08a73dd5248413650d1318182ba309da86db390", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "914af9df-8885-4724-b233-f1e8982aed9f", "node_type": "1", "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "hash": "cdb37977a325ab15cc0d4d0ff65ddfeb42399be2ce115ab457b59df2f300bb32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "332b15be-0f94-4319-8f80-303365cafe05", "node_type": "1", "metadata": {}, "hash": "3c2c0916f4a6c54108e1b6aa20fcbaf5e655128907be4ac73978e39674a36e25", "class_name": "RelatedNodeInfo"}}, "text": "Uncertainty around future developments: This risk factor is mentioned in the quarter ending March 2022 but is not explicitly mentioned in the subsequent quarters. However, it may still be an underlying concern, especially with the ongoing impact of COVID-19 and potential permanent changes to end-users' behavior.\n    \n    6. Accounting pronouncements: This risk factor is introduced in the quarter ending June 2022, with the adoption and impact of recently issued accounting pronouncements potentially affecting Uber's consolidated financial statements.\n    \n    7. Integration and performance of acquired businesses: This risk factor is introduced in the quarter ending June 2022, with the acquisition of Transplace contributing to the increase in Freight revenue.\n    \n    8. Competition: This risk factor is introduced in the quarter ending September 2022, highlighting the highly competitive nature of the mobility, delivery, and logistics industries, and the need to lower fares, offer incentives, and promotions to remain competitive.\n    \n    9. Significant losses and increasing operating expenses: This risk factor is introduced in the quarter ending September 2022, emphasizing the financial challenges Uber faces in achieving profitability.\n\nThe job of the agent is once again simple: it chooses the graph query engine\nthat is optimized to perform \u201ccompare/contrast\u201d queries. The result is much\nmore detailed than the result obtained from the GPT-3 ReAct agent (and is\ncomparable to the GPT-4 ReAct agent). It contains a detailed analysis of all\nrisk factors in bullet points, and offers a comparison across time for all\nbullet points.\n\n**Query 3**\n\n    \n    \n    response = query_engine.query(\"Analyze Uber revenue growth and risk factors over the last few quarters\")\n\nResponse (intermediate steps):\n\n    \n    \n    > Current query: Analyze Uber revenue growth and risk factors over quarters\n    > New query:  What is Uber's revenue growth and risk factors for the quarter ending March 2022?\n    > Current query: Analyze Uber revenue growth and risk factors over quarters\n    > New query:  What is Uber's revenue growth and risk factors for the quarter ending March 2022?\n    > Current query: Analyze Uber revenue growth and risk factors over quarters\n    > New query:  What is Uber's revenue growth and risk factors for the quarter ending June 2022?\n    > Current query: Analyze Uber revenue growth and risk factors over quarters\n    > New query:  What is Uber's revenue growth and risk factors for the quarter ending June 2022?\n    > Current query: Analyze Uber revenue growth and risk factors over quarters\n    > New query:  What were Uber's total revenues for the quarter ending September 2022?\n    > Current query: Analyze Uber revenue growth and risk factors over quarters\n    > New query:  What were Uber's total revenues for the quarter ending September 2022?\n\nFinal Response\n\n    \n    \n    Uber's revenue growth has been impressive over the past few quarters, with the quarter ending March 2022 showing a 136% year-over-year growth and the quarter ending June 2022 showing a 105% year-over-year growth. The revenue for the quarter ending June 2022 was $8,343 million. \n    \n    The risk factors for Uber's business have been largely related to the ongoing impacts of the COVID-19 pandemic, including reduced global demand for Mobility rides, supply constraints, and potential permanent changes to end-user behavior. These risk factors have been present in both the quarter ending March 2022 and the quarter ending June 2022, though the specific risks have varied slightly.\n\n#  Concluding Thoughts\n\nReAct-based agents offer a powerful, general reasoning loop, and have the\npotential to solve complex tasks over your data. But they tend to only work\nreliably with more powerful language models such as GPT-4. ", "mimetype": "text/plain", "start_char_idx": 23354, "end_char_idx": 27182, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "332b15be-0f94-4319-8f80-303365cafe05": {"__data__": {"id_": "332b15be-0f94-4319-8f80-303365cafe05", "embedding": null, "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "50ab3a10-e38e-4b1c-b2dc-eb1cd03df774", "node_type": "4", "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "hash": "089f9620e42064ae0cb2f5aea08a73dd5248413650d1318182ba309da86db390", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af188508-8db4-452a-ba79-32646a92725a", "node_type": "1", "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "hash": "89181c2875f3ea572d5bfb7555d2023e46853ea16759e8cd2be54a46360ea53b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8388ee03-c887-4edf-aaaa-6ae89ec17414", "node_type": "1", "metadata": {}, "hash": "2ec03561112691f1a86cf5f9c49b16c89e2327505b0b3a6554a335760b5f2d39", "class_name": "RelatedNodeInfo"}}, "text": "Less sophisticated\nmodels (e.g. GPT-3) will make more unpredictable and erroneous decisions,\nleading to subpar query performance over your data sources.\n\nAgents implemented with \u201cdumber\u201d models need more interaction constraints in\norder to make more reliable, less erroneous decisions. We find that if we\nexplicitly constrain the agent interface and push the complexity down to the\nTool layer, we can still create agents that offer good performance over your\ndata.\n\nOf course, this is just an initial analysis and there\u2019s a few\ncaveats/limitations:\n\n  * You may be able to \u201cprompt hack\u201d the default ReAct loop to get more consistent results, and we did not try that. \n  * We only tested this over a set of three financial documents. There\u2019s a lot more work that needs to be done if we want to test this out on thousands of docs. \n  * We only compared GPT-3 and GPT-4, there\u2019s so many more models to compare/benchmark, e.g ChatGPT, any open-source model, Anthropic Claude, etc. \n  * We did not test out other agent interaction patterns besides ReAct: \u201cplan and solve\u201d agents (though we do have similar formulations in LlamaIndex), AutoGPT-like task management, and more. \n\nWhether you\u2019ve run into similar findings or you disagree with our analysis,\nlet us know! ", "mimetype": "text/plain", "start_char_idx": 27182, "end_char_idx": 28443, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8388ee03-c887-4edf-aaaa-6ae89ec17414": {"__data__": {"id_": "8388ee03-c887-4edf-aaaa-6ae89ec17414", "embedding": null, "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "50ab3a10-e38e-4b1c-b2dc-eb1cd03df774", "node_type": "4", "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "hash": "089f9620e42064ae0cb2f5aea08a73dd5248413650d1318182ba309da86db390", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "332b15be-0f94-4319-8f80-303365cafe05", "node_type": "1", "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}, "hash": "c9a526fc25eb8d7b07d7c7d7a26cd9cc891cbbf4e354b06beb6488505750605c", "class_name": "RelatedNodeInfo"}}, "text": "We\u2019d love to facilitate this discussion on our [ Discord\n](https://discord.gg/dGcwcsnxhU) .\n\n##  Notebook Walkthrough\n\nYou can find the full notebook walkthrough here: [\nhttps://colab.research.google.com/drive/1uP38k4nr8OPmXbY4dLoKKQW0F29WtNuY?usp=sharing\n](https://colab.research.google.com/drive/1uP38k4nr8OPmXbY4dLoKKQW0F29WtNuY?usp=sharing)\n\n", "mimetype": "text/plain", "start_char_idx": 28443, "end_char_idx": 28789, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "000671aa-f9cd-40c7-b6a1-66cf6c364ab6": {"__data__": {"id_": "000671aa-f9cd-40c7-b6a1-66cf6c364ab6", "embedding": null, "metadata": {"filename": "build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.md", "extension": ".md", "title": "Build a ChatGPT with your Private Data using LlamaIndex and MongoDB", "date": "May 18, 2023", "url": "https://www.llamaindex.ai/blog/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "455c6fb6-4eb7-4b07-9b11-9699928b66da", "node_type": "4", "metadata": {"filename": "build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.md", "extension": ".md", "title": "Build a ChatGPT with your Private Data using LlamaIndex and MongoDB", "date": "May 18, 2023", "url": "https://www.llamaindex.ai/blog/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c"}, "hash": "dae946a02014c2b660127995e6439f2a2cabfc35f21259bab080ce84eb4daa93", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "649498a6-2d08-49e9-a466-b4c5ff75a7ad", "node_type": "1", "metadata": {}, "hash": "697df2e7f8670f133b95c141e8824ed7975f3044cc13e9d44d6193eb22a62fb9", "class_name": "RelatedNodeInfo"}}, "text": "**Co-authors:**\n\n  * Prakul Agarwal \u2014 Senior Product Manager, Machine Learning at MongoDB \n  * Jerry Liu \u2014 co-founder at LlamaIndex \n\n**Update (6/22/2023):** The preferred way to use LlamaIndex + MongoDB is now\nwith our MongoDBAtlasVectorSearch class. Take a look at our guide here: [\nhttps://gpt-\nindex.readthedocs.io/en/latest/examples/vector_stores/MongoDBAtlasVectorSearch.html\n](https://gpt-\nindex.readthedocs.io/en/latest/examples/vector_stores/MongoDBAtlasVectorSearch.html)\n\n#  **Summary**\n\nLarge Language Models (LLMs) like ChatGPT have revolutionized the way users\ncan get answers to their questions. However, the \u201cknowledge\u201d of LLMs is\nrestricted by what they were trained on, which for ChatGPT means publicly\navailable information on the internet till September 2021. How can LLMs answer\nquestions using private knowledge sources like your company\u2019s data and unlock\nits true transformative power?\n\nThis blog will discuss how LlamaIndex and MongoDB can enable you to achieve\nthis outcome quickly. The [ attached notebook\n](https://colab.research.google.com/drive/1SNIeLW38Nvx6MtL3-_LPS2XTIzqD4gS6?usp=sharing)\nprovides a code walkthrough on how to query any PDF document using English\nqueries.\n\n#  **Background**\n\nTraditionally, AI has been used to analyze data, identify patterns and make\npredictions based on existing data. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1337, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "649498a6-2d08-49e9-a466-b4c5ff75a7ad": {"__data__": {"id_": "649498a6-2d08-49e9-a466-b4c5ff75a7ad", "embedding": null, "metadata": {"filename": "build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.md", "extension": ".md", "title": "Build a ChatGPT with your Private Data using LlamaIndex and MongoDB", "date": "May 18, 2023", "url": "https://www.llamaindex.ai/blog/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "455c6fb6-4eb7-4b07-9b11-9699928b66da", "node_type": "4", "metadata": {"filename": "build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.md", "extension": ".md", "title": "Build a ChatGPT with your Private Data using LlamaIndex and MongoDB", "date": "May 18, 2023", "url": "https://www.llamaindex.ai/blog/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c"}, "hash": "dae946a02014c2b660127995e6439f2a2cabfc35f21259bab080ce84eb4daa93", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "000671aa-f9cd-40c7-b6a1-66cf6c364ab6", "node_type": "1", "metadata": {"filename": "build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.md", "extension": ".md", "title": "Build a ChatGPT with your Private Data using LlamaIndex and MongoDB", "date": "May 18, 2023", "url": "https://www.llamaindex.ai/blog/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c"}, "hash": "728b96f58af410e92ca0c7732fe909c1b2bf837855a613787682ffd60a16efb6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "649c1c44-1348-47c7-9afa-23146c4cb920", "node_type": "1", "metadata": {}, "hash": "b41c1d8d9d9a6427ffe1475f9bc8cfadeb27d2247d1ee220ead961997078574b", "class_name": "RelatedNodeInfo"}}, "text": "The recent advancements have led to AI\nbecoming better at generating new things (rather than just analyzing existing\nthings). This is referred to as Generative AI. Generative AI is powered mainly\nby machine learning models called Large Language Models (LLM). LLMs are pre-\ntrained on large quantities of publicly available text. There are various\nproprietary LLMs from companies like OpenAI, Cohere, AI21, as well as a lot of\nemerging open-source LLMs like Llama, Dolly, etc.\n\nThere are 2 main scenarios where the knowledge of LLMs falls short:\n\n  * Private data such as your company\u2019s internal knowledge base spread across PDFs, Google Docs, Wiki pages, and applications like Salesforce and Slack \n  * Newer data than when the LLMs were last trained. Example question: Who is the most recent UK prime minister? \n\nThere are 2 main paradigms currently for extending the amazing reasoning and\nknowledge generation capabilities of LLMs: Model finetuning and in-context\nlearning.\n\nModel Finetuning can be more complex and expensive to operationalize. ", "mimetype": "text/plain", "start_char_idx": 1337, "end_char_idx": 2384, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "649c1c44-1348-47c7-9afa-23146c4cb920": {"__data__": {"id_": "649c1c44-1348-47c7-9afa-23146c4cb920", "embedding": null, "metadata": {"filename": "build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.md", "extension": ".md", "title": "Build a ChatGPT with your Private Data using LlamaIndex and MongoDB", "date": "May 18, 2023", "url": "https://www.llamaindex.ai/blog/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "455c6fb6-4eb7-4b07-9b11-9699928b66da", "node_type": "4", "metadata": {"filename": "build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.md", "extension": ".md", "title": "Build a ChatGPT with your Private Data using LlamaIndex and MongoDB", "date": "May 18, 2023", "url": "https://www.llamaindex.ai/blog/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c"}, "hash": "dae946a02014c2b660127995e6439f2a2cabfc35f21259bab080ce84eb4daa93", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "649498a6-2d08-49e9-a466-b4c5ff75a7ad", "node_type": "1", "metadata": {"filename": "build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.md", "extension": ".md", "title": "Build a ChatGPT with your Private Data using LlamaIndex and MongoDB", "date": "May 18, 2023", "url": "https://www.llamaindex.ai/blog/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c"}, "hash": "bf24a850fbaf57ad612d7082e0a603e94d406f74b956324f15084c77701ba814", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc6f5378-6152-4b83-922c-e8e504e0360d", "node_type": "1", "metadata": {}, "hash": "202b21e890305eca062ce11fd8bb5418373730cfe11df4fb3f50f54e5a28f5ce", "class_name": "RelatedNodeInfo"}}, "text": "There\nare also some open questions like how to delete information from a fine-tuned\nmodel to ensure you comply with local laws (ex. GDPR in Europe), and for\nchanging data you need to fine-tune again constantly.\n\nIn-context learning requires inserting the new data as part of the input\nprompts to the LLM. To perform this data augmentation in a secure, high\nperformance and cost-effective manner is where tools like LlamaIndex and\nMongoDB Developer Data Platform can help.\n\n#  **Introduction to LlamaIndex**\n\nLlamaIndex provides a simple, flexible interface to connect LLMs with external\ndata.\n\n  * Offers data connectors to various data sources and data formats (APIs, PDFs, docs, etc). \n  * Provides indices over the unstructured and structured data for use with LLMs. \n  * Structures external information so that it can be used with the prompt window limitations of any LLM. \n  * Exposes a query interface which takes in an input prompt and returns a knowledge-augmented output. \n\n#  **MongoDB as the Datastore**\n\nIt is effortless to store the ingested documents (i.e. Node objects), index\nmetadata, etc to MongoDB using the inbuilt abstractions in LlamaIndex. There\nis an option to store the \u201cdocuments\u201d as an actual collection in MongoDB using\n` MongoDocumentStore ` . There is an option to persist the \u201cIndexes\u201d using the\n` MongoIndexStore ` .\n\nStoring LlamaIndex\u2019s documents and indexes in a database becomes necessary in\na couple of scenarios:\n\n  1. Use cases with large datasets may require more than in-memory storage. \n  ", "mimetype": "text/plain", "start_char_idx": 2384, "end_char_idx": 3915, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc6f5378-6152-4b83-922c-e8e504e0360d": {"__data__": {"id_": "dc6f5378-6152-4b83-922c-e8e504e0360d", "embedding": null, "metadata": {"filename": "build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.md", "extension": ".md", "title": "Build a ChatGPT with your Private Data using LlamaIndex and MongoDB", "date": "May 18, 2023", "url": "https://www.llamaindex.ai/blog/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "455c6fb6-4eb7-4b07-9b11-9699928b66da", "node_type": "4", "metadata": {"filename": "build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.md", "extension": ".md", "title": "Build a ChatGPT with your Private Data using LlamaIndex and MongoDB", "date": "May 18, 2023", "url": "https://www.llamaindex.ai/blog/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c"}, "hash": "dae946a02014c2b660127995e6439f2a2cabfc35f21259bab080ce84eb4daa93", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "649c1c44-1348-47c7-9afa-23146c4cb920", "node_type": "1", "metadata": {"filename": "build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.md", "extension": ".md", "title": "Build a ChatGPT with your Private Data using LlamaIndex and MongoDB", "date": "May 18, 2023", "url": "https://www.llamaindex.ai/blog/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c"}, "hash": "90c4232c38469c4acbd72d2c61a26e0f8719561bf1ec45392146f35e469bd7d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fae7e716-817d-4257-9c2d-52554397f174", "node_type": "1", "metadata": {}, "hash": "e6a3ed50d658d9da4cad760f356bdc9f611e8b1a2778209ac6a1d888ef893fd1", "class_name": "RelatedNodeInfo"}}, "text": "2. Ingesting and processing data from various sources (for example, PDFs, Google Docs, Slack). \n  3. The requirement to continuously maintain updates from the underlying data sources. \n\nBeing able to persist this data enables processing the data once and then\nbeing able to query it for various downstream applications.\n\n#  **MongoDB Atlas**\n\nMongoDB offers a free forever Atlas cluster in the public cloud service of\nyour choice. This can be accomplished very quickly by following this [\ntutorial ](https://www.mongodb.com/developer/products/atlas/free-atlas-\ncluster/) . Or you can get started directly [ here\n](https://www.mongodb.com/cloud/atlas/register) .\n\n#  **Use of LLMs**\n\nLlamaIndex uses LangChain\u2019s (another popular framework for building Generative\nAI applications) LLM modules and allows for customizing the underlying LLM to\nbe used (default being OpenAI\u2019s text-davinci-003 model). The chosen LLM is\nalways used by LlamaIndex to construct the final answer and is sometimes used\nduring index creation as well.\n\n#  **The workflow**\n\n  1. Connect private knowledge sources using LlamaIndex connectors (offered through [ LlamaHub ](https://llamahub.ai/) ). \n  2. ", "mimetype": "text/plain", "start_char_idx": 3915, "end_char_idx": 5089, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fae7e716-817d-4257-9c2d-52554397f174": {"__data__": {"id_": "fae7e716-817d-4257-9c2d-52554397f174", "embedding": null, "metadata": {"filename": "build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.md", "extension": ".md", "title": "Build a ChatGPT with your Private Data using LlamaIndex and MongoDB", "date": "May 18, 2023", "url": "https://www.llamaindex.ai/blog/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "455c6fb6-4eb7-4b07-9b11-9699928b66da", "node_type": "4", "metadata": {"filename": "build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.md", "extension": ".md", "title": "Build a ChatGPT with your Private Data using LlamaIndex and MongoDB", "date": "May 18, 2023", "url": "https://www.llamaindex.ai/blog/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c"}, "hash": "dae946a02014c2b660127995e6439f2a2cabfc35f21259bab080ce84eb4daa93", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc6f5378-6152-4b83-922c-e8e504e0360d", "node_type": "1", "metadata": {"filename": "build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.md", "extension": ".md", "title": "Build a ChatGPT with your Private Data using LlamaIndex and MongoDB", "date": "May 18, 2023", "url": "https://www.llamaindex.ai/blog/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c"}, "hash": "ec43a4aca5b824f4b361fa2081c8f6cb774ebe77fb72bfd52bfce05c6564afcc", "class_name": "RelatedNodeInfo"}}, "text": "Load in the Documents. A Document represents a lightweight container around the data source. \n  3. Parse the Documents objects into Node objects. Nodes represent \u201cchunks\u201d of source Documents (ex. a text chunk). These node objects can be persisted to a MongoDB collection or kept in memory. \n  4. Construct Index from Nodes. There are various kinds of indexes in LlamaIndex like \u201cList Index\u201d (this stores nodes as Sequential chain), \u201cVector Store Index\u201d (this stores each node and a corresponding embedding in a vector store). Depending on the type of Index, these indexes can be persisted into a MongoDB collection or a Vector Database. \n  5. Finally query the index. This is where the the query is parsed, relevant Nodes retrieved through the use of indexes, and provided as an input to a \u201cLarge Language Model\u201d (LLM). Different types of queries can use different indexes. \n\nLlamaIndex + MongoDB Workflow Diagram\n\n#  **Getting questions answered over your private data**\n\nWe want to query the \u201cGPT-4 Technical Report\u201d published by OpenAI in March\n2023. This was a [ PDF document ](https://arxiv.org/pdf/2303.08774.pdf) with\n100 pages. This is a recent publication, so was not included as part of the\noriginal ChatGPT training data.\n\nHere\u2019s the summary of the various queries we can ask the PDF.\n\n> **_Query_ **\n>\n> _\u201cHow does GPT4 do on the bar exam?\u201d_\n>\n> **_Response_ **\n>\n> _\u201cGPT-4 performs well on the Uniform Bar Exam, with a score in the top 10%\n> of test takers (Table 1, Figure 4).\u201d_\n>\n> **_LLM token usage_ **\n>\n> _Total embedding token usage: 18 tokens Total LLM token usage: 1889 tokens_\n>\n> **_Query_ **\n>\n> _\u201cHow much better is GPT-4 in reducing hallucinations over GPT-3.5?\u201d_\n>\n> **_Response_ **\n>\n> _\u201cGPT-4 improves on the latest GPT-3.5 model by 19 percentage points, with\n> significant gains across all topics.\u201d_\n>\n> **_Query_ **\n>\n> _\u201cWhat issues were observed after fine-tuning GPT-4 with RHLF??\u201d_\n>\n> **_Response_ **\n>\n> _\u201cAfter fine-tuning GPT-4 with RHLF, issues observed included the model\n> becoming overly cautious in certain ways, such as refusing innocuous\n> requests and excessively hedging or \u201coverrefusing\u201d. Additionally, the model\n> was still quite brittle and sometimes exhibited undesired behaviors based on\n> prompts where instructions to labelers were underspecified.\u201d_\n>\n> **_Query_ **\n>\n> _\u201cWhat is RBRM?\u201d_\n>\n> **_Response_ **\n>\n> _\u201cRBRM stands for Rule-Based Reward Model. It is a technique used to provide\n> an additional reward signal to the GPT-4 policy model during PPO fine-tuning\n> on a subset of training prompts. The RBRM takes three things as input: the\n> prompt (optional), the output from the policy model, and a human-written\n> rubric (e.g., a set of rules in multiple-choice style) for how this output\n> should be evaluated. The RBRM then classifies the output based on the\n> rubric.\u201d_\n\nThe screenshots below show how the PDF document is converted into \u201cLlamaIndex\nnodes\u201d and \u201cLlamaIndex indices\u201d and persisted into MongoDB.\n\n#  **Relevant Resources**\n\nFurther details can be found here. Also check out the reference notebook\nbelow!\n\nReading data from MongoDB: [ link ](https://gpt-\nindex.readthedocs.io/en/latest/examples/data_connectors/MongoDemo.html)\n\nVarious Indexes in LlamaIndex: [ link ](https://gpt-\nindex.readthedocs.io/en/latest/guides/primer/index_guide.html)\n\n##  **Reference Notebook**\n\n[\nhttps://colab.research.google.com/drive/1SNIeLW38Nvx6MtL3-_LPS2XTIzqD4gS6?usp=sharing\n](https://colab.research.google.com/drive/1SNIeLW38Nvx6MtL3-_LPS2XTIzqD4gS6?usp=sharing)\n\n", "mimetype": "text/plain", "start_char_idx": 5089, "end_char_idx": 8620, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c947eab8-e9ec-44ff-a0aa-e1c58bcfe9d2": {"__data__": {"id_": "c947eab8-e9ec-44ff-a0aa-e1c58bcfe9d2", "embedding": null, "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11b375e8-e8d6-4d21-a47e-502ae9531bbd", "node_type": "4", "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "af8d98ece7e1a1cac7bf823a56c9995022f6eaac0ea91d442c4c37000c95b26a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3769b6d9-3967-4afb-b744-22ff538b13e3", "node_type": "1", "metadata": {}, "hash": "a35d84a6c7b524e9c324746344edb4e8112d850abd184cc3b4a426f21aa149d8", "class_name": "RelatedNodeInfo"}}, "text": "#  **Summary**\n\nThis blog post outlines some of the core abstractions we have created in [\nLlamaIndex ](https://github.com/jerryjliu/llama_index) around LLM-powered\nretrieval and reranking, which helps to create enhancements to document\nretrieval beyond naive top-k embedding-based lookup.\n\nLLM-powered retrieval can return more relevant documents than embedding-based\nretrieval, with the tradeoff being much higher latency and cost. We show how\nusing embedding-based retrieval as a first-stage pass, and second-stage\nretrieval as a reranking step can help provide a happy medium. We provide\nresults over the Great Gatsby and the Lyft SEC 10-k.\n\nTwo-stage retrieval pipeline: 1) Top-k embedding retrieval, then 2) LLM-based\nreranking\n\n#  **Introduction and Background**\n\nThere has been a wave of \u201cBuild a chatbot over your data\u201d applications in the\npast few months, made possible with frameworks like [ LlamaIndex\n](https://github.com/jerryjliu/llama_index) and [ LangChain\n](https://github.com/hwchase17/langchain) . A lot of these applications use a\nstandard stack for retrieval augmented generation (RAG):\n\n  * Use a vector store to store unstructured documents (knowledge corpus) \n  * Given a query, use a **retrieval model** to retrieve relevant documents from the corpus, and a **synthesis model** to generate a response. \n  * The **retrieval model** fetches the top-k documents by embedding similarity to the query. \n\nIn this stack, the retrieval model is not a novel idea; the concept of top-k\nembedding-based semantic search has been around for at least a decade, and\ndoesn\u2019t involve the LLM at all.\n\nThere are a lot of benefits to embedding-based retrieval:\n\n  * It\u2019s very fast to compute dot products. Doesn\u2019t require any model calls during query-time. \n  * Even if not perfect, embeddings can encode the semantics of the document and query reasonably well. There\u2019s a class of queries where embedding-based retrieval returns very relevant results. \n\nYet for a variety of reasons, embedding-based retrieval can be imprecise and\nreturn irrelevant context to the query, which in turn degrades the quality of\nthe overall RAG system, regardless of the quality of the LLM.\n\nThis is also not a new problem: one approach to resolve this in existing IR\nand recommendation systems is to create a **two stage process** . The first\nstage uses embedding-based retrieval with a high top-k value to maximize\nrecall while accepting a lower precision. Then the second stage uses a\nslightly more computationally expensive process that is higher precision and\nlower recall (for instance with BM25) to \u201crerank\u201d the existing retrieved\ncandidates.\n\nCovering the downsides of embedding-based retrieval is worth an entire series\nof blog posts. This blog post is an initial exploration of an alternative\nretrieval method and how it can (potentially) augment embedding-based\nretrieval methods.\n\n#  **LLM Retrieval and Reranking**\n\nOver the past week, we\u2019ve developed a variety of initial abstractions around\nthe concept of \u201cLLM-based\u201d retrieval and reranking. At a high-level, this\napproach uses the LLM to decide which document(s) / text chunk(s) are relevant\nto the given query. The input prompt would consist of a set of candidate\ndocuments, and the LLM is tasked with selecting the relevant set of documents\nas well as scoring their relevance with an internal metric.\n\nSimple diagram of how LLM-based retrieval works\n\nAn example prompt would look like the following:\n\n    \n    \n    A list of documents is shown below. Each document has a number next to it along with a summary of the document. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3583, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3769b6d9-3967-4afb-b744-22ff538b13e3": {"__data__": {"id_": "3769b6d9-3967-4afb-b744-22ff538b13e3", "embedding": null, "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11b375e8-e8d6-4d21-a47e-502ae9531bbd", "node_type": "4", "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "af8d98ece7e1a1cac7bf823a56c9995022f6eaac0ea91d442c4c37000c95b26a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c947eab8-e9ec-44ff-a0aa-e1c58bcfe9d2", "node_type": "1", "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "46bb43b5ede01431be09a04c16b9026894b15401303ded31c2e140e1d4dd5380", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16501f8a-21f5-483b-8887-b09838a60245", "node_type": "1", "metadata": {}, "hash": "c1bb06d472d699e64a2dbbd0aa5fbe62e7962be129ea140bac16e77857d0c293", "class_name": "RelatedNodeInfo"}}, "text": "A question is also provided.\n      Respond with the numbers of the documents you should consult to answer the question, in order of relevance, as well\n      as the relevance score. The relevance score is a number from 1\u201310 based on how relevant you think the document is to the question.\n      Do not include any documents that are not relevant to the question.\n      Example format:\n      Document 1:\n      <summary of document 1>\n      Document 2:\n      <summary of document 2>\n      \u2026\n      Document 10:\n      <summary of document 10>\n      Question: <question>\n      Answer:\n      Doc: 9, Relevance: 7\n      Doc: 3, Relevance: 4\n      Doc: 7, Relevance: 3\n      Let's try this now:\n      {context_str}\n      Question: {query_str}\n      Answer:\n\nThe prompt format implies that the text for each document should be relatively\nconcise. There are two ways of feeding in the text to the prompt corresponding\nto each document:\n\n  * You can directly feed in the raw text corresponding to the document. This works well if the document corresponds to a bite-sized text chunk. \n  * You can feed in a condensed summary for each document. This would be preferred if the document itself corresponds to a long-piece of text. We do this under the hood with our new [ document summary index ](https://medium.com/llamaindex-blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec) , but you can also choose to do it yourself. \n\nGiven a collection of documents, we can then create document \u201cbatches\u201d and\nsend each batch into the LLM input prompt. The output of each batch would be\nthe set of relevant documents + relevance scores within that batch. The final\nretrieval response would aggregate relevant documents from all batches.\n\nYou can use our abstractions in two forms: as a standalone retriever module (\n` ListIndexLLMRetriever ` ) or a reranker module ( ` LLMRerank ` ). The\nremainder of this blog primarily focuses on the reranker module given the\nspeed/cost.\n\n##  **LLM Retriever** ` (ListIndexLLMRetriever) `\n\nThis module is defined over a list index, which simply stores a set of nodes\nas a flat list. You can build the list index over a set of documents and then\nuse the LLM retriever to retrieve the relevant documents from the index.\n\n    \n    \n    from llama_index import GPTListIndex\n    from llama_index.indices.list.retrievers import ListIndexLLMRetriever\n    index = GPTListIndex.from_documents(documents, service_context=service_context)\n    # high - level API\n    query_str = \"What did the author do during his time in college?\"\n    retriever = index.as_retriever(retriever_mode=\"llm\")\n    nodes = retriever.retrieve(query_str)\n    # lower-level API\n    retriever = ListIndexLLMRetriever()\n    response_synthesizer = ResponseSynthesizer.from_args()\n    query_engine = RetrieverQueryEngine(retriever=retriever, response_synthesizer=response_synthesizer)\n    response = query_engine.query(query_str)\n\n**Use Case:** This could potentially be used in place of our vector store\nindex. You use the LLM instead of embedding-based lookup to select the nodes.\n\n##  **LLM Reranker (LLMRerank)**\n\nThis module is defined as part of our ` NodePostprocessor ` abstraction, which\nis defined for second-stage processing after an initial retrieval pass.\n\nThe postprocessor can be used on its own or as part of a `\nRetrieverQueryEngine ` call. In the below example we show how to use the\npostprocessor as an independent module after an initial retriever call from a\nvector index.\n\n    \n    \n    from llama_index.indices.query.schema import QueryBundle\n    query_bundle = QueryBundle(query_str)\n    # configure retriever\n    retriever = VectorIndexRetriever(\n    index=index,\n    similarity_top_k=vector_top_k,\n    )\n    retrieved_nodes = retriever.retrieve(query_bundle)\n    # configure reranker\n    reranker = LLMRerank(choice_batch_size=5, top_n=reranker_top_n, service_context=service_context)\n    retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n\n##  **Limitations/Caveats**\n\nThere are certain limitations and caveats to LLM-based retrieval, especially\nwith this initial version.\n\n  * LLM-based retrieval is orders of magnitude slower than embedding-based retrieval. ", "mimetype": "text/plain", "start_char_idx": 3583, "end_char_idx": 7788, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16501f8a-21f5-483b-8887-b09838a60245": {"__data__": {"id_": "16501f8a-21f5-483b-8887-b09838a60245", "embedding": null, "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11b375e8-e8d6-4d21-a47e-502ae9531bbd", "node_type": "4", "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "af8d98ece7e1a1cac7bf823a56c9995022f6eaac0ea91d442c4c37000c95b26a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3769b6d9-3967-4afb-b744-22ff538b13e3", "node_type": "1", "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "12bb1319da730f59a03dd2f101f94cf0ac9158371700844dcd1f86ab1234cc61", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c99a89c-5f4f-4127-b451-be5243c479eb", "node_type": "1", "metadata": {}, "hash": "25ca9ac408334db8ef2f08ad98647d7b277134f52fb994d636a5cc1d66021eb3", "class_name": "RelatedNodeInfo"}}, "text": "Embedding search over thousands or even millions of embeddings can take less than a second. Each LLM prompt of 4000 tokens to OpenAI can take minutes to complete. \n  * Using third-party LLM API\u2019s costs money. \n  * The current method of batching documents may not be optimal, because it relies on an assumption that document batches can be scored independently of each other. This lacks a global view of the ranking for all documents. \n\nUsing the LLM to retrieve and rank every node in the document corpus can be\nprohibitively expensive. This is why using the LLM as a second-stage reranking\nstep, after a first-stage embedding pass, can be helpful.\n\n#  **Initial Experimental Results**\n\nLet\u2019s take a look at how well LLM reranking works!\n\nWe show some comparisons between naive top-k embedding-based retrieval as well\nas the two-stage retrieval pipeline with a first-stage embedding-retrieval\nfilter and second-stage LLM reranking. We also showcase some results of pure\nLLM-based retrieval (though we don\u2019t showcase as many results given that it\ntends to run a lot slower than either of the first two approaches).\n\nWe analyze results over two very different sources of data: the Great Gatsby\nand the 2021 Lyft SEC 10-k. We only analyze results over the \u201cretrieval\u201d\nportion and not synthesis to better isolate the performance of different\nretrieval methods.\n\nThe results are presented in a qualitative fashion. A next step would\ndefinitely be more comprehensive evaluation over an entire dataset!\n\n##  **The Great Gatsby**\n\nIn our first example, we load in the Great Gatsby as a ` Document ` object,\nand build a vector index over it (with chunk size set to 512).\n\n    \n    \n    # LLM Predictor (gpt-3.5-turbo) + service context\n    llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size_limit=512)\n    # load documents\n    documents = SimpleDirectoryReader('../../../examples/gatsby/data').load_data()\n    index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n\nWe then define a ` get_retrieved_nodes ` function \u2014 this function can either\ndo just embedding-based retrieval over the index, or embedding-based retrieval\n+ reranking.\n\n    \n    \n    def get_retrieved_nodes(\n        query_str, vector_top_k=10, reranker_top_n=3, with_reranker=False\n    ):\n      query_bundle = QueryBundle(query_str)\n      # configure retriever\n      retriever = VectorIndexRetriever(\n        index=index,\n        similarity_top_k=vector_top_k,\n      )\n      retrieved_nodes = retriever.retrieve(query_bundle)\n      if with_reranker:\n        # configure reranker\n        reranker = LLMRerank(choice_batch_size=5, top_n=reranker_top_n, service_context=service_context)\n        retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n      return retrieved_nodes\n\nWe then ask some questions. With embedding-based retrieval we set k=3. ", "mimetype": "text/plain", "start_char_idx": 7788, "end_char_idx": 10775, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c99a89c-5f4f-4127-b451-be5243c479eb": {"__data__": {"id_": "1c99a89c-5f4f-4127-b451-be5243c479eb", "embedding": null, "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11b375e8-e8d6-4d21-a47e-502ae9531bbd", "node_type": "4", "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "af8d98ece7e1a1cac7bf823a56c9995022f6eaac0ea91d442c4c37000c95b26a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16501f8a-21f5-483b-8887-b09838a60245", "node_type": "1", "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "c7c01942b3c31f103a2289c4b33bfc284321b7ad989e443d4423cacd47701a0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c009d4ea-ab0b-4343-8438-2a4fc7611a6b", "node_type": "1", "metadata": {}, "hash": "7db8b879e743b18407c34e90f20f6a7a23a2c4c80afc2787378074087a8d5857", "class_name": "RelatedNodeInfo"}}, "text": "With\ntwo-stage retrieval we set k=10 for embedding retrieval and n=3 for LLM-based\nreranking.\n\n**Question: \u201dWho was driving the car that hit Myrtle?\u201d**\n\n(For those of you who are not familiar with the Great Gatsby, the narrator\nfinds out later on from Gatsby that Daisy was actually the one driving the\ncar, but Gatsby takes the blame for her).\n\nThe top retrieved contexts are shown in the images below. ", "mimetype": "text/plain", "start_char_idx": 10775, "end_char_idx": 11179, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c009d4ea-ab0b-4343-8438-2a4fc7611a6b": {"__data__": {"id_": "c009d4ea-ab0b-4343-8438-2a4fc7611a6b", "embedding": null, "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11b375e8-e8d6-4d21-a47e-502ae9531bbd", "node_type": "4", "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "af8d98ece7e1a1cac7bf823a56c9995022f6eaac0ea91d442c4c37000c95b26a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c99a89c-5f4f-4127-b451-be5243c479eb", "node_type": "1", "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "9435779303ccb31da99b679a5881bf797d34f01ea059c37fdab98b1f46e2d52b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9206c8eb-fdaf-4d75-9c5d-c6a5db0ffed9", "node_type": "1", "metadata": {}, "hash": "58a8b6c6aae3462a089f2079f8b12ecaf67f8e3e2e9962408edd96ed8b69cf9d", "class_name": "RelatedNodeInfo"}}, "text": "We see that in\nembedding-based retrieval, the top two texts contain semantics of the car\ncrash but give no details as to who was actually responsible. Only the third\ntext contains the proper answer.\n\nRetrieved context using top-k embedding lookup (baseline)\n\nIn contrast, the two-stage approach returns just one relevant context, and it\ncontains the correct answer.\n\nRetrieved context using two-stage pipeline (embedding lookup then rerank)\n\n##  **2021 Lyft SEC 10-K**\n\nWe want to ask some questions over the 2021 Lyft SEC 10-K, specifically about\nthe COVID-19 impacts and responses. The Lyft SEC 10-K is 238 pages long, and a\nctrl-f for \u201cCOVID-19\u201d returns 127 matches.\n\nWe use a similar setup as the Gatsby example above. The main differences are\nthat we set the chunk size to 128 instead of 512, we set k=5 for the embedding\nretrieval baseline, and an embedding k=40 and reranker n=5 for the two-stage\napproach.\n\nWe then ask the following questions and analyze the results.\n\n**Question: \u201cWhat initiatives are the company focusing on independently of\nCOVID-19?\u201d**\n\nResults for the baseline are shown in the image above. We see that results\ncorresponding to indices 0, 1, 3, 4, are about measures directly in response\nto Covid-19, even though the question was specifically about company\ninitiatives that were independent of the COVID-19 pandemic.\n\nRetrieved context using top-k embedding lookup (baseline)\n\nWe get more relevant results in approach 2, by widening the top-k to 40 and\nthen using an LLM to filter for the top-5 contexts. The independent company\ninitiatives include \u201cexpansion of Light Vehicles\u201d (1), \u201cincremental\ninvestments in brand/marketing\u201d (2), international expansion (3), and\naccounting for misc. risks such as natural disasters and operational risks in\nterms of financial performance (4).\n\n", "mimetype": "text/plain", "start_char_idx": 11179, "end_char_idx": 12991, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9206c8eb-fdaf-4d75-9c5d-c6a5db0ffed9": {"__data__": {"id_": "9206c8eb-fdaf-4d75-9c5d-c6a5db0ffed9", "embedding": null, "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11b375e8-e8d6-4d21-a47e-502ae9531bbd", "node_type": "4", "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "af8d98ece7e1a1cac7bf823a56c9995022f6eaac0ea91d442c4c37000c95b26a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c009d4ea-ab0b-4343-8438-2a4fc7611a6b", "node_type": "1", "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}, "hash": "06a26352144a4dd4429cc7d34def019fa92831445d0897556eb3effc737a0a82", "class_name": "RelatedNodeInfo"}}, "text": "Retrieved context using two-stage pipeline (embedding lookup then rerank)\n\n#  **Conclusion**\n\nThat\u2019s it for now! We\u2019ve added some initial functionality to help support LLM-\naugmented retrieval pipelines, but of course there\u2019s a ton of future steps\nthat we couldn\u2019t quite get to. Some questions we\u2019d love to explore:\n\n  * How our LLM reranking implementation compares to other reranking methods (e.g. BM25, Cohere Rerank, etc.) \n  * What the optimal values of embedding top-k and reranking top-n are for the two stage pipeline, accounting for latency, cost, and performance. \n  * Exploring different prompts and text summarization methods to help determine document relevance \n  * Exploring if there\u2019s a class of applications where LLM-based retrieval on its own would suffice, without embedding-based filtering (maybe over smaller document collections?) \n\n##  Resources\n\nYou can play around with the notebooks yourself!\n\n[ Great Gatsby Notebook\n](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/node_postprocessor/LLMReranker-\nGatsby.ipynb)\n\n[ 2021 Lyft 10-K Notebook\n](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/node_postprocessor/LLMReranker-\nLyft-10k.ipynb)\n\n", "mimetype": "text/plain", "start_char_idx": 12991, "end_char_idx": 14192, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7645b065-77c6-427e-af0e-453e135b751d": {"__data__": {"id_": "7645b065-77c6-427e-af0e-453e135b751d", "embedding": null, "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f9f418a7-e124-424c-b5b7-b35cc94d96f4", "node_type": "4", "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "hash": "d23518db6ea14b22a2bcfeed1a8db3a97e45ad3edbbb75c07d3f4f421ba4c37a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d332cce-a3fc-4ece-bde6-11a55fdd5a67", "node_type": "1", "metadata": {}, "hash": "5f54e089e64f8827420f39533f0bdcf0ba3748da297d5e5902ce3d1ab6733809", "class_name": "RelatedNodeInfo"}}, "text": "Anthropic\u2019s [ 100K Context Window\n](https://www.anthropic.com/index/100k-context-windows) expansion, just\nreleased yesterday, has taken the AI community by storm. A 100k token limit is\napproximately 75k words (~3x GPT4\u201332k\u2019s context window, ~25x that of\nGPT-3.5/ChatGPT); this means that you can now fit 300 pages of text in a\n_single_ inference call _._\n\nOne of the core use cases highlighted in the Anthropic blog is [ analyzing an\nSEC 10-K filing ](https://vimeo.com/825669443) ; the model is capable of\ningesting the entire report, and producing answers to different questions.\n\nCoincidentally, we [ published a tutorial\n](https://medium.com/@jerryjliu98/how-unstructured-and-llamaindex-can-help-\nbring-the-power-of-llms-to-your-own-data-3657d063e30d) a few months ago\nshowing how LlamaIndex + Unstructured + GPT3 could help you perform different\nqueries over UBER SEC 10-k filings. LlamaIndex provides a comprehensive\ntoolkit to help manage external data on top of any LLM with limited context\nwindows, and we show that we can execute a diverse range of queries, from\nquestions over a single document to comparing sections across documents.\n\nHow well does Anthropic\u2019s 100k model do over UBER SEC 10-k filings? Moreover,\nhow well does it do _without_ the help of any of LlamaIndex\u2019s more advanced\ndata structures? In this blog we show the performance of Anthropic\u2019s model on\ndifferent queries, using the simplest data structure available: the list\nindex.\n\n#  High-Level Findings\n\nWhere Anthropic\u2019s 100k model does well:\n\n  * **Holistic understanding of the data (kind of, after some prompt tuning):** Anthropic\u2019s model does demonstrate an impressive capability to synthesize insights across the entire context window to answer the question at hand (assuming we set ` response_mode=\"tree_summarize\" ` , see below). It can miss details though; see below! \n  * **Latency:** This one was surprising to us. Anthropic\u2019s model is able to crunch an entire UBER 10-k filing in ~60\u201390 seconds, which seems long but is much faster than repeated API calls to GPT-3 (which when added up can take minutes). \n\nWhere Anthropic\u2019s 100k model doesn\u2019t do well:\n\n  * **Cost:** This one is obvious. Every query we ran processed hundreds of thousands of tokens. At [ $11 per million tokens for Claude-v1 ](https://console.anthropic.com/account/pricing) , this equates to $1 per query, which can quickly add up. \n  * **Reasoning over more complicated prompts:** Anthropic\u2019s model demonstrated a surprising lack of ability to understand our refine prompt for [ \u201ccreate-and-refine\u201d response synthesis ](https://gpt-index.readthedocs.io/en/latest/how_to/query/response_synthesis.html#refine) , returning incorrect/irrelevant results. We ended up switching to [ \u201ctree summarization\u201d instead ](https://gpt-index.readthedocs.io/en/latest/how_to/query/response_synthesis.html#tree-summarize) . See below for results. \n\n#  Overview of Methodology\n\nWe want to test the capabilities of Anthropic\u2019s 100K model on top of UBER 10-k\nfilings from 2019\u20132022. We also want to do this while using as little\nretrieval/synthesis constructs as possible. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3113, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d332cce-a3fc-4ece-bde6-11a55fdd5a67": {"__data__": {"id_": "5d332cce-a3fc-4ece-bde6-11a55fdd5a67", "embedding": null, "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f9f418a7-e124-424c-b5b7-b35cc94d96f4", "node_type": "4", "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "hash": "d23518db6ea14b22a2bcfeed1a8db3a97e45ad3edbbb75c07d3f4f421ba4c37a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7645b065-77c6-427e-af0e-453e135b751d", "node_type": "1", "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "hash": "9e18e14bb3b91680d4cab5dc15f9bcbe3ccf2c3a30d52939a9e2c914cfb2c910", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "434bed5b-9c63-482f-a9ab-5e56aa10cb2a", "node_type": "1", "metadata": {}, "hash": "3fa421ed712c57b232fbf798b72817f0f0a27660462a18527cf06c23130f17a9", "class_name": "RelatedNodeInfo"}}, "text": "This means no embeddings, and no\nfancy retrieval mechanisms.\n\nIdeally, we can directly insert an entire 10-k filing (or even all four 10-k\nfilings) into the prompt. However, we found that a single UBER 10-k filing\nactually consists of ~ **160k tokens, which is greater than the 100k context\nwindow.** This means that we still have to chunk up each filing!\n\nWe end up using our [ list index data structure ](https://gpt-\nindex.readthedocs.io/en/latest/guides/primer/index_guide.html#querying) \u2014 we\nsplit each text up into massive ~100k token chunks, and use our **response\nsynthesis strategies** to synthesize an answer across multiple chunks.\n\nWe run some queries over each filing as well as over multiple filings, similar\nto our original blog post. ", "mimetype": "text/plain", "start_char_idx": 3113, "end_char_idx": 3863, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "434bed5b-9c63-482f-a9ab-5e56aa10cb2a": {"__data__": {"id_": "434bed5b-9c63-482f-a9ab-5e56aa10cb2a", "embedding": null, "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f9f418a7-e124-424c-b5b7-b35cc94d96f4", "node_type": "4", "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "hash": "d23518db6ea14b22a2bcfeed1a8db3a97e45ad3edbbb75c07d3f4f421ba4c37a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d332cce-a3fc-4ece-bde6-11a55fdd5a67", "node_type": "1", "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "hash": "2e5aeaef5ce9f0ff85cdd9ea78bc26ad602648ecbded67c1d14bf57abf1fc7df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f04b2ee7-ef13-4efb-a7dc-749f7d707a7c", "node_type": "1", "metadata": {}, "hash": "159aed1810982c09c19ba362267fcfcb793e743c16d58a1ed022eda21a0a2767", "class_name": "RelatedNodeInfo"}}, "text": "We report the results below.\n\n#  Tutorial Setup\n\nOur data ingestion is the same as the LlamaIndex + Unstructured blog post. We\nuse Unstructured\u2019s HTML parser to parse the HTML DOM into nicely formatted\ntext. We then create a Document object for each SEC filing.\n\nYou can access Unstructured data loaders on [ LlamaHub\n](https://llamahub.ai/l/file-unstructured) .\n\n    \n    \n    from llama_index import download_loader\n    from pathlib import Path\n    \n    UnstructuredReader = download_loader(\"UnstructuredReader\", refresh_cache=True)\n    \n    loader = UnstructuredReader()\n    doc_set = {}\n    all_docs = []\n    years = [2022, 2021, 2020, 2019]\n    for year in years:\n        year_doc = loader.load_data(file=Path(f'./data/UBER/UBER_{year}.html'), split_documents=False)[0]\n        # insert year metadata into each year\n        year_doc.extra_info = {\"year\": year}\n        doc_set[year] = year_doc\n        all_docs.append(year_doc)\n\nNext, we want to setup the Anthropic LLM. We\u2019re using claude-v1 by default. ", "mimetype": "text/plain", "start_char_idx": 3863, "end_char_idx": 4873, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f04b2ee7-ef13-4efb-a7dc-749f7d707a7c": {"__data__": {"id_": "f04b2ee7-ef13-4efb-a7dc-749f7d707a7c", "embedding": null, "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f9f418a7-e124-424c-b5b7-b35cc94d96f4", "node_type": "4", "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "hash": "d23518db6ea14b22a2bcfeed1a8db3a97e45ad3edbbb75c07d3f4f421ba4c37a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "434bed5b-9c63-482f-a9ab-5e56aa10cb2a", "node_type": "1", "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "hash": "c88298ff4674dac77139916e0009571299a8fd9fe623b3da07686479a578c1a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c08ed13-db8f-48b0-b841-4ab23972f146", "node_type": "1", "metadata": {}, "hash": "0950f2a7df2bf0a91223ccca5ab63c1574f82567b80166304717b29a30f2d6d5", "class_name": "RelatedNodeInfo"}}, "text": "We\nalso want to manually define the new 100k-token input size within our `\nPromptHelper ` object; this will help us figure out how to \u201ccompact\u201d context\ninto the input prompt space during response synthesis.\n\nWe set the ` max_input_size ` to 100k and the output length to 2048. We also\nset the context chunk size to a high value (95k, leaving some buffer room for\nrest of the prompt). Context will only be chunked if the number of tokens\nexceeds this limit.\n\n    \n    \n    from llama_index import PromptHelper, LLMPredictor, ServiceContext\n    from langchain.llms import Anthropic\n    \n    # define prompt helper\n    # set maximum input size\n    max_input_size = 100000\n    # set number of output tokens\n    num_output = 2048\n    # set maximum chunk overlap\n    max_chunk_overlap = 20\n    prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n    \n    llm_predictor = LLMPredictor(llm=Anthropic(model=\"claude-v1.3-100k\", temperature=0, max_tokens_to_sample=num_output))\n    service_context = ServiceContext.from_defaults(\n        llm_predictor=llm_predictor, prompt_helper=prompt_helper,\n        chunk_size_limit=95000\n    )\n\n#  Analyzing a Single Document\n\nLet\u2019s first analyze queries over a single document. We build a list index over\nthe 2019 UBER 10-K:\n\n    \n    \n    list_index = GPTListIndex.from_documents([doc_set[2019]], service_context=service_context)\n    print(len(list_index.index_struct.nodes))\n\nAs mentioned, the 10-K exceeds the 100k token limit, and so there are two\nnodes within the list index.\n\nWe then ask a query: \u201cWhat were some of the biggest risk factors in 2019?\u201d\n\nRecall that there are two approaches within LlamaIndex for response synthesis\nacross multiple nodes (where the total context exceeds the context window): a\n\u201ccreate-and-refine\u201d strategy, and a \u201ctree summarize\u201d strategy.\n\n  * **Create-and-Refine:** sequentially go through each retrieved ` Node ` . Use a Question-Answer Prompt for the first Node, and use a Refine Prompt for subsequent Nodes. Make a separate LLM call per Node. \n  * **Tree Summarize:** Given a set of ` Node ` objects and the query, recursively construct a tree using the Question Answer Prompt and return the root node as the response. Good for summarization purposes. \n\n", "mimetype": "text/plain", "start_char_idx": 4873, "end_char_idx": 7122, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c08ed13-db8f-48b0-b841-4ab23972f146": {"__data__": {"id_": "3c08ed13-db8f-48b0-b841-4ab23972f146", "embedding": null, "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f9f418a7-e124-424c-b5b7-b35cc94d96f4", "node_type": "4", "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "hash": "d23518db6ea14b22a2bcfeed1a8db3a97e45ad3edbbb75c07d3f4f421ba4c37a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f04b2ee7-ef13-4efb-a7dc-749f7d707a7c", "node_type": "1", "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "hash": "9d7c2f30a55cd15ced89491ac1c3ef2c79d32ceaefdfd0330272649bca257027", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f23a11d-bd1f-49e0-b995-d33458feba0e", "node_type": "1", "metadata": {}, "hash": "bd57488244a2460a7d68a82000fa4a54ea0a45e84602e9d7470b08f378b95aa1", "class_name": "RelatedNodeInfo"}}, "text": "Claude-v1 100K does not do well with the \u201ccreate-and-refine\u201d strategy (which\nis the default).\n\n    \n    \n    # NOTE: the default create/refine approach does not give good answers\n    query = \"What were some of the biggest risk factors in 2019?\"\n    query_engine = list_index.as_query_engine(service_context=service_context)\n    response = query_engine.query(query)\n\nHere is the retrieved answer:\n\n    \n    \n    Original answer:\n    \n    Some of the biggest risk factors for Uber in 2019 included:\n    \n    \u2022 Regulatory challenges and uncertainty. Uber faced regulatory challenges and uncertainty in many markets, including restrictions on its products and services, caps on pricing, and licensing requirements. ", "mimetype": "text/plain", "start_char_idx": 7122, "end_char_idx": 7833, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f23a11d-bd1f-49e0-b995-d33458feba0e": {"__data__": {"id_": "5f23a11d-bd1f-49e0-b995-d33458feba0e", "embedding": null, "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f9f418a7-e124-424c-b5b7-b35cc94d96f4", "node_type": "4", "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "hash": "d23518db6ea14b22a2bcfeed1a8db3a97e45ad3edbbb75c07d3f4f421ba4c37a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c08ed13-db8f-48b0-b841-4ab23972f146", "node_type": "1", "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "hash": "cd48f4ecc2c7f77f09ec140035eacf6fbdcdf18b53e491634280c591f5f723f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26d1be8a-0c5f-4f07-b243-d4a5c5e73577", "node_type": "1", "metadata": {}, "hash": "168f68dc7dfd193c03d0d2391df28f6f8bcfaf5a42a842e1ee9e8f306392b8e8", "class_name": "RelatedNodeInfo"}}, "text": "For example, California's AB5 law and other similar laws increased the risk of Drivers being classified as employees. Uber also faced regulatory scrutiny and bans in London, Barcelona, and other markets.\n    \n    \u2022 Competition. The markets in which Uber operates are highly competitive, and Uber faced significant competition from well-established and low-cost alternatives in 2019. Competitors also aggressively competed for Drivers and consumers by offering significant incentives and discounts. \n    \n    \u2022 Safety and security. There were risks related to the safety and security of Uber's platform, including risks from vehicle or scooter accidents, assaults, and other incidents. Uber released a safety report in 2019 detailing reports of sexual assaults on its platform which brought additional scrutiny.\n    \n    \u2022 Financial performance and profitability. Uber continued to experience significant losses in 2019 as it invested in growth. There was no guarantee that Uber would achieve or sustain profitability in the future.  \n    \n    \u2022 Reliance on third parties. Uber relied on third parties for key elements of its business like cloud computing services, payment processing, and background checks. If these third parties failed to provide services or increased costs, it could adversely impact Uber's business.\n    \n    \u2022 Macroeconomic conditions. Uber's business was sensitive to economic conditions and changes that impacted discretionary consumer spending. A decline in the economy could reduce demand for Uber's products and services.  \n    \n    \u2022 Reliance on Drivers and Restaurants. Uber's success depended on attracting and retaining enough Drivers and Restaurants to its platform. If Uber was unable to attract or retain enough Drivers and Restaurants, it could negatively impact its operations.\n    \n    \u2022 Intellectual property. If Uber was unable to protect its intellectual property, or if third parties claimed Uber was infringing on their intellectual property, it could harm Uber's business. Uber relied on a combination of copyright, trademark, patent, and trade secret laws to establish its intellectual property rights.\n    \n    Refined answer:\n    \n    Some of the biggest risk factors for Uber in 2019 included:\n    \n    \u2022 Regulatory challenges and uncertainty. Uber faced significant regulatory challenges in 2019, including AB5 in California which increased the risk of Drivers being classified as employees and regulatory scrutiny and temporary bans in London and other markets. These regulatory issues created uncertainty and posed risks to Uber's business model and financial results.\n    \n    \u2022 Competition. The ridesharing and meal delivery markets are highly competitive, and competitors offered significant incentives and discounts to take market share from Uber in 2019. This competition could negatively impact Uber's growth and profitability.\n    \n    \u2022 Safety and security. There were risks related to the safety and security of Uber's platform, including risks from vehicle or scooter accidents, assaults, and other incidents. Uber's safety report detailing sexual assault reports brought additional scrutiny and risks. Any failure to ensure safety could significantly damage Uber's reputation and business.\n    \n    \u2022 Financial performance and profitability. Uber continued to experience substantial losses in 2019 due to investments in growth, and there was no guarantee of future profitability. Uber's path to profitability was uncertain, and failure to become profitable could threaten its business model and access to capital.  \n    \n    \u2022 Reliance on third parties. Uber relied on third parties for services like cloud computing, payment processing, and background checks. If these third parties failed to provide services or increased costs, it could negatively impact Uber's offerings, growth, and profitability.  \n    \n    \u2022 Macroeconomic conditions. Uber's business was sensitive to the economy and consumer discretionary spending. An economic downturn could reduce demand for Uber's offerings and threaten its growth and financial performance.\n    \n    \u2022 Reliance on Drivers and Restaurants. Uber's success depended on attracting and retaining enough Drivers and Restaurants to support growth and meet consumer demand. Failure to attract or retain Drivers and Restaurants could significantly impact Uber's business.  \n    \n    \u2022 Intellectual property. Failure to protect intellectual property or infringement of others' intellectual property could damage Uber's competitive position and business. Uber relied on laws and policies to protect intellectual property, but monitoring infringement and unauthorized use was difficult.\n\nAs shown, the answer contains the prompt within it; the point of the refine\nprompt was to produce just the refined answer, not produce both the original\nand refined answers.\n\nFor some reason, using the Tree Summarize strategy works much better.\n\n    \n    \n    # NOTE: tree_summarize gives better answers\n    query = \"What were some of the biggest risk factors in 2019?\"\n    query_engine = list_index.as_query_engine(service_context=service_context, response_mode=\"tree_summarize\")\n    response = query_engine.query(query)\n\nReturned response:\n\n    \n    \n    \u2022 Regulatory challenges and uncertainty: Uber faced significant regulatory challenges and uncertainty in 2019, including AB5 in California which codified a new test for determining whether workers should be classified as employees or independent contractors. Uber also faced regulatory scrutiny and bans in other markets like London, UK. These regulatory issues created uncertainty and risk around Uber's business model and operations.\n    \n    \u2022 Safety and security: Uber received negative publicity around safety incidents on its platform which could damage its brand and reputation. Uber released a safety report in 2019 on sexual assaults and other incidents which led to additional scrutiny. Safety and security risks remain an ongoing issue for Uber's business.\n    \n    \u2022 Competition: The markets in which Uber competes are intensely competitive, and Uber faces competition from new and existing companies in the various segments it operates in like ridesharing, food delivery, and logistics. Increased competition could make it difficult for Uber to attract and retain platform users.\n    \n    \u2022 Financial losses: Uber continued to experience significant losses from operations in 2019 as it invested in growth. Uber may not achieve or sustain profitability in the future which could negatively impact its business and stock price.\n    \n    \u2022 Reliance on third-parties: Uber relies on third-party data centers, cloud computing services, and software which exposes it to risks if these third-parties experience any disruptions or interference. Any disruption of third-party services could negatively impact Uber's business.\n    \n    \u2022 Macroeconomic conditions: Uber's business is sensitive to macroeconomic conditions, and any downturn could reduce discretionary consumer spending which would harm Uber's business. Issues like a recession, increased unemployment, or reduced consumer confidence could all potentially impact Uber's business.\n    \n    \u2022 Reliance on Drivers and Restaurants: Uber's business depends on attracting and retaining Drivers and Restaurants to its platform. If Uber is unable to attract or maintain a critical mass of Drivers and Restaurants, its platform will become less appealing to platform users and its financial results would be adversely impacted.\n\nThe answer is much better. ", "mimetype": "text/plain", "start_char_idx": 7833, "end_char_idx": 15388, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26d1be8a-0c5f-4f07-b243-d4a5c5e73577": {"__data__": {"id_": "26d1be8a-0c5f-4f07-b243-d4a5c5e73577", "embedding": null, "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f9f418a7-e124-424c-b5b7-b35cc94d96f4", "node_type": "4", "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "hash": "d23518db6ea14b22a2bcfeed1a8db3a97e45ad3edbbb75c07d3f4f421ba4c37a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f23a11d-bd1f-49e0-b995-d33458feba0e", "node_type": "1", "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "hash": "1185e77eb7abf636549661abc0ceaf0eece15c60c4a3ec1d2bbd05738912bd01", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc7669e1-ff69-4731-9506-1f6d4c07a3c1", "node_type": "1", "metadata": {}, "hash": "55a3ecbc3d457d230189d986d262e0e2059c1cf1bc21e8f2221bbd5299eab691", "class_name": "RelatedNodeInfo"}}, "text": "It goes into details about the US-China trade war,\nslowing economic growth, Brexit, and more (keep in mind 2019 is pre-COVID).\n\n##  Token Usage and Latency\n\nThe document contains around ~170K tokens. For some reason, this number is not\nreflected on the Anthropic usage logs (the \u201cPrompt Tokens\u201d section seems\ncapped at 10240). But the Prompt Length (in characters) is logged, as well as\nthe model latency.\n\nGiven the pricing, ~170K tokens would be equivalent to $1.5\u20132 USD.\n\nA query through one Uber SEC-10K takes around **150** **seconds** , including\nall LLM calls. This is actually a bit faster than repeated calls to\nChatGPT/davinci. Each ChatGPT/davinci call (with the 4K token window\nmaximized), empirically can take 6\u201310 seconds to complete \u2192 **125\u2013250 seconds\n(** or more).\n\n#  Analyzing Multiple Documents\n\nA popular example in our [ previous blog post\n](https://medium.com/@jerryjliu98/how-unstructured-and-llamaindex-can-help-\nbring-the-power-of-llms-to-your-own-data-3657d063e30d) was showcasing that you\ncould compare/contrast different documents with LlamaIndex graph structures.\n\nWe test whether we can do that here as well, by feeding in multiple SEC\nreports into Claude-v1 100k.\n\n**Caveat:** Considering that one UBER SEC-10K filing doesn\u2019t even fit in the\ncontext window, we\u2019ll of course also need to implement response synthesis\nstrategies in order to handle ingesting multiple 10K filings.\n\nWe build a list index over all 4 10K filings: 2019, 2020, 2021, and 2022.\n\n    \n    \n    list_index = GPTListIndex.from_documents(all_docs, service_context=service_context)\n    print(len(list_index.index_struct.nodes))\n\nWe then ask our question using our Tree Summarize response mode.\n\n    \n    \n    query = \"How are the risk factors changing across years? ", "mimetype": "text/plain", "start_char_idx": 15388, "end_char_idx": 17156, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc7669e1-ff69-4731-9506-1f6d4c07a3c1": {"__data__": {"id_": "bc7669e1-ff69-4731-9506-1f6d4c07a3c1", "embedding": null, "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f9f418a7-e124-424c-b5b7-b35cc94d96f4", "node_type": "4", "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "hash": "d23518db6ea14b22a2bcfeed1a8db3a97e45ad3edbbb75c07d3f4f421ba4c37a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26d1be8a-0c5f-4f07-b243-d4a5c5e73577", "node_type": "1", "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "hash": "06df752978d10efb6c2bb9ebc0c98af3db6d18f9bec3727993e86905c082e9e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a127ad4-5783-40f6-9725-5ad5f9b20ba0", "node_type": "1", "metadata": {}, "hash": "c9054775c427098cd1d29ec38b7b847bb5a95ca554c8855a372096e5bc7dd296", "class_name": "RelatedNodeInfo"}}, "text": "Compare/contrast the risk factors across the SEC filings.\"\n    query_engine = list_index.as_query_engine(response_mode=\"tree_summarize\")\n    response = query_engine.query(query)\n\nThe full answer is given below:\n\n    \n    \n    The risk factors disclosed in Uber's SEC filings have evolved over time based on Uber's business and industry changes. Some of the key differences in risk factors across the filings are:\n    \n    2017 10-K:\n    - Focused heavily on risks related to negative publicity, competition, dependence on independent contractors, and regulatory challenges as Uber was still facing backlash from various PR crises and regulatory pushback. \n    - Also highlighted risks from intellectual property litigation given various IP disputes at the time.\n    \n    2018 10-K:\n    - Added more risks related to autonomous vehicles as Uber ramped up its self-driving car efforts. Specifically called out risks from accidents, technical challenges, and competition in the AV space.\n    ", "mimetype": "text/plain", "start_char_idx": 17156, "end_char_idx": 18145, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a127ad4-5783-40f6-9725-5ad5f9b20ba0": {"__data__": {"id_": "5a127ad4-5783-40f6-9725-5ad5f9b20ba0", "embedding": null, "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f9f418a7-e124-424c-b5b7-b35cc94d96f4", "node_type": "4", "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "hash": "d23518db6ea14b22a2bcfeed1a8db3a97e45ad3edbbb75c07d3f4f421ba4c37a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc7669e1-ff69-4731-9506-1f6d4c07a3c1", "node_type": "1", "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "hash": "347ac8703147bdb861a12dc123a72e51209007df10d550492b0e413ff97774e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85e7a5e9-8530-4cb3-9cd5-28445dc13d6e", "node_type": "1", "metadata": {}, "hash": "b40031293d95c16f41b5c75ed0e3327d7bf7f5a44149e080cc144223d2948bd8", "class_name": "RelatedNodeInfo"}}, "text": "- Removed some risks related to negative publicity and PR crises as those issues had subsided. But added risks related to corporate culture and workplace environment given the Fowler scandal.\n    \n    2019 10-K: \n    - Further expanded AV risks to include risks from partnerships and third party relationships as Uber relied more on partnerships with other companies like Toyota and Volvo. \n    - Added risks related to environmental regulations and climate change as those issues received more mainstream attention. \n    - Removed risks related to initial scaling challenges as Uber had matured as a company. But added risks related to growth into new offerings and markets as Uber expanded into freight shipping, scooters, etc.\n    \n    In summary, Uber's risk factors have evolved to reflect the key issues and challenges facing the company at any point in time. The risks highlight both external factors like regulations as well as internal factors related to Uber's business, technology, and operations. The changes over time show how an innovative company's risks can shift quite quickly.\n    \n    Some of the key trends in how the risk factors are changing include:\n    \n    1. Risks related to negative publicity and PR crises decreased over time as those issues subsided, while risks related to other issues like AV technology, workplace culture, and new initiatives increased. This shows how Uber's risks evolved as the company matured.\n    \n    2. Risks tend to increase in areas where Uber is actively investing or expanding into. For example, risks related to AVs, partnerships, and new mobility offerings increased as Uber ramped up efforts in those areas. This highlights how risk profiles change with a company's strategic priorities.\n    \n    3. External risks like regulations and climate change were added as those issues gained more mainstream attention and importance. This shows how companies have to adapt their risk factors to account for changes in the overall business environment. \n    \n    4. Certain foundational risks around competition, growth, and reliance on independent contractors persisted over time. But the specifics and details provided for those risks evolved based on Uber's current challenges and priorities. So while the themes remained, the risks were actively updated.\n    \n    In summary, Uber's risk factors changed over time to provide more details on the company's priorities, challenges, partnerships, and external environment at any given point. But certain core risks inherent to Uber's business model have also remained consistently highlighted, demonstrating how those foundational risks are long-term in nature. The changes in risks over time provide a glimpse into how an innovative company's risk profile is constantly evolving.\n\nThis response only contains risk refactor analysis over the 2019 10-K (which\nin turn contains risk refactors for 2017 and 2018). It does not contain the\nyears from 2020 onwards. Part of this is potentially due to our tree summarize\nresponse synthesis strategy. Nevertheless, it shows that trying to naively\n\u201cstuff\u201d documents into big 100K token chunks with simple response synthesis\nstrategies still does not produce the optimal answers.\n\n##  **Token Usage and Latency**\n\nAs expected, feeding all four documents into Anthropic necessitates many more\nchained LLM calls, which consumes way more tokens and takes a lot longer (on\nthe order of 9\u201310 minutes).\n\n#  Conclusion\n\nIn general, the new 100K context window is incredibly exciting and offers\ndevelopers a new mode of feeding in data into the LLM for different\ntasks/queries. It offers coherent analysis with a marginal token cost that is\nmuch cheaper than that of GPT-4.\n\nThat said, trying to maximize this context window with each inference call\ndoes come with tradeoffs in terms of latency and cost.\n\nWe look forward to doing more experiments/comparisons/thought pieces on top of\nClaude! ", "mimetype": "text/plain", "start_char_idx": 18145, "end_char_idx": 22075, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85e7a5e9-8530-4cb3-9cd5-28445dc13d6e": {"__data__": {"id_": "85e7a5e9-8530-4cb3-9cd5-28445dc13d6e", "embedding": null, "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f9f418a7-e124-424c-b5b7-b35cc94d96f4", "node_type": "4", "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "hash": "d23518db6ea14b22a2bcfeed1a8db3a97e45ad3edbbb75c07d3f4f421ba4c37a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a127ad4-5783-40f6-9725-5ad5f9b20ba0", "node_type": "1", "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}, "hash": "f903f4a7a02a7fbfa55088fa4c9e384c29a0bff90434da80fce689da053ef9e7", "class_name": "RelatedNodeInfo"}}, "text": "Let us know your feedback.\n\n##  Resources\n\nYou can check out our [ full Colab notebook here\n](https://colab.research.google.com/drive/1uuqvPI2_WNFMd7g-ahFoioSHV7ExB2GR?usp=sharing)\n.\n\n", "mimetype": "text/plain", "start_char_idx": 22075, "end_char_idx": 22259, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc98d873-7413-48ac-8653-f6ffb1ccc589": {"__data__": {"id_": "fc98d873-7413-48ac-8653-f6ffb1ccc589", "embedding": null, "metadata": {"filename": "llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.md", "extension": ".md", "title": "LlamaIndex on TWIML AI: A Distilled Summary (using LlamaIndex)", "date": "May 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cc03d67e-9f21-4521-b472-551874bf4c37", "node_type": "4", "metadata": {"filename": "llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.md", "extension": ".md", "title": "LlamaIndex on TWIML AI: A Distilled Summary (using LlamaIndex)", "date": "May 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595"}, "hash": "adde2250c4c6b4a98d2a892874c0b3f884cfead5097dcace098c9991e2212113", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43543bd7-41a5-47f8-8147-f214729708dd", "node_type": "1", "metadata": {}, "hash": "4a09e28266196ef05c594cfa187a6a99c7ea0acd706be12814fad953156b8b06", "class_name": "RelatedNodeInfo"}}, "text": "#  Overview\n\nI had the pleasure of speaking with Sam Charrington on the [ TWIML AI podcast\n](https://twimlai.com/) about LlamaIndex, and the episode was just released\nthis past Monday (5/8/23).\n\nI thought it would be a fun experiment to distill some highlights from the\npodcast! And what better way to do this than using LlamaIndex itself (plus\nOpenAI Whisper for transcription)?\n\nI did the following:\n\n  1. Ran the podcast through [ whisper.cpp ](https://github.com/ggerganov/whisper.cpp)\n  2. Did some light manual cleaning of the text, and uploaded it as a [ Dropbox file ](https://www.dropbox.com/s/gn2rpfvkjkygemb/twiml.txt?dl=0) . \n  3. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 643, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43543bd7-41a5-47f8-8147-f214729708dd": {"__data__": {"id_": "43543bd7-41a5-47f8-8147-f214729708dd", "embedding": null, "metadata": {"filename": "llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.md", "extension": ".md", "title": "LlamaIndex on TWIML AI: A Distilled Summary (using LlamaIndex)", "date": "May 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cc03d67e-9f21-4521-b472-551874bf4c37", "node_type": "4", "metadata": {"filename": "llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.md", "extension": ".md", "title": "LlamaIndex on TWIML AI: A Distilled Summary (using LlamaIndex)", "date": "May 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595"}, "hash": "adde2250c4c6b4a98d2a892874c0b3f884cfead5097dcace098c9991e2212113", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc98d873-7413-48ac-8653-f6ffb1ccc589", "node_type": "1", "metadata": {"filename": "llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.md", "extension": ".md", "title": "LlamaIndex on TWIML AI: A Distilled Summary (using LlamaIndex)", "date": "May 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595"}, "hash": "6ab41af4f9526d6c2c09f1d94781a7808248045b10e3334e95a4d81208075b76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5c37210-9e3b-421a-b6e7-0e1309f8d3a4", "node_type": "1", "metadata": {}, "hash": "0f7f55896a849dbf04b5f09d1ebf7300d1181e755da92e4ca1afb8099049e9bb", "class_name": "RelatedNodeInfo"}}, "text": "Get a high-level summary + ask some more targeted questions over the podcast transcript. Uses LlamaIndex . [ Colab notebook here ](https://colab.research.google.com/drive/1sAHWbyQRjtp_w-r-HOMpL0gkkhigkdGR?usp=sharing) ! \n\nWe used the our list index to get an overall summary of the podcast, and also\nour vector index to ask more specific questions. We ask some basic questions\nourselves, but also directly ask questions that Sam asked during the podcast\nin order to extract more concise versions of the responses that I gave\n(including removing a bunch of filler \u201cyou know\u201d words).\n\nAll the distilled highlights using LlamaIndex are showcased below. Let us know\nwhat you think!\n\n#  Distilled Highlights\n\n**What are three key points described in this podcast? Give a followup of each\nkey point with an explanation.", "mimetype": "text/plain", "start_char_idx": 643, "end_char_idx": 1456, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5c37210-9e3b-421a-b6e7-0e1309f8d3a4": {"__data__": {"id_": "d5c37210-9e3b-421a-b6e7-0e1309f8d3a4", "embedding": null, "metadata": {"filename": "llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.md", "extension": ".md", "title": "LlamaIndex on TWIML AI: A Distilled Summary (using LlamaIndex)", "date": "May 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cc03d67e-9f21-4521-b472-551874bf4c37", "node_type": "4", "metadata": {"filename": "llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.md", "extension": ".md", "title": "LlamaIndex on TWIML AI: A Distilled Summary (using LlamaIndex)", "date": "May 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595"}, "hash": "adde2250c4c6b4a98d2a892874c0b3f884cfead5097dcace098c9991e2212113", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "43543bd7-41a5-47f8-8147-f214729708dd", "node_type": "1", "metadata": {"filename": "llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.md", "extension": ".md", "title": "LlamaIndex on TWIML AI: A Distilled Summary (using LlamaIndex)", "date": "May 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595"}, "hash": "dfffa5acc30cc7cf3164db1b07307b1cfea2ecc36f43db6b17fc494a4d28a3ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf124a05-b249-4c64-b661-24da58101554", "node_type": "1", "metadata": {}, "hash": "31444a0a77f50f1f1a4c8ca04214e654d4ad9dec43dd622e1d73007bf0e9f50a", "class_name": "RelatedNodeInfo"}}, "text": "**\n\n  1. _LlamaIndex is a toolkit for connecting language models to data._ It was initially designed as a fun tool to play around with and solve a specific problem, but has since evolved into a set of useful tools and instructions that people can use to build applications on top of. The technique people were playing around with at the time was the idea of prompting and context learning, which is providing the right conditioning variable for the input and using that to send some sort of input prompt to the language model and get back a response. Additionally, LlamaIndex offers more advanced primitives such as decision-making at the top level to route queries to the right sub-data structure, and the ability to synthesize an answer from multiple data sources. It also provides an outer abstraction layer that can automatically reason which structure to use given an outer query request, and can be used as a drop-in module on top of existing data systems without having to worry about complexity. Examples of applications that can be built on top of LlamaIndex include ingesting video and structured data to parse into an audio transcript, running image captioning models, and creating augmented chatbot experiences on top of web scrapers. \n  2. _LlamaIndex is also exploring the idea of automation and unifying everything under a single query interface,_ so that users don\u2019t have to specify a different parameter for every use case. This includes optimizing token usage, making queries faster, and reducing costs for the user. Additionally, LlamaIndex is looking into applying automation to the data system world, such as teaching Oracle databases how to spit out natural language prompt responses, and making the data stack more efficient. This includes simplifying the data stack over time, especially as language models take off, and leveraging capabilities of LLM\u2019s and various components of the data landscape to simplify the number of steps it takes from raw data to insight for the user. They are also exploring the idea of inferring the right schemas and writing structured data from unstructured data, as well as automatically building a natural language query interface with a view of the data within the data system. \n  ", "mimetype": "text/plain", "start_char_idx": 1456, "end_char_idx": 3695, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf124a05-b249-4c64-b661-24da58101554": {"__data__": {"id_": "cf124a05-b249-4c64-b661-24da58101554", "embedding": null, "metadata": {"filename": "llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.md", "extension": ".md", "title": "LlamaIndex on TWIML AI: A Distilled Summary (using LlamaIndex)", "date": "May 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cc03d67e-9f21-4521-b472-551874bf4c37", "node_type": "4", "metadata": {"filename": "llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.md", "extension": ".md", "title": "LlamaIndex on TWIML AI: A Distilled Summary (using LlamaIndex)", "date": "May 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595"}, "hash": "adde2250c4c6b4a98d2a892874c0b3f884cfead5097dcace098c9991e2212113", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5c37210-9e3b-421a-b6e7-0e1309f8d3a4", "node_type": "1", "metadata": {"filename": "llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.md", "extension": ".md", "title": "LlamaIndex on TWIML AI: A Distilled Summary (using LlamaIndex)", "date": "May 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595"}, "hash": "e53cc068de79b66768ba1804fbf85c3e56b016cfb4608fe1851655871393416d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82d45681-a1cc-4d2b-a19f-c5264aa75363", "node_type": "1", "metadata": {}, "hash": "50cb08c6d5da85bec9e6c188bbad22a9df81f770749be59e28c31cd8e1e33192", "class_name": "RelatedNodeInfo"}}, "text": "3. _LlamaIndex is also exploring the idea of agents as a layer of automation for decision making over any sort of function that you want to run._ This includes taking in some input and doing reasoning under the hood to decide, make a decision over some input, as well as some access to some context, for instance, over your data or over the set of tools that is able to have access to. Additionally, LlamaIndex is looking into ways to reduce cost and latency, such as using more fine-tuned distilled models that are a bit smaller, and making sure that the more decisions that are chained together, the less errors propagate over time. They are also exploring the idea of observability and evidence across a chain of relatively independent decisions that individual agents are making, as well as the interfaces that these agents might use, such as traditional software and agent worlds. \n\n**What is the origin story of LlamaIndex?**\n\nThe origin story of LlamaIndex is that it was founded in November by Jerry,\nwho was trying to build a sales bot. He was playing around with GPT-3 and\nwanted to use it on his internal company data. ", "mimetype": "text/plain", "start_char_idx": 3695, "end_char_idx": 4825, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82d45681-a1cc-4d2b-a19f-c5264aa75363": {"__data__": {"id_": "82d45681-a1cc-4d2b-a19f-c5264aa75363", "embedding": null, "metadata": {"filename": "llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.md", "extension": ".md", "title": "LlamaIndex on TWIML AI: A Distilled Summary (using LlamaIndex)", "date": "May 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cc03d67e-9f21-4521-b472-551874bf4c37", "node_type": "4", "metadata": {"filename": "llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.md", "extension": ".md", "title": "LlamaIndex on TWIML AI: A Distilled Summary (using LlamaIndex)", "date": "May 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595"}, "hash": "adde2250c4c6b4a98d2a892874c0b3f884cfead5097dcace098c9991e2212113", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf124a05-b249-4c64-b661-24da58101554", "node_type": "1", "metadata": {"filename": "llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.md", "extension": ".md", "title": "LlamaIndex on TWIML AI: A Distilled Summary (using LlamaIndex)", "date": "May 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595"}, "hash": "92b66a6a2c4ea14175bb145354c3d290c4ab9c105943ea9cd931d6a2abeb9a30", "class_name": "RelatedNodeInfo"}}, "text": "He wanted to use it to\nsynthesize a to do list for him for the next customer meeting, as he had to\nspend 20\u201330 minutes reviewing notes from the previous call transcripts. This\nled to the idea of stuffing data from Notions, Slack, Salesforce, data lakes,\nvector databases, and structure databases into language models. This was the\nimpetus for LlamaIndex, which is focused on connecting data to language models\nand tapping into the capabilities of language models to utilize them on top of\nprivate sources of data.\n\n**What is LlamaIndex doing beyond top-k retrieval?**\n\nLlamaIndex is offering more advanced primitives on top of basic top-k\nretrieval in order to provide responses to more complicated questions. These\nprimitives include decision-making at the top level to route queries to the\nright sub-data structure, synthesizing information from multiple data systems,\nand providing trade-offs between different approaches.\n\nAdditionally, LlamaIndex is working on building tooling to help users create\ncustomizable indexes and views of their data to allow them to execute\ndifferent types of queries. This includes connecting to existing data systems,\ndefining metadata on top of each unit of data, providing the building blocks\nto create different types of indexes, and abstracting away complexity with an\nouter agent layer that can automatically reason which structure to use given a\nquery request. This allows users to get the best results for a query, while\nalso providing an alternative to something like a langchain or using it as\npart of building a broader solution.\n\n**[Sam] It sounds like we\u2019re starting to identify a higher level of\nabstraction that different use cases will fall under. Is it more the case that\nthere\u2019s some manageable number of these primitives, like 10, 20, or is it that\nevery use case is going to be a little bit different, and there are hundreds\nof thousands of kind of fundamental ways that people want to work with their\ndocuments, and so you need to just give them a very open capability?**\n\nJerry\u2019s response is that there are probably a few different use cases that\npeople tend to want to get answers from over their data, and it is possible\nthere is a giant long tail of different tasks. He believes that the complexity\nof the task scales with the number of steps it requires to execute, and that\nusers need to be given customizable building blocks in order to get the\nresults they want. He also believes that the next natural step is to automate\nthe process and unify everything under a single query interface, so that users\ndon\u2019t have to specify different parameters for every use case.\n\nHe also believes that this paradigm is displacing more static paradigms like\nETL, and that it is applicable to a wide range of applications. He sees this\nagent type environment becoming fundamental infrastructure that reimagines the\nentire existing enterprise data stack, and that it can be used to parse\nunstructured data into structured data, as well as to automatically reason how\nto best transform data from one place to another. He also believes that this\nwill make the job of the data engineer and data scientist much more efficient,\nand that it will enable the creation of natural language query interfaces that\nhave a view of the data within the data system.\n\n**[Sam] When you think about the interface between LLM-based data processing\nsystem and the data sources of record, what does that interface evolve to look\nlike? For example, does it evolve to look like the chat GPT plugin model,\nwhere we\u2019re going to teach our Oracle databases how to spit out natural\nlanguage prompt responses, that kind of thing, or do you think that there\u2019s\nsome more efficient way of doing that or is that more efficient? Like, what\u2019s\nyour view of the way these things evolve?**\n\nI think the way this interface will evolve is that it will become more\nautomated and efficient. We will be able to use language models to understand\nraw text and extract the relevant information from it, without having to\nmanually enter data into a structured format. We will also be able to use\nagents to automate decision making and provide a unified query interface, so\nthat users don\u2019t have to specify different parameters for every use case.\n\nAdditionally, we can use LlamaIndex to structure data in a way that allows us\nto make use of the limited prompt size of GPT-3, while still being able to\nachieve the task. We can also use this data stack to infer the right schemas\nand further write structured data from unstructured data, as well as\nautomatically build a natural language query interface that has a view of the\ndata within the data system. This will enable us to make the job of the data\nengineer and data scientist much more efficient by having automated reasoning\nagents over deciding, making decisions at every stage of the data\ninfrastructure stack.\n\n#  Want to ask your own questions over the podcast?\n\nIf you want to build your own LLM-powered chatbot over our TWIML podcast,\ncheck out the resources below!\n\n[ Colab notebook ](https://colab.research.google.com/drive/1sAHWbyQRjtp_w-r-\nHOMpL0gkkhigkdGR?usp=sharing)\n\n[ Raw Transcript ](https://www.dropbox.com/s/gn2rpfvkjkygemb/twiml.txt?dl=0)\n\n[ Podcast on Spotify\n](https://open.spotify.com/episode/2vEO6dkzfEw5e7eZqngsGz?si=397cc8d7496a479c)\n\n", "mimetype": "text/plain", "start_char_idx": 4825, "end_char_idx": 10137, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "349fbca7-4d4b-460c-9132-02ac5ea6fa88": {"__data__": {"id_": "349fbca7-4d4b-460c-9132-02ac5ea6fa88", "embedding": null, "metadata": {"filename": "a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.md", "extension": ".md", "title": "A New Document Summary Index for LLM-powered QA Systems", "date": "May 8, 2023", "url": "https://www.llamaindex.ai/blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "912df07d-6a63-4391-95b4-01eec2fb54f7", "node_type": "4", "metadata": {"filename": "a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.md", "extension": ".md", "title": "A New Document Summary Index for LLM-powered QA Systems", "date": "May 8, 2023", "url": "https://www.llamaindex.ai/blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec"}, "hash": "c13b9e5b53fa64063ee68bbae6febc9982992cf8a33a184c99279a81ee8b9f07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b12870dc-ad6a-4439-b844-def01c4f956d", "node_type": "1", "metadata": {}, "hash": "dd375a2fcb081cc1d7af56c9f75e5118c6e84b2bdef5cdc11e9987f5c4262af2", "class_name": "RelatedNodeInfo"}}, "text": "In this blog post, we introduce a brand new LlamaIndex data structure: a\nDocument Summary Index. We describe how it can help offer better retrieval\nperformance compared to traditional semantic search, and also walk through an\nexample.\n\n#  Background\n\nOne of the core use cases of Large Language Models (LLMs) is question-\nanswering over your own data. To do this, we pair the LLM with a \u201cretrieval\u201d\nmodel that can perform information retrieval over a knowledge corpus, and\nperform response synthesis over the retrieved texts using the LLM. This\noverall framework is called Retrieval-Augmented Generation.\n\nMost users building LLM-powered QA systems today tend to do some form of the\nfollowing:\n\n  1. Take source documents, split each one into text chunks \n  2. Store text chunks in a vector db \n  3. During query-time, retrieve text chunks by embedding similarity and/or keyword filters. \n  4. Perform response synthesis \n\nFor a variety of reasons, this approach provides limited retrieval\nperformance.\n\n##  Limitations of Existing Approaches\n\nThere are a few limitations of embedding retrieval using text chunks.\n\n  * **Text chunks lack global context.** Oftentimes the question requires context beyond what is indexed in a specific chunk. \n  * **Careful tuning of top-k / similarity score thresholds.** Make the value too small and you\u2019ll miss context. Make the value too big and cost/latency might increase with more irrelevant context. \n  * **Embeddings don\u2019t always select the most relevant context for a question.** Embeddings are inherently determined separately between text and the context. \n\nAdding keyword filters are one way to enhance the retrieval results. But that\ncomes with its own set of challenges. We would need to adequately determine\nthe proper keywords for each document, either manually or through an NLP\nkeyword extraction/topic tagging model. Also we would need to adequately infer\nthe proper keywords from the query.\n\n#  Document Summary Index\n\nA diagram for the Document Summary Index\n\nWe propose a new index in [ LlamaIndex\n](https://github.com/jerryjliu/llama_index) that will extract/index an\n**unstructured text summary for each document** . This index can help enhance\nretrieval performance beyond existing retrieval approaches. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2262, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b12870dc-ad6a-4439-b844-def01c4f956d": {"__data__": {"id_": "b12870dc-ad6a-4439-b844-def01c4f956d", "embedding": null, "metadata": {"filename": "a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.md", "extension": ".md", "title": "A New Document Summary Index for LLM-powered QA Systems", "date": "May 8, 2023", "url": "https://www.llamaindex.ai/blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "912df07d-6a63-4391-95b4-01eec2fb54f7", "node_type": "4", "metadata": {"filename": "a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.md", "extension": ".md", "title": "A New Document Summary Index for LLM-powered QA Systems", "date": "May 8, 2023", "url": "https://www.llamaindex.ai/blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec"}, "hash": "c13b9e5b53fa64063ee68bbae6febc9982992cf8a33a184c99279a81ee8b9f07", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "349fbca7-4d4b-460c-9132-02ac5ea6fa88", "node_type": "1", "metadata": {"filename": "a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.md", "extension": ".md", "title": "A New Document Summary Index for LLM-powered QA Systems", "date": "May 8, 2023", "url": "https://www.llamaindex.ai/blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec"}, "hash": "cb5d939aad75aea4e364bcfb70c075c2e97455a69c301f215b711dda8063e344", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b97ba75-cde5-4162-8086-40ba2978feca", "node_type": "1", "metadata": {}, "hash": "856448353cd84585ac0f1e3272e3768bc3705bd1a0e163d19bb388f23b4ae8e7", "class_name": "RelatedNodeInfo"}}, "text": "It helps to index\nmore information than a single text chunk, and carries more semantic meaning\nthan keyword tags. It also allows for a more flexible form of retrieval: we\ncan do both LLM retrieval and embedding-based retrieval.\n\n##  How It Works\n\nDuring build-time, we ingest each document, and use a LLM to extract a summary\nfrom each document. We also split the document up into text chunks (nodes).\nBoth the summary and the nodes are stored within our [ Document Store\n](https://gpt-index.readthedocs.io/en/latest/how_to/storage.html) abstraction.\nWe maintain a mapping from the summary to the source document/nodes.\n\nDuring query-time, we retrieve relevant documents to the query based on their\nsummaries, using the following approaches:\n\n  * **LLM-based Retrieval:** We present sets of document summaries to the LLM, and ask the LLM to determine which documents are relevant + their relevance score. \n  * **Embedding-based Retrieval:** We retrieve relevant documents based on summary embedding similarity (with a top-k cutoff). \n\nNote that this approach of retrieval for document summaries (even with the\nembedding-based approach) is different than embedding-based retrieval over\ntext chunks. The retrieval classes for the document summary index retrieve\n**all nodes** for any selected document, instead of returning relevant chunks\nat the node-level.\n\nStoring summaries for a document also enables **LLM-based retrieval** .\nInstead of feeding the entire document to the LLM in the beginning, we can\nfirst have the LLM inspect the concise document summary to see if it\u2019s\nrelevant to the query at all. This leverages the reasoning capabilities of\nLLM\u2019s which are more advanced than embedding-based lookup, but avoids the\ncost/latency of feeding the entire document to the LLM\n\n##  Additional Insights\n\nDocument retrieval with summaries can be thought of as a \u201cmiddle ground\u201d\nbetween semantic search and brute-force summarization across all docs. We look\nup documents based on summary relevance with the given query, and then return\nall *nodes* corresponding to the retrieved docs.\n\nWhy should we do this? This retrieval method gives user more context than\ntop-k over a text-chunk, by retrieving context at a document-level. But, it\u2019s\nalso a more flexible/automatic approach than topic modeling; no more worrying\nabout whether your text has the right keyword tags!\n\n#  Example\n\nLet\u2019s walk through an example that showcases the document summary index, over\nWikipedia articles about different cities.\n\nThe rest of this guide showcases the relevant code snippets. You can find the\n[ full walkthrough here ](https://gpt-\nindex.readthedocs.io/en/latest/examples/index_structs/doc_summary/DocSummary.html)\n(and here\u2019s the [ notebook link\n](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/index_structs/doc_summary/DocSummary.ipynb)\n).\n\nWe can build the ` GPTDocumentSummaryIndex ` over a set of documents, and pass\nin a ` ResponseSynthesizer ` object to synthesize summaries for the documents.\n\n    \n    \n    from llama_index import (\n        SimpleDirectoryReader,\n        LLMPredictor,\n        ServiceContext,\n        ResponseSynthesizer\n    )\n    from llama_index.indices.document_summary import GPTDocumentSummaryIndex\n    from langchain.chat_models import ChatOpenAI\n    \n    # load docs, define service context\n    ...\n    \n    # build the index\n    response_synthesizer = ResponseSynthesizer.from_args(response_mode=\"tree_summarize\", use_async=True)\n    doc_summary_index = GPTDocumentSummaryIndex.from_documents(\n        city_docs, \n        service_context=service_context,\n        response_synthesizer=response_synthesizer\n    )\n\nOnce the index is built, we can get the summary for any given document:\n\n    \n    \n    summary = doc_summary_index.get_document_summary(\"Boston\")\n\nNext, let\u2019s walk through an example LLM-based retrieval over the index.\n\n    \n    \n    from llama_index.indices.document_summary import DocumentSummaryIndexRetriever\n    \n    retriever = DocumentSummaryIndexRetriever(\n        doc_summary_index,\n        # choice_select_prompt=choice_select_prompt,\n        # choice_batch_size=choice_batch_size,\n        # format_node_batch_fn=format_node_batch_fn,\n        # parse_choice_select_answer_fn=parse_choice_select_answer_fn,\n        # service_context=service_context\n    )\n    retrieved_nodes = retriever.retrieve(\"What are the sports teams in Toronto?\")\n    print(retrieved_nodes[0].score)\n    print(retrieved_nodes[0].node.get_text())The retriever will retrieve a set of relevant nodes for a given index.\n\nNote that the LLM returns relevance scores in addition to the document text:\n\n    \n    \n    8.0\n    Toronto ( (listen) t\u0259-RON-toh; locally [t\u0259\u02c8\u0279\u0252\u027e\u0303\u0259] or [\u02c8t\u0279\u0252\u027e\u0303\u0259]) is the capital city of the Canadian province of Ontario. With a recorded population of 2,794,356 in 2021, it is the most populous city in Canada...\n\nWe can also use the index as part of an overall query engine, to not only\nretrieve the relevant context, but also synthesize a response to a given\nquestion. We can do this through both the high-level API as well as lower-\nlevel API.\n\n", "mimetype": "text/plain", "start_char_idx": 2262, "end_char_idx": 7361, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b97ba75-cde5-4162-8086-40ba2978feca": {"__data__": {"id_": "3b97ba75-cde5-4162-8086-40ba2978feca", "embedding": null, "metadata": {"filename": "a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.md", "extension": ".md", "title": "A New Document Summary Index for LLM-powered QA Systems", "date": "May 8, 2023", "url": "https://www.llamaindex.ai/blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "912df07d-6a63-4391-95b4-01eec2fb54f7", "node_type": "4", "metadata": {"filename": "a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.md", "extension": ".md", "title": "A New Document Summary Index for LLM-powered QA Systems", "date": "May 8, 2023", "url": "https://www.llamaindex.ai/blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec"}, "hash": "c13b9e5b53fa64063ee68bbae6febc9982992cf8a33a184c99279a81ee8b9f07", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b12870dc-ad6a-4439-b844-def01c4f956d", "node_type": "1", "metadata": {"filename": "a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.md", "extension": ".md", "title": "A New Document Summary Index for LLM-powered QA Systems", "date": "May 8, 2023", "url": "https://www.llamaindex.ai/blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec"}, "hash": "11b28dc375bd604f7d76a0bed01ad99f026389b418edf124a77c7875788ae5cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "807e1318-818c-4b17-a1ae-45be84889df2", "node_type": "1", "metadata": {}, "hash": "d1132a7a0cd9f21a1ca6dda74e49651aaed7f729b3fe75d20074f8ca2f253307", "class_name": "RelatedNodeInfo"}}, "text": "**High-level API**\n\n    \n    \n    query_engine = doc_summary_index.as_query_engine(\n      response_mode=\"tree_summarize\", use_async=True\n    )\n    response = query_engine.query(\"What are the sports teams in Toronto?\")\n    print(response)\n\n**Lower-level API**\n\n    \n    \n    # use retriever as part of a query engine\n    from llama_index.query_engine import RetrieverQueryEngine\n    \n    # configure response synthesizer\n    response_synthesizer = ResponseSynthesizer.from_args()\n    \n    # assemble query engine\n    query_engine = RetrieverQueryEngine(\n        retriever=retriever,\n        response_synthesizer=response_synthesizer,\n    )\n    \n    # query\n    response = query_engine.query(\"What are the sports teams in Toronto?\")\n    print(response)\n\n#  **Next Steps**\n\nThe approach of autosummarization over any piece of text is really exciting.\nWe\u2019re excited to develop extensions in two areas:\n\n  * Continue exploring autosummarization in different layers. Currently it\u2019s at the doc-level, but what about summarizing a big text chunk into a smaller one? (e.g. ", "mimetype": "text/plain", "start_char_idx": 7361, "end_char_idx": 8425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "807e1318-818c-4b17-a1ae-45be84889df2": {"__data__": {"id_": "807e1318-818c-4b17-a1ae-45be84889df2", "embedding": null, "metadata": {"filename": "a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.md", "extension": ".md", "title": "A New Document Summary Index for LLM-powered QA Systems", "date": "May 8, 2023", "url": "https://www.llamaindex.ai/blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "912df07d-6a63-4391-95b4-01eec2fb54f7", "node_type": "4", "metadata": {"filename": "a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.md", "extension": ".md", "title": "A New Document Summary Index for LLM-powered QA Systems", "date": "May 8, 2023", "url": "https://www.llamaindex.ai/blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec"}, "hash": "c13b9e5b53fa64063ee68bbae6febc9982992cf8a33a184c99279a81ee8b9f07", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b97ba75-cde5-4162-8086-40ba2978feca", "node_type": "1", "metadata": {"filename": "a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.md", "extension": ".md", "title": "A New Document Summary Index for LLM-powered QA Systems", "date": "May 8, 2023", "url": "https://www.llamaindex.ai/blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec"}, "hash": "4204c790ef4dcc6eb2fa3cbb9955a0ce9c1f8ea3e1f8ac13edaa79bfa436311d", "class_name": "RelatedNodeInfo"}}, "text": "a one-liner). \n  * Continue exploring LLM-based retrieval, which summarization helps to unlock. \n\nAlso we\u2019re sharing the example guide/notebook below in case you missed it\nabove:\n\n[ Document Summary Guide ](https://gpt-\nindex.readthedocs.io/en/latest/examples/index_structs/doc_summary/DocSummary.html)\n\n[ Notebook Link\n](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/index_structs/doc_summary/DocSummary.ipynb)\n\n", "mimetype": "text/plain", "start_char_idx": 8425, "end_char_idx": 8857, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d38d5355-2cdc-42b5-be74-15c561dddfbd": {"__data__": {"id_": "d38d5355-2cdc-42b5-be74-15c561dddfbd", "embedding": null, "metadata": {"filename": "building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.md", "extension": ".md", "title": "Building and Evaluating a QA System with LlamaIndex", "date": "May 7, 2023", "url": "https://www.llamaindex.ai/blog/building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86c77f5d-1c44-44fe-a276-ab681d141894", "node_type": "4", "metadata": {"filename": "building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.md", "extension": ".md", "title": "Building and Evaluating a QA System with LlamaIndex", "date": "May 7, 2023", "url": "https://www.llamaindex.ai/blog/building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1"}, "hash": "a29d03441466ea20013e11aeec73c50e734a7f8c3b9eea3061b778ada215ba31", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cfd90d07-f32d-4963-97d8-768828ad927d", "node_type": "1", "metadata": {}, "hash": "fc8ad0f6393e9099077e6192bdf41771050aabd4d61a2e8f1d7a21be350dc8ac", "class_name": "RelatedNodeInfo"}}, "text": "#  **Introduction**\n\n[ LlamaIndex (GPT Index) ](https://github.com/jerryjliu/llama_index) offers an\ninterface to connect your Large Language Models (LLMs) with external data.\nLlamaIndex provides various data structures to index your data, such as the\nlist index, vector index, keyword index, and tree index. It offers both a\nhigh-level API and low-level API \u2014 the high-level API allows you to build a\nQuestion-Answering (QA) system in just five lines of code, whereas the lower-\nlevel API allows you to customize various aspects of retrieval and synthesis.\n\nHowever, taking these systems into production requires careful evaluation of\nthe performance of the overall system \u2014 the quality of the outputs given the\ninputs. Evaluation of retrieval-augmented generation can be challenging\nbecause the user would need to come up with a dataset of relevant questions\nfor a given context. To overcome these obstacles, LlamaIndex provides Question\nGeneration and label-free Evaluation modules.\n\nIn this blog, we will discuss the three-step evaluation process using Question\nGeneration and Evaluation modules:\n\n  1. Question Generation from the document \n  2. Generate answers/source nodes for questions using LlamaIndex QueryEngine abstractions, which manage the interaction between the LLM and data indices. \n  3. Evaluate if the question (query), answer, and source nodes are matching/inline \n\n#  **1\\. Question Generation**\n\nIt should be noted that this approach does not require ground-truth labels.\nThe purpose of question generation is to generate an initial dataset of inputs\nover context that can be used to evaluate the question-answering system.\n\nLlamaIndex offers the DataGenerator class, which generates questions from a\ngiven document using ListIndex. By default, it uses OpenAI ChatGPT\n(get-3.5-turbo) for question generation.\n\n    \n    \n    from llama_index.evaluation import DatasetGenerator\n    from llama_index import SimpleDirectoryReader\n    \n    # Load documents\n    reader = SimpleDirectoryReader(\"./data\")\n    documents = reader.load_data()\n    \n    # Generate Question\n    data_generator = DatasetGenerator.from_documents(documents)\n    question = data_generator.generate_questions_from_nodes()\n\n#  **2\\. Generate Answers/Source Nodes (Context)**\n\nUsing List Index, we generate answers and source nodes for the generated\nquestions in the response object.\n\n    \n    \n    from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, load_index_from_storage, StorageContext\n    \n    # load documents\n    documents = SimpleDirectoryReader('./data').load_data()\n    \n    # Create Index\n    index = GPTVectorStoreIndex.from_documents(documents)\n    \n    # save index to disk\n    index.set_index_id(\"vector_index\")\n    index.storage_context.persist('storage')\n    \n    # rebuild storage context\n    storage_context = StorageContext.from_defaults(persist_dir='storage')\n    # load index\n    index = load_index_from_storage(storage_context, index_id=\"vector_index\")\n    \n    # Query the index\n    query_engine = index.as_query_engine(similarity_top_k=3)\n    response = query_engine.query(&lt;Query&gt;)\n    \n    # Response object has both response and source nodes.\n\n#  **3\\. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3189, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cfd90d07-f32d-4963-97d8-768828ad927d": {"__data__": {"id_": "cfd90d07-f32d-4963-97d8-768828ad927d", "embedding": null, "metadata": {"filename": "building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.md", "extension": ".md", "title": "Building and Evaluating a QA System with LlamaIndex", "date": "May 7, 2023", "url": "https://www.llamaindex.ai/blog/building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86c77f5d-1c44-44fe-a276-ab681d141894", "node_type": "4", "metadata": {"filename": "building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.md", "extension": ".md", "title": "Building and Evaluating a QA System with LlamaIndex", "date": "May 7, 2023", "url": "https://www.llamaindex.ai/blog/building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1"}, "hash": "a29d03441466ea20013e11aeec73c50e734a7f8c3b9eea3061b778ada215ba31", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d38d5355-2cdc-42b5-be74-15c561dddfbd", "node_type": "1", "metadata": {"filename": "building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.md", "extension": ".md", "title": "Building and Evaluating a QA System with LlamaIndex", "date": "May 7, 2023", "url": "https://www.llamaindex.ai/blog/building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1"}, "hash": "292f6d5adcddf29ebf27b303c266913de5a0b707ee1f26620dbf5d768bc68c56", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f885f21a-8aa1-4987-a3da-f0480e2d32d5", "node_type": "1", "metadata": {}, "hash": "1c5477df14dda2b9b0a47371370d4a62a7bbc3a6b1e56a30447fd3051c53f9de", "class_name": "RelatedNodeInfo"}}, "text": "Evaluation**\n\nThe evaluation module can be used to answer the following three questions:\n\n  1. Are the response generated and source nodes (context) matching? \u2014 Response + Source Nodes (Context) \n  2. Are response generated, source nodes (context), and query matching? \u2014 Query + Response + Source Nodes (Context) \n  3. Which source nodes of the retrieved source nodes are used to generate a response? \u2014 Query + Response + Individual Source Nodes (Context) \n\nEvaluation can be done with some combination of the query, context, and\nresponse, combining these with LLM calls.\n\n##  **Response + Source Nodes (Context)**\n\nThis function answers the question: Are the response generated and source\nnodes (context) matching?\n\nThe response object for a given query returns both the response and source\nnodes (context) with which it generated the response. We can now evaluate the\nresponse against the retrieved sources \u2014 without taking into account the\nquery! This allows you to measure hallucination \u2014 if the response does not\nmatch the retrieved sources, this means that the model may be \u201challucinating\u201d\nan answer since it is not rooting the answer in the context provided to it in\nthe prompt.\n\nThe result is a binary response \u2014 either \u201cYES/NO\u201d.\n\n  * YES \u2014 Response and Source Nodes (Context) are matching. \n  ", "mimetype": "text/plain", "start_char_idx": 3189, "end_char_idx": 4491, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f885f21a-8aa1-4987-a3da-f0480e2d32d5": {"__data__": {"id_": "f885f21a-8aa1-4987-a3da-f0480e2d32d5", "embedding": null, "metadata": {"filename": "building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.md", "extension": ".md", "title": "Building and Evaluating a QA System with LlamaIndex", "date": "May 7, 2023", "url": "https://www.llamaindex.ai/blog/building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86c77f5d-1c44-44fe-a276-ab681d141894", "node_type": "4", "metadata": {"filename": "building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.md", "extension": ".md", "title": "Building and Evaluating a QA System with LlamaIndex", "date": "May 7, 2023", "url": "https://www.llamaindex.ai/blog/building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1"}, "hash": "a29d03441466ea20013e11aeec73c50e734a7f8c3b9eea3061b778ada215ba31", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cfd90d07-f32d-4963-97d8-768828ad927d", "node_type": "1", "metadata": {"filename": "building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.md", "extension": ".md", "title": "Building and Evaluating a QA System with LlamaIndex", "date": "May 7, 2023", "url": "https://www.llamaindex.ai/blog/building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1"}, "hash": "a28ea83b62d23d2527b9eab02714afc6e89713795fd53a0102b72968b85becda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70bc463c-ccbf-414a-a1d3-eeeccd876b8a", "node_type": "1", "metadata": {}, "hash": "a598d4fb023f96c6b88f62002ce1ff748940640ce4dedde7586d8d90f9679359", "class_name": "RelatedNodeInfo"}}, "text": "* NO \u2014 Response and Source Nodes (Context) are not matching. \n\n    \n    \n    from llama_index.evaluation import ResponseEvaluator\n    \n    # build service context\n    llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"))\n    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n    \n    # Build index and get response object\n    ...\n    \n    # define evaluator\n    evaluator = ResponseEvaluator(service_context=service_context)\n    \n    # evaluate using the response object\n    eval_result = evaluator.evaluate(response)\n\n##  **Query + Response + Source Nodes (Context)**\n\nThis function answers the question: Are response generated, source nodes\n(context), and query matching?\n\nOften with the \u201cResponse + Source Nodes (Context)\u201d approach, the response\ngenerated is in line with the source nodes but may not be the answer to the\nquery. Therefore, considering the query along with the response and source\nnodes is a good approach for a more accurate analysis.\n\nThe goal is to determine if the response + source context answers the query.\nThe result is a binary response \u2014 either \u201cYES/NO\u201d.\n\n  * YES \u2014 Query, Response, and Source Nodes (Context) are matching. \n  * NO \u2014 Query, Response, and Source Nodes (Context) are not matching. \n\n    \n    \n    from llama_index.evaluation import QueryResponseEvaluator\n    \n    # build service context\n    llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"))\n    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n    \n    # Build index and get response object\n    ...\n    \n    # define evaluator\n    evaluator = QueryResponseEvaluator(service_context=service_context)\n    \n    # evaluate using the response object\n    eval_result = evaluator.evaluate(query, response)\n\n##  **Query + Response + Individual Source Nodes (Context)**\n\nThis function answers the question: Which source nodes of the retrieved source\nnodes are used to generate a response?\n\nOften in the real world, the source nodes can be nodes from different\ndocuments. ", "mimetype": "text/plain", "start_char_idx": 4491, "end_char_idx": 6557, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "70bc463c-ccbf-414a-a1d3-eeeccd876b8a": {"__data__": {"id_": "70bc463c-ccbf-414a-a1d3-eeeccd876b8a", "embedding": null, "metadata": {"filename": "building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.md", "extension": ".md", "title": "Building and Evaluating a QA System with LlamaIndex", "date": "May 7, 2023", "url": "https://www.llamaindex.ai/blog/building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86c77f5d-1c44-44fe-a276-ab681d141894", "node_type": "4", "metadata": {"filename": "building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.md", "extension": ".md", "title": "Building and Evaluating a QA System with LlamaIndex", "date": "May 7, 2023", "url": "https://www.llamaindex.ai/blog/building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1"}, "hash": "a29d03441466ea20013e11aeec73c50e734a7f8c3b9eea3061b778ada215ba31", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f885f21a-8aa1-4987-a3da-f0480e2d32d5", "node_type": "1", "metadata": {"filename": "building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.md", "extension": ".md", "title": "Building and Evaluating a QA System with LlamaIndex", "date": "May 7, 2023", "url": "https://www.llamaindex.ai/blog/building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1"}, "hash": "fad7aaa1c720f46c67055481ec4ca272aac1e4a502a1953a7caba0d2175fd610", "class_name": "RelatedNodeInfo"}}, "text": "In these cases, it\u2019s important to understand which source nodes are\nrelevant and show those documents to the users. This mode of evaluation will\nlook at each source node and see if each source node contains an answer to the\nquery.\n\n    \n    \n    from llama_index.evaluation import QueryResponseEvaluator\n    \n    # build service context\n    llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"))\n    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n    \n    # build index and get response object \n    ...\n    \n    # define evaluator\n    evaluator = QueryResponseEvaluator(service_context=service_context)\n    \n    # evaluate using the response object\n    eval_result = evaluator.evaluate_source_nodes(response)\n\nGoogle Colab notebook for Evaluating QA systems using LlamaIndex \u2014\n\n## [ Google Colaboratory  Evaluating QA systems using LlamaIndex\n](https://colab.research.google.com/drive/1J7ZaTx746T9Xaglr-9PhdB5knnHs25ws?usp=sharing&source=post_page\n-----3f02e9d87ce1--------------------------------)\n\n#  **Conclusion**\n\nLlamaIndex provides a comprehensive solution for building and evaluating QA\nsystems without the need for ground-truth labels. By using the Question\nGeneration and Evaluation modules, you can ensure that your system is accurate\nand reliable, making it suitable for production environments.\n\n", "mimetype": "text/plain", "start_char_idx": 6557, "end_char_idx": 7920, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"a677fa02-6d9d-42e7-a2db-b9065adb193a": {"doc_hash": "bcc2a0d556c56229f19b10a2544d8ee8db6ab08e035c6405c1b782defa0275ee", "ref_doc_id": "79eb28d7-e2ec-4994-9d3b-20d8e9c18fc5"}, "aa39e8a5-b224-47bc-93ce-583a9dd22c98": {"doc_hash": "559301b042a9116dd24bc0cd2fc231b7b62570faa37f24860993a609ae736bd2", "ref_doc_id": "79eb28d7-e2ec-4994-9d3b-20d8e9c18fc5"}, "4e039c69-ca59-404a-b1e8-77af83825991": {"doc_hash": "c1499c7306512206d79d197bb0bfb72574ddd27693ff8c83ef376277d2f9e3c8", "ref_doc_id": "79eb28d7-e2ec-4994-9d3b-20d8e9c18fc5"}, "9b26c34d-54af-4530-9762-9bbea0b23def": {"doc_hash": "59a2976635cf0679d7577515ec647c33bb456c9cb2c0dc271a85962d73487967", "ref_doc_id": "0d9fb0a3-eb34-4516-992d-0ca9644c6eb5"}, "4db6b442-65b8-46b5-b631-69489f0eb0d9": {"doc_hash": "267d20159b19f0eadd4846631cfe57087b335d5f48f91a4bd8ca7333bf45daa7", "ref_doc_id": "0d9fb0a3-eb34-4516-992d-0ca9644c6eb5"}, "2999d86c-b90c-43df-821d-1814e7ff0b48": {"doc_hash": "40713d665aba8d333f39167d73b8f140aa8c04976545ff87a05cec418c56a070", "ref_doc_id": "0d9fb0a3-eb34-4516-992d-0ca9644c6eb5"}, "b3eb2234-5067-4718-a77d-01323fa73868": {"doc_hash": "2d0b0e089fe63e04fc119d90433f1ffd61d27d7620b476ced8f9af9926f01dbc", "ref_doc_id": "0d9fb0a3-eb34-4516-992d-0ca9644c6eb5"}, "d5f5f812-a8b0-40b3-8f51-66b2c49f53b4": {"doc_hash": "d38a432a2843293bd45246dfd1fecc4c1bb3d95fec44f7e2b8725189a54a4d6b", "ref_doc_id": "6e83ca1f-180e-4e5d-b417-df488eda2f0f"}, "dffdd78b-ca74-4469-a6e2-22902100b64b": {"doc_hash": "7e05d4ca14adc7f10960dcacab807ef76ebce9b9191e8563f70d5b246e614264", "ref_doc_id": "6e83ca1f-180e-4e5d-b417-df488eda2f0f"}, "35f3b3de-27fd-4c37-96cc-c5ff7a053302": {"doc_hash": "8b0ec8c597f4e0df7b2f481fa54ab76665944f1153b8f39d407799c6d449b776", "ref_doc_id": "6e83ca1f-180e-4e5d-b417-df488eda2f0f"}, "ea2a5822-ed60-4d58-9781-b44c2a990ac4": {"doc_hash": "a615d79559e952f942ce3726d6ed2958ab085e0b215a2119c95ba5a50ef5da22", "ref_doc_id": "62dafea5-0c5a-49ee-93da-d159adb5c9ce"}, "f6c514cb-bfff-4e25-bbac-19d0f27c3310": {"doc_hash": "8c11c1e67572244e7a3c81952aac0cf2f54af0c5cfadb244ff034bd944b1f999", "ref_doc_id": "62dafea5-0c5a-49ee-93da-d159adb5c9ce"}, "b67b4ca6-4385-4c60-8610-ca8bdcdf71ca": {"doc_hash": "6ac6c6f92bd43617a78eafd4b9f90d5f1fed23f93824ca19766aae22e7b74bcf", "ref_doc_id": "62dafea5-0c5a-49ee-93da-d159adb5c9ce"}, "59245926-175b-4550-9356-21d4787a5282": {"doc_hash": "564f3e1065735df79e05f9b433af4a97be598558c0365dcfc897a30fe3d4da2a", "ref_doc_id": "6692003a-9368-461c-84d3-d7bd2c425d07"}, "cca9a457-90ff-4392-b8cb-1d835d8018fb": {"doc_hash": "44a9f1e8a25260d5ad8a07f1cb0f29d52db042a5dd949dd45771e134b4ae0249", "ref_doc_id": "6692003a-9368-461c-84d3-d7bd2c425d07"}, "f958558b-14a0-43a7-b64b-becb1a547737": {"doc_hash": "a697a81384fc189bdf1f2bc1f6352472e9737f674cf3c13bb773c68e84a768b0", "ref_doc_id": "6692003a-9368-461c-84d3-d7bd2c425d07"}, "043f5478-70af-4c13-94dd-f8f170d7f626": {"doc_hash": "708a2f6a8ac7b9bece57b45a42a7f88bce2941e60db421bf7bd5befc9e9565f7", "ref_doc_id": "81f29a86-fa6c-4ffa-b1e9-fdfaab10fba1"}, "ee0e7d22-a211-4531-94e2-055e5692d80f": {"doc_hash": "087d05e1ea280f8ef4a8ee689826e10ed486a5b5238bd525c6d8f3921eec9a3c", "ref_doc_id": "81f29a86-fa6c-4ffa-b1e9-fdfaab10fba1"}, "434b8bbf-a0c8-404a-bce0-4c88cbddd802": {"doc_hash": "3096fe252d4662974a41763fcbac18568b6185d3e02e316a93739af7cc0f58aa", "ref_doc_id": "81f29a86-fa6c-4ffa-b1e9-fdfaab10fba1"}, "d8ec7259-830c-4ce7-beb7-faeeaea47e5e": {"doc_hash": "42c7e38a191560ab848316284502630f388806aa51a79a0391be2924741c4fea", "ref_doc_id": "888d9985-99d0-4ab5-9412-e40a25f14143"}, "5793b9b4-e936-4c8c-a986-4dcd5d8ef53d": {"doc_hash": "1350119c1d77260eceb347e84d5c365e84f2807015765d081dc5c776dc79fd39", "ref_doc_id": "888d9985-99d0-4ab5-9412-e40a25f14143"}, "0d4bf7ae-fcea-47d7-b709-bd3bec637ca5": {"doc_hash": "4ca421bc13e241805e4ce37b45c15a8727b6fc97ea7cf3d47b3f023f823d3e2b", "ref_doc_id": "888d9985-99d0-4ab5-9412-e40a25f14143"}, "3260b5a2-dace-438e-8877-ec7f3fc5c445": {"doc_hash": "60ec01afa82cfaef06921e4aa7189f8343322c96241529ffe5c497702b4fafcd", "ref_doc_id": "888d9985-99d0-4ab5-9412-e40a25f14143"}, "2798414e-f31e-44a2-86de-03188775da06": {"doc_hash": "bb08b0a83119819de51c3baeedc885d77fc71ccdcd160b54e5cecbd49edbc48c", "ref_doc_id": "033242bb-7f8d-4a19-a628-ab5236c05991"}, "947cb5e3-20f3-42c5-b5fa-4283600e5ba9": {"doc_hash": "8147e0df268bca1d4a0f2e25f523ec119ee6e64dfd361082383247273c008810", "ref_doc_id": "033242bb-7f8d-4a19-a628-ab5236c05991"}, "b01e2db8-c6b6-40fa-96c6-aff5385ab659": {"doc_hash": "d39a39bbcdf8660474c8a8466e6e1e4f114b7147cb62befcea208b86e848b282", "ref_doc_id": "033242bb-7f8d-4a19-a628-ab5236c05991"}, "20c6f958-2273-459f-98e9-59241e26ca72": {"doc_hash": "a65043886d52209c38c5a26aa7b54c0662d5c3b3ed58842a350ebad4c769693b", "ref_doc_id": "033242bb-7f8d-4a19-a628-ab5236c05991"}, "a546027d-ea6b-45e8-863d-f1dac57f8702": {"doc_hash": "97adad11bfb77d9259f64586f6809b1465617723d6640d6379c03ff3706d4795", "ref_doc_id": "033242bb-7f8d-4a19-a628-ab5236c05991"}, "dca747ba-640f-4e21-87b1-5cf6e7d83654": {"doc_hash": "192f8a322726bf930a310e0590287dc071beb903ba5500c252a53403bc237b2d", "ref_doc_id": "033242bb-7f8d-4a19-a628-ab5236c05991"}, "c00c7960-382f-4d06-832c-f9b9c35391a1": {"doc_hash": "07d24b4d9af3a6be4bebdff5936c45fbe775820e94ae3e1913e4087d095e3324", "ref_doc_id": "033242bb-7f8d-4a19-a628-ab5236c05991"}, "3bb32e4f-f76a-4d26-823a-366091ae03e6": {"doc_hash": "08e47690b48de8169a288da5a54469dac8a018125101c1e47fcb457ea66b41fc", "ref_doc_id": "645df4f8-7d44-4511-a5c0-8c55474e99b4"}, "9a3136c8-0786-462e-9ba2-feb87d836589": {"doc_hash": "005beaa0fe0e185f9bf61ab6a3bb9a7c9020d01229f714d339186e0ed8189da8", "ref_doc_id": "645df4f8-7d44-4511-a5c0-8c55474e99b4"}, "fdf5cead-ba49-4d5d-83eb-3854d4234766": {"doc_hash": "76ed9f356feda8cd554bf4e71a784af7bcd35809552e5c026b4715e2829a59a1", "ref_doc_id": "1b0c2b01-a080-44e0-9dc3-e783628d2eee"}, "5d387f68-cc83-4a2a-b2a0-e73cb221e64f": {"doc_hash": "80bef5bdc95178d11a850a74584ba484f228d7bef988de8c99669bd66d89e708", "ref_doc_id": "1b0c2b01-a080-44e0-9dc3-e783628d2eee"}, "519117a3-c9ca-4cb2-9c3f-d711d84d20b3": {"doc_hash": "5508e80380a5de0a868dff42aca735f2652bba60ffb4d3c4ea567f393a4e0c7a", "ref_doc_id": "1b0c2b01-a080-44e0-9dc3-e783628d2eee"}, "02305182-6743-4f03-8342-d6461b02c6c5": {"doc_hash": "bfd7e8a497c465f4abb8f26a8d0105e9ee20c8cb0809cf66716bf1f2ada67adf", "ref_doc_id": "c26ea0e7-6cd9-4a86-bcff-9ff279215e36"}, "89dc47e1-3765-42a2-aa08-9c3ba27702c7": {"doc_hash": "3803a039cafaa259f49e7072c7cd9559d1bb63969e7ee1d1420c5ced62af93f0", "ref_doc_id": "c26ea0e7-6cd9-4a86-bcff-9ff279215e36"}, "8ef8ae00-5ed7-4536-a6d3-63683048e637": {"doc_hash": "c7ff91a0d02ad2476f1a51926d311e86f1a32726351271399559602e525dd75e", "ref_doc_id": "c26ea0e7-6cd9-4a86-bcff-9ff279215e36"}, "99d0d58e-5992-42d3-ad15-fb8a337ac590": {"doc_hash": "47a118ed84a9c2bf671af6794c563b0203402d63eb5f39e7cfd988e5f57b2017", "ref_doc_id": "c26ea0e7-6cd9-4a86-bcff-9ff279215e36"}, "404c7933-e8ce-48c4-9d44-d5e893380d1f": {"doc_hash": "c5fbb53b2f599959863ce2854ab718d77d70bafcc9c44871bf5088030e89ee89", "ref_doc_id": "c26ea0e7-6cd9-4a86-bcff-9ff279215e36"}, "78633de4-b8c4-47f8-b646-c3eb5d71e37a": {"doc_hash": "83bc6c50c731de94b29785d21ab2fb177231ff9310298f746a180c9b559e7d8d", "ref_doc_id": "c26ea0e7-6cd9-4a86-bcff-9ff279215e36"}, "c27d85e9-7b67-4b32-9e32-f6f031d1735b": {"doc_hash": "ab2c75a28f58b34a9c777cbe06b884fc744bb95af8460340c613313f7ed1f941", "ref_doc_id": "c745a3ea-f895-4b3f-aca6-71413beba7d6"}, "c505e468-4788-49f5-896a-c8f103c9ee30": {"doc_hash": "bc8772a4e19fe0df120b18ebe81876404e8e37587e2fffc68d1495a7e15cc5a6", "ref_doc_id": "c745a3ea-f895-4b3f-aca6-71413beba7d6"}, "c0151546-4171-4e92-b18b-70e28bd7c4eb": {"doc_hash": "e85267e6a202217bf08b76e29f3c64a651ef8f37770a34fabd05abc773451d4c", "ref_doc_id": "c745a3ea-f895-4b3f-aca6-71413beba7d6"}, "056e2fd5-2ef6-4f35-b05f-c955c1e653b2": {"doc_hash": "4c3802106f14ab7487f3327ae9e3e607e0f42e31d5437f677e6260e7507240bf", "ref_doc_id": "9f3cc26a-e7ce-4a76-9684-cbce575ad49b"}, "f65a9f90-b9e1-4a3c-a1fa-167210874251": {"doc_hash": "d799c50adefa81780a5269318262178ed8a7e35d04e55fb7da3ce1135c064e32", "ref_doc_id": "9f3cc26a-e7ce-4a76-9684-cbce575ad49b"}, "f52edd71-c7cd-4b8d-95c0-edadb1bff567": {"doc_hash": "ee6ed2f2bc4429370652f6de67e1fe55fb4b525b120eb57f37444287d75f0100", "ref_doc_id": "f3d8b800-430b-447e-8711-5a07466261ac"}, "218276ac-b3d6-40c2-8457-bca9bdbf321c": {"doc_hash": "fd438493268abb16f7c79a5559d496c722c5b2c66f7d38b21e10435c0478e1e6", "ref_doc_id": "f3d8b800-430b-447e-8711-5a07466261ac"}, "10e1a522-d29f-4053-9922-b16e5104465f": {"doc_hash": "5ebb0342f92d2ddd68f23ed82c1edcd1ad0bfb677482a614b406c743489ae738", "ref_doc_id": "f3d8b800-430b-447e-8711-5a07466261ac"}, "eb177c3e-fa44-4dd5-bc55-fa99ff7a7799": {"doc_hash": "2c60f72121145aa7fc25311840740b9f77848408af59dbbf9d44eb3235725ea6", "ref_doc_id": "70ccc5c6-8e72-4a65-a7b2-86bf59b26221"}, "4dfce439-df67-4e9b-ba22-a1512a12d200": {"doc_hash": "50d9689198a746823c2ab2d5157a3c203b843b34a0d6082bde404f48d2e075df", "ref_doc_id": "70ccc5c6-8e72-4a65-a7b2-86bf59b26221"}, "5b25908b-73ad-4503-90c1-74d98f67a3f4": {"doc_hash": "ad50876014d6bbbedd6e7c2bfea3a39a3ede8ad6085e869f8c217c16e588432d", "ref_doc_id": "70ccc5c6-8e72-4a65-a7b2-86bf59b26221"}, "c3b96c4b-e570-46bb-a032-da22dd9edc86": {"doc_hash": "c6022c3eb4eda12e814c85ef2a6e3cc26bba2bac59b8dc60f471fab93ee832ff", "ref_doc_id": "70ccc5c6-8e72-4a65-a7b2-86bf59b26221"}, "45a52a84-1325-405e-8b93-d8c99499409a": {"doc_hash": "954fb6b80501f518b81cd3e952f00a9f66860aa340cc2b01c0726362aa55038d", "ref_doc_id": "daf66bb2-a0d8-4f36-a6e5-40ecb0ad9745"}, "b3b07c3c-9974-4085-81b0-7c52c23e1343": {"doc_hash": "a35df7cbea436d041f935a05021e59f7449b57dad823c97024c2f6d12b29db91", "ref_doc_id": "daf66bb2-a0d8-4f36-a6e5-40ecb0ad9745"}, "33bf00bb-dc2b-4f24-9145-449e9a06f346": {"doc_hash": "1b9a05e67d1a0d9230e1e13459a898820763ca66893889fa914dfe8f4d24ba9b", "ref_doc_id": "daf66bb2-a0d8-4f36-a6e5-40ecb0ad9745"}, "917501e0-e0ed-4a8d-b7ea-34abf7ea6265": {"doc_hash": "e4ee66ee63f769a5667b06c11b72e9a27489720d6720bc89a65d87e8c2e907e6", "ref_doc_id": "daf66bb2-a0d8-4f36-a6e5-40ecb0ad9745"}, "fcd3e45d-e6f7-4965-840f-ddb8004dfebe": {"doc_hash": "6ba415d506aa49137d995780e8773d874eb5285617a2c44178942383c6a5dea7", "ref_doc_id": "71e4c768-21b6-44c5-9a87-672c07afb160"}, "6d08611c-16e1-4579-aa4f-8f55231f8cda": {"doc_hash": "4296f2b4fc60a9bdff83b150687d5053b3993eb0ca84b7e1e98c0c6d82d8a8fc", "ref_doc_id": "71e4c768-21b6-44c5-9a87-672c07afb160"}, "28cfa0ba-2816-42ab-bf07-a970a381109a": {"doc_hash": "15996f2a8077358e2615b35931c00b3eb032fa19a72e929e12ab3b99318b543e", "ref_doc_id": "71e4c768-21b6-44c5-9a87-672c07afb160"}, "bc7557e3-de9e-49fe-b1a3-68d2c128bc4e": {"doc_hash": "48ce886c3fe22d8170feb9b159256b56124b29c0b1ff6ce8cb626b958cd81758", "ref_doc_id": "b6700bfd-653b-4d5f-9db9-6f04c642990b"}, "f2e7561c-89f2-470b-9c3e-163a1014475d": {"doc_hash": "1f30327e5619b6d92f21a0413726a6b1b2aba296984c58317cc25bd710700b3b", "ref_doc_id": "b6700bfd-653b-4d5f-9db9-6f04c642990b"}, "18739f06-e810-4316-aa13-cb984bbd1ea9": {"doc_hash": "287df68bb68caac73797a906528c9306ad02125f9bd7a0181c129a77b2514c9a", "ref_doc_id": "b6700bfd-653b-4d5f-9db9-6f04c642990b"}, "c21b1199-0bff-4a84-8d9f-0a3d94e19682": {"doc_hash": "94bab304ffb40d0c6fc4b01344aaedc0afaaae7df3f5ee33bad424d7b935cd3b", "ref_doc_id": "b6700bfd-653b-4d5f-9db9-6f04c642990b"}, "9c9fc031-8fb4-4175-bba6-2676a382f25c": {"doc_hash": "3aff98b4f12f7e39a18bedace5083dabebfa75ea251d98a478b98dde1e2d2fed", "ref_doc_id": "8d27ef78-ba49-4b72-a6a0-e9c3a62c3b14"}, "740931d1-e134-453b-9517-bd3ad041aee7": {"doc_hash": "e15e13bd02606354c093d095b8799fec41a153d501b1fbb111a12e32b95489b3", "ref_doc_id": "8d27ef78-ba49-4b72-a6a0-e9c3a62c3b14"}, "97efe43a-7f79-4bd5-b561-95d363341bc4": {"doc_hash": "d245b6c86055a2b2bf7185883abb5af61ec21dd6f1f6eec4c26d0801bdddc5b0", "ref_doc_id": "8d27ef78-ba49-4b72-a6a0-e9c3a62c3b14"}, "2a3a43a8-6e12-4bcf-962a-3d7fc15f2f40": {"doc_hash": "a3dab09f839ec6f705afd616391e00d454dc9b1352f652545394470b67051c92", "ref_doc_id": "0a5d297c-1c31-4730-89e5-e0b02aa9ef1e"}, "0ca2d89d-db47-4bfd-b892-da19dded09da": {"doc_hash": "c5afb739e1783c8f5bea6735370e257153158ff844cc4341361e81bad491b738", "ref_doc_id": "0a5d297c-1c31-4730-89e5-e0b02aa9ef1e"}, "60457367-3313-4aee-9caa-5970b843fa5a": {"doc_hash": "aba171de7fd0901c50f3938f4d36102a516af8d2b6d9a29b48f9b0676a0eac9f", "ref_doc_id": "0a5d297c-1c31-4730-89e5-e0b02aa9ef1e"}, "edc33c91-72a3-491a-926c-f5ceb1ca4ca6": {"doc_hash": "7f9c1c0fbe8e7d518f662b65be3e9efc1c61463d101e0e7250d08803ba737095", "ref_doc_id": "55a84224-699c-4b12-9c4b-12367806480c"}, "bbd9d33d-6d02-462c-99e1-1bd7c177cab5": {"doc_hash": "9a233f143cb5d9150736ce0202b10ae24fe2ca0962e5be2788c9ef23669b1f03", "ref_doc_id": "55a84224-699c-4b12-9c4b-12367806480c"}, "1b99dd4b-5607-4a9c-a4a1-51a5307b0977": {"doc_hash": "971509748795ec2be0ee7ca1ae5a98243a67d705df5eb71c2d742d5a337cc2a9", "ref_doc_id": "55a84224-699c-4b12-9c4b-12367806480c"}, "451d84f2-5030-499f-b2e2-0a2761039850": {"doc_hash": "393ec61bda78d66fbda457fa9e8a5f22f3b00b969964075dd4f035c47e8006ad", "ref_doc_id": "55a84224-699c-4b12-9c4b-12367806480c"}, "8427a98a-8271-40c6-a476-4bb7c73b2576": {"doc_hash": "3c4fa5dbae0d0d05c32abe0fd7bb8b0975cd0b0b1e87594fcb5c8722dd0b3536", "ref_doc_id": "55a84224-699c-4b12-9c4b-12367806480c"}, "0197b0e5-d1e8-426d-8bb8-2dfe11693e3e": {"doc_hash": "223823e3e56e0e102e91cb617d60d638ea6b5599527537e2254cdf8356bc6aa8", "ref_doc_id": "55a84224-699c-4b12-9c4b-12367806480c"}, "fd99a727-efbb-473e-91a1-1d712d6dc12a": {"doc_hash": "025e35b7388cb995890b898c59024324c73f0a9528f0e77575c25e69365fd4e1", "ref_doc_id": "2337ae24-e176-4aca-95bb-15c5bc0857de"}, "25770b6f-adbf-4d62-b8fd-0efc198cf728": {"doc_hash": "431a9fc22d01507d863fd7466603475a96250503d4ed9d0d4301453f2d737568", "ref_doc_id": "2337ae24-e176-4aca-95bb-15c5bc0857de"}, "6a53f81d-7e22-4b61-b8ce-3c46c1b2e8b3": {"doc_hash": "f746ef9467a2162c4fa0ad1278bb129dd476e645d54f1da1ca36014920948f57", "ref_doc_id": "2337ae24-e176-4aca-95bb-15c5bc0857de"}, "6d3e257e-727f-4ccf-9633-dc8fa0284ea8": {"doc_hash": "3dd13b198f9cf2f906d49afbd4e4bdfee03fe3d58a67b7561aba85267abde987", "ref_doc_id": "8f6774da-c6df-4b05-86e6-d291f430d976"}, "cfde95e4-5a2e-4d6b-9653-b14131bd52a2": {"doc_hash": "fbcd12a5256e5866430477c2469612d4d81a72b2ea834e46f3b039f972c467e4", "ref_doc_id": "8f6774da-c6df-4b05-86e6-d291f430d976"}, "9f24f694-3817-41bf-a19a-5a9c44e58d6c": {"doc_hash": "a3283152e1bf268f211da147271097fdd7d2dfc92d6bee868c31cdaa74369e71", "ref_doc_id": "8f6774da-c6df-4b05-86e6-d291f430d976"}, "570cd9f5-8273-46a3-b34e-1f581f3c11c3": {"doc_hash": "c5b7b94b4d859ba9d7b3d1153e791a3defb1ae3e483c904d49583721244d148a", "ref_doc_id": "a80607e8-6bf5-42bf-9cbe-31a1388845ba"}, "153d06e5-e3cb-48f0-9544-c2640cfc237e": {"doc_hash": "dec74983cdb082ca206d9d06b677a0dff5060a86c1162ea8da67f325afe93a53", "ref_doc_id": "a80607e8-6bf5-42bf-9cbe-31a1388845ba"}, "c419a347-46eb-4183-a3d4-7be3c5ec2d81": {"doc_hash": "c8b42e85e453c95296a8df381ee7b403080b61965ede3d8814e74157ed0ca851", "ref_doc_id": "a80607e8-6bf5-42bf-9cbe-31a1388845ba"}, "4547f9cd-fc3a-4db5-864a-a62a63dcc668": {"doc_hash": "65fff80b8eaa673b76c8422fda2f6ef52382d5b8aabbd71afd0ab32b6d9e66b5", "ref_doc_id": "45edea59-d0cf-49c5-83a4-240649039ec3"}, "fdbe389e-850d-41d4-9212-c783cdd76265": {"doc_hash": "03466ac6d95d22408a9f7ff24fe223fae63456d8d634d5378baf815ec0852ce2", "ref_doc_id": "45edea59-d0cf-49c5-83a4-240649039ec3"}, "81ad8b41-396b-424c-8a2f-0f7a73df51a5": {"doc_hash": "45a232d033d90e4b01231812a35ec9c4acf12a4e62dd3fdac716e4fdb4216c0a", "ref_doc_id": "45edea59-d0cf-49c5-83a4-240649039ec3"}, "0d3ea23a-df0e-451d-a2d8-0f426b3b3bc8": {"doc_hash": "ef885341fcdaa6d2e91d15b9fc0f6cf6839259fdf85fa2e40b92cc8628f324c7", "ref_doc_id": "09ff30b1-4a55-41c9-8a24-0b49a7293cb1"}, "31722dad-a163-4722-baba-1bf8fe4db7f5": {"doc_hash": "8be3696819ef5e80b2276d575721a697efd24bde914611d87128227f87cc0d8f", "ref_doc_id": "09ff30b1-4a55-41c9-8a24-0b49a7293cb1"}, "b9700408-1613-40df-ab35-c1ed67472ee7": {"doc_hash": "674a156deb92e4403944aecaeca47781238820d2e1818147d415b2900a3c90d1", "ref_doc_id": "09ff30b1-4a55-41c9-8a24-0b49a7293cb1"}, "1912b081-3cca-42be-9db5-0230d93f0f47": {"doc_hash": "feeee1c8733329b8d7271432cec117c720c49bdb93414f5bebc1318985f07b9e", "ref_doc_id": "ee6537a5-76b4-467c-bdde-63d6310ac37c"}, "a2577cf2-3cc3-404a-923b-d6bb5d2be0f8": {"doc_hash": "a2a929247328e7aa59021badb76b4950e2bb1afa4580b9c9f8ab6460a6383164", "ref_doc_id": "ee6537a5-76b4-467c-bdde-63d6310ac37c"}, "0db569a2-bdf7-41f6-b7d1-0888824195d0": {"doc_hash": "d760ad7f2044771af5ed1a96a37c73b57ee4546fc326c7c259110e85349b5c07", "ref_doc_id": "ee6537a5-76b4-467c-bdde-63d6310ac37c"}, "61a60dcd-a8f6-4800-81c1-5b399378ece9": {"doc_hash": "83eee6c696a5e6f7fb334652558706110feacd789fb1019c710d2d0d9cef1575", "ref_doc_id": "bb446d01-a356-4334-995f-9c48f9fd774a"}, "f9df290b-a6e4-4837-97c6-e002dc05b0fa": {"doc_hash": "a2f78b8cb8c07293b148df3a25391c095db9dca50a4b7d4332712ad07ea6abfa", "ref_doc_id": "bb446d01-a356-4334-995f-9c48f9fd774a"}, "32ea91e0-0f1e-412b-9dec-f279408e985e": {"doc_hash": "35cab26d90bd5f70bd4544435aa056dde97779bac5cc7947fb9d1758be6859ba", "ref_doc_id": "bb446d01-a356-4334-995f-9c48f9fd774a"}, "eb3df1f1-5478-4f10-945b-2e064039911b": {"doc_hash": "4233322405de00d8a261eaaeeabfbf621ae3ce9269fcd7b1c8965fbc6381c118", "ref_doc_id": "bb446d01-a356-4334-995f-9c48f9fd774a"}, "c59f89b2-7d31-428a-9bfd-d0d421809981": {"doc_hash": "5b5186eb7d270d36bb02db6f7d29c84a817fd604ecf3d362052e9888ec2675b2", "ref_doc_id": "bb446d01-a356-4334-995f-9c48f9fd774a"}, "fd379a1d-3770-4ef7-a1fe-dba9fdd1f28c": {"doc_hash": "04d4a552f56353e13e2ccd2b545886e1d4465f081bd2d21288b8c41eccc9dbfd", "ref_doc_id": "bb446d01-a356-4334-995f-9c48f9fd774a"}, "233c9a73-4a6b-4487-addb-f802e9facc2c": {"doc_hash": "09a896ab6fa17e9919c5f9302becb873788035a0267c1953490417fa7addd588", "ref_doc_id": "50909071-1542-4e51-b237-0fc7ae4c7343"}, "00b6f41d-74ae-4d2e-90ef-397129c43695": {"doc_hash": "87cb64cbd0d7c8af2a2fed20cb851176598b8018d093b7c4f21eeee8dff50c21", "ref_doc_id": "50909071-1542-4e51-b237-0fc7ae4c7343"}, "849ee03e-dc51-47b6-9342-23475e2aebce": {"doc_hash": "f3e4192ad51da95b49fc48444313d69ddcde05cff9226cf0ac392adf785daa50", "ref_doc_id": "50909071-1542-4e51-b237-0fc7ae4c7343"}, "1f9df9c1-aafb-4235-9c2f-59534251ac72": {"doc_hash": "6118c9eafdd8438b0b441c355d6a2b42bcffbc342d05439343ca0ae7fc401dc8", "ref_doc_id": "2fd405cc-abb1-4276-a505-f0c54683c981"}, "2a452cb9-b9ec-4d21-844a-66f02127e785": {"doc_hash": "51c5d5af58069f4b89fbf43f331ade4476e378f94f0908376ac2cc49797749d2", "ref_doc_id": "2fd405cc-abb1-4276-a505-f0c54683c981"}, "11704085-188f-4d20-a04f-7af15c810a1b": {"doc_hash": "68693f91891fa192b56dbdedf4ab6d2e7290c709986b0f95b6bc702287d3154a", "ref_doc_id": "2fd405cc-abb1-4276-a505-f0c54683c981"}, "1dbd87e1-a3a3-4fc7-90b9-1facc33d8802": {"doc_hash": "c9f32cf890f0ced4e44f84add734d78d8de848c93b02f9df343952c55e262e13", "ref_doc_id": "2fd405cc-abb1-4276-a505-f0c54683c981"}, "39b61921-4798-46f0-adb7-964408b5ee30": {"doc_hash": "3e9566e10d06992997c3675cb66bf622d9978c72bc601def2a2560a9ad7bf3a3", "ref_doc_id": "bafef8a1-f34a-4811-a591-36e13ee567c8"}, "bc6a8923-7d50-4c27-a26e-14118d8627eb": {"doc_hash": "a41c41637a4fd5ff59c4853660941501cc3276faec8403eef79e8548b5f06fcb", "ref_doc_id": "bafef8a1-f34a-4811-a591-36e13ee567c8"}, "9cc8dc14-3b6e-4da8-ad44-070655b5ac67": {"doc_hash": "33bb345386d27c403a6c569ae887f3c92e3433825847b4af8b228537864641a0", "ref_doc_id": "bafef8a1-f34a-4811-a591-36e13ee567c8"}, "d833caa7-9457-44ae-9773-11cd1c95d4a2": {"doc_hash": "95a32472cd5b20a1e9d0c4fe7be7d68406deab5affc1b93a68bda8285d512b7b", "ref_doc_id": "bafef8a1-f34a-4811-a591-36e13ee567c8"}, "8be4bc96-a20c-44af-906b-110eedee19b2": {"doc_hash": "b5e1acfd6da3103483df7e3b5b10c53b91d10a133b61ba92ba9bb2b5ed49eeef", "ref_doc_id": "bafef8a1-f34a-4811-a591-36e13ee567c8"}, "90e6a7d0-f6b4-4b3e-af5a-f0e00faed3fc": {"doc_hash": "67b5372d059479e04f9eb6466b94e79677ec81c157236caf8d75c661591f8592", "ref_doc_id": "ac308aa5-05f0-4693-bec1-037bacdeb574"}, "166658dd-c3b2-4f95-8aa8-fa5514696daf": {"doc_hash": "e49f4237b9c43413d4d787bf72137af9f388d146393dbec80ad601a803a2d92f", "ref_doc_id": "ac308aa5-05f0-4693-bec1-037bacdeb574"}, "bdc0bf98-af01-437b-bcb7-ed8ed05085c9": {"doc_hash": "f7df891e6319fa6b3c5efd4c22ea6801044a4a91dcdb1822d4d77ec33f1f55b6", "ref_doc_id": "ac308aa5-05f0-4693-bec1-037bacdeb574"}, "d216f694-079a-4d62-9245-ae16d17a0451": {"doc_hash": "9a5bf62bb3d454ec8aef29f03259d547dc4273f27dda6eb83a7aba9275c19bfd", "ref_doc_id": "d2235e16-18fc-4f83-a5fa-e394cdf57e7d"}, "62e3d134-9d6a-466a-8be3-6dd61862701b": {"doc_hash": "9e0b9a79bd970ddb7f8194261df37e927053f8e5147b85b1b020f22ba89e9a7e", "ref_doc_id": "d2235e16-18fc-4f83-a5fa-e394cdf57e7d"}, "1740996f-08ba-418e-bb10-932067e0f673": {"doc_hash": "768a46e756373415efdc6992017d5a7fa94efc6d26da9918dcd73e166ca552f3", "ref_doc_id": "d2235e16-18fc-4f83-a5fa-e394cdf57e7d"}, "a50e86bb-7445-439d-b248-974e2f76f7eb": {"doc_hash": "3321acd9ee4efcfaf9f68b3411af419e3ad2b68888f30ccd2d91999f3911d297", "ref_doc_id": "5c4b727b-76da-42d5-bbb0-8aa2d6fecbef"}, "7fd4c6c2-9798-4b54-8908-a83c8167553f": {"doc_hash": "5715bc9e2c0e7c12ec1ed50381ae70359e5e42fc58e7e07b170de9d250a2233f", "ref_doc_id": "5c4b727b-76da-42d5-bbb0-8aa2d6fecbef"}, "ac1226b9-8b01-49c0-937d-a1b1511395b4": {"doc_hash": "5952f40bfff4c6046f5227ee0a967c2f494092948ef087fc7331635c9aa57d8f", "ref_doc_id": "5c4b727b-76da-42d5-bbb0-8aa2d6fecbef"}, "596e6bab-2f92-40ce-9d63-1cdaa6528faa": {"doc_hash": "9c484bb2d6cf025d5292d1a4b85fdda9524125aea55e30de06850d59a3047ea3", "ref_doc_id": "12f4f396-4852-44b5-a835-e401ce95bb35"}, "2b4b331a-d25b-4313-8bd8-b6e83df3ca96": {"doc_hash": "8219d4efd2a69f26bb3e3f32495928e899bf2c28e1b013f55209ae9a2d098803", "ref_doc_id": "12f4f396-4852-44b5-a835-e401ce95bb35"}, "138e5330-8019-4f2b-a3e1-1a9cffe23a9f": {"doc_hash": "7791c1915a2efb9beecbef371851041f34d55f8a6026d5be2f8ed47f572bbfec", "ref_doc_id": "12f4f396-4852-44b5-a835-e401ce95bb35"}, "0f5312e2-5743-4d78-ba89-e715c2eaff3a": {"doc_hash": "de0bce2a2155562460ac06bc42746f4b23ff8b3b03351f4963208755b9f9a033", "ref_doc_id": "12f4f396-4852-44b5-a835-e401ce95bb35"}, "11f4622f-0fac-47d5-ae35-2022bf4226d7": {"doc_hash": "66fafb157384cc0ca3f0acb4a3d9fb9822b87a2b24abed643c391af354096012", "ref_doc_id": "12f4f396-4852-44b5-a835-e401ce95bb35"}, "3a037525-5661-45c9-8432-78b91891cd1b": {"doc_hash": "9c3e1b51f026eb416ded701fac0c652a4915a99bb367d5faba8fafe87ea4e530", "ref_doc_id": "92099854-6b11-439b-a990-718a7476119a"}, "d683169e-0629-495a-9182-d6a60c242eb7": {"doc_hash": "3235fed77e6421d07c70675ea1417e17bfc8ba91a5020a23b6fb31bc0005a0d9", "ref_doc_id": "92099854-6b11-439b-a990-718a7476119a"}, "b925097b-b92e-4a5f-911a-2c3ea81e6bd0": {"doc_hash": "9870caaf69406ec5c107b60b24eb9ddbee2c28bc204ff3dcc1ad24bb15cd4c45", "ref_doc_id": "92099854-6b11-439b-a990-718a7476119a"}, "5e404499-d155-4e89-a838-240a6641b0da": {"doc_hash": "6346cfb1b01ad26cebd924a7804345b5e7d13e0dcc348be2dcbf2cde0ca4ded5", "ref_doc_id": "d4aed660-228a-454e-9d8b-f0402edd3d6b"}, "5360d1d2-2df1-4e9b-8fa7-1bb19441bdeb": {"doc_hash": "f84477e745abb3e6d83258aa8a47a8366f5777a43d951ae31fdcc0eb7efd3986", "ref_doc_id": "d4aed660-228a-454e-9d8b-f0402edd3d6b"}, "e1b4dbee-ecbb-4ee8-a407-844e22b629f5": {"doc_hash": "65963947a363f2a3fd183d89c377f0334fc6ed3d6534d162bc9cfdd2c90eae13", "ref_doc_id": "d4aed660-228a-454e-9d8b-f0402edd3d6b"}, "a6c71f0a-ce7f-4b32-88e0-c96ff56daf5b": {"doc_hash": "2ff79a6295d31851fbbdc6c17a1714cabacf55459cf3995c5616a69bcd30372a", "ref_doc_id": "d4aed660-228a-454e-9d8b-f0402edd3d6b"}, "10c0cd5c-1f7b-4960-a65a-e7381e7468f0": {"doc_hash": "5c890380fabb38612171a542792a76fbb4007bf67b982ae059b0b0d11f20f845", "ref_doc_id": "e8da54c0-1c6e-4205-bfa1-e94c535666ad"}, "26947e47-2847-463c-b177-33acb517f78b": {"doc_hash": "7cba123b10f7878b0563f317f55bd58d51565b43bafb4e3b17a674a1587b941e", "ref_doc_id": "e8da54c0-1c6e-4205-bfa1-e94c535666ad"}, "3a0c7415-674c-4615-adc9-30878caf52ca": {"doc_hash": "543f20459854099a0f0fbfdf0e4d50bb2866e76e8a6af9311e13d7350d62dba0", "ref_doc_id": "e8da54c0-1c6e-4205-bfa1-e94c535666ad"}, "a529084c-9523-4372-8ff2-f6dd9f5b6978": {"doc_hash": "f91331300eeb32cc9d4eb5ab099af4070a76852a3b2acc2e2a427a0134c27601", "ref_doc_id": "586dbf6c-0124-4c92-afea-b93b6368d319"}, "128c4ac9-7ee6-4378-a57b-a3323fc3af70": {"doc_hash": "5095c74d2c94ddc545d8f9539cdeb26366aa1657b9298c94dec16515ae0cdb6a", "ref_doc_id": "586dbf6c-0124-4c92-afea-b93b6368d319"}, "50877f86-fc75-4faf-abc9-6936bcd6b5fe": {"doc_hash": "c91fd812ca033aa7c0a85fe802fa8dc40987b61d4b274d298405306635d20c90", "ref_doc_id": "586dbf6c-0124-4c92-afea-b93b6368d319"}, "89097361-fe0d-4ed9-9e41-9a65c664b4a5": {"doc_hash": "659b4a564b9af814dcb825a41ada5c934025ab25c6051f9448cdacf1cb149499", "ref_doc_id": "ae3c90e6-c392-4877-8f3a-cd210ee29352"}, "a3e6ec6d-644d-48b2-aaf4-4d06e2661f5d": {"doc_hash": "97f33af17b21b9398edd9b56c4d0e148b16019fa3dad334c68ad80298829d127", "ref_doc_id": "ae3c90e6-c392-4877-8f3a-cd210ee29352"}, "8f07df39-1b58-4c24-a551-2cf41b6e3f4c": {"doc_hash": "7b6dac8123a7ea3a3879120e66746304b06f7d1235d71f6a7f96ccddd07bb82c", "ref_doc_id": "ae3c90e6-c392-4877-8f3a-cd210ee29352"}, "a2343ae5-961d-41ce-8767-9ce5d4df3134": {"doc_hash": "cf26e42cb3a947e96fb7bb97793d815f5c59586c0b505b9255f2dfcb7a53ce0d", "ref_doc_id": "285ce5de-c5d9-41ee-b830-567ecace871a"}, "0a7ae56f-18c7-41c0-aefb-20b649267ee9": {"doc_hash": "e5484cca65077544035d185a21391cc80638049777bf141c0cf6df38318f0227", "ref_doc_id": "285ce5de-c5d9-41ee-b830-567ecace871a"}, "bda59a28-dc2a-4ac8-9c0e-89079449c61c": {"doc_hash": "a0a4133122dba7e965fa9a94f1acb0ed2e23c580e5beb3db4015de277f355d51", "ref_doc_id": "285ce5de-c5d9-41ee-b830-567ecace871a"}, "67aab263-dfeb-4c2f-8b99-b98488533e09": {"doc_hash": "28ca5e6129a721f540a64eddea95d088fcfd021e63c2fa61e5e70470545ed890", "ref_doc_id": "a862e693-ee31-4336-8f5d-fee03192c4d0"}, "b296474e-5d28-4b9a-a11c-953298ebc5f3": {"doc_hash": "772a27c3c38265065ba4200355c3d9a42142e965e8c5ce03d241b400c4a34f0b", "ref_doc_id": "a862e693-ee31-4336-8f5d-fee03192c4d0"}, "6c9e8377-b523-4ab7-b60b-44f3f01c1032": {"doc_hash": "bb5cc59de96d411cf23b86b0d45d0e54b2e0e65b758115054d88f1f9e4192e1b", "ref_doc_id": "a862e693-ee31-4336-8f5d-fee03192c4d0"}, "fbacc335-cd04-4b6e-b3e1-526672cf5bda": {"doc_hash": "4aa51d132cc8c3757ebf5cb2ba8d2d9fe3cf622b1f7cd3223a6529f478a41e67", "ref_doc_id": "a862e693-ee31-4336-8f5d-fee03192c4d0"}, "af8f5c03-5619-46e8-9b44-5086fcd51851": {"doc_hash": "5eced6bfce2139ce74d978135114a0db5b6572d707dff91712eb80d5c575bcea", "ref_doc_id": "a862e693-ee31-4336-8f5d-fee03192c4d0"}, "a422e3c5-9810-47e9-ba7f-c6ba9a1c72b0": {"doc_hash": "f44e70a424963f376774fc7e1a71431132b341d8328352d54096b30b44372e7d", "ref_doc_id": "a862e693-ee31-4336-8f5d-fee03192c4d0"}, "fa1b58eb-7d4a-49b4-8338-72f14fbb8aab": {"doc_hash": "1d52e19d27457396235b117d28f7c8257f60cf3521fad925706143d1386b74c6", "ref_doc_id": "9cae519d-8301-4f1a-aeaa-acfb3292c99b"}, "de098a77-ad7f-42c9-a3db-ab3d8db35c36": {"doc_hash": "2a4d7dc64af861ad6d30e2459bd18cdbe62e037e05235f0089792916d110c093", "ref_doc_id": "9cae519d-8301-4f1a-aeaa-acfb3292c99b"}, "7a50644c-df14-4eae-ba9b-6d575af2d5cd": {"doc_hash": "38506b1deaaa8718c9cf84489a1129f172e61b9f91fe7a8252b1b2f4fad21817", "ref_doc_id": "9cae519d-8301-4f1a-aeaa-acfb3292c99b"}, "7033d9be-effe-4e62-9074-76037665321c": {"doc_hash": "9edf11d93b5ac83db74329ee37a84d6ac4e7b87deaed3f5ae0908b2b71bb2a32", "ref_doc_id": "9cae519d-8301-4f1a-aeaa-acfb3292c99b"}, "175ba084-a296-4e9f-b71d-b501d5a22ad4": {"doc_hash": "9ab49ace775649dfdb601691d58d0c94b676a3c300ef6a0830064a61c7ece53a", "ref_doc_id": "9cae519d-8301-4f1a-aeaa-acfb3292c99b"}, "4d8b3c20-1778-4967-a7af-f110282a989b": {"doc_hash": "e4f4b7951185fe06e919a156e968efa38a1b3680e7ce6fbf2824f258c3f72a58", "ref_doc_id": "fd63239a-c906-41ee-b807-30467082eeda"}, "b73e9c3f-7df7-4cb1-af7e-8ef34a8d24a4": {"doc_hash": "950f3ace972bc3a3c5deed39d2b1de931c0c897e39eb6afbe5e9d765ecaacca4", "ref_doc_id": "fd63239a-c906-41ee-b807-30467082eeda"}, "a056ff1b-5f59-4101-9172-35ad569a254a": {"doc_hash": "293d8b6605e8d7d5676091eb051aefe50bd2f59858f65c5ee22b9a0f127b78ca", "ref_doc_id": "fd63239a-c906-41ee-b807-30467082eeda"}, "260ad09d-bcef-42c4-91bb-558ee9762842": {"doc_hash": "dc6180b3cb3eebee347f3704b36dae0efc40751e8198cd47543026393242d9a9", "ref_doc_id": "4fa009e1-cf5d-4a11-9279-7d0fc2a8888c"}, "33ee8c71-de46-496a-87c4-3be3d580c4c4": {"doc_hash": "5ddc00f2a9900ed026f25a0cba05acbb280e4bebacd28cf81c4647573052e685", "ref_doc_id": "4fa009e1-cf5d-4a11-9279-7d0fc2a8888c"}, "ed5f6a20-a67e-403a-840a-065155aa89a1": {"doc_hash": "dade78f54dca2b026b54f818c7b573ad6ccd927e8d06d3e7c86529d3f6661254", "ref_doc_id": "4fa009e1-cf5d-4a11-9279-7d0fc2a8888c"}, "d6efb13b-fa6b-49da-ae6f-b3ab71a2c906": {"doc_hash": "54f96a396c852b82545688bf9880480bfdaf2461d639948b7f457ab39dda31ce", "ref_doc_id": "133fdeaf-3f1e-4672-8e84-db5a8958ecf7"}, "471455da-d5ed-4ccc-b356-469a805e8bec": {"doc_hash": "55165fe889d99a601268ca89e941fdb490a2ecf6e5069d7333899827b681f340", "ref_doc_id": "133fdeaf-3f1e-4672-8e84-db5a8958ecf7"}, "56e6e5dc-9928-4491-9bdf-2ef14b86a35a": {"doc_hash": "61a9c7f47548c394217b098ac33a995a13ed43c0d2fa55ff953cf566778901b4", "ref_doc_id": "34dce242-97ab-4f72-a5e0-97a98365e7be"}, "66f83256-8a5d-4cfe-bd52-dccd52fb7b7f": {"doc_hash": "93c96ece897a1daf80a4cb30e236a510141130d348a3395cb445249da5ca42eb", "ref_doc_id": "34dce242-97ab-4f72-a5e0-97a98365e7be"}, "e08eb439-8c47-42a9-b61d-9e0ae87e9d49": {"doc_hash": "4339e94cdc3927157c69de3579a131822cd405aafd46177b27c508b004816ebd", "ref_doc_id": "34dce242-97ab-4f72-a5e0-97a98365e7be"}, "53887dbf-7135-4ccd-b275-8dfe44c4643e": {"doc_hash": "e949ec328c29be547af2165cb224282a49a00d2642db4c56ec8f6c011e958016", "ref_doc_id": "34dce242-97ab-4f72-a5e0-97a98365e7be"}, "d9051207-6887-4310-84ec-d38f342dbb1e": {"doc_hash": "c399fb73e96db80d0af9b3e6d3029036d580018d82731d854c5b8a29a4329786", "ref_doc_id": "34dce242-97ab-4f72-a5e0-97a98365e7be"}, "905219a8-1a01-4c15-9ddb-aeb8a4a2b17c": {"doc_hash": "6f2af71ce0054667a0aeeddffa933f2b2b3366970e2f480196bdedcfca06e4a1", "ref_doc_id": "34dce242-97ab-4f72-a5e0-97a98365e7be"}, "fc135e87-5378-4b9a-9760-d4c8bd1f81e8": {"doc_hash": "b63c01c1ef4e39f479a7ad1e647afc0ba92670038d07bd19b31375a5ee3da5a1", "ref_doc_id": "f6a5ddab-07c3-418f-87a9-4d00c7c5da26"}, "d12785df-5e5a-46c7-b887-60400c3257c1": {"doc_hash": "c2f62806f3977dff726aea82927c6c46a9bd7c0f3991e28a658e9b3e275cc109", "ref_doc_id": "f6a5ddab-07c3-418f-87a9-4d00c7c5da26"}, "9143fa24-704e-4121-8f67-9c7884d86800": {"doc_hash": "f6d14e70d140db380ff8f7ee62336a29b050410fc28c8315cbbf9fa92b4687b0", "ref_doc_id": "f6a5ddab-07c3-418f-87a9-4d00c7c5da26"}, "fa83e3ed-d876-4370-9e1f-55f8cd65502f": {"doc_hash": "82bddfbef788c8754354c6a92427efff1cbaa4a44018b9ded2a940874149c44e", "ref_doc_id": "ac98b60c-713a-42c1-a0ad-aac8ad68c12a"}, "dab6a365-d236-4462-b42a-b9a8b93b2523": {"doc_hash": "c801e974a466be35c41e4d2d8dc6c156e8cac8f0455f5961ecf1ba6209e9093b", "ref_doc_id": "ac98b60c-713a-42c1-a0ad-aac8ad68c12a"}, "381a8915-892c-4638-bdcb-eaff7a0295eb": {"doc_hash": "7a72748cd6122b7cd337d841da675525d79b69ac5491cfd1220377fe689f73e3", "ref_doc_id": "ac98b60c-713a-42c1-a0ad-aac8ad68c12a"}, "588475ff-3808-4e3d-b7e3-83860dcfab8a": {"doc_hash": "02380b95e7caa4cccc7d84ec007ed2322f792b66456ecfbef54ed5baa7afd1a0", "ref_doc_id": "bc3fc09a-4c0f-4c9d-af9b-9f3313592874"}, "16b414d6-12a7-4c30-b8d2-7aee4d0dabc3": {"doc_hash": "f0af358bd9e918bcb5f8ac49ad120264446b624ddb0cc1164134066220efbf2f", "ref_doc_id": "bc3fc09a-4c0f-4c9d-af9b-9f3313592874"}, "aa2e2706-accd-4073-a6a4-6d2f4745cb40": {"doc_hash": "c0290c735bf672fdb6e23b7d142a801f48d6ddf8f1da0136b2c2d61a7d51e97e", "ref_doc_id": "bc3fc09a-4c0f-4c9d-af9b-9f3313592874"}, "acac801a-11e9-459b-8301-46c67e452175": {"doc_hash": "2d449826b81ebb58266d3a14c2da11ffbeb65ce68dc617bc1279766cf5852e5b", "ref_doc_id": "cd0d369d-13e9-421e-aa70-c388b567acdb"}, "02602e9d-2105-4940-8370-03865c493134": {"doc_hash": "242027ff275163e715761b29913665e0892baa8d16f918e06a7bf118598fca3e", "ref_doc_id": "cd0d369d-13e9-421e-aa70-c388b567acdb"}, "718c3d47-5861-4b73-9869-ce3c66aa7553": {"doc_hash": "ac5335d2f6bc1f60ce4972afb6c0255f19b0c1fb49763563539ebe7ad45dd701", "ref_doc_id": "cd0d369d-13e9-421e-aa70-c388b567acdb"}, "7b94dc85-06a4-4523-813a-452dea9f54a7": {"doc_hash": "668cd176e67cb4dcbc1703047525f60f5c0c2d4e3c9bd753a33a2f74d19ea642", "ref_doc_id": "cd0d369d-13e9-421e-aa70-c388b567acdb"}, "dcc7c189-997d-4c59-b90b-7a960855455d": {"doc_hash": "7a69ef0137d27ecbbd11294b76d4bb8d2aaf93c5f478a6df46a449a73622e062", "ref_doc_id": "cd0d369d-13e9-421e-aa70-c388b567acdb"}, "2a5d2c4e-6d13-4767-9ccb-3a6afc90294f": {"doc_hash": "f127ba741f8efa9be06e0bd4b68966d423500d3d321e145c80b6c4b86a1db38b", "ref_doc_id": "cd0d369d-13e9-421e-aa70-c388b567acdb"}, "5ae21d20-8814-4335-acfb-06cff996debd": {"doc_hash": "ad948b74989ce4e65a8e0be934c5ce17a7ee588da4563c6ef3bfa527e3c6d49f", "ref_doc_id": "cd0d369d-13e9-421e-aa70-c388b567acdb"}, "75f9bc1d-d52d-4d68-9944-5ee41193f402": {"doc_hash": "8eb2b773236e3dfca37944c4659ede0b88129a5de5835412dd35503f3602e3a1", "ref_doc_id": "cd0d369d-13e9-421e-aa70-c388b567acdb"}, "45264645-55de-4c64-9052-ddec4ded2279": {"doc_hash": "b7ea459236c86da446a4b437dba3b52d7ade172b6d94a7e24c9b3be9f94dbb92", "ref_doc_id": "337943d5-5cbd-4bed-b913-4521e0538d90"}, "a6759d47-370c-4801-afbe-de24b1b6b902": {"doc_hash": "981844027b1753e2fbf2844b2cec25e0568bd5dc6a636a4d61bf8ed0a90b43b8", "ref_doc_id": "337943d5-5cbd-4bed-b913-4521e0538d90"}, "cf2563e5-cac9-4f32-8926-560c46584cca": {"doc_hash": "3591af96536e70d0f84e60f46d0f2854604ca6aa7b38ffe2e51f410db9b549bb", "ref_doc_id": "337943d5-5cbd-4bed-b913-4521e0538d90"}, "01a98d61-b97f-464c-a64a-e0bc922e24db": {"doc_hash": "439625dc51749707dbff8adc2df12dc418327ec7a94a0103e5198ba7aeb7c018", "ref_doc_id": "337943d5-5cbd-4bed-b913-4521e0538d90"}, "beecd20f-0b6c-4d09-b1a4-9a117447a6db": {"doc_hash": "7c2efc0d539fe009b5d7cb1f386004711f2603c37206840fc81047bf8c2ba8df", "ref_doc_id": "ed7eb9b0-c368-4e08-b1bd-46a0ad5c3a26"}, "44a8f9aa-f002-405a-8a4d-bb65d640ede8": {"doc_hash": "b0ae385eb1d3e2c84ab9beb6e32392626d81319e7208d5c612abcb093711c18c", "ref_doc_id": "ed7eb9b0-c368-4e08-b1bd-46a0ad5c3a26"}, "fa07156d-ccb3-4eb9-a7f5-11b73c0f94ed": {"doc_hash": "2fb3a36ec387349a271bc3adbbac3ddb94991fec776a1a4352dcb5e5bf6d369f", "ref_doc_id": "ed7eb9b0-c368-4e08-b1bd-46a0ad5c3a26"}, "8939bf29-d781-47e4-9f71-a48ed04e3bdd": {"doc_hash": "3802f5d0b45a59f62e0fb87b961d41683de766e9914b52811dcea1e622c3f735", "ref_doc_id": "ed7eb9b0-c368-4e08-b1bd-46a0ad5c3a26"}, "9a74719b-11be-43ed-b862-c3ca8b1eabff": {"doc_hash": "2def2f3f0fccacb2b6d403af32c6a19e9c31b881b64494e11317c69b5f82af4c", "ref_doc_id": "4532b31d-298d-4194-8b14-6f6600bd0257"}, "57ef073f-1ac3-4a8d-b96a-de078e1543e0": {"doc_hash": "92c05e2dd5f9836704ab4502d989c7000d425221ce702b95620b652229fca439", "ref_doc_id": "4532b31d-298d-4194-8b14-6f6600bd0257"}, "af9e2a35-b430-4ac7-a010-7257d9e834bb": {"doc_hash": "14010e398be426d0644c4b7a2389eb12158be56e445c62188d01f543603a70b3", "ref_doc_id": "4532b31d-298d-4194-8b14-6f6600bd0257"}, "618aa816-a97d-48b5-a5f6-65027c369d59": {"doc_hash": "00088812fe050283efb4f796218b2da92d46878e6955a7992f836be4797e0ac6", "ref_doc_id": "4532b31d-298d-4194-8b14-6f6600bd0257"}, "e0276ad8-d3d1-4e63-823e-7ccf677cf833": {"doc_hash": "21137fb7223393970c5d97718292585961f41298be8927028c0e63610378537f", "ref_doc_id": "2e013f95-3d19-452b-b708-42053700a82e"}, "9357d29e-c5e6-4598-8647-5b915b6751b7": {"doc_hash": "3b253b5efb0518b9c0617a108b516c52d938815a93905d6d614c85c4fb35e738", "ref_doc_id": "2e013f95-3d19-452b-b708-42053700a82e"}, "3c8b172c-a586-4077-a307-52d5e88e7293": {"doc_hash": "86c2df61a2946b4ba0faaa873d2910158f7dd51f78217ac50a678e0918064fac", "ref_doc_id": "2e013f95-3d19-452b-b708-42053700a82e"}, "cd52511f-8cb2-4daf-9542-3e278bc8069e": {"doc_hash": "682d7381098ddd2eda7aace45523ae4c1a339683377e031de209ecaff6f96d25", "ref_doc_id": "2e013f95-3d19-452b-b708-42053700a82e"}, "ae339b9a-8def-4927-89b4-16068e692e80": {"doc_hash": "6f38adbf7e95294a22a21e9a5e2749a4e6f3029e74dac8563f2052e8d39303ec", "ref_doc_id": "9fc37359-c51e-48bb-804a-ef7d247412bd"}, "3e012de1-bfcd-4851-b9b2-f29350ba5c39": {"doc_hash": "e855ee7fad3b741a324d18c6c3086cf761870abff7fb7d7de6be4ea1808330ca", "ref_doc_id": "9fc37359-c51e-48bb-804a-ef7d247412bd"}, "c1a44eb5-8121-4e86-804b-72dc2cf5484b": {"doc_hash": "482c7988066044a6b984d765911e6e4c311a4bb1f869175a1a1e7818b9f024a2", "ref_doc_id": "9fc37359-c51e-48bb-804a-ef7d247412bd"}, "eb34f723-a4f1-4ec6-9e13-4fca08352897": {"doc_hash": "140fe73059bd3c7862cef1e2f74ef0ff1740236d4c40ec5aaafc8fc3ab3d22d0", "ref_doc_id": "9fc37359-c51e-48bb-804a-ef7d247412bd"}, "abf4768d-cbeb-42c4-bebe-9921ec8a4e09": {"doc_hash": "f757fe9f68d21c50b307528c0fde2e0593b5b19627ab95af2ef93102f8ac8217", "ref_doc_id": "9fc37359-c51e-48bb-804a-ef7d247412bd"}, "191eee4f-1cb3-453f-9882-5a023ef3b322": {"doc_hash": "89efe6855593acfed4022db9b5926202295d6c4045cc5c21c7cde0b484a84f6a", "ref_doc_id": "9fc37359-c51e-48bb-804a-ef7d247412bd"}, "2b3ce324-70a4-4d36-971f-7e42bd1d34a0": {"doc_hash": "00cad552893f9ba49d9fc38d69af2da5b0a434d630aa9c7a65b4376eba6abe21", "ref_doc_id": "ea1967e3-2009-45c0-a71c-01bad66e14eb"}, "1e29cb32-defe-4a40-bd86-f22b5af0e2ce": {"doc_hash": "a38ec2079b8e90aca874a8a64ccda65481aae0707e7036423a0961aec7096823", "ref_doc_id": "ea1967e3-2009-45c0-a71c-01bad66e14eb"}, "180f436d-aca5-4c60-9ee3-84f0c1f62231": {"doc_hash": "ccf22e1f05c170d9f0dddddd60b0d021f0f5f9fa68d9abc7b9bf62710f0df01b", "ref_doc_id": "ea1967e3-2009-45c0-a71c-01bad66e14eb"}, "061febb8-3e45-4f38-abb4-3ccefaeedd23": {"doc_hash": "99bc4587e52c0f960deb29e646690316b90eca01358c8da38a15072a62a4502e", "ref_doc_id": "b8cd3127-a3c7-459c-a783-369cf6a33b74"}, "290c23c6-727d-49b9-a85c-f7036a47028d": {"doc_hash": "9b7a86624d5abb6e69ac81a6ccfb4bccf73041a1e1ed7159b1648651e554dd09", "ref_doc_id": "b8cd3127-a3c7-459c-a783-369cf6a33b74"}, "2a71302d-5033-48a7-8790-4617a480a82d": {"doc_hash": "7f7e8bf86356e18bcbd054a232ed71cacaac3bb26696753a55f71ddd3c8bf387", "ref_doc_id": "b8cd3127-a3c7-459c-a783-369cf6a33b74"}, "f8ff0b29-eb62-41c6-b659-37665ce9ee1b": {"doc_hash": "612bb9c9de94d24dd43b0e46d0e69c147366dc1d973f24c8c776b638ecc482d6", "ref_doc_id": "b8cd3127-a3c7-459c-a783-369cf6a33b74"}, "c7d4ed71-33d9-4a5d-b36b-6d02aef48179": {"doc_hash": "bd0f5ec27ef1e992c76a83447b216be7b80fc1e8d9adbb2f2d723918c45c7113", "ref_doc_id": "b8cd3127-a3c7-459c-a783-369cf6a33b74"}, "67bd89c7-52bc-4c77-a198-80080f13d8c6": {"doc_hash": "bc54d0f60b5890c2730375ad7da95d3da8b23d4dee4490e014b912d8e5572a34", "ref_doc_id": "15ff97d4-bffb-4581-9df0-92c7b7f7734d"}, "251060f2-d29c-4482-9377-29d742582bc7": {"doc_hash": "410d584e24fa4b68d133966d63ccbcee12451aeb03bfe288078600c92fde048e", "ref_doc_id": "15ff97d4-bffb-4581-9df0-92c7b7f7734d"}, "6838fbe2-4f7b-46b0-9185-9c3c83708142": {"doc_hash": "2bb5b71a46c76f4c868b58e2ec971571f363498a17c1bb83b57fa713d38fb227", "ref_doc_id": "15ff97d4-bffb-4581-9df0-92c7b7f7734d"}, "d7bd5afe-8e98-4156-a2e9-6b323ba618c1": {"doc_hash": "4987d1baf603991e1b97a8a87257cfdcdde6d6eafb77d4b9eefdb49ebf6ef608", "ref_doc_id": "15ff97d4-bffb-4581-9df0-92c7b7f7734d"}, "3f74226d-914a-41c0-bbf4-f839ea8a49d4": {"doc_hash": "23b466a4fb4a946d5956c749f204ca262596b0e946e9f3cda61b39a977062082", "ref_doc_id": "7711566b-e14a-4779-ab42-616803ecce59"}, "619eb7c5-19b5-4c78-81cf-93def5e91d85": {"doc_hash": "5ce6544d1102f731cbe6db2e6032e8057b28d98162abe5ca523b923e6367af9c", "ref_doc_id": "7711566b-e14a-4779-ab42-616803ecce59"}, "f66a14f9-fe30-48b0-ba2d-605149fcc8d9": {"doc_hash": "1223153dd5ca3f0b6d989fd040ddb76622fc4ec1ea15b530de75b6edf92f87a9", "ref_doc_id": "7711566b-e14a-4779-ab42-616803ecce59"}, "ca181d59-c6b9-4bfd-8f26-7fae74584e12": {"doc_hash": "e313569db7007f54853fd0fcbbf8156be480c32325bf7c1e6d7f2f0100b2548a", "ref_doc_id": "7711566b-e14a-4779-ab42-616803ecce59"}, "4863a3df-c1d9-463e-b44b-5d867c6d456b": {"doc_hash": "e9680a79b500cc4e47ea4e40e6b27ac280cec6ee50e665a61cfab1d0b398177f", "ref_doc_id": "4081ae4b-e041-4da7-b180-b7dd2925f42f"}, "f9c25d5b-9356-401b-ad9e-45cad86c4c8a": {"doc_hash": "4e0a1b152a1d4f9af0214ccb7114df6feb0d4a2bc6feeb557cbaf7c80adc7118", "ref_doc_id": "4081ae4b-e041-4da7-b180-b7dd2925f42f"}, "8a335b17-4458-4884-ae09-14196a762a0a": {"doc_hash": "489211c3bda8ae011694b57f04da706cd485734eef6f7caa92ad61cbd6afcbb5", "ref_doc_id": "4081ae4b-e041-4da7-b180-b7dd2925f42f"}, "016945a6-5455-46f4-929e-cdc5b4a7263c": {"doc_hash": "03057f5f5054a37e02d16b1a5b1081c89f5ff32b7e20bca081bec6c16de68808", "ref_doc_id": "4081ae4b-e041-4da7-b180-b7dd2925f42f"}, "a7324830-a2e6-4098-ad67-434c76fdfa7d": {"doc_hash": "28068fa383ce3f054dff408e60cb1b0c8dfeb0ba8ae856801986ec136d8bf753", "ref_doc_id": "4081ae4b-e041-4da7-b180-b7dd2925f42f"}, "d5963f23-cee8-44e4-9829-f0e3b21a2915": {"doc_hash": "b46d3e7883e62d14003e08000d18b1c720f65a66407819be54454b2324cf154a", "ref_doc_id": "4081ae4b-e041-4da7-b180-b7dd2925f42f"}, "38fb43a3-a889-41cc-bd0c-6fb0e50ce780": {"doc_hash": "6637887a84f29692ba6ca58b3cbf25240e12d3be0a04a6bdeaa77be8c10c7c45", "ref_doc_id": "4fe84a89-4090-49b9-8cab-400b4b1abe3b"}, "be2d7434-f8da-4e82-8653-b5044604e165": {"doc_hash": "2931f34e65f1b2f16375c336f66b0ac19d896f393e744ef811ff230a1d61c3cd", "ref_doc_id": "4fe84a89-4090-49b9-8cab-400b4b1abe3b"}, "dd0fc390-b5c8-43c5-89a7-2a3ddbb1ef78": {"doc_hash": "4735490bf9c9bcb30e1eeb7d41688b455f607da0680dc75e27f92a66015c6c69", "ref_doc_id": "4fe84a89-4090-49b9-8cab-400b4b1abe3b"}, "a8135804-ab63-4597-96a7-159057558f35": {"doc_hash": "684269def8ed4257c00374bfb285ddb4bd9a9135c098f4c24a1dcd3daa1b157b", "ref_doc_id": "7d0ec380-908e-4cfe-a9cc-2294cd62ec76"}, "eb1954cb-075e-451a-bc4a-228bf923b8b0": {"doc_hash": "e229efe6f4f317264ea69312d02ef94a169e55ff0e24df5a51d02d26ace6c4c5", "ref_doc_id": "7d0ec380-908e-4cfe-a9cc-2294cd62ec76"}, "340b6f5d-ab5c-490b-bb80-b8ce6ec92b33": {"doc_hash": "7f8eeb4af164d2be5764b6e3c9273a58633d62f6cab4901000801f7e90ed0aa0", "ref_doc_id": "7d0ec380-908e-4cfe-a9cc-2294cd62ec76"}, "c3ba5ae9-9611-46eb-9843-3a8af6ad93fa": {"doc_hash": "28f413b40224ce308ac7da0a6bab046906e02294b21b86a420807b1ee49d22d2", "ref_doc_id": "ce6327bc-87c8-422f-9e8f-e30fa9222be5"}, "3f32db4e-c89d-4a96-9e35-77acabfc5ef1": {"doc_hash": "8ca75f07e5203053ca88b9c79ac847d5cacdb5b7db129758a96b9e67cc411684", "ref_doc_id": "ce6327bc-87c8-422f-9e8f-e30fa9222be5"}, "baab64ef-8c30-4272-a2db-4316cf1e4ca5": {"doc_hash": "d33f9cddaf29c0bc7d4d6731efc0317748b0bd0492dae12f8787a77815c284e6", "ref_doc_id": "ce6327bc-87c8-422f-9e8f-e30fa9222be5"}, "bad81132-00e1-4006-b70d-77e115ca1ee4": {"doc_hash": "5b3646f9146a4cc98b2efeaca326a650a8bd700fc8073e0db77623c6bdb40bb6", "ref_doc_id": "e5972247-4238-4dc0-8884-fe424808f9b0"}, "35e17dcf-4d2f-4abf-a255-60db9414fc65": {"doc_hash": "27f86b18cea6368dc5f0493243f7e696139e68281c4c7e88214bce2451fc051b", "ref_doc_id": "e5972247-4238-4dc0-8884-fe424808f9b0"}, "29d66008-36de-4c9b-86d7-37809a635dbb": {"doc_hash": "2054ddc48a19916962ae508a6802516a525862d3c7483f4baef6a0a4a7701abb", "ref_doc_id": "e5972247-4238-4dc0-8884-fe424808f9b0"}, "0d47f71f-43ac-403d-b5ca-8e76f41bda22": {"doc_hash": "5dc2ce983c00e1ce3845475e17a8571ebd36a1e17fd6196651ee7736979d3d80", "ref_doc_id": "e5972247-4238-4dc0-8884-fe424808f9b0"}, "0d475a7e-8fba-4477-b219-f339c222f706": {"doc_hash": "0d3f391439df87c5b89c32413dbc0c839cd76aed90a5e627d23521e5e372af18", "ref_doc_id": "e5972247-4238-4dc0-8884-fe424808f9b0"}, "ae0233f8-3d39-46d5-a534-ebb7eeb5470e": {"doc_hash": "ec6734c6a18401b751fd18f98169c1623012b57667c95952f7a64b9ef6d2105f", "ref_doc_id": "e5972247-4238-4dc0-8884-fe424808f9b0"}, "c2529ebf-87af-40d5-b115-067c0a51316c": {"doc_hash": "2ca564b161550b070530433b223d0ff24878de0e38a3a2aea5d7d1afe7a20481", "ref_doc_id": "e5972247-4238-4dc0-8884-fe424808f9b0"}, "f3ea832d-5230-4909-a5d0-32377f3c32ca": {"doc_hash": "c153277439dd702eb775e1316fa05c311ccfc5e37f88a2c0db511f9933d9f91e", "ref_doc_id": "e5972247-4238-4dc0-8884-fe424808f9b0"}, "b3ce4930-8c30-4584-adbc-c0edfd3e4d1e": {"doc_hash": "51990b5a49bce866121d47c78c47b5dc4075c3e479143a14aa21575fd3ee1d6d", "ref_doc_id": "8cc35d72-043c-479f-b0bd-499aac454c6e"}, "83d83596-7bba-4912-ac7f-c2c8ecf3b71c": {"doc_hash": "55ad72f6040de4078169c24fb9b1b8c4ddab6476b274f54516ce13851789bc99", "ref_doc_id": "8cc35d72-043c-479f-b0bd-499aac454c6e"}, "c6c9c97f-004c-4959-a13d-af0a2566ddf4": {"doc_hash": "1493b916da36207f747e84bb205dcd283c00e46ddbb98abc27a1eac152239610", "ref_doc_id": "8cc35d72-043c-479f-b0bd-499aac454c6e"}, "adb0e3fa-300e-41fe-9515-dae0c8fd5bc5": {"doc_hash": "327fde679188f2be8742b34c72487bbc408241b15db4c8544c6bb7918995c189", "ref_doc_id": "8cc35d72-043c-479f-b0bd-499aac454c6e"}, "f5735661-0bd1-4550-8402-80137bc4c456": {"doc_hash": "83d24277fdcc53fa5507092fdd49ebcad91c8ed36d10cf8d2df38023932bed58", "ref_doc_id": "56b78b37-dd0a-4961-83bb-e841cd578c6b"}, "b30a738e-75d1-4a93-8f76-26a7fc4d836e": {"doc_hash": "d4df2898b749385aa7bc207b0c08c26739b27e4f24243b3be259d46d6cc6c774", "ref_doc_id": "56b78b37-dd0a-4961-83bb-e841cd578c6b"}, "d1613584-22e3-4529-9e3c-c824a8eeefe5": {"doc_hash": "93bb03c3e8bc2c7c25996a0af293052ecd3b11e4b52cb7add9185a153879bdbd", "ref_doc_id": "56b78b37-dd0a-4961-83bb-e841cd578c6b"}, "1cc00555-83ad-4f3e-a9d8-7b022044e05f": {"doc_hash": "7335f017915cbf07c6748b7b1bdca9f79f6d93eb704f5b113eb3dec26dedf05e", "ref_doc_id": "512d7ecd-13a0-4eec-a049-a41fbd99357e"}, "fe3d7723-1c52-4fd9-8ea8-49bd23c63f75": {"doc_hash": "0524f66790c21a67100d313e677ee8f9d8b3f9facdb55bc6dbedc9332e58788f", "ref_doc_id": "512d7ecd-13a0-4eec-a049-a41fbd99357e"}, "bc219ed7-fb2c-4e74-807e-5d875061ae96": {"doc_hash": "b22ec561dece328bb356fb107cb3ead77ed815152012c410957f1c230d2d66a9", "ref_doc_id": "512d7ecd-13a0-4eec-a049-a41fbd99357e"}, "e178418f-8218-4de5-94f6-e0fa2c2ae364": {"doc_hash": "c00f4d9f86d40bc04dfd036f2fe6f38d155e5ff4b6adc960a06594ab0fee6fa2", "ref_doc_id": "512d7ecd-13a0-4eec-a049-a41fbd99357e"}, "2883a768-6223-482c-8bda-bcf82f594110": {"doc_hash": "925e5e833a4d43ad47e476a08642828277d5e10743b92bf8f1e35b23ed59adcc", "ref_doc_id": "b1660f5f-af05-40c0-aeb6-ae58303ad8bf"}, "7935ff8b-b1fe-4d26-9eac-b423de5155fd": {"doc_hash": "087f1be63199be39d8a59dfb211a92e5c415508425d276d9c1bc9f2044147c92", "ref_doc_id": "b1660f5f-af05-40c0-aeb6-ae58303ad8bf"}, "9521efa1-e6f9-47ea-bec2-bd9caea748e4": {"doc_hash": "ff235426da4d278ec26eb175855f0090ebbe1cecfe12b96760f136b2e68608a0", "ref_doc_id": "b1660f5f-af05-40c0-aeb6-ae58303ad8bf"}, "281043d1-aed9-4ac1-8b37-a032571ecf6f": {"doc_hash": "6f7fe12b7a018ee0e0f6bd5f347a8bcd03d172b775cb183a31f6cb302a5285b6", "ref_doc_id": "b1660f5f-af05-40c0-aeb6-ae58303ad8bf"}, "22bd3cc4-87fd-4c88-ac9e-6fe2c6de8670": {"doc_hash": "f25d6fa34daf6bc8c9baa5269981e7b4e02459c0e8e8b05e39c53e92d91f6948", "ref_doc_id": "b1660f5f-af05-40c0-aeb6-ae58303ad8bf"}, "d0a1f9d6-3dc6-4eb4-a6a3-3d9631fdd279": {"doc_hash": "dd959570df23b6c49fbbc82139e43a8d48878d6723896b7f43365d648a6fa818", "ref_doc_id": "b1660f5f-af05-40c0-aeb6-ae58303ad8bf"}, "acdafbb2-b8bd-4cb6-8651-cc92a56ee9f3": {"doc_hash": "8d4188b9dc5e8e729304d0cd50de9587fb6416435fe6d56a212c4158a03c533e", "ref_doc_id": "68c2afb4-da61-42bd-aab7-4000cc2aa8be"}, "b27bbcbd-6512-412d-98ee-ac97197315cc": {"doc_hash": "c0592703afa6641ad2a23071f671d0b0250e468f7e2b3e6140fd7c809956fd70", "ref_doc_id": "68c2afb4-da61-42bd-aab7-4000cc2aa8be"}, "dc47a609-ed74-452a-a5ed-fe1c0e804eef": {"doc_hash": "3e03d23f560ae29b3dc304b575ab05bf31456a170314a1e4dd44d0927a80cc3d", "ref_doc_id": "e0e19263-36ae-4334-93ff-95f882469e66"}, "00693a2e-e6d3-4649-acc1-0e04ae6c575b": {"doc_hash": "a6bcdd0a45cb16a8193f77617e25775e475be5e48c67acba7d3cb7f601ffba83", "ref_doc_id": "e0e19263-36ae-4334-93ff-95f882469e66"}, "97f4c66a-1315-459f-9ae9-0eb0ffbf9e76": {"doc_hash": "718727ee5ed98c55be34ece3abb28b0838bb8d39562dde9d3d02cd430bb3fe8c", "ref_doc_id": "e0e19263-36ae-4334-93ff-95f882469e66"}, "22912f4c-8db2-4d26-a771-96e895fdf90f": {"doc_hash": "d4f6764fc7cc25f89fcba6669b2eaf7ba0efb237765353fc2b4013d224f98773", "ref_doc_id": "0f766867-73f3-4e06-bd1e-3b879e6bd02a"}, "abbe847f-9f06-47ed-9148-aa6eebe08901": {"doc_hash": "c76186094cc0cc7ef261ea93ad9592eb1154ad5074b2e5659607deedab06a199", "ref_doc_id": "0f766867-73f3-4e06-bd1e-3b879e6bd02a"}, "96e3351b-7ca0-4d92-96b3-eb0e4e243611": {"doc_hash": "5e1984b9808d780cc8f6414ede7dbded22acd8105077e88b0e3d4ab11fe68eff", "ref_doc_id": "0f766867-73f3-4e06-bd1e-3b879e6bd02a"}, "b6cf405f-c94a-457e-8995-f94b75d9db5e": {"doc_hash": "5805a78b15ca68feaec19b39b98f97b367bee60d8dbf6dbf9333d7540f21553a", "ref_doc_id": "0f766867-73f3-4e06-bd1e-3b879e6bd02a"}, "ff00f6f5-830c-4309-bea0-7bf983820985": {"doc_hash": "73459c56d2ac1d0bc5c15f6123b2f1f99506492f5ede0a8a6005b40423c05ecc", "ref_doc_id": "0f766867-73f3-4e06-bd1e-3b879e6bd02a"}, "5a147782-6e42-4a1c-8732-8bf0ec2a26a7": {"doc_hash": "3e0b2241367b34a31480b9c744384479097c0650b6f231e4ea9675069e08fc79", "ref_doc_id": "0f766867-73f3-4e06-bd1e-3b879e6bd02a"}, "142c0792-e5a7-475c-8a9c-a13eec8e0965": {"doc_hash": "284f931538361d05b0dbc16d64feff5bf66632cc9e402e2e64f358366b4a4e5b", "ref_doc_id": "0f766867-73f3-4e06-bd1e-3b879e6bd02a"}, "1ada1dc8-1654-4337-bd86-0cf3bf97db86": {"doc_hash": "b6d7b1a048f3a0f0027ae9f9d27a4bbf62e79a7776b751c9a3e63cd728eb8977", "ref_doc_id": "84c4145c-1895-4f42-aa06-9247a881868d"}, "a55b8e0b-2ceb-4942-8f97-2c1358dc1403": {"doc_hash": "f4167ec891ea8d8536e77fe811977bab6772a88490a2071df6815b65569ef2ea", "ref_doc_id": "84c4145c-1895-4f42-aa06-9247a881868d"}, "48019a9d-8eb3-4045-9ccf-081eab9d65eb": {"doc_hash": "43d871b641fb59a7fff8bda03b3ca2fe4f9d3f9670de1beefd8d2ae58986e39f", "ref_doc_id": "84c4145c-1895-4f42-aa06-9247a881868d"}, "3b7a9f81-06f4-464d-b4b2-e3f4c402ce29": {"doc_hash": "10f9ba41045e1973dce68b960c6baffd28616923245c0b17d486b16bede258e8", "ref_doc_id": "84c4145c-1895-4f42-aa06-9247a881868d"}, "fa32b6fa-21c3-4d3c-99a1-43f261c5781b": {"doc_hash": "24383bef4545f4cebbc02c6ea80e0fe0f442510eb27e13c9ff906bbedfaeb1a5", "ref_doc_id": "819a17cb-b272-4cca-9b9e-eabf7957697d"}, "d2c23206-e86d-4352-881c-3a20c737a790": {"doc_hash": "5b0b5e54656204ab049fd0c33171fde4051fea1501f44e88f83510c31359a421", "ref_doc_id": "819a17cb-b272-4cca-9b9e-eabf7957697d"}, "56c9efa5-38f7-4d0e-a148-18df7f4a28a5": {"doc_hash": "b480e2ecb1653b2e4df3a15c7350289bb8f54605da95f0e66d027f5ff422b318", "ref_doc_id": "819a17cb-b272-4cca-9b9e-eabf7957697d"}, "bdd78b24-dbcb-41b7-803d-60f35812df3b": {"doc_hash": "a90ddd2b4d27b6d487d6c0f2ccdfaaae9285632d9dbbf28750624b3842d3f72c", "ref_doc_id": "819a17cb-b272-4cca-9b9e-eabf7957697d"}, "8553ee89-1e4f-4a85-b522-14eac71941ff": {"doc_hash": "45aecf57b4ac89a2fbca51280a90ad90939711cd18d785a5b3c4b29cf0f16882", "ref_doc_id": "a0fee36a-1d60-45bc-a806-3fd5d7e1a65d"}, "65b9aeca-613f-4a52-bf71-ed062d7b8ee1": {"doc_hash": "1ed52204202fe209b7061141cd18f53b37aa007c9ee1543c5beddf8f65d7492a", "ref_doc_id": "a0fee36a-1d60-45bc-a806-3fd5d7e1a65d"}, "fcacdd5b-b27b-4927-af75-b4fd9f305786": {"doc_hash": "c0cd044a820e9b5d37c57cf45a6ed1e1e90e528a6440236c7b0f9b3056d081b4", "ref_doc_id": "a0fee36a-1d60-45bc-a806-3fd5d7e1a65d"}, "840506da-759a-471a-8eaa-82ba31a344b2": {"doc_hash": "770839313a138cdea301eebb931dec7a0c4a0ee107f44c8e9dffb26fbc01e25d", "ref_doc_id": "a0fee36a-1d60-45bc-a806-3fd5d7e1a65d"}, "9cf7c8b8-e4cf-4f7e-a882-e401956078d8": {"doc_hash": "099da3d81a9a916fa0571eae001b1c8d8ab26340b4091f529602bbc7e216f453", "ref_doc_id": "7c8973c1-3304-48b1-8580-1cb04ed8c797"}, "cffb6521-86cf-48de-8b3d-2470ba9f3880": {"doc_hash": "6e69fa05541179e8ccb9211f6d6b926e87dc43b9c1aebb32617bdcac41eaf900", "ref_doc_id": "7c8973c1-3304-48b1-8580-1cb04ed8c797"}, "92429ec3-28cb-4f85-ae09-d00556a1b25e": {"doc_hash": "158b792d8c9cb528fa71870316dd67e5a100903cd4ccecf2b2756e722cc50d44", "ref_doc_id": "7c8973c1-3304-48b1-8580-1cb04ed8c797"}, "192daad8-2b82-44d7-8eaf-edcee61d1f3f": {"doc_hash": "1158dd6a30d41390b5aba1c08f8fd5f25b783b3272dd0a80f015e4425df072f6", "ref_doc_id": "7c8973c1-3304-48b1-8580-1cb04ed8c797"}, "a8d32e65-742f-47af-94ba-b9dc5978b82d": {"doc_hash": "9d348628a31ad524522a3bd8ace3f99e982b0e4eeeec103b2bc2aecfeb8d2480", "ref_doc_id": "ac7e9570-c8ee-402d-a176-7922bacf0a97"}, "40e3233c-b1f9-43da-9d25-deeb842164cd": {"doc_hash": "d953da30f0d71d5b469db0ed0b3d9ea5b841fc594567f66edfd13e2a52f766c7", "ref_doc_id": "ac7e9570-c8ee-402d-a176-7922bacf0a97"}, "1b8bc2e4-461c-4678-8845-415d5812e559": {"doc_hash": "070fc6c9c70089d5fdd24db5be453f93b511220f14c81f70195e4a2d4d5e28c1", "ref_doc_id": "ac7e9570-c8ee-402d-a176-7922bacf0a97"}, "41f386be-0356-4202-9fea-06b51384e45e": {"doc_hash": "8b1418d73d6ffc8c716a4709e998315b78ac19adf6e2e61193e063decefe0a38", "ref_doc_id": "c059752a-faa0-43e5-8e13-9df793b05d79"}, "c9ea4b57-cc2c-4a7b-9735-27521df6f2e8": {"doc_hash": "2fd98b9b857736afd3ca8112a6ab536299a2331783a06fed7d4c9d9214feff69", "ref_doc_id": "c059752a-faa0-43e5-8e13-9df793b05d79"}, "a8491b99-8663-41b8-b792-4c866442ec77": {"doc_hash": "8cc5782c9806e18fb545cc512f7925f63ff77563268cfda1e82e62d89bebdc59", "ref_doc_id": "c059752a-faa0-43e5-8e13-9df793b05d79"}, "37cfcf4c-06ec-469e-89b2-41d186e54067": {"doc_hash": "c4e94b85fb50b99c55f22319296b2e6be65c68e8f5c1d5a08ac44954607bee2f", "ref_doc_id": "c059752a-faa0-43e5-8e13-9df793b05d79"}, "4b1757a6-9975-490f-a6e8-fe72da5bdd61": {"doc_hash": "f4b61b9fcf72e4ed6e39f2b6d67edbcaaf09577dd21b283105846d39e9bb34a7", "ref_doc_id": "c059752a-faa0-43e5-8e13-9df793b05d79"}, "7b33f739-902c-47d8-894d-7523428b86e9": {"doc_hash": "868c539dd6ef95c5efad3f88f40e3ac371aff11ef454f402d4bdc1772bfab6c8", "ref_doc_id": "5ea01df2-9d80-441c-9644-23133cd296e9"}, "13df4dc0-ddbe-445e-80a9-7e75c81b3c47": {"doc_hash": "e057b5203bbbf8d163ac9611a66bfafafbcf76c9997eb97df9da1a3085b86f85", "ref_doc_id": "5ea01df2-9d80-441c-9644-23133cd296e9"}, "2cd81065-2b75-4f47-a7a3-6aa57c31fa52": {"doc_hash": "7026fbd24c04ccda7ec2b572d58e1fd4486e51ab9f0ebe97819e7f7246d68f8f", "ref_doc_id": "5ea01df2-9d80-441c-9644-23133cd296e9"}, "161dd6a1-65cb-4f81-b7e8-923f22dc7da7": {"doc_hash": "e972b37fa2915cd82b193983cee16683f7a48e1c2e8564b30b12868733fad62f", "ref_doc_id": "5ea01df2-9d80-441c-9644-23133cd296e9"}, "4029841c-d639-4f67-b4ec-204a13585c56": {"doc_hash": "02e84caa1927eab9454232c8b4dcd76151578f7f9decdb730db4dd3e6da0bfe6", "ref_doc_id": "5ea01df2-9d80-441c-9644-23133cd296e9"}, "65dacfe5-5846-462d-a0f2-e94162cf3d1e": {"doc_hash": "d2df5d9dd7244391f185bf91a5a7934e2af6febe3c63e9b669de4f43002bf05b", "ref_doc_id": "77fee692-6ef9-4e8d-ad00-def27f2412b5"}, "c44a504a-5623-4337-9e38-27269b6d0e70": {"doc_hash": "622b5d5450d2f5cdf802ac79d2bfab3f699828c3ad5c0f5886b8972e20278655", "ref_doc_id": "77fee692-6ef9-4e8d-ad00-def27f2412b5"}, "f0e1093b-4fa6-45e0-ad6b-47e63fe5ae85": {"doc_hash": "7f1d42e7b9060f6a1339dc97cdaa2c6cb1e3cf6d7699e4a69b3da529043244ef", "ref_doc_id": "77fee692-6ef9-4e8d-ad00-def27f2412b5"}, "7adf07d5-df32-4cf9-8e17-e8a1c4f756de": {"doc_hash": "d863a30a69b3049d0ded56b3ba68bee98075f293dd6ea2939b992f75580392d2", "ref_doc_id": "77fee692-6ef9-4e8d-ad00-def27f2412b5"}, "0ef9d429-a42d-4a86-b132-0bcf9b9df7d5": {"doc_hash": "7930ade8bbad9994f602f3124fe4c7b1ba6a5ba0a7700d4220708a0a561a6d8b", "ref_doc_id": "20a8b548-65c5-4f9c-a2df-1920263843c1"}, "eee7c626-81be-42b9-a77d-2f0cf7d5140a": {"doc_hash": "93aac50e2612fe4a073f6388a426d4f9a4acf3377f9725358e4aa71ace2d459f", "ref_doc_id": "20a8b548-65c5-4f9c-a2df-1920263843c1"}, "9b00db98-5a7e-4558-a318-7418081dc0d7": {"doc_hash": "30de291013721ed5fccc375a4d3bc04a79494bc817da4b45c6cc60299e8c5f6f", "ref_doc_id": "20a8b548-65c5-4f9c-a2df-1920263843c1"}, "29a6eecc-32a8-4e33-a07e-19600e980267": {"doc_hash": "0d760ae6fc04384296be758c4d0f4edb4465834a3d9e93d47147a9c3d1bb5c17", "ref_doc_id": "20a8b548-65c5-4f9c-a2df-1920263843c1"}, "c087d458-7a77-4982-a459-ac995ad7b74e": {"doc_hash": "336efce505056e6a60840583019742ba95b1eee714f6f1d56d7256afc0b2f764", "ref_doc_id": "20a8b548-65c5-4f9c-a2df-1920263843c1"}, "a9bd8085-0ca1-4438-b1c9-1e1b0a348030": {"doc_hash": "cf92d3e7bb42d9bc38c95bfe773b668ef8312944aac1aba1f6f4fdc0aa4d6cda", "ref_doc_id": "25398961-e7fd-4608-acd6-cd550a028140"}, "67f27054-92f9-44e7-90cc-a76ddc333734": {"doc_hash": "25b96277154b4ad4e4e138636718d104f3eb673faf44c18014b74d5c95f48a6f", "ref_doc_id": "25398961-e7fd-4608-acd6-cd550a028140"}, "5b2507da-9367-4259-ad7c-c18653a28d62": {"doc_hash": "bb8e6b835fc2e8f2bd5b6b76331047afa9ddddaf51f1cb832e596bc2c68ad188", "ref_doc_id": "25398961-e7fd-4608-acd6-cd550a028140"}, "cc63fa5b-4ef5-412f-b702-5f4372553f47": {"doc_hash": "658d7b9ef5c977138732abf5f25ed7afe7c06d289d3d790cd06df16ddd3000e3", "ref_doc_id": "25398961-e7fd-4608-acd6-cd550a028140"}, "12be728d-efb4-43a5-a7d0-7e7e501c8bc2": {"doc_hash": "b73545d674fa95e18cad0bcb2771138ff8e283a6fe2d46c925a5228b380ff78b", "ref_doc_id": "25398961-e7fd-4608-acd6-cd550a028140"}, "304d1902-4c36-4fa1-8179-9086a042d84a": {"doc_hash": "3d48a2d13128146c5d6314dd4e5a9ac618d3746b745d29d95d98d15ccd13ae3d", "ref_doc_id": "25398961-e7fd-4608-acd6-cd550a028140"}, "fb5a9f0e-e306-4cf4-b6a2-7b1b0a309ae4": {"doc_hash": "eb6d45da8da728a048941477df611bf54190b547a9e481cf44637eea9e762a59", "ref_doc_id": "9c26f422-ea35-4946-917e-5016eab0ed12"}, "953e3a37-dc50-429b-ac34-e17ffb8ffeae": {"doc_hash": "d57aec0786be9ec5761694f53aa46273780ad2edcc5f046cbfff54e86da609fa", "ref_doc_id": "9c26f422-ea35-4946-917e-5016eab0ed12"}, "4917c373-bb8d-4f5d-936b-9d5f16ae2f1e": {"doc_hash": "62896624619bd6529b59a5f55fd7132fee6a794e88abdc99f8ecdb038f020ae4", "ref_doc_id": "9c26f422-ea35-4946-917e-5016eab0ed12"}, "0b939c0f-7774-4aa9-b73c-3415eee41198": {"doc_hash": "948793b41e56330602a1962e2d58edc31ec14697627a306838657ef35be483ac", "ref_doc_id": "9c26f422-ea35-4946-917e-5016eab0ed12"}, "b29f7e93-d59e-4d60-aaa6-36ef6e7ef66c": {"doc_hash": "2a8336d6faddc832a89ce9d26a9487a74f958a75064aed584c741f3840b5ac16", "ref_doc_id": "9c26f422-ea35-4946-917e-5016eab0ed12"}, "58bea4e7-1166-4ae8-aa0c-a54346ab7bfb": {"doc_hash": "e95123b7f945d7ffebda567ef1386a956873740a981f3e8ccb799ce0beec0bd1", "ref_doc_id": "19393bcc-7aaf-4a70-a123-7165df7cae9f"}, "3ff0d37c-c7ea-4ec6-bfae-4ada7afe0e76": {"doc_hash": "7f0a1c5ac57dd5a742a6efb3c8647561af5c832e10f1fa9106483534cffcab54", "ref_doc_id": "19393bcc-7aaf-4a70-a123-7165df7cae9f"}, "5a6dcc48-3cd1-4245-bad8-18f28281a24b": {"doc_hash": "e3fc6f82504c32ddde8926a5d563724d5b92b955d68e03b907f5409e9b6b3ef9", "ref_doc_id": "19393bcc-7aaf-4a70-a123-7165df7cae9f"}, "d616b854-2510-473b-9127-2149199f9782": {"doc_hash": "25cf978af0cba8af60347b471be1d35989968272c0cdb1d2b204ad05199789f6", "ref_doc_id": "814ddc3d-e890-4b5f-8252-2560c4136e7a"}, "6666610a-8d21-430e-bb48-4f2822ef035b": {"doc_hash": "80b8acbda7cec3c3b1675c576fd4121e1ee0dcc770d6999eb45e7bf7f7e2446b", "ref_doc_id": "814ddc3d-e890-4b5f-8252-2560c4136e7a"}, "2be8bf24-10bd-4b0b-8b54-0728121e10ee": {"doc_hash": "85d2aa8a144c04b12bfeb55ed8e6f3e2045a4f72b91312f0464863433e29f528", "ref_doc_id": "814ddc3d-e890-4b5f-8252-2560c4136e7a"}, "20ea7e3b-a294-4238-b615-d6e3f8a98d23": {"doc_hash": "0b87b7138967781fe73c85a7d902cf89d0faa12ba9a1631979ebf37789654e18", "ref_doc_id": "814ddc3d-e890-4b5f-8252-2560c4136e7a"}, "f0fbe445-6905-4625-8859-842f44ce3799": {"doc_hash": "e6f3307573d2f94eb0921407432ba35fb2642f766d3e3566ec809ee533621d36", "ref_doc_id": "814ddc3d-e890-4b5f-8252-2560c4136e7a"}, "a30755c0-9451-431d-8aa2-5ee3bdac4daa": {"doc_hash": "9532a5b0ebdadde2320b996f202e73a8f710c7276dec617b8df8820e7bb62ccd", "ref_doc_id": "0a0e4afe-6a76-4665-9740-12633d9b8098"}, "45c16d28-b885-484a-814f-2aab8ab2d46a": {"doc_hash": "180c070ced7b1d00bbfc67058fcbc0a649b1c4854d846ed5a050919ed5d2f7b0", "ref_doc_id": "0a0e4afe-6a76-4665-9740-12633d9b8098"}, "63c215d4-ab98-4e1f-940d-61506cc1c84c": {"doc_hash": "edfd608cba2d6a8acd3bb63f30f43c6f44a4df215f3d3c75c078892f97b33161", "ref_doc_id": "0a0e4afe-6a76-4665-9740-12633d9b8098"}, "f2638c94-7b1e-429d-b4d2-6fad2484d474": {"doc_hash": "16f15f4d62eb5180970175d7e1d626159f9f0603f132b711dcb7667b4d6fbbc6", "ref_doc_id": "0a0e4afe-6a76-4665-9740-12633d9b8098"}, "cf650bd9-2ee0-4f89-9c2a-cae16a23b4fc": {"doc_hash": "998f3962a8170c4d5a5f043d4e981c02fa5b697f9e7148f167a7f6b9a3cbf985", "ref_doc_id": "58f9b62f-4cd5-4a6d-868b-9c239209fc9d"}, "f82eb55a-66bf-499a-a7f1-163a883839c6": {"doc_hash": "ebfc33b4027f34eff6f0c09059711b09f028ee4a7914cca7170ba740f58d7a71", "ref_doc_id": "58f9b62f-4cd5-4a6d-868b-9c239209fc9d"}, "3b05f8e9-63ab-4e60-9362-05647e0c0eed": {"doc_hash": "b364277e16296fb0b372b8f65672e3a0dec5041cb021c1820bf0bd5e92756f25", "ref_doc_id": "58f9b62f-4cd5-4a6d-868b-9c239209fc9d"}, "80bdd7c7-89c1-4c22-8cb9-7b040ffda2fd": {"doc_hash": "451add0c5aa19d962d8c46288b6619543cbb4fb160d1a811762c7422af5e39c5", "ref_doc_id": "58f9b62f-4cd5-4a6d-868b-9c239209fc9d"}, "344e4e8d-1eb9-4a20-b3b3-8a7053f52d7b": {"doc_hash": "fdf0e8f3d9bc43a68e5af6f56628f2d92993dd3754e0f99b797ad6f26bd84d88", "ref_doc_id": "ffcfae4d-8856-4f8e-a7a2-1c4ecc49fd5c"}, "b452c1da-d789-453c-8f86-81899915cae8": {"doc_hash": "76e7cf64c15c02933dc257854be4d83313871999bf79adfa7301ecbe1613288e", "ref_doc_id": "ffcfae4d-8856-4f8e-a7a2-1c4ecc49fd5c"}, "10f1776e-bea1-4521-833c-9832bfcb42c5": {"doc_hash": "a719122a97a475fa93d252198ef3fd8fd50a9480ba97c9d398a503f29ec9c4cd", "ref_doc_id": "ffcfae4d-8856-4f8e-a7a2-1c4ecc49fd5c"}, "8f6d6e1f-d2f6-4789-bde9-43b234346f30": {"doc_hash": "20542ceba44b09448b7cd42517899ffa8bfb2c14f2941b1825174f2cefe557df", "ref_doc_id": "ffcfae4d-8856-4f8e-a7a2-1c4ecc49fd5c"}, "f559afe9-d5dd-434b-930a-90abcd55113b": {"doc_hash": "6922769d3dc425cf4139c8d02cb101608ff3da755041fa12a4f1390dac05ca9b", "ref_doc_id": "8c09340b-c9bc-4e99-81d0-4d3b5f27a93f"}, "50d9a68a-f6dd-4452-bb2f-e2c67611c944": {"doc_hash": "0ab452b12bba3104530ffc62c1233aad654c93fca88590c1c2a119fd74ab9fca", "ref_doc_id": "8c09340b-c9bc-4e99-81d0-4d3b5f27a93f"}, "ec2fb48a-0649-452b-89c3-3f38ccae78c8": {"doc_hash": "726ce6dee94cedae15976d0f3431d3ff84cc4cb27b6549f8213f347025e95a2e", "ref_doc_id": "8c09340b-c9bc-4e99-81d0-4d3b5f27a93f"}, "9988d12a-5944-4e91-abf0-d66469d3ca5a": {"doc_hash": "a922ee4633d7b486ff29e5449a23544ff015ae99d2f02bbd465f7e95e8494926", "ref_doc_id": "8c09340b-c9bc-4e99-81d0-4d3b5f27a93f"}, "4529d783-3a57-43d8-9913-a01811e3cf29": {"doc_hash": "fce450b88ba50b806bde41b692a3886602b88620fbcf8b564ea3edda919dd5bc", "ref_doc_id": "8c09340b-c9bc-4e99-81d0-4d3b5f27a93f"}, "b9f7b424-d60b-417d-a6e0-934fc2d8b648": {"doc_hash": "6ea3062a699bbff4cfe57ede01fdab07f86a03ffb35daf5c14ffb0715a8a1ab8", "ref_doc_id": "8c09340b-c9bc-4e99-81d0-4d3b5f27a93f"}, "b4de9fca-ab0c-4684-8fbc-9bdea3f64b4b": {"doc_hash": "29b6ef7865f32498fb9947dd97361baeaa01faa177cd808f5b1b3c95f7251af1", "ref_doc_id": "8c09340b-c9bc-4e99-81d0-4d3b5f27a93f"}, "8d1eadd2-a059-450e-80f6-c1f8a176eabc": {"doc_hash": "e1299a4e40d78553829859da3a36f722b573341fc97fe62215f4808cbce8e4ba", "ref_doc_id": "8c09340b-c9bc-4e99-81d0-4d3b5f27a93f"}, "62724080-a637-43af-a485-d5d285d3b98f": {"doc_hash": "1829eee7ce292e59c5e2a7c65dddb66e4e224907c199a84781d111839d176a61", "ref_doc_id": "8c09340b-c9bc-4e99-81d0-4d3b5f27a93f"}, "78e4a0df-c49d-4182-bd2d-e64424e27f44": {"doc_hash": "5e1bfd12cb91c7b02e30467b1c6290ef9883d0d4a5fa218e98472eb8f779bbb9", "ref_doc_id": "8c09340b-c9bc-4e99-81d0-4d3b5f27a93f"}, "bbab962b-3ab5-45d4-bfe7-0328168d8d39": {"doc_hash": "346f6cd34ba31b5ea94a7be61ebcbb7ea2a58cde8cad8de3c13fc8b1a467ef66", "ref_doc_id": "8c09340b-c9bc-4e99-81d0-4d3b5f27a93f"}, "24093838-9e4d-4daf-8656-8a5f0dc0c2b3": {"doc_hash": "810665e09fbf209e7ef96c1abc5514b09c5b20325d2405229cfef018d3084b1e", "ref_doc_id": "8c09340b-c9bc-4e99-81d0-4d3b5f27a93f"}, "34ca7382-0f07-4d95-878f-79c5ac5a1109": {"doc_hash": "3e48148e1132d8f13a24e0e4833cc06e0ebbce653211eb7b074111a98fe3ea24", "ref_doc_id": "f11e9218-db47-48f8-bf87-3c3d700db39c"}, "537a9f6c-37f1-4b96-98ce-76bd3379e7fa": {"doc_hash": "1e92284086c22d2ba615d75724a0e34b4aa41d7648f3c818b1719e04a75d061b", "ref_doc_id": "f11e9218-db47-48f8-bf87-3c3d700db39c"}, "67924a08-df78-449a-81d0-67f61824f6a5": {"doc_hash": "80c8c0b855fdc1a79d57e8a5bd9b5f8e41db7e00eda557d3242e14238b5c672c", "ref_doc_id": "f11e9218-db47-48f8-bf87-3c3d700db39c"}, "538124e1-771b-44f9-8773-7a265f55e1ca": {"doc_hash": "eeb858a5e778663bf1190bef705f0df1478aace968f6445b3ec66464a0b69fd1", "ref_doc_id": "f11e9218-db47-48f8-bf87-3c3d700db39c"}, "b64e4239-e8f2-4e03-86c6-374d0a68be7f": {"doc_hash": "fb05fd341aa7be3a2ec1b00d4612d0318d6e2584c6b9f88a22ef0653485599fd", "ref_doc_id": "02424788-2019-4be0-b478-3a4403d07983"}, "270d7f32-048e-4298-a8f8-d5475ed6f4db": {"doc_hash": "58b714ddb2069dbc0e5372037390ba04a58018379dc65445cf921af944f6c755", "ref_doc_id": "02424788-2019-4be0-b478-3a4403d07983"}, "a76b6744-d2b0-42b7-af6a-eb74b01c1472": {"doc_hash": "5605972c436f5d2318d04e29b615c21a98102f2be52e8255c8dee09225540c1a", "ref_doc_id": "02424788-2019-4be0-b478-3a4403d07983"}, "48100188-165e-4180-b1fa-44109f792ded": {"doc_hash": "39d6ca7b5974205da1732d742214d684124dbacad398282a8669961a5dd7d059", "ref_doc_id": "02424788-2019-4be0-b478-3a4403d07983"}, "bce8eb25-2a90-4f2f-af59-4ffc56ca64b3": {"doc_hash": "db0e2f64c32ad19b5453bc492dfb65282f540d15bbeac7292661cd4ab8d2e225", "ref_doc_id": "80ed9bec-3de7-42b1-bc6c-6f7c62e1ca1e"}, "bd754407-c8c6-42d6-8b21-88e7d36e945d": {"doc_hash": "d1e22a0df7443a497e3109d444e103346994b86a8ce548fc577006d758bb2743", "ref_doc_id": "80ed9bec-3de7-42b1-bc6c-6f7c62e1ca1e"}, "a2d34e56-51d6-43f1-bc18-6531d2e772c9": {"doc_hash": "644abc85682997be52e6320949f0c375d32cdf3834e91422e0caadf804e17536", "ref_doc_id": "80ed9bec-3de7-42b1-bc6c-6f7c62e1ca1e"}, "16337efa-18e4-40f9-84ec-c47f5a031f51": {"doc_hash": "7ae998a1e878c314cfeef45dec62d4d3c86ee4bdd944f4c93dd8aca538668d5b", "ref_doc_id": "80ed9bec-3de7-42b1-bc6c-6f7c62e1ca1e"}, "5bd74849-d524-4659-a25e-fe95b3e5380d": {"doc_hash": "d4125aa362ffa5487ed2c33c365ecb663c32ef563673feee432957acffbec3fa", "ref_doc_id": "80ed9bec-3de7-42b1-bc6c-6f7c62e1ca1e"}, "3f90e571-61d1-4103-ac8c-31b801915973": {"doc_hash": "8006aaa5ed63231a5167d90b84670dee66dbe90103fca1ecb787919ff2431e03", "ref_doc_id": "033e4d61-87ff-431e-a45d-74bf4eea80d0"}, "607b2025-b5f4-4850-afd3-73e1e9e5b377": {"doc_hash": "5d334ebb1b217ec3bcdf3e496f7ecb08aa43014dd3ae8f2b646b15928e5ba118", "ref_doc_id": "033e4d61-87ff-431e-a45d-74bf4eea80d0"}, "8720746f-1e9c-4f9c-8065-81233cf191ae": {"doc_hash": "6f6ccfefe725536b2dcd20a47d6bfc79474459e050ad90e039766e89bc64cae3", "ref_doc_id": "033e4d61-87ff-431e-a45d-74bf4eea80d0"}, "393c563d-253f-4de9-a4e4-099b04f03be6": {"doc_hash": "450f4fb2bac7d4caa1a85e60c48babc150b459125143346d15f209fcaba398ca", "ref_doc_id": "033e4d61-87ff-431e-a45d-74bf4eea80d0"}, "951cfd40-1b14-4096-a3ac-8edefbd45b61": {"doc_hash": "8e0f94af092955fcbc7fc0617fe1a407019ee842fd907b54bab673e1b45fa116", "ref_doc_id": "fc551b46-b793-437e-9b58-99461950ca6f"}, "c8d02e82-9ebb-4540-bc3f-20043ad27aaa": {"doc_hash": "0653392cd7274a454beee1c3311bc4029426eef7fdf7d7606411e410d89e6907", "ref_doc_id": "fc551b46-b793-437e-9b58-99461950ca6f"}, "37c06b17-7703-4678-b96f-d344ad99bb12": {"doc_hash": "dfdfabcf2cd2ef3ab128062099a366704abdd049c24baedf01554c3e7df43fae", "ref_doc_id": "fc551b46-b793-437e-9b58-99461950ca6f"}, "5b654dc8-9db5-4a77-b5de-0c4f300e8d5c": {"doc_hash": "11bd4b013e75538d86ee913e5db75bbb9e8d444efa317d4791871ca1f860a771", "ref_doc_id": "fc551b46-b793-437e-9b58-99461950ca6f"}, "60d1748f-c66a-4077-967f-01b8cedbc4a9": {"doc_hash": "695d3fdaad8f19e3685551eaadbed8c7af455a90402166aa4c6e775ed8c0c118", "ref_doc_id": "fc551b46-b793-437e-9b58-99461950ca6f"}, "2e496535-e2ec-4ab9-97b9-5b5113debbce": {"doc_hash": "656f82f5e64a29e36901ebd032e7a8bb1ad09c7e4fb22f4af48e4a198daf3a51", "ref_doc_id": "1f669af6-4a8c-4e5b-a391-e6ed61f214a2"}, "46a6aea0-e272-40a6-9fe2-f36e92efd13c": {"doc_hash": "680243faf168370d54cd018e24667409c6d3e332bf8ed3e32f5d23b7d814085a", "ref_doc_id": "1f669af6-4a8c-4e5b-a391-e6ed61f214a2"}, "d5d8d0a7-5d3d-4be9-9044-219e60047a74": {"doc_hash": "4305a2f2c796853b138d1c8b50a2b0dc5c6cc2383cbbc7fd70132c4d122baefb", "ref_doc_id": "1f669af6-4a8c-4e5b-a391-e6ed61f214a2"}, "968ae840-c8ad-4899-a9ec-ad73d3d7032a": {"doc_hash": "b049d25baf92e4a005e3120ee564cd7f2ae448dd20d8cc36301c1552aa85d719", "ref_doc_id": "1f669af6-4a8c-4e5b-a391-e6ed61f214a2"}, "7ccd7967-6a64-41f5-a71b-ea4d025e145c": {"doc_hash": "8e6b4f95cf51c1c3b165ae5c42d426cfe7835fb6cd5016e0cd3ba8c51b9eeea0", "ref_doc_id": "1f669af6-4a8c-4e5b-a391-e6ed61f214a2"}, "448dd6b3-e0bb-4d21-a69d-e73ec33d4532": {"doc_hash": "763821731b50dcd3aaafcf42744df123c01fa25563ca91022b5d9accd567af20", "ref_doc_id": "9fe69c12-a585-49f8-b1c8-90f65d411cc5"}, "8d80b8c2-2c42-4132-b57e-5ef41908869c": {"doc_hash": "cc7f87c4ab54c3095d381697e6998b7370357f242218e72324c86d72e108f555", "ref_doc_id": "9fe69c12-a585-49f8-b1c8-90f65d411cc5"}, "231b47c6-6612-4685-b050-7df9309db9c3": {"doc_hash": "f96ec05f039d7db1bdcd58e38ec8f4379282dd377da30836f5b62d8ee1ee6d1d", "ref_doc_id": "9fe69c12-a585-49f8-b1c8-90f65d411cc5"}, "8ae08cc1-28db-4050-84e3-7f12a0acf85f": {"doc_hash": "276b070f0f42bbb19c9791121268719cfd7f2800c94220b95eaa0ed6c354b397", "ref_doc_id": "9fe69c12-a585-49f8-b1c8-90f65d411cc5"}, "c2d1d84e-259d-4a30-9999-26c83d3336c4": {"doc_hash": "bd9a3d5321231254d8f61a64b6dcd79dc33223782e784fb8f30a0deb3f39202e", "ref_doc_id": "9020d3c0-ea98-458e-b093-bfb19ba950a5"}, "b40da8aa-59cb-4a86-b053-bae827b6e345": {"doc_hash": "b1e3cd2a92adc05d5fdca35671b10a553d1fa16d6768edf011d69273c9e12eda", "ref_doc_id": "9020d3c0-ea98-458e-b093-bfb19ba950a5"}, "0d82a1ae-9669-4d49-8154-fa057703200d": {"doc_hash": "91f7e341032219bdc5490acaf39dd1bc9ec72ebdc08483da4cb3030ff9aa4746", "ref_doc_id": "9d682bfd-2112-4f76-b0c4-1d3fc55aa6d8"}, "8a7e9471-e116-443c-bd82-f85609fc5639": {"doc_hash": "f6196cb264dc2ee52bc812f855fcfb80ecdc7abffa4c1b1866c06735bcb9b6f0", "ref_doc_id": "9d682bfd-2112-4f76-b0c4-1d3fc55aa6d8"}, "72c08c84-aef1-4c33-aa1d-ce9415dd09fd": {"doc_hash": "5f12da0178c77e1780bf24b834c1b120c0a9a34c8cee332b0cd887023bec9c81", "ref_doc_id": "9d682bfd-2112-4f76-b0c4-1d3fc55aa6d8"}, "b4eb55d3-e7d9-4439-8126-af7e0488f989": {"doc_hash": "eeaa50858c1cfec115b1bce182ffaf714a97760536ee80f918c4ae01a4d8a54b", "ref_doc_id": "9d682bfd-2112-4f76-b0c4-1d3fc55aa6d8"}, "3486ff03-0424-440a-a08c-02f49b68da72": {"doc_hash": "b00195c282cffc9f6d270e73cda07c9594f22b2b1e1aa140f433d753f3366a31", "ref_doc_id": "f2ac32cb-8853-4f0b-bf40-3a908cd68c45"}, "b2d7354d-d242-421e-9294-b9be74b90c74": {"doc_hash": "fb2a7fe40f5648bd0fac35a77f79be26a3d532ef02936b870cb07f41bf9a2abf", "ref_doc_id": "f2ac32cb-8853-4f0b-bf40-3a908cd68c45"}, "94c2d12c-4bad-4e32-9294-6f3f24feda9d": {"doc_hash": "136d5f999c3def3cf564ad277dcc71d4af4dc80670f79484e33af1579126cd78", "ref_doc_id": "f2ac32cb-8853-4f0b-bf40-3a908cd68c45"}, "e4e17092-a87d-4fa5-9695-907c6cd848ed": {"doc_hash": "f4c2a567d1820ece7f3c039d66861bdf9b33377fe3ffbb050d46a33c95491947", "ref_doc_id": "f2ac32cb-8853-4f0b-bf40-3a908cd68c45"}, "381ac082-ae46-47e5-bae7-c73907c6df1f": {"doc_hash": "d8a799acd11cca386300583381777ce2154f1f20491b1818787d069237fadb15", "ref_doc_id": "f2ac32cb-8853-4f0b-bf40-3a908cd68c45"}, "3325ba3d-d290-4020-9e3d-e594243ca8eb": {"doc_hash": "1b5924297134ae8ca13c82cbafc272cbb120cc9fb016d48c37514c3f7ed42918", "ref_doc_id": "786afdd8-7c14-4b62-8622-503cacbf0cdb"}, "d8e581fe-c49b-4e6b-8b36-2e53bc33e2eb": {"doc_hash": "e7f72ee4cfffc4dee55e157afe494ab233de2af911d7f3b148c9f182c1ec502b", "ref_doc_id": "786afdd8-7c14-4b62-8622-503cacbf0cdb"}, "cfa03656-ff5d-4368-a396-6cb3b356f5f0": {"doc_hash": "20c87d82f2346aa3929ba047731dc86c77690385c455fe09107c46a7f049b85a", "ref_doc_id": "786afdd8-7c14-4b62-8622-503cacbf0cdb"}, "43ec9c28-fec0-494b-880d-3edaecb1e4c9": {"doc_hash": "9233cb9f1a146b2334d34a969a4ca837c68cc6f4e7562b21410acc2da48dce0b", "ref_doc_id": "786afdd8-7c14-4b62-8622-503cacbf0cdb"}, "fcf0a4a5-36da-4ee3-a857-4fd021be8f66": {"doc_hash": "4ac26d1d31c6b81b15f0e1d83462d0a4a6a7a47be306b3ade7ba7552124bfc77", "ref_doc_id": "3724df83-66e6-4336-ad5c-6eb97505d14c"}, "cc3d2d64-4f33-4c92-87d3-1e1daa229155": {"doc_hash": "d73ab5d7eda190c8e48cd8368519cf59dd8e64007a6bd26938464a7e1995fc4c", "ref_doc_id": "3724df83-66e6-4336-ad5c-6eb97505d14c"}, "15c9e7d2-1c89-4e8a-bde9-de53cdc7dc35": {"doc_hash": "a0eab953f57ac8bbad22b68393354006616a7b2caabc994f7e247680155bfb53", "ref_doc_id": "3724df83-66e6-4336-ad5c-6eb97505d14c"}, "66fba681-53a6-48d7-a112-798324730a22": {"doc_hash": "20033d5049f57a18fb28dffaf5323c38fe872479313bd93ad49bb5e405c9c286", "ref_doc_id": "3724df83-66e6-4336-ad5c-6eb97505d14c"}, "7018f358-532c-4074-9d27-b36655a82e22": {"doc_hash": "ac35f5623e7bfa07d1d2d5b2ce5065c73755223c5915389ebf2e0e040b6dbbe6", "ref_doc_id": "40c02fcc-f4c0-4628-9529-19a1b7cee237"}, "8aaaefde-e2dd-4933-9b15-06de41397939": {"doc_hash": "ecc3a34cb83915e2701f8a11eb15d4787acfe85cf3856852e5c4adc87acd44e5", "ref_doc_id": "40c02fcc-f4c0-4628-9529-19a1b7cee237"}, "732720c5-d8ca-4898-8730-dd6bc86812c1": {"doc_hash": "cfb3b19cf2d4773d8c48fc43583a5b12bc85e6012fc01be20f40118034e0ffa0", "ref_doc_id": "40c02fcc-f4c0-4628-9529-19a1b7cee237"}, "9c946997-cc83-41de-a59c-6ed89059e7ed": {"doc_hash": "f5403a000732d3926d7b1d7c68c3891e4b5106754268e7d60985582f74ed9e2f", "ref_doc_id": "fe05bb37-1ac3-4bbb-95b3-55d05f180eb1"}, "352b5a2c-f77d-43b9-bff4-f7a9dfe07ee5": {"doc_hash": "9b91196ba197a429dabc6636802cc6f9a02a653a5e8d06cc77e3864a0bc85066", "ref_doc_id": "fe05bb37-1ac3-4bbb-95b3-55d05f180eb1"}, "17681f3e-8afb-4626-a2c9-971b0b60d1a6": {"doc_hash": "ff0baf621e67ba96549814242080a295a687de25dbbc3382049a3bbb051a2a6d", "ref_doc_id": "fe05bb37-1ac3-4bbb-95b3-55d05f180eb1"}, "63cd6adc-43b1-42ac-9116-933b85930fb4": {"doc_hash": "a01e98f0dac39ab882645b7ebb96c95b0b91664460192100e69d103789de0203", "ref_doc_id": "fe05bb37-1ac3-4bbb-95b3-55d05f180eb1"}, "339c273f-e338-4c9b-a835-a980ade1db00": {"doc_hash": "c455ccf34392daa08cebf7a7c70ce06be544f1338feaeafdc317daae8297752a", "ref_doc_id": "f71218bd-cd96-4a39-8a5c-a83ac287d7e8"}, "0cfbf6b0-53c7-41f1-b190-6604128f2790": {"doc_hash": "cd579eae5cd3e3e05697bdf1c1d8b255b2d315644606fc5a37396a0a3b571d1d", "ref_doc_id": "f71218bd-cd96-4a39-8a5c-a83ac287d7e8"}, "5a834743-33b6-44a1-af90-680d35765b31": {"doc_hash": "b52bb7f21c879a60d4140d0e7b81b5ae0c717d3ee206aac3ace9d62c6435b63a", "ref_doc_id": "f71218bd-cd96-4a39-8a5c-a83ac287d7e8"}, "192d1825-da2c-4b53-950a-9890490f33bf": {"doc_hash": "e260fbb6f436fc0a2b03d9cc6a1549a196f4f883181f33b775cb22b49e54be7a", "ref_doc_id": "c44fa078-fabe-4a04-b679-98a3d6e9d1e7"}, "e44d3a86-03ec-4479-be4e-8f2cf1868a29": {"doc_hash": "080c3825a06f5bed3e67b09c215aac90009d8b3dd8dba0b5e90bdd9d94d9be5b", "ref_doc_id": "c44fa078-fabe-4a04-b679-98a3d6e9d1e7"}, "09bfdbab-8c90-409c-8d33-47b476827205": {"doc_hash": "e82f08d2d2dbc691d0fa91ae53a02d3421f87a14153e228df4a23bb5aaf0c20e", "ref_doc_id": "c44fa078-fabe-4a04-b679-98a3d6e9d1e7"}, "bc4d3507-759d-45cd-a9d8-aa16b220a4e1": {"doc_hash": "91e36155dbdb800dae1c5e74d709933abbe4d071a704c392b0fcc388fd4a167a", "ref_doc_id": "c44fa078-fabe-4a04-b679-98a3d6e9d1e7"}, "4719f466-7177-4845-9a4f-833732001f7c": {"doc_hash": "bae33cbc93f2d526940f9551127fa11cedb95c368fd6cdb6c45685f3fc04c4af", "ref_doc_id": "c44fa078-fabe-4a04-b679-98a3d6e9d1e7"}, "16203309-295c-455e-a3da-8f5451dcce82": {"doc_hash": "a211c02971d6de3aa300d0a8e65c5e0cc81535f9a28f2d7e2caa0747f3829153", "ref_doc_id": "c44fa078-fabe-4a04-b679-98a3d6e9d1e7"}, "aa01bc3a-ef98-4438-9b6b-5a0a4073ef22": {"doc_hash": "4606fe8a17abbe962fb386ff4ea459252f8b2e2e620e3d4f4367e19ff04bce52", "ref_doc_id": "c44fa078-fabe-4a04-b679-98a3d6e9d1e7"}, "801743ea-ad6a-4814-9da0-e8ff56843ecb": {"doc_hash": "43493989d89676a7eb7ba1c18a23b298cbc340b31f90ac7822feda11b079f2c0", "ref_doc_id": "c44fa078-fabe-4a04-b679-98a3d6e9d1e7"}, "1ec20dc4-df55-48bd-b572-9b002bfad77a": {"doc_hash": "4c8fdb485b6439621009d3390dae13acdc96d965420b845a49e4286a4e570b7b", "ref_doc_id": "c44fa078-fabe-4a04-b679-98a3d6e9d1e7"}, "38df1837-5840-4389-a189-7896b715a876": {"doc_hash": "761a05373681b7958f56cba9adc2f54914e41e1aca1ed6eb2453356062616a42", "ref_doc_id": "c44fa078-fabe-4a04-b679-98a3d6e9d1e7"}, "015e6aaa-24b6-4db3-90d3-419abafe4559": {"doc_hash": "5386f089844ea834715bda68d20c08ca3a886e399e403902793aeb465a9d17a3", "ref_doc_id": "294d42ff-4eee-42ef-8901-13835558b8ea"}, "b54977b4-c6c6-4312-ab5f-b78ad510318d": {"doc_hash": "e57b28400202f8705e4de3c3571b0a1cec8fdc67c5165b76817ecba2698c9087", "ref_doc_id": "294d42ff-4eee-42ef-8901-13835558b8ea"}, "fbba97c1-b7ba-4d8a-882e-fa143267550f": {"doc_hash": "55f07bfb28a7a3d30f0f603a2e2cd81ddd4a1228ea3c59f9abac8f1ec435366d", "ref_doc_id": "294d42ff-4eee-42ef-8901-13835558b8ea"}, "abf0869e-6647-4429-9896-d90ff94409c1": {"doc_hash": "e2c74c17885742c9bae1a6d99d3d097c2b470e76b198a1a4279e1491846b8e78", "ref_doc_id": "294d42ff-4eee-42ef-8901-13835558b8ea"}, "a5e9221e-3013-40ca-8b49-f0847546dc00": {"doc_hash": "0c9d6d2db65e4bd734c2c6e904092802805fdca7f3fc9a373eea26b955592400", "ref_doc_id": "d0ec3e5a-73d6-496c-96d5-f72de017f5c6"}, "a85a2431-4949-4b1d-9c35-b8c0b90fb203": {"doc_hash": "8e9d858016b51c47f4f62a3259a51f35d01a07d6d4fffc70d95e3e036febd75c", "ref_doc_id": "d0ec3e5a-73d6-496c-96d5-f72de017f5c6"}, "cc447517-f0f7-4792-8fd3-b484c8495f2c": {"doc_hash": "295a8275df450f172f30b5f6da8b7fc0427f1c4129d74acdf23e2ef9eaef6ad6", "ref_doc_id": "d0ec3e5a-73d6-496c-96d5-f72de017f5c6"}, "7c2d01c4-84de-495b-b34d-79f90ed3c3a1": {"doc_hash": "2e893ae1e9666e0cab59454d0ff1665dbf984674c6eb9d2317c80e8f5eb367fe", "ref_doc_id": "d0ec3e5a-73d6-496c-96d5-f72de017f5c6"}, "d0a032b4-e6fb-4b96-8c24-19d369466173": {"doc_hash": "e8f8b6b6892bc2088457dbe22c05014f3db4801539483d0dada98f8db3e719e7", "ref_doc_id": "d0ec3e5a-73d6-496c-96d5-f72de017f5c6"}, "dc6dde66-2c7b-48d3-a4b2-e73b88f13dfe": {"doc_hash": "53b35c952f58107ce3d5ea33c90c0679c768f3c0d01420bc75c2f7ac6126aba8", "ref_doc_id": "4043b8fe-9170-4006-a512-902fbe629cbd"}, "def7a4bb-3a1a-4781-9470-e40c53c84d92": {"doc_hash": "ecd8022e3053c4a65b36b0f623b0d15ea50cbc8b5a781d6d3155b1f83d3df99e", "ref_doc_id": "4043b8fe-9170-4006-a512-902fbe629cbd"}, "16c592b8-2b29-4a3c-b6f9-62b014f337f0": {"doc_hash": "a40f965a2d53b00b2eded10d6aa30b8054733fd2dc04a79d36c921422648291e", "ref_doc_id": "4043b8fe-9170-4006-a512-902fbe629cbd"}, "b16f0bc6-0c24-4392-b001-071fc5cbaec3": {"doc_hash": "f605251795f0bdff1717ba62873cfbb9e6ccfe4409c0060f54ad11ac86c1373d", "ref_doc_id": "4043b8fe-9170-4006-a512-902fbe629cbd"}, "7b4eb05c-b17b-4d06-868b-0b11ec8d3674": {"doc_hash": "53cd90672756daa6bc2fdfede026d3fbbe6f1168565759a63a5ef9fa9e4c74a6", "ref_doc_id": "822085ed-0a59-4fff-ac44-48beec94b85a"}, "b8f3a337-bf5d-48a8-9498-c59c12b222db": {"doc_hash": "c92bf02ff2b6ae87f170a42d8a1c0b864d0192927563569f77265015104d5010", "ref_doc_id": "822085ed-0a59-4fff-ac44-48beec94b85a"}, "ad53a63e-01fe-44c5-b758-26938a14f9ed": {"doc_hash": "b38a1463c97be6d3024a0ea286581b7cc4e55b6d5e7d328f0583669cd7d75fec", "ref_doc_id": "822085ed-0a59-4fff-ac44-48beec94b85a"}, "206f525c-3084-440b-a80c-aa8daf5ec796": {"doc_hash": "ec8a30481151a06e0ec8edaa185bab27c81cf321dad94a576423edd66f67dafa", "ref_doc_id": "163f57e3-00fa-46b9-af06-92027d7739ab"}, "2c333a53-d5ef-4abd-b2d4-049b72fbebb9": {"doc_hash": "183ec4d4d3906451d7b0f7cd8d71f77c491d1e6622722bcdba03cda8c8fd8b88", "ref_doc_id": "163f57e3-00fa-46b9-af06-92027d7739ab"}, "388ef834-e2bc-4373-ad73-c45a5660a97e": {"doc_hash": "5ec65c8ab410e6745bb4f9ec3fd00d34f2313bf7a2d8188ceabe57468df51eb0", "ref_doc_id": "163f57e3-00fa-46b9-af06-92027d7739ab"}, "e360c4b2-5438-4fe5-93c8-29dc68a0f7cb": {"doc_hash": "f987cdf769f74a51026c588b7cadb1f35355cecf19b9d01e922621a3b778d873", "ref_doc_id": "163f57e3-00fa-46b9-af06-92027d7739ab"}, "f62b90cb-44d9-4643-b66f-15b2cb832f11": {"doc_hash": "efdcbcd6efeae67e4158bc4bd16f3ae0ce6fbb64681c31820cd957a330e75562", "ref_doc_id": "15668294-2e00-4059-8c83-b1f6c9f7058a"}, "93fa4e33-c1cc-4b3b-9b32-e98bc20264bb": {"doc_hash": "04dd1ef24f369231cb2f5384c7125e040290222600e0f94f9dc3e04412a14063", "ref_doc_id": "15668294-2e00-4059-8c83-b1f6c9f7058a"}, "9577e07c-b4dc-4143-b677-6268ebda0584": {"doc_hash": "26d24c73e188bf327f272502200891068e3c030bb9753859dd1d54e73de834e0", "ref_doc_id": "15668294-2e00-4059-8c83-b1f6c9f7058a"}, "cb55c0c7-f019-4d66-93e2-62766d2ca7b1": {"doc_hash": "e72613b8e53d66c56f3f7fc094a8e97ee344fe6ab7f975fdfcce017c472b6078", "ref_doc_id": "92d1e148-39e4-4191-bffe-29e0c382ed6b"}, "534cce68-c8bf-45a6-8f8d-625ec15b52cb": {"doc_hash": "36212c88c787c1debbe7892ea8a023ed5eb16cbcfa9fa95df67665b21151f594", "ref_doc_id": "92d1e148-39e4-4191-bffe-29e0c382ed6b"}, "25d3115f-bcb6-45da-9182-1dde299cca90": {"doc_hash": "d49a8d193ea896ae0ed61fd554c295ad495c2bdea3c1a48d5188ab58fef1f188", "ref_doc_id": "92d1e148-39e4-4191-bffe-29e0c382ed6b"}, "f8b823b7-5a55-49f8-8f11-5549440d9f91": {"doc_hash": "2e7c1bb866e8086f6f693f7d048003292a6eb0a0463d4b7ba1e2a270a6409ff0", "ref_doc_id": "92d1e148-39e4-4191-bffe-29e0c382ed6b"}, "22225985-0132-49ce-8a92-8fc72c2ac428": {"doc_hash": "1796a4dfe296405ce6bc12723d391033b58d6779ff445eade25fb41fd2884502", "ref_doc_id": "49984c49-8047-4f24-b368-c323ea28475e"}, "ad9e678c-aa5a-45d0-aafd-05149455b1b5": {"doc_hash": "8c398bb50b5217525778abe66ca4ef1130b17f7b2b5071bccc28e0819ae58043", "ref_doc_id": "49984c49-8047-4f24-b368-c323ea28475e"}, "6a31b7a5-2f0d-4ad7-8b74-a0baae7b555f": {"doc_hash": "c6d70e9684905f060129be18a763c17107cc8a5bb3d9e10463de766420847ced", "ref_doc_id": "49984c49-8047-4f24-b368-c323ea28475e"}, "2b220322-7f99-48ff-b12a-b8bf18050123": {"doc_hash": "162aa8dc9172ce8aca72fae536494bfaf432678a04f2f63fec1448e1384f6c78", "ref_doc_id": "49984c49-8047-4f24-b368-c323ea28475e"}, "34a49539-768e-4306-a218-d966d40bb1eb": {"doc_hash": "85c2eb7325feb38907f1b8c0a62e50890b8253cb67ccc3505bf46bb6e6497a93", "ref_doc_id": "eeca3be1-f689-470a-8579-92f5bd5b94f8"}, "a82c9c59-5dfd-4d1f-af51-657f3fab04ea": {"doc_hash": "419d5be3cfdf2fec175e25a3223af4a173e9f034d890764bda3a9956bd0693fb", "ref_doc_id": "eeca3be1-f689-470a-8579-92f5bd5b94f8"}, "f69d6d7a-1a66-4cc1-9764-e2e458a7295f": {"doc_hash": "f0c0cc164e64dd40f0fdf8d84f50f57d3815d40cfec45e57c8846cf54fa0185a", "ref_doc_id": "eeca3be1-f689-470a-8579-92f5bd5b94f8"}, "9b223462-52a5-4956-8c3e-810a14c77536": {"doc_hash": "7000d603d4619383e4408684f759cd249ce461d4918524d25fbcaa90835e945c", "ref_doc_id": "eeca3be1-f689-470a-8579-92f5bd5b94f8"}, "d8f02931-738a-488e-a5d0-4c28cc65e2ed": {"doc_hash": "1016ebd8168a62638e48234800cd669144801e1c001d4d30e4465d13d0f0485b", "ref_doc_id": "39b782aa-ccc7-48e3-a0bd-7c22c696b36f"}, "371fd1a5-8921-46a4-83ac-8a9e0788ab95": {"doc_hash": "93c395cc6f213c3989c8018e06530146cb24555a4de3ce046b52c8ab9f2a59b3", "ref_doc_id": "39b782aa-ccc7-48e3-a0bd-7c22c696b36f"}, "87df5e70-2944-4abc-a728-e8d839c47103": {"doc_hash": "82abb4b380c1dbd198f85b3aa8e54f83ca990110ca3c2e4fa681bb6d1c419aca", "ref_doc_id": "324cafbc-56c7-404b-b679-7ed0ea4b23d9"}, "985ae504-25c1-4f63-a8f8-c7b28dd690b9": {"doc_hash": "8aff81dfc10e4455a34e5ae9ccc0e43abf5ed103326447dd7e0e5cdbd094bea8", "ref_doc_id": "324cafbc-56c7-404b-b679-7ed0ea4b23d9"}, "09743600-2dbe-4a7b-bea5-3e1d09390023": {"doc_hash": "1d25376a0c2c43131d2ceb1689a7f93bef63490eda7d2646e55b5ca4a60b6960", "ref_doc_id": "324cafbc-56c7-404b-b679-7ed0ea4b23d9"}, "a8b58843-89ce-4db3-b16a-31dca33f9e38": {"doc_hash": "78760133b7a0a0594d2858882b56485ecc5028f4e98a1b2ca1adc416e5bd04ee", "ref_doc_id": "324cafbc-56c7-404b-b679-7ed0ea4b23d9"}, "089fbe81-bfa7-4cb1-9b5a-9aa4a231ce5e": {"doc_hash": "5704de50b053db066db0b5d810a740f92eae75e3ede1d0b5f9d22c60a305d663", "ref_doc_id": "324cafbc-56c7-404b-b679-7ed0ea4b23d9"}, "f8325e61-2985-4bfb-987a-a8ae2fa3fc43": {"doc_hash": "bb38939bbb3e1345ce451816ad1f25f367f3501704636f1e87b1950b3d1f9bc1", "ref_doc_id": "324cafbc-56c7-404b-b679-7ed0ea4b23d9"}, "cd358be3-afc4-4fb7-9b49-55586b6c9298": {"doc_hash": "68f89a12d706e4e26fe96e1f77a4318f9c8dc0399188ea9d6a0066bb7d0b1a22", "ref_doc_id": "524180ae-69c6-4a66-87dc-435ff90bbbe2"}, "1c722851-0779-44b8-a2ab-896ceff2ba75": {"doc_hash": "3a9b9e921ccc42c04f72b5f075fbeb4821aa41c37e97c61416a2a505a6f6b1d7", "ref_doc_id": "524180ae-69c6-4a66-87dc-435ff90bbbe2"}, "54e3f422-fa81-4877-a646-9e4f48b07b9d": {"doc_hash": "37037d13978d7240d6e7c77d48ff62b005607eeddce4f56d09ab871389204808", "ref_doc_id": "524180ae-69c6-4a66-87dc-435ff90bbbe2"}, "c7446b39-edf0-46c4-bbfa-49b4ec76863e": {"doc_hash": "74ac1f33c1913d430e4afd729f4c96d0d7d2e2d81d1889d9483aaed270906005", "ref_doc_id": "524180ae-69c6-4a66-87dc-435ff90bbbe2"}, "c7ee99ec-2767-4115-8495-04f916ddc3cc": {"doc_hash": "19d51347885fd951101daa66a68dbdcd3c7cb2bf829ff6a3526679d9b9b8225e", "ref_doc_id": "524180ae-69c6-4a66-87dc-435ff90bbbe2"}, "595c0b11-b3e7-4840-9c73-bb1ac92b924a": {"doc_hash": "dd42827fdd129213912eeda0e4b1a902243df35b310b9ac1b91721eff95b8f4b", "ref_doc_id": "0af58c14-b89b-4e71-b0c0-f44aecc01df8"}, "e511e3c2-9221-4837-ba18-6686c38be5ac": {"doc_hash": "5ea33ef73f1a1898101bda85f5884a222cafded1bc6005b703dafbaf2b43b977", "ref_doc_id": "0af58c14-b89b-4e71-b0c0-f44aecc01df8"}, "b636bcb5-d351-4b53-803c-7d1eb05678ff": {"doc_hash": "ea78b13e44d53070850ed6179416b36649d0a01ca378e0a5a3a65637eccfe83f", "ref_doc_id": "0af58c14-b89b-4e71-b0c0-f44aecc01df8"}, "b1973930-6a86-4dfc-b1a4-0cf7d4760fac": {"doc_hash": "001baa2cc73d22f1a6e18323b2edc5b1f37c3b66afa75b069599f0ec7f4756e9", "ref_doc_id": "0af58c14-b89b-4e71-b0c0-f44aecc01df8"}, "6fe93808-454e-4eaf-b721-806769a9bb15": {"doc_hash": "32ef42b7fd938fefa5383d99732cff3a81d18d049d7823ac0e67f92f68ffd36d", "ref_doc_id": "0a6424e4-e254-446e-ad5f-1516a078a21c"}, "def23383-0da5-463e-8a2f-248cd2cfe5bc": {"doc_hash": "28c1b7cbf38f0dba74101517aada126e247badcc7b4ca223136d5234db520ad0", "ref_doc_id": "0a6424e4-e254-446e-ad5f-1516a078a21c"}, "0ed8b746-3bb7-4c03-87a4-9ac4af8fc31e": {"doc_hash": "6660204d6aedb6fae87dc40b0fef702beb342dc3b36099e96db669bb47d8d8b0", "ref_doc_id": "0a6424e4-e254-446e-ad5f-1516a078a21c"}, "369b8ffe-a535-4ab4-8ff2-1b9f7b88d747": {"doc_hash": "c978c3b7cc76a1ed418f6e5ac360d90ba1dea77e370dd42e1a0295719cf2b1d4", "ref_doc_id": "0a6424e4-e254-446e-ad5f-1516a078a21c"}, "af751b91-a0f4-4d61-9ca1-99b542f60916": {"doc_hash": "aafd9e9b314eb5beaaa3513eaf4e64f1da762952223c7ac09e6fd0b92de66a10", "ref_doc_id": "0241af91-0a6c-4c3a-8415-514c401f8eec"}, "a535a019-debf-468c-beaf-28f050bb29ca": {"doc_hash": "31a701a8722b2ba1b5f2510c96a0a9f814bc71b2f2213ac406adb48fee7bbcd2", "ref_doc_id": "0241af91-0a6c-4c3a-8415-514c401f8eec"}, "ee02cec5-bf9f-46bc-97b1-beea8709e4a7": {"doc_hash": "ac67e1bd16cbaa3e14dc85f95f39480a2c726df89a624fb741f50271f7d29277", "ref_doc_id": "0241af91-0a6c-4c3a-8415-514c401f8eec"}, "be92b2de-809b-4da8-824d-cb203657c46c": {"doc_hash": "915dfdbbe70afb4014f93be85cbab3119216b217dafb23098f554f74e0d36d14", "ref_doc_id": "0241af91-0a6c-4c3a-8415-514c401f8eec"}, "df20729e-935e-42c6-a041-62105847891e": {"doc_hash": "c35ecc4acb8fe4ab76b245825af9f32fd171755b8dc456a848d603d273271bc8", "ref_doc_id": "3f0176f8-5129-4bbb-8c61-038dc7513060"}, "61a59674-815e-41d6-a897-95f2b48ff29b": {"doc_hash": "b530dcbdc9d6d2306ec25fd5b9fc3aef1c5ae7a30016a935d83e2c64811f426a", "ref_doc_id": "3f0176f8-5129-4bbb-8c61-038dc7513060"}, "a7f44c74-38db-4fec-91ca-4aa9de208d0d": {"doc_hash": "996d9e7e0a0747ff0249983869c921083f67b508bb8de415352f01a726062701", "ref_doc_id": "3f0176f8-5129-4bbb-8c61-038dc7513060"}, "24ebc2b3-20dc-4ec6-ab48-7704567ecf90": {"doc_hash": "5cb3820bacd28123afda4987bba3b15fc94c2a00da5e19c21104fa2ffb977b22", "ref_doc_id": "3f0176f8-5129-4bbb-8c61-038dc7513060"}, "ad77265a-ed10-4356-bd52-4265ff1070e3": {"doc_hash": "50865632b7cb816db73861976068b503354d4ebb8cb7fd8d148e994473081402", "ref_doc_id": "3f0176f8-5129-4bbb-8c61-038dc7513060"}, "84a8c831-c6dc-4ea5-a62b-69cd31631f93": {"doc_hash": "e7b0b57bf67b1c3744d8bf7a48b8b9b469aa8d570d8a9f7521db790d1e3c6738", "ref_doc_id": "1788f98a-120e-4cf7-ab78-b49e3b4383fe"}, "41b82351-bf95-40c6-84ab-dd6d1d6bd769": {"doc_hash": "ce7b279e8b088c7518e74fca708e263993a13ac6a547c665811c047e5177dbdd", "ref_doc_id": "1788f98a-120e-4cf7-ab78-b49e3b4383fe"}, "775ec11e-d34f-494d-9736-a124b5b79b9c": {"doc_hash": "7538a9fd513321ff9c585f04ac90a48233dc3b3f8b47ef84d13718b7a74c5b16", "ref_doc_id": "1788f98a-120e-4cf7-ab78-b49e3b4383fe"}, "8e026835-2536-4eba-8a03-e49b770da1b8": {"doc_hash": "d7a7ad746c6bb44045d85b8858d447633b816a9409e7b1a6f060e6d97658f8a7", "ref_doc_id": "1788f98a-120e-4cf7-ab78-b49e3b4383fe"}, "6468bcb2-4947-46fa-bd76-75e7c41defd0": {"doc_hash": "3603ae5abc0888e2117b70580c5e7cafcaf1cf8ad283780c60bca0ce74ee7574", "ref_doc_id": "1788f98a-120e-4cf7-ab78-b49e3b4383fe"}, "fcd8e912-0195-4e24-9c21-8509b225fbbf": {"doc_hash": "a509c4dc2b4f2358486de369786b45a3eb4a2a91d4b77812248fc042812ccd8e", "ref_doc_id": "bb3697bd-299a-4dfa-90d5-53dd8c4e0aa4"}, "d363544b-f39e-4422-9265-f88b47679c6d": {"doc_hash": "0d511909af07b842ecf6827ea681a77149ebf74811bec75fd381e9af21eee526", "ref_doc_id": "bb3697bd-299a-4dfa-90d5-53dd8c4e0aa4"}, "60102647-dcc7-41b6-bbf6-fa829bfc87ec": {"doc_hash": "9c6cbad3493d48f434ca7053157e76bbbc685abd4ede101d374667b222e87582", "ref_doc_id": "bb3697bd-299a-4dfa-90d5-53dd8c4e0aa4"}, "28c624cc-f041-4233-bb90-d46080f91ebb": {"doc_hash": "509aeee5ba321c7afd6b278320a13b2f2ba71c0b57d0d15e258e9bd46dc5c433", "ref_doc_id": "bb3697bd-299a-4dfa-90d5-53dd8c4e0aa4"}, "511c6606-3983-4f53-aeb2-382f61a2184b": {"doc_hash": "a57715edfe2718df03d30416eb3639a45633490f410fe42cff7735ac6242b328", "ref_doc_id": "364aebea-6a85-4c68-bd51-9a0c1fe3ff8d"}, "e4adfc69-a069-48a4-b7b0-08dd3a729406": {"doc_hash": "dba189fb7d13f2d5a873630c60c9c16c58e2e15ce6a589d83e5d4237fa22258a", "ref_doc_id": "364aebea-6a85-4c68-bd51-9a0c1fe3ff8d"}, "ea37959c-36ec-4424-b249-d509ab852e41": {"doc_hash": "1880c7d64d675d71479513b7d9718d2720e75c4a8b1ecd7892e5f7da721d5239", "ref_doc_id": "364aebea-6a85-4c68-bd51-9a0c1fe3ff8d"}, "5b83b813-8a9f-4100-a4dd-355d4d3acf8d": {"doc_hash": "00fcb96c5ed61dd1ee09dd8db5b7ef628605c159a7f7bd59de2c04da38904964", "ref_doc_id": "364aebea-6a85-4c68-bd51-9a0c1fe3ff8d"}, "eb1ceaf2-d210-45ec-8739-d19d19229471": {"doc_hash": "a94907c94f8c98a08c55d7f37f169366a16fcdfc0bed4f8561923bfa0af09d1c", "ref_doc_id": "364aebea-6a85-4c68-bd51-9a0c1fe3ff8d"}, "a373ce28-04de-4522-9d1c-143baaf09faa": {"doc_hash": "5ef89d9df9621309ace5eabf3829d40e2f786fc7368df5488b9b04391dd7a8d7", "ref_doc_id": "198fece2-5bff-4144-8779-547f01b4661c"}, "945c0b0d-da50-4ead-b037-de677f0acd9b": {"doc_hash": "60c3d3f462df43b38640eb6f1e99067273693e82695f2a0d796248e66e0d5f39", "ref_doc_id": "198fece2-5bff-4144-8779-547f01b4661c"}, "43845168-c598-4d15-aaea-3442fb59738c": {"doc_hash": "a8da699bc3d29cb85d99519f6a10f342fbc1d8b86cdc23c21132848e757dfd26", "ref_doc_id": "198fece2-5bff-4144-8779-547f01b4661c"}, "964b65b3-b31b-4859-9c5c-e03364bddfec": {"doc_hash": "ee93d8893f6b13f1ddcc2266062106bd61e47d5672fa666f6120292933adb981", "ref_doc_id": "198fece2-5bff-4144-8779-547f01b4661c"}, "bf8120ab-67e5-46b2-85ce-99d3a7f94c11": {"doc_hash": "5cee712813b0c96a4121698808f80cd0feb7b0a60413c4d83e7cdd66729d1c28", "ref_doc_id": "198fece2-5bff-4144-8779-547f01b4661c"}, "fc613e71-6d0f-4670-b40f-af0174adf3a6": {"doc_hash": "e2ec01f562589348b4c9f04c287ab17a6fe827f7489f1bd20d199f2333135a06", "ref_doc_id": "198fece2-5bff-4144-8779-547f01b4661c"}, "61e3a267-bbed-4df5-8c56-93b233d3d696": {"doc_hash": "d0982e014c0df1fdd7c7af6d9df1f1283ba32b32fb23dd5d945170f41c3ba349", "ref_doc_id": "a4960151-cc26-4057-8056-565439ffd414"}, "05405b3d-281a-4186-9d2c-9b81ffba9eaa": {"doc_hash": "4e60de22702d2c2a31004ecbb9078edf8d1169d38a5860b03e1a48a42cacc933", "ref_doc_id": "a4960151-cc26-4057-8056-565439ffd414"}, "0d89deec-c3b6-4ed3-8676-3c8d80cfba25": {"doc_hash": "4f85c6589240cccd70fa74d628548e9ee675cd6edaf9547c8dadb03cacd1ec09", "ref_doc_id": "a4960151-cc26-4057-8056-565439ffd414"}, "091c4290-f2e2-4bfd-bc40-b9fc6f17d198": {"doc_hash": "bf39028256f42718ef03db72b49a4ebc61ec4bd1a0148aae07844dc8b15804c9", "ref_doc_id": "a4960151-cc26-4057-8056-565439ffd414"}, "c5875733-82e8-4baf-8b09-2c911cf02b4c": {"doc_hash": "431c2e48060942f95222c837301c72b0fc32bdf62316dd9c908492908e5de776", "ref_doc_id": "a4960151-cc26-4057-8056-565439ffd414"}, "0da4b06c-6cde-477e-a489-bc841281c89f": {"doc_hash": "f69cd87d6d4e32b57e70a9bb2ac793500400e78f6f19a1f91d2352b4451e5d6f", "ref_doc_id": "a4960151-cc26-4057-8056-565439ffd414"}, "bec9f5fb-2778-4e3e-a321-ff2c79883c45": {"doc_hash": "13ddffb6bdbe115718e6596f5c4a7ad6335836638fe784b6196d3b43d301bb3c", "ref_doc_id": "a4960151-cc26-4057-8056-565439ffd414"}, "8f1131c9-1525-4199-955c-66e020f1c690": {"doc_hash": "7aa7a93f7a4a7db813f485b597397bf10c8c33293a66bca90adbaf7e7804800e", "ref_doc_id": "a4960151-cc26-4057-8056-565439ffd414"}, "9862ca5c-3166-48b0-828c-21b8b61ffb53": {"doc_hash": "1a06d75d1dca85c00f1dbe5e0802734c79b13016ce53074bce9916e508930c42", "ref_doc_id": "834eaff1-0a04-4c4b-a7fe-b3859be73a69"}, "ed8d5543-956c-4f20-9f13-d9bd3d42c116": {"doc_hash": "663f92a9058c031f18df3009f8a4c2d720969b1460dae8a107d45dd428cd032f", "ref_doc_id": "834eaff1-0a04-4c4b-a7fe-b3859be73a69"}, "2151e821-2644-4e0f-8783-f09f84eea08c": {"doc_hash": "af74ca055e7427150f905da00f2c6f565e9202869852b8aa076a05e52c0b42b2", "ref_doc_id": "834eaff1-0a04-4c4b-a7fe-b3859be73a69"}, "f87720ff-a172-48a9-9d3d-c82aed6981d6": {"doc_hash": "2ba2df8ea878f7e37b1db9c7fbeff56b316958cf74aa1ae981d9a1672e89cfcd", "ref_doc_id": "834eaff1-0a04-4c4b-a7fe-b3859be73a69"}, "6d027f33-c6a8-44ac-8e7d-6c3b018395fd": {"doc_hash": "e937bec90e482b4682f1fda27647d8bd31075aeb8dee4c289e795c3232edd9ce", "ref_doc_id": "92c32a37-38b7-47aa-bc5f-5b3b3abdc47e"}, "19a8b33e-e0e7-4055-8619-0616ae29e740": {"doc_hash": "75494f646d110e2b2ef8e643a89a330ee502d9e98f81af5641b3e1641f0d4bf8", "ref_doc_id": "92c32a37-38b7-47aa-bc5f-5b3b3abdc47e"}, "6b589868-02c2-4f87-88e7-866141f66d52": {"doc_hash": "042316e33ef03c577c01a9c29b05f2289a310a4df2ececadd9da95c82a52f7d9", "ref_doc_id": "92c32a37-38b7-47aa-bc5f-5b3b3abdc47e"}, "224b5f28-ffa6-48f9-bb42-da5a5721d422": {"doc_hash": "36e45ec7503d5c1003be37e9aa11b302ea63bb806d2941147685b2b29a2e0504", "ref_doc_id": "1bce5740-226b-40a0-8fea-8fb8455ea975"}, "d2ee521d-9790-4922-b86d-2e1f3d122dda": {"doc_hash": "9ad1a8c635f7970f5b40dc873fb64d333240c510c8e7eb8548985c45d9e29dd9", "ref_doc_id": "1bce5740-226b-40a0-8fea-8fb8455ea975"}, "04f27749-44bf-4da4-931f-97f785159d4f": {"doc_hash": "c2740538d457bc328669272bf890ee3bc8029d85ac7db12622f734ec0c2b09d1", "ref_doc_id": "1bce5740-226b-40a0-8fea-8fb8455ea975"}, "ff83150b-a794-4297-a9bd-5d149494424f": {"doc_hash": "faa227cda94cc156b1b6879c04d87c39378dd699f6cf68ae7318c6c2bbfb41f6", "ref_doc_id": "1bce5740-226b-40a0-8fea-8fb8455ea975"}, "80f4fc6a-c426-4c80-9b00-9a8b2dac6e78": {"doc_hash": "334fa3d3795b168ecf69a22c3458699a987d3916309b47ef07fbe8cbdad215f4", "ref_doc_id": "1bce5740-226b-40a0-8fea-8fb8455ea975"}, "032e89bb-be00-474a-861c-fbd9d2648674": {"doc_hash": "b30d9a3bfa3a333512a2ab6aebb8ac56c64502107d9633848b13ff687403da76", "ref_doc_id": "1bce5740-226b-40a0-8fea-8fb8455ea975"}, "62f3aebd-f97f-4b2f-9b3f-86fb3c83978c": {"doc_hash": "14dc816cc31c0a8a15fd43f552f9d170a008f02c44d40b02d1161aa1dd87cbee", "ref_doc_id": "1bce5740-226b-40a0-8fea-8fb8455ea975"}, "673f9672-867b-4fc9-be46-3d59b6527fa4": {"doc_hash": "5ca7269443217a698daec65c9ed1770142d18bc2917195401cc2fd642269a931", "ref_doc_id": "c03101fd-c33e-428e-98cc-850dc59baa6f"}, "cfa0f7cc-794e-42bd-8df1-e591cbd48a3f": {"doc_hash": "78e63d785fb6b4ef217e4038aa39890a8e6ca3bc4ea0e81f1cc36c578559a55c", "ref_doc_id": "c03101fd-c33e-428e-98cc-850dc59baa6f"}, "f21144d7-2e27-400e-9214-6c0e6d00803f": {"doc_hash": "9ad8ef5875431589c8313db0749014904ddda24cc206bae96f81892de4853c73", "ref_doc_id": "c03101fd-c33e-428e-98cc-850dc59baa6f"}, "590e9080-8837-454a-89ab-eaadd605821d": {"doc_hash": "b6b36742205ad2006f0ed554184d4503b2185f3ee6ab9a96b1524456b81a9de0", "ref_doc_id": "c03101fd-c33e-428e-98cc-850dc59baa6f"}, "e6cb8cf0-23e9-4d62-bef3-91f26346153d": {"doc_hash": "f1828826cc097540288d39c05e453efa9518228aa2564af90e7010281fa00b2e", "ref_doc_id": "c03101fd-c33e-428e-98cc-850dc59baa6f"}, "f0260805-3129-406f-8391-abe3f258a4b2": {"doc_hash": "1a27348feb4fb9d4fc07626fa2e8ed9a128c22419b3aa7e8c1d92fe472dbbbe0", "ref_doc_id": "c03101fd-c33e-428e-98cc-850dc59baa6f"}, "066f374f-9302-4926-8162-d2396b7bfc01": {"doc_hash": "55247cfaacf77e2ed828b2d01475c1f411cdc9c4006289b432cc7f17f11d150f", "ref_doc_id": "14687a53-0578-47a9-ac10-048904a69276"}, "0cdbc951-b42b-43a5-b22c-68ecd6243377": {"doc_hash": "bb04cfc4ca8e9b3a5c85d1fbe44d4a220779d82c616338b9a899055e95ee8a93", "ref_doc_id": "14687a53-0578-47a9-ac10-048904a69276"}, "2ab76f83-3dc3-4ee1-aa23-6f3d5b67b56a": {"doc_hash": "f4023c5ff5bd665b1384d365ecf920c23e8f1bb99f02124286681c70e548cb46", "ref_doc_id": "14687a53-0578-47a9-ac10-048904a69276"}, "8f78586b-37bc-4bcf-8979-a8e08e9a7dc8": {"doc_hash": "eb75ca4e73d23119db3d41f6ad2334d2a1140e069e269377e493c656287a48f0", "ref_doc_id": "14687a53-0578-47a9-ac10-048904a69276"}, "5d5af6f3-eb70-4cb5-b0d0-5a3d4b147618": {"doc_hash": "fc3710f24c77085e148dcf22d41ff55ac5e108f75ed09377ac58d599d1a5660b", "ref_doc_id": "36c588af-23f5-402f-b06b-dc8285afc728"}, "c2eb2835-9ba6-4a5c-a98f-ea5e485429d4": {"doc_hash": "158a4aaaef0ae772016abf7e6e78c84090696bc5129a8e5b8aa2d743519c61ea", "ref_doc_id": "36c588af-23f5-402f-b06b-dc8285afc728"}, "b83794da-0574-4954-b8c7-5c8d2294337b": {"doc_hash": "021676ddc9720c36e406fd73255d4a1d60773ec46cfd925da2efc200869bec18", "ref_doc_id": "36c588af-23f5-402f-b06b-dc8285afc728"}, "c15036c5-c315-489c-b2a3-a17d41798fc9": {"doc_hash": "929b70e51cda15c8c88c7c483d5888db887f6fb3f1e86ab93a5400cf6a736749", "ref_doc_id": "36c588af-23f5-402f-b06b-dc8285afc728"}, "35eae915-917c-4612-91e8-ee556488c079": {"doc_hash": "2ad64422f2548159b693633869d75aa903aec31e51c8d41b15fa5bcd36f6fc23", "ref_doc_id": "36c588af-23f5-402f-b06b-dc8285afc728"}, "be94974d-5da9-401f-afc1-05865e901ac5": {"doc_hash": "30c40219fe3771266132fe2a2d220fa58fcfb30b0912f43d636c7c8804d3f6f9", "ref_doc_id": "36c588af-23f5-402f-b06b-dc8285afc728"}, "6a67ab9b-8d25-4987-9fa9-3e915769a0ee": {"doc_hash": "1fec55092c1cc2f3a6df2ab6881d54d48af0080713d6d80ff16114f9182f8c88", "ref_doc_id": "36c588af-23f5-402f-b06b-dc8285afc728"}, "1e7afcca-75e4-4309-a9fa-54bc294f4918": {"doc_hash": "a8a1780f9deac8c3bb8ddf7a970181344172b39143890329cf86d0cefaa38991", "ref_doc_id": "36c588af-23f5-402f-b06b-dc8285afc728"}, "aea65467-b9c4-4e50-8832-a2dd2be570d5": {"doc_hash": "f1dadb4c74f4ce7e519b5ef959ac2af85c7567dd8464f3224787aa1c2fe1b265", "ref_doc_id": "36c588af-23f5-402f-b06b-dc8285afc728"}, "909e284a-6c8f-4290-b3c9-a8aeb832eae1": {"doc_hash": "ae45aebe02ca4ed6ed5aa1db50fcdb5ff7c28b06a9378ae699171e9ced111ed6", "ref_doc_id": "36c588af-23f5-402f-b06b-dc8285afc728"}, "71891c40-c446-424d-93c2-9e516c34a890": {"doc_hash": "49c76d31956670d760073e267c2ab187724a977377a898e0c12348ac31bde6c5", "ref_doc_id": "36c588af-23f5-402f-b06b-dc8285afc728"}, "a025fe32-a6aa-448c-ad69-099fa95e1b35": {"doc_hash": "9a5f6a2ce5231cad37b026cba6002c7e2dde9f89b909fc6de219a3c144e675b6", "ref_doc_id": "36c588af-23f5-402f-b06b-dc8285afc728"}, "b26a0614-6bf1-4cb2-aa5f-d7b11ad254d7": {"doc_hash": "80332e1fd8af56d35493898d24ba641a12c2d073d5567a9622b98463fdd177fe", "ref_doc_id": "36c588af-23f5-402f-b06b-dc8285afc728"}, "76569759-779b-4f28-9338-f12cc3cd07cb": {"doc_hash": "488be6432b25b49efb272677efdf351a6aab76efcf608275f54f8631f78330f9", "ref_doc_id": "36c588af-23f5-402f-b06b-dc8285afc728"}, "55887fe7-0029-444d-b5e8-de80cd66f4a5": {"doc_hash": "5e3289f5c59317a5a105bb5107e88564f9b27bb25da58c237ffa944567dc892d", "ref_doc_id": "2f25b56e-7640-4445-adf3-27a80a5e62b2"}, "e92d4bb3-fb4e-4147-beb1-a2e3fdaffa0d": {"doc_hash": "18875806598ec17122f5fae1bb290c870e6a30390904c41c5ca0db3d3516414d", "ref_doc_id": "2f25b56e-7640-4445-adf3-27a80a5e62b2"}, "f140c749-e4c0-4fe7-b5fa-0b816c2edb20": {"doc_hash": "bb44a92c7a22e3e73e0dea7728d31aef2be3dc9fe2c6f8d0f9b1d3060681552d", "ref_doc_id": "2f25b56e-7640-4445-adf3-27a80a5e62b2"}, "a153adf2-d711-4f8a-a736-f80c61e80676": {"doc_hash": "6cccea968f84e27dd2dd9aa6533430601f96310bffe08074be7bf693921f1d12", "ref_doc_id": "2f25b56e-7640-4445-adf3-27a80a5e62b2"}, "8421e132-1b64-4199-be5e-ca2e7fa4f2fc": {"doc_hash": "bc19a32d0c3bc47219f03b9650fe9899c0ceb96b0eea7b20d1cc1eb54cc45134", "ref_doc_id": "03d5b93b-ec0f-4c92-9e23-4d7185e9e703"}, "0c6f5af0-d4ed-41b9-a34d-9aa87fd60111": {"doc_hash": "e466e31c92ba06d05968d5d1f519eefa28ed7f5cc823a8a7c6a48fe2eac96cd0", "ref_doc_id": "03d5b93b-ec0f-4c92-9e23-4d7185e9e703"}, "72f51578-3ec9-49bf-a46e-8968f52b7951": {"doc_hash": "cdd4cd4adafe1c09033d3c09fa1a59b75fab7f5e015e7eea603ace8615b57ab9", "ref_doc_id": "03d5b93b-ec0f-4c92-9e23-4d7185e9e703"}, "79e6915a-78a2-48b0-a216-2cbd9e0ae05f": {"doc_hash": "919e2c156d0857482e03490ff68f7d80fbde0ae109010c33103252e59cd495a7", "ref_doc_id": "f6e5d8be-352a-484e-af22-cdf2841640b5"}, "4ddc887d-82d7-437a-9f2d-9595ab84a39f": {"doc_hash": "5fd22dcfd32db88a4762cf504954d87a92cfffa6be939341030a3f22cc02af12", "ref_doc_id": "f6e5d8be-352a-484e-af22-cdf2841640b5"}, "5e1821c9-0c49-4c8b-85a2-09ca1777f1bd": {"doc_hash": "272cb1ad3ce8909e765bf56829c96e7e8bbaefa84a02539119c366a993c36661", "ref_doc_id": "f6e5d8be-352a-484e-af22-cdf2841640b5"}, "e4bbdf3b-8778-4a14-a285-40d6e24868e3": {"doc_hash": "c3e562f3bd408f2f1163a9b4a0cac8371b446a8fbe8fc69aa3273ed10789071b", "ref_doc_id": "3035a753-bc13-4060-b9c2-2b453fe5b319"}, "d3b040b3-4724-4cc0-b9f7-70ae0b3b7813": {"doc_hash": "2cb76c0896290422889de5e51739daf2f0f4360389521e07452e95d1bf7283b1", "ref_doc_id": "3035a753-bc13-4060-b9c2-2b453fe5b319"}, "aa7e0b9d-4c2e-4914-9ba2-cc381ce1bfbe": {"doc_hash": "50a188478fdeb842eed1b42c00f1ca56a18324ccc35bd6107298198fd791f022", "ref_doc_id": "3035a753-bc13-4060-b9c2-2b453fe5b319"}, "715a4591-4278-4d67-8ab7-1d9ddaee2340": {"doc_hash": "56f42a70e97ab6c2c3f3f0cbe6b0e2c827367147820c545d57a05e38d46f877f", "ref_doc_id": "3035a753-bc13-4060-b9c2-2b453fe5b319"}, "58ac2940-929a-4289-9e94-c4c603682483": {"doc_hash": "f2fbfe00d10f70057817318c28bbd5c2091028f0bb602f1e890a058140599aba", "ref_doc_id": "3035a753-bc13-4060-b9c2-2b453fe5b319"}, "2dcb4780-e9c1-456a-8ba1-a41033e5772d": {"doc_hash": "1aaa096160283b9024096bddb8c5fa827e6fb1e666ce194b7b2853d39c8126fb", "ref_doc_id": "ffc1e5ca-3684-4aba-b4bb-8e22550bbbd3"}, "87fcb361-7153-4ff3-9919-035be55520e9": {"doc_hash": "b64f5ebf4a7a55a4bd81ac7701492f82f324b50119efd6526c7f648a36b03d2f", "ref_doc_id": "ffc1e5ca-3684-4aba-b4bb-8e22550bbbd3"}, "613b6bc4-d901-4cd4-8b4a-6acde642f3bc": {"doc_hash": "655795fd3fcc596e775194b4275e57c83cd927891bffaa315f2ecc1809493ab9", "ref_doc_id": "ffc1e5ca-3684-4aba-b4bb-8e22550bbbd3"}, "5d846d79-1450-4e8e-952d-12fcfa1d2e8e": {"doc_hash": "a82eb9ef25cbd5d23e616247129c3ef5af6238121e1eaa3a3d50c384c1cc6529", "ref_doc_id": "ffc1e5ca-3684-4aba-b4bb-8e22550bbbd3"}, "2bf542ad-c282-49a7-b2f5-913b01325080": {"doc_hash": "61e54fcc87d328bba57bed3af948d031e6ad64b2aa5277aafa861512c4fd1f37", "ref_doc_id": "ffc1e5ca-3684-4aba-b4bb-8e22550bbbd3"}, "c0b81199-539a-4f24-805d-d5b08fbc3b05": {"doc_hash": "03005969805187d5b47e8eb197fe00bc1c0dc92b427c68326a1373c6ee5df0e9", "ref_doc_id": "340f46ae-5714-42b4-84ab-42bf54612094"}, "50681b0d-1c8a-4207-9e45-475b92178e91": {"doc_hash": "892b7c5ad4fd495f001bf74c80be6021aec390be107b4daa0b28c129f3df20e0", "ref_doc_id": "340f46ae-5714-42b4-84ab-42bf54612094"}, "8c4ae5c2-1a3c-4798-b4e8-fe81ab38cf1b": {"doc_hash": "4ce3a9df6b125e57fe12ba0ff22bc6b3f35ebeeeb5b8da4c5143edbb09b7e322", "ref_doc_id": "340f46ae-5714-42b4-84ab-42bf54612094"}, "0d5bf640-e9ca-4b1f-ac08-281aeb32efcb": {"doc_hash": "707fd2adb1a260304aa53c028817af5104f898422d9fcc4c1b06f604fc2a7288", "ref_doc_id": "340f46ae-5714-42b4-84ab-42bf54612094"}, "c2bfffa7-5a3b-424d-a87a-f5948453de81": {"doc_hash": "21a517b7746fb8d6aed18672563a485891a97601ed8279023235d91ce29af3e8", "ref_doc_id": "340f46ae-5714-42b4-84ab-42bf54612094"}, "49fbcc0a-9ef7-406e-b580-8e0b77b30268": {"doc_hash": "be4df49ca0a32c84b1083778f7763d1bcd68d5b5d532b4a61ade08ee15d77c08", "ref_doc_id": "340f46ae-5714-42b4-84ab-42bf54612094"}, "0f63b1f8-0880-488c-b11c-5da0f987ff32": {"doc_hash": "f09a89bc5cdbc597359d3508d46302c5281b51a7c88702f2bbc6aefdcdbe57ed", "ref_doc_id": "bab77fe8-f4b9-466e-95ea-cf9ad876baf4"}, "1c5bff10-f3b9-4caf-9453-080d6531f289": {"doc_hash": "dc3f18e741459a6e62b70385c9d0cdc3755c42ed7e1d055a7a076ce5cf7d7537", "ref_doc_id": "bab77fe8-f4b9-466e-95ea-cf9ad876baf4"}, "dff0714c-ca56-4a5d-8071-f090cdb580ce": {"doc_hash": "fe7116fdf3f85fee93b2cdcc7294885a613241e24a3b263afa490edbb462fda6", "ref_doc_id": "bab77fe8-f4b9-466e-95ea-cf9ad876baf4"}, "3c98a4e4-1786-4f92-80ff-ba018be5ae7d": {"doc_hash": "2b3ab2e5f7115c827f8b7f274a07c865069da54746a3fde5460d771ba6dcfdc6", "ref_doc_id": "bab77fe8-f4b9-466e-95ea-cf9ad876baf4"}, "4c18ad5d-bbcf-442e-8d17-f32b288c853e": {"doc_hash": "dfa4bccab5b700360ca12f7cf8b3dbb17f7fcb342e521d63755e8709a1071618", "ref_doc_id": "bab77fe8-f4b9-466e-95ea-cf9ad876baf4"}, "b701c90b-3bde-4a5b-a5eb-886e47f1e748": {"doc_hash": "a254a74e94a456313b9297325c4c94aeb7ba9a963d721001a5c50d7a353565bf", "ref_doc_id": "9f5e7985-169b-42e2-af2c-99878578fd00"}, "9f96fdff-5315-43bf-a356-c63c7149ac4e": {"doc_hash": "9a0e793bd6164992edef5e7833919763f2344aa4778d1a18f07c1b926cdf0e4b", "ref_doc_id": "9f5e7985-169b-42e2-af2c-99878578fd00"}, "2d5b3a3d-a8bf-4b1a-9c24-1143b12ad18f": {"doc_hash": "1af2c4022dfb3195e83d8bb1f4100e0a1bd5e6457c0950f38b38454e12d0e892", "ref_doc_id": "9f5e7985-169b-42e2-af2c-99878578fd00"}, "0212bcbf-721c-42a7-909e-c5a1632be4b6": {"doc_hash": "a48fb0a419a362b15fe82c0589e6a600de8657ec968558f9eb3d4ae625330b8a", "ref_doc_id": "9f5e7985-169b-42e2-af2c-99878578fd00"}, "108b0c19-c33c-4faf-af99-5b35d32d708c": {"doc_hash": "988225120d7847b9c428bf8b4d18c4115bedcbf612febe985f116f23bb45c6c1", "ref_doc_id": "9f5e7985-169b-42e2-af2c-99878578fd00"}, "ee032901-d0bb-4bf8-b84b-7c51d379c548": {"doc_hash": "789db76e3837f609acc31d709796ce1b43ebf2a0d330f49e0bb631db73dcd7b1", "ref_doc_id": "9f5e7985-169b-42e2-af2c-99878578fd00"}, "5284a101-cc2e-4e81-b5c3-c7d442b4d288": {"doc_hash": "c41aabe3ae81daec3b793934298f379561916bd3501b233fbffabc7b29e06f0e", "ref_doc_id": "635b2115-d0c1-4c5f-9b24-cebc3f4db1ec"}, "7b554c31-66ed-47fd-98bd-b7168cdbe402": {"doc_hash": "3a667773a80ff9de75f88491eb172b8c1e0b0efc25ca7f13e85115df00e63ea0", "ref_doc_id": "635b2115-d0c1-4c5f-9b24-cebc3f4db1ec"}, "8d107517-6473-4bb7-9fa1-0416c06bb656": {"doc_hash": "b44279582907b1655f409d73f2db5c1ceb3f3f4ecd06e551472292cc8d82a74d", "ref_doc_id": "635b2115-d0c1-4c5f-9b24-cebc3f4db1ec"}, "a9e14192-8020-4c06-bbd7-3189c19e7a11": {"doc_hash": "c832920399a466885cafcdf62068097dd08579bbe69dc78ac1755f72c5c1bc8c", "ref_doc_id": "bc2828e9-fbd2-4fe5-96df-1001a43afd6c"}, "a1ff3678-dd54-4c4c-86af-14dd23a06df3": {"doc_hash": "6e851d92e570928a4f32b064847568593bf3f86b41490bed4e00ea493400b6c1", "ref_doc_id": "bc2828e9-fbd2-4fe5-96df-1001a43afd6c"}, "3d7d4754-97cb-42e2-a69f-1f5143e7178c": {"doc_hash": "e23de6975a3d1ad0908929bef65409dbe87dc89383a1d37dda8e5a4be170e4aa", "ref_doc_id": "bc2828e9-fbd2-4fe5-96df-1001a43afd6c"}, "ca64912f-dfa1-4351-b428-8e45b105672f": {"doc_hash": "4bdeb8cb6cfa1919684fa41dd3b56e2926157956f0bd6f58afe9659cdc4673ac", "ref_doc_id": "bc2828e9-fbd2-4fe5-96df-1001a43afd6c"}, "3d8ef2d0-1688-4a68-8c71-ff04421feb49": {"doc_hash": "a94d9b46058881d9ae9949ba959da45890f0d8a0a2c14c045f0610eaa7e9e414", "ref_doc_id": "bc2828e9-fbd2-4fe5-96df-1001a43afd6c"}, "7285fb7d-92db-4838-9e6b-0e1a58a6d3c6": {"doc_hash": "178b97245c1b20d2fe5adfb743642e4f8843f6616d412b6b37a90e00ac403d03", "ref_doc_id": "bc2828e9-fbd2-4fe5-96df-1001a43afd6c"}, "3f3c29b3-abde-4e89-9a55-b59a6619bec6": {"doc_hash": "685a82cd6b888504c9604da9de53cbf61c6322c2441b097329c17bcacb5aa314", "ref_doc_id": "bc2828e9-fbd2-4fe5-96df-1001a43afd6c"}, "8f92f2dc-465c-4a3a-ba2c-cf29d9b4755f": {"doc_hash": "85ac946e76d5c998b1eb52d91cc2ae26d536d9599c46d521fd52ad47f735d0a8", "ref_doc_id": "e0160dfb-5f5f-4596-9b02-10e879ce4929"}, "3d506bfd-465f-44a6-a811-0c8830fe9323": {"doc_hash": "881005cfa5081f3d1fcbb49bb5d3cbabfa6c53f860a84db763007aeb41e0994c", "ref_doc_id": "e0160dfb-5f5f-4596-9b02-10e879ce4929"}, "1ddcd0fb-65db-41c3-8bf8-766a5beb6f9f": {"doc_hash": "b4317e6d9bdba0ac57657a331f6d6355912ae04572c3b1586952602440180b7a", "ref_doc_id": "301b8db7-e100-4df2-966e-691d2ebcffb3"}, "746aa0c5-e71f-4898-ab77-7726792b00f2": {"doc_hash": "27a2a2d73e5b63e458b680274914475d63065792d1a238983d0358ef29269316", "ref_doc_id": "301b8db7-e100-4df2-966e-691d2ebcffb3"}, "5c832198-4c34-4caa-a5a8-cfb19960a642": {"doc_hash": "d9b3dde842430b848f596732c2b24995dd5baeb1c322ebb8d4cd3ab3760d6dae", "ref_doc_id": "301b8db7-e100-4df2-966e-691d2ebcffb3"}, "7b54db65-c237-46e0-9812-87b37dc6bfa8": {"doc_hash": "e4afc61654d27d69a7e60de25c2957b2ba854e2cdabdcf10223b5ed5ce808066", "ref_doc_id": "8078c614-37b9-4aa0-8f9d-96fdb8e4c9a8"}, "e7dfc7af-5a42-4b39-b54c-8a0370e39ab1": {"doc_hash": "2c417f81879d5c3ba094271746725db6bd0e13e38801591bb516d626a78d5b97", "ref_doc_id": "8078c614-37b9-4aa0-8f9d-96fdb8e4c9a8"}, "5c9c07e8-dd35-4f66-af42-fca1e791030f": {"doc_hash": "78be71c3e9ef60df4adf77a878726699c1ee78faef9c42c37f4fe201849f639a", "ref_doc_id": "8078c614-37b9-4aa0-8f9d-96fdb8e4c9a8"}, "ff9c2fa0-8fbb-4a96-a3da-dd92dded1103": {"doc_hash": "62cae2e5cb60addeaf7c3f7caceeb7d75ba495e4b9fb4cec5047511f6356d654", "ref_doc_id": "8078c614-37b9-4aa0-8f9d-96fdb8e4c9a8"}, "8cc38866-a386-4f5c-8adf-0de3ac9be560": {"doc_hash": "168708f8c7ccd31f9040719237fa4d0eeb31b54270f783497a0d5f7e8e3b9fd0", "ref_doc_id": "8078c614-37b9-4aa0-8f9d-96fdb8e4c9a8"}, "b1943b98-0bb0-4114-aa96-fbe54df31b7e": {"doc_hash": "26e0cba7dee363436a6506c29c286643044c9359955e670683895002bf4299e3", "ref_doc_id": "8078c614-37b9-4aa0-8f9d-96fdb8e4c9a8"}, "23c95b9b-6e4e-4077-8c06-05543351b4d8": {"doc_hash": "abaa23731c354bba4a54d37b9867da8c106ae8165dc8a9bd783f204ee2b8ef50", "ref_doc_id": "bf175024-92ea-4126-a6fb-9f1bb834913c"}, "524770c4-6f60-48d2-ae21-e92a87622539": {"doc_hash": "f2f0724f5ac89b73b448b8c1f873ca3917a879eef4001ac0a816c4291789cca9", "ref_doc_id": "bf175024-92ea-4126-a6fb-9f1bb834913c"}, "da9ce0f0-722d-4454-a6ab-c47cbce1b94d": {"doc_hash": "bd00894df1667e5c529ffddffbf44daf9698ab65bc620a0ec0cd60de981489cd", "ref_doc_id": "bf175024-92ea-4126-a6fb-9f1bb834913c"}, "557d5334-b2dd-4b26-b03d-8e221515ddb9": {"doc_hash": "deb9ed7ee962b671cabe77da29ca73c07a6b339ac7c81a812cd84faa86f43b86", "ref_doc_id": "bf175024-92ea-4126-a6fb-9f1bb834913c"}, "abb120b3-9231-42a5-b3a8-cfdb0b14584a": {"doc_hash": "479ab06a65580908759d40a95af5d170de740e3d935bcfd4ed53b1eb92b5edf3", "ref_doc_id": "bf175024-92ea-4126-a6fb-9f1bb834913c"}, "c62be553-1e07-49b3-9578-92cf8a9b2648": {"doc_hash": "eb79b2e902e3da3745ea286521450747894dfd55909502c7df092e245d71dbf9", "ref_doc_id": "bf175024-92ea-4126-a6fb-9f1bb834913c"}, "0a4f716e-2a19-4aeb-b981-92c4a0e8c156": {"doc_hash": "919c14026979611f6054a3019c32b5027a659e121dd9e80697bc5ccc335a5020", "ref_doc_id": "bf175024-92ea-4126-a6fb-9f1bb834913c"}, "55d15888-53e7-4b17-b896-10889f03d2d8": {"doc_hash": "5480b36b5e76a5313ee3eae3e4829f0be0c38dfdf2f9243b90ea320562c95563", "ref_doc_id": "bf175024-92ea-4126-a6fb-9f1bb834913c"}, "3021bfef-2e95-4411-8eaf-049192a50eab": {"doc_hash": "5c1b5d8af1b0beb571e3893ebc799ae94f801b1c85cb610e94526a011d38bdd3", "ref_doc_id": "bf175024-92ea-4126-a6fb-9f1bb834913c"}, "93236a20-3f7f-4d21-ab91-4214fd5cf2bb": {"doc_hash": "d6f981fd523618d9d1bc5965e9d52be19c8f6b4d258f2cb808896fdf8dbb203f", "ref_doc_id": "8c33628f-7781-4a91-912c-dc524ca918db"}, "e6c3eeb1-ffb5-4869-b225-1aa0c0ac70af": {"doc_hash": "ef25bc6f0cad18c085fb062a5e5d3785a5bcce3157f971f7ecf3413a3b865409", "ref_doc_id": "8c33628f-7781-4a91-912c-dc524ca918db"}, "ac2036b1-502c-4d11-821c-b194483aaffa": {"doc_hash": "85c87790784e297ccb9a3ff90d12336217616665807a2a0d71a92135df412fcc", "ref_doc_id": "8c33628f-7781-4a91-912c-dc524ca918db"}, "c4debc68-bd48-4223-ba34-e592fdbe37d0": {"doc_hash": "34bb701bb01c283e593650f90c95aef03279a8ce18ec1663e2e3528aa77d5fa5", "ref_doc_id": "8c33628f-7781-4a91-912c-dc524ca918db"}, "f66691de-b78e-47a0-b07f-105f7c5821c5": {"doc_hash": "a4f57e15eef683f56ba91b2783426291e984899e28472a8d7e18fba6cd0cf54b", "ref_doc_id": "8c33628f-7781-4a91-912c-dc524ca918db"}, "4ec93946-a525-494a-80e9-75a247fb06ef": {"doc_hash": "5e7299d890672bc552fdc479b752c404a20f35e2d06ff199b708c7d92939e835", "ref_doc_id": "dd317f66-60d5-495b-9596-574f69d4beba"}, "a74c9239-ce7f-4a43-a912-0988773b58f1": {"doc_hash": "eeb35b14c0e5c171628ee6702c71e22459c041195bc0065833d2a59969599623", "ref_doc_id": "dd317f66-60d5-495b-9596-574f69d4beba"}, "8ec8044c-1ffe-429e-a8f4-8c04d465d017": {"doc_hash": "82c92f3ed0ea89d1d612034f1862e31f84d7946c1ba73ea07bcee841a8ec9ecc", "ref_doc_id": "dd317f66-60d5-495b-9596-574f69d4beba"}, "c16d9f7e-93c8-41a5-9579-d21f19e20176": {"doc_hash": "9f5c8c3e55916fef9d05cc2026bf0862a0a6f66432bce0328613150fda557894", "ref_doc_id": "dd317f66-60d5-495b-9596-574f69d4beba"}, "33fededd-4739-4e68-b7fb-f8032c872cae": {"doc_hash": "4760683d44cd325fd9a775bb6c2591ddaf5f54dd4aaa68c60c419fbacc96f7fd", "ref_doc_id": "dd317f66-60d5-495b-9596-574f69d4beba"}, "811b5dee-0ded-41a2-a1f5-1d2dc7166ee2": {"doc_hash": "8988392d2d20a8ba81e002e28a32c92b3533593b11759862171f3030553c5416", "ref_doc_id": "22603646-039a-4d97-bb34-9b49c6a6ac9a"}, "8af87fb7-5ed6-4dbc-9f95-5df3882c75a8": {"doc_hash": "e4d38054a21fa7f4f228ae67189e66c50e07441c5fef4a7768989ae31e610239", "ref_doc_id": "22603646-039a-4d97-bb34-9b49c6a6ac9a"}, "c56c18bd-c834-450a-be22-b9b21880a430": {"doc_hash": "474085d34781c879c4e3cb32dccdcf5d7963fac74e4e2142af05eb374cde201a", "ref_doc_id": "22603646-039a-4d97-bb34-9b49c6a6ac9a"}, "2b816843-31fc-4f97-8972-663c90ffac2f": {"doc_hash": "3343d33dc890d71b9465654fb800128d165e5f9840b577b6a77c17c38c8fbbd5", "ref_doc_id": "22603646-039a-4d97-bb34-9b49c6a6ac9a"}, "0703fb6b-99bc-4bda-ac83-fb286ce88a12": {"doc_hash": "fc4f0dd0a25a985bf2d15310c7d53a99b3968419d79d03341861257000a38f3e", "ref_doc_id": "22603646-039a-4d97-bb34-9b49c6a6ac9a"}, "3653c951-0fa2-4772-8383-5770180b0049": {"doc_hash": "ff4f25182eed9c2079e6e0598d038c42fe46e1593f40774645ba2c2ae466fbd5", "ref_doc_id": "22603646-039a-4d97-bb34-9b49c6a6ac9a"}, "7c887066-60a0-4e68-b36c-b9938cdbe8d6": {"doc_hash": "3fa0bbb3145e4a857ddb4d5c76b2ef0531580b7161cd926a7fc7fa1f16791ab0", "ref_doc_id": "22603646-039a-4d97-bb34-9b49c6a6ac9a"}, "c0958479-b601-438a-8156-1cb8f8ffde6c": {"doc_hash": "98baab6ed30c5a5261856ee174c6f0a95691c1430bc5e750762bc14bf4d90eb9", "ref_doc_id": "e31106da-d110-425b-8f6e-2b8cf866f208"}, "b51f70fc-9e97-4c52-bfab-9c9243f19b3e": {"doc_hash": "62fd8304e78f4fe885c36c2714f09f609c64b83f4b37881635e5d5ced409d3be", "ref_doc_id": "e31106da-d110-425b-8f6e-2b8cf866f208"}, "c8dc349f-9c21-47a1-8dc4-fc92e6923995": {"doc_hash": "61c3b6154861489f470deab38c9e6f357b044a04d05867b1b2e07a6bb2a4d270", "ref_doc_id": "e31106da-d110-425b-8f6e-2b8cf866f208"}, "bcda5d5b-bbb7-46ef-ae41-e924864e9856": {"doc_hash": "c72b541191c028ea4e7066b1619ec34c3c1081e41d7444f71f7ae316dc80233d", "ref_doc_id": "e31106da-d110-425b-8f6e-2b8cf866f208"}, "274164e0-5a9c-4f3c-9d5b-844dfc3612bf": {"doc_hash": "a844eed2bfcd557c9103a29649b0f1219536746816c4d624d2ccae1368886788", "ref_doc_id": "e31106da-d110-425b-8f6e-2b8cf866f208"}, "3ba408ab-1c68-49f0-a9d5-5dda3acea107": {"doc_hash": "3ebdf77939bc6a8f66e9eab369e68ba544db7395c2ed337272c8f3625016340f", "ref_doc_id": "e31106da-d110-425b-8f6e-2b8cf866f208"}, "6c3d3104-b82d-4c3b-b12b-dd1e73aed651": {"doc_hash": "a4247431ccbd74f481cb3cd37f1437e4d602b4ac88a14afe069782ff37fec236", "ref_doc_id": "e31106da-d110-425b-8f6e-2b8cf866f208"}, "26a5ed24-7606-4cb3-bb79-5a33c3ce85ca": {"doc_hash": "6f7ab1d3ec064e1ef6042413560b71736a8e6ae7e7b5f7847ac6abd09f484de8", "ref_doc_id": "e31106da-d110-425b-8f6e-2b8cf866f208"}, "47ae2bf1-83d6-483a-a1ba-f7f8e7b5989d": {"doc_hash": "473ba57e09d46d9de04f780743274d13032845faf03e579c7ae55326a47905f8", "ref_doc_id": "68669e3a-bedb-4584-a56b-d99aecc96570"}, "1a3d307c-eb1d-4dd3-a45b-d84f57c81a32": {"doc_hash": "7900d001356e683586ca2babf5b56233a86a127a86a22771d45f5a6234be3d97", "ref_doc_id": "68669e3a-bedb-4584-a56b-d99aecc96570"}, "c92f247c-40d0-4d97-87f1-c49959c72819": {"doc_hash": "18757e93542611667116c9685c2d7872a1793e5d2ae017dba4975d19f15f4bea", "ref_doc_id": "68669e3a-bedb-4584-a56b-d99aecc96570"}, "b391dba6-8c48-4f5f-94f3-74b4bd694f3c": {"doc_hash": "ddab60211fd9c10399a1fc1b98ab7fe5d3e46d0c04a01a99b249be0d513c6785", "ref_doc_id": "68669e3a-bedb-4584-a56b-d99aecc96570"}, "fbdaeaa5-5218-43ea-a3c6-c1b7f2b5d00e": {"doc_hash": "022d186db53db786f5bf51e3251de8ca5a4c189436e9b593485008ef52e9b763", "ref_doc_id": "68669e3a-bedb-4584-a56b-d99aecc96570"}, "f8339e2c-af41-4400-8bf4-c550b5917a4e": {"doc_hash": "09e0f078f84f584c1267cfe75e1aefd1e8ba21f134573aa89940eddc4dd848a9", "ref_doc_id": "68669e3a-bedb-4584-a56b-d99aecc96570"}, "c26dc2fd-5628-4e57-b782-4cd3c2a4f9c1": {"doc_hash": "3b75196427682ac6b02d040997967bb15e984a7989764cb080b20a0047d0b026", "ref_doc_id": "68669e3a-bedb-4584-a56b-d99aecc96570"}, "4ef2f201-8c12-4519-ae96-4eabb8f4f3a6": {"doc_hash": "4d2f89a60b86a282f4ee81f39ad1e33addc1aac7e78c999574a4041754628cc9", "ref_doc_id": "68669e3a-bedb-4584-a56b-d99aecc96570"}, "eba5dc32-49af-4d25-b19b-d2fbbff1abe1": {"doc_hash": "275b96156fed1f8cb0d295be0b402b6f74e80bd122f1ced97c1d83736f347419", "ref_doc_id": "29b9e75b-2597-42da-b8a7-a40eba97e76a"}, "6abb63ad-84a2-444e-a36f-b35e2c452e3a": {"doc_hash": "90eaad74c50c4d142bb2a12bbf1001a8ac4f18d1f678e08b272db3b0e6cd55c1", "ref_doc_id": "29b9e75b-2597-42da-b8a7-a40eba97e76a"}, "e28414a9-144b-421a-be57-4a41ba3ecb0a": {"doc_hash": "2e5482aaedac72e29ad5ec5ac60f38d4fb981e82c132811b4e8e9fb6a7ee1570", "ref_doc_id": "29b9e75b-2597-42da-b8a7-a40eba97e76a"}, "2f6b7060-177f-45d4-85ca-6cf6b95f9cef": {"doc_hash": "3f36ff43f2146aa0c712c599124364b5b006870321d18a6f6b4f6dae31dec080", "ref_doc_id": "29b9e75b-2597-42da-b8a7-a40eba97e76a"}, "473b6047-54ee-4838-98e3-335210a020e8": {"doc_hash": "3bcaa1519fc9734b50ee1b6640a274bc7f89beebaf3127e74fda551e8d18fcea", "ref_doc_id": "29b9e75b-2597-42da-b8a7-a40eba97e76a"}, "a6145080-3f9e-4d56-8aad-088f97b2b2d7": {"doc_hash": "1206c66775230c4a51b2258318ebd0ca82d8c401f5205d59a2e0750367614348", "ref_doc_id": "29b9e75b-2597-42da-b8a7-a40eba97e76a"}, "f940bd71-f40e-45bf-a65c-92a855fdbb14": {"doc_hash": "aaef9d2c35462119847ed0ecb3b44979d32d378e39fa3c7c11bde7f41a045ef9", "ref_doc_id": "29b9e75b-2597-42da-b8a7-a40eba97e76a"}, "81774e96-39f8-4062-8529-771906d1b28d": {"doc_hash": "078cad91b5a52f15b3fddf62ecb759764f4278b6e7fc3f3bbce9a1188ce03eef", "ref_doc_id": "65437f97-2e48-4ffb-a733-840a4165e5da"}, "91f8a5a2-095f-4b83-a757-a744f8b6f815": {"doc_hash": "bb4ecc0ea53f2615ae18eebda72e7c0443c0683ffcf5ab05cab17a474f78bac5", "ref_doc_id": "65437f97-2e48-4ffb-a733-840a4165e5da"}, "1ddad1e0-88a5-44f5-b252-49f2ab85e91e": {"doc_hash": "798d1b617ae21cbc53e417ed263464f3d0928f690cd69ccf8a8020fd6fbd53ab", "ref_doc_id": "65437f97-2e48-4ffb-a733-840a4165e5da"}, "1a3b996c-2ed9-4789-b7ce-4610d4ebc18a": {"doc_hash": "d6c6481ae159a9b59d25dbe3e49af9e268bb27405a8df400a745bc6309906bfe", "ref_doc_id": "65437f97-2e48-4ffb-a733-840a4165e5da"}, "2fa8afd8-68bf-4c74-8234-fdd3f1417a88": {"doc_hash": "38c098cc29e9ad713a33942ab756029a8cc1ecc595114bcace3107990d2aebf5", "ref_doc_id": "da12eaf4-bd48-4e41-b6e6-a80d2aa6b208"}, "cbd261f1-2c7f-4ed1-9615-a7258db8cc8d": {"doc_hash": "9fface649e3d7107b00d7c78d06d0cc413cfeef31d3dfc14995c5bffd4fd2277", "ref_doc_id": "da12eaf4-bd48-4e41-b6e6-a80d2aa6b208"}, "b4df2592-d90f-49a3-8e57-191bea270f84": {"doc_hash": "5b94bd4b99bdd73b9f3f2b4505a86907c0c1382afd0e22624d6c47970b5fde22", "ref_doc_id": "da12eaf4-bd48-4e41-b6e6-a80d2aa6b208"}, "424a04cb-7c54-4c64-b68b-6aecc03dcc94": {"doc_hash": "f56ecdefc0158d8272c083c3c12553d135f3f15f50ae422aa946f0881c16dafb", "ref_doc_id": "da12eaf4-bd48-4e41-b6e6-a80d2aa6b208"}, "3421d7be-aea9-4025-900f-b5cd128185b2": {"doc_hash": "71d26f972c1cd41bd5c7503ce3a7751277053d7691ad5928e15bcc38f09e3e83", "ref_doc_id": "04fe36ff-9050-4f95-a893-a90249ba18bc"}, "9ee2b3ff-33e3-4b9c-a1ee-4431dcdffe8e": {"doc_hash": "29f7dcfd2a80a6c21734ac4d1c4beb95736348c70571a33309e3f553c0e4b6fa", "ref_doc_id": "04fe36ff-9050-4f95-a893-a90249ba18bc"}, "c777f0fd-53d4-44b7-aceb-c450f6b069ff": {"doc_hash": "26ac57244b144091614160137c884043b361982a10eaa66a299fea71fac333cf", "ref_doc_id": "04fe36ff-9050-4f95-a893-a90249ba18bc"}, "5ff15592-3757-4ae9-9456-58fef7c63622": {"doc_hash": "6ca091368fffb329fa0de30d309d347e90fd895af84106aa855149d8804f05d5", "ref_doc_id": "04fe36ff-9050-4f95-a893-a90249ba18bc"}, "95a7620c-16e4-47a6-a867-60604e082249": {"doc_hash": "87720b36fe15bc1347617a53fab59104b76faf336a9f32d010a7c1169640dbb7", "ref_doc_id": "d38e934e-c3a1-4a48-b1f7-8b9f89ed3c1a"}, "26f125b2-5132-465b-acd1-6186c3b41edb": {"doc_hash": "202325b35fba65cc942a671fd7ba4680bea07308116235439d3e99cdb3d4d807", "ref_doc_id": "d38e934e-c3a1-4a48-b1f7-8b9f89ed3c1a"}, "57667410-a6b4-41f5-9aae-a5899ef66b80": {"doc_hash": "eddcda6b26441902257d75982657e1fbde1594a91a4618c7896b7c892a8a69f8", "ref_doc_id": "d38e934e-c3a1-4a48-b1f7-8b9f89ed3c1a"}, "55c4c151-a5db-4089-a361-73f4f245a637": {"doc_hash": "2762d2ba0711eebd8a9375f976979deb0365a67383cf18f9cb467cb60795dd53", "ref_doc_id": "dc4358bd-2df7-4d22-af6c-f4cc87129ae4"}, "819391e7-dbb6-4b77-9117-87a647854800": {"doc_hash": "e90bf9c262c15b449cbaed50f1cd828407c93e5cda1b74ebaafac4f06f58c9a4", "ref_doc_id": "dc4358bd-2df7-4d22-af6c-f4cc87129ae4"}, "c1cc555f-fe43-431b-9fe9-aacdb09b70ee": {"doc_hash": "afddc35c68b55279305afda8b21ea7a07e89f977161d0617c7559f588afe9090", "ref_doc_id": "dc4358bd-2df7-4d22-af6c-f4cc87129ae4"}, "f0d5e85a-4568-4043-9499-c0a2c05fc739": {"doc_hash": "f6106d4eb9840e3e2b2f4d7ef83424bb38ceb5c663fd314f381e52f48cc9bbde", "ref_doc_id": "dc4358bd-2df7-4d22-af6c-f4cc87129ae4"}, "4b3ef8a6-eddb-4b14-b743-6e19c3a9022e": {"doc_hash": "165c88bb567977758e05c676f51b6226e0c37a5a042d49ca84324447c8891c2e", "ref_doc_id": "dc4358bd-2df7-4d22-af6c-f4cc87129ae4"}, "6013a694-9489-4651-85cb-be8c1c90d8e6": {"doc_hash": "e43115b17efde10c14b1ae7fc48f99c3deb18e51128ce187dced1e8fa3564e13", "ref_doc_id": "8a1f04fc-2aef-4384-8c26-74fc2938e91a"}, "56bbeb99-5b37-4c3d-81d4-0fabf8003056": {"doc_hash": "aca81432816bcd3bd43b2cfcde4fb1d0bb43d2dc7110ec99baaf4e6a00441e66", "ref_doc_id": "8a1f04fc-2aef-4384-8c26-74fc2938e91a"}, "bac6a762-03a6-4c4c-94b8-ea4a3f078c9d": {"doc_hash": "d99651f8223d3c5820dec90c9ee0be11f99860ddd4f2b6ddf469cae7d1ab0c22", "ref_doc_id": "8a1f04fc-2aef-4384-8c26-74fc2938e91a"}, "b334187e-9b11-4d9b-a0c9-19177d3d55bd": {"doc_hash": "b45ef55c4130af734fbea6164e5f098da2f8ab9b44c9b38359587847c270c6e5", "ref_doc_id": "8a1f04fc-2aef-4384-8c26-74fc2938e91a"}, "eae9f1f7-146a-437b-8c69-b37af3720d5f": {"doc_hash": "8ff8a8c7f3871b36f64e90635ffd00f6e2397eb6d90be6e1736cc52a7122216e", "ref_doc_id": "1c699067-f5df-4a3c-a513-45453dc811f2"}, "38392555-0e09-49d2-9a09-194d4ac176fb": {"doc_hash": "43fe1a06f0aea38b0d7c12c2559eb0c36638da9521d652c60b428a9184ee03b6", "ref_doc_id": "1c699067-f5df-4a3c-a513-45453dc811f2"}, "6c7d9410-b6c0-4655-9966-05e18e50ebe3": {"doc_hash": "b2347936c4d9a98b3dfdc6a5b08788d77597c1decef09917d93107b695a2c800", "ref_doc_id": "1c699067-f5df-4a3c-a513-45453dc811f2"}, "f76a50e8-370b-45ef-b12c-28acf5385ccd": {"doc_hash": "2d82f78f3ae920c103e48d5921742970fbb78b7bf3c9128ff4e6fd126c2714c3", "ref_doc_id": "1c699067-f5df-4a3c-a513-45453dc811f2"}, "e370bda8-5ed0-4ca3-81fc-3a80411452df": {"doc_hash": "4fd5d27c4bc870c7fefe577db717bd99d79dfe74848781e1db45d786a7cc6b5e", "ref_doc_id": "1c699067-f5df-4a3c-a513-45453dc811f2"}, "f6db23bf-bd37-4899-8071-6e5facc4e15d": {"doc_hash": "cb7ff9495e038a261ab2054ab383401a72424558fe11a61992c0871a36d45e2c", "ref_doc_id": "1c699067-f5df-4a3c-a513-45453dc811f2"}, "39af304a-cf03-4a1f-a482-d3a93f5e89e1": {"doc_hash": "4bf4f66c3b37850aec89c825c446c868794e6a0e087f6ddfa58aabbcb8978279", "ref_doc_id": "1c699067-f5df-4a3c-a513-45453dc811f2"}, "6f03331a-a943-4a29-aa3e-943022f4a6ad": {"doc_hash": "5793d92d9a51fa28eade863d5d770cf737f3124f06e9efb94ed957d53262f493", "ref_doc_id": "50ab3a10-e38e-4b1c-b2dc-eb1cd03df774"}, "66a3d680-a36e-4eba-83fa-4c0571f01cbc": {"doc_hash": "2a56b4dc103a06c350573ef6698b7930a721f208c46128921056371e54566658", "ref_doc_id": "50ab3a10-e38e-4b1c-b2dc-eb1cd03df774"}, "36a99eb7-2155-4c9c-a153-6277c93f7ef5": {"doc_hash": "7282189f1b4c8e3ee93133e0095cd16d0e93bc5f954d05df797e810a52899a89", "ref_doc_id": "50ab3a10-e38e-4b1c-b2dc-eb1cd03df774"}, "cba26f53-2cee-4563-bc1d-d4242dcc2437": {"doc_hash": "f984c0e806c0eb12a819eda8079e5a785d6401679df32ab2feaf4fe080165a91", "ref_doc_id": "50ab3a10-e38e-4b1c-b2dc-eb1cd03df774"}, "e3955352-61f1-4c45-92f2-b2c5f2638bf7": {"doc_hash": "27a8355ffef2e7a06395cd130664d3ecc8aa71057b210a2f0a55a02e1d5d0190", "ref_doc_id": "50ab3a10-e38e-4b1c-b2dc-eb1cd03df774"}, "249765ad-5eaa-4f4e-ace6-f144ca1a24f9": {"doc_hash": "993e5882338c2debec633cef720e6ca4e1ccdf9aa018f19fdb76c8b4af371e2b", "ref_doc_id": "50ab3a10-e38e-4b1c-b2dc-eb1cd03df774"}, "914af9df-8885-4724-b233-f1e8982aed9f": {"doc_hash": "cdb37977a325ab15cc0d4d0ff65ddfeb42399be2ce115ab457b59df2f300bb32", "ref_doc_id": "50ab3a10-e38e-4b1c-b2dc-eb1cd03df774"}, "af188508-8db4-452a-ba79-32646a92725a": {"doc_hash": "89181c2875f3ea572d5bfb7555d2023e46853ea16759e8cd2be54a46360ea53b", "ref_doc_id": "50ab3a10-e38e-4b1c-b2dc-eb1cd03df774"}, "332b15be-0f94-4319-8f80-303365cafe05": {"doc_hash": "c9a526fc25eb8d7b07d7c7d7a26cd9cc891cbbf4e354b06beb6488505750605c", "ref_doc_id": "50ab3a10-e38e-4b1c-b2dc-eb1cd03df774"}, "8388ee03-c887-4edf-aaaa-6ae89ec17414": {"doc_hash": "5b0afafcd489fc3e3492ebadf150cd0ded34c1e6fe18a1187bc2a745e5e002cb", "ref_doc_id": "50ab3a10-e38e-4b1c-b2dc-eb1cd03df774"}, "000671aa-f9cd-40c7-b6a1-66cf6c364ab6": {"doc_hash": "728b96f58af410e92ca0c7732fe909c1b2bf837855a613787682ffd60a16efb6", "ref_doc_id": "455c6fb6-4eb7-4b07-9b11-9699928b66da"}, "649498a6-2d08-49e9-a466-b4c5ff75a7ad": {"doc_hash": "bf24a850fbaf57ad612d7082e0a603e94d406f74b956324f15084c77701ba814", "ref_doc_id": "455c6fb6-4eb7-4b07-9b11-9699928b66da"}, "649c1c44-1348-47c7-9afa-23146c4cb920": {"doc_hash": "90c4232c38469c4acbd72d2c61a26e0f8719561bf1ec45392146f35e469bd7d6", "ref_doc_id": "455c6fb6-4eb7-4b07-9b11-9699928b66da"}, "dc6f5378-6152-4b83-922c-e8e504e0360d": {"doc_hash": "ec43a4aca5b824f4b361fa2081c8f6cb774ebe77fb72bfd52bfce05c6564afcc", "ref_doc_id": "455c6fb6-4eb7-4b07-9b11-9699928b66da"}, "fae7e716-817d-4257-9c2d-52554397f174": {"doc_hash": "d55cd8019ccf4e3aaf015bdca289dbb9d937b1a747a60058f7b73768d2306442", "ref_doc_id": "455c6fb6-4eb7-4b07-9b11-9699928b66da"}, "c947eab8-e9ec-44ff-a0aa-e1c58bcfe9d2": {"doc_hash": "46bb43b5ede01431be09a04c16b9026894b15401303ded31c2e140e1d4dd5380", "ref_doc_id": "11b375e8-e8d6-4d21-a47e-502ae9531bbd"}, "3769b6d9-3967-4afb-b744-22ff538b13e3": {"doc_hash": "12bb1319da730f59a03dd2f101f94cf0ac9158371700844dcd1f86ab1234cc61", "ref_doc_id": "11b375e8-e8d6-4d21-a47e-502ae9531bbd"}, "16501f8a-21f5-483b-8887-b09838a60245": {"doc_hash": "c7c01942b3c31f103a2289c4b33bfc284321b7ad989e443d4423cacd47701a0f", "ref_doc_id": "11b375e8-e8d6-4d21-a47e-502ae9531bbd"}, "1c99a89c-5f4f-4127-b451-be5243c479eb": {"doc_hash": "9435779303ccb31da99b679a5881bf797d34f01ea059c37fdab98b1f46e2d52b", "ref_doc_id": "11b375e8-e8d6-4d21-a47e-502ae9531bbd"}, "c009d4ea-ab0b-4343-8438-2a4fc7611a6b": {"doc_hash": "06a26352144a4dd4429cc7d34def019fa92831445d0897556eb3effc737a0a82", "ref_doc_id": "11b375e8-e8d6-4d21-a47e-502ae9531bbd"}, "9206c8eb-fdaf-4d75-9c5d-c6a5db0ffed9": {"doc_hash": "32b02dbaa38c766a8ee5ef0472aa5fd1714ade900ef7656527975b49c4929849", "ref_doc_id": "11b375e8-e8d6-4d21-a47e-502ae9531bbd"}, "7645b065-77c6-427e-af0e-453e135b751d": {"doc_hash": "9e18e14bb3b91680d4cab5dc15f9bcbe3ccf2c3a30d52939a9e2c914cfb2c910", "ref_doc_id": "f9f418a7-e124-424c-b5b7-b35cc94d96f4"}, "5d332cce-a3fc-4ece-bde6-11a55fdd5a67": {"doc_hash": "2e5aeaef5ce9f0ff85cdd9ea78bc26ad602648ecbded67c1d14bf57abf1fc7df", "ref_doc_id": "f9f418a7-e124-424c-b5b7-b35cc94d96f4"}, "434bed5b-9c63-482f-a9ab-5e56aa10cb2a": {"doc_hash": "c88298ff4674dac77139916e0009571299a8fd9fe623b3da07686479a578c1a4", "ref_doc_id": "f9f418a7-e124-424c-b5b7-b35cc94d96f4"}, "f04b2ee7-ef13-4efb-a7dc-749f7d707a7c": {"doc_hash": "9d7c2f30a55cd15ced89491ac1c3ef2c79d32ceaefdfd0330272649bca257027", "ref_doc_id": "f9f418a7-e124-424c-b5b7-b35cc94d96f4"}, "3c08ed13-db8f-48b0-b841-4ab23972f146": {"doc_hash": "cd48f4ecc2c7f77f09ec140035eacf6fbdcdf18b53e491634280c591f5f723f2", "ref_doc_id": "f9f418a7-e124-424c-b5b7-b35cc94d96f4"}, "5f23a11d-bd1f-49e0-b995-d33458feba0e": {"doc_hash": "1185e77eb7abf636549661abc0ceaf0eece15c60c4a3ec1d2bbd05738912bd01", "ref_doc_id": "f9f418a7-e124-424c-b5b7-b35cc94d96f4"}, "26d1be8a-0c5f-4f07-b243-d4a5c5e73577": {"doc_hash": "06df752978d10efb6c2bb9ebc0c98af3db6d18f9bec3727993e86905c082e9e1", "ref_doc_id": "f9f418a7-e124-424c-b5b7-b35cc94d96f4"}, "bc7669e1-ff69-4731-9506-1f6d4c07a3c1": {"doc_hash": "347ac8703147bdb861a12dc123a72e51209007df10d550492b0e413ff97774e7", "ref_doc_id": "f9f418a7-e124-424c-b5b7-b35cc94d96f4"}, "5a127ad4-5783-40f6-9725-5ad5f9b20ba0": {"doc_hash": "f903f4a7a02a7fbfa55088fa4c9e384c29a0bff90434da80fce689da053ef9e7", "ref_doc_id": "f9f418a7-e124-424c-b5b7-b35cc94d96f4"}, "85e7a5e9-8530-4cb3-9cd5-28445dc13d6e": {"doc_hash": "1a9f134c4b7d1f8571b8c61380356196bcf8abd366eed83d0e44c77517a6ee3f", "ref_doc_id": "f9f418a7-e124-424c-b5b7-b35cc94d96f4"}, "fc98d873-7413-48ac-8653-f6ffb1ccc589": {"doc_hash": "6ab41af4f9526d6c2c09f1d94781a7808248045b10e3334e95a4d81208075b76", "ref_doc_id": "cc03d67e-9f21-4521-b472-551874bf4c37"}, "43543bd7-41a5-47f8-8147-f214729708dd": {"doc_hash": "dfffa5acc30cc7cf3164db1b07307b1cfea2ecc36f43db6b17fc494a4d28a3ed", "ref_doc_id": "cc03d67e-9f21-4521-b472-551874bf4c37"}, "d5c37210-9e3b-421a-b6e7-0e1309f8d3a4": {"doc_hash": "e53cc068de79b66768ba1804fbf85c3e56b016cfb4608fe1851655871393416d", "ref_doc_id": "cc03d67e-9f21-4521-b472-551874bf4c37"}, "cf124a05-b249-4c64-b661-24da58101554": {"doc_hash": "92b66a6a2c4ea14175bb145354c3d290c4ab9c105943ea9cd931d6a2abeb9a30", "ref_doc_id": "cc03d67e-9f21-4521-b472-551874bf4c37"}, "82d45681-a1cc-4d2b-a19f-c5264aa75363": {"doc_hash": "a8ff0e84dfb5da6a9e76ef48da1405864db9af5a1ff203fbf2791e32939e0af5", "ref_doc_id": "cc03d67e-9f21-4521-b472-551874bf4c37"}, "349fbca7-4d4b-460c-9132-02ac5ea6fa88": {"doc_hash": "cb5d939aad75aea4e364bcfb70c075c2e97455a69c301f215b711dda8063e344", "ref_doc_id": "912df07d-6a63-4391-95b4-01eec2fb54f7"}, "b12870dc-ad6a-4439-b844-def01c4f956d": {"doc_hash": "11b28dc375bd604f7d76a0bed01ad99f026389b418edf124a77c7875788ae5cd", "ref_doc_id": "912df07d-6a63-4391-95b4-01eec2fb54f7"}, "3b97ba75-cde5-4162-8086-40ba2978feca": {"doc_hash": "4204c790ef4dcc6eb2fa3cbb9955a0ce9c1f8ea3e1f8ac13edaa79bfa436311d", "ref_doc_id": "912df07d-6a63-4391-95b4-01eec2fb54f7"}, "807e1318-818c-4b17-a1ae-45be84889df2": {"doc_hash": "9842cd7268d5de263db9d207f539e3ea5a2060b5fd5b6540bff2fe97c61ca59e", "ref_doc_id": "912df07d-6a63-4391-95b4-01eec2fb54f7"}, "d38d5355-2cdc-42b5-be74-15c561dddfbd": {"doc_hash": "292f6d5adcddf29ebf27b303c266913de5a0b707ee1f26620dbf5d768bc68c56", "ref_doc_id": "86c77f5d-1c44-44fe-a276-ab681d141894"}, "cfd90d07-f32d-4963-97d8-768828ad927d": {"doc_hash": "a28ea83b62d23d2527b9eab02714afc6e89713795fd53a0102b72968b85becda", "ref_doc_id": "86c77f5d-1c44-44fe-a276-ab681d141894"}, "f885f21a-8aa1-4987-a3da-f0480e2d32d5": {"doc_hash": "fad7aaa1c720f46c67055481ec4ca272aac1e4a502a1953a7caba0d2175fd610", "ref_doc_id": "86c77f5d-1c44-44fe-a276-ab681d141894"}, "70bc463c-ccbf-414a-a1d3-eeeccd876b8a": {"doc_hash": "c52c7ad9ccc4f9657cd52cdb0a3d618922c3bd6cf9515ef0d621f9cb1bd6daea", "ref_doc_id": "86c77f5d-1c44-44fe-a276-ab681d141894"}}, "docstore/ref_doc_info": {"79eb28d7-e2ec-4994-9d3b-20d8e9c18fc5": {"node_ids": ["a677fa02-6d9d-42e7-a2db-b9065adb193a", "aa39e8a5-b224-47bc-93ce-583a9dd22c98", "4e039c69-ca59-404a-b1e8-77af83825991"], "metadata": {"filename": "llamaindex-newsletter-2024-08-06.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-08-06", "date": "Aug 6, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-08-06"}}, "0d9fb0a3-eb34-4516-992d-0ca9644c6eb5": {"node_ids": ["9b26c34d-54af-4530-9762-9bbea0b23def", "4db6b442-65b8-46b5-b631-69489f0eb0d9", "2999d86c-b90c-43df-821d-1814e7ff0b48", "b3eb2234-5067-4718-a77d-01323fa73868"], "metadata": {"filename": "introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex.md", "extension": ".md", "title": "Introducing workflows beta: a new way to create complex AI applications with LlamaIndex", "date": "Aug 1, 2024", "url": "https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex"}}, "6e83ca1f-180e-4e5d-b417-df488eda2f0f": {"node_ids": ["d5f5f812-a8b0-40b3-8f51-66b2c49f53b4", "dffdd78b-ca74-4469-a6e2-22902100b64b", "35f3b3de-27fd-4c37-96cc-c5ff7a053302"], "metadata": {"filename": "jamba-instruct-s-256k-context-window-on-llamaindex.md", "extension": ".md", "title": "Jamba-Instruct's 256k context window on LlamaIndex", "date": "Jul 31, 2024", "url": "https://www.llamaindex.ai/blog/jamba-instruct-s-256k-context-window-on-llamaindex"}}, "62dafea5-0c5a-49ee-93da-d159adb5c9ce": {"node_ids": ["ea2a5822-ed60-4d58-9781-b44c2a990ac4", "f6c514cb-bfff-4e25-bbac-19d0f27c3310", "b67b4ca6-4385-4c60-8610-ca8bdcdf71ca"], "metadata": {"filename": "llamaindex-newsletter-2024-07-30.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-30", "date": "Jul 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-30"}}, "6692003a-9368-461c-84d3-d7bd2c425d07": {"node_ids": ["59245926-175b-4550-9356-21d4787a5282", "cca9a457-90ff-4392-b8cb-1d835d8018fb", "f958558b-14a0-43a7-b64b-becb1a547737"], "metadata": {"filename": "dynamic-retrieval-with-llamacloud.md", "extension": ".md", "title": "Dynamic Retrieval with LlamaCloud", "date": "Jul 30, 2024", "url": "https://www.llamaindex.ai/blog/dynamic-retrieval-with-llamacloud"}}, "81f29a86-fa6c-4ffa-b1e9-fdfaab10fba1": {"node_ids": ["043f5478-70af-4c13-94dd-f8f170d7f626", "ee0e7d22-a211-4531-94e2-055e5692d80f", "434b8bbf-a0c8-404a-bce0-4c88cbddd802"], "metadata": {"filename": "introducing-llamaextract-beta-structured-data-extraction-in-just-a-few-clicks.md", "extension": ".md", "title": "Introducing LlamaExtract Beta: structured data extraction in just a few clicks", "date": "Jul 25, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamaextract-beta-structured-data-extraction-in-just-a-few-clicks"}}, "888d9985-99d0-4ab5-9412-e40a25f14143": {"node_ids": ["d8ec7259-830c-4ce7-beb7-faeeaea47e5e", "5793b9b4-e936-4c8c-a986-4dcd5d8ef53d", "0d4bf7ae-fcea-47d7-b709-bd3bec637ca5", "3260b5a2-dace-438e-8877-ec7f3fc5c445"], "metadata": {"filename": "llamaindex-newsletter-2024-07-23.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-23", "date": "Jul 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-23"}}, "033242bb-7f8d-4a19-a628-ab5236c05991": {"node_ids": ["2798414e-f31e-44a2-86de-03188775da06", "947cb5e3-20f3-42c5-b5fa-4283600e5ba9", "b01e2db8-c6b6-40fa-96c6-aff5385ab659", "20c6f958-2273-459f-98e9-59241e26ca72", "a546027d-ea6b-45e8-863d-f1dac57f8702", "dca747ba-640f-4e21-87b1-5cf6e7d83654", "c00c7960-382f-4d06-832c-f9b9c35391a1"], "metadata": {"filename": "improving-vector-search-reranking-with-postgresml-and-llamaindex.md", "extension": ".md", "title": "Improving Vector Search - Reranking with PostgresML and LlamaIndex", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/improving-vector-search-reranking-with-postgresml-and-llamaindex"}}, "645df4f8-7d44-4511-a5c0-8c55474e99b4": {"node_ids": ["3bb32e4f-f76a-4d26-823a-366091ae03e6", "9a3136c8-0786-462e-9ba2-feb87d836589"], "metadata": {"filename": "the-latest-updates-to-llamacloud.md", "extension": ".md", "title": "The latest updates to LlamaCloud", "date": "Jul 19, 2024", "url": "https://www.llamaindex.ai/blog/the-latest-updates-to-llamacloud"}}, "1b0c2b01-a080-44e0-9dc3-e783628d2eee": {"node_ids": ["fdf5cead-ba49-4d5d-83eb-3854d4234766", "5d387f68-cc83-4a2a-b2a0-e73cb221e64f", "519117a3-c9ca-4cb2-9c3f-d711d84d20b3"], "metadata": {"filename": "case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud.md", "extension": ".md", "title": "Case Study: How Scaleport.ai Accelerated Development and Improved Sales with LlamaCloud", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud"}}, "c26ea0e7-6cd9-4a86-bcff-9ff279215e36": {"node_ids": ["02305182-6743-4f03-8342-d6461b02c6c5", "89dc47e1-3765-42a2-aa08-9c3ba27702c7", "8ef8ae00-5ed7-4536-a6d3-63683048e637", "99d0d58e-5992-42d3-ad15-fb8a337ac590", "404c7933-e8ce-48c4-9d44-d5e893380d1f", "78633de4-b8c4-47f8-b646-c3eb5d71e37a"], "metadata": {"filename": "building-a-multi-agent-concierge-system.md", "extension": ".md", "title": "Building a multi-agent concierge system", "date": "Jul 17, 2024", "url": "https://www.llamaindex.ai/blog/building-a-multi-agent-concierge-system"}}, "c745a3ea-f895-4b3f-aca6-71413beba7d6": {"node_ids": ["c27d85e9-7b67-4b32-9e32-f6f031d1735b", "c505e468-4788-49f5-896a-c8f103c9ee30", "c0151546-4171-4e92-b18b-70e28bd7c4eb"], "metadata": {"filename": "llamaindex-newsletter-2024-07-16.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-16", "date": "Jul 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-16"}}, "9f3cc26a-e7ce-4a76-9684-cbce575ad49b": {"node_ids": ["056e2fd5-2ef6-4f35-b05f-c955c1e653b2", "f65a9f90-b9e1-4a3c-a1fa-167210874251"], "metadata": {"filename": "arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications.md", "extension": ".md", "title": "Arize AI and LlamaIndex Roll Out Joint Platform for Evaluating LLM Applications", "date": "Jul 11, 2024", "url": "https://www.llamaindex.ai/blog/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications"}}, "f3d8b800-430b-447e-8711-5a07466261ac": {"node_ids": ["f52edd71-c7cd-4b8d-95c0-edadb1bff567", "218276ac-b3d6-40c2-8457-bca9bdbf321c", "10e1a522-d29f-4053-9922-b16e5104465f"], "metadata": {"filename": "case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex.md", "extension": ".md", "title": "Case study: Lyzr: Taking autonomous AI agents to $1M+ ARR with LlamaIndex", "date": "Jul 10, 2024", "url": "https://www.llamaindex.ai/blog/case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex"}}, "70ccc5c6-8e72-4a65-a7b2-86bf59b26221": {"node_ids": ["eb177c3e-fa44-4dd5-bc55-fa99ff7a7799", "4dfce439-df67-4e9b-ba22-a1512a12d200", "5b25908b-73ad-4503-90c1-74d98f67a3f4", "c3b96c4b-e570-46bb-a032-da22dd9edc86"], "metadata": {"filename": "llamaindex-newsletter-2024-07-09.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-09", "date": "Jul 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-09"}}, "daf66bb2-a0d8-4f36-a6e5-40ecb0ad9745": {"node_ids": ["45a52a84-1325-405e-8b93-d8c99499409a", "b3b07c3c-9974-4085-81b0-7c52c23e1343", "33bf00bb-dc2b-4f24-9145-449e9a06f346", "917501e0-e0ed-4a8d-b7ea-34abf7ea6265"], "metadata": {"filename": "llamacloud-built-for-enterprise-llm-app-builders.md", "extension": ".md", "title": "LlamaCloud - Built for Enterprise LLM App Builders", "date": "Jul 9, 2024", "url": "https://www.llamaindex.ai/blog/llamacloud-built-for-enterprise-llm-app-builders"}}, "71e4c768-21b6-44c5-9a87-672c07afb160": {"node_ids": ["fcd3e45d-e6f7-4965-840f-ddb8004dfebe", "6d08611c-16e1-4579-aa4f-8f55231f8cda", "28cfa0ba-2816-42ab-bf07-a970a381109a"], "metadata": {"filename": "llamaindex-newsletter-2024-07-02.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-07-02", "date": "Jul 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-02"}}, "b6700bfd-653b-4d5f-9db9-6f04c642990b": {"node_ids": ["bc7557e3-de9e-49fe-b1a3-68d2c128bc4e", "f2e7561c-89f2-470b-9c3e-163a1014475d", "18739f06-e810-4316-aa13-cb984bbd1ea9", "c21b1199-0bff-4a84-8d9f-0a3d94e19682"], "metadata": {"filename": "introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.md", "extension": ".md", "title": "Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems", "date": "Jun 26, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems"}}, "8d27ef78-ba49-4b72-a6a0-e9c3a62c3b14": {"node_ids": ["9c9fc031-8fb4-4175-bba6-2676a382f25c", "740931d1-e134-453b-9517-bd3ad041aee7", "97efe43a-7f79-4bd5-b561-95d363341bc4"], "metadata": {"filename": "llamaindex-newsletter-2024-06-25.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-25", "date": "Jun 25, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-25"}}, "0a5d297c-1c31-4730-89e5-e0b02aa9ef1e": {"node_ids": ["2a3a43a8-6e12-4bcf-962a-3d7fc15f2f40", "0ca2d89d-db47-4bfd-b892-da19dded09da", "60457367-3313-4aee-9caa-5970b843fa5a"], "metadata": {"filename": "llamaindex-newsletter-2024-06-18.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-18", "date": "Jun 18, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-18"}}, "55a84224-699c-4b12-9c4b-12367806480c": {"node_ids": ["edc33c91-72a3-491a-926c-f5ceb1ca4ca6", "bbd9d33d-6d02-462c-99e1-1bd7c177cab5", "1b99dd4b-5607-4a9c-a4a1-51a5307b0977", "451d84f2-5030-499f-b2e2-0a2761039850", "8427a98a-8271-40c6-a476-4bb7c73b2576", "0197b0e5-d1e8-426d-8bb8-2dfe11693e3e"], "metadata": {"filename": "customizing-property-graph-index-in-llamaindex.md", "extension": ".md", "title": "Customizing property graph index in LlamaIndex", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamaindex"}}, "2337ae24-e176-4aca-95bb-15c5bc0857de": {"node_ids": ["fd99a727-efbb-473e-91a1-1d712d6dc12a", "25770b6f-adbf-4d62-b8fd-0efc198cf728", "6a53f81d-7e22-4b61-b8ce-3c46c1b2e8b3"], "metadata": {"filename": "llamaindex-newsletter-2024-06-11.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-11", "date": "Jun 11, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-11"}}, "8f6774da-c6df-4b05-86e6-d291f430d976": {"node_ids": ["6d3e257e-727f-4ccf-9633-dc8fa0284ea8", "cfde95e4-5a2e-4d6b-9653-b14131bd52a2", "9f24f694-3817-41bf-a19a-5a9c44e58d6c"], "metadata": {"filename": "llamaindex-newsletter-2024-06-04.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-06-04", "date": "Jun 4, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-04"}}, "a80607e8-6bf5-42bf-9cbe-31a1388845ba": {"node_ids": ["570cd9f5-8273-46a3-b34e-1f581f3c11c3", "153d06e5-e3cb-48f0-9544-c2640cfc237e", "c419a347-46eb-4183-a3d4-7be3c5ec2d81"], "metadata": {"filename": "introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms.md", "extension": ".md", "title": "Introducing the Property Graph Index: A Powerful New Way to Build Knowledge Graphs with LLMs", "date": "May 29, 2024", "url": "https://www.llamaindex.ai/blog/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms"}}, "45edea59-d0cf-49c5-83a4-240649039ec3": {"node_ids": ["4547f9cd-fc3a-4db5-864a-a62a63dcc668", "fdbe389e-850d-41d4-9212-c783cdd76265", "81ad8b41-396b-424c-8a2f-0f7a73df51a5"], "metadata": {"filename": "simplify-your-rag-application-architecture-with-llamaindex-postgresml.md", "extension": ".md", "title": "Simplify your RAG application architecture with LlamaIndex + PostgresML", "date": "May 28, 2024", "url": "https://www.llamaindex.ai/blog/simplify-your-rag-application-architecture-with-llamaindex-postgresml"}}, "09ff30b1-4a55-41c9-8a24-0b49a7293cb1": {"node_ids": ["0d3ea23a-df0e-451d-a2d8-0f426b3b3bc8", "31722dad-a163-4722-baba-1bf8fe4db7f5", "b9700408-1613-40df-ab35-c1ed67472ee7"], "metadata": {"filename": "llamaindex-newsletter-2024-05-28.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-28", "date": "May 28, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-28"}}, "ee6537a5-76b4-467c-bdde-63d6310ac37c": {"node_ids": ["1912b081-3cca-42be-9db5-0230d93f0f47", "a2577cf2-3cc3-404a-923b-d6bb5d2be0f8", "0db569a2-bdf7-41f6-b7d1-0888824195d0"], "metadata": {"filename": "automate-online-tasks-with-multion-and-llamaindex.md", "extension": ".md", "title": "Automate online tasks with MultiOn and LlamaIndex", "date": "May 23, 2024", "url": "https://www.llamaindex.ai/blog/automate-online-tasks-with-multion-and-llamaindex"}}, "bb446d01-a356-4334-995f-9c48f9fd774a": {"node_ids": ["61a60dcd-a8f6-4800-81c1-5b399378ece9", "f9df290b-a6e4-4837-97c6-e002dc05b0fa", "32ea91e0-0f1e-412b-9dec-f279408e985e", "eb3df1f1-5478-4f10-945b-2e064039911b", "c59f89b2-7d31-428a-9bfd-d0d421809981", "fd379a1d-3770-4ef7-a1fe-dba9fdd1f28c"], "metadata": {"filename": "batch-inference-with-mymagic-ai-and-llamaindex.md", "extension": ".md", "title": "Batch inference with MyMagic AI and LlamaIndex", "date": "May 22, 2024", "url": "https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex"}}, "50909071-1542-4e51-b237-0fc7ae4c7343": {"node_ids": ["233c9a73-4a6b-4487-addb-f802e9facc2c", "00b6f41d-74ae-4d2e-90ef-397129c43695", "849ee03e-dc51-47b6-9342-23475e2aebce"], "metadata": {"filename": "llamaindex-newsletter-2024-05-21.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-21", "date": "May 21, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-21"}}, "2fd405cc-abb1-4276-a505-f0c54683c981": {"node_ids": ["1f9df9c1-aafb-4235-9c2f-59534251ac72", "2a452cb9-b9ec-4d21-844a-66f02127e785", "11704085-188f-4d20-a04f-7af15c810a1b", "1dbd87e1-a3a3-4fc7-90b9-1facc33d8802"], "metadata": {"filename": "secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.md", "extension": ".md", "title": "Secure code execution in LlamaIndex with Azure Container Apps dynamic sessions", "date": "May 21, 2024", "url": "https://www.llamaindex.ai/blog/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions"}}, "bafef8a1-f34a-4811-a591-36e13ee567c8": {"node_ids": ["39b61921-4798-46f0-adb7-964408b5ee30", "bc6a8923-7d50-4c27-a26e-14118d8627eb", "9cc8dc14-3b6e-4da8-ad44-070655b5ac67", "d833caa7-9457-44ae-9773-11cd1c95d4a2", "8be4bc96-a20c-44af-906b-110eedee19b2"], "metadata": {"filename": "using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md", "extension": ".md", "title": "Using LlamaIndex and llamafile to build a local, private research assistant", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"}}, "ac308aa5-05f0-4693-bec1-037bacdeb574": {"node_ids": ["90e6a7d0-f6b4-4b3e-af5a-f0e00faed3fc", "166658dd-c3b2-4f95-8aa8-fa5514696daf", "bdc0bf98-af01-437b-bcb7-ed8ed05085c9"], "metadata": {"filename": "llamaindex-newsletter-2024-05-14.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-14", "date": "May 14, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-14"}}, "d2235e16-18fc-4f83-a5fa-e394cdf57e7d": {"node_ids": ["d216f694-079a-4d62-9245-ae16d17a0451", "62e3d134-9d6a-466a-8be3-6dd61862701b", "1740996f-08ba-418e-bb10-932067e0f673"], "metadata": {"filename": "llamaindex-newsletter-2024-05-07.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-05-07", "date": "May 7, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-07"}}, "5c4b727b-76da-42d5-bbb0-8aa2d6fecbef": {"node_ids": ["a50e86bb-7445-439d-b248-974e2f76f7eb", "7fd4c6c2-9798-4b54-8908-a83c8167553f", "ac1226b9-8b01-49c0-937d-a1b1511395b4"], "metadata": {"filename": "llamaindex-newsletter-2024-04-30.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-30", "date": "Apr 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-30"}}, "12f4f396-4852-44b5-a835-e401ce95bb35": {"node_ids": ["596e6bab-2f92-40ce-9d63-1cdaa6528faa", "2b4b331a-d25b-4313-8bd8-b6e83df3ca96", "138e5330-8019-4f2b-a3e1-1a9cffe23a9f", "0f5312e2-5743-4d78-ba89-e715c2eaff3a", "11f4622f-0fac-47d5-ae35-2022bf4226d7"], "metadata": {"filename": "streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.md", "extension": ".md", "title": "Streamlining knowledge work with LlamaIndex, Fireworks and MongoDB", "date": "Apr 29, 2024", "url": "https://www.llamaindex.ai/blog/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb"}}, "92099854-6b11-439b-a990-718a7476119a": {"node_ids": ["3a037525-5661-45c9-8432-78b91891cd1b", "d683169e-0629-495a-9182-d6a60c242eb7", "b925097b-b92e-4a5f-911a-2c3ea81e6bd0"], "metadata": {"filename": "llamaindex-newsletter-2024-04-23.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-23", "date": "Apr 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-23"}}, "d4aed660-228a-454e-9d8b-f0402edd3d6b": {"node_ids": ["5e404499-d155-4e89-a838-240a6641b0da", "5360d1d2-2df1-4e9b-8fa7-1bb19441bdeb", "e1b4dbee-ecbb-4ee8-a407-844e22b629f5", "a6c71f0a-ce7f-4b32-88e0-c96ff56daf5b"], "metadata": {"filename": "llamaindex-newsletter-2024-04-16.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-16", "date": "Apr 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-16"}}, "e8da54c0-1c6e-4205-bfa1-e94c535666ad": {"node_ids": ["10c0cd5c-1f7b-4960-a65a-e7381e7468f0", "26947e47-2847-463c-b177-33acb517f78b", "3a0c7415-674c-4615-adc9-30878caf52ca"], "metadata": {"filename": "llamaindex-newsletter-2024-04-09.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-09", "date": "Apr 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-09"}}, "586dbf6c-0124-4c92-afea-b93b6368d319": {"node_ids": ["a529084c-9523-4372-8ff2-f6dd9f5b6978", "128c4ac9-7ee6-4378-a57b-a3323fc3af70", "50877f86-fc75-4faf-abc9-6936bcd6b5fe"], "metadata": {"filename": "llamaindex-newsletter-2024-04-02.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-04-02", "date": "Apr 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-02"}}, "ae3c90e6-c392-4877-8f3a-cd210ee29352": {"node_ids": ["89097361-fe0d-4ed9-9e41-9a65c664b4a5", "a3e6ec6d-644d-48b2-aaf4-4d06e2661f5d", "8f07df39-1b58-4c24-a551-2cf41b6e3f4c"], "metadata": {"filename": "llamaindex-newsletter-2024-03-26.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-26", "date": "Mar 26, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-26"}}, "285ce5de-c5d9-41ee-b830-567ecace871a": {"node_ids": ["a2343ae5-961d-41ce-8767-9ce5d4df3134", "0a7ae56f-18c7-41c0-aefb-20b649267ee9", "bda59a28-dc2a-4ac8-9c0e-89079449c61c"], "metadata": {"filename": "secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.md", "extension": ".md", "title": "Secure RAG with LlamaIndex and LLM Guard by Protect AI", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai"}}, "a862e693-ee31-4336-8f5d-fee03192c4d0": {"node_ids": ["67aab263-dfeb-4c2f-8b99-b98488533e09", "b296474e-5d28-4b9a-a11c-953298ebc5f3", "6c9e8377-b523-4ab7-b60b-44f3f01c1032", "fbacc335-cd04-4b6e-b3e1-526672cf5bda", "af8f5c03-5619-46e8-9b44-5086fcd51851", "a422e3c5-9810-47e9-ba7f-c6ba9a1c72b0"], "metadata": {"filename": "retrieving-privacy-safe-documents-over-a-network.md", "extension": ".md", "title": "Retrieving Privacy-Safe Documents Over A Network", "date": "Mar 20, 2024", "url": "https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network"}}, "9cae519d-8301-4f1a-aeaa-acfb3292c99b": {"node_ids": ["fa1b58eb-7d4a-49b4-8338-72f14fbb8aab", "de098a77-ad7f-42c9-a3db-ab3d8db35c36", "7a50644c-df14-4eae-ba9b-6d575af2d5cd", "7033d9be-effe-4e62-9074-76037665321c", "175ba084-a296-4e9f-b71d-b501d5a22ad4"], "metadata": {"filename": "supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.md", "extension": ".md", "title": "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations", "date": "Mar 19, 2024", "url": "https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations"}}, "fd63239a-c906-41ee-b807-30467082eeda": {"node_ids": ["4d8b3c20-1778-4967-a7af-f110282a989b", "b73e9c3f-7df7-4cb1-af7e-8ef34a8d24a4", "a056ff1b-5f59-4101-9172-35ad569a254a"], "metadata": {"filename": "llamaindex-newsletter-2024-03-19.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-19", "date": "Mar 19, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-19"}}, "4fa009e1-cf5d-4a11-9279-7d0fc2a8888c": {"node_ids": ["260ad09d-bcef-42c4-91bb-558ee9762842", "33ee8c71-de46-496a-87c4-3be3d580c4c4", "ed5f6a20-a67e-403a-840a-065155aa89a1"], "metadata": {"filename": "one-click-open-source-rag-observability-with-langfuse.md", "extension": ".md", "title": "One-click Open Source RAG Observability with Langfuse", "date": "Mar 18, 2024", "url": "https://www.llamaindex.ai/blog/one-click-open-source-rag-observability-with-langfuse"}}, "133fdeaf-3f1e-4672-8e84-db5a8958ecf7": {"node_ids": ["d6efb13b-fa6b-49da-ae6f-b3ab71a2c906", "471455da-d5ed-4ccc-b356-469a805e8bec"], "metadata": {"filename": "llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim.md", "extension": ".md", "title": "LlamaIndex Accelerates Enterprise Generative AI with NVIDIA NIM", "date": "Mar 18, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim"}}, "34dce242-97ab-4f72-a5e0-97a98365e7be": {"node_ids": ["56e6e5dc-9928-4491-9bdf-2ef14b86a35a", "66f83256-8a5d-4cfe-bd52-dccd52fb7b7f", "e08eb439-8c47-42a9-b61d-9e0ae87e9d49", "53887dbf-7135-4ccd-b275-8dfe44c4643e", "d9051207-6887-4310-84ec-d38f342dbb1e", "905219a8-1a01-4c15-9ddb-aeb8a4a2b17c"], "metadata": {"filename": "pii-detector-hacking-privacy-in-rag.md", "extension": ".md", "title": "PII Detector: hacking privacy in RAG", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/pii-detector-hacking-privacy-in-rag"}}, "f6a5ddab-07c3-418f-87a9-4d00c7c5da26": {"node_ids": ["fc135e87-5378-4b9a-9760-d4c8bd1f81e8", "d12785df-5e5a-46c7-b887-60400c3257c1", "9143fa24-704e-4121-8f67-9c7884d86800"], "metadata": {"filename": "launching-the-first-genai-native-document-parsing-platform.md", "extension": ".md", "title": "Launching the first GenAI-native document parsing platform", "date": "Mar 13, 2024", "url": "https://www.llamaindex.ai/blog/launching-the-first-genai-native-document-parsing-platform"}}, "ac98b60c-713a-42c1-a0ad-aac8ad68c12a": {"node_ids": ["fa83e3ed-d876-4370-9e1f-55f8cd65502f", "dab6a365-d236-4462-b42a-b9a8b93b2523", "381a8915-892c-4638-bdcb-eaff7a0295eb"], "metadata": {"filename": "llamaindex-newsletter-2024-03-12.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-12", "date": "Mar 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-12"}}, "bc3fc09a-4c0f-4c9d-af9b-9f3313592874": {"node_ids": ["588475ff-3808-4e3d-b7e3-83860dcfab8a", "16b414d6-12a7-4c30-b8d2-7aee4d0dabc3", "aa2e2706-accd-4073-a6a4-6d2f4745cb40"], "metadata": {"filename": "llamaindex-newsletter-2024-03-05.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024-03-05", "date": "Mar 5, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-05"}}, "cd0d369d-13e9-421e-aa70-c388b567acdb": {"node_ids": ["acac801a-11e9-459b-8301-46c67e452175", "02602e9d-2105-4940-8370-03865c493134", "718c3d47-5861-4b73-9869-ce3c66aa7553", "7b94dc85-06a4-4523-813a-452dea9f54a7", "dcc7c189-997d-4c59-b90b-7a960855455d", "2a5d2c4e-6d13-4767-9ccb-3a6afc90294f", "5ae21d20-8814-4335-acfb-06cff996debd", "75f9bc1d-d52d-4d68-9944-5ee41193f402"], "metadata": {"filename": "towards-long-context-rag.md", "extension": ".md", "title": "Towards Long Context RAG", "date": "Mar 1, 2024", "url": "https://www.llamaindex.ai/blog/towards-long-context-rag"}}, "337943d5-5cbd-4bed-b913-4521e0538d90": {"node_ids": ["45264645-55de-4c64-9052-ddec4ded2279", "a6759d47-370c-4801-afbe-de24b1b6b902", "cf2563e5-cac9-4f32-8926-560c46584cca", "01a98d61-b97f-464c-a64a-e0bc922e24db"], "metadata": {"filename": "unlocking-the-3rd-dimension-for-generative-ai-part-1.md", "extension": ".md", "title": "Unlocking the 3rd Dimension for Generative AI (Part 1)", "date": "Feb 29, 2024", "url": "https://www.llamaindex.ai/blog/unlocking-the-3rd-dimension-for-generative-ai-part-1"}}, "ed7eb9b0-c368-4e08-b1bd-46a0ad5c3a26": {"node_ids": ["beecd20f-0b6c-4d09-b1a4-9a117447a6db", "44a8f9aa-f002-405a-8a4d-bb65d640ede8", "fa07156d-ccb3-4eb9-a7f5-11b73c0f94ed", "8939bf29-d781-47e4-9f71-a48ed04e3bdd"], "metadata": {"filename": "querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.md", "extension": ".md", "title": "Querying a network of knowledge with llama-index-networks", "date": "Feb 27, 2024", "url": "https://www.llamaindex.ai/blog/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f"}}, "4532b31d-298d-4194-8b14-6f6600bd0257": {"node_ids": ["9a74719b-11be-43ed-b862-c3ca8b1eabff", "57ef073f-1ac3-4a8d-b96a-de078e1543e0", "af9e2a35-b430-4ac7-a010-7257d9e834bb", "618aa816-a97d-48b5-a5f6-65027c369d59"], "metadata": {"filename": "llamaindex-newsletter-2024-02-27-4b9102a0f824.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201327", "date": "Feb 27, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-27-4b9102a0f824"}}, "2e013f95-3d19-452b-b708-42053700a82e": {"node_ids": ["e0276ad8-d3d1-4e63-823e-7ccf677cf833", "9357d29e-c5e6-4598-8647-5b915b6751b7", "3c8b172c-a586-4077-a307-52d5e88e7293", "cd52511f-8cb2-4daf-9542-3e278bc8069e"], "metadata": {"filename": "bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.md", "extension": ".md", "title": "Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot", "date": "Feb 24, 2024", "url": "https://www.llamaindex.ai/blog/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3"}}, "9fc37359-c51e-48bb-804a-ef7d247412bd": {"node_ids": ["ae339b9a-8def-4927-89b4-16068e692e80", "3e012de1-bfcd-4851-b9b2-f29350ba5c39", "c1a44eb5-8121-4e86-804b-72dc2cf5484b", "eb34f723-a4f1-4ec6-9e13-4fca08352897", "abf4768d-cbeb-42c4-bebe-9921ec8a4e09", "191eee4f-1cb3-453f-9882-5a023ef3b322"], "metadata": {"filename": "introducing-llamacloud-and-llamaparse-af8cedf9006b.md", "extension": ".md", "title": "Introducing LlamaCloud and LlamaParse", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b"}}, "ea1967e3-2009-45c0-a71c-01bad66e14eb": {"node_ids": ["2b3ce324-70a4-4d36-971f-7e42bd1d34a0", "1e29cb32-defe-4a40-bd86-f22b5af0e2ce", "180f436d-aca5-4c60-9ee3-84f0c1f62231"], "metadata": {"filename": "llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201320: introducing LlamaCloud", "date": "Feb 20, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4"}}, "b8cd3127-a3c7-459c-a783-369cf6a33b74": {"node_ids": ["061febb8-3e45-4f38-abb4-3ccefaeedd23", "290c23c6-727d-49b9-a85c-f7036a47028d", "2a71302d-5033-48a7-8790-4617a480a82d", "f8ff0b29-eb62-41c6-b659-37665ce9ee1b", "c7d4ed71-33d9-4a5d-b36b-6d02aef48179"], "metadata": {"filename": "multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.md", "extension": ".md", "title": "MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB", "date": "Feb 17, 2024", "url": "https://www.llamaindex.ai/blog/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e"}}, "15ff97d4-bffb-4581-9df0-92c7b7f7734d": {"node_ids": ["67bd89c7-52bc-4c77-a198-80080f13d8c6", "251060f2-d29c-4482-9377-29d742582bc7", "6838fbe2-4f7b-46b0-9185-9c3c83708142", "d7bd5afe-8e98-4156-a2e9-6b323ba618c1"], "metadata": {"filename": "llamaindex-newsletter-2023-02-13-26fa79601ba5.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201302\u201313", "date": "Feb 13, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-02-13-26fa79601ba5"}}, "7711566b-e14a-4779-ab42-616803ecce59": {"node_ids": ["3f74226d-914a-41c0-bbf4-f839ea8a49d4", "619eb7c5-19b5-4c78-81cf-93def5e91d85", "f66a14f9-fe30-48b0-ba2d-605149fcc8d9", "ca181d59-c6b9-4bfd-8f26-7fae74584e12"], "metadata": {"filename": "pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.md", "extension": ".md", "title": "Pioneering the Future of Housing: Introducing GenAI-Driven ADU Planning", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f"}}, "4081ae4b-e041-4da7-b180-b7dd2925f42f": {"node_ids": ["4863a3df-c1d9-463e-b44b-5d867c6d456b", "f9c25d5b-9356-401b-ad9e-45cad86c4c8a", "8a335b17-4458-4884-ae09-14196a762a0a", "016945a6-5455-46f4-929e-cdc5b4a7263c", "a7324830-a2e6-4098-ad67-434c76fdfa7d", "d5963f23-cee8-44e4-9829-f0e3b21a2915"], "metadata": {"filename": "llamaindex-v0-10-838e735948f8.md", "extension": ".md", "title": "LlamaIndex v0.10", "date": "Feb 12, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-v0-10-838e735948f8"}}, "4fe84a89-4090-49b9-8cab-400b4b1abe3b": {"node_ids": ["38fb43a3-a889-41cc-bd0c-6fb0e50ce780", "be2d7434-f8da-4e82-8653-b5044604e165", "dd0fc390-b5c8-43c5-89a7-2a3ddbb1ef78"], "metadata": {"filename": "how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa.md", "extension": ".md", "title": "How to build LLM Agents in TypeScript with LlamaIndex.TS", "date": "Feb 8, 2024", "url": "https://www.llamaindex.ai/blog/how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa"}}, "7d0ec380-908e-4cfe-a9cc-2294cd62ec76": {"node_ids": ["a8135804-ab63-4597-96a7-159057558f35", "eb1954cb-075e-451a-bc4a-228bf923b8b0", "340b6f5d-ab5c-490b-bb80-b8ce6ec92b33"], "metadata": {"filename": "llamaindex-newsletter-2024-02-06-9a303130ad9f.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201302\u201306", "date": "Feb 6, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-06-9a303130ad9f"}}, "ce6327bc-87c8-422f-9e8f-e30fa9222be5": {"node_ids": ["c3ba5ae9-9611-46eb-9843-3a8af6ad93fa", "3f32db4e-c89d-4a96-9e35-77acabfc5ef1", "baab64ef-8c30-4272-a2db-4316cf1e4ca5"], "metadata": {"filename": "ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.md", "extension": ".md", "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "date": "Feb 2, 2024", "url": "https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"}}, "e5972247-4238-4dc0-8884-fe424808f9b0": {"node_ids": ["bad81132-00e1-4006-b70d-77e115ca1ee4", "35e17dcf-4d2f-4abf-a255-60db9414fc65", "29d66008-36de-4c9b-86d7-37809a635dbb", "0d47f71f-43ac-403d-b5ca-8e76f41bda22", "0d475a7e-8fba-4477-b219-f339c222f706", "ae0233f8-3d39-46d5-a534-ebb7eeb5470e", "c2529ebf-87af-40d5-b115-067c0a51316c", "f3ea832d-5230-4909-a5d0-32377f3c32ca"], "metadata": {"filename": "llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.md", "extension": ".md", "title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG", "date": "Jan 31, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00"}}, "8cc35d72-043c-479f-b0bd-499aac454c6e": {"node_ids": ["b3ce4930-8c30-4584-adbc-c0edfd3e4d1e", "83d83596-7bba-4912-ac7f-c2c8ecf3b71c", "c6c9c97f-004c-4959-a13d-af0a2566ddf4", "adb0e3fa-300e-41fe-9515-dae0c8fd5bc5"], "metadata": {"filename": "building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.md", "extension": ".md", "title": "Building a Fully Open Source Retriever with Nomic Embed and LlamaIndex", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4"}}, "56b78b37-dd0a-4961-83bb-e841cd578c6b": {"node_ids": ["f5735661-0bd1-4550-8402-80137bc4c456", "b30a738e-75d1-4a93-8f76-26a7fc4d836e", "d1613584-22e3-4529-9e3c-c824a8eeefe5"], "metadata": {"filename": "agentic-rag-with-llamaindex-2721b8a49ff6.md", "extension": ".md", "title": "Agentic RAG With LlamaIndex", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/agentic-rag-with-llamaindex-2721b8a49ff6"}}, "512d7ecd-13a0-4eec-a049-a41fbd99357e": {"node_ids": ["1cc00555-83ad-4f3e-a9d8-7b022044e05f", "fe3d7723-1c52-4fd9-8ea8-49bd23c63f75", "bc219ed7-fb2c-4e74-807e-5d875061ae96", "e178418f-8218-4de5-94f6-e0fa2c2ae364"], "metadata": {"filename": "llamaindex-newsletter-2024-01-30-0d01eb0d8cef.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201330", "date": "Jan 30, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-30-0d01eb0d8cef"}}, "b1660f5f-af05-40c0-aeb6-ae58303ad8bf": {"node_ids": ["2883a768-6223-482c-8bda-bcf82f594110", "7935ff8b-b1fe-4d26-9eac-b423de5155fd", "9521efa1-e6f9-47ea-bec2-bd9caea748e4", "281043d1-aed9-4ac1-8b37-a032571ecf6f", "22bd3cc4-87fd-4c88-ac9e-6fe2c6de8670", "d0a1f9d6-3dc6-4eb4-a6a3-3d9631fdd279"], "metadata": {"filename": "tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.md", "extension": ".md", "title": "Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex", "date": "Jan 26, 2024", "url": "https://www.llamaindex.ai/blog/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9"}}, "68c2afb4-da61-42bd-aab7-4000cc2aa8be": {"node_ids": ["acdafbb2-b8bd-4cb6-8651-cc92a56ee9f3", "b27bbcbd-6512-412d-98ee-ac97197315cc"], "metadata": {"filename": "introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41.md", "extension": ".md", "title": "Introducing the LlamaIndex retrieval-augmented generation command-line tool", "date": "Jan 26, 2024", "url": "https://www.llamaindex.ai/blog/introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41"}}, "e0e19263-36ae-4334-93ff-95f882469e66": {"node_ids": ["dc47a609-ed74-452a-a5ed-fe1c0e804eef", "00693a2e-e6d3-4649-acc1-0e04ae6c575b", "97f4c66a-1315-459f-9ae9-0eb0ffbf9e76"], "metadata": {"filename": "building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf.md", "extension": ".md", "title": "Building Scalable RAG Applications with LlamaIndex and Zilliz Cloud Pipelines", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf"}}, "0f766867-73f3-4e06-bd1e-3b879e6bd02a": {"node_ids": ["22912f4c-8db2-4d26-a771-96e895fdf90f", "abbe847f-9f06-47ed-9148-aa6eebe08901", "96e3351b-7ca0-4d92-96b3-eb0e4e243611", "b6cf405f-c94a-457e-8995-f94b75d9db5e", "ff00f6f5-830c-4309-bea0-7bf983820985", "5a147782-6e42-4a1c-8732-8bf0ec2a26a7", "142c0792-e5a7-475c-8a9c-a13eec8e0965"], "metadata": {"filename": "building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.md", "extension": ".md", "title": "Building a Slack bot that learns with LlamaIndex, Qdrant and Render", "date": "Jan 25, 2024", "url": "https://www.llamaindex.ai/blog/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840"}}, "84c4145c-1895-4f42-aa06-9247a881868d": {"node_ids": ["1ada1dc8-1654-4337-bd86-0cf3bf97db86", "a55b8e0b-2ceb-4942-8f97-2c1358dc1403", "48019a9d-8eb3-4045-9ccf-081eab9d65eb", "3b7a9f81-06f4-464d-b4b2-e3f4c402ce29"], "metadata": {"filename": "llamaindex-newsletter-2024-01-23-11ee2c211bab.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201323", "date": "Jan 23, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-23-11ee2c211bab"}}, "819a17cb-b272-4cca-9b9e-eabf7957697d": {"node_ids": ["fa32b6fa-21c3-4d3c-99a1-43f261c5781b", "d2c23206-e86d-4352-881c-3a20c737a790", "56c9efa5-38f7-4d0e-a148-18df7f4a28a5", "bdd78b24-dbcb-41b7-803d-60f35812df3b"], "metadata": {"filename": "llamaindex-newsletter-2024-01-16-752195bed96d.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201316", "date": "Jan 16, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-16-752195bed96d"}}, "a0fee36a-1d60-45bc-a806-3fd5d7e1a65d": {"node_ids": ["8553ee89-1e4f-4a85-b522-14eac71941ff", "65b9aeca-613f-4a52-bf71-ed062d7b8ee1", "fcacdd5b-b27b-4927-af75-b4fd9f305786", "840506da-759a-471a-8eaa-82ba31a344b2"], "metadata": {"filename": "building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.md", "extension": ".md", "title": "Building Multi-Tenancy RAG System with LlamaIndex", "date": "Jan 15, 2024", "url": "https://www.llamaindex.ai/blog/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b"}}, "7c8973c1-3304-48b1-8580-1cb04ed8c797": {"node_ids": ["9cf7c8b8-e4cf-4f7e-a882-e401956078d8", "cffb6521-86cf-48de-8b3d-2470ba9f3880", "92429ec3-28cb-4f85-ae09-d00556a1b25e", "192daad8-2b82-44d7-8eaf-edcee61d1f3f"], "metadata": {"filename": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.md", "extension": ".md", "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "date": "Jan 14, 2024", "url": "https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"}}, "ac7e9570-c8ee-402d-a176-7922bacf0a97": {"node_ids": ["a8d32e65-742f-47af-94ba-b9dc5978b82d", "40e3233c-b1f9-43da-9d25-deeb842164cd", "1b8bc2e4-461c-4678-8845-415d5812e559"], "metadata": {"filename": "join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb.md", "extension": ".md", "title": "Free Advanced RAG Certification course with Activeloop and LlamaIndex", "date": "Jan 11, 2024", "url": "https://www.llamaindex.ai/blog/join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb"}}, "c059752a-faa0-43e5-8e13-9df793b05d79": {"node_ids": ["41f386be-0356-4202-9fea-06b51384e45e", "c9ea4b57-cc2c-4a7b-9735-27521df6f2e8", "a8491b99-8663-41b8-b792-4c866442ec77", "37cfcf4c-06ec-469e-89b2-41d186e54067", "4b1757a6-9975-490f-a6e8-fe72da5bdd61"], "metadata": {"filename": "llamaindex-newsletter-2024-01-09-6209000da2e6.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201309", "date": "Jan 9, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-09-6209000da2e6"}}, "5ea01df2-9d80-441c-9644-23133cd296e9": {"node_ids": ["7b33f739-902c-47d8-894d-7523428b86e9", "13df4dc0-ddbe-445e-80a9-7e75c81b3c47", "2cd81065-2b75-4f47-a7a3-6aa57c31fa52", "161dd6a1-65cb-4f81-b7e8-923f22dc7da7", "4029841c-d639-4f67-b4ec-204a13585c56"], "metadata": {"filename": "introducing-query-pipelines-025dc2bb0537.md", "extension": ".md", "title": "Introducing Query Pipelines", "date": "Jan 8, 2024", "url": "https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537"}}, "77fee692-6ef9-4e8d-ad00-def27f2412b5": {"node_ids": ["65dacfe5-5846-462d-a0f2-e94162cf3d1e", "c44a504a-5623-4337-9e38-27269b6d0e70", "f0e1093b-4fa6-45e0-ad6b-47e63fe5ae85", "7adf07d5-df32-4cf9-8e17-e8a1c4f756de"], "metadata": {"filename": "a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.md", "extension": ".md", "title": "A Cheat Sheet and Some Recipes For Building Advanced RAG", "date": "Jan 5, 2024", "url": "https://www.llamaindex.ai/blog/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b"}}, "20a8b548-65c5-4f9c-a2df-1920263843c1": {"node_ids": ["0ef9d429-a42d-4a86-b132-0bcf9b9df7d5", "eee7c626-81be-42b9-a77d-2f0cf7d5140a", "9b00db98-5a7e-4558-a318-7418081dc0d7", "29a6eecc-32a8-4e33-a07e-19600e980267", "c087d458-7a77-4982-a459-ac995ad7b74e"], "metadata": {"filename": "building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.md", "extension": ".md", "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "date": "Jan 3, 2024", "url": "https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"}}, "25398961-e7fd-4608-acd6-cd550a028140": {"node_ids": ["a9bd8085-0ca1-4438-b1c9-1e1b0a348030", "67f27054-92f9-44e7-90cc-a76ddc333734", "5b2507da-9367-4259-ad7c-c18653a28d62", "cc63fa5b-4ef5-412f-b702-5f4372553f47", "12be728d-efb4-43a5-a7d0-7e7e501c8bc2", "304d1902-4c36-4fa1-8179-9086a042d84a"], "metadata": {"filename": "scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.md", "extension": ".md", "title": "Scaling LlamaIndex with AWS and Hugging Face", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"}}, "9c26f422-ea35-4946-917e-5016eab0ed12": {"node_ids": ["fb5a9f0e-e306-4cf4-b6a2-7b1b0a309ae4", "953e3a37-dc50-429b-ac34-e17ffb8ffeae", "4917c373-bb8d-4f5d-936b-9d5f16ae2f1e", "0b939c0f-7774-4aa9-b73c-3415eee41198", "b29f7e93-d59e-4d60-aaa6-36ef6e7ef66c"], "metadata": {"filename": "llamaindex-newsletter-2024-01-02-f349db8c1842.md", "extension": ".md", "title": "LlamaIndex Newsletter 2024\u201301\u201302", "date": "Jan 2, 2024", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-02-f349db8c1842"}}, "19393bcc-7aaf-4a70-a123-7165df7cae9f": {"node_ids": ["58bea4e7-1166-4ae8-aa0c-a54346ab7bfb", "3ff0d37c-c7ea-4ec6-bfae-4ada7afe0e76", "5a6dcc48-3cd1-4245-bad8-18f28281a24b"], "metadata": {"filename": "running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.md", "extension": ".md", "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "date": "Dec 21, 2023", "url": "https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"}}, "814ddc3d-e890-4b5f-8252-2560c4136e7a": {"node_ids": ["d616b854-2510-473b-9127-2149199f9782", "6666610a-8d21-430e-bb48-4f2822ef035b", "2be8bf24-10bd-4b0b-8b54-0728121e10ee", "20ea7e3b-a294-4238-b615-d6e3f8a98d23", "f0fbe445-6905-4625-8859-842f44ce3799"], "metadata": {"filename": "two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.md", "extension": ".md", "title": "Two new llama-datasets and a Gemini vs. GPT showdown", "date": "Dec 20, 2023", "url": "https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"}}, "0a0e4afe-6a76-4665-9740-12633d9b8098": {"node_ids": ["a30755c0-9451-431d-8aa2-5ee3bdac4daa", "45c16d28-b885-484a-814f-2aab8ab2d46a", "63c215d4-ab98-4e1f-940d-61506cc1c84c", "f2638c94-7b1e-429d-b4d2-6fad2484d474"], "metadata": {"filename": "llamaindex-newsletter-2023-12-19-2965a2d03726.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201319", "date": "Dec 19, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-19-2965a2d03726"}}, "58f9b62f-4cd5-4a6d-868b-9c239209fc9d": {"node_ids": ["cf650bd9-2ee0-4f89-9c2a-cae16a23b4fc", "f82eb55a-66bf-499a-a7f1-163a883839c6", "3b05f8e9-63ab-4e60-9362-05647e0c0eed", "80bdd7c7-89c1-4c22-8cb9-7b040ffda2fd"], "metadata": {"filename": "multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.md", "extension": ".md", "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "date": "Dec 18, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"}}, "ffcfae4d-8856-4f8e-a7a2-1c4ecc49fd5c": {"node_ids": ["344e4e8d-1eb9-4a20-b3b3-8a7053f52d7b", "b452c1da-d789-453c-8f86-81899915cae8", "10f1776e-bea1-4521-833c-9832bfcb42c5", "8f6d6e1f-d2f6-4789-bde9-43b234346f30"], "metadata": {"filename": "transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.md", "extension": ".md", "title": "Transforming Natural Language to SQL and Insights for E-commerce with LlamaIndex, GPT3.5, and Streamlit", "date": "Dec 17, 2023", "url": "https://www.llamaindex.ai/blog/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9"}}, "8c09340b-c9bc-4e99-81d0-4d3b5f27a93f": {"node_ids": ["f559afe9-d5dd-434b-930a-90abcd55113b", "50d9a68a-f6dd-4452-bb2f-e2c67611c944", "ec2fb48a-0649-452b-89c3-3f38ccae78c8", "9988d12a-5944-4e91-abf0-d66469d3ca5a", "4529d783-3a57-43d8-9913-a01811e3cf29", "b9f7b424-d60b-417d-a6e0-934fc2d8b648", "b4de9fca-ab0c-4684-8fbc-9bdea3f64b4b", "8d1eadd2-a059-450e-80f6-c1f8a176eabc", "62724080-a637-43af-a485-d5d285d3b98f", "78e4a0df-c49d-4182-bd2d-e64424e27f44", "bbab962b-3ab5-45d4-bfe7-0328168d8d39", "24093838-9e4d-4daf-8656-8a5f0dc0c2b3"], "metadata": {"filename": "llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.md", "extension": ".md", "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "date": "Dec 15, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"}}, "f11e9218-db47-48f8-bf87-3c3d700db39c": {"node_ids": ["34ca7382-0f07-4d95-878f-79c5ac5a1109", "537a9f6c-37f1-4b96-98ce-76bd3379e7fa", "67924a08-df78-449a-81d0-67f61824f6a5", "538124e1-771b-44f9-8773-7a265f55e1ca"], "metadata": {"filename": "how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.md", "extension": ".md", "title": "How to train a custom GPT on your data with EmbedAI + LlamaIndex", "date": "Dec 14, 2023", "url": "https://www.llamaindex.ai/blog/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070"}}, "02424788-2019-4be0-b478-3a4403d07983": {"node_ids": ["b64e4239-e8f2-4e03-86c6-374d0a68be7f", "270d7f32-048e-4298-a8f8-d5475ed6f4db", "a76b6744-d2b0-42b7-af6a-eb74b01c1472", "48100188-165e-4180-b1fa-44109f792ded"], "metadata": {"filename": "llamaindex-gemini-8d7c3b9ea97e.md", "extension": ".md", "title": "LlamaIndex + Gemini", "date": "Dec 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e"}}, "80ed9bec-3de7-42b1-bc6c-6f7c62e1ca1e": {"node_ids": ["bce8eb25-2a90-4f2f-af59-4ffc56ca64b3", "bd754407-c8c6-42d6-8b21-88e7d36e945d", "a2d34e56-51d6-43f1-bc18-6531d2e772c9", "16337efa-18e4-40f9-84ec-c47f5a031f51", "5bd74849-d524-4659-a25e-fe95b3e5380d"], "metadata": {"filename": "llamaindex-newsletter-2023-12-12-4a5d542fbb1e.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201312", "date": "Dec 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-12-4a5d542fbb1e"}}, "033e4d61-87ff-431e-a45d-74bf4eea80d0": {"node_ids": ["3f90e571-61d1-4103-ac8c-31b801915973", "607b2025-b5f4-4850-afd3-73e1e9e5b377", "8720746f-1e9c-4f9c-8065-81233cf191ae", "393c563d-253f-4de9-a4e4-099b04f03be6"], "metadata": {"filename": "bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.md", "extension": ".md", "title": "Bridging the Language Gap in Programming: Introducing AutoTranslateDoc", "date": "Dec 8, 2023", "url": "https://www.llamaindex.ai/blog/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8"}}, "fc551b46-b793-437e-9b58-99461950ca6f": {"node_ids": ["951cfd40-1b14-4096-a3ac-8edefbd45b61", "c8d02e82-9ebb-4540-bc3f-20043ad27aaa", "37c06b17-7703-4678-b96f-d344ad99bb12", "5b654dc8-9db5-4a77-b5de-0c4f300e8d5c", "60d1748f-c66a-4077-967f-01b8cedbc4a9"], "metadata": {"filename": "llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.md", "extension": ".md", "title": "LlamaIndex + Waii: Combining Structured Data from your Database with PDFs for Enhanced Data Analysis", "date": "Dec 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82"}}, "1f669af6-4a8c-4e5b-a391-e6ed61f214a2": {"node_ids": ["2e496535-e2ec-4ab9-97b9-5b5113debbce", "46a6aea0-e272-40a6-9fe2-f36e92efd13c", "d5d8d0a7-5d3d-4be9-9044-219e60047a74", "968ae840-c8ad-4899-a9ec-ad73d3d7032a", "7ccd7967-6a64-41f5-a71b-ea4d025e145c"], "metadata": {"filename": "llamaindex-newsletter-2023-12-05-faf5ab930264.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201312\u201305", "date": "Dec 5, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-05-faf5ab930264"}}, "9fe69c12-a585-49f8-b1c8-90f65d411cc5": {"node_ids": ["448dd6b3-e0bb-4d21-a69d-e73ec33d4532", "8d80b8c2-2c42-4132-b57e-5ef41908869c", "231b47c6-6612-4685-b050-7df9309db9c3", "8ae08cc1-28db-4050-84e3-7f12a0acf85f"], "metadata": {"filename": "introducing-llama-datasets-aadb9994ad9e.md", "extension": ".md", "title": "Introducing Llama Datasets \ud83e\udd99\ud83d\udcdd", "date": "Dec 4, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llama-datasets-aadb9994ad9e"}}, "9020d3c0-ea98-458e-b093-bfb19ba950a5": {"node_ids": ["c2d1d84e-259d-4a30-9999-26c83d3336c4", "b40da8aa-59cb-4a86-b053-bae827b6e345"], "metadata": {"filename": "openai-cookbook-evaluating-rag-systems-fe393c61fb93.md", "extension": ".md", "title": "OpenAI Cookbook: Evaluating RAG systems", "date": "Nov 28, 2023", "url": "https://www.llamaindex.ai/blog/openai-cookbook-evaluating-rag-systems-fe393c61fb93"}}, "9d682bfd-2112-4f76-b0c4-1d3fc55aa6d8": {"node_ids": ["0d82a1ae-9669-4d49-8154-fa057703200d", "8a7e9471-e116-443c-bd82-f85609fc5639", "72c08c84-aef1-4c33-aa1d-ce9415dd09fd", "b4eb55d3-e7d9-4439-8126-af7e0488f989"], "metadata": {"filename": "llamaindex-newsletter-2023-11-28-a31be430a786.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201328", "date": "Nov 28, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-28-a31be430a786"}}, "f2ac32cb-8853-4f0b-bf40-3a908cd68c45": {"node_ids": ["3486ff03-0424-440a-a08c-02f49b68da72", "b2d7354d-d242-421e-9294-b9be74b90c74", "94c2d12c-4bad-4e32-9294-6f3f24feda9d", "e4e17092-a87d-4fa5-9695-907c6cd848ed", "381ac082-ae46-47e5-bae7-c73907c6df1f"], "metadata": {"filename": "multimodal-rag-building-ainimal-go-fecf8404ed97.md", "extension": ".md", "title": "Multimodal RAG: Building \u2018AInimal Go!\u2019, a Pok\u00e9mon Go-Inspired App with ResNet, Cohere and Llamaindex", "date": "Nov 27, 2023", "url": "https://www.llamaindex.ai/blog/multimodal-rag-building-ainimal-go-fecf8404ed97"}}, "786afdd8-7c14-4b62-8622-503cacbf0cdb": {"node_ids": ["3325ba3d-d290-4020-9e3d-e594243ca8eb", "d8e581fe-c49b-4e6b-8b36-2e53bc33e2eb", "cfa03656-ff5d-4368-a396-6cb3b356f5f0", "43ec9c28-fec0-494b-880d-3edaecb1e4c9"], "metadata": {"filename": "introducing-llama-packs-e14f453b913a.md", "extension": ".md", "title": "Introducing Llama Packs", "date": "Nov 22, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llama-packs-e14f453b913a"}}, "3724df83-66e6-4336-ad5c-6eb97505d14c": {"node_ids": ["fcf0a4a5-36da-4ee3-a857-4fd021be8f66", "cc3d2d64-4f33-4c92-87d3-1e1daa229155", "15c9e7d2-1c89-4e8a-bde9-de53cdc7dc35", "66fba681-53a6-48d7-a112-798324730a22"], "metadata": {"filename": "introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.md", "extension": ".md", "title": "Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data", "date": "Nov 21, 2023", "url": "https://www.llamaindex.ai/blog/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1"}}, "40c02fcc-f4c0-4628-9529-19a1b7cee237": {"node_ids": ["7018f358-532c-4074-9d27-b36655a82e22", "8aaaefde-e2dd-4933-9b15-06de41397939", "732720c5-d8ca-4898-8730-dd6bc86812c1"], "metadata": {"filename": "llamaindex-newsletter-2023-11-21-aa3a71e339f8.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201321", "date": "Nov 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-21-aa3a71e339f8"}}, "fe05bb37-1ac3-4bbb-95b3-55d05f180eb1": {"node_ids": ["9c946997-cc83-41de-a59c-6ed89059e7ed", "352b5a2c-f77d-43b9-bff4-f7a9dfe07ee5", "17681f3e-8afb-4626-a2c9-971b0b60d1a6", "63cd6adc-43b1-42ac-9116-933b85930fb4"], "metadata": {"filename": "becoming-proficient-in-document-extraction-32aa13046ed5.md", "extension": ".md", "title": "Becoming Proficient in Document Extraction", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5"}}, "f71218bd-cd96-4a39-8a5c-a83ac287d7e8": {"node_ids": ["339c273f-e338-4c9b-a835-a980ade1db00", "0cfbf6b0-53c7-41f1-b190-6604128f2790", "5a834743-33b6-44a1-af90-680d35765b31"], "metadata": {"filename": "shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d.md", "extension": ".md", "title": "Shipping your Retrieval-Augmented Generation app to production with create-llama", "date": "Nov 20, 2023", "url": "https://www.llamaindex.ai/blog/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d"}}, "c44fa078-fabe-4a04-b679-98a3d6e9d1e7": {"node_ids": ["192d1825-da2c-4b53-950a-9890490f33bf", "e44d3a86-03ec-4479-be4e-8f2cf1868a29", "09bfdbab-8c90-409c-8d33-47b476827205", "bc4d3507-759d-45cd-a9d8-aa16b220a4e1", "4719f466-7177-4845-9a4f-833732001f7c", "16203309-295c-455e-a3da-8f5451dcce82", "aa01bc3a-ef98-4438-9b6b-5a0a4073ef22", "801743ea-ad6a-4814-9da0-e8ff56843ecb", "1ec20dc4-df55-48bd-b572-9b002bfad77a", "38df1837-5840-4389-a189-7896b715a876"], "metadata": {"filename": "gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.md", "extension": ".md", "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "date": "Nov 17, 2023", "url": "https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"}}, "294d42ff-4eee-42ef-8901-13835558b8ea": {"node_ids": ["015e6aaa-24b6-4db3-90d3-419abafe4559", "b54977b4-c6c6-4312-ab5f-b78ad510318d", "fbba97c1-b7ba-4d8a-882e-fa143267550f", "abf0869e-6647-4429-9896-d90ff94409c1"], "metadata": {"filename": "evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.md", "extension": ".md", "title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"}}, "d0ec3e5a-73d6-496c-96d5-f72de017f5c6": {"node_ids": ["a5e9221e-3013-40ca-8b49-f0847546dc00", "a85a2431-4949-4b1d-9c35-b8c0b90fb203", "cc447517-f0f7-4792-8fd3-b484c8495f2c", "7c2d01c4-84de-495b-b34d-79f90ed3c3a1", "d0a032b4-e6fb-4b96-8c24-19d369466173"], "metadata": {"filename": "improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.md", "extension": ".md", "title": "Improving Retrieval Performance by Fine-tuning Cohere Reranker with LlamaIndex", "date": "Nov 16, 2023", "url": "https://www.llamaindex.ai/blog/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b"}}, "4043b8fe-9170-4006-a512-902fbe629cbd": {"node_ids": ["dc6dde66-2c7b-48d3-a4b2-e73b88f13dfe", "def7a4bb-3a1a-4781-9470-e40c53c84d92", "16c592b8-2b29-4a3c-b6f9-62b014f337f0", "b16f0bc6-0c24-4392-b001-071fc5cbaec3"], "metadata": {"filename": "announcing-llamaindex-0-9-719f03282945.md", "extension": ".md", "title": "Announcing LlamaIndex 0.9", "date": "Nov 15, 2023", "url": "https://www.llamaindex.ai/blog/announcing-llamaindex-0-9-719f03282945"}}, "822085ed-0a59-4fff-ac44-48beec94b85a": {"node_ids": ["7b4eb05c-b17b-4d06-868b-0b11ec8d3674", "b8f3a337-bf5d-48a8-9498-c59c12b222db", "ad53a63e-01fe-44c5-b758-26938a14f9ed"], "metadata": {"filename": "create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191.md", "extension": ".md", "title": "create-llama, a command line tool to generate LlamaIndex apps", "date": "Nov 14, 2023", "url": "https://www.llamaindex.ai/blog/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191"}}, "163f57e3-00fa-46b9-af06-92027d7739ab": {"node_ids": ["206f525c-3084-440b-a80c-aa8daf5ec796", "2c333a53-d5ef-4abd-b2d4-049b72fbebb9", "388ef834-e2bc-4373-ad73-c45a5660a97e", "e360c4b2-5438-4fe5-93c8-29dc68a0f7cb"], "metadata": {"filename": "llamaindex-newsletter-2023-11-14-dad06ae4284a.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201311\u201314", "date": "Nov 14, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-14-dad06ae4284a"}}, "15668294-2e00-4059-8c83-b1f6c9f7058a": {"node_ids": ["f62b90cb-44d9-4643-b66f-15b2cb832f11", "93fa4e33-c1cc-4b3b-9b32-e98bc20264bb", "9577e07c-b4dc-4143-b677-6268ebda0584"], "metadata": {"filename": "llamaindex-turns-1-f69dcdd45fe3.md", "extension": ".md", "title": "LlamaIndex turns 1!", "date": "Nov 13, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-turns-1-f69dcdd45fe3"}}, "92d1e148-39e4-4191-bffe-29e0c382ed6b": {"node_ids": ["cb55c0c7-f019-4d66-93e2-62766d2ca7b1", "534cce68-c8bf-45a6-8f8d-625ec15b52cb", "25d3115f-bcb6-45da-9182-1dde299cca90", "f8b823b7-5a55-49f8-8f11-5549440d9f91"], "metadata": {"filename": "multi-modal-rag-621de7525fea.md", "extension": ".md", "title": "Multi-Modal RAG", "date": "Nov 10, 2023", "url": "https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea"}}, "49984c49-8047-4f24-b368-c323ea28475e": {"node_ids": ["22225985-0132-49ce-8a92-8fc72c2ac428", "ad9e678c-aa5a-45d0-aafd-05149455b1b5", "6a31b7a5-2f0d-4ad7-8b74-a0baae7b555f", "2b220322-7f99-48ff-b12a-b8bf18050123"], "metadata": {"filename": "building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.md", "extension": ".md", "title": "Building My Own ChatGPT Vision with PaLM, KOSMOS-2 and LlamaIndex", "date": "Nov 8, 2023", "url": "https://www.llamaindex.ai/blog/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566"}}, "eeca3be1-f689-470a-8579-92f5bd5b94f8": {"node_ids": ["34a49539-768e-4306-a218-d966d40bb1eb", "a82c9c59-5dfd-4d1f-af51-657f3fab04ea", "f69d6d7a-1a66-4cc1-9764-e2e458a7295f", "9b223462-52a5-4956-8c3e-810a14c77536"], "metadata": {"filename": "llamaindex-newsletter-2023-11-07-cf20b9a833aa.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023-11\u201307", "date": "Nov 8, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-07-cf20b9a833aa"}}, "39b782aa-ccc7-48e3-a0bd-7c22c696b36f": {"node_ids": ["d8f02931-738a-488e-a5d0-4c28cc65e2ed", "371fd1a5-8921-46a4-83ac-8a9e0788ab95"], "metadata": {"filename": "llamaindex-news-special-edition-openai-developer-day-e955f16db4e2.md", "extension": ".md", "title": "LlamaIndex news special edition: OpenAI developer day!", "date": "Nov 7, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-news-special-edition-openai-developer-day-e955f16db4e2"}}, "324cafbc-56c7-404b-b679-7ed0ea4b23d9": {"node_ids": ["87df5e70-2944-4abc-a728-e8d839c47103", "985ae504-25c1-4f63-a8f8-c7b28dd690b9", "09743600-2dbe-4a7b-bea5-3e1d09390023", "a8b58843-89ce-4db3-b16a-31dca33f9e38", "089fbe81-bfa7-4cb1-9b5a-9aa4a231ce5e", "f8325e61-2985-4bfb-987a-a8ae2fa3fc43"], "metadata": {"filename": "longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.md", "extension": ".md", "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "date": "Nov 6, 2023", "url": "https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"}}, "524180ae-69c6-4a66-87dc-435ff90bbbe2": {"node_ids": ["cd358be3-afc4-4fb7-9b49-55586b6c9298", "1c722851-0779-44b8-a2ab-896ceff2ba75", "54e3f422-fa81-4877-a646-9e4f48b07b9d", "c7446b39-edf0-46c4-bbfa-49b4ec76863e", "c7ee99ec-2767-4115-8495-04f916ddc3cc"], "metadata": {"filename": "boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.md", "extension": ".md", "title": "Boosting RAG: Picking the Best Embedding & Reranker models", "date": "Nov 3, 2023", "url": "https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83"}}, "0af58c14-b89b-4e71-b0c0-f44aecc01df8": {"node_ids": ["595c0b11-b3e7-4840-9c73-bb1ac92b924a", "e511e3c2-9221-4837-ba18-6686c38be5ac", "b636bcb5-d351-4b53-803c-7d1eb05678ff", "b1973930-6a86-4dfc-b1a4-0cf7d4760fac"], "metadata": {"filename": "llamaindex-newsletter-2023-10-31-36244e2b3f0c.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201331", "date": "Oct 31, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-31-36244e2b3f0c"}}, "0a6424e4-e254-446e-ad5f-1516a078a21c": {"node_ids": ["6fe93808-454e-4eaf-b721-806769a9bb15", "def23383-0da5-463e-8a2f-248cd2cfe5bc", "0ed8b746-3bb7-4c03-87a4-9ac4af8fc31e", "369b8ffe-a535-4ab4-8ff2-1b9f7b88d747"], "metadata": {"filename": "newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.md", "extension": ".md", "title": "NewsGPT(Neotice): Summarize news articles with LlamaIndex \u2014 Hackathon winning app", "date": "Oct 27, 2023", "url": "https://www.llamaindex.ai/blog/newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11"}}, "0241af91-0a6c-4c3a-8415-514c401f8eec": {"node_ids": ["af751b91-a0f4-4d61-9ca1-99b542f60916", "a535a019-debf-468c-beaf-28f050bb29ca", "ee02cec5-bf9f-46bc-97b1-beea8709e4a7", "be92b2de-809b-4da8-824d-cb203657c46c"], "metadata": {"filename": "llamaindex-newsletter-2023-10-24-4a76204eeaa3.md", "extension": ".md", "title": "LlamaIndex newsletter 2023\u201310\u201324", "date": "Oct 24, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-24-4a76204eeaa3"}}, "3f0176f8-5129-4bbb-8c61-038dc7513060": {"node_ids": ["df20729e-935e-42c6-a041-62105847891e", "61a59674-815e-41d6-a897-95f2b48ff29b", "a7f44c74-38db-4fec-91ca-4aa9de208d0d", "24ebc2b3-20dc-4ec6-ab48-7704567ecf90", "ad77265a-ed10-4356-bd52-4265ff1070e3"], "metadata": {"filename": "nvidia-research-rag-with-long-context-llms-7d94d40090c4.md", "extension": ".md", "title": "NVIDIA Research: RAG with Long Context LLMs", "date": "Oct 22, 2023", "url": "https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4"}}, "1788f98a-120e-4cf7-ab78-b49e3b4383fe": {"node_ids": ["84a8c831-c6dc-4ea5-a62b-69cd31631f93", "41b82351-bf95-40c6-84ab-dd6d1d6bd769", "775ec11e-d34f-494d-9736-a124b5b79b9c", "8e026835-2536-4eba-8a03-e49b770da1b8", "6468bcb2-4947-46fa-bd76-75e7c41defd0"], "metadata": {"filename": "mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.md", "extension": ".md", "title": "Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125"}}, "bb3697bd-299a-4dfa-90d5-53dd8c4e0aa4": {"node_ids": ["fcd8e912-0195-4e24-9c21-8509b225fbbf", "d363544b-f39e-4422-9265-f88b47679c6d", "60102647-dcc7-41b6-bbf6-fa829bfc87ec", "28c624cc-f041-4233-bb90-d46080f91ebb"], "metadata": {"filename": "improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.md", "extension": ".md", "title": "Improving RAG effectiveness with Retrieval-Augmented Dual Instruction Tuning (RA-DIT)", "date": "Oct 18, 2023", "url": "https://www.llamaindex.ai/blog/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d"}}, "364aebea-6a85-4c68-bd51-9a0c1fe3ff8d": {"node_ids": ["511c6606-3983-4f53-aeb2-382f61a2184b", "e4adfc69-a069-48a4-b7b0-08dd3a729406", "ea37959c-36ec-4424-b249-d509ab852e41", "5b83b813-8a9f-4100-a4dd-355d4d3acf8d", "eb1ceaf2-d210-45ec-8739-d19d19229471"], "metadata": {"filename": "how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.md", "extension": ".md", "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"}}, "198fece2-5bff-4144-8779-547f01b4661c": {"node_ids": ["a373ce28-04de-4522-9d1c-143baaf09faa", "945c0b0d-da50-4ead-b037-de677f0acd9b", "43845168-c598-4d15-aaea-3442fb59738c", "964b65b3-b31b-4859-9c5c-e03364bddfec", "bf8120ab-67e5-46b2-85ce-99d3a7f94c11", "fc613e71-6d0f-4670-b40f-af0174adf3a6"], "metadata": {"filename": "llamaindex-newsletter-2023-10-17-33514cbc04a2.md", "extension": ".md", "title": "LlamaIndex Newsletter 2023\u201310\u201317", "date": "Oct 17, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-17-33514cbc04a2"}}, "a4960151-cc26-4057-8056-565439ffd414": {"node_ids": ["61e3a267-bbed-4df5-8c56-93b233d3d696", "05405b3d-281a-4186-9d2c-9b81ffba9eaa", "0d89deec-c3b6-4ed3-8676-3c8d80cfba25", "091c4290-f2e2-4bfd-bc40-b9fc6f17d198", "c5875733-82e8-4baf-8b09-2c911cf02b4c", "0da4b06c-6cde-477e-a489-bc841281c89f", "bec9f5fb-2778-4e3e-a321-ff2c79883c45", "8f1131c9-1525-4199-955c-66e020f1c690"], "metadata": {"filename": "llamaindex-update-2023-10-10-3718a3d19fb9.md", "extension": ".md", "title": "LlamaIndex update 2023\u201310\u201310", "date": "Oct 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9"}}, "834eaff1-0a04-4c4b-a7fe-b3859be73a69": {"node_ids": ["9862ca5c-3166-48b0-828c-21b8b61ffb53", "ed8d5543-956c-4f20-9f13-d9bd3d42c116", "2151e821-2644-4e0f-8783-f09f84eea08c", "f87720ff-a172-48a9-9d3d-c82aed6981d6"], "metadata": {"filename": "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.md", "extension": ".md", "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "date": "Oct 5, 2023", "url": "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"}}, "92c32a37-38b7-47aa-bc5f-5b3b3abdc47e": {"node_ids": ["6d027f33-c6a8-44ac-8e7d-6c3b018395fd", "19a8b33e-e0e7-4055-8619-0616ae29e740", "6b589868-02c2-4f87-88e7-866141f66d52"], "metadata": {"filename": "llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff.md", "extension": ".md", "title": "LlamaIndex + Laurie Voss: an alpaca joins the llamas", "date": "Oct 2, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff"}}, "1bce5740-226b-40a0-8fea-8fb8455ea975": {"node_ids": ["224b5f28-ffa6-48f9-bb42-da5a5721d422", "d2ee521d-9790-4922-b86d-2e1f3d122dda", "04f27749-44bf-4da4-931f-97f785159d4f", "ff83150b-a794-4297-a9bd-5d149494424f", "80f4fc6a-c426-4c80-9b00-9a8b2dac6e78", "032e89bb-be00-474a-861c-fbd9d2648674", "62f3aebd-f97f-4b2f-9b3f-86fb3c83978c"], "metadata": {"filename": "timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.md", "extension": ".md", "title": "Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications", "date": "Sep 27, 2023", "url": "https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0"}}, "c03101fd-c33e-428e-98cc-850dc59baa6f": {"node_ids": ["673f9672-867b-4fc9-be46-3d59b6527fa4", "cfa0f7cc-794e-42bd-8df1-e591cbd48a3f", "f21144d7-2e27-400e-9214-6c0e6d00803f", "590e9080-8837-454a-89ab-eaadd605821d", "e6cb8cf0-23e9-4d62-bef3-91f26346153d", "f0260805-3129-406f-8391-abe3f258a4b2"], "metadata": {"filename": "llamaindex-update-20-09-2023-86ed66f78bac.md", "extension": ".md", "title": "LlamaIndex Update \u2014 20/09/2023", "date": "Sep 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-20-09-2023-86ed66f78bac"}}, "14687a53-0578-47a9-ac10-048904a69276": {"node_ids": ["066f374f-9302-4926-8162-d2396b7bfc01", "0cdbc951-b42b-43a5-b22c-68ecd6243377", "2ab76f83-3dc3-4ee1-aa23-6f3d5b67b56a", "8f78586b-37bc-4bcf-8979-a8e08e9a7dc8"], "metadata": {"filename": "llamaindex-vectara-7a3889cd34cb.md", "extension": ".md", "title": "LlamaIndex + Vectara", "date": "Sep 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb"}}, "36c588af-23f5-402f-b06b-dc8285afc728": {"node_ids": ["5d5af6f3-eb70-4cb5-b0d0-5a3d4b147618", "c2eb2835-9ba6-4a5c-a98f-ea5e485429d4", "b83794da-0574-4954-b8c7-5c8d2294337b", "c15036c5-c315-489c-b2a3-a17d41798fc9", "35eae915-917c-4612-91e8-ee556488c079", "be94974d-5da9-401f-afc1-05865e901ac5", "6a67ab9b-8d25-4987-9fa9-3e915769a0ee", "1e7afcca-75e4-4309-a9fa-54bc294f4918", "aea65467-b9c4-4e50-8832-a2dd2be570d5", "909e284a-6c8f-4290-b3c9-a8aeb832eae1", "71891c40-c446-424d-93c2-9e516c34a890", "a025fe32-a6aa-448c-ad69-099fa95e1b35", "b26a0614-6bf1-4cb2-aa5f-d7b11ad254d7", "76569759-779b-4f28-9338-f12cc3cd07cb"], "metadata": {"filename": "llamaindex-update-09-03-2023-4a7c21c0f60b.md", "extension": ".md", "title": "LlamaIndex Update \u2014 09/03/2023", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b"}}, "2f25b56e-7640-4445-adf3-27a80a5e62b2": {"node_ids": ["55887fe7-0029-444d-b5e8-de80cd66f4a5", "e92d4bb3-fb4e-4147-beb1-a2e3fdaffa0d", "f140c749-e4c0-4fe7-b5fa-0b816c2edb20", "a153adf2-d711-4f8a-a736-f80c61e80676"], "metadata": {"filename": "fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.md", "extension": ".md", "title": "Fine-Tuning a Linear Adapter for Any Embedding Model", "date": "Sep 6, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383"}}, "03d5b93b-ec0f-4c92-9e23-4d7185e9e703": {"node_ids": ["8421e132-1b64-4199-be5e-ca2e7fa4f2fc", "0c6f5af0-d4ed-41b9-a34d-9aa87fd60111", "72f51578-3ec9-49bf-a46e-8968f52b7951"], "metadata": {"filename": "chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c.md", "extension": ".md", "title": "ChatGPT\u2019s Knowledge is Two Years Old: What to do if you\u2019re building applications?", "date": "Sep 1, 2023", "url": "https://www.llamaindex.ai/blog/chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c"}}, "f6e5d8be-352a-484e-af22-cdf2841640b5": {"node_ids": ["79e6915a-78a2-48b0-a216-2cbd9e0ae05f", "4ddc887d-82d7-437a-9f2d-9595ab84a39f", "5e1821c9-0c49-4c8b-85a2-09ca1777f1bd"], "metadata": {"filename": "introducing-airbyte-sources-within-llamaindex-42209071722f.md", "extension": ".md", "title": "Introducing Airbyte sources within LlamaIndex", "date": "Aug 29, 2023", "url": "https://www.llamaindex.ai/blog/introducing-airbyte-sources-within-llamaindex-42209071722f"}}, "3035a753-bc13-4060-b9c2-2b453fe5b319": {"node_ids": ["e4bbdf3b-8778-4a14-a285-40d6e24868e3", "d3b040b3-4724-4cc0-b9f7-70ae0b3b7813", "aa7e0b9d-4c2e-4914-9ba2-cc381ce1bfbe", "715a4591-4278-4d67-8ab7-1d9ddaee2340", "58ac2940-929a-4289-9e94-c4c603682483"], "metadata": {"filename": "llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.md", "extension": ".md", "title": "LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases", "date": "Aug 29, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af"}}, "ffc1e5ca-3684-4aba-b4bb-8e22550bbbd3": {"node_ids": ["2dcb4780-e9c1-456a-8ba1-a41033e5772d", "87fcb361-7153-4ff3-9919-035be55520e9", "613b6bc4-d901-4cd4-8b4a-6acde642f3bc", "5d846d79-1450-4e8e-952d-12fcfa1d2e8e", "2bf542ad-c282-49a7-b2f5-913b01325080"], "metadata": {"filename": "fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.md", "extension": ".md", "title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "date": "Aug 25, 2023", "url": "https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"}}, "340f46ae-5714-42b4-84ab-42bf54612094": {"node_ids": ["c0b81199-539a-4f24-805d-d5b08fbc3b05", "50681b0d-1c8a-4207-9e45-475b92178e91", "8c4ae5c2-1a3c-4798-b4e8-fe81ab38cf1b", "0d5bf640-e9ca-4b1f-ac08-281aeb32efcb", "c2bfffa7-5a3b-424d-a87a-f5948453de81", "49fbcc0a-9ef7-406e-b580-8e0b77b30268"], "metadata": {"filename": "llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.md", "extension": ".md", "title": "LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs", "date": "Aug 21, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f"}}, "bab77fe8-f4b9-466e-95ea-cf9ad876baf4": {"node_ids": ["0f63b1f8-0880-488c-b11c-5da0f987ff32", "1c5bff10-f3b9-4caf-9453-080d6531f289", "dff0714c-ca56-4a5d-8071-f090cdb580ce", "3c98a4e4-1786-4f92-80ff-ba018be5ae7d", "4c18ad5d-bbcf-442e-8d17-f32b288c853e"], "metadata": {"filename": "easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.md", "extension": ".md", "title": "Easily Finetune Llama 2 for Your Text-to-SQL Applications", "date": "Aug 17, 2023", "url": "https://www.llamaindex.ai/blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d"}}, "9f5e7985-169b-42e2-af2c-99878578fd00": {"node_ids": ["b701c90b-3bde-4a5b-a5eb-886e47f1e748", "9f96fdff-5315-43bf-a356-c63c7149ac4e", "2d5b3a3d-a8bf-4b1a-9c24-1143b12ad18f", "0212bcbf-721c-42a7-909e-c5a1632be4b6", "108b0c19-c33c-4faf-af99-5b35d32d708c", "ee032901-d0bb-4bf8-b84b-7c51d379c548"], "metadata": {"filename": "llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.md", "extension": ".md", "title": "LlamaIndex: Harnessing the Power of Text2SQL and RAG to Analyze Product Reviews", "date": "Aug 12, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b"}}, "635b2115-d0c1-4c5f-9b24-cebc3f4db1ec": {"node_ids": ["5284a101-cc2e-4e81-b5c3-c7d442b4d288", "7b554c31-66ed-47fd-98bd-b7168cdbe402", "8d107517-6473-4bb7-9fa1-0416c06bb656"], "metadata": {"filename": "zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc.md", "extension": ".md", "title": "Zep and LlamaIndex: A Vector Store Walkthrough", "date": "Aug 11, 2023", "url": "https://www.llamaindex.ai/blog/zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc"}}, "bc2828e9-fbd2-4fe5-96df-1001a43afd6c": {"node_ids": ["a9e14192-8020-4c06-bbd7-3189c19e7a11", "a1ff3678-dd54-4c4c-86af-14dd23a06df3", "3d7d4754-97cb-42e2-a69f-1f5143e7178c", "ca64912f-dfa1-4351-b428-8e45b105672f", "3d8ef2d0-1688-4a68-8c71-ff04421feb49", "7285fb7d-92db-4838-9e6b-0e1a58a6d3c6", "3f3c29b3-abde-4e89-9a55-b59a6619bec6"], "metadata": {"filename": "llamaindex-update-08-01-2023-185514d9b897.md", "extension": ".md", "title": "LlamaIndex Update \u2014 08/01/2023", "date": "Aug 1, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-08-01-2023-185514d9b897"}}, "e0160dfb-5f5f-4596-9b02-10e879ce4929": {"node_ids": ["8f92f2dc-465c-4a3a-ba2c-cf29d9b4755f", "3d506bfd-465f-44a6-a811-0c8830fe9323"], "metadata": {"filename": "data-agents-zapier-nla-67146395ce1.md", "extension": ".md", "title": "Data Agents + Zapier NLA", "date": "Jul 25, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-zapier-nla-67146395ce1"}}, "301b8db7-e100-4df2-966e-691d2ebcffb3": {"node_ids": ["1ddcd0fb-65db-41c3-8bf8-766a5beb6f9f", "746aa0c5-e71f-4898-ab77-7726792b00f2", "5c832198-4c34-4caa-a5a8-cfb19960a642"], "metadata": {"filename": "introducing-llamaindex-ts-89f41a1f24ab.md", "extension": ".md", "title": "Introducing LlamaIndex.TS", "date": "Jul 24, 2023", "url": "https://www.llamaindex.ai/blog/introducing-llamaindex-ts-89f41a1f24ab"}}, "8078c614-37b9-4aa0-8f9d-96fdb8e4c9a8": {"node_ids": ["7b54db65-c237-46e0-9812-87b37dc6bfa8", "e7dfc7af-5a42-4b39-b54c-8a0370e39ab1", "5c9c07e8-dd35-4f66-af42-fca1e791030f", "ff9c2fa0-8fbb-4a96-a3da-dd92dded1103", "8cc38866-a386-4f5c-8adf-0de3ac9be560", "b1943b98-0bb0-4114-aa96-fbe54df31b7e"], "metadata": {"filename": "building-better-tools-for-llm-agents-f8c5a6714f11.md", "extension": ".md", "title": "Building Better Tools for LLM Agents", "date": "Jul 17, 2023", "url": "https://www.llamaindex.ai/blog/building-better-tools-for-llm-agents-f8c5a6714f11"}}, "bf175024-92ea-4126-a6fb-9f1bb834913c": {"node_ids": ["23c95b9b-6e4e-4077-8c06-05543351b4d8", "524770c4-6f60-48d2-ae21-e92a87622539", "da9ce0f0-722d-4454-a6ab-c47cbce1b94d", "557d5334-b2dd-4b26-b03d-8e221515ddb9", "abb120b3-9231-42a5-b3a8-cfdb0b14584a", "c62be553-1e07-49b3-9578-92cf8a9b2648", "0a4f716e-2a19-4aeb-b981-92c4a0e8c156", "55d15888-53e7-4b17-b896-10889f03d2d8", "3021bfef-2e95-4411-8eaf-049192a50eab"], "metadata": {"filename": "data-agents-eed797d7972f.md", "extension": ".md", "title": "Data Agents", "date": "Jul 12, 2023", "url": "https://www.llamaindex.ai/blog/data-agents-eed797d7972f"}}, "8c33628f-7781-4a91-912c-dc524ca918db": {"node_ids": ["93236a20-3f7f-4d21-ab91-4214fd5cf2bb", "e6c3eeb1-ffb5-4869-b225-1aa0c0ac70af", "ac2036b1-502c-4d11-821c-b194483aaffa", "c4debc68-bd48-4223-ba34-e592fdbe37d0", "f66691de-b78e-47a0-b07f-105f7c5821c5"], "metadata": {"filename": "llamaindex-update-07-10-2023-4ceebdab96cb.md", "extension": ".md", "title": "LlamaIndex Update \u2014 07/11/2023", "date": "Jul 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-07-10-2023-4ceebdab96cb"}}, "dd317f66-60d5-495b-9596-574f69d4beba": {"node_ids": ["4ec93946-a525-494a-80e9-75a247fb06ef", "a74c9239-ce7f-4a43-a912-0988773b58f1", "8ec8044c-1ffe-429e-a8f4-8c04d465d017", "c16d9f7e-93c8-41a5-9579-d21f19e20176", "33fededd-4739-4e68-b7fb-f8032c872cae"], "metadata": {"filename": "llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.md", "extension": ".md", "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "date": "Jul 4, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"}}, "22603646-039a-4d97-bb34-9b49c6a6ac9a": {"node_ids": ["811b5dee-0ded-41a2-a1f5-1d2dc7166ee2", "8af87fb7-5ed6-4dbc-9f95-5df3882c75a8", "c56c18bd-c834-450a-be22-b9b21880a430", "2b816843-31fc-4f97-8972-663c90ffac2f", "0703fb6b-99bc-4bda-ac83-fb286ce88a12", "3653c951-0fa2-4772-8383-5770180b0049", "7c887066-60a0-4e68-b36c-b9938cdbe8d6"], "metadata": {"filename": "special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.md", "extension": ".md", "title": "Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0"}}, "e31106da-d110-425b-8f6e-2b8cf866f208": {"node_ids": ["c0958479-b601-438a-8156-1cb8f8ffde6c", "b51f70fc-9e97-4c52-bfab-9c9243f19b3e", "c8dc349f-9c21-47a1-8dc4-fc92e6923995", "bcda5d5b-bbb7-46ef-ae41-e924864e9856", "274164e0-5a9c-4f3c-9d5b-844dfc3612bf", "3ba408ab-1c68-49f0-a9d5-5dda3acea107", "6c3d3104-b82d-4c3b-b12b-dd1e73aed651", "26a5ed24-7606-4cb3-bb79-5a33c3ce85ca"], "metadata": {"filename": "enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.md", "extension": ".md", "title": "Enriching LlamaIndex Models with GraphQL and Graph Databases", "date": "Jun 30, 2023", "url": "https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7"}}, "68669e3a-bedb-4584-a56b-d99aecc96570": {"node_ids": ["47ae2bf1-83d6-483a-a1ba-f7f8e7b5989d", "1a3d307c-eb1d-4dd3-a45b-d84f57c81a32", "c92f247c-40d0-4d97-87f1-c49959c72819", "b391dba6-8c48-4f5f-94f3-74b4bd694f3c", "fbdaeaa5-5218-43ea-a3c6-c1b7f2b5d00e", "f8339e2c-af41-4400-8bf4-c550b5917a4e", "c26dc2fd-5628-4e57-b782-4cd3c2a4f9c1", "4ef2f201-8c12-4519-ae96-4eabb8f4f3a6"], "metadata": {"filename": "build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.md", "extension": ".md", "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "date": "Jun 27, 2023", "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"}}, "29b9e75b-2597-42da-b8a7-a40eba97e76a": {"node_ids": ["eba5dc32-49af-4d25-b19b-d2fbbff1abe1", "6abb63ad-84a2-444e-a36f-b35e2c452e3a", "e28414a9-144b-421a-be57-4a41ba3ecb0a", "2f6b7060-177f-45d4-85ca-6cf6b95f9cef", "473b6047-54ee-4838-98e3-335210a020e8", "a6145080-3f9e-4d56-8aad-088f97b2b2d7", "f940bd71-f40e-45bf-a65c-92a855fdbb14"], "metadata": {"filename": "llamaindex-update-6-26-2023-ed30a9d45f84.md", "extension": ".md", "title": "LlamaIndex Update \u2014 06/26/2023", "date": "Jun 26, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-update-6-26-2023-ed30a9d45f84"}}, "65437f97-2e48-4ffb-a733-840a4165e5da": {"node_ids": ["81774e96-39f8-4062-8529-771906d1b28d", "91f8a5a2-095f-4b83-a757-a744f8b6f815", "1ddad1e0-88a5-44f5-b252-49f2ab85e91e", "1a3b996c-2ed9-4789-b7ce-4610d4ebc18a"], "metadata": {"filename": "build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.md", "extension": ".md", "title": "Build and Evaluate LLM Apps with LlamaIndex and TruLens", "date": "Jun 23, 2023", "url": "https://www.llamaindex.ai/blog/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c"}}, "da12eaf4-bd48-4e41-b6e6-a80d2aa6b208": {"node_ids": ["2fa8afd8-68bf-4c74-8234-fdd3f1417a88", "cbd261f1-2c7f-4ed1-9615-a7258db8cc8d", "b4df2592-d90f-49a3-8e57-191bea270f84", "424a04cb-7c54-4c64-b68b-6aecc03dcc94"], "metadata": {"filename": "llama-index-prem-ai-join-forces-51702fecedec.md", "extension": ".md", "title": "Llama Index & Prem AI Join Forces", "date": "Jun 23, 2023", "url": "https://www.llamaindex.ai/blog/llama-index-prem-ai-join-forces-51702fecedec"}}, "04fe36ff-9050-4f95-a893-a90249ba18bc": {"node_ids": ["3421d7be-aea9-4025-900f-b5cd128185b2", "9ee2b3ff-33e3-4b9c-a1ee-4431dcdffe8e", "c777f0fd-53d4-44b7-aceb-c450f6b069ff", "5ff15592-3757-4ae9-9456-58fef7c63622"], "metadata": {"filename": "llamaindex-and-weaviate-ba3ff1cbf5f4.md", "extension": ".md", "title": "LlamaIndex and Weaviate", "date": "Jun 22, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-and-weaviate-ba3ff1cbf5f4"}}, "d38e934e-c3a1-4a48-b1f7-8b9f89ed3c1a": {"node_ids": ["95a7620c-16e4-47a6-a867-60604e082249", "26f125b2-5132-465b-acd1-6186c3b41edb", "57667410-a6b4-41f5-9aae-a5899ef66b80"], "metadata": {"filename": "llamaindex-and-transformers-agents-67042ee1d8d6.md", "extension": ".md", "title": "LlamaIndex and Transformers Agents", "date": "Jun 8, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-and-transformers-agents-67042ee1d8d6"}}, "dc4358bd-2df7-4d22-af6c-f4cc87129ae4": {"node_ids": ["55c4c151-a5db-4089-a361-73f4f245a637", "819391e7-dbb6-4b77-9117-87a647854800", "c1cc555f-fe43-431b-9fe9-aacdb09b70ee", "f0d5e85a-4568-4043-9499-c0a2c05fc739", "4b3ef8a6-eddb-4b14-b743-6e19c3a9022e"], "metadata": {"filename": "building-the-data-framework-for-llms-bca068e89e0e.md", "extension": ".md", "title": "Building the data framework for LLMs", "date": "Jun 6, 2023", "url": "https://www.llamaindex.ai/blog/building-the-data-framework-for-llms-bca068e89e0e"}}, "8a1f04fc-2aef-4384-8c26-74fc2938e91a": {"node_ids": ["6013a694-9489-4651-85cb-be8c1c90d8e6", "56bbeb99-5b37-4c3d-81d4-0fabf8003056", "bac6a762-03a6-4c4c-94b8-ea4a3f078c9d", "b334187e-9b11-4d9b-a0c9-19177d3d55bd"], "metadata": {"filename": "vellum-llamaindex-integration-58b476a1e33f.md", "extension": ".md", "title": "Vellum <> LlamaIndex Integration", "date": "Jun 5, 2023", "url": "https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f"}}, "1c699067-f5df-4a3c-a513-45453dc811f2": {"node_ids": ["eae9f1f7-146a-437b-8c69-b37af3720d5f", "38392555-0e09-49d2-9a09-194d4ac176fb", "6c7d9410-b6c0-4655-9966-05e18e50ebe3", "f76a50e8-370b-45ef-b12c-28acf5385ccd", "e370bda8-5ed0-4ca3-81fc-3a80411452df", "f6db23bf-bd37-4899-8071-6e5facc4e15d", "39af304a-cf03-4a1f-a482-d3a93f5e89e1"], "metadata": {"filename": "combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.md", "extension": ".md", "title": "Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation", "date": "May 28, 2023", "url": "https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b"}}, "50ab3a10-e38e-4b1c-b2dc-eb1cd03df774": {"node_ids": ["6f03331a-a943-4a29-aa3e-943022f4a6ad", "66a3d680-a36e-4eba-83fa-4c0571f01cbc", "36a99eb7-2155-4c9c-a153-6277c93f7ef5", "cba26f53-2cee-4563-bc1d-d4242dcc2437", "e3955352-61f1-4c45-92f2-b2c5f2638bf7", "249765ad-5eaa-4f4e-ace6-f144ca1a24f9", "914af9df-8885-4724-b233-f1e8982aed9f", "af188508-8db4-452a-ba79-32646a92725a", "332b15be-0f94-4319-8f80-303365cafe05", "8388ee03-c887-4edf-aaaa-6ae89ec17414"], "metadata": {"filename": "dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.md", "extension": ".md", "title": "Dumber LLM Agents Need More Constraints and Better Tools", "date": "May 23, 2023", "url": "https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"}}, "455c6fb6-4eb7-4b07-9b11-9699928b66da": {"node_ids": ["000671aa-f9cd-40c7-b6a1-66cf6c364ab6", "649498a6-2d08-49e9-a466-b4c5ff75a7ad", "649c1c44-1348-47c7-9afa-23146c4cb920", "dc6f5378-6152-4b83-922c-e8e504e0360d", "fae7e716-817d-4257-9c2d-52554397f174"], "metadata": {"filename": "build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.md", "extension": ".md", "title": "Build a ChatGPT with your Private Data using LlamaIndex and MongoDB", "date": "May 18, 2023", "url": "https://www.llamaindex.ai/blog/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c"}}, "11b375e8-e8d6-4d21-a47e-502ae9531bbd": {"node_ids": ["c947eab8-e9ec-44ff-a0aa-e1c58bcfe9d2", "3769b6d9-3967-4afb-b744-22ff538b13e3", "16501f8a-21f5-483b-8887-b09838a60245", "1c99a89c-5f4f-4127-b451-be5243c479eb", "c009d4ea-ab0b-4343-8438-2a4fc7611a6b", "9206c8eb-fdaf-4d75-9c5d-c6a5db0ffed9"], "metadata": {"filename": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6.md", "extension": ".md", "title": "Using LLM\u2019s for Retrieval and Reranking", "date": "May 17, 2023", "url": "https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"}}, "f9f418a7-e124-424c-b5b7-b35cc94d96f4": {"node_ids": ["7645b065-77c6-427e-af0e-453e135b751d", "5d332cce-a3fc-4ece-bde6-11a55fdd5a67", "434bed5b-9c63-482f-a9ab-5e56aa10cb2a", "f04b2ee7-ef13-4efb-a7dc-749f7d707a7c", "3c08ed13-db8f-48b0-b841-4ab23972f146", "5f23a11d-bd1f-49e0-b995-d33458feba0e", "26d1be8a-0c5f-4f07-b243-d4a5c5e73577", "bc7669e1-ff69-4731-9506-1f6d4c07a3c1", "5a127ad4-5783-40f6-9725-5ad5f9b20ba0", "85e7a5e9-8530-4cb3-9cd5-28445dc13d6e"], "metadata": {"filename": "testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.md", "extension": ".md", "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings", "date": "May 12, 2023", "url": "https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"}}, "cc03d67e-9f21-4521-b472-551874bf4c37": {"node_ids": ["fc98d873-7413-48ac-8653-f6ffb1ccc589", "43543bd7-41a5-47f8-8147-f214729708dd", "d5c37210-9e3b-421a-b6e7-0e1309f8d3a4", "cf124a05-b249-4c64-b661-24da58101554", "82d45681-a1cc-4d2b-a19f-c5264aa75363"], "metadata": {"filename": "llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.md", "extension": ".md", "title": "LlamaIndex on TWIML AI: A Distilled Summary (using LlamaIndex)", "date": "May 10, 2023", "url": "https://www.llamaindex.ai/blog/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595"}}, "912df07d-6a63-4391-95b4-01eec2fb54f7": {"node_ids": ["349fbca7-4d4b-460c-9132-02ac5ea6fa88", "b12870dc-ad6a-4439-b844-def01c4f956d", "3b97ba75-cde5-4162-8086-40ba2978feca", "807e1318-818c-4b17-a1ae-45be84889df2"], "metadata": {"filename": "a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.md", "extension": ".md", "title": "A New Document Summary Index for LLM-powered QA Systems", "date": "May 8, 2023", "url": "https://www.llamaindex.ai/blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec"}}, "86c77f5d-1c44-44fe-a276-ab681d141894": {"node_ids": ["d38d5355-2cdc-42b5-be74-15c561dddfbd", "cfd90d07-f32d-4963-97d8-768828ad927d", "f885f21a-8aa1-4987-a3da-f0480e2d32d5", "70bc463c-ccbf-414a-a1d3-eeeccd876b8a"], "metadata": {"filename": "building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.md", "extension": ".md", "title": "Building and Evaluating a QA System with LlamaIndex", "date": "May 7, 2023", "url": "https://www.llamaindex.ai/blog/building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1"}}}}