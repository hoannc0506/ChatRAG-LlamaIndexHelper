{"docstore/metadata": {"07a29a81-1d9f-445e-b1a3-cec315ffcd79": {"doc_hash": "e622a0a8d1532d5ca3d35838e53ef4c193b6d4170eba7de5c72e033171779acc"}, "55bbca88-eaf3-4d38-b759-898354717207": {"doc_hash": "10894293b5a982499949a7a0e6d8050e1fa0075765f31179217da413a4ba6825"}, "0b5088c8-9467-4361-a295-76d163336b51": {"doc_hash": "8fb9802b11fdf1a7cc9b88f317602b2d90a2f243e9ddf1f50ef979e235feede9"}, "37bc1481-63f1-4203-b395-0fb229570adf": {"doc_hash": "6117b1853de7eeb213f072f64310f9bba29c1a0aae4b95f4c59ccd1e327cbd0d"}, "9a5b129a-cf71-41a1-8f62-d32fbb2f5bc2": {"doc_hash": "aca8508a39c87881acdb169298f52331e0b3eedfd67890790ebe9784f483caa4"}, "41d5b086-ffb5-4c55-b101-b6207854a6d1": {"doc_hash": "7d0c9d2a06901f729d08fa37ab254fd825b78ffd58342553c1b89ffa19bb6d68", "ref_doc_id": "07a29a81-1d9f-445e-b1a3-cec315ffcd79"}, "5bd609fa-f1e8-4c52-a6ea-79fdb47a51f3": {"doc_hash": "ab59fa92a29ca81d8915022af0811bb3598dceba3b347367cc78b448c5ad7b82", "ref_doc_id": "55bbca88-eaf3-4d38-b759-898354717207"}, "2e44e215-8834-4311-bf81-88c02de64919": {"doc_hash": "24ec9d686be0fdd436d2e125de268d36e345e213b30dfa090d56b30f05522f4e", "ref_doc_id": "0b5088c8-9467-4361-a295-76d163336b51"}, "8b5dfa16-fc80-4788-b048-8a9102dabe98": {"doc_hash": "e012c083c67486b3a75ed759518acb446dac1be08cc0290706535f6045c52baf", "ref_doc_id": "37bc1481-63f1-4203-b395-0fb229570adf"}, "bd717d20-9a29-4785-b703-84ecbe3ec43b": {"doc_hash": "28f45a9d51fb1d08693b1cf5e1487bdcdab8a36fec872fb1347b1049d4001f97", "ref_doc_id": "9a5b129a-cf71-41a1-8f62-d32fbb2f5bc2"}, "2f0e0e9d-3ff2-4a97-9db7-788051eec855": {"doc_hash": "1dd1becebcff20f3254a81e5a894519cabbcf7101221eb16e6a00e1b80467f81", "ref_doc_id": "07a29a81-1d9f-445e-b1a3-cec315ffcd79"}, "ff5998ad-43af-4a67-89fa-8668f4d1950a": {"doc_hash": "8ff9817485a9705b7259ec3da050101151978211565da0a6c19ece988c0fd3ce", "ref_doc_id": "55bbca88-eaf3-4d38-b759-898354717207"}, "ad7676fd-722c-4833-b908-1ee56127f081": {"doc_hash": "e453fa712ae300d635f1746717ae9a955acd36642c2189426ed8f7f2c142e40f", "ref_doc_id": "0b5088c8-9467-4361-a295-76d163336b51"}, "d7959866-c8b5-4c9b-a977-efce26fc05e9": {"doc_hash": "830fc4677d3daa7400e6bc70493c31e7146163bbbcf6f48cd7a760c478bec1c3", "ref_doc_id": "37bc1481-63f1-4203-b395-0fb229570adf"}, "94c96755-d3a8-433e-9f79-57552112af3b": {"doc_hash": "e1fc602b184c13b6876832699208f66d9382c5512b26878049f6655717021091", "ref_doc_id": "9a5b129a-cf71-41a1-8f62-d32fbb2f5bc2"}}, "docstore/data": {"41d5b086-ffb5-4c55-b101-b6207854a6d1": {"__data__": {"id_": "41d5b086-ffb5-4c55-b101-b6207854a6d1", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.html", "file_name": "a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.html", "file_type": "text/html", "file_size": 24708, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "07a29a81-1d9f-445e-b1a3-cec315ffcd79", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.html", "file_name": "a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.html", "file_type": "text/html", "file_size": 24708, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "e622a0a8d1532d5ca3d35838e53ef4c193b6d4170eba7de5c72e033171779acc", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  It\u2019s the start of a new year and perhaps you\u2019re looking to break into the RAG scene by building your very first RAG system. Or, maybe you\u2019ve built Basic RAG systems and are now looking to enhance them to something more advanced in order to better handle your user\u2019s queries and data structures.\n </p>\n <p>\n  In either case, knowing where or how to begin may be a challenge in and of itself! If that\u2019s true, then hopefully this blog post points you in the right direction for your next steps, and moreover, provides for you a mental model for you to anchor your decisions when building advanced RAG systems.\n </p>\n <p>\n  The RAG cheat sheet shared above was greatly inspired by a recent RAG survey paper (\n  <a href=\"https://arxiv.org/pdf/2312.10997.pdf\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   \u201cRetrieval-Augmented Generation for Large Language Models: A Survey\u201d Gao, Yunfan, et al. 2023\n  </a>\n  ).\n </p>\n <h1>\n  Basic RAG\n </h1>\n <p>\n  Mainstream RAG as defined today involves retrieving documents from an external knowledge database and passing these along with the user\u2019s query to an LLM for response generation. In other words, RAG involves a Retrieval component, an External Knowledge database and a Generation component.\n </p>\n <p>\n  <strong>\n   LlamaIndex Basic RAG Recipe:\n  </strong>\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"503a\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> SimpleDirectoryReader, VectorStoreIndex\n\n<span class=\"hljs-comment\"># load data</span>\ndocuments = SimpleDirectoryReader(input_dir=<span class=\"hljs-string\">\"...\"</span>).load_data()\n\n<span class=\"hljs-comment\"># build VectorStoreIndex that takes care of chunking documents</span>\n<span class=\"hljs-comment\"># and encoding chunks to embeddings for future retrieval</span>\nindex = VectorStoreIndex.from_documents(documents=documents)\n\n<span class=\"hljs-comment\"># The QueryEngine class is equipped with the generator</span>\n<span class=\"hljs-comment\"># and facilitates the retrieval and generation steps</span>\nquery_engine = index.as_query_engine()\n\n<span class=\"hljs-comment\"># Use your Default RAG</span>\nresponse = query_engine.query(<span class=\"hljs-string\">\"A user's query\"</span>)</span></pre>\n <h1>\n  Success Requirements for RAG\n </h1>\n <p>\n  In order for a RAG system to be deemed as a success (in the sense of providing useful and relevant answers to user questions), there are really only two high level requirements:\n </p>\n <ol>\n  <li>\n   Retrieval must be able to find the most relevant documents to a user query.\n  </li>\n  <li>\n   Generation must be able to make good use of the retrieved documents to sufficiently answer the user query.\n  </li>\n </ol>\n <h1>\n  Advanced RAG\n </h1>\n <p>\n  With the success requirements defined, we can then say that building advanced RAG is really about the application of more sophisticated techniques and strategies (to the Retrieval or Generation components) to ensure that they are ultimately met. Furthermore, we can categorize a sophisticated technique as either one that addresses one of the two high-level success requirements independent (more or less) of the other, or one that addresses both of these requirements simultaneously.\n </p>\n <h1>\n  Advanced techniques for Retrieval must be able to find the most relevant documents to a user query\n </h1>\n <p>\n  Below we briefly describe a couple of the more sophisticated techniques to help achieve the first success requirement.\n </p>\n <ol>\n  <li>\n   <strong>\n    Chunk-Size Optimization:\n   </strong>\n   Since LLMs are restricted by context length, it is necessary to chunk documents when building the External Knowledge database. Chunks that are too big or too small can pose problems for the Generation component leading to in accurate responses.\n  </li>\n </ol>\n <p>\n  <strong>\n   LlamaIndex Chunk Size Optimization Recipe\n  </strong>\n  (\n  <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/param_optimizer/param_optimizer.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   notebook guide\n  </a>\n  )\n  <strong>\n   :\n  </strong>\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"0631\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> ServiceContext\n<span class=\"hljs-keyword\">from</span> llama_index.param_tuner.base <span class=\"hljs-keyword\">import</span> ParamTuner, RunResult\n<span class=\"hljs-keyword\">from</span> llama_index.evaluation <span class=\"hljs-keyword\">import</span> SemanticSimilarityEvaluator, BatchEvalRunner\n\n<span class=\"hljs-comment\">### Recipe</span>\n<span class=\"hljs-comment\">### Perform hyperparameter tuning as in traditional ML via grid-search</span>\n<span class=\"hljs-comment\">### 1. Define an objective function that ranks different parameter combos</span>\n<span class=\"hljs-comment\">### 2. Build ParamTuner object</span>\n<span class=\"hljs-comment\">### 3. Execute hyperparameter tuning with ParamTuner.tune()</span>\n\n<span class=\"hljs-comment\"># 1. Define objective function</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">objective_function</span>(<span class=\"hljs-params\">params_dict</span>):\n    chunk_size = params_dict[<span class=\"hljs-string\">\"chunk_size\"</span>]\n    docs = params_dict[<span class=\"hljs-string\">\"docs\"</span>]\n    top_k = params_dict[<span class=\"hljs-string\">\"top_k\"</span>]\n    eval_qs = params_dict[<span class=\"hljs-string\">\"eval_qs\"</span>]\n    ref_response_strs = params_dict[<span class=\"hljs-string\">\"ref_response_strs\"</span>]\n\n    <span class=\"hljs-comment\"># build RAG pipeline</span>\n    index = _build_index(chunk_size, docs)  <span class=\"hljs-comment\"># helper function not shown here</span>\n    query_engine = index.as_query_engine(similarity_top_k=top_k)\n  \n    <span class=\"hljs-comment\"># perform inference with RAG pipeline on a provided questions `eval_qs`</span>\n    pred_response_objs = get_responses(\n        eval_qs, query_engine, show_progress=<span class=\"hljs-literal\">True</span>\n    )\n\n    <span class=\"hljs-comment\"># perform evaluations of predictions by comparing them to reference</span>\n    <span class=\"hljs-comment\"># responses `ref_response_strs`</span>\n    evaluator = SemanticSimilarityEvaluator(...)\n    eval_batch_runner = BatchEvalRunner(\n        {<span class=\"hljs-string\">\"semantic_similarity\"</span>: evaluator}, workers=<span class=\"hljs-number\">2</span>, show_progress=<span class=\"hljs-literal\">True</span>\n    )\n    eval_results = eval_batch_runner.evaluate_responses(\n        eval_qs, responses=pred_response_objs, reference=ref_response_strs\n    )\n\n    <span class=\"hljs-comment\"># get semantic similarity metric</span>\n    mean_score = np.array(\n        [r.score <span class=\"hljs-keyword\">for</span> r <span class=\"hljs-keyword\">in</span> eval_results[<span class=\"hljs-string\">\"semantic_similarity\"</span>]]\n    ).mean()\n\n    <span class=\"hljs-keyword\">return</span> RunResult(score=mean_score, params=params_dict)\n\n<span class=\"hljs-comment\"># 2. Build ParamTuner object</span>\nparam_dict = {<span class=\"hljs-string\">\"chunk_size\"</span>: [<span class=\"hljs-number\">256</span>, <span class=\"hljs-number\">512</span>, <span class=\"hljs-number\">1024</span>]} <span class=\"hljs-comment\"># params/values to search over</span>\nfixed_param_dict = { <span class=\"hljs-comment\"># fixed hyperparams</span>\n  <span class=\"hljs-string\">\"top_k\"</span>: <span class=\"hljs-number\">2</span>,\n    <span class=\"hljs-string\">\"docs\"</span>: docs,\n    <span class=\"hljs-string\">\"eval_qs\"</span>: eval_qs[:<span class=\"hljs-number\">10</span>],\n    <span class=\"hljs-string\">\"ref_response_strs\"</span>: ref_response_strs[:<span class=\"hljs-number\">10</span>],\n}\nparam_tuner = ParamTuner(\n    param_fn=objective_function,\n    param_dict=param_dict,\n    fixed_param_dict=fixed_param_dict,\n    show_progress=<span class=\"hljs-literal\">True</span>,\n)\n\n<span class=\"hljs-comment\"># 3. Execute hyperparameter search</span>\nresults = param_tuner.tune()\nbest_result = results.best_run_result\nbest_chunk_size = results.best_run_result.params[<span class=\"hljs-string\">\"chunk_size\"</span>]</span></pre>\n <p>\n  <strong>\n   2. Structured External Knowledge:\n  </strong>\n  In complex scenarios, it may be necessary to build your external knowledge with a bit more structure than the basic vector index so as to permit recursive retrievals or routed retrieval when dealing with sensibly separated external knowledge sources.\n </p>\n <p>\n  <strong>\n   LlamaIndex Recursive Retrieval Recipe\n  </strong>\n  (\n  <a href=\"https://docs.llamaindex.ai/en/stable/examples/retrievers/recursive_retriever_nodes.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   notebook guide\n  </a>\n  )\n  <strong>\n   :\n  </strong>\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"4a9d\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> SimpleDirectoryReader, VectorStoreIndex\n<span class=\"hljs-keyword\">from</span> llama_index.node_parser <span class=\"hljs-keyword\">import</span> SentenceSplitter\n<span class=\"hljs-keyword\">from</span> llama_index.schema <span class=\"hljs-keyword\">import</span> IndexNode\n\n<span class=\"hljs-comment\">### Recipe</span>\n<span class=\"hljs-comment\">### Build a recursive retriever that retrieves using small chunks</span>\n<span class=\"hljs-comment\">### but passes associated larger chunks to the generation stage</span>\n\n<span class=\"hljs-comment\"># load data</span>\ndocuments = SimpleDirectoryReader(\n  input_file=<span class=\"hljs-string\">\"some_data_path/llama2.pdf\"</span>\n).load_data()\n\n<span class=\"hljs-comment\"># build parent chunks via NodeParser</span>\nnode_parser = SentenceSplitter(chunk_size=<span class=\"hljs-number\">1024</span>)\nbase_nodes = node_parser.get_nodes_from_documents(documents)\n\n<span class=\"hljs-comment\"># define smaller child chunks</span>\nsub_chunk_sizes = [<span class=\"hljs-number\">256</span>, <span class=\"hljs-number\">512</span>]\nsub_node_parsers = [\n    SentenceSplitter(chunk_size=c, chunk_overlap=<span class=\"hljs-number\">20</span>) <span class=\"hljs-keyword\">for</span> c <span class=\"hljs-keyword\">in</span> sub_chunk_sizes\n]\nall_nodes = []\n<span class=\"hljs-keyword\">for</span> base_node <span class=\"hljs-keyword\">in</span> base_nodes:\n    <span class=\"hljs-keyword\">for</span> n <span class=\"hljs-keyword\">in</span> sub_node_parsers:\n        sub_nodes = n.get_nodes_from_documents([base_node])\n        sub_inodes = [\n            IndexNode.from_text_node(sn, base_node.node_id) <span class=\"hljs-keyword\">for</span> sn <span class=\"hljs-keyword\">in</span> sub_nodes\n        ]\n        all_nodes.extend(sub_inodes)\n    <span class=\"hljs-comment\"># also add original node to node</span>\n    original_node = IndexNode.from_text_node(base_node, base_node.node_id)\n    all_nodes.append(original_node)\n\n<span class=\"hljs-comment\"># define a VectorStoreIndex with all of the nodes</span>\nvector_index_chunk = VectorStoreIndex(\n    all_nodes, service_context=service_context\n)\nvector_retriever_chunk = vector_index_chunk.as_retriever(similarity_top_k=<span class=\"hljs-number\">2</span>)\n\n<span class=\"hljs-comment\"># build RecursiveRetriever</span>\nall_nodes_dict = {n.node_id: n <span class=\"hljs-keyword\">for</span> n <span class=\"hljs-keyword\">in</span> all_nodes}\nretriever_chunk = RecursiveRetriever(\n    <span class=\"hljs-string\">\"vector\"</span>,\n    retriever_dict={<span class=\"hljs-string\">\"vector\"</span>: vector_retriever_chunk},\n    node_dict=all_nodes_dict,\n    verbose=<span class=\"hljs-literal\">True</span>,\n)\n\n<span class=\"hljs-comment\"># build RetrieverQueryEngine using recursive_retriever</span>\nquery_engine_chunk = RetrieverQueryEngine.from_args(\n    retriever_chunk, service_context=service_context\n)\n\n<span class=\"hljs-comment\"># perform inference with advanced RAG (i.e. query engine)</span>\nresponse = query_engine_chunk.query(\n    <span class=\"hljs-string\">\"Can you tell me about the key concepts for safety finetuning\"</span>\n)</span></pre>\n <p>\n  <strong>\n   Other useful links\n  </strong>\n </p>\n <p>\n  We have several of guides demonstrating the application of other advanced techniques to help ensure accurate retrieval in complex cases. Here are links to a select few of them:\n </p>\n <ol>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/query_engine/knowledge_graph_rag_query_engine.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Building External Knowledge using Knowledge Graphs\n   </a>\n  </li>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/vector_stores/elasticsearch_auto_retriever.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Performing Mixed Retrieval with Auto Retrievers\n   </a>\n  </li>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/retrievers/simple_fusion.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Building Fusion Retrievers\n   </a>\n  </li>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/finetuning/embeddings/finetune_embedding.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Fine-tuning Embedding Models used in Retrieval\n   </a>\n  </li>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/query_transformations/HyDEQueryTransformDemo.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Transforming Query Embeddings (HyDE)\n   </a>\n  </li>\n </ol>\n <h1>\n  Advanced techniques for Generation must be able to make good use of the retrieved documents\n </h1>\n <p>\n  Similar to previous section, we provide a couple of examples of the sophisticated techniques under this category, which can be described as ensuring that the retrieved documents are well aligned to the LLM of the Generator.\n </p>\n <ol>\n  <li>\n   <strong>\n    Information Compression:\n   </strong>\n   Not only are LLMs are restricted by context length, but there can be response degradation if the retrieved documents carry too much noise (i.e. irrelevant information).\n  </li>\n </ol>\n <p>\n  <strong>\n   LlamaIndex Information Compression Recipe\n  </strong>\n  (\n  <a href=\"https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/LongLLMLingua.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   notebook guide\n  </a>\n  )\n  <strong>\n   :\n  </strong>\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"f078\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> SimpleDirectoryReader, VectorStoreIndex\n<span class=\"hljs-keyword\">from</span> llama_index.query_engine <span class=\"hljs-keyword\">import</span> RetrieverQueryEngine\n<span class=\"hljs-keyword\">from</span> llama_index.postprocessor <span class=\"hljs-keyword\">import</span> LongLLMLinguaPostprocessor\n\n<span class=\"hljs-comment\">### Recipe</span>\n<span class=\"hljs-comment\">### Define a Postprocessor object, here LongLLMLinguaPostprocessor</span>\n<span class=\"hljs-comment\">### Build QueryEngine that uses this Postprocessor on retrieved docs</span>\n\n<span class=\"hljs-comment\"># Define Postprocessor</span>\nnode_postprocessor = LongLLMLinguaPostprocessor(\n    instruction_str=<span class=\"hljs-string\">\"Given the context, please answer the final question\"</span>,\n    target_token=<span class=\"hljs-number\">300</span>,\n    rank_method=<span class=\"hljs-string\">\"longllmlingua\"</span>,\n    additional_compress_kwargs={\n        <span class=\"hljs-string\">\"condition_compare\"</span>: <span class=\"hljs-literal\">True</span>,\n        <span class=\"hljs-string\">\"condition_in_question\"</span>: <span class=\"hljs-string\">\"after\"</span>,\n        <span class=\"hljs-string\">\"context_budget\"</span>: <span class=\"hljs-string\">\"+100\"</span>,\n        <span class=\"hljs-string\">\"reorder_context\"</span>: <span class=\"hljs-string\">\"sort\"</span>,  <span class=\"hljs-comment\"># enable document reorder</span>\n    },\n)\n\n<span class=\"hljs-comment\"># Define VectorStoreIndex</span>\ndocuments = SimpleDirectoryReader(input_dir=<span class=\"hljs-string\">\"...\"</span>).load_data()\nindex = VectorStoreIndex.from_documents(documents)\n\n<span class=\"hljs-comment\"># Define QueryEngine</span>\nretriever = index.as_retriever(similarity_top_k=<span class=\"hljs-number\">2</span>)\nretriever_query_engine = RetrieverQueryEngine.from_args(\n    retriever, node_postprocessors=[node_postprocessor]\n)\n\n<span class=\"hljs-comment\"># Used your advanced RAG</span>\nresponse = retriever_query_engine.query(<span class=\"hljs-string\">\"A user query\"</span>)</span></pre>\n <p>\n  <strong>\n   2. Result Re-Rank:\n  </strong>\n  LLMs suffer from the so-called \u201cLost in the Middle\u201d phenomena which stipulates that LLMs focus on the extreme ends of the prompts. In light of this, it is beneficial to re-rank retrieved documents before passing them off to the Generation component.\n </p>\n <p>\n  <strong>\n   LlamaIndex Re-Ranking For Better Generation Recipe\n  </strong>\n  (\n  <a href=\"https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/CohereRerank.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   notebook guide\n  </a>\n  )\n  <strong>\n   :\n  </strong>\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"28c3\"><span class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> SimpleDirectoryReader, VectorStoreIndex\n<span class=\"hljs-keyword\">from</span> llama_index.postprocessor.cohere_rerank <span class=\"hljs-keyword\">import</span> CohereRerank\n<span class=\"hljs-keyword\">from</span> llama_index.postprocessor <span class=\"hljs-keyword\">import</span> LongLLMLinguaPostprocessor\n\n<span class=\"hljs-comment\">### Recipe</span>\n<span class=\"hljs-comment\">### Define a Postprocessor object, here CohereRerank</span>\n<span class=\"hljs-comment\">### Build QueryEngine that uses this Postprocessor on retrieved docs</span>\n\n<span class=\"hljs-comment\"># Build CohereRerank post retrieval processor</span>\napi_key = os.environ[<span class=\"hljs-string\">\"COHERE_API_KEY\"</span>]\ncohere_rerank = CohereRerank(api_key=api_key, top_n=<span class=\"hljs-number\">2</span>)\n\n<span class=\"hljs-comment\"># Build QueryEngine (RAG) using the post processor</span>\ndocuments = SimpleDirectoryReader(<span class=\"hljs-string\">\"./data/paul_graham/\"</span>).load_data()\nindex = VectorStoreIndex.from_documents(documents=documents)\nquery_engine = index.as_query_engine(\n    similarity_top_k=<span class=\"hljs-number\">10</span>,\n    node_postprocessors=[cohere_rerank],\n)\n\n<span class=\"hljs-comment\"># Use your advanced RAG</span>\nresponse = query_engine.query(\n    <span class=\"hljs-string\">\"What did Sam Altman do in this essay?\"</span>\n)</span></pre>\n <h1>\n  Advanced techniques for simultaneously addressing Retrieval and Generation success requirements\n </h1>\n <p>\n  In this sub section, we consider sophisticated methods that use the synergy of retrieval and generation in order to achieve both better retrieval as well as more accurate generated responses to user queries).\n </p>\n <ol>\n  <li>\n   <strong>\n    Generator-Enhanced Retrieval:\n   </strong>\n   These techniques make use of the LLM\u2019s inherent reasoning abilities to refine the user query before retrieval is performed so as to better indicate what exactly it requires to provide a useful response.\n  </li>\n </ol>\n <p>\n  <strong>\n   LlamaIndex Generator-Enhanced Retrieval Recipe\n  </strong>\n  (\n  <a href=\"https://docs.llamaindex.ai/en/stable/examples/query_engine/flare_query_engine.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   notebook guide\n  </a>\n  )\n  <strong>\n   :\n  </strong>\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"ac1d\"><span class=\"hljs-keyword\">from</span> llama_index.llms <span class=\"hljs-keyword\">import</span> OpenAI\n<span class=\"hljs-keyword\">from</span> llama_index.query_engine <span class=\"hljs-keyword\">import</span> FLAREInstructQueryEngine\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> (\n    VectorStoreIndex,\n    SimpleDirectoryReader,\n    ServiceContext,\n)\n<span class=\"hljs-comment\">### Recipe</span>\n<span class=\"hljs-comment\">### Build a FLAREInstructQueryEngine which has the generator LLM play</span>\n<span class=\"hljs-comment\">### a more active role in retrieval by prompting it to elicit retrieval</span>\n<span class=\"hljs-comment\">### instructions on what it needs to answer the user query.</span>\n\n<span class=\"hljs-comment\"># Build FLAREInstructQueryEngine</span>\ndocuments = SimpleDirectoryReader(<span class=\"hljs-string\">\"./data/paul_graham\"</span>).load_data()\nindex = VectorStoreIndex.from_documents(documents)\nindex_query_engine = index.as_query_engine(similarity_top_k=<span class=\"hljs-number\">2</span>)\nservice_context = ServiceContext.from_defaults(llm=OpenAI(model=<span class=\"hljs-string\">\"gpt-4\"</span>))\nflare_query_engine = FLAREInstructQueryEngine(\n    query_engine=index_query_engine,\n    service_context=service_context,\n    max_iterations=<span class=\"hljs-number\">7</span>,\n    verbose=<span class=\"hljs-literal\">True</span>,\n)\n\n<span class=\"hljs-comment\"># Use your advanced RAG</span>\nresponse = flare_query_engine.query(\n    <span class=\"hljs-string\">\"Can you tell me about the author's trajectory in the startup world?\"</span>\n)</span></pre>\n <p>\n  <strong>\n   2. Iterative Retrieval-Generator RAG:\n  </strong>\n  For some complex cases, multi-step reasoning may be required to provide a useful and relevant answer to the user query.\n </p>\n <p>\n  <strong>\n   LlamaIndex Iterative Retrieval-Generator Recipe (\n  </strong>\n  <a href=\"https://docs.llamaindex.ai/en/stable/examples/evaluation/RetryQuery.html#retry-query-engine\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   notebook guide\n  </a>\n  <strong>\n   ):\n  </strong>\n </p>\n <pre><span class=\"qd os gt qa b bf qe qf l qg qh\" id=\"cd1c\"><span class=\"hljs-keyword\">from</span> llama_index.query_engine <span class=\"hljs-keyword\">import</span> RetryQueryEngine\n<span class=\"hljs-keyword\">from</span> llama_index.evaluation <span class=\"hljs-keyword\">import</span> RelevancyEvaluator\n\n<span class=\"hljs-comment\">### Recipe</span>\n<span class=\"hljs-comment\">### Build a RetryQueryEngine which performs retrieval-generation cycles</span>\n<span class=\"hljs-comment\">### until it either achieves a passing evaluation or a max number of</span>\n<span class=\"hljs-comment\">### cycles has been reached</span>\n\n<span class=\"hljs-comment\"># Build RetryQueryEngine</span>\ndocuments = SimpleDirectoryReader(<span class=\"hljs-string\">\"./data/paul_graham\"</span>).load_data()\nindex = VectorStoreIndex.from_documents(documents)\nbase_query_engine = index.as_query_engine()\nquery_response_evaluator = RelevancyEvaluator() <span class=\"hljs-comment\"># evaluator to critique </span>\n                                                <span class=\"hljs-comment\"># retrieval-generation cycles</span>\nretry_query_engine = RetryQueryEngine(\n    base_query_engine, query_response_evaluator\n)\n\n<span class=\"hljs-comment\"># Use your advanced rag</span>\nretry_response = retry_query_engine.query(<span class=\"hljs-string\">\"A user query\"</span>)</span></pre>\n <h1>\n  Measurement Aspects of RAG\n </h1>\n <p>\n  Evaluating RAG systems are, of course, of utmost importance. In their survey paper, Gao, Yunfan et al. indicate 7 measurement aspects as seen in the top-right portion of the attached RAG cheat sheet. The llama-index library consists of several evaluation abstractions as well as integrations to RAGAs in order to help builders gain an understanding of the level to which their RAG system achieves the success requirements through the lens of these measurement aspects. Below, we list a select few of the evaluation notebook guides.\n </p>\n <ol>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/latest/examples/evaluation/answer_and_context_relevancy.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Answer Relevancy and Context Relevancy\n   </a>\n  </li>\n  <li>\n   <a href=\"https://www.notion.so/LlamaIndex-Platform-0754edd9af1c4159bde12649c184c8ef?pvs=21\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Faithfulness\n   </a>\n  </li>\n  <li>\n   <a href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/evaluation/retrieval/retriever_eval.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Retrieval Evaluation\n   </a>\n  </li>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/stable/examples/evaluation/batch_eval.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Batch Evaluations with BatchEvalRunner\n   </a>\n  </li>\n </ol>\n <h1>\n  You\u2019re Now Equipped To Do Advanced RAG\n </h1>\n <p>\n  After reading this blog post, we hope that you feel more equipped and confident to apply some of these sophisticated techniques for building Advanced RAG systems!\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 24683, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5bd609fa-f1e8-4c52-a6ea-79fdb47a51f3": {"__data__": {"id_": "5bd609fa-f1e8-4c52-a6ea-79fdb47a51f3", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.html", "file_name": "a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.html", "file_type": "text/html", "file_size": 11715, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "55bbca88-eaf3-4d38-b759-898354717207", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.html", "file_name": "a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.html", "file_type": "text/html", "file_size": 11715, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "10894293b5a982499949a7a0e6d8050e1fa0075765f31179217da413a4ba6825", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  In this blog post, we introduce a brand new LlamaIndex data structure: a Document Summary Index. We describe how it can help offer better retrieval performance compared to traditional semantic search, and also walk through an example.\n </p>\n <h1>\n  Background\n </h1>\n <p>\n  One of the core use cases of Large Language Models (LLMs) is question-answering over your own data. To do this, we pair the LLM with a \u201cretrieval\u201d model that can perform information retrieval over a knowledge corpus, and perform response synthesis over the retrieved texts using the LLM. This overall framework is called Retrieval-Augmented Generation.\n </p>\n <p>\n  Most users building LLM-powered QA systems today tend to do some form of the following:\n </p>\n <ol>\n  <li>\n   Take source documents, split each one into text chunks\n  </li>\n  <li>\n   Store text chunks in a vector db\n  </li>\n  <li>\n   During query-time, retrieve text chunks by embedding similarity and/or keyword filters.\n  </li>\n  <li>\n   Perform response synthesis\n  </li>\n </ol>\n <p>\n  For a variety of reasons, this approach provides limited retrieval performance.\n </p>\n <h2>\n  Limitations of Existing Approaches\n </h2>\n <p>\n  There are a few limitations of embedding retrieval using text chunks.\n </p>\n <ul>\n  <li>\n   <strong>\n    Text chunks lack global context.\n   </strong>\n   Oftentimes the question requires context beyond what is indexed in a specific chunk.\n  </li>\n  <li>\n   <strong>\n    Careful tuning of top-k / similarity score thresholds.\n   </strong>\n   Make the value too small and you\u2019ll miss context. Make the value too big and cost/latency might increase with more irrelevant context.\n  </li>\n  <li>\n   <strong>\n    Embeddings don\u2019t always select the most relevant context for a question.\n   </strong>\n   Embeddings are inherently determined separately between text and the context.\n  </li>\n </ul>\n <p>\n  Adding keyword filters are one way to enhance the retrieval results. But that comes with its own set of challenges. We would need to adequately determine the proper keywords for each document, either manually or through an NLP keyword extraction/topic tagging model. Also we would need to adequately infer the proper keywords from the query.\n </p>\n <h1>\n  Document Summary Index\n </h1>\n <figure>\n  <figcaption class=\"qn fe qo pz qa qp qq be b bf z dt\">\n   A diagram for the Document Summary Index\n  </figcaption>\n </figure>\n <p>\n  We propose a new index in\n  <a href=\"https://github.com/jerryjliu/llama_index\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   LlamaIndex\n  </a>\n  that will extract/index an\n  <strong>\n   unstructured text summary for each document\n  </strong>\n  . This index can help enhance retrieval performance beyond existing retrieval approaches. It helps to index more information than a single text chunk, and carries more semantic meaning than keyword tags. It also allows for a more flexible form of retrieval: we can do both LLM retrieval and embedding-based retrieval.\n </p>\n <h2>\n  How It Works\n </h2>\n <p>\n  During build-time, we ingest each document, and use a LLM to extract a summary from each document. We also split the document up into text chunks (nodes). Both the summary and the nodes are stored within our\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/how_to/storage.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Document Store\n  </a>\n  abstraction. We maintain a mapping from the summary to the source document/nodes.\n </p>\n <p>\n  During query-time, we retrieve relevant documents to the query based on their summaries, using the following approaches:\n </p>\n <ul>\n  <li>\n   <strong>\n    LLM-based Retrieval:\n   </strong>\n   We present sets of document summaries to the LLM, and ask the LLM to determine which documents are relevant + their relevance score.\n  </li>\n  <li>\n   <strong>\n    Embedding-based Retrieval:\n   </strong>\n   We retrieve relevant documents based on summary embedding similarity (with a top-k cutoff).\n  </li>\n </ul>\n <p>\n  Note that this approach of retrieval for document summaries (even with the embedding-based approach) is different than embedding-based retrieval over text chunks. The retrieval classes for the document summary index retrieve\n  <strong>\n   all nodes\n  </strong>\n  for any selected document, instead of returning relevant chunks at the node-level.\n </p>\n <p>\n  Storing summaries for a document also enables\n  <strong>\n   LLM-based retrieval\n  </strong>\n  . Instead of feeding the entire document to the LLM in the beginning, we can first have the LLM inspect the concise document summary to see if it\u2019s relevant to the query at all. This leverages the reasoning capabilities of LLM\u2019s which are more advanced than embedding-based lookup, but avoids the cost/latency of feeding the entire document to the LLM\n </p>\n <h2>\n  Additional Insights\n </h2>\n <p>\n  Document retrieval with summaries can be thought of as a \u201cmiddle ground\u201d between semantic search and brute-force summarization across all docs. We look up documents based on summary relevance with the given query, and then return all *nodes* corresponding to the retrieved docs.\n </p>\n <p>\n  Why should we do this? This retrieval method gives user more context than top-k over a text-chunk, by retrieving context at a document-level. But, it\u2019s also a more flexible/automatic approach than topic modeling; no more worrying about whether your text has the right keyword tags!\n </p>\n <h1>\n  Example\n </h1>\n <p>\n  Let\u2019s walk through an example that showcases the document summary index, over Wikipedia articles about different cities.\n </p>\n <p>\n  The rest of this guide showcases the relevant code snippets. You can find the\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/index_structs/doc_summary/DocSummary.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   full walkthrough here\n  </a>\n  (and here\u2019s the\n  <a href=\"https://github.com/jerryjliu/llama_index/blob/main/docs/examples/index_structs/doc_summary/DocSummary.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   notebook link\n  </a>\n  ).\n </p>\n <p>\n  We can build the\n  <code class=\"cw qs qt qu qv b\">\n   GPTDocumentSummaryIndex\n  </code>\n  over a set of documents, and pass in a\n  <code class=\"cw qs qt qu qv b\">\n   ResponseSynthesizer\n  </code>\n  object to synthesize summaries for the documents.\n </p>\n <pre><span class=\"qz nz gt qv b bf ra rb l rc rd\" id=\"1784\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> (\n    SimpleDirectoryReader,\n    LLMPredictor,\n    ServiceContext,\n    ResponseSynthesizer\n)\n<span class=\"hljs-keyword\">from</span> llama_index.indices.document_summary <span class=\"hljs-keyword\">import</span> GPTDocumentSummaryIndex\n<span class=\"hljs-keyword\">from</span> langchain.chat_models <span class=\"hljs-keyword\">import</span> ChatOpenAI\n\n<span class=\"hljs-comment\"># load docs, define service context</span>\n...\n\n<span class=\"hljs-comment\"># build the index</span>\nresponse_synthesizer = ResponseSynthesizer.from_args(response_mode=<span class=\"hljs-string\">\"tree_summarize\"</span>, use_async=<span class=\"hljs-literal\">True</span>)\ndoc_summary_index = GPTDocumentSummaryIndex.from_documents(\n    city_docs, \n    service_context=service_context,\n    response_synthesizer=response_synthesizer\n)</span></pre>\n <p>\n  Once the index is built, we can get the summary for any given document:\n </p>\n <pre><span class=\"qz nz gt qv b bf ra rb l rc rd\" id=\"3180\">summary = doc_summary_index.get_document_summary(\"Boston\")</span></pre>\n <p>\n  Next, let\u2019s walk through an example LLM-based retrieval over the index.\n </p>\n <pre><span class=\"qz nz gt qv b bf ra rb l rc rd\" id=\"b47a\"><span class=\"hljs-keyword\">from</span> llama_index.indices.document_summary <span class=\"hljs-keyword\">import</span> DocumentSummaryIndexRetriever\n\nretriever = DocumentSummaryIndexRetriever(\n    doc_summary_index,\n    <span class=\"hljs-comment\"># choice_select_prompt=choice_select_prompt,</span>\n    <span class=\"hljs-comment\"># choice_batch_size=choice_batch_size,</span>\n    <span class=\"hljs-comment\"># format_node_batch_fn=format_node_batch_fn,</span>\n    <span class=\"hljs-comment\"># parse_choice_select_answer_fn=parse_choice_select_answer_fn,</span>\n    <span class=\"hljs-comment\"># service_context=service_context</span>\n)\nretrieved_nodes = retriever.retrieve(<span class=\"hljs-string\">\"What are the sports teams in Toronto?\"</span>)\n<span class=\"hljs-built_in\">print</span>(retrieved_nodes[<span class=\"hljs-number\">0</span>].score)\n<span class=\"hljs-built_in\">print</span>(retrieved_nodes[<span class=\"hljs-number\">0</span>].node.get_text())The retriever will retrieve a <span class=\"hljs-built_in\">set</span> of relevant nodes <span class=\"hljs-keyword\">for</span> a given index.</span></pre>\n <p>\n  Note that the LLM returns relevance scores in addition to the document text:\n </p>\n <pre><span class=\"qz nz gt qv b bf ra rb l rc rd\" id=\"fde9\">8.0\nToronto ( (listen) t\u0259-RON-toh; locally [t\u0259\u02c8\u0279\u0252\u027e\u0303\u0259] or [\u02c8t\u0279\u0252\u027e\u0303\u0259]) is the capital city of the Canadian province of Ontario. With a recorded population of 2,794,356 in 2021, it is the most populous city in Canada...</span></pre>\n <p>\n  We can also use the index as part of an overall query engine, to not only retrieve the relevant context, but also synthesize a response to a given question. We can do this through both the high-level API as well as lower-level API.\n </p>\n <p>\n  <strong>\n   High-level API\n  </strong>\n </p>\n <pre><span class=\"qz nz gt qv b bf ra rb l rc rd\" id=\"c21b\">query_engine = doc_summary_index.as_query_engine(\n  response_mode=<span class=\"hljs-string\">\"tree_summarize\"</span>, use_async=<span class=\"hljs-literal\">True</span>\n)\nresponse = query_engine.query(<span class=\"hljs-string\">\"What are the sports teams in Toronto?\"</span>)\n<span class=\"hljs-built_in\">print</span>(response)</span></pre>\n <p>\n  <strong>\n   Lower-level API\n  </strong>\n </p>\n <pre><span class=\"qz nz gt qv b bf ra rb l rc rd\" id=\"d22f\"><span class=\"hljs-comment\"># use retriever as part of a query engine</span>\n<span class=\"hljs-keyword\">from</span> llama_index.query_engine <span class=\"hljs-keyword\">import</span> RetrieverQueryEngine\n\n<span class=\"hljs-comment\"># configure response synthesizer</span>\nresponse_synthesizer = ResponseSynthesizer.from_args()\n\n<span class=\"hljs-comment\"># assemble query engine</span>\nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    response_synthesizer=response_synthesizer,\n)\n\n<span class=\"hljs-comment\"># query</span>\nresponse = query_engine.query(<span class=\"hljs-string\">\"What are the sports teams in Toronto?\"</span>)\n<span class=\"hljs-built_in\">print</span>(response)</span></pre>\n <h1>\n  <strong>\n   Next Steps\n  </strong>\n </h1>\n <p>\n  The approach of autosummarization over any piece of text is really exciting. We\u2019re excited to develop extensions in two areas:\n </p>\n <ul>\n  <li>\n   Continue exploring autosummarization in different layers. Currently it\u2019s at the doc-level, but what about summarizing a big text chunk into a smaller one? (e.g. a one-liner).\n  </li>\n  <li>\n   Continue exploring LLM-based retrieval, which summarization helps to unlock.\n  </li>\n </ul>\n <p>\n  Also we\u2019re sharing the example guide/notebook below in case you missed it above:\n </p>\n <p>\n  <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/index_structs/doc_summary/DocSummary.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Document Summary Guide\n  </a>\n </p>\n <p>\n  <a href=\"https://github.com/jerryjliu/llama_index/blob/main/docs/examples/index_structs/doc_summary/DocSummary.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Notebook Link\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 11670, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2e44e215-8834-4311-bf81-88c02de64919": {"__data__": {"id_": "2e44e215-8834-4311-bf81-88c02de64919", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/agentic-rag-with-llamaindex-2721b8a49ff6.html", "file_name": "agentic-rag-with-llamaindex-2721b8a49ff6.html", "file_type": "text/html", "file_size": 7713, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0b5088c8-9467-4361-a295-76d163336b51", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/agentic-rag-with-llamaindex-2721b8a49ff6.html", "file_name": "agentic-rag-with-llamaindex-2721b8a49ff6.html", "file_type": "text/html", "file_size": 7713, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "8fb9802b11fdf1a7cc9b88f317602b2d90a2f243e9ddf1f50ef979e235feede9", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  The topic of Agentic RAG explores how agents can be incorporated into existing\n  RAG pipelines for enhanced, conversational search and retrieval.\n </p>\n <h1>\n  Introduction\n </h1>\n <p>\n  <strong>\n   Considering the architecture below, it is evident how Agentic RAG creates\n      an implementation which easily scales. New documents can be added with\n      each new set being managed by a sub-agent.\n  </strong>\n </p>\n <p>\n  The basic structure of LlamaIndex\u2019s approach called Agentic RAG is shown in\n  the diagram below where a large set of documents are ingested, in this case it\n  was limited to 100.\n </p>\n <p>\n  The large corpus of data is broken up into smaller documents. An agent is\n  created for each document, and each of the numerous document agents have the\n  power of search via embeddings and to summarise the response.\n </p>\n <p>\n  A top-level agent is created over the set of document agents. The meta-agent /\n  top-level agent performs tool retrieval and then uses Chain-of-Thought to\n  answer the user\u2019s question.\n </p>\n <p>\n  The Rerank endpoint computes a relevance score for the query and each\n  document, and returns a sorted list from the most to the least relevant\n  document.\n </p>\n <h1>\n  Notebook Example\n </h1>\n <p>\n  Here you will find a Colab\n  <a href=\"https://github.com/cobusgreyling/LlamaIndex/blob/d8902482a247c76c7902ded143a875d5580f072a/Agentic_RAG_Multi_Document_Agents-v1.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   notebook\n  </a>\n  with a fully working and executed example of this\n  <a href=\"https://github.com/cobusgreyling/LlamaIndex/blob/d8902482a247c76c7902ded143a875d5580f072a/Agentic_RAG_Multi_Document_Agents-v1.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   implementation\n  </a>\n  .\n </p>\n <p>\n  To run the notebook, you will need an OpenAI and Cohere API key\u2026\n </p>\n <pre><span>import osimport openaiimport cohereos.environ[\"OPENAI_API_KEY\"] = \"Your API Key goes here\"os.environ[\"COHERE_API_KEY\"] = \"Your API Key goes here\"</span></pre>\n <p>\n  For each document agent, the system prompt is:\n </p>\n <pre><span>You are a specialized agent designed to answer queries about the `{file_base}.html` part of the LlamaIndex docs.You must ALWAYS use at least one of the tools provided when answering a question; do NOT rely on prior knowledge.</span></pre>\n <p>\n  The system prompt for the meta-agent /top-agent:\n </p>\n <pre><span> You are an agent designed to answer queries about the documentation. Please always use the tools provided to answer a question. Do not rely on prior knowledge</span></pre>\n <h2>\n  Working Example\n </h2>\n <p>\n  I asked the agent the following question:\n </p>\n <p>\n  <code>\n   Tell me about LlamaIndex connectors\n  </code>\n </p>\n <p>\n  Below is the detailed answer generated by the agent:\n </p>\n <p>\n  <strong>\n   <em>\n    Thought: I need to use a tool to help me answer the question.\n   </em>\n  </strong>\n </p>\n <p>\n  <strong>\n   <em>\n    Action: tool_latest_search Action Input: {\u2018input\u2019: \u2018LlamaIndex\n      connectors\u2019}\n   </em>\n  </strong>\n </p>\n <p>\n  Added user message to memory: LlamaIndex connectors\n </p>\n <p>\n  === Calling Function ===\n  <br>\n   Calling function: vector_tool_latest_search\n  with args: {\n   <br>\n    \u201cinput\u201d: \u201cLlamaIndex connectors\u201d\n    <br>\n     }\n     <br>\n      Got output:\n  The LlamaIndex connectors can be found on the\n  /content/docs.llamaindex.ai/en/latest/search.html page.\n  ========================\n     </br>\n    </br>\n   </br>\n  </br>\n </p>\n <p>\n  <strong>\n   <em>\n    Observation: The LlamaIndex connectors are not specifically mentioned in\n      the `latest_search.html` part of the LlamaIndex docs. You may need to\n      check other sections of the documentation for detailed information about\n      LlamaIndex connectors.\n   </em>\n  </strong>\n </p>\n <p>\n  <strong>\n   <em>\n    Thought: I need to search for information about LlamaIndex connectors in\n      a different tool.\n   </em>\n  </strong>\n </p>\n <p>\n  <strong>\n   <em>\n    Action: tool_latest_index Action Input: {\u2018input\u2019: \u2018LlamaIndex\n      connectors\u2019}\n   </em>\n  </strong>\n </p>\n <p>\n  Added user message to memory: LlamaIndex connectors\n </p>\n <p>\n  === Calling Function ===\n  <br>\n   Calling function: vector_tool_latest_index with\n  args: {\n   <br>\n    \u201cinput\u201d: \u201cLlamaIndex connectors\u201d\n    <br>\n     }\n    </br>\n   </br>\n  </br>\n </p>\n <p>\n  Got output: LlamaIndex provides data connectors that allow you to ingest your\n  existing data from various sources and formats. These connectors can work with\n  APIs, PDFs, SQL, and more. By using LlamaIndex connectors, you can easily\n  bring your data into the LlamaIndex ecosystem and make it accessible for\n  natural language access and retrieval. ========================\n </p>\n <p>\n  <strong>\n   <em>\n    Observation: LlamaIndex offers data connectors that enable you to import\n      your existing data from a variety of sources and formats. These connectors\n      are compatible with APIs, PDFs, SQL, and more. Utilizing LlamaIndex\n      connectors allows you to seamlessly integrate your data into the\n      LlamaIndex ecosystem, making it readily available for natural language\n      access and retrieval.\n   </em>\n  </strong>\n </p>\n <p>\n  <strong>\n   <em>\n    Thought: I can answer without using any more tools.\n   </em>\n  </strong>\n </p>\n <p>\n  <strong>\n   <em>\n    Answer: LlamaIndex connectors are used to import existing data from\n      various sources and formats into the LlamaIndex ecosystem. These\n      connectors are compatible with APIs, PDFs, SQL, and more, allowing\n      seamless integration of data for natural language access and\n      retrieval.\n   </em>\n  </strong>\n </p>\n <p>\n  Below is a snipped from the Colab notebook:\n </p>\n <figure>\n  <figcaption>\n   <a href=\"https://github.com/cobusgreyling/LlamaIndex/blob/d8902482a247c76c7902ded143a875d5580f072a/Agentic_RAG_Multi_Document_Agents-v1.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Source\n   </a>\n  </figcaption>\n </figure>\n <p>\n  This complex implementation from LlamaIndex is an example of multi-document\n  agents which can:\n </p>\n <ol>\n  <li>\n   Select documents relevant to a user query\n  </li>\n  <li>\n   Execute an agentic loop over the documents relevant to the query; including\n    chain-of-thought, summarisation and reranking.\n  </li>\n </ol>\n <h1>\n  In Conclusion\n </h1>\n <p>\n  This implementation by LlamaIndex illustrates a few key principles\u2026\n </p>\n <ol>\n  <li>\n   Agentic RAG, where an agent approach is followed for a RAG implementation\n    adds resilience and intelligence to the RAG implementation.\n  </li>\n  <li>\n   It is a good illustration of multi-agent orchestration.\n  </li>\n  <li>\n   This architecture serves as a good reference framework of how scaling an\n    agent can be optimised with a second tier of smaller worker-agents.\n  </li>\n  <li>\n   Agentic RAG is an example of a controlled and well defined\n   <strong>\n    <em>\n     autonomous\n    </em>\n   </strong>\n   <strong>\n    <em>\n     agent\n    </em>\n   </strong>\n   implementation.\n  </li>\n  <li>\n   One of the most sought-after enterprise LLM implementation types are RAG,\n    Agentic RAG is a natural progression of this.\n  </li>\n  <li>\n   It is easy to envisage how this architecture can grow and expand over an\n    organisation with more sub bots being added.\n  </li>\n </ol>\n <p>\n  <strong>\n   <em>\n    \u2b50\ufe0f Follow me on\n   </em>\n  </strong>\n  <a href=\"https://www.linkedin.com/in/cobusgreyling/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   <strong>\n    <em>\n     LinkedIn\n    </em>\n   </strong>\n  </a>\n  <strong>\n   <em>\n    for updates on Large Language Models \u2b50\ufe0f\n   </em>\n  </strong>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 7664, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8b5dfa16-fc80-4788-b048-8a9102dabe98": {"__data__": {"id_": "8b5dfa16-fc80-4788-b048-8a9102dabe98", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.html", "file_name": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.html", "file_type": "text/html", "file_size": 18536, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37bc1481-63f1-4203-b395-0fb229570adf", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.html", "file_name": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.html", "file_type": "text/html", "file_size": 18536, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "6117b1853de7eeb213f072f64310f9bba29c1a0aae4b95f4c59ccd1e327cbd0d", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <h1>\n  Introduction\n </h1>\n <p>\n  The C3 Voice Assistant is my latest project aimed at making Large Language Model (LLM) and Retrieval-Augmented Generation (RAG) applications more accessible. This voice-activated assistant caters to a broad audience, including those facing typing challenges or accessibility issues.\n </p>\n <h1>\n  Features\n </h1>\n <ul>\n  <li>\n   <strong>\n    Voice Activation:\n   </strong>\n   Initiated by saying \u201cC3.\u201d Alternatively, users can click the blue ring to activate the listening mode of the app. The wake word \u201cC3\u201d is configurable and you can choose any other word.\n  </li>\n  <li>\n   <strong>\n    Universal Accessibility:\n   </strong>\n   Ideal for users preferring voice commands or facing typing challenges.\n  </li>\n  <li>\n   <strong>\n    LLM Integration:\n   </strong>\n   Capable of general queries and document-specific inquiries (e.g., Nvidia\u2019s FY 2023 10K report).\n  </li>\n  <li>\n   <strong>\n    User-Friendly Interface:\n   </strong>\n   The interface of the AI voice assistant is designed for simplicity and ease of use, focusing on voice chat interactions. It features a minimalistic and user-friendly React.js layout. Additionally, there is a convenient sidebar that displays the entire chat history in text format, allowing users to review and reflect on their interactions with the AI.\n  </li>\n </ul>\n <h1>\n  The Tech Stack\n </h1>\n <p>\n  The app is built on a robust and flexible tech stack that ensures a smooth, reliable, and efficient user experience. Here\u2019s an overview:\n </p>\n <ul>\n  <li>\n   <strong>\n    Frontend:\n   </strong>\n   The user interface is a custom application developed using React.js. It\u2019s designed to be minimalistic yet highly functional, prioritizing ease of use and accessibility.\n  </li>\n  <li>\n   <strong>\n    Backend:\n   </strong>\n   The server-side operations are powered by Python Flask. I\u2019ve utilized the innovative \u2018create-llama\u2019 feature from LlamaIndex, which significantly streamlines the development process.\n  </li>\n  <li>\n   <strong>\n    Hosting:\n   </strong>\n   For a seamless performance, the frontend of the C3 Voice Assistant is hosted on Vercel. The backend, on the other hand, is deployed on Render, ensuring efficient management and operation of server-side tasks.\n  </li>\n </ul>\n <h1>\n  Building the Frontend\n </h1>\n <p>\n  The frontend, built with React.js, focuses on user interaction and accessibility. The\n  <code class=\"cw qb qc qd qe b\">\n   App.js\n  </code>\n  script incorporates features like wake word recognition, speech-to-text conversion, state management, and dynamic UI elements like speech bubbles and spinners.\n </p>\n <h2>\n  1. Component and State Initialization\n </h2>\n <p>\n  This section sets up the React component and initializes various states, such as\n  <code class=\"cw qb qc qd qe b\">\n   appState\n  </code>\n  to track the current state of the app (idle, listening, speaking), and\n  <code class=\"cw qb qc qd qe b\">\n   transcript\n  </code>\n  to store the text transcribed from user speech.\n </p>\n <pre><span class=\"qx nb gt qe b bf qy qz l ra rb\" id=\"0e15\"><span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">React</span>, { useState, useRef, useEffect } <span class=\"hljs-keyword\">from</span> <span class=\"hljs-string\">\"react\"</span>;\n<span class=\"hljs-keyword\">import</span> <span class=\"hljs-string\">\"./App.css\"</span>;\n\n<span class=\"hljs-keyword\">const</span> <span class=\"hljs-title class_\">App</span> = () =&amp;gt; {\n  <span class=\"hljs-keyword\">const</span> [appState, setAppState] = <span class=\"hljs-title function_\">useState</span>(<span class=\"hljs-string\">\"idle\"</span>);\n  <span class=\"hljs-keyword\">const</span> [transcript, setTranscript] = <span class=\"hljs-title function_\">useState</span>(<span class=\"hljs-string\">\"\"</span>);\n  <span class=\"hljs-comment\">// Additional state and ref declarations...</span>\n};</span></pre>\n <h2>\n  2. Speech Recognition Setup\n </h2>\n <p>\n  In this useEffect hook, two speech recognition instances are initialized: one for detecting the wake word \u201cC3\u201d and another for the main speech recognition. This setup ensures that the app starts listening for commands when \u201cC3\u201d is mentioned.\n </p>\n <p>\n  You can easily swap \u201cC3\u201d with any other wake word of your choice.\n </p>\n <pre><span class=\"qx nb gt qe b bf qy qz l ra rb\" id=\"d657\">  <span class=\"hljs-title function_\">useEffect</span>(() =&amp;gt; {\n    <span class=\"hljs-comment\">// Wake word listener setup</span>\n    <span class=\"hljs-keyword\">const</span> <span class=\"hljs-title class_\">WakeWordSpeechRecognition</span> =\n      <span class=\"hljs-variable language_\">window</span>.<span class=\"hljs-property\">SpeechRecognition</span> || <span class=\"hljs-variable language_\">window</span>.<span class=\"hljs-property\">webkitSpeechRecognition</span>;\n    <span class=\"hljs-keyword\">if</span> (<span class=\"hljs-title class_\">WakeWordSpeechRecognition</span> &amp;amp;&amp;amp; !wakeWordRecognitionRef.<span class=\"hljs-property\">current</span>) {\n      wakeWordRecognitionRef.<span class=\"hljs-property\">current</span> = <span class=\"hljs-keyword\">new</span> <span class=\"hljs-title class_\">WakeWordSpeechRecognition</span>();\n      wakeWordRecognitionRef.<span class=\"hljs-property\">current</span>.<span class=\"hljs-property\">continuous</span> = <span class=\"hljs-literal\">true</span>;\n      wakeWordRecognitionRef.<span class=\"hljs-property\">current</span>.<span class=\"hljs-property\">interimResults</span> = <span class=\"hljs-literal\">false</span>;\n\n      wakeWordRecognitionRef.<span class=\"hljs-property\">current</span>.<span class=\"hljs-property\">onresult</span> = (event) =&amp;gt; {\n        <span class=\"hljs-keyword\">const</span> transcript = event.<span class=\"hljs-property\">results</span>[event.<span class=\"hljs-property\">results</span>.<span class=\"hljs-property\">length</span> - <span class=\"hljs-number\">1</span>][<span class=\"hljs-number\">0</span>].<span class=\"hljs-property\">transcript</span>\n          .<span class=\"hljs-title function_\">trim</span>()\n          .<span class=\"hljs-title function_\">toLowerCase</span>();\n        <span class=\"hljs-keyword\">if</span> (transcript.<span class=\"hljs-title function_\">includes</span>(<span class=\"hljs-string\">\"c3\"</span>)) {\n          <span class=\"hljs-title function_\">toggleRecording</span>(); <span class=\"hljs-comment\">// Start the main speech recognition process</span>\n        }\n      };\n\n      wakeWordRecognitionRef.<span class=\"hljs-property\">current</span>.<span class=\"hljs-title function_\">start</span>();\n    }\n\n    <span class=\"hljs-comment\">// Main speech recognition setup</span>\n    <span class=\"hljs-keyword\">const</span> <span class=\"hljs-title class_\">SpeechRecognition</span> =\n      <span class=\"hljs-variable language_\">window</span>.<span class=\"hljs-property\">SpeechRecognition</span> || <span class=\"hljs-variable language_\">window</span>.<span class=\"hljs-property\">webkitSpeechRecognition</span>;\n    <span class=\"hljs-keyword\">if</span> (<span class=\"hljs-title class_\">SpeechRecognition</span> &amp;amp;&amp;amp; !recognitionRef.<span class=\"hljs-property\">current</span>) {\n      recognitionRef.<span class=\"hljs-property\">current</span> = <span class=\"hljs-keyword\">new</span> <span class=\"hljs-title class_\">SpeechRecognition</span>();\n      recognitionRef.<span class=\"hljs-property\">current</span>.<span class=\"hljs-property\">continuous</span> = <span class=\"hljs-literal\">false</span>;\n      recognitionRef.<span class=\"hljs-property\">current</span>.<span class=\"hljs-property\">interimResults</span> = <span class=\"hljs-literal\">false</span>;\n\n      recognitionRef.<span class=\"hljs-property\">current</span>.<span class=\"hljs-property\">onresult</span> = (event) =&amp;gt; {\n        <span class=\"hljs-keyword\">const</span> lastResultIndex = event.<span class=\"hljs-property\">results</span>.<span class=\"hljs-property\">length</span> - <span class=\"hljs-number\">1</span>;\n        <span class=\"hljs-keyword\">const</span> transcriptResult = event.<span class=\"hljs-property\">results</span>[lastResultIndex][<span class=\"hljs-number\">0</span>].<span class=\"hljs-property\">transcript</span>;\n        <span class=\"hljs-title function_\">setTranscript</span>(transcriptResult);\n        <span class=\"hljs-title function_\">setAppState</span>(<span class=\"hljs-string\">\"playing\"</span>);\n        <span class=\"hljs-title function_\">setShowSpeechBubble</span>(<span class=\"hljs-literal\">true</span>);\n        <span class=\"hljs-built_in\">setTimeout</span>(() =&amp;gt; <span class=\"hljs-title function_\">setShowSpeechBubble</span>(<span class=\"hljs-literal\">false</span>), speechBubbleTimeout);\n        <span class=\"hljs-title function_\">fetchResponseFromLLM</span>(transcriptResult);\n      };\n\n      recognitionRef.<span class=\"hljs-property\">current</span>.<span class=\"hljs-property\">onend</span> = () =&amp;gt; {\n        <span class=\"hljs-title function_\">setShowSpinner</span>(<span class=\"hljs-literal\">true</span>);\n      };\n    }\n  }, []);</span></pre>\n <h2>\n  3. Handling User Speech and Response\n </h2>\n <p>\n  <code class=\"cw qb qc qd qe b\">\n   toggleRecording\n  </code>\n  controls the speech recognition process, while\n  <code class=\"cw qb qc qd qe b\">\n   fetchResponseFromLLM\n  </code>\n  sends the user's speech to the LLM backend and handles the response. This response is then spoken out via speech synthesis and also used to update the chat history displayed on the UI.\n </p>\n <pre><span class=\"qx nb gt qe b bf qy qz l ra rb\" id=\"79b9\"> <span class=\"hljs-keyword\">const</span> toggleRecording = () =&amp;gt; {\n    <span class=\"hljs-keyword\">try</span> {\n      <span class=\"hljs-keyword\">if</span> (appState === <span class=\"hljs-string\">\"idle\"</span>) {\n        recognitionRef.<span class=\"hljs-property\">current</span>.<span class=\"hljs-title function_\">start</span>();\n        <span class=\"hljs-title function_\">setAppState</span>(<span class=\"hljs-string\">\"listening\"</span>);\n      } <span class=\"hljs-keyword\">else</span> <span class=\"hljs-keyword\">if</span> (appState === <span class=\"hljs-string\">\"listening\"</span>) {\n        recognitionRef.<span class=\"hljs-property\">current</span>.<span class=\"hljs-title function_\">stop</span>();\n      }\n    } <span class=\"hljs-keyword\">catch</span> (error) {\n    }\n  };</span></pre>\n <pre><span class=\"qx nb gt qe b bf qy qz l ra rb\" id=\"05ef\">  <span class=\"hljs-keyword\">const</span> fetchResponseFromLLM = <span class=\"hljs-keyword\">async</span> (text) =&amp;gt; {\n    <span class=\"hljs-keyword\">try</span> {\n      <span class=\"hljs-keyword\">const</span> response = <span class=\"hljs-keyword\">await</span> <span class=\"hljs-title function_\">fetch</span>(\n        <span class=\"hljs-string\">`https://c3-python-nostream.onrender.com/api/chat`</span>,\n        {\n          <span class=\"hljs-attr\">method</span>: <span class=\"hljs-string\">\"POST\"</span>,\n          <span class=\"hljs-attr\">headers</span>: { <span class=\"hljs-string\">\"Content-Type\"</span>: <span class=\"hljs-string\">\"application/json\"</span> },\n          <span class=\"hljs-attr\">body</span>: <span class=\"hljs-title class_\">JSON</span>.<span class=\"hljs-title function_\">stringify</span>({\n            <span class=\"hljs-attr\">messages</span>: [\n              {\n                <span class=\"hljs-attr\">role</span>: <span class=\"hljs-string\">\"user\"</span>,\n                <span class=\"hljs-attr\">content</span>:\n                  <span class=\"hljs-string\">\"You are an AI voice assistant called C3. You can provide any general information as well as answer basic questions about the Nvidia 10k report for year ended Jan 2023\"</span> +\n                  text,\n              },\n            ],\n          }),\n        }\n      );\n      <span class=\"hljs-keyword\">const</span> data = <span class=\"hljs-keyword\">await</span> response.<span class=\"hljs-title function_\">json</span>();\n\n      <span class=\"hljs-title function_\">setChatHistory</span>((prevHistory) =&amp;gt; [\n        ...prevHistory,\n        { <span class=\"hljs-attr\">query</span>: text, <span class=\"hljs-attr\">response</span>: data.<span class=\"hljs-property\">result</span>.<span class=\"hljs-property\">content</span> },\n      ]);\n      <span class=\"hljs-title function_\">speak</span>(data.<span class=\"hljs-property\">result</span>.<span class=\"hljs-property\">content</span>);\n    } <span class=\"hljs-keyword\">catch</span> (error) {\n      <span class=\"hljs-variable language_\">console</span>.<span class=\"hljs-title function_\">error</span>(<span class=\"hljs-string\">\"Error communicating with LLM:\"</span>, error);\n    }\n  };</span></pre>\n <h2>\n  4. Speech Synthesis\n </h2>\n <p>\n  The\n  <code class=\"cw qb qc qd qe b\">\n   speak\n  </code>\n  function takes the text response from the LLM and uses the SpeechSynthesis API to read it aloud, providing an interactive experience for the user.\n </p>\n <pre><span class=\"qx nb gt qe b bf qy qz l ra rb\" id=\"e336\">  <span class=\"hljs-keyword\">const</span> speak = (text) =&amp;gt; {\n    <span class=\"hljs-keyword\">if</span> (synthRef.<span class=\"hljs-property\">current</span> &amp;amp;&amp;amp; text) {\n      <span class=\"hljs-keyword\">const</span> utterance = <span class=\"hljs-keyword\">new</span> <span class=\"hljs-title class_\">SpeechSynthesisUtterance</span>(text);\n\n      <span class=\"hljs-keyword\">const</span> voices = <span class=\"hljs-variable language_\">window</span>.<span class=\"hljs-property\">speechSynthesis</span>.<span class=\"hljs-title function_\">getVoices</span>();\n      <span class=\"hljs-keyword\">if</span> (voices.<span class=\"hljs-property\">length</span> &amp;gt; <span class=\"hljs-number\">0</span>) {\n        utterance.<span class=\"hljs-property\">voice</span> = voices[<span class=\"hljs-number\">3</span>]; <span class=\"hljs-comment\">// You can change this to select different voices</span>\n      }\n\n      utterance.<span class=\"hljs-property\">onstart</span> = () =&amp;gt; {\n        <span class=\"hljs-variable language_\">console</span>.<span class=\"hljs-title function_\">log</span>(<span class=\"hljs-string\">\"TTS starts speaking\"</span>);\n        <span class=\"hljs-title function_\">setShowSpinner</span>(<span class=\"hljs-literal\">false</span>);\n      };\n\n      utterance.<span class=\"hljs-property\">onend</span> = () =&amp;gt; {\n        <span class=\"hljs-title function_\">setAppState</span>(<span class=\"hljs-string\">\"idle\"</span>);\n        <span class=\"hljs-keyword\">if</span> (wakeWordRecognitionRef.<span class=\"hljs-property\">current</span>) {\n          wakeWordRecognitionRef.<span class=\"hljs-property\">current</span>.<span class=\"hljs-title function_\">start</span>(); <span class=\"hljs-comment\">// Restart wake word listener after speaking</span>\n        }\n      };\n      synthRef.<span class=\"hljs-property\">current</span>.<span class=\"hljs-title function_\">speak</span>(utterance);\n    }</span></pre>\n <h2>\n  5. UI Rendering\n </h2>\n <p>\n  The return statement of the\n  <code class=\"cw qb qc qd qe b\">\n   App\n  </code>\n  function contains the JSX code for rendering the app's UI. This includes buttons for starting/stopping the voice interaction, a display area for the transcript, and a chat sidebar showing the history of interactions.\n </p>\n <p>\n  By combining voice recognition, LLM integration, and speech synthesis, this frontend component provides a comprehensive and accessible interface for interacting with the C3 Voice Assistant.\n </p>\n <h1>\n  Backend Server Setup\n </h1>\n <ol>\n  <li>\n   Initialize Create-Llama: Run\n   <code class=\"cw qb qc qd qe b\">\n    npx create-llama@latest\n   </code>\n   in your terminal.\n  </li>\n  <li>\n   Follow the prompts to set up a Python FastAPI backend, which we can be integrated with our frontend.\n  </li>\n  <li>\n   Use\n   <code class=\"cw qb qc qd qe b\">\n    poetry install\n   </code>\n   and\n   <code class=\"cw qb qc qd qe b\">\n    poetry shell\n   </code>\n   to prepare the environment.\n  </li>\n  <li>\n   Create a\n   <code class=\"cw qb qc qd qe b\">\n    .env\n   </code>\n   file with\n   <code class=\"cw qb qc qd qe b\">\n    OPENAI_API_KEY=&lt;openai_api_key&gt;\n   </code>\n   .\n  </li>\n  <li>\n   Generate Embeddings (optional): If a\n   <code class=\"cw qb qc qd qe b\">\n    ./data\n   </code>\n   directory exists, run\n   <code class=\"cw qb qc qd qe b\">\n    python app/engine/generate.py\n   </code>\n   .\n  </li>\n  <li>\n   Execute\n   <code class=\"cw qb qc qd qe b\">\n    python main.py\n   </code>\n   to start the server.\n  </li>\n  <li>\n   Test the API: Use\n   <code class=\"cw qb qc qd qe b\">\n    curl --location 'localhost:8000/api/chat' --header 'Content-Type: application/json' --data '{ \"messages\": [{ \"role\": \"user\", \"content\": \"Hello\" }] }'\n   </code>\n   to test.\n  </li>\n  <li>\n   Modify API behavior in\n   <code class=\"cw qb qc qd qe b\">\n    app/api/routers/chat.py\n   </code>\n   . The server supports CORS for all origins, alterable with the\n   <code class=\"cw qb qc qd qe b\">\n    ENVIRONMENT=prod\n   </code>\n   setting.\n  </li>\n </ol>\n <h1>\n  Integration\n </h1>\n <p>\n  Once the backend server is set up, integrating it with the frontend is straightforward. Simply update the\n  <code class=\"cw qb qc qd qe b\">\n   fetchResponseFromLLM\n  </code>\n  function in your frontend's\n  <code class=\"cw qb qc qd qe b\">\n   App.js\n  </code>\n  to call the backend server URL. This change ensures that when the frontend makes a request, it communicates with your newly configured backend, thus effectively integrating the two components.\n </p>\n <h1>\n  Final Thoughts\n </h1>\n <p>\n  Wrapping up, the C3 Voice Assistant isn\u2019t just a tech showcase; it\u2019s a stride towards democratizing AI. It\u2019s about making powerful AI tools, like LLMs and RAG, accessible and user-friendly. This project is more than lines of code \u2014 it\u2019s a push to break down tech barriers and empower everyone.\n </p>\n <p>\n  Your thoughts and feedback are invaluable \u2014 let\u2019s make AI more accessible together!\n </p>\n <p>\n  Link to Github Repo:\n  <a href=\"https://github.com/AI-ANK/C3-Voice-Assistant-UI\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Frontend\n  </a>\n  and\n  <a href=\"https://github.com/AI-ANK/c3-python-nostream\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Backend\n  </a>\n </p>\n <p>\n  <a href=\"https://www.linkedin.com/in/harshadsuryawanshi/\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Connect with Me on LinkedIn\n  </a>\n </p>\n <p>\n  <a href=\"https://www.linkedin.com/posts/harshadsuryawanshi_ai-llamaindex-gpt3-activity-7149796976442740736-1lXj?utm_source=share&amp;utm_medium=member_desktop\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   Linkedin Post\n  </a>\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 18489, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bd717d20-9a29-4785-b703-84ecbe3ec43b": {"__data__": {"id_": "bd717d20-9a29-4785-b703-84ecbe3ec43b", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/announcing-llamaindex-0-9-719f03282945.html", "file_name": "announcing-llamaindex-0-9-719f03282945.html", "file_type": "text/html", "file_size": 18985, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9a5b129a-cf71-41a1-8f62-d32fbb2f5bc2", "node_type": "4", "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/announcing-llamaindex-0-9-719f03282945.html", "file_name": "announcing-llamaindex-0-9-719f03282945.html", "file_type": "text/html", "file_size": 18985, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "hash": "aca8508a39c87881acdb169298f52331e0b3eedfd67890790ebe9784f483caa4", "class_name": "RelatedNodeInfo"}}, "text": "<div class=\"BlogPost_htmlPost__Z5oDL\">\n <p>\n  Our hard-working team is delighted to announce our latest major release, LlamaIndex 0.9! You can get it right now:\n </p>\n <p>\n  <code class=\"cw om on oo op b\">\n   pip install --upgrade llama_index\n  </code>\n </p>\n <p>\n  In LlamaIndex v0.9, we are taking the time to refine several key aspects of the user experience, including token counting, text splitting, and more!\n </p>\n <p>\n  As part of this, there are some new features and minor changes to current usage that developers should be aware of:\n </p>\n <ul>\n  <li>\n   New\n   <code class=\"cw om on oo op b\">\n    IngestionPipline\n   </code>\n   concept for ingesting and transforming data\n  </li>\n  <li>\n   Data ingestion and transforms are now automatically cached\n  </li>\n  <li>\n   Updated interface for node parsing/text splitting/metadata extraction modules\n  </li>\n  <li>\n   Changes to the default tokenizer, as well as customizing the tokenizer\n  </li>\n  <li>\n   Packaging/Installation changes with PyPi (reduced bloat, new install options)\n  </li>\n  <li>\n   More predictable and consistent import paths\n  </li>\n  <li>\n   Plus, in beta: MultiModal RAG Modules for handling text and images!\n  </li>\n </ul>\n <p>\n  Have questions or concerns? You can\n  <a href=\"https://github.com/run-llama/llama_index/issues\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   report an issue\n  </a>\n  on GitHub or\n  <a href=\"https://discord.com/invite/eN6D2HQ4aX\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   ask a question on our Discord\n  </a>\n  !\n </p>\n <p>\n  Read on for more details on our new features and changes.\n </p>\n <h1>\n  IngestionPipeline \u2014 New abstraction for purely ingesting data\n </h1>\n <p>\n  Sometimes, all you want is to ingest and embed nodes from data sources, for instance if your application allows users to upload new data. New in LlamaIndex V0.9 is the concept of an\n  <code class=\"cw om on oo op b\">\n   IngestionPipepline\n  </code>\n  .\n </p>\n <p>\n  An\n  <code class=\"cw om on oo op b\">\n   IngestionPipeline\n  </code>\n  uses a new concept of\n  <code class=\"cw om on oo op b\">\n   Transformations\n  </code>\n  that are applied to input data.\n </p>\n <p>\n  What is a\n  <code class=\"cw om on oo op b\">\n   Transformation\n  </code>\n  though? It could be a:\n </p>\n <ul>\n  <li>\n   text splitter\n  </li>\n  <li>\n   node parser\n  </li>\n  <li>\n   metadata extractor\n  </li>\n  <li>\n   embeddings model\n  </li>\n </ul>\n <p>\n  Here\u2019s a quick example of the basic usage pattern:\n </p>\n <pre><span class=\"qk pa gt op b bf ql qm l qn qo\" id=\"d8ed\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> Document\n<span class=\"hljs-keyword\">from</span> llama_index.embeddings <span class=\"hljs-keyword\">import</span> OpenAIEmbedding\n<span class=\"hljs-keyword\">from</span> llama_index.text_splitter <span class=\"hljs-keyword\">import</span> SentenceSplitter\n<span class=\"hljs-keyword\">from</span> llama_index.extractors <span class=\"hljs-keyword\">import</span> TitleExtractor\n<span class=\"hljs-keyword\">from</span> llama_index.ingestion <span class=\"hljs-keyword\">import</span> IngestionPipeline, IngestionCache\n\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=<span class=\"hljs-number\">25</span>, chunk_overlap=<span class=\"hljs-number\">0</span>),\n        TitleExtractor(),\n        OpenAIEmbedding(),\n    ]\n)\nnodes = pipeline.run(documents=[Document.example()])</span></pre>\n <h1>\n  Transformation Caching\n </h1>\n <p>\n  Each time you run the same\n  <code class=\"cw om on oo op b\">\n   IngestionPipeline\n  </code>\n  object, it caches a hash of the input nodes + transformations and the output of that transformation for each transformation in the pipeline.\n </p>\n <p>\n  In subsequent runs, if there is a cache hit, that transformation will be skipped and the cached result will be used instead. The greatly speeds up duplicate runs, and can help improve iteration times when deciding which transformations to use.\n </p>\n <p>\n  Here\u2019s an example with a saving and loading a local cache:\n </p>\n <pre><span class=\"qk pa gt op b bf ql qm l qn qo\" id=\"1882\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> Document\n<span class=\"hljs-keyword\">from</span> llama_index.embeddings <span class=\"hljs-keyword\">import</span> OpenAIEmbedding\n<span class=\"hljs-keyword\">from</span> llama_index.text_splitter <span class=\"hljs-keyword\">import</span> SentenceSplitter\n<span class=\"hljs-keyword\">from</span> llama_index.extractors <span class=\"hljs-keyword\">import</span> TitleExtractor\n<span class=\"hljs-keyword\">from</span> llama_index.ingestion <span class=\"hljs-keyword\">import</span> IngestionPipeline, IngestionCache\n\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=<span class=\"hljs-number\">25</span>, chunk_overlap=<span class=\"hljs-number\">0</span>),\n        TitleExtractor(),\n        OpenAIEmbedding(),\n    ]\n)\n<span class=\"hljs-comment\"># will only execute full pipeline once</span>\nnodes = pipeline.run(documents=[Document.example()])\nnodes = pipeline.run(documents=[Document.example()])\n<span class=\"hljs-comment\"># save and load</span>\npipeline.cache.persist(<span class=\"hljs-string\">\"./test_cache.json\"</span>)\nnew_cache = IngestionCache.from_persist_path(<span class=\"hljs-string\">\"./test_cache.json\"</span>)\nnew_pipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=<span class=\"hljs-number\">25</span>, chunk_overlap=<span class=\"hljs-number\">0</span>),\n        TitleExtractor(),\n    ],\n    cache=new_cache,\n)\n<span class=\"hljs-comment\"># will run instantly due to the cache</span>\nnodes = pipeline.run(documents=[Document.example()])</span></pre>\n <p>\n  And here\u2019s another example using Redis as a cache and Qdrant as a vector store. Running this will directly insert the nodes into your vector store and cache each transformation step in Redis.\n </p>\n <pre><span class=\"qk pa gt op b bf ql qm l qn qo\" id=\"14d4\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> Document\n<span class=\"hljs-keyword\">from</span> llama_index.embeddings <span class=\"hljs-keyword\">import</span> OpenAIEmbedding\n<span class=\"hljs-keyword\">from</span> llama_index.text_splitter <span class=\"hljs-keyword\">import</span> SentenceSplitter\n<span class=\"hljs-keyword\">from</span> llama_index.extractors <span class=\"hljs-keyword\">import</span> TitleExtractor\n<span class=\"hljs-keyword\">from</span> llama_index.ingestion <span class=\"hljs-keyword\">import</span> IngestionPipeline, IngestionCache\n<span class=\"hljs-keyword\">from</span> llama_index.ingestion.cache <span class=\"hljs-keyword\">import</span> RedisCache\n<span class=\"hljs-keyword\">from</span> llama_index.vector_stores.qdrant <span class=\"hljs-keyword\">import</span> QdrantVectorStore\n\n<span class=\"hljs-keyword\">import</span> qdrant_client\nclient = qdrant_client.QdrantClient(location=<span class=\"hljs-string\">\":memory:\"</span>)\nvector_store = QdrantVectorStore(client=client, collection_name=<span class=\"hljs-string\">\"test_store\"</span>)\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=<span class=\"hljs-number\">25</span>, chunk_overlap=<span class=\"hljs-number\">0</span>),\n        TitleExtractor(),\n        OpenAIEmbedding(),\n    ],\n    cache=IngestionCache(cache=RedisCache(), collection=<span class=\"hljs-string\">\"test_cache\"</span>),\n    vector_store=vector_store,\n)\n<span class=\"hljs-comment\"># Ingest directly into a vector db</span>\npipeline.run(documents=[Document.example()])\n<span class=\"hljs-comment\"># Create your index</span>\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> VectorStoreIndex\nindex = VectorStoreIndex.from_vector_store(vector_store)</span></pre>\n <h1>\n  Custom Transformations\n </h1>\n <p>\n  Implementing custom transformations is easy! Let\u2019s add a transform to remove special characters from the text before calling embeddings.\n </p>\n <p>\n  The only real requirement for transformations is that they must accept a list of nodes and return a list of nodes.\n </p>\n <pre><span class=\"qk pa gt op b bf ql qm l qn qo\" id=\"b3ca\"><span class=\"hljs-keyword\">import</span> re\n<span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> Document\n<span class=\"hljs-keyword\">from</span> llama_index.embeddings <span class=\"hljs-keyword\">import</span> OpenAIEmbedding\n<span class=\"hljs-keyword\">from</span> llama_index.text_splitter <span class=\"hljs-keyword\">import</span> SentenceSplitter\n<span class=\"hljs-keyword\">from</span> llama_index.ingestion <span class=\"hljs-keyword\">import</span> IngestionPipeline\n<span class=\"hljs-keyword\">from</span> llama_index.schema <span class=\"hljs-keyword\">import</span> TransformComponent\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">TextCleaner</span>(<span class=\"hljs-title class_ inherited__\">TransformComponent</span>):\n  <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__call__</span>(<span class=\"hljs-params\">self, nodes, **kwargs</span>):\n    <span class=\"hljs-keyword\">for</span> node <span class=\"hljs-keyword\">in</span> nodes:\n      node.text = re.sub(<span class=\"hljs-string\">r'[^0-9A-Za-z ]'</span>, <span class=\"hljs-string\">\"\"</span>, node.text)\n    <span class=\"hljs-keyword\">return</span> nodes\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=<span class=\"hljs-number\">25</span>, chunk_overlap=<span class=\"hljs-number\">0</span>),\n        TextCleaner(),\n        OpenAIEmbedding(),\n    ],\n)\nnodes = pipeline.run(documents=[Document.example()])</span></pre>\n <h1>\n  Node Parsing/Text Splitting \u2014 Flattened and Simplified Interface\n </h1>\n <p>\n  We\u2019ve made our interface for parsing and splitting text a lot cleaner.\n </p>\n <h1>\n  Before:\n </h1>\n <pre><span class=\"qk pa gt op b bf ql qm l qn qo\" id=\"c330\"><span class=\"hljs-keyword\">from</span> llama_index.node_parser <span class=\"hljs-keyword\">import</span> SimpleNodeParser\n<span class=\"hljs-keyword\">from</span> llama_index.node_parser.extractors <span class=\"hljs-keyword\">import</span> (\n\tMetadataExtractor, TitleExtractor\n) \n<span class=\"hljs-keyword\">from</span> llama_index.text_splitter <span class=\"hljs-keyword\">import</span> SentenceSplitter\n\nnode_parser = SimpleNodeParser(\n  text_splitter=SentenceSplitter(chunk_size=<span class=\"hljs-number\">512</span>),\n  metadata_extractor=MetadataExtractor(\n  extractors=[TitleExtractor()]\n ),\n)\nnodes = node_parser.get_nodes_from_documents(documents)</span></pre>\n <h1>\n  After:\n </h1>\n <pre><span class=\"qk pa gt op b bf ql qm l qn qo\" id=\"5692\"><span class=\"hljs-keyword\">from</span> llama_index.text_splitter <span class=\"hljs-keyword\">import</span> SentenceSplitter\n<span class=\"hljs-keyword\">from</span> llama_index.extractors <span class=\"hljs-keyword\">import</span> TitleExtractor \n\nnode_parser = SentenceSplitter(chunk_size=<span class=\"hljs-number\">512</span>)\nextractor = TitleExtractor()\n\n<span class=\"hljs-comment\"># use transforms directly</span>\nnodes = node_parser(documents)\nnodes = extractor(nodes)</span></pre>\n <p>\n  Previously, the\n  <code class=\"cw om on oo op b\">\n   NodeParser\n  </code>\n  object in LlamaIndex had become extremely bloated, holding both text splitters and metadata extractors, which caused both pains for users when changing these components, and pains for us trying to maintain and develop them.\n </p>\n <p>\n  In V0.9, we have\n  <strong>\n   flattened\n  </strong>\n  the entire interface into a single\n  <code class=\"cw om on oo op b\">\n   TransformComponent\n  </code>\n  abstraction, so that these transformations are easier to setup, use, and customize.\n </p>\n <p>\n  We\u2019ve done our best to minimize the impacts on users, but the main thing to note is that\n  <code class=\"cw om on oo op b\">\n   <strong>\n    SimpleNodeParser\n   </strong>\n  </code>\n  <strong>\n   has been removed\n  </strong>\n  , and other node parsers and text splitters have been elevated to have the same features, just with different parsing and splitting techniques.\n </p>\n <p>\n  Any old imports of\n  <code class=\"cw om on oo op b\">\n   SimpleNodeParser\n  </code>\n  will redirect to the most equivalent module,\n  <code class=\"cw om on oo op b\">\n   SentenceSplitter\n  </code>\n  .\n </p>\n <p>\n  Furthermore, the wrapper object\n  <code class=\"cw om on oo op b\">\n   <strong>\n    MetadataExtractor\n   </strong>\n  </code>\n  <strong>\n   has been removed\n  </strong>\n  , in favour of using extractors directly.\n </p>\n <p>\n  Full documentation for all this can be found below:\n </p>\n <ul>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Node Parsers and Text Splitters\n   </a>\n  </li>\n  <li>\n   <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/indexing/metadata_extraction.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n    Metadata Extractors\n   </a>\n  </li>\n </ul>\n <h1>\n  Tokenization and Token Counting \u2014 Improved defaults and Customization\n </h1>\n <p>\n  A big pain point in LlamaIndex previously was tokenization. Many components used a non-configurable\n  <code class=\"cw om on oo op b\">\n   gpt2\n  </code>\n  tokenizer for token counting, causing headaches for users using non-OpenAI models, or even some hacky fixes\n  <a href=\"https://github.com/run-llama/llama_index/blob/336a88db4f13cfc598c473f9b5a3bc073b5d7ef4/llama_index/indices/prompt_helper.py#L119\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   like this\n  </a>\n  for OpenAI models too!\n </p>\n <p>\n  In LlamaIndex V0.9, this\n  <strong>\n   global tokenizer is now configurable and defaults to the CL100K tokenizer\n  </strong>\n  to match our default GPT-3.5 LLM.\n </p>\n <p>\n  The single requirement for a tokenizer is that it is a callable function, that takes a string, and returns a list.\n </p>\n <p>\n  Some examples of configuring this are below:\n </p>\n <pre><span class=\"qk pa gt op b bf ql qm l qn qo\" id=\"5d70\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> set_global_tokenizer\n\n<span class=\"hljs-comment\"># tiktoken</span>\n<span class=\"hljs-keyword\">import</span> tiktoken\nset_global_tokenizer(\n  tiktoken.encoding_for_model(<span class=\"hljs-string\">\"gpt-3.5-turbo\"</span>).encode\n)\n<span class=\"hljs-comment\"># huggingface</span>\n<span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer\nset_global_tokenizer(\n  AutoTokenizer.from_pretrained(<span class=\"hljs-string\">\"HuggingFaceH4/zephyr-7b-beta\"</span>).encode\n)</span></pre>\n <p>\n  Furthermore, the\n  <code class=\"cw om on oo op b\">\n   TokenCountingHandler\n  </code>\n  has gotten an upgrade with better token counting, as well as using token counts from API responses directly when available.\n </p>\n <h1>\n  Packaging \u2014 Reduced Bloat\n </h1>\n <p>\n  In an effort to modernize the packaging of LlamaIndex, V0.9 also comes with changes to installation.\n </p>\n <p>\n  The biggest change here is that\n  <code class=\"cw om on oo op b\">\n   LangChain\n  </code>\n  is now an optional package, and will not be installed by default.\n </p>\n <p>\n  To install\n  <code class=\"cw om on oo op b\">\n   LangChain\n  </code>\n  as part of your llama-index installation you can follow the example below. There are also other installation options depending on your needs, and we are welcoming further contributions to the extras in the future.\n </p>\n <pre><span class=\"qk pa gt op b bf ql qm l qn qo\" id=\"09ee\"># installs langchain\npip install llama-index[langchain]\n \n# installs tools needed for running local models\npip install llama-index[local_models]\n\n# installs tools needed for postgres\npip install llama-index[postgres]\n\n# combinations!\npip isntall llama-index[local_models,postgres]</span></pre>\n <p>\n  <strong>\n   If you were previously importing\n  </strong>\n  <code class=\"cw om on oo op b\">\n   <strong>\n    langchain\n   </strong>\n  </code>\n  <strong>\n   modules\n  </strong>\n  in your code, please update your project packaging requirements appropriately.\n </p>\n <h1>\n  Import Paths \u2014 More Consistent and Predictable\n </h1>\n <p>\n  We are making two changes to our import paths:\n </p>\n <ol>\n  <li>\n   We\u2019ve removed uncommonly used imports from the root level to make importing\n   <code class=\"cw om on oo op b\">\n    llama_index\n   </code>\n   faster\n  </li>\n  <li>\n   We now have a consistent policy for making \u201cuser-facing\u201d concepts import-able at level-1 modules.\n  </li>\n </ol>\n <pre><span class=\"qk pa gt op b bf ql qm l qn qo\" id=\"13d2\"><span class=\"hljs-keyword\">from</span> llama_index.llms <span class=\"hljs-keyword\">import</span> OpenAI, ...\n<span class=\"hljs-keyword\">from</span> llama_index.embeddings <span class=\"hljs-keyword\">import</span> OpenAIEmbedding, ...\n<span class=\"hljs-keyword\">from</span> llama_index.prompts <span class=\"hljs-keyword\">import</span> PromptTemplate, ...\n<span class=\"hljs-keyword\">from</span> llama_index.readers <span class=\"hljs-keyword\">import</span> SimpleDirectoryReader, ...\n<span class=\"hljs-keyword\">from</span> llama_index.text_splitter <span class=\"hljs-keyword\">import</span> SentenceSplitter, ...\n<span class=\"hljs-keyword\">from</span> llama_index.extractors <span class=\"hljs-keyword\">import</span> TitleExtractor, ...\n<span class=\"hljs-keyword\">from</span> llama_index.vector_stores <span class=\"hljs-keyword\">import</span> SimpleVectorStore, ...</span></pre>\n <p>\n  We still expose some of the most commonly used modules at the root level.\n </p>\n <pre><span class=\"qk pa gt op b bf ql qm l qn qo\" id=\"299d\"><span class=\"hljs-keyword\">from</span> llama_index <span class=\"hljs-keyword\">import</span> SimpleDirectoryReader, VectorStoreIndex, ...</span></pre>\n <h1>\n  MultiModal RAG\n </h1>\n <p>\n  Given the recent announcements of the GPT-4V API, multi-modal use cases are more accessible than ever before.\n </p>\n <p>\n  To help users use these features, we\u2019ve started to introduce a number of new modules to help support use-cases for MultiModal RAG:\n </p>\n <ul>\n  <li>\n   MultiModal LLMs (GPT-4V, Llava, Fuyu, etc.)\n  </li>\n  <li>\n   MultiModal Embeddings (i.e clip) for join image-text embedding/retrieval\n  </li>\n  <li>\n   MultiModal RAG, combining indexes and query engines\n  </li>\n </ul>\n <p>\n  Our documentation has a\n  <a href=\"https://docs.llamaindex.ai/en/latest/examples/multi_modal/gpt4v_multi_modal_retrieval.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   full guide to multi-modal retrieval\n  </a>\n  .\n </p>\n <h1>\n  Thanks for all your support!\n </h1>\n <p>\n  As an open-source project we couldn\u2019t exist without our\n  <a href=\"https://github.com/run-llama/llama_index/graphs/contributors\" rel=\"noopener ugc nofollow\" target=\"_blank\">\n   hundreds of contributors\n  </a>\n  . We are so grateful for them and the support of the hundreds of thousands of LlamaIndex users around the world. See you on the Discord!\n </p>\n</div>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 18952, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2f0e0e9d-3ff2-4a97-9db7-788051eec855": {"__data__": {"id_": "2f0e0e9d-3ff2-4a97-9db7-788051eec855", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.html", "file_name": "a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.html", "file_type": "text/html", "file_size": 24708, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "07a29a81-1d9f-445e-b1a3-cec315ffcd79", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses RAG (Recall, Accuracy, and Generality) evaluation for natural language generation (NLG) systems. It mentions a survey paper by Gao et al. That outlines seven measurement aspects for RAG evaluation. The text also introduces the llama-index library, which offers evaluation abstractions and integrations to help builders assess their NLG system's success in meeting the success requirements through these measurement aspects. The text provides links to several evaluation notebook guides, including answer and context relevancy, faithfulness, and retrieval evaluation. This text can answer questions regarding how to measure answer and context relevancy, faithfulness, and retrieval performance in NLG systems. Some potential questions that this text could answer include:\n\n- What are the seven measurement aspects for RAG evaluation outlined in the survey paper by Gao et al.?\n- How can builders assess their NLG system's success in meeting the success requirements through these measurement aspects using the llama-index library?\n- What evaluation notebook guides are provided in the text for measuring answer and context relevancy, faithfulness, and retrieval performance in NLG systems?\n- How can these measurement aspects help builders evaluate the performance of their NLG system in terms of answer and context relevancy, faithfulness, and retrieval?\n- How can these measurement aspects be integrated into the overall design and development process of NLG systems?\n- How can these measurement aspects be used to improve the performance and accuracy of NLG systems in real-world scenarios?\n- How can these measurement aspects be used to address specific evaluation challenges or concerns in NLG systems?\n- How can these measurement aspects be integrated into the overall evaluation and testing process of NLG systems?\n- How can these measurement aspects be used to compare and benchmark the performance of different NLG systems?\n- How can these measurement aspects be used to improve the generality and applicability of NLG systems in various contexts and domains?\n- How can these measurement aspects be used to ensure the overall quality and effectiveness of NLG systems in terms of accuracy, relevance, and generality?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ff5998ad-43af-4a67-89fa-8668f4d1950a": {"__data__": {"id_": "ff5998ad-43af-4a67-89fa-8668f4d1950a", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.html", "file_name": "a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.html", "file_type": "text/html", "file_size": 11715, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "55bbca88-eaf3-4d38-b759-898354717207", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses the introduction of a new indexing structure, called the Document Summary Index, for use with Large Language Models (LLMs) in question-answering systems. This indexing structure involves extracting and indexing a summary from each document, as well as splitting the document into text chunks, which are then stored in a vector database. During query time, relevant documents are retrieved based on their summaries, using either LLM-based retrieval or embedding-based retrieval. This approach offers improved retrieval performance compared to traditional semantic search, as it provides more context than text chunks alone and allows for a more flexible form of retrieval. The text also provides examples of how to use this indexing structure with the LlamaIndex library. Some questions that this text can answer include: what is the Document Summary Index and how is it used with LLMs? How does this indexing structure improve retrieval performance compared to traditional semantic search? How is this indexing structure implemented using the LlamaIndex library?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ad7676fd-722c-4833-b908-1ee56127f081": {"__data__": {"id_": "ad7676fd-722c-4833-b908-1ee56127f081", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/agentic-rag-with-llamaindex-2721b8a49ff6.html", "file_name": "agentic-rag-with-llamaindex-2721b8a49ff6.html", "file_type": "text/html", "file_size": 7713, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0b5088c8-9467-4361-a295-76d163336b51", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is about Agentic RAG, a specific implementation of RAG (Retrieval-as-a-Service) using agent technology. It explains how Agentic RAG adds intelligence and resilience to RAG implementations by incorporating agents into existing pipelines for enhanced, conversational search and retrieval. The text provides an example of this implementation from LlamaIndex and illustrates multi-agent orchestration, the benefits of a second tier of smaller worker-agents, and a controlled and well-defined autonomous agent implementation. Some questions that this text can answer include: what is Agentic RAG, how does it differ from traditional RAG implementations, how does it incorporate agents into existing pipelines, and what are some benefits of this implementation, such as multi-agent orchestration and a second tier of smaller worker-agents. The text also provides an example of how Agentic RAG can be used to answer specific questions, such as the location of LlamaIndex connectors in the documentation.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d7959866-c8b5-4c9b-a977-efce26fc05e9": {"__data__": {"id_": "d7959866-c8b5-4c9b-a977-efce26fc05e9", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.html", "file_name": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.html", "file_type": "text/html", "file_size": 18536, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "37bc1481-63f1-4203-b395-0fb229570adf", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is an introduction to a voice-activated AI voice assistant called C3, which aims to enhance accessibility for users facing typing challenges or accessibility issues. The assistant is powered by Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) applications and is designed to be user-friendly and accessible, with features such as voice activation, user-friendly interface, and LLM integration for general and document-specific queries. Some questions that this text can answer include how the assistant caters to users with typing challenges or accessibility issues, what technologies power the assistant, how it is designed for simplicity and ease of use, how it is built using components like React.js, Python Flask, and hosting services like Vercel and Render, how to set up the backend server using Create-Llama and integrate it with the frontend, and how to use the C3 Voice Assistant to interact with AI tools like LLMs and RAG. It also discusses the benefits of making AI tools more accessible to everyone and encourages feedback and collaboration.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "94c96755-d3a8-433e-9f79-57552112af3b": {"__data__": {"id_": "94c96755-d3a8-433e-9f79-57552112af3b", "embedding": null, "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/announcing-llamaindex-0-9-719f03282945.html", "file_name": "announcing-llamaindex-0-9-719f03282945.html", "file_type": "text/html", "file_size": 18985, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9a5b129a-cf71-41a1-8f62-d32fbb2f5bc2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text is an announcement regarding the release of a new version, 0.9, of the LlamaIndex library. Some of the improvements included in this release are updates to the node parsing and metadata extraction features, as well as improvements to tokenization and token counting. The text also introduces MultiModal RAG for multi-modal use cases with various LLMs and embeddings. The text mentions the contributions of hundreds of users and encourages further involvement in the Discord community. Some questions that this text can answer include: what specific updates have been made to node parsing and metadata extraction, and why are these changes significant? What is the new tokenization feature, and how can it be configured? How can MultiModal RAG be utilized with various LLMs and embeddings, and what are these technologies? Who have contributed to the project, and how can interested individuals join the Discord community?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}}, "docstore/ref_doc_info": {"07a29a81-1d9f-445e-b1a3-cec315ffcd79": {"node_ids": ["41d5b086-ffb5-4c55-b101-b6207854a6d1", "2f0e0e9d-3ff2-4a97-9db7-788051eec855"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.html", "file_name": "a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.html", "file_type": "text/html", "file_size": 24708, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "55bbca88-eaf3-4d38-b759-898354717207": {"node_ids": ["5bd609fa-f1e8-4c52-a6ea-79fdb47a51f3", "ff5998ad-43af-4a67-89fa-8668f4d1950a"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.html", "file_name": "a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.html", "file_type": "text/html", "file_size": 11715, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "0b5088c8-9467-4361-a295-76d163336b51": {"node_ids": ["2e44e215-8834-4311-bf81-88c02de64919", "ad7676fd-722c-4833-b908-1ee56127f081"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/agentic-rag-with-llamaindex-2721b8a49ff6.html", "file_name": "agentic-rag-with-llamaindex-2721b8a49ff6.html", "file_type": "text/html", "file_size": 7713, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "37bc1481-63f1-4203-b395-0fb229570adf": {"node_ids": ["8b5dfa16-fc80-4788-b048-8a9102dabe98", "d7959866-c8b5-4c9b-a977-efce26fc05e9"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.html", "file_name": "ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.html", "file_type": "text/html", "file_size": 18536, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}, "9a5b129a-cf71-41a1-8f62-d32fbb2f5bc2": {"node_ids": ["bd717d20-9a29-4785-b703-84ecbe3ec43b", "94c96755-d3a8-433e-9f79-57552112af3b"], "metadata": {"file_path": "/workspace/projects/LlamindexHelper/data/announcing-llamaindex-0-9-719f03282945.html", "file_name": "announcing-llamaindex-0-9-719f03282945.html", "file_type": "text/html", "file_size": 18985, "creation_date": "2024-07-21", "last_modified_date": "2024-07-21"}}}}