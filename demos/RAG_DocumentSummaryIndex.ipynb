{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efebc64b-1515-4657-8a40-af54d61081e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "693de2d9-4449-4e25-b53d-93b0730b22e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2a4f399-0e6a-4467-b518-522dffd43113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca53a383-1eaa-4687-8c92-a19fccd0d5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/dev/lib/python3.10/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.3.0+cu118 available.\n",
      "PyTorch version 2.3.0+cu118 available.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, get_response_synthesizer\n",
    "from llama_index.core import DocumentSummaryIndex, StorageContext, Settings\n",
    "from llama_index.core import load_index_from_storage\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.response.notebook_utils import display_response, display_metadata\n",
    "import chromadb\n",
    "import model_utils\n",
    "import prompt_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1654d9b9-7299-4638-a664-4b2b1cf83f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: models/bge-small-en-v1.5\n",
      "Load pretrained SentenceTransformer: models/bge-small-en-v1.5\n",
      "INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2 prompts are loaded, with the keys: ['query', 'text']\n"
     ]
    }
   ],
   "source": [
    "# load embeddings\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"models/bge-small-en-v1.5\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95245485-0215-4d0f-87cf-24e893108a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model with quantization config from: models/Meta-Llama-3-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b4bba5c3264b3dbb89c17f1b5abf35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load model\n",
    "model_name = \"models/Meta-Llama-3-8B-Instruct\"\n",
    "model, tokenizer = model_utils.load_quantized_model(\n",
    "    model_name_or_path=model_name,\n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8920030-422d-419e-9e6a-8d8648a239a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config llm and embed_model to llamaindex\n",
    "llm_hf = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=512,\n",
    "    query_wrapper_prompt=PromptTemplate(prompt_utils.get_llama3_prompt_template()),\n",
    "    generate_kwargs={\n",
    "        \"temperature\": 0.7,\n",
    "        \"do_sample\": True\n",
    "    },\n",
    "    device_map=\"cuda\",\n",
    "    model_name=model_name,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "Settings.llm = llm_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e630ac8a-a947-45fa-9822-220d3f071189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings.embed_model = embed_model\n",
    "# Settings.llm = llm_hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072574f1-fc29-4d4b-98e2-0ca4a1e32e31",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17ebacb5-d137-49e5-a9da-e13caa9fb4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    input_dir=\"./data\",\n",
    "    filename_as_id=True,\n",
    ").load_data()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65b9471c-3794-4596-8d43-183e1ce96b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "# Creates a persistent instance of Chroma that saves to disk\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# Get or create a collection with the given name and metadata.\n",
    "vector_collection = chroma_client.get_or_create_collection(\"blogs_summary\")\n",
    "vector_store = ChromaVectorStore(\n",
    "    chroma_collection=vector_collection, \n",
    "    persist_dir=\"./chroma_db/blogs_summary\"\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "092bb2ce-547a-4308-bb6d-7be34d5b0463",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = SentenceSplitter(\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_size=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25aac4fa-1230-4b28-948f-d806d0014926",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesizer = get_response_synthesizer(\n",
    "    llm=llm_hf,\n",
    "    response_mode=\"tree_summarize\", \n",
    "    use_async=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85284a0-476e-456e-b118-000c9e3e5bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test only 3 documents\n",
    "# doc_summary_index = DocumentSummaryIndex.from_documents(\n",
    "#     documents=documents[:3],\n",
    "#     llm=llm_hf,\n",
    "#     embed_model=embed_model,\n",
    "#     transformations=[splitter],\n",
    "#     response_synthesizer=response_synthesizer,\n",
    "#     show_progress=True,\n",
    "#     storage_context=storage_context\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3db4af8-cce6-495d-a00b-50999d51e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_summary_index.storage_context.persist(\"summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94360892-fcf3-48fd-bda1-dba95b3df839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "# doc_summary_index.get_document_summary(\"07a29a81-1d9f-445e-b1a3-cec315ffcd79\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb496cd-54e3-4190-ae71-5f27722aeedd",
   "metadata": {},
   "source": [
    "## Load doc sumary index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be82ce7e-dd93-4ec3-aa1f-a849eb84c317",
   "metadata": {},
   "source": [
    "### Load by chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "001d3030-7dfd-422e-99a9-a5d9f1770c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Collection(id=e2bc79ee-49dd-45e4-85f3-6acb87185f7a, name=blogs_summary), 159)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "test_vector_collection = chroma_client.get_or_create_collection(\"blogs_summary\")\n",
    "test_vector_collection, test_vector_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d324f39-4bf2-478c-93dc-8cfd23e3ab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_vector_store = ChromaVectorStore(\n",
    "    chroma_collection=test_vector_collection,\n",
    "    # persist_dir=\"./backup/blogs_summary\"\n",
    ")\n",
    "chroma_storage_context = StorageContext.from_defaults(\n",
    "    vector_store=chroma_vector_store, \n",
    "    persist_dir=\"./database/blogs_summary_index/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5ddb446-5cb6-474e-ac05-0851f591303d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_struct = chroma_storage_context.index_store.get_index_struct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d38ba0e7-727e-4408-8e8c-d98005318dd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/projects/LlamindexHelper/data/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.html\n",
      "83001b14-fb2d-400e-b31b-3a48501408ce\n",
      "/workspace/projects/LlamindexHelper/data/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec.html\n",
      "90d0dad3-83e4-48ff-969a-41a450639f14\n",
      "/workspace/projects/LlamindexHelper/data/agentic-rag-with-llamaindex-2721b8a49ff6.html\n",
      "8459bd37-a59b-4eb1-b346-49c451ffd64b\n",
      "/workspace/projects/LlamindexHelper/data/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a.html\n",
      "78e2927e-d1d5-46d0-9f7d-a83732e7369b\n",
      "/workspace/projects/LlamindexHelper/data/announcing-llamaindex-0-9-719f03282945.html\n",
      "77ba3a80-549e-47e9-bbf3-f70fbaa39686\n",
      "/workspace/projects/LlamindexHelper/data/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications.html\n",
      "c07eb866-c51d-4e7a-b9af-1df89655b2d6\n",
      "/workspace/projects/LlamindexHelper/data/automate-online-tasks-with-multion-and-llamaindex.html\n",
      "6c94c9c1-8eaa-45d4-befa-54e84f2ecebe\n",
      "/workspace/projects/LlamindexHelper/data/batch-inference-with-mymagic-ai-and-llamaindex.html\n",
      "f32cd777-600a-4a8e-91f1-c5340cfd85ea\n",
      "/workspace/projects/LlamindexHelper/data/becoming-proficient-in-document-extraction-32aa13046ed5.html\n",
      "51cf26f6-53c6-47e1-8ff7-3b845c7744cc\n",
      "/workspace/projects/LlamindexHelper/data/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83.html\n",
      "2e1d8446-49c6-4183-ad1a-6f5dfc6bc6da\n",
      "/workspace/projects/LlamindexHelper/data/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3.html\n",
      "71360fb6-350f-4905-aef8-19f79ee01ff5\n",
      "/workspace/projects/LlamindexHelper/data/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8.html\n",
      "d1003737-974b-4885-96e1-a46fd31a3e22\n",
      "/workspace/projects/LlamindexHelper/data/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c.html\n",
      "4d6dc69f-9dae-4907-a8c0-14331d223732\n",
      "/workspace/projects/LlamindexHelper/data/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c.html\n",
      "e5fb4319-54c6-4233-91cc-2367005a7202\n",
      "/workspace/projects/LlamindexHelper/data/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4.html\n",
      "cf82d9b0-25ef-4a65-959c-bc0f5589ced8\n",
      "/workspace/projects/LlamindexHelper/data/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4.html\n",
      "288844f4-181a-4fb5-813b-81b6f7fe1b2f\n",
      "/workspace/projects/LlamindexHelper/data/building-a-multi-agent-concierge-system.html\n",
      "7c22a9a7-21f8-4e51-9b36-542b2a213711\n",
      "/workspace/projects/LlamindexHelper/data/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840.html\n",
      "650a5a60-f82f-432e-9069-94ced09b1840\n",
      "/workspace/projects/LlamindexHelper/data/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf.html\n",
      "5761f8d9-2484-4561-80f6-fb3eb3b80d7c\n",
      "/workspace/projects/LlamindexHelper/data/building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1.html\n",
      "1f12b488-aa4a-44fe-8351-c959e234c53e\n",
      "/workspace/projects/LlamindexHelper/data/building-better-tools-for-llm-agents-f8c5a6714f11.html\n",
      "7f9f3383-657e-4718-868e-2beea6e45e25\n",
      "/workspace/projects/LlamindexHelper/data/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b.html\n",
      "46aef392-a42f-4159-846f-713d933c31d9\n",
      "/workspace/projects/LlamindexHelper/data/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566.html\n",
      "efdb4bc2-a61c-4fa3-9b1f-3ee7a9fd6f02\n",
      "/workspace/projects/LlamindexHelper/data/building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf.html\n",
      "29dde6e0-b00a-4ec8-8fc9-91c1a11cced8\n",
      "/workspace/projects/LlamindexHelper/data/building-the-data-framework-for-llms-bca068e89e0e.html\n",
      "fc5bc645-280c-44b1-8f55-a923ad5a53b4\n",
      "/workspace/projects/LlamindexHelper/data/case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud.html\n",
      "bc255221-4c45-42cd-9448-37ce4bf85975\n",
      "/workspace/projects/LlamindexHelper/data/case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex.html\n",
      "ec2945bc-f663-4011-96cb-9f322642710a\n",
      "/workspace/projects/LlamindexHelper/data/chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c.html\n",
      "ad5e5614-fd2d-40b8-b5fd-77e2351f5c1c\n",
      "/workspace/projects/LlamindexHelper/data/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b.html\n",
      "13840a23-c7d3-466a-a5ef-3ca13ad3c289\n",
      "/workspace/projects/LlamindexHelper/data/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191.html\n",
      "4ac9b548-a6ce-44f0-a298-8b1bc6870b58\n",
      "/workspace/projects/LlamindexHelper/data/customizing-property-graph-index-in-llamaindex.html\n",
      "6f9d733b-3654-42e7-8055-4a5b1cc7929b\n",
      "/workspace/projects/LlamindexHelper/data/data-agents-eed797d7972f.html\n",
      "c0c3b479-4e27-488f-96bb-2f7958531f88\n",
      "/workspace/projects/LlamindexHelper/data/data-agents-zapier-nla-67146395ce1.html\n",
      "b89fc93f-8fe1-455b-b2e2-3f039fc4c821\n",
      "/workspace/projects/LlamindexHelper/data/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12.html\n",
      "dc3aae11-61f2-45bc-97a1-6485c18dcfbe\n",
      "/workspace/projects/LlamindexHelper/data/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d.html\n",
      "8d53a5a5-a61b-48d1-9726-c37659c594b0\n",
      "/workspace/projects/LlamindexHelper/data/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7.html\n",
      "3a4b10b9-3792-4e0b-9b47-a409b6735b7f\n",
      "/workspace/projects/LlamindexHelper/data/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428.html\n",
      "c239bd6c-3955-4c0a-a528-3ec9f4e865bf\n",
      "/workspace/projects/LlamindexHelper/data/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5.html\n",
      "5c6a1117-be96-4783-aa30-8d8d0f1f0fc7\n",
      "/workspace/projects/LlamindexHelper/data/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383.html\n",
      "492d2827-5f2e-4694-b90e-02beba3aab66\n",
      "/workspace/projects/LlamindexHelper/data/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971.html\n",
      "5c818674-12f6-4454-98fd-f5ca4e575a4a\n",
      "/workspace/projects/LlamindexHelper/data/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9.html\n",
      "e24625a9-19a9-4658-aead-0413b1fa8d86\n",
      "/workspace/projects/LlamindexHelper/data/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0.html\n",
      "b12f7641-553c-46e2-b38a-96edbfdf383e\n",
      "/workspace/projects/LlamindexHelper/data/how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa.html\n",
      "4d400bfc-be64-4be7-91c4-cee2a8590dfc\n",
      "/workspace/projects/LlamindexHelper/data/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070.html\n",
      "a0e32d09-7d5e-4924-a122-f574b006ce43\n",
      "/workspace/projects/LlamindexHelper/data/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d.html\n",
      "3f3c8fa1-e2ab-4f59-84db-9579abf12516\n",
      "/workspace/projects/LlamindexHelper/data/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b.html\n",
      "076ff5b2-eeac-4b00-a2d3-a69736af563f\n",
      "/workspace/projects/LlamindexHelper/data/improving-vector-search-reranking-with-postgresml-and-llamaindex.html\n",
      "7182dfc4-59d1-40bb-a441-0eaae03678b7\n",
      "/workspace/projects/LlamindexHelper/data/introducing-airbyte-sources-within-llamaindex-42209071722f.html\n",
      "e6c2f380-dae5-46d1-b859-51ecf810ab63\n",
      "/workspace/projects/LlamindexHelper/data/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html\n",
      "60ff6c04-bd0a-4e31-926c-93f341377cab\n",
      "/workspace/projects/LlamindexHelper/data/introducing-llama-datasets-aadb9994ad9e.html\n",
      "abfec326-12d7-4050-8535-fb19b5e8d1ec\n",
      "/workspace/projects/LlamindexHelper/data/introducing-llama-packs-e14f453b913a.html\n",
      "123f9c77-f764-495f-b50a-463c23d9ef6d\n",
      "/workspace/projects/LlamindexHelper/data/introducing-llamacloud-and-llamaparse-af8cedf9006b.html\n",
      "1d700344-f592-4c7e-ad65-58c9a2b23dae\n",
      "/workspace/projects/LlamindexHelper/data/introducing-llamaindex-ts-89f41a1f24ab.html\n",
      "93df4a84-3e21-488e-be5c-574732ab057f\n",
      "/workspace/projects/LlamindexHelper/data/introducing-query-pipelines-025dc2bb0537.html\n",
      "23c0c342-b5ff-4097-9372-42a689349c3d\n",
      "/workspace/projects/LlamindexHelper/data/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1.html\n",
      "f6dc427b-b915-4647-a282-6d68bc6fc466\n",
      "/workspace/projects/LlamindexHelper/data/introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41.html\n",
      "3f8bbe65-f50d-4736-91b8-f2c2d95f664c\n",
      "/workspace/projects/LlamindexHelper/data/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms.html\n",
      "b34bddd8-397b-49b8-aee0-35fa2312c3c7\n",
      "/workspace/projects/LlamindexHelper/data/join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb.html\n",
      "88fb8b71-9417-463e-9594-a800a6bd7002\n",
      "/workspace/projects/LlamindexHelper/data/launching-the-first-genai-native-document-parsing-platform.html\n",
      "edde25d6-2068-483d-9f6c-d1efc27c2289\n",
      "/workspace/projects/LlamindexHelper/data/llama-index-prem-ai-join-forces-51702fecedec.html\n",
      "0a331bec-d6d4-44e9-b437-bc33c4cbcdf0\n",
      "/workspace/projects/LlamindexHelper/data/llamacloud-built-for-enterprise-llm-app-builders.html\n",
      "01310167-4545-4da6-832e-2377043f67a9\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024.html\n",
      "871bde9f-8afd-44ee-9e8a-778424484ad6\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim.html\n",
      "0413acd1-7b0e-4b3d-bd40-f5b40cc74b56\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-and-transformers-agents-67042ee1d8d6.html\n",
      "1fc794d0-dd4b-4232-ac2d-bcfdd2f737c9\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-and-weaviate-ba3ff1cbf5f4.html\n",
      "1e80d500-411f-412e-8c7d-0ba3aabb3930\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af.html\n",
      "90128250-545b-4fcd-92b9-504e7d59be68\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00.html\n",
      "e2a7e039-d3fb-49dc-a063-62b9bcf32e48\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-gemini-8d7c3b9ea97e.html\n",
      "bd573d2c-d04b-4397-9d42-579528d43f12\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b.html\n",
      "f2a9fcbf-4e45-4a09-9280-2ab03dbd2b91\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff.html\n",
      "d0855239-4e1a-41e7-8feb-593b16c4512d\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f.html\n",
      "155d642c-6e3b-4c3b-9178-080677b69062\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-news-special-edition-openai-developer-day-e955f16db4e2.html\n",
      "b94ada67-94c1-4eb7-a4dd-6f203bb98d8c\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-02-13-26fa79601ba5.html\n",
      "49f1def6-1d5e-41b6-9a7a-8bcb23d9a250\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-17-33514cbc04a2.html\n",
      "db379d9e-d881-4f08-bb6a-bf7ea6c03076\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-24-4a76204eeaa3.html\n",
      "ae3fa659-adca-476a-a932-11719039fdc4\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-10-31-36244e2b3f0c.html\n",
      "8eb79a05-ff5d-4057-86bf-9f1bbd6bc8b3\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-07-cf20b9a833aa.html\n",
      "369ac48f-7909-45e3-b24d-7c27e92212db\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-14-dad06ae4284a.html\n",
      "91d0debf-63e5-4b72-b5de-658f6d9bbcdf\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-21-aa3a71e339f8.html\n",
      "e56ce492-07a5-4612-a6fe-5677c2b2ecf0\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-11-28-a31be430a786.html\n",
      "0f199c3b-e433-46ed-b4ec-67d697c45bb8\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-05-faf5ab930264.html\n",
      "ace7cd41-9c8e-45ae-a906-cbd7bdea2de3\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-12-4a5d542fbb1e.html\n",
      "393e9985-d181-4b5c-b17a-e2641a6d865d\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2023-12-19-2965a2d03726.html\n",
      "eec58ba9-7ab0-455a-9b62-b68a9faf700d\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-02-f349db8c1842.html\n",
      "f2b6eec6-c895-492e-8b70-45b0c98b4436\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-09-6209000da2e6.html\n",
      "ab81fa3d-da8f-42cf-8b30-fa58fab75ffb\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-16-752195bed96d.html\n",
      "b4f4106f-d3e0-42c8-97a3-3014f377c37e\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-23-11ee2c211bab.html\n",
      "5eabadc2-9f73-4400-81df-e9167e1c1595\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-01-30-0d01eb0d8cef.html\n",
      "6916b918-0b45-4117-bab9-1c322353fb32\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-06-9a303130ad9f.html\n",
      "df738da3-01fa-4c8f-accf-3057e7d28913\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4.html\n",
      "3bdedc09-2d20-4d3c-b0e7-5048dc2d3a23\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-02-27-4b9102a0f824.html\n",
      "a921b43b-bc2d-4860-9b27-e223b02236fe\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-05.html\n",
      "7a547101-75f8-4471-92bf-c1b454ce94fb\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-12.html\n",
      "57234cb7-d09a-4463-bbcc-fa30974cfa17\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-19.html\n",
      "6678a088-8c15-4de7-bc3a-1644f9402804\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-03-26.html\n",
      "8d67b4d2-3a1f-44eb-8833-57deaecf65db\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-02.html\n",
      "75952d41-8f05-41a6-9a17-8a13e46d162e\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-09.html\n",
      "3d1d146f-234d-4722-9a44-7cae1d22ebeb\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-16.html\n",
      "da7a008c-194b-4127-8013-c0b28d2ac838\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-23.html\n",
      "969e0723-8b7a-400b-85b0-14e539747a3f\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-04-30.html\n",
      "f10e29e1-4b5a-4907-b71e-704f75f98031\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-07.html\n",
      "c0a1109d-4e46-4423-b755-80a3bb4af515\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-14.html\n",
      "3f1ff09e-313f-4b07-9807-32dc5a2b9fc2\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-21.html\n",
      "b086fcf1-f08b-4afc-8097-9471a965c1ce\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-05-28.html\n",
      "097d486c-14e7-46cd-819f-029c8fdedf6f\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-04.html\n",
      "124f07cf-be0c-49f5-bd03-4d3dc22ff9eb\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-11.html\n",
      "afd66fa0-1373-44e1-af17-88f0aeb7d1c6\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-18.html\n",
      "aad60b80-c0fb-4533-a3d9-887157de0f0a\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-06-25.html\n",
      "c70e20ef-07ef-40b0-b508-debf9e34f01c\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-02.html\n",
      "07f91239-eab0-4b77-9d44-83bbfed20201\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-09.html\n",
      "87813008-0ec9-4f71-b345-d58f4909ecd6\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-newsletter-2024-07-16.html\n",
      "442318f5-bd6e-4b42-8c8d-905dde904aca\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595.html\n",
      "54c4d011-d894-42de-8b06-3f68bfdfae0a\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277.html\n",
      "e550df24-500a-47da-871e-45c9c237c6c5\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-turns-1-f69dcdd45fe3.html\n",
      "7122d6bc-8686-4963-8965-c45b7dcc93da\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-update-07-10-2023-4ceebdab96cb.html\n",
      "4f20eb78-358b-4525-950a-d014c9b5a4e4\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-update-08-01-2023-185514d9b897.html\n",
      "7a7ff547-14ef-496b-a8e6-4ed239047a07\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-update-09-03-2023-4a7c21c0f60b.html\n",
      "5e35d539-6390-48bb-9205-e3b17e1cd9e0\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-update-20-09-2023-86ed66f78bac.html\n",
      "3285e0fd-d0b5-48f4-b79e-57eb2c3e4b26\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-update-2023-10-10-3718a3d19fb9.html\n",
      "9bb11259-1760-43ad-b6f5-00a7d237f2fb\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-update-6-26-2023-ed30a9d45f84.html\n",
      "e10a12c9-04f9-4687-8409-d2b28ecf0a17\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-v0-10-838e735948f8.html\n",
      "081baa8f-e7a1-4804-b75e-e6c7702709b3\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-vectara-7a3889cd34cb.html\n",
      "a382cabf-7150-4c27-8e29-9158df4d8cd8\n",
      "/workspace/projects/LlamindexHelper/data/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82.html\n",
      "dd19d281-9b4e-442f-8f4f-c9a6007f3ff6\n",
      "/workspace/projects/LlamindexHelper/data/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7.html\n",
      "6530c364-f6dc-4b04-9631-9c18b4241caf\n",
      "/workspace/projects/LlamindexHelper/data/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125.html\n",
      "9260d03a-a64d-44d8-837a-371c97930488\n",
      "/workspace/projects/LlamindexHelper/data/multi-modal-rag-621de7525fea.html\n",
      "ec91c3cb-204a-4184-b27c-60145bb5ea4c\n",
      "/workspace/projects/LlamindexHelper/data/multimodal-rag-building-ainimal-go-fecf8404ed97.html\n",
      "713febfb-2255-496c-9648-087c01d69fcf\n",
      "/workspace/projects/LlamindexHelper/data/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e.html\n",
      "1b398c01-24b6-4aa4-afda-d1b0fab6dd83\n",
      "/workspace/projects/LlamindexHelper/data/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206.html\n",
      "7516225c-ac9f-4846-b9df-7b9fd66a796e\n",
      "/workspace/projects/LlamindexHelper/data/newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11.html\n",
      "bdb955cb-3791-4a33-bb8b-9b49ae4dc880\n",
      "/workspace/projects/LlamindexHelper/data/nvidia-research-rag-with-long-context-llms-7d94d40090c4.html\n",
      "003fa2ae-c366-4f7e-b6d1-2b03b19a8c23\n",
      "/workspace/projects/LlamindexHelper/data/one-click-open-source-rag-observability-with-langfuse.html\n",
      "097b6b16-d85c-42fc-863c-cf2d6a082ee2\n",
      "/workspace/projects/LlamindexHelper/data/openai-cookbook-evaluating-rag-systems-fe393c61fb93.html\n",
      "79033d51-6e72-4407-bcd3-8a1c1ea243f3\n",
      "/workspace/projects/LlamindexHelper/data/pii-detector-hacking-privacy-in-rag.html\n",
      "4c78b9be-10aa-4203-b3a5-95c6452201a0\n",
      "/workspace/projects/LlamindexHelper/data/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f.html\n",
      "c990777c-dce7-4e5b-bb02-54cdd9180314\n",
      "/workspace/projects/LlamindexHelper/data/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f.html\n",
      "2716650b-a624-4ddc-8de6-9664b0a43286\n",
      "/workspace/projects/LlamindexHelper/data/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089.html\n",
      "3a8628c5-14f9-493f-83fc-5ec0da7827f1\n",
      "/workspace/projects/LlamindexHelper/data/retrieving-privacy-safe-documents-over-a-network.html\n",
      "4e63a095-cf8f-4bfc-922d-5ae7cee288a2\n",
      "/workspace/projects/LlamindexHelper/data/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab.html\n",
      "c8cee370-e834-4710-af6e-1c4d640d1d22\n",
      "/workspace/projects/LlamindexHelper/data/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716.html\n",
      "28df59df-30a7-4aee-90b7-cb990c758c92\n",
      "/workspace/projects/LlamindexHelper/data/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions.html\n",
      "7e6bcfe0-e8f2-47bc-975f-c4e0a1ec6170\n",
      "/workspace/projects/LlamindexHelper/data/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai.html\n",
      "c7312d12-c832-4281-a316-83f147e4a5b0\n",
      "/workspace/projects/LlamindexHelper/data/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d.html\n",
      "8e3da68c-661c-4d88-80dd-4a6b639985f2\n",
      "/workspace/projects/LlamindexHelper/data/simplify-your-rag-application-architecture-with-llamaindex-postgresml.html\n",
      "d713bde2-c216-45ca-a50c-4d82447ddcc9\n",
      "/workspace/projects/LlamindexHelper/data/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0.html\n",
      "ed5e1673-d674-4279-8727-32d03c128bf7\n",
      "/workspace/projects/LlamindexHelper/data/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb.html\n",
      "d49bc2cf-8eb4-4a55-8753-f055713a20c0\n",
      "/workspace/projects/LlamindexHelper/data/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations.html\n",
      "d3bea69b-d7cf-4192-b5f1-bda08ed5f075\n",
      "/workspace/projects/LlamindexHelper/data/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba.html\n",
      "dca7137f-1b4b-49cf-b8e1-365b6ee4f349\n",
      "/workspace/projects/LlamindexHelper/data/the-latest-updates-to-llamacloud.html\n",
      "8bf758c5-7993-4480-86e4-31efc1b5b2e1\n",
      "/workspace/projects/LlamindexHelper/data/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0.html\n",
      "7aeff5aa-425a-4a5d-b684-c669ed5b53ad\n",
      "/workspace/projects/LlamindexHelper/data/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9.html\n",
      "68cacd2a-53b3-4748-87b4-3daca98a318b\n",
      "/workspace/projects/LlamindexHelper/data/towards-long-context-rag.html\n",
      "907c913e-6863-4e67-ba21-99654c410ecd\n",
      "/workspace/projects/LlamindexHelper/data/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9.html\n",
      "2e713d08-09cb-465a-84ad-f47c210ab63d\n",
      "/workspace/projects/LlamindexHelper/data/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5.html\n",
      "9a277129-75eb-4cf1-8762-14f409a7daba\n",
      "/workspace/projects/LlamindexHelper/data/unlocking-the-3rd-dimension-for-generative-ai-part-1.html\n",
      "bb7b2a61-3cdd-4d17-adcf-7f7dee63829f\n",
      "/workspace/projects/LlamindexHelper/data/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.html\n",
      "0a2f8bfb-56cc-4800-b16e-bddb54595e21\n",
      "/workspace/projects/LlamindexHelper/data/using-llms-for-retrieval-and-reranking-23cf2d3a14b6.html\n",
      "b6de8c2f-dc98-46a4-be20-8f3960df6968\n",
      "/workspace/projects/LlamindexHelper/data/vellum-llamaindex-integration-58b476a1e33f.html\n",
      "9c957df3-76e6-4ecd-9cb6-7c86f1819cdc\n",
      "/workspace/projects/LlamindexHelper/data/zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc.html\n",
      "5d4d5ede-c2ae-41d9-b9a9-8b0394f363af\n"
     ]
    }
   ],
   "source": [
    "for k,v in index_struct.doc_id_to_summary_id.items():\n",
    "    print(k)\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "400cbd67-6834-415b-bd1d-ab27196c9cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "doc_sum_index = load_index_from_storage(\n",
    "    llm=llm_hf,\n",
    "    embed_model=embed_model,\n",
    "    storage_context=chroma_storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd25ec4c-ec0a-4198-9fd7-17be2b88a2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided text is about using the llama-index library to evaluate the performance of RAG systems based on seven measurement aspects outlined in a survey paper by Gao et al. The text highlights the evaluation notebook guides provided by the llama-index library and explains the concept of faithfulness, which is further explained in a Notion document. This text can answer questions related to the evaluation capabilities of the llama-index library, the measurement aspects outlined by Gao et al., and how to use the library to assess the performance of RAG systems in relation to these aspects.\n",
      "\n",
      "Some potential questions that this text can answer include:\n",
      "- What is the llama-index library and how can it be used to evaluate RAG systems?\n",
      "- What measurement aspects are outlined in the survey paper by Gao et al.?\n",
      "- How can the evaluation notebook guides provided by the llama-index library be used to assess the performance of RAG systems in relation to these measurement aspects?\n",
      "- What is faithfulness and how is it related to RAG systems?\n",
      "- How can the Notion document help in understanding the concept of faithfulness in relation to RAG systems?\n",
      "- What are some sophisticated techniques for building Advanced RAG systems, and how can they be applied using the llama-index library?\n",
      "\n",
      "Overall, this text provides insights into using the llama-index library to evaluate RAG systems based on the measurement aspects outlined by Gao et al., and can help builders assess the level to which their RAG system meets success requirements through these measurement aspects.\n"
     ]
    }
   ],
   "source": [
    "# print sample summarization\n",
    "print(doc_sum_index.get_document_summary(\"/workspace/projects/LlamindexHelper/data/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765d222f-0270-41e0-8fb6-86c12dbe5788",
   "metadata": {},
   "source": [
    "## Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60df6f27-eb26-4fed-8e57-f4ff2a665977",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"What are key features of llama-agents?\"\n",
    "question2 = '''What are the two critical areas of RAG system performance that are assessed \\\n",
    "in the \"Evaluating RAG with LlamaIndex\" section of the OpenAI Cookbook?'''\n",
    "question3 = '''What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93669b4d-8831-45a7-961c-8ba5075a5ae8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc_sum_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m query_engine \u001b[38;5;241m=\u001b[39m \u001b[43mdoc_sum_index\u001b[49m\u001b[38;5;241m.\u001b[39mas_query_engine(\n\u001b[1;32m      2\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm_hf,\n\u001b[1;32m      3\u001b[0m     response_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompact\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      4\u001b[0m     use_async\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'doc_sum_index' is not defined"
     ]
    }
   ],
   "source": [
    "query_engine = doc_sum_index.as_query_engine(\n",
    "    llm=llm_hf,\n",
    "    response_mode=\"compact\", \n",
    "    use_async=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42311320-02ae-41b3-b482-1ad626a6f89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e72ebf7e453457b84ff66913ef97d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "resp1 = query_engine.query(question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85eab728-b195-4efe-b400-906fd7ff946c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Based on the new context, I refine the original answer to provide a more detailed and accurate response.\n",
       "\n",
       "The key features of Llama Agents are:\n",
       "\n",
       "1. **Text-to-Image Prompt Generation**: Llama Agents use LlamaIndex, a vector database created from DiffusionDB, to generate better prompts for text-to-image tasks. This allows for more accurate and varied image generation.\n",
       "2. **Customizability**: Llama Agents can be customized to suit specific use cases by adjusting parameters such as temperature, which controls the variation of generated prompts.\n",
       "3. **Integration with Transformers Agents**: Llama Agents can be integrated with Transformers Agents, allowing for the creation of text-to-image prompt assistants that can generate images based on user input.\n",
       "4. **Easy Distribution and Sharing**: Llama Agents can be easily distributed and shared using Hugging Face Spaces, enabling collaboration and innovation in the community.\n",
       "5. **Vector Database**: LlamaIndex is built on a vector database created from DiffusionDB, which allows for efficient and accurate prompt generation.\n",
       "\n",
       "These features enable Llama Agents to generate high-quality images based on user input, making them a powerful tool for creative applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(resp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7b64112-61ca-42c5-ba2a-60729ed40178",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_path': '/workspace/projects/LlamindexHelper/data/llamaindex-and-transformers-agents-67042ee1d8d6.html', 'file_name': 'llamaindex-and-transformers-agents-67042ee1d8d6.html', 'file_type': 'text/html', 'file_size': 14762, 'creation_date': '2024-07-21', 'last_modified_date': '2024-07-21'}\n"
     ]
    }
   ],
   "source": [
    "for node in resp1.source_nodes:\n",
    "    print(node.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8e4e06-c47e-4ba6-bea6-78a952ed8f37",
   "metadata": {},
   "source": [
    "### Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07e07518-4eb6-426d-97a4-8a7a4f02494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core import QueryBundle\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd1e8c69-83b7-4554-83d6-b64a5e11ad1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 100)\n",
    "\n",
    "def get_retrieved_nodes(\n",
    "    query_str, index, vector_top_k=10, reranker=None\n",
    "):\n",
    "    query_bundle = QueryBundle(query_str)\n",
    "    # configure retriever\n",
    "    retriever = VectorIndexRetriever(\n",
    "        index=index,\n",
    "        similarity_top_k=vector_top_k,\n",
    "    )\n",
    "    retrieved_nodes = retriever.retrieve(query_bundle)\n",
    "\n",
    "    if reranker is not None:\n",
    "        retrieved_nodes = reranker.postprocess_nodes(\n",
    "            retrieved_nodes, query_bundle\n",
    "        )\n",
    "\n",
    "    return retrieved_nodes\n",
    "\n",
    "def pretty_print(df):\n",
    "    return display(HTML(df.to_html().replace(\"\\\\n\", \"<br>\")))\n",
    "\n",
    "\n",
    "def visualize_retrieved_nodes(nodes) -> None:\n",
    "    result_dicts = []\n",
    "    for node in nodes:\n",
    "        result_dict = {\"Score\": node.score, \"Metadata\": node.metadata, \"Text\": node.node.get_text()}\n",
    "        result_dicts.append(result_dict)\n",
    "\n",
    "    pretty_print(pd.DataFrame(result_dicts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46596e49-3a05-4136-8045-5a0df694424f",
   "metadata": {},
   "source": [
    "### Retrieval without reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ff44344-d8b6-4844-babb-9aa27884ea06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0799e89b8c4c4fb39cf17d198e909421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_nodes = get_retrieved_nodes(\n",
    "    question1,\n",
    "    index=doc_sum_index,\n",
    "    vector_top_k=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74799368-23c4-4a76-8c74-fb4364c3e226",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Metadata</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.575996</td>\n",
       "      <td>{'file_path': '/workspace/projects/LlamindexHelper/data/llamaindex-and-transformers-agents-67042ee1d8d6.html', 'file_name': 'llamaindex-and-transformers-agents-67042ee1d8d6.html', 'file_type': 'text/html', 'file_size': 14762, 'creation_date': '2024-07-21', 'last_modified_date': '2024-07-21'}</td>\n",
       "      <td>The provided text discusses the use of Large Language Models (LLMs) and agents, which are programs that can perform tasks and make decisions. It mentions the release of Transformers Agents, a popular use-case for LLMs, as well as the LlamaIndex tool, which can augment agents' image-generation capabilities by suggesting better prompts. The text also introduces a Text2Image Prompt Assistant tool, which can re-write prompts to generate more beautiful images. The text suggests that LlamaIndex can be used by agents and provides some context on the capabilities and limitations of LLMs and agents, as well as the challenges and opportunities associated with their use. Some potential questions that this text can answer include:<br><br>- What are LLMs and agents, and how are they being used in practice?<br>- How can LLMs and agents be integrated into specific applications, such as image generation?<br>- What are some of the key benefits and drawbacks of using LLMs and agents, and how are these factors influencing their adoption and use?<br>- How are LLMs and agents being developed and improved over time, and what are some of the key trends and innovations in this area?<br>- How can LLMs and agents be made more robust and reliable, particularly in the face of uncertainty and ambiguity in input data?<br>- How can LLMs and agents be made more efficient and scalable, particularly in terms of resource usage and computational complexity?<br>- How can LLMs and agents be made more transparent and interpretable, particularly in terms of their decision-making processes and outcomes?<br>- How can LLMs and agents be made more secure and trustworthy, particularly in terms of their privacy and confidentiality concerns?<br>- How can LLMs and agents be made more adaptable and flexible, particularly in terms of their ability to handle new and diverse types of input data?<br>- How can LlamaIndex be used with LLM agents, and how can custom tools in Transformers Agents be distributed and shared using Hugging Face Spaces?<br><br>The text also provides examples of how the Text2Image Prompt Assistant tool can be used to generate different images based on varying prompts and temperatures.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.575554</td>\n",
       "      <td>{'file_path': '/workspace/projects/LlamindexHelper/data/building-better-tools-for-llm-agents-f8c5a6714f11.html', 'file_name': 'building-better-tools-for-llm-agents-f8c5a6714f11.html', 'file_type': 'text/html', 'file_size': 26798, 'creation_date': '2024-07-21', 'last_modified_date': '2024-07-21'}</td>\n",
       "      <td>The provided text discusses various aspects related to the creation and usage of tools for large language model (LLM) agents. It touches upon the LlamaHub Tools library, which enables LLMs such as ChatGPT to connect to APIs and perform actions on behalf of users. The author highlights the importance of writing informative and useful tool prompts and making them tolerant of partial inputs to minimize errors. The text also discusses the development of tools for Google Workspace applications, specifically focusing on Google Calendar and Gmail APIs. It emphasizes the need to provide simple deterministic functions for agents to use and return prompts from functions that perform mutations. The text also mentions the consideration of the size of the context window when building tools and making them compatible with the LoadAndSearchTool. The author suggests asking the agent about its own tools to debug them during development. The text also provides an example conversation between an Agent and a user to demonstrate how to debug tools. Overall, the text offers insights into creating and utilizing tools for LLamaIndex, as well as tips and techniques for optimizing their functionality and usability. Some of the questions that this text can answer include how to create tolerant tools, how to provide simple deterministic functions for agents, how to return prompts from functions that perform mutations, how to make tools compatible with the LoadAndSearchTool, and how to consider the size of the context window when building tools.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.571714</td>\n",
       "      <td>{'file_path': '/workspace/projects/LlamindexHelper/data/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html', 'file_name': 'introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html', 'file_type': 'text/html', 'file_size': 18790, 'creation_date': '2024-07-21', 'last_modified_date': '2024-07-21'}</td>\n",
       "      <td>The provided text is introducing a Python library called llama-agents, which allows for the creation of complex multi-agent AI systems. The text explains how to use this library to construct a Query Rewriting RAG system, which combines query rewriting with RAG to enhance question-answering capabilities. This is an alpha release, and the developers are seeking feedback from the public to improve the library's features for use in production. Some potential questions that this text can answer include inquiries about the uniqueness of llama-agents in comparison to other AI libraries, examples of multi-agent AI systems that can be built using this library, how the Query Rewriting RAG system functions, and how to set up and utilize the multi-agent system with llama-agents. The text also mentions the availability of examples and resources in the llama-agents repository and explains how users can contribute to the public roadmap by providing feedback on the features. Overall, the text serves as a resource for acquiring knowledge about the library's capabilities and potential applications.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_retrieved_nodes(raw_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b63de52-d1e8-4c97-940c-0c5aa7f57943",
   "metadata": {},
   "source": [
    "### Retrieve with reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c9f91ca-6531-4278-b965-80be0a08b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_postprocessor = SentenceTransformerRerank(\n",
    "    model='models/mxbai-rerank-xsmall-v1',\n",
    "    top_n=2, # number of nodes after re-ranking,\n",
    "    keep_retrieval_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b055b50c-cddf-47b2-8471-fab3c387b9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2879b75376764930b4f09b6772ae1a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991382ad273c418ea3d559ca82e6d806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reranked_nodes = get_retrieved_nodes(\n",
    "    question1,\n",
    "    index=doc_sum_index,\n",
    "    vector_top_k=5,\n",
    "    reranker= rerank_postprocessor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c03c3f0-c7aa-4395-85e6-a038e9d51e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Metadata</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.864365</td>\n",
       "      <td>{'file_path': '/workspace/projects/LlamindexHelper/data/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html', 'file_name': 'introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html', 'file_type': 'text/html', 'file_size': 18790, 'creation_date': '2024-07-21', 'last_modified_date': '2024-07-21', 'retrieval_score': 0.5717135510066469}</td>\n",
       "      <td>The provided text is introducing a Python library called llama-agents, which allows for the creation of complex multi-agent AI systems. The text explains how to use this library to construct a Query Rewriting RAG system, which combines query rewriting with RAG to enhance question-answering capabilities. This is an alpha release, and the developers are seeking feedback from the public to improve the library's features for use in production. Some potential questions that this text can answer include inquiries about the uniqueness of llama-agents in comparison to other AI libraries, examples of multi-agent AI systems that can be built using this library, how the Query Rewriting RAG system functions, and how to set up and utilize the multi-agent system with llama-agents. The text also mentions the availability of examples and resources in the llama-agents repository and explains how users can contribute to the public roadmap by providing feedback on the features. Overall, the text serves as a resource for acquiring knowledge about the library's capabilities and potential applications.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.411383</td>\n",
       "      <td>{'file_path': '/workspace/projects/LlamindexHelper/data/llamaindex-and-transformers-agents-67042ee1d8d6.html', 'file_name': 'llamaindex-and-transformers-agents-67042ee1d8d6.html', 'file_type': 'text/html', 'file_size': 14762, 'creation_date': '2024-07-21', 'last_modified_date': '2024-07-21', 'retrieval_score': 0.5759962319721066}</td>\n",
       "      <td>The provided text discusses the use of Large Language Models (LLMs) and agents, which are programs that can perform tasks and make decisions. It mentions the release of Transformers Agents, a popular use-case for LLMs, as well as the LlamaIndex tool, which can augment agents' image-generation capabilities by suggesting better prompts. The text also introduces a Text2Image Prompt Assistant tool, which can re-write prompts to generate more beautiful images. The text suggests that LlamaIndex can be used by agents and provides some context on the capabilities and limitations of LLMs and agents, as well as the challenges and opportunities associated with their use. Some potential questions that this text can answer include:<br><br>- What are LLMs and agents, and how are they being used in practice?<br>- How can LLMs and agents be integrated into specific applications, such as image generation?<br>- What are some of the key benefits and drawbacks of using LLMs and agents, and how are these factors influencing their adoption and use?<br>- How are LLMs and agents being developed and improved over time, and what are some of the key trends and innovations in this area?<br>- How can LLMs and agents be made more robust and reliable, particularly in the face of uncertainty and ambiguity in input data?<br>- How can LLMs and agents be made more efficient and scalable, particularly in terms of resource usage and computational complexity?<br>- How can LLMs and agents be made more transparent and interpretable, particularly in terms of their decision-making processes and outcomes?<br>- How can LLMs and agents be made more secure and trustworthy, particularly in terms of their privacy and confidentiality concerns?<br>- How can LLMs and agents be made more adaptable and flexible, particularly in terms of their ability to handle new and diverse types of input data?<br>- How can LlamaIndex be used with LLM agents, and how can custom tools in Transformers Agents be distributed and shared using Hugging Face Spaces?<br><br>The text also provides examples of how the Text2Image Prompt Assistant tool can be used to generate different images based on varying prompts and temperatures.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_retrieved_nodes(reranked_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adace3a0-6366-4663-b95f-240d4b680e1a",
   "metadata": {},
   "source": [
    "### LLM as reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b51aee88-cea5-44e0-b23d-1d672fcc3432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor.rankgpt_rerank import RankGPTRerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1421ce72-51ac-4e50-83cc-10ab509523fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_reranker = RankGPTRerank(\n",
    "    llm=llm_hf,\n",
    "    top_n=2,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d73f36ee-33a3-4b44-9a9e-11740e1e4bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519f15fb991a45cfba6510fdf23aeca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Reranking, new rank list for nodes: [2, 0, 3, 1, 4]"
     ]
    }
   ],
   "source": [
    "llm_reranked_nodes = get_retrieved_nodes(\n",
    "    question1,\n",
    "    index=doc_sum_index,\n",
    "    vector_top_k=5,\n",
    "    reranker=llm_reranker\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d0439a3-4c14-4b40-bbfe-4f301406bbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Metadata</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.571714</td>\n",
       "      <td>{'file_path': '/workspace/projects/LlamindexHelper/data/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html', 'file_name': 'introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html', 'file_type': 'text/html', 'file_size': 18790, 'creation_date': '2024-07-21', 'last_modified_date': '2024-07-21'}</td>\n",
       "      <td>The provided text is introducing a Python library called llama-agents, which allows for the creation of complex multi-agent AI systems. The text explains how to use this library to construct a Query Rewriting RAG system, which combines query rewriting with RAG to enhance question-answering capabilities. This is an alpha release, and the developers are seeking feedback from the public to improve the library's features for use in production. Some potential questions that this text can answer include inquiries about the uniqueness of llama-agents in comparison to other AI libraries, examples of multi-agent AI systems that can be built using this library, how the Query Rewriting RAG system functions, and how to set up and utilize the multi-agent system with llama-agents. The text also mentions the availability of examples and resources in the llama-agents repository and explains how users can contribute to the public roadmap by providing feedback on the features. Overall, the text serves as a resource for acquiring knowledge about the library's capabilities and potential applications.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.575996</td>\n",
       "      <td>{'file_path': '/workspace/projects/LlamindexHelper/data/llamaindex-and-transformers-agents-67042ee1d8d6.html', 'file_name': 'llamaindex-and-transformers-agents-67042ee1d8d6.html', 'file_type': 'text/html', 'file_size': 14762, 'creation_date': '2024-07-21', 'last_modified_date': '2024-07-21'}</td>\n",
       "      <td>The provided text discusses the use of Large Language Models (LLMs) and agents, which are programs that can perform tasks and make decisions. It mentions the release of Transformers Agents, a popular use-case for LLMs, as well as the LlamaIndex tool, which can augment agents' image-generation capabilities by suggesting better prompts. The text also introduces a Text2Image Prompt Assistant tool, which can re-write prompts to generate more beautiful images. The text suggests that LlamaIndex can be used by agents and provides some context on the capabilities and limitations of LLMs and agents, as well as the challenges and opportunities associated with their use. Some potential questions that this text can answer include:<br><br>- What are LLMs and agents, and how are they being used in practice?<br>- How can LLMs and agents be integrated into specific applications, such as image generation?<br>- What are some of the key benefits and drawbacks of using LLMs and agents, and how are these factors influencing their adoption and use?<br>- How are LLMs and agents being developed and improved over time, and what are some of the key trends and innovations in this area?<br>- How can LLMs and agents be made more robust and reliable, particularly in the face of uncertainty and ambiguity in input data?<br>- How can LLMs and agents be made more efficient and scalable, particularly in terms of resource usage and computational complexity?<br>- How can LLMs and agents be made more transparent and interpretable, particularly in terms of their decision-making processes and outcomes?<br>- How can LLMs and agents be made more secure and trustworthy, particularly in terms of their privacy and confidentiality concerns?<br>- How can LLMs and agents be made more adaptable and flexible, particularly in terms of their ability to handle new and diverse types of input data?<br>- How can LlamaIndex be used with LLM agents, and how can custom tools in Transformers Agents be distributed and shared using Hugging Face Spaces?<br><br>The text also provides examples of how the Text2Image Prompt Assistant tool can be used to generate different images based on varying prompts and temperatures.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_retrieved_nodes(llm_reranked_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5063a8b1-1a3c-48c3-b89e-d2c4fe62d38b",
   "metadata": {},
   "source": [
    "## Query with reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6643604f-e8e0-42c6-b1ec-64a3797306c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = doc_sum_index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    node_postprocessors=[rerank_postprocessor],\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6163ec31-aa53-4a63-8410-0b77477d559f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be980bfcc794cccad3ff74a06dcc0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0912d8703f445fa65a820f0cdf6d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "536220d4-a301-4654-ac89-b2ab48a87339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Based on the provided context information, the key features of llama-agents are:\n",
       "\n",
       "1. **Distributed Service Oriented Architecture**: Every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\n",
       "2. **Communication via standardized API interfaces**: Interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.\n",
       "3. **Define agentic and explicit orchestration flows**: Developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an “agentic orchestrator” that decides which agents are relevant to the task.\n",
       "4. **Ease of deployment**: Launch, scale, and monitor each agent and the control plane independently.\n",
       "5. **Scalability and resource management**: Use built-in observability tools to monitor the quality and performance of the system and each individual agent service.\n",
       "6. **Multi-agent system**: Llama-agents provides a framework for building complex multi-agent systems, allowing you to create multiple agents that can interact with each other.\n",
       "7. **Query rewriting**: The system can rewrite user queries to improve retrieval, making it easier to find relevant information.\n",
       "8. **Retriever Query Engine**: The system uses a Retriever Query Engine to execute queries and retrieve relevant information.\n",
       "9. **OpenAI integration**: Llama-agents integrates with OpenAI, allowing you to leverage their large language models (LLMs) to perform tasks such as question-answering and natural language processing.\n",
       "10. **Customizable**: The system is highly customizable, allowing you to define your own agents, tools, and workflows.\n",
       "11. **Pipeline-based architecture**: Llama-agents uses a pipeline-based architecture, which allows you to chain multiple agents and tools together to create complex workflows.\n",
       "12. **Agent orchestration**: The system provides an orchestration layer that allows you to manage and coordinate the execution of multiple agents and tools.\n",
       "13. **Multi-source data integration**: Llama-agents can integrate with multiple data sources, allowing you to retrieve information from various sources and combine it to answer complex questions.\n",
       "\n",
       "Note that some of these features may be mentioned in multiple sources, but they are all related to the key features of llama-agents."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0173b14a-1fa8-4e7c-ab47-345416cec563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c18d9bd8-763e-4a8c-b32f-2e8c87b5c3be': {'file_path': '/workspace/projects/LlamindexHelper/data/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html',\n",
       "  'file_name': 'introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html',\n",
       "  'file_type': 'text/html',\n",
       "  'file_size': 18790,\n",
       "  'creation_date': '2024-07-21',\n",
       "  'last_modified_date': '2024-07-21',\n",
       "  'retrieval_score': None},\n",
       " '7e016859-6da8-439e-859d-93470687a784': {'file_path': '/workspace/projects/LlamindexHelper/data/how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa.html',\n",
       "  'file_name': 'how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa.html',\n",
       "  'file_type': 'text/html',\n",
       "  'file_size': 16248,\n",
       "  'creation_date': '2024-07-21',\n",
       "  'last_modified_date': '2024-07-21',\n",
       "  'retrieval_score': None}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b89d53-aada-40cb-8643-2e520e978168",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
