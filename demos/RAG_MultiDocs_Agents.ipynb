{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5f27f8c-d937-43c3-b9b6-461f27114045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "476a8372-65ce-4f12-bf16-93fba5124147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66071ee0-a354-4d8a-bfef-e23be38d96b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/llm/lib/python3.10/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.3.0+cu118 available.\n",
      "PyTorch version 2.3.0+cu118 available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from llama_index.core import (\n",
    "    Settings,\n",
    "    VectorStoreIndex, SummaryIndex,\n",
    "    SimpleKeywordTableIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage\n",
    ")\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core.schema import IndexNode\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "from IPython.display import Markdown, display\n",
    "import agent_utils, data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eaf6aa6-6967-45de-b0ab-5039f640de19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: models/bge-base-en-v1.5\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: models/bge-base-en-v1.5\n",
      "Load pretrained SentenceTransformer: models/bge-base-en-v1.5\n",
      "INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2 prompts are loaded, with the keys: ['query', 'text']\n",
      "Loading LLM: models/Meta-Llama-3.1-8B-Instruct\n",
      "Loading tokenizer and model with quantization config from: models/Meta-Llama-3.1-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LLM and embedding models\n"
     ]
    }
   ],
   "source": [
    "device_map = \"cuda:0\"\n",
    "llm_hf, embed_model = agent_utils.load_llm_embed_models(\n",
    "    llm_name=\"models/Meta-Llama-3.1-8B-Instruct\", embed_name=\"models/bge-base-en-v1.5\", device_map=device_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc59da40-a98b-4425-837c-354a78a8672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm_hf\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa677b1-5ef9-4df0-9be4-a61d265b5371",
   "metadata": {},
   "source": [
    "## Load documents/ nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0e73d25-ec74-4b25-af17-fb596d9dded6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num documents: 166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing documents: 100%|██████████| 166/166 [00:00<00:00, 4324.99it/s]\n"
     ]
    }
   ],
   "source": [
    "documents = data_utils.load_md_documents(\n",
    "    docs_dir=\"data/llama-blogs-md\", docs_metadata=\"data/llama_blogs_metadata.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d266b3df-c56a-4ad0-9829-74ab2f2f8f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6c32bbd-3a5f-41e7-9dbf-a493b8f8d79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': 'building-a-multi-agent-concierge-system.md',\n",
       " 'extension': '.md',\n",
       " 'title': 'Building a multi-agent concierge system',\n",
       " 'date': 'Jul 17, 2024',\n",
       " 'url': 'https://www.llamaindex.ai/blog/building-a-multi-agent-concierge-system'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[10].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9c695d9-9435-417c-8735-4a752755f492",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "##  Why build this?\n",
       "\n",
       "Interactive chat bots are by this point a familiar solution to customer\n",
       "service, and agents are a frequent component of chat bot implementations. They\n",
       "provide memory, introspection, tool use and other features necessary for a\n",
       "competent bot.\n",
       "\n",
       "We have become interested in larger-scale chatbots: ones that can complete\n",
       "dozens of tasks, some of which have dependencies on each other, using hundreds\n",
       "of tools. What would that agent look like? It would have an enormous system\n",
       "prompt and a huge number of tools to choose from, which can be confusing for\n",
       "an agent.\n",
       "\n",
       "Imagine a bank implementing a system that can:\n",
       "\n",
       "  * Look up the price of a specific stock \n",
       "  * Authenticate a user \n",
       "  * Check your account balance \n",
       "    * Which requires the user be authenticated \n",
       "  * Transfer money between accounts \n",
       "    * Which requires the user be authenticated \n",
       "    * And also that the user checks their account balance first \n",
       "\n",
       "Each of these top-level tasks has sub-tasks, for instance:\n",
       "\n",
       "  * The stock price lookup might need to look up the stock symbol first \n",
       "  * The user authentication would need to gather a username and a password \n",
       "  * The account balance would need to know which of the user's accounts to check \n",
       "\n",
       "Coming up with a single primary prompt for all of these tasks and sub-tasks\n",
       "would be very complex. So instead, we designed a multi-agent system with\n",
       "agents responsible for each top-level task, plus a \"concierge\" agent that can\n",
       "direct the user to the correct agent.\n",
       "\n",
       "##  What we built\n",
       "\n",
       "We built a system of agents to complete the above tasks. [ It's open-source\n",
       "](https://github.com/run-llama/multi-agent-concierge/) ! There are four basic\n",
       "\"task\" agents:\n",
       "\n",
       "  * A stock lookup agent (which takes care of sub-tasks like looking up symbols) \n",
       "  * An authentication agent (which asks for username and password) \n",
       "  * An account balance agent (which takes care of sub-tasks like selecting an account) \n",
       "  * A money transfer agent (which takes care of tasks like asking what account to trans"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(documents[10].text[:2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03761d01-c7fb-4214-b846-0e0f1bb3c24b",
   "metadata": {},
   "source": [
    "## Create agents dict from document tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d86280d-792c-46a2-9930-0fdcfcda1e5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1168.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-08-06\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 456.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1111.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from jamba-instruct-s-256k-context-window-on-llamaindex\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 894.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-07-30\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1240.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from dynamic-retrieval-with-llamacloud\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1027.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from introducing-llamaextract-beta-structured-data-extraction-in-just-a-few-clicks\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 998.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-07-23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 788.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from improving-vector-search-reranking-with-postgresml-and-llamaindex\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1390.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from the-latest-updates-to-llamacloud\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1599.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 831.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from building-a-multi-agent-concierge-system\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1086.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-07-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 2403.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 707.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 726.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-07-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1982.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamacloud-built-for-enterprise-llm-app-builders\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1170.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-07-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 574.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1267.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-06-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1036.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-06-18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 669.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from customizing-property-graph-index-in-llamaindex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1096.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-06-11\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 881.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-06-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 926.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 2359.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from simplify-your-rag-application-architecture-with-llamaindex-postgresml\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1098.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-05-28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1214.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from automate-online-tasks-with-multion-and-llamaindex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 704.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from batch-inference-with-mymagic-ai-and-llamaindex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 858.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-05-21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1279.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 703.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1292.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-05-14\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 2978.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-05-07\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 2772.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-04-30\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 804.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1382.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-04-23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 917.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-04-16\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1143.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-04-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1094.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-04-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1037.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-03-26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1677.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from secure-rag-with-llamaindex-and-llm-guard-by-protect-ai\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 330.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from retrieving-privacy-safe-documents-over-a-network\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 491.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 2991.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-03-19\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 641.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from one-click-open-source-rag-observability-with-langfuse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 2606.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 383.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from pii-detector-hacking-privacy-in-rag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 472.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from launching-the-first-genai-native-document-parsing-platform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 2242.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-03-12\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1982.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-03-05\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 423.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from towards-long-context-rag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 979.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from unlocking-the-3rd-dimension-for-generative-ai-part-1\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 900.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 2352.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-02-27-4b9102a0f824\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 596.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 708.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from introducing-llamacloud-and-llamaparse-af8cedf9006b\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1937.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 487.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1986.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2023-02-13-26fa79601ba5\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 580.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 392.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-v0-10-838e735948f8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 548.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 3310.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-02-06-9a303130ad9f\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 470.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 588.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 932.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1265.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from agentic-rag-with-llamaindex-2721b8a49ff6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 2642.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-01-30-0d01eb0d8cef\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 557.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 2024.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1088.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 367.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 3077.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-01-23-11ee2c211bab\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 2077.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-01-16-752195bed96d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 417.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 633.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 752.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1425.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-01-09-6209000da2e6\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 324.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from introducing-query-pipelines-025dc2bb0537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 630.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1068.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 548.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 2608.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2024-01-02-f349db8c1842\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 747.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 458.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 2000.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2023-12-19-2965a2d03726\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 621.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 354.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 143.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 924.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 785.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-gemini-8d7c3b9ea97e\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 2639.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2023-12-12-4a5d542fbb1e\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 2198.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 647.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 3032.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2023-12-05-faf5ab930264\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 648.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from introducing-llama-datasets-aadb9994ad9e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 3837.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from openai-cookbook-evaluating-rag-systems-fe393c61fb93\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 2030.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2023-11-28-a31be430a786\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 549.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from multimodal-rag-building-ainimal-go-fecf8404ed97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 875.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from introducing-llama-packs-e14f453b913a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 690.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 3331.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2023-11-21-aa3a71e339f8\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1058.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from becoming-proficient-in-document-extraction-32aa13046ed5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 649.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 252.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1022.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 561.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 548.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from announcing-llamaindex-0-9-719f03282945\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1865.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 3104.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2023-11-14-dad06ae4284a\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1391.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-turns-1-f69dcdd45fe3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 617.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from multi-modal-rag-621de7525fea\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1158.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 2951.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2023-11-07-cf20b9a833aa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 922.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-news-special-edition-openai-developer-day-e955f16db4e2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 699.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 492.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 2603.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2023-10-31-36244e2b3f0c\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 410.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 3404.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2023-10-24-4a76204eeaa3\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 559.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from nvidia-research-rag-with-long-context-llms-7d94d40090c4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 765.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 586.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 570.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 790.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-newsletter-2023-10-17-33514cbc04a2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1840.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-update-2023-10-10-3718a3d19fb9\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 358.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 2310.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 301.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 530.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-update-20-09-2023-86ed66f78bac\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 864.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-vectara-7a3889cd34cb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 710.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-update-09-03-2023-4a7c21c0f60b\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 705.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1128.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 763.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from introducing-airbyte-sources-within-llamaindex-42209071722f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 637.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 497.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 723.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 727.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 742.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 667.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 980.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-update-08-01-2023-185514d9b897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 5391.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from data-agents-zapier-nla-67146395ce1\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1129.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from introducing-llamaindex-ts-89f41a1f24ab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 327.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from building-better-tools-for-llm-agents-f8c5a6714f11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 573.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from data-agents-eed797d7972f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1220.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-update-07-10-2023-4ceebdab96cb\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 523.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 348.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 559.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 492.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1179.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-update-6-26-2023-ed30a9d45f84\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 931.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 909.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llama-index-prem-ai-join-forces-51702fecedec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 558.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-and-weaviate-ba3ff1cbf5f4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 573.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-and-transformers-agents-67042ee1d8d6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 598.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from building-the-data-framework-for-llms-bca068e89e0e\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 516.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from vellum-llamaindex-integration-58b476a1e33f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 638.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 379.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 786.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 584.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from using-llms-for-retrieval-and-reranking-23cf2d3a14b6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 335.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 864.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 516.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 510.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    }
   ],
   "source": [
    "# Build agents dictionary\n",
    "agents = {}\n",
    "# query_engines = {}\n",
    "\n",
    "# this is for the baseline\n",
    "all_nodes = []\n",
    "\n",
    "for idx, doc in enumerate(documents):\n",
    "    nodes = data_utils.parse_md_doc([doc])\n",
    "    all_nodes.extend(nodes)\n",
    "    blog_title = doc.metadata[\"title\"]\n",
    "    name_id = doc.metadata[\"filename\"].split(\".\")[0]\n",
    "    \n",
    "    # init tools from nodes\n",
    "    vector_tool, summary_tool = agent_utils.get_tools_from_nodes(nodes, doc_metadata=doc.metadata)\n",
    "\n",
    "    # build agents\n",
    "    agent = ReActAgent.from_tools(\n",
    "        tools = [vector_tool, summary_tool],\n",
    "        llm=Settings.llm,\n",
    "        verbose=True,\n",
    "        context=f\"\"\"\\\n",
    "You are a specialized agent designed to answer queries about {blog_title}.\n",
    "You must ALWAYS use at least one of the tools provided when answering a question; do NOT rely on prior knowledge.\\\n",
    "\"\"\",\n",
    "    )\n",
    "\n",
    "    # store to agent dicts and query engine dict\n",
    "    agent_id = name_id.replace(\"-\", \"_\")\n",
    "    agents[agent_id] = agent\n",
    "    # query_engines[name_id] = vector_index.as_query_engine(similarity_top_k=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e73100-ce64-47e4-8560-abd590b21c3b",
   "metadata": {},
   "source": [
    "### Create agent tools and agent index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6a42602-0e9f-49e4-90b4-397662c7c05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating agent tools: 100%|██████████| 166/166 [00:00<00:00, 101008.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# define tool for each document agent\n",
    "all_tools = []\n",
    "for doc in tqdm(documents, desc=\"Creating agent tools\"):\n",
    "    blog_title = doc.metadata[\"title\"]\n",
    "    name_id = doc.metadata[\"filename\"].split(\".\")[0]\n",
    "    agent_id = name_id.replace(\"-\", \"_\")\n",
    "    \n",
    "    blog_summary = (\n",
    "        f\"This content contains llamaindex blogs about {blog_title}. Use\"\n",
    "        f\" this tool if you want to answer any questions about {blog_title}.\\n\"\n",
    "    )\n",
    "\n",
    "    doc_tool = QueryEngineTool(\n",
    "        query_engine=agents[agent_id],\n",
    "        metadata=ToolMetadata(\n",
    "            name=f\"tool_{agent_id}\",\n",
    "            description=blog_summary,\n",
    "        ),\n",
    "    )\n",
    "    all_tools.append(doc_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22a5a2d5-dead-4e4b-b1fe-2753ac9bc113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='This content contains llamaindex blogs about LlamaIndex Newsletter 2024-08-06. Use this tool if you want to answer any questions about LlamaIndex Newsletter 2024-08-06.\\n', name='tool_llamaindex_newsletter_2024_08_06', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tools[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4d77c1-2376-4f27-809c-cf3f4472f7a1",
   "metadata": {},
   "source": [
    "### define an \"object\" index and retriever over these tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13ee2bf7-c3c9-45ba-8c9b-741701aeb254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.15it/s]\n"
     ]
    }
   ],
   "source": [
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e082c0-bfa9-412a-9425-8b54d2225e34",
   "metadata": {},
   "source": [
    "### Define top agent to select doc tools given a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13e5b5f1-9a6f-440e-aa10-f16ed0470cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    }
   ],
   "source": [
    "top_agent = ReActAgent.from_tools(\n",
    "    tool_retriever=obj_index.as_retriever(similarity_top_k=3),\n",
    "    context=\"\"\" \\\n",
    "You are an agent designed to answer queries about a set of web blogs content.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\"\"\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2ace02-e168-4400-afef-1949c40d858b",
   "metadata": {},
   "source": [
    "## Comparing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07613b9e-a3a2-4a87-8477-f0068de55dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"What are key features of llama-agents?\"\n",
    "question2 = '''What are the two critical areas of RAG system performance that are assessed \\\n",
    "in the \"Evaluating RAG with LlamaIndex\" section of the OpenAI Cookbook?'''\n",
    "question3 = '''What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7722a31-2844-49e8-a8f4-bc6b3a1baff3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_index = VectorStoreIndex(all_nodes)\n",
    "base_query_engine = base_index.as_query_engine(similarity_top_k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "607c9e93-1fb5-4295-8d7e-aaf815ff7472",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step c672ee8a-87c2-4e9b-9f7e-9fe2b9f9d60e. Step input: What are key features of llama-agents?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 54.62it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: tool_introducing_llama_agents_a_powerful_framework_for_building_production_multi_agent_ai_systems\n",
      "Action Input: {'input': 'key features of llama-agents'}\n",
      "\u001b[0m> Running step 969f42e4-621f-4fab-a863-c884614e91c3. Step input: key features of llama-agents\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: vector_tool_introducing_llama_agents_a_powerful_framework_for_building_production_multi_agent_ai_systems\n",
      "Action Input: {'input': 'key features of llama-agents'}\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 79.87it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mObservation: \n",
      "\n",
      "Based on the provided context, the key features of llama-agents are:\n",
      "\n",
      "1. **Distributed Service Oriented Architecture:** every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\n",
      "2. **Communication via standardized API interfaces:** interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.\n",
      "3. **Define agentic and explicit orchestration flows:** developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an “agentic orchestrator” that decides which agents are relevant to the task.\n",
      "4. **Ease of deployment:** launch, scale and monitor each agent and your control plane independently.\n",
      "5. **Scalability and resource management:** use our built-in observability tools to monitor the quality and performance of the system and each individual agent service.\n",
      "\u001b[0m> Running step 713138e0-eb2d-4ce8-82a7-8a65f783c168. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: The key features of llama-agents are:\n",
      "\n",
      "1. Distributed Service Oriented Architecture, where every agent is an independently running microservice, orchestrated by a customizable LLM-powered control plane.\n",
      "2. Communication via standardized API interfaces, allowing agents to pass messages between each other through a message queue.\n",
      "3. The ability to define agentic and explicit orchestration flows, giving developers flexibility in determining the sequence of interactions between agents or leaving it to an \"agentic orchestrator\".\n",
      "4. Ease of deployment, allowing for the launch, scaling, and monitoring of each agent and the control plane independently.\n",
      "5. Scalability and resource management, facilitated by built-in observability tools that monitor the quality and performance of the system and individual agent services.\n",
      "\u001b[0m\u001b[1;3;34mObservation: The key features of llama-agents are:\n",
      "\n",
      "1. Distributed Service Oriented Architecture, where every agent is an independently running microservice, orchestrated by a customizable LLM-powered control plane.\n",
      "2. Communication via standardized API interfaces, allowing agents to pass messages between each other through a message queue.\n",
      "3. The ability to define agentic and explicit orchestration flows, giving developers flexibility in determining the sequence of interactions between agents or leaving it to an \"agentic orchestrator\".\n",
      "4. Ease of deployment, allowing for the launch, scaling, and monitoring of each agent and the control plane independently.\n",
      "5. Scalability and resource management, facilitated by built-in observability tools that monitor the quality and performance of the system and individual agent services.\n",
      "\u001b[0m> Running step e33cd294-627c-453a-8617-f3692f0107f7. Step input: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 60.85it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: The key features of llama-agents are:\n",
      "\n",
      "1. Distributed Service Oriented Architecture, where every agent is an independently running microservice, orchestrated by a customizable LLM-powered control plane.\n",
      "2. Communication via standardized API interfaces, allowing agents to pass messages between each other through a message queue.\n",
      "3. The ability to define agentic and explicit orchestration flows, giving developers flexibility in determining the sequence of interactions between agents or leaving it to an \"agentic orchestrator\".\n",
      "4. Ease of deployment, allowing for the launch, scaling, and monitoring of each agent and the control plane independently.\n",
      "5. Scalability and resource management, facilitated by built-in observability tools that monitor the quality and performance of the system and individual agent services.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# should use Boston agent -> vector tool\n",
    "response = top_agent.chat(question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0613ddd1-4bf3-46e6-8e33-babb147cb640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The key features of llama-agents are:\n",
       "\n",
       "1. Distributed Service Oriented Architecture, where every agent is an independently running microservice, orchestrated by a customizable LLM-powered control plane.\n",
       "2. Communication via standardized API interfaces, allowing agents to pass messages between each other through a message queue.\n",
       "3. The ability to define agentic and explicit orchestration flows, giving developers flexibility in determining the sequence of interactions between agents or leaving it to an \"agentic orchestrator\".\n",
       "4. Ease of deployment, allowing for the launch, scaling, and monitoring of each agent and the control plane independently.\n",
       "5. Scalability and resource management, facilitated by built-in observability tools that monitor the quality and performance of the system and individual agent services."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d85d28c-fff3-4914-982b-919187d76eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 128.62it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55912d68-9774-405e-8ebf-88c8361db715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 7c2556a0-48ab-497a-ba12-39d5217200b6. Step input: What is LlamaExtract?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 74.56it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: I need to use a tool to help me answer the question.\n",
      "Action: tool_introducing_llamaextract_beta_structured_data_extraction_in_just_a_few_clicks\n",
      "Action Input: {'input': 'What is LlamaExtract?'}\n",
      "\u001b[0m> Running step c303d178-ccf4-4db7-864f-a4af4a5c2bb4. Step input: What is LlamaExtract?\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: summary_tool_introducing_llamaextract_beta_structured_data_extraction_in_just_a_few_clicks\n",
      "Action Input: {'input': 'What is LlamaExtract?'}\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 56.31it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mObservation: \n",
      "\n",
      "LlamaExtract is a structured data extraction tool that allows users to extract data in just a few clicks. It is available to all users and can be accessed by creating an account at cloud.llamaindex.ai.\n",
      "\u001b[0m> Running step 5a86edfb-cf67-45cd-9ed3-7c4f473f1a3f. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: LlamaExtract is a structured data extraction tool that allows users to extract data in just a few clicks. It is available to all users and can be accessed by creating an account at cloud.llamaindex.ai.\n",
      "\u001b[0m\u001b[1;3;34mObservation: LlamaExtract is a structured data extraction tool that allows users to extract data in just a few clicks. It is available to all users and can be accessed by creating an account at cloud.llamaindex.ai.\n",
      "\u001b[0m> Running step d2cabb00-846c-49a3-8c76-78758e65c524. Step input: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 116.87it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: I need to use a tool to help me answer the question.\n",
      "Action: tool_introducing_llamaextract_beta_structured_data_extraction_in_just_a_few_clicks\n",
      "Action Input: {'input': 'What are the key features of LlamaExtract?'}\n",
      "\u001b[0m> Running step 06588313-62bc-4a3c-a7fd-27d63ab1e2d7. Step input: What are the key features of LlamaExtract?\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: vector_tool_introducing_llamaextract_beta_structured_data_extraction_in_just_a_few_clicks\n",
      "Action Input: {'input': 'key features of LlamaExtract'}\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.03it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mObservation: \n",
      "\n",
      "Based on the provided context, the key features of LlamaExtract are:\n",
      "\n",
      "1. **Multimodal extraction**: LlamaExtract can extract data from various file formats, including PDFs.\n",
      "2. **Decoupling schema creation from extraction**: Users can define a schema separately from the extraction process, allowing for more flexibility.\n",
      "3. **Human-in-the-loop schema creation**: Users can create schemas with human input, making it a first-class UX feature.\n",
      "4. **Automated schema inference**: LlamaExtract can automatically infer a schema from a document, such as a 10K filing.\n",
      "5. **Support for Pydantic JSON schema format**: The inferred schema follows the Pydantic JSON schema format, allowing users to customize and modify it as needed.\n",
      "6. **Drag-and-drop file upload**: Users can easily upload files to the UI for extraction.\n",
      "7. **Schema visualization**: The UI provides a visualization of the schema, making it easier to understand and modify.\n",
      "8. **Extraction**: LlamaExtract can extract data from files and return a JSON object with extracted keys and values in adherence with the schema.\n",
      "9. **API integration**: The LlamaExtract API allows users to integrate schema inference and extraction into their workflows.\n",
      "10. **Python client**: LlamaExtract has a Python client package, making it easy to use the API programmatically.\n",
      "\n",
      "These features make LlamaExtract a powerful tool for structured data extraction, allowing users to easily extract data from various file formats and customize the extraction process to their needs.\n",
      "\u001b[0m> Running step f3f4acc9-bbe4-4b17-bad5-02ebfdaccf23. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: The key features of LlamaExtract include multimodal extraction, decoupling schema creation from extraction, human-in-the-loop schema creation, automated schema inference, support for Pydantic JSON schema format, drag-and-drop file upload, schema visualization, extraction, API integration, and a Python client.\n",
      "\u001b[0m\u001b[1;3;34mObservation: The key features of LlamaExtract include multimodal extraction, decoupling schema creation from extraction, human-in-the-loop schema creation, automated schema inference, support for Pydantic JSON schema format, drag-and-drop file upload, schema visualization, extraction, API integration, and a Python client.\n",
      "\u001b[0m> Running step 133025d9-25cb-47ce-86fa-c476e27f4b71. Step input: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 165.21it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: LlamaExtract is a structured data extraction tool that allows users to extract data in just a few clicks, and its key features include multimodal extraction, decoupling schema creation from extraction, human-in-the-loop schema creation, automated schema inference, support for Pydantic JSON schema format, drag-and-drop file upload, schema visualization, extraction, API integration, and a Python client.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = top_agent.chat(\"What is LlamaExtract?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26167d93-ae16-41bc-ad6a-6767639145fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LlamaExtract is a structured data extraction tool that allows users to extract data in just a few clicks, and its key features include multimodal extraction, decoupling schema creation from extraction, human-in-the-loop schema creation, automated schema inference, support for Pydantic JSON schema format, drag-and-drop file upload, schema visualization, extraction, API integration, and a Python client."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60ef708e-0767-430b-9822-959a06ff7753",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step a743b23c-9682-427d-bfe8-ef8afed3e452. Step input: Summarize latest updates to LlamaCloud\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 53.98it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: tool_the_latest_updates_to_llamacloud\n",
      "Action Input: {'input': 'latest updates to LlamaCloud'}\n",
      "\u001b[0m> Running step 576f576f-fc2d-435e-87fd-e93e78c49e2a. Step input: latest updates to LlamaCloud\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: summary_tool_the_latest_updates_to_llamacloud\n",
      "Action Input: {'input': 'latest updates to LlamaCloud'}\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 59.35it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mObservation: \n",
      "\n",
      "The latest updates to LlamaCloud include:\n",
      "\n",
      "1. Improved Data and Metadata Access:\n",
      "   - Added Notion, Slack, and Jira connectors\n",
      "   - Enhanced Sharepoint connector with native integration of user IDs for filtering and access control\n",
      "   - Ability to attach metadata to uploaded files as a CSV through the UI or API\n",
      "\u001b[0m> Running step 5f099057-14a4-45f9-a51e-83fac7c59483. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: The latest updates to LlamaCloud include improved data and metadata access, with added connectors for Notion, Slack, and Jira, enhanced Sharepoint connector with native integration of user IDs, and the ability to attach metadata to uploaded files as a CSV through the UI or API.\n",
      "\u001b[0m\u001b[1;3;34mObservation: The latest updates to LlamaCloud include improved data and metadata access, with added connectors for Notion, Slack, and Jira, enhanced Sharepoint connector with native integration of user IDs, and the ability to attach metadata to uploaded files as a CSV through the UI or API.\n",
      "\u001b[0m> Running step adfdfa33-481b-48ae-9226-d91e5e372544. Step input: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 59.12it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: The latest updates to LlamaCloud include improved data and metadata access, with added connectors for Notion, Slack, and Jira, enhanced Sharepoint connector with native integration of user IDs, and the ability to attach metadata to uploaded files as a CSV through the UI or API.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = top_agent.chat(\"Summarize latest updates to LlamaCloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9783e85-641f-4b1e-b92a-7ba16039655a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The latest updates to LlamaCloud include improved data and metadata access, with added connectors for Notion, Slack, and Jira, enhanced Sharepoint connector with native integration of user IDs, and the ability to attach metadata to uploaded files as a CSV through the UI or API."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df07c562-dc6d-4793-811c-677a80db7534",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 7e6d0f38-5a4f-4acb-b41f-7021323df9fd. Step input: Compare agentic rag to rag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 60.03it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: I need to use a tool to help me answer the question.\n",
      "Action: tool_agentic_rag_with_llamaindex_2721b8a49ff6\n",
      "Action Input: {'input': 'agentic rag vs rag'}\n",
      "\u001b[0m> Running step 9c90aa3c-f620-4858-b880-68c8215570e6. Step input: agentic rag vs rag\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: vector_tool_agentic_rag_with_llamaindex_2721b8a49ff6\n",
      "Action Input: {'input': 'agentic rag vs rag'}\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 101.22it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mObservation: \n",
      "\n",
      "Agentic RAG (Agentic Relevance Architecture with Graph) is an enhanced version of the traditional RAG (Relevance Architecture with Graph) implementation. The key differences between Agentic RAG and RAG are:\n",
      "\n",
      "1. **Agent approach**: Agentic RAG incorporates an agent approach, which adds resilience and intelligence to the RAG implementation. This means that each document is managed by a sub-agent, making the system more scalable and efficient.\n",
      "2. **Multi-agent orchestration**: Agentic RAG is a good illustration of multi-agent orchestration, where a top-level agent oversees a set of document agents, enabling more complex queries and conversations.\n",
      "3. **Scalability**: Agentic RAG is designed to scale more easily than traditional RAG, as new documents can be added with each new set being managed by a sub-agent.\n",
      "4. **Autonomous agent implementation**: Agentic RAG is an example of a controlled and well-defined autonomous agent implementation, allowing for more flexibility and adaptability.\n",
      "\n",
      "In contrast, traditional RAG is a more basic implementation that relies on a single graph to manage the relevance of documents. While RAG is a good starting point, Agentic RAG offers more advanced features and capabilities, making it a natural progression for enterprise LLM implementation.\n",
      "\u001b[0m> Running step e1c3ee28-6e74-4f05-ad2b-321977731f4d. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: vector_tool_agentic_rag_with_llamaindex_2721b8a49ff6\n",
      "Action Input: {'input': 'agentic rag vs rag advantages'}\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 71.47it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mObservation: \n",
      "\n",
      "Based on the provided context, here are the advantages of Agentic RAG compared to traditional RAG:\n",
      "\n",
      "1. **Resilience and Intelligence**: Agentic RAG adds an agent approach, making the RAG implementation more resilient and intelligent.\n",
      "2. **Multi-agent Orchestration**: It illustrates a good example of multi-agent orchestration, allowing for better management of a large set of documents.\n",
      "3. **Scalability**: The architecture is designed to scale easily, with new documents being managed by sub-agents, making it suitable for large organizations.\n",
      "4. **Autonomous Agent Implementation**: Agentic RAG is a controlled and well-defined autonomous agent implementation, which is a natural progression of the sought-after enterprise LLM implementation types.\n",
      "5. **Easy Expansion**: The architecture can grow and expand over an organization with more sub-bots being added.\n",
      "\n",
      "In contrast, traditional RAG may not have these advantages, as it does not incorporate an agent approach, making it less resilient and intelligent.\n",
      "\u001b[0m> Running step 94608690-eafc-4167-a6ac-7c99555aa8bb. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: vector_tool_agentic_rag_with_llamaindex_2721b8a49ff6\n",
      "Action Input: {'input': 'agentic rag vs rag disadvantages'}\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 145.75it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mObservation: \n",
      "\n",
      "Based on the provided context, it appears that Agentic RAG is an enhanced version of the traditional RAG (Retriever-Answer Generator) architecture. The advantages of Agentic RAG mentioned in the context include:\n",
      "\n",
      "1. Resilience and intelligence added to the RAG implementation\n",
      "2. Good illustration of multi-agent orchestration\n",
      "3. Scalability and optimization of agent scaling with a second tier of smaller worker-agents\n",
      "4. Controlled and well-defined autonomous agent implementation\n",
      "5. Easy growth and expansion over an organization with more sub-bots being added\n",
      "\n",
      "However, the context does not explicitly mention the disadvantages of Agentic RAG compared to traditional RAG. But, based on general knowledge, some potential disadvantages of Agentic RAG could be:\n",
      "\n",
      "1. **Increased complexity**: With the addition of agents and multi-agent orchestration, the architecture might become more complex, which could lead to increased development and maintenance costs.\n",
      "2. **Higher resource requirements**: The use of multiple agents and embeddings might require more computational resources, which could lead to higher costs and infrastructure requirements.\n",
      "3. **Potential for agent misalignment**: If the agents are not properly aligned with the overall goals and objectives, they might not work together effectively, leading to suboptimal results.\n",
      "4. **Dependence on agent performance**: The performance of the Agentic RAG architecture heavily depends on the performance of the individual agents, which could lead to issues if any of the agents fail or underperform.\n",
      "5. **Potential for over-engineering**: The use of agents and multi-agent orchestration might lead to over-engineering, where the solution becomes more complex than necessary, leading to increased costs and maintenance requirements.\n",
      "\n",
      "Please note that these disadvantages are speculative and not explicitly mentioned in the provided context.\n",
      "\u001b[0m> Running step 55609bcf-f801-46e1-b16c-8ac148c74818. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: vector_tool_agentic_rag_with_llamaindex_2721b8a49ff6\n",
      "Action Input: {'input': 'agentic rag vs rag in real-world applications'}\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 105.68it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mObservation: \n",
      "\n",
      "Based on the provided context, Agentic RAG is an enhanced version of the traditional RAG (Retrieval-Augmented Generation) implementation. In real-world applications, Agentic RAG offers several advantages over traditional RAG:\n",
      "\n",
      "1. **Scalability**: Agentic RAG allows for the creation of a top-level agent that oversees a set of document agents, making it easier to manage and scale large datasets.\n",
      "2. **Intelligence and Resilience**: By incorporating an agent approach, Agentic RAG adds intelligence and resilience to the RAG implementation, enabling it to handle complex queries and provide more accurate responses.\n",
      "3. **Multi-Agent Orchestration**: Agentic RAG demonstrates a good illustration of multi-agent orchestration, where smaller worker-agents can be added to handle specific tasks, making it easier to expand and grow the architecture.\n",
      "4. **Autonomous Implementation**: Agentic RAG is an example of a controlled and well-defined autonomous agent implementation, which can be beneficial in real-world applications where autonomy is crucial.\n",
      "5. **Easy Expansion**: The architecture of Agentic RAG can easily grow and expand over an organization with the addition of more sub-bots, making it a suitable choice for large-scale applications.\n",
      "\n",
      "In contrast, traditional RAG may struggle with:\n",
      "\n",
      "1. **Scalability limitations**: Traditional RAG may become cumbersome to manage as the dataset grows, requiring manual intervention to handle each document individually.\n",
      "2. **Limited intelligence**: Without an agent approach, traditional RAG may not be able to handle complex queries or provide accurate responses.\n",
      "3. **Difficulty in expansion**: Traditional RAG may require significant rework to add new documents or sub-agents, making it less suitable for large-scale applications.\n",
      "\n",
      "In real-world applications, Agentic RAG is a natural progression of traditional RAG, offering improved scalability, intelligence, and resilience. Its ability to easily expand and grow makes it a suitable choice for large-scale applications, such as:\n",
      "\n",
      "1. **Enterprise search**: Agentic RAG can handle large datasets and provide accurate responses to complex queries.\n",
      "2. **Conversational AI**: Agentic RAG can be used to create conversational interfaces that can understand and respond to user queries.\n",
      "3. **Document management**: Agentic RAG can help manage and organize large datasets, making it easier to retrieve relevant information.\n",
      "\n",
      "In summary, Agentic RAG offers several advantages over traditional RAG in real-world applications, making it a more suitable choice for large-scale and complex use cases.\n",
      "\u001b[0m> Running step 112c087c-10be-49dc-8b8f-3daca1991980. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: vector_tool_agentic_rag_with_llamaindex_2721b8a49ff6\n",
      "Action Input: {'input': 'agentic rag vs rag in terms of deployment'}\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 109.22it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mObservation: \n",
      "\n",
      "Based on the provided context, it appears that Agentic RAG and RAG (Regular Aggregative Retrieval) are compared in terms of their architecture and scalability. \n",
      "\n",
      "Agentic RAG is described as an implementation that incorporates an agent approach, which adds resilience and intelligence to the RAG implementation. This architecture is designed to scale easily, with new documents being managed by sub-agents. \n",
      "\n",
      "In contrast, RAG is not explicitly described in the provided context, but it is implied to be a more traditional or basic implementation of aggregative retrieval. \n",
      "\n",
      "In terms of deployment, Agentic RAG seems to offer several advantages over RAG, including:\n",
      "\n",
      "1. **Scalability**: Agentic RAG is designed to scale easily, with new documents being managed by sub-agents, making it a more suitable choice for large-scale deployments.\n",
      "2. **Resilience and Intelligence**: The incorporation of an agent approach adds resilience and intelligence to the RAG implementation, making it more robust and capable of handling complex queries.\n",
      "3. **Multi-Agent Orchestration**: Agentic RAG demonstrates a good illustration of multi-agent orchestration, which can be beneficial in complex deployment scenarios.\n",
      "\n",
      "However, the provided context does not provide a direct comparison of the deployment complexities or requirements of Agentic RAG and RAG. It is essential to consider the specific needs and constraints of your project to determine which approach is more suitable for your deployment.\n",
      "\u001b[0m\u001b[1;3;34mObservation: Error: Reached max iterations.\n",
      "\u001b[0m> Running step 9dc2785d-765b-432c-a57f-c14da45bc478. Step input: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 63.53it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: I need to use a different tool to help me answer the question.\n",
      "Action: tool_improving_rag_effectiveness_with_retrieval_augmented_dual_instruction_tuning_ra_dit_01e73116655d\n",
      "Action Input: {'input': 'agentic rag vs rag'}\n",
      "\u001b[0m> Running step 9b104078-05bb-4d8f-97c0-cfd720a56aac. Step input: agentic rag vs rag\n",
      "\u001b[1;3;38;5;200mThought: The user is asking about the difference between Agentic RAG and RAG. I need to use a tool to help me answer the question.\n",
      "Action: vector_tool_improving_rag_effectiveness_with_retrieval_augmented_dual_instruction_tuning_ra_dit_01e73116655d\n",
      "Action Input: {'input': 'Agentic RAG vs RAG'}\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 59.54it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mObservation: \n",
      "\n",
      "Based on the provided context, I'm assuming you're asking about the comparison between Agentic RAG and RAG (Retrieval-Augmented Generation). However, the text doesn't explicitly mention Agentic RAG. \n",
      "\n",
      "But, I can infer that RA-DIT (Retrieval-Augmented Dual Instruction Tuning) is being compared to LLAMA 65B and LLAMA 65B REPlug. Since RA-DIT is a form of RAG, I'll provide an answer based on the information given.\n",
      "\n",
      "RA-DIT seems to outperform LLAMA 65B and LLAMA 65B REPlug in the provided evaluation settings (0-shot, 5-shot, and 64-shot fine-tuning). This suggests that RA-DIT is more effective than the other two models in the given scenarios.\n",
      "\n",
      "However, without explicit information about Agentic RAG, I'll provide a general answer:\n",
      "\n",
      "It appears that RA-DIT (a form of RAG) outperforms LLAMA 65B and LLAMA 65B REplug in the provided evaluation settings.\n",
      "\u001b[0m> Running step 5cb51034-dc75-44af-9a11-3ed265633f1b. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I provided an answer based on the given information, but I'm not entirely sure if it accurately reflects the comparison between Agentic RAG and RAG. I need to use a tool to gather more information.\n",
      "Action: vector_tool_improving_rag_effectiveness_with_retrieval_augmented_dual_instruction_tuning_ra_dit_01e73116655d\n",
      "Action Input: {'input': 'Agentic RAG vs RAG comparison'}\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.97it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mObservation: \n",
      "\n",
      "The context does not mention \"Agentic RAG\" explicitly, but it does compare RA-DIT (Retrieval-Augmented Dual Instruction Tuning) with LLAMA 65B and LLAMA 65B REPlug. \n",
      "\n",
      "However, I can infer that the comparison is between the original RAG (Retrieval Augmentation Generation) and the RA-DIT approach. \n",
      "\n",
      "RA-DIT is shown to outperform the original RAG approach (LLAMA 65B) in 0-shot, 5-shot, and 64-shot fine-tuning settings, indicating that RA-DIT is more effective in improving RAG performance.\n",
      "\u001b[0m> Running step 90ffd054-99ca-4ef7-8a10-2e0a78ab4b31. Step input: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = top_agent.chat(\"Compare agentic rag to rag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877997cb-5288-4d6e-95e1-5efb2912aa9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-py310",
   "language": "python",
   "name": "llm-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
