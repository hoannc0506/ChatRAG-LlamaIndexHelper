{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "201bba93-da64-4b4e-bf25-7da303ecb158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5ece7e5-d5f0-4ac2-9023-364be4ed50ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0defa9c-f875-49fd-be85-32fb015f3133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/llm/lib/python3.10/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.3.0+cu118 available.\n",
      "PyTorch version 2.3.0+cu118 available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from llama_index.core import (\n",
    "    Settings,\n",
    "    VectorStoreIndex, SummaryIndex,\n",
    "    SimpleKeywordTableIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage\n",
    ")\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core.schema import IndexNode\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "from IPython.display import Markdown, display\n",
    "import agent_utils, data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed759f90-7296-42c0-8b0e-4dc9a3e9d396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: models/bge-base-en-v1.5\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: models/bge-base-en-v1.5\n",
      "Load pretrained SentenceTransformer: models/bge-base-en-v1.5\n",
      "INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2 prompts are loaded, with the keys: ['query', 'text']\n",
      "Loading LLM: models/Meta-Llama-3.1-8B-Instruct\n",
      "Loading tokenizer and model with quantization config from: models/Meta-Llama-3.1-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LLM and embedding models\n"
     ]
    }
   ],
   "source": [
    "device_map = \"cuda:1\"\n",
    "llm_hf, embed_model = agent_utils.load_llm_embed_models(\n",
    "    llm_name=\"models/Meta-Llama-3.1-8B-Instruct\", embed_name=\"models/bge-base-en-v1.5\", device_map=device_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b9cca0-2193-447a-b03e-546ed106599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm_hf\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899233d8-3a93-44e3-933f-ba2ec461d8f0",
   "metadata": {},
   "source": [
    "## Load documents/ nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3daf0d45-4693-4717-98ca-26b68dbdb0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num documents: 166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing documents: 100%|██████████| 166/166 [00:00<00:00, 12504.79it/s]\n"
     ]
    }
   ],
   "source": [
    "documents = data_utils.load_md_documents(\n",
    "    docs_dir=\"data/llama-blogs-md\", docs_metadata=\"data/llama_blogs_metadata.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee0b050e-8d83-4fff-bc53-5cbba0c2b4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': 'using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant.md',\n",
       " 'extension': '.md',\n",
       " 'title': 'Using LlamaIndex and llamafile to build a local, private research assistant',\n",
       " 'date': 'May 14, 2024',\n",
       " 'url': 'https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_doc = documents[30]\n",
    "test_doc.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "522b111b-7ca9-4234-90e6-7b197872a348",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 532.27it/s]\n"
     ]
    }
   ],
   "source": [
    "nodes = data_utils.parse_md_doc([test_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7f3611d-ad71-475e-bed0-f00358bc8962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf6e988-a408-47ab-bbf5-e5261570a6fd",
   "metadata": {},
   "source": [
    "## Get tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09b8f0d6-554f-40c8-9f31-45c06997a12a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index from using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "vector_tool, summary_tool = agent_utils.get_tools_from_nodes(nodes, doc_metadata=test_doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59f60564-7c23-4920-81d7-3c0deef257da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tools:\n",
      "vector_tool_using_llamaindex_and_llamafile_to_build_a_local_private_research_assistant\n",
      "summary_tool_using_llamaindex_and_llamafile_to_build_a_local_private_research_assistant\n"
     ]
    }
   ],
   "source": [
    "print(\"Loaded tools:\", vector_tool.metadata.name, summary_tool.metadata.name, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb976d8-4fe7-4826-bfb8-5298b387273e",
   "metadata": {},
   "source": [
    "## Define Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8420b21f-bc2e-4f75-a5e0-db4ca8a602e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b681c84a-ffc3-4359-b751-70a102d75dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import AgentRunner, ReActAgentWorker, ReActAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f006a26b-9c36-4381-97fe-cc32424aca6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n",
      "ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
     ]
    }
   ],
   "source": [
    "agent_context = '''\\\n",
    "You are CTO of LLamaIndex, you can answer question related to LlamaIndex blog'''\n",
    "agent = ReActAgent.from_tools(\n",
    "    [vector_tool, summary_tool],\n",
    "    llm=Settings.llm,\n",
    "    context=agent_context,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49f37b1-90b7-488e-86bc-661d32136c72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35a19be1-779e-406e-8a6c-df10cc1e6714",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 9f314a31-7d6b-4585-af13-7b42eeeb96c3. Step input: What is llamafile?\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: vector_tool_using_llamaindex_and_llamafile_to_build_a_local_private_research_assistant\n",
      "Action Input: {'input': 'What is llamafile?'}\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.37it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mObservation: \n",
      "\n",
      "A llamafile is an executable Large Language Model (LLM) that you can run on your own computer. It contains the weights for a given open-source LLM, as well as everything needed to actually run that model on your computer.\n",
      "\u001b[0m> Running step 1e465e29-06fe-4dbc-8fdb-8e0d482a92ef. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: A llamafile is an executable Large Language Model (LLM) that you can run on your own computer. It contains the weights for a given open-source LLM, as well as everything needed to actually run that model on your own computer.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What is llamafile?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6d54472-de54-466f-bc66-f95f95a3e255",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A llamafile is an executable Large Language Model (LLM) that you can run on your own computer. It contains the weights for a given open-source LLM, as well as everything needed to actually run that model on your own computer.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9da97bf6-4c8e-4417-810e-cccd63669012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 8fc150f2-322c-445e-a434-bf493b6270c2. Step input: How to download and run a llamafile?\n",
      "\u001b[1;3;38;5;200mThought: The user wants to know how to download and run a llamafile. I need to use a tool to help me answer the question.\n",
      "Action: vector_tool_using_llamaindex_and_llamafile_to_build_a_local_private_research_assistant\n",
      "Action Input: {'input': 'downloading and running a llamafile'}\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 59.65it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mObservation: \n",
      "\n",
      "To download and run a llamafile, follow these steps:\n",
      "\n",
      "1. **Download the llamafile-ized model**: You can either click the download link or use a command like `wget` to download the model. For this example, we'll use the `wget` command:\n",
      "   ```\n",
      "   wget https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile\n",
      "   ```\n",
      "2. **Make it executable (you only need to do this once)**: If you're on macOS, Linux, or BSD, you'll need to grant permission for your computer to execute this new file by running:\n",
      "   ```\n",
      "   chmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile\n",
      "   ```\n",
      "   If you're on Windows, simply rename the file by adding \".exe\" on the end.\n",
      "3. **Run in server mode**: Once the file is executable, you can run it in server mode using the following command:\n",
      "   ```\n",
      "  ./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding\n",
      "   ```\n",
      "\n",
      "Note: Make sure to replace the file name with the actual name of the downloaded llamafile.\n",
      "\u001b[0m> Running step db422da8-dd41-4d81-b34c-29fbaea2bbb3. Step input: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mObservation: Error: Could not parse output. Please follow the thought-action-input format. Try again.\n",
      "\u001b[0m> Running step 1d87297d-4676-4f22-8490-e0176bc8fcea. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: The user wants to know how to download and run a llamafile. I need to use a tool to help me answer the question.\n",
      "Action: vector_tool_using_llamaindex_and_llamafile_to_build_a_local_private_research_assistant\n",
      "Action Input: {'input': 'downloading and running a llamafile, step-by-step instructions'}\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 77.06it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mObservation: \n",
      "\n",
      "To download and run a llamafile, follow these step-by-step instructions:\n",
      "\n",
      "**Step 1: Download a llamafile**\n",
      "\n",
      "1. Go to the [HuggingFace model hub](https://huggingface.co/models?sort=trending&search=llamafile) and search for 'llamafile'.\n",
      "2. Choose a llamafile, such as [TinyLlama-1.1B](https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile) (0.67 GB).\n",
      "3. Click the download link or use a command like `wget` to download the model. The download should take 5-10 minutes depending on your internet connection.\n",
      "\n",
      "**Step 2: Make the llamafile executable**\n",
      "\n",
      "1. If you're using macOS, Linux, or BSD, you need to grant permission for your computer to execute the file. Run the command `chmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile`.\n",
      "2. If you're on Windows, rename the file by adding \".exe\" on the end, e.g., `TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile.exe`.\n",
      "\n",
      "**Step 3: Run the llamafile in server mode**\n",
      "\n",
      "1. Open your computer's terminal and navigate to the directory where your llamafile is stored.\n",
      "2. Run the command `./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding`\n",
      "\n",
      "This will start the llamafile in server mode, allowing you to interact with it.\n",
      "\u001b[0m> Running step 88716e91-ccab-4d2b-9b26-1d190408c8bf. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: To download and run a llamafile, follow these step-by-step instructions: First, go to the HuggingFace model hub and search for 'llamafile'. Choose a llamafile, such as TinyLlama-1.1B, and click the download link or use a command like `wget` to download the model. Then, make the llamafile executable by running `chmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile` on macOS, Linux, or BSD, or by renaming the file with \".exe\" on Windows. Finally, run the llamafile in server mode by navigating to the directory where the llamafile is stored and running `./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding`.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"How to download and run a llamafile?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98a35131-17ef-4d1d-9061-124f94d09ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To download and run a llamafile, follow these step-by-step instructions: First, go to the HuggingFace model hub and search for 'llamafile'. Choose a llamafile, such as TinyLlama-1.1B, and click the download link or use a command like `wget` to download the model. Then, make the llamafile executable by running `chmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile` on macOS, Linux, or BSD, or by renaming the file with \".exe\" on Windows. Finally, run the llamafile in server mode by navigating to the directory where the llamafile is stored and running `./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42eccf3a-7bcd-4bed-9120-b71737fdcaf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.USER: 'user'>, content='What is llamafile?', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='A llamafile is an executable Large Language Model (LLM) that you can run on your own computer. It contains the weights for a given open-source LLM, as well as everything needed to actually run that model on your own computer.', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content='How to download and run a llamafile?', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='To download and run a llamafile, follow these step-by-step instructions: First, go to the HuggingFace model hub and search for \\'llamafile\\'. Choose a llamafile, such as TinyLlama-1.1B, and click the download link or use a command like `wget` to download the model. Then, make the llamafile executable by running `chmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile` on macOS, Linux, or BSD, or by renaming the file with \".exe\" on Windows. Finally, run the llamafile in server mode by navigating to the directory where the llamafile is stored and running `./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding`.', additional_kwargs={})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ca15290-0d40-4479-ad97-ce2d6c1fb9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMemoryBuffer(chat_store=SimpleChatStore(store={'chat_history': [ChatMessage(role=<MessageRole.USER: 'user'>, content='What is llamafile?', additional_kwargs={}), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='A llamafile is an executable Large Language Model (LLM) that you can run on your own computer. It contains the weights for a given open-source LLM, as well as everything needed to actually run that model on your own computer.', additional_kwargs={}), ChatMessage(role=<MessageRole.USER: 'user'>, content='How to download and run a llamafile?', additional_kwargs={}), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='To download and run a llamafile, follow these step-by-step instructions: First, go to the HuggingFace model hub and search for \\'llamafile\\'. Choose a llamafile, such as TinyLlama-1.1B, and click the download link or use a command like `wget` to download the model. Then, make the llamafile executable by running `chmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile` on macOS, Linux, or BSD, or by renaming the file with \".exe\" on Windows. Finally, run the llamafile in server mode by navigating to the directory where the llamafile is stored and running `./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding`.', additional_kwargs={})]}), chat_store_key='chat_history', token_limit=3072, tokenizer_fn=functools.partial(<bound method Encoding.encode of <Encoding 'cl100k_base'>>, allowed_special='all'))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54760576-2123-4a9f-bd3b-fdf3a5caacac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-py310",
   "language": "python",
   "name": "llm-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
