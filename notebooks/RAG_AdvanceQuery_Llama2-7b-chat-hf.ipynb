{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "900c4d38-4f3a-48ce-826b-98bcd5a44744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d75825-c192-419a-a7f1-6819aac18aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/dev/lib/python3.10/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.3.0+cu118 available.\n",
      "PyTorch version 2.3.0+cu118 available.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext, Settings\n",
    "from llama_index.core.indices import load_index_from_storage\n",
    "from llama_index.core import VectorStoreIndex, DocumentSummaryIndex\n",
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.response.notebook_utils import display_response\n",
    "import chromadb\n",
    "import torch\n",
    "import model_utils\n",
    "import prompt_utils\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13d1d2e1-ccdf-415e-ae2b-aee2b508bc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: models/bge-small-en-v1.5\n",
      "Load pretrained SentenceTransformer: models/bge-small-en-v1.5\n",
      "INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2 prompts are loaded, with the keys: ['query', 'text']\n"
     ]
    }
   ],
   "source": [
    "# loads BAAI/bge-small-en-v1.5\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"models/bge-small-en-v1.5\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2205677e-2bfa-479e-962f-59ba71a5e99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model with quantization config from: models/Llama-2-7b-chat-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786a9e07bebb434796a7be7aa855ae84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load local llm llama\n",
    "model_name = \"models/Llama-2-7b-chat-hf\"\n",
    "model, tokenizer = model_utils.load_quantized_model(\n",
    "    model_name_or_path=model_name,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "# config llm and embed_model to llamaindex\n",
    "llm_hf = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=512,\n",
    "    query_wrapper_prompt=PromptTemplate(\"<s> [INST] {query_str} [/INST] \"),\n",
    "    generate_kwargs={\n",
    "        \"temperature\": 0.7,\n",
    "        \"do_sample\": True\n",
    "    },\n",
    "    device_map=\"cuda\",\n",
    "    model_name=\"models/Llama-2-7b-chat-hf\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "576f25f1-000c-40e5-9769-ed8cdce0ad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings.embed_model = embed_model\n",
    "# Settings.llm = llm_hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dbd071-6987-44ea-a43c-853c4d1878e2",
   "metadata": {},
   "source": [
    "## Load vector index and document summary index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f9a2bad-01a8-4ed0-aa4d-fb9f103db78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "# Creates a persistent instance of Chroma that saves to disk\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0174fe7b-6ac2-412d-a2bf-4022824784a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(id=3186f0ec-26e5-46fa-b687-281a3a26066f, name=llama_index_blogs),\n",
       " Collection(id=46aee9cc-50b0-4474-90d9-61e1e160c15d, name=blogs_vector_index),\n",
       " Collection(id=cf744ea1-c23d-41c8-8204-363adcd4b3fe, name=llma_blogs_summary),\n",
       " Collection(id=e2bc79ee-49dd-45e4-85f3-6acb87185f7a, name=blogs_summary)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5ae16af-4694-462c-9264-84f64ea9d500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageContext(docstore=<llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore object at 0x7fb93167fee0>, index_store=<llama_index.core.storage.index_store.simple_index_store.SimpleIndexStore object at 0x7fb933a79a20>, vector_stores={'default': ChromaVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, collection_name=None, host=None, port=None, ssl=False, headers=None, persist_dir=None, collection_kwargs={}), 'image': SimpleVectorStore(stores_text=False, is_embedding_query=True, data=SimpleVectorStoreData(embedding_dict={}, text_id_to_ref_doc_id={}, metadata_dict={}))}, graph_store=<llama_index.core.graph_stores.simple.SimpleGraphStore object at 0x7fb93169d480>, property_graph_store=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get or create a collection with the given name and metadata.\n",
    "vector_collection = chroma_client.get_or_create_collection(\"blogs_vector_index\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=vector_collection)\n",
    "vector_storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "vector_storage_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b88d17f-4832-4fa8-aeee-139afb439670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x7fb93169da50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load your index from stored vectors\n",
    "vector_index = VectorStoreIndex.from_vector_store(\n",
    "    llm=llm_hf,\n",
    "    embed_model=embed_model,\n",
    "    vector_store=vector_store, \n",
    "    storage_context=vector_storage_context\n",
    ")\n",
    "vector_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5d00da3-837e-4718-9fe9-565b97e4d9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageContext(docstore=<llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore object at 0x7fb93169d150>, index_store=<llama_index.core.storage.index_store.simple_index_store.SimpleIndexStore object at 0x7fb90e11b910>, vector_stores={'default': ChromaVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, collection_name=None, host=None, port=None, ssl=False, headers=None, persist_dir=None, collection_kwargs={})}, graph_store=<llama_index.core.graph_stores.simple.SimpleGraphStore object at 0x7fb90e11b6a0>, property_graph_store=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get or create a collection with the given name and metadata.\n",
    "doc_sum_collection = chroma_client.get_or_create_collection(\"blogs_summary\")\n",
    "doc_sum_vector_store = ChromaVectorStore(\n",
    "    chroma_collection=doc_sum_collection\n",
    ")\n",
    "\n",
    "doc_sum_storage_context = StorageContext.from_defaults(\n",
    "    vector_store=doc_sum_vector_store,\n",
    "    persist_dir=\"./database/blogs_summary_index/\"\n",
    ")\n",
    "doc_sum_storage_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48cd9948-aed2-43d5-bc9f-fef2e475a224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.document_summary.base.DocumentSummaryIndex at 0x7fb93169cf70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_summary_index = load_index_from_storage(\n",
    "    llm=llm_hf,\n",
    "    embed_model=embed_model,\n",
    "    storage_context=doc_sum_storage_context\n",
    ")\n",
    "doc_summary_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d2ec04-be0d-43da-8c94-451f6d97d21a",
   "metadata": {},
   "source": [
    "## Advanced Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c2b3add-bc1b-45b9-8853-f70d064d8d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"What are key features of llama-agents?\"\n",
    "question2 = '''\n",
    "What are the two critical areas of RAG system performance that are assessed in the \"Evaluating RAG with LlamaIndex\" section of the OpenAI Cookbook?\n",
    "'''\n",
    "question3 = '''\n",
    "What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9878ad79-6471-4ca0-88e3-9bae6b6e9d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ref_docs(resp_metadata):\n",
    "    print(\"References:\")\n",
    "    base_url = \"https://www.llamaindex.ai/blog/\"\n",
    "    for idx, (_, doc_metatada) in enumerate(resp_metadata.items()):\n",
    "        ref_url = base_url + doc_metatada['file_name'].split(\".\")[0]\n",
    "        print(f\"{idx+1}.\", ref_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d3e20a2-9f26-4b0b-aabe-77412cb7b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm_hf\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "454cd317-734e-4e83-b40f-dc2fcb6a8b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracing log\n",
    "import llama_index.core\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "llama_index.core.set_global_handler(\"simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0798e7b-0335-48dd-b1e0-7f0d0ce73021",
   "metadata": {},
   "source": [
    "### VectorIndex as query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df6e2f68-c30f-4e93-8907-b6a4a3a6a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_query_engine = vector_index.as_query_engine(\n",
    "    response_mode=\"compact\", \n",
    "    use_async=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2705ecb5-9c2e-4ab9-9783-ddd88815e78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are key features of llama-agents?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3b3b4110944caf887f1f7de8e1d4a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Based on the provided context information, the key features of llama-agents are:\n",
       "\n",
       "1. Distributed Service Oriented Architecture: LlamaIndex allows each agent to be its own independently running microservice, with a fully customizable LLM-powered control plane that routes and distributes tasks.\n",
       "2. Communication via standardized API interfaces: Agents can communicate with each other using a central control plane orchestrator, and pass messages between agents using a message queue.\n",
       "3. Define agentic and explicit orchestration flows: Developers have the flexibility to directly define the sequence of interactions between agents or leave it up to an \"agentic orchestrator\" that decides which agents are relevant to the task.\n",
       "4. Ease of deployment: LlamaIndex allows developers to launch, scale, and monitor each agent and the control plane independently.\n",
       "5. Scalability and resource management: LlamaIndex provides built-in observability tools to monitor the quality and performance of the system and each individual agent service.\n",
       "\n",
       "These are the main features of llama-agents, based on the context information provided."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "References:\n",
      "1. https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\n"
     ]
    }
   ],
   "source": [
    "print(\"Question:\", question1)\n",
    "response1 = vector_query_engine.query(question1)\n",
    "display_response(response1)\n",
    "print_ref_docs(response1.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca3bb3b-edf3-415d-ad13-b240e1110a09",
   "metadata": {},
   "source": [
    "### DocumentSummaryIndex as query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f8a6d6c-d8d1-4a62-a018-295db831d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_query_engine = doc_summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\", \n",
    "    use_async=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25a1edea-2b81-4c01-b8cd-44b03d345a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are key features of llama-agents?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33697f5f34b1488884e902bb5c1bb832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    }
   ],
   "source": [
    "print(\"Question:\", question1)\n",
    "response = summary_query_engine.query(question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c406b85-5e5f-4eaf-8ffa-03e433f62daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Based on the information provided in the article, the key features of LlmaAgents are:\n",
       "\n",
       "1. Vector database: LlmaAgents use a vector database created from DiffusionDB, which is a large-scale knowledge graph that contains billions of vectors. This database is used to suggest better prompts when generating images.\n",
       "2. Text-to-image prompts: LlmaAgents can generate text-to-image prompts using the transformers model. This allows for the generation of images from text descriptions.\n",
       "3. Temperature variable: The temperature variable allows for controlling the variation in the generated prompts. With a temperature above zero, each prompt generated by LlmaIndex with the same agent prompt will be brand new.\n",
       "4. Custom tools: LlmaAgents can be used to distribute and share custom tools in Transformers Agents using Hugging Face Spaces.\n",
       "5. Easy to use: The article mentions that the tool is easy to use, and the author provides an example of how to use the tool in the article.\n",
       "6. Improved image generation: The article claims that the tool can generate more stylized and varied images compared to the existing image-generator tool.\n",
       "\n",
       "Overall, LlmaAgents are a powerful tool for generating text-to-image prompts using transformers models, and they offer a range of features that make them easy to use and customize."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "References:\n",
      "1. https://www.llamaindex.ai/blog/llamaindex-and-transformers-agents-67042ee1d8d6\n"
     ]
    }
   ],
   "source": [
    "display_response(response)\n",
    "print_ref_docs(response.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80060b08-2593-4ab0-98c9-8705aa91c504",
   "metadata": {},
   "source": [
    "### Router Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d33419a7-905b-432f-a229-239141005973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector, LLMMultiSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cdf40ce-8618-40c8-8339-af673f5f8ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_tool = QueryEngineTool(\n",
    "    vector_query_engine,\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"vector_search\",\n",
    "        description=\"Useful for retrieving specific context\"\n",
    "    )\n",
    ")\n",
    "\n",
    "summary_tool = QueryEngineTool(\n",
    "    summary_query_engine,\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"summary\",\n",
    "        description=\"Useful for summarization questions related to document content\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467dd416-3d56-4e3a-90b4-5603bdacbfbe",
   "metadata": {},
   "source": [
    "#### Single selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdd69932-6246-4a34-80fe-204cf685ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_query_engine = RouterQueryEngine.from_defaults(\n",
    "    [vector_tool, summary_tool],\n",
    "    selector=LLMSingleSelector.from_defaults()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3241687-4f71-4f9c-b1e1-9b28d02860da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.query_engine.router_query_engine:Selecting query engine 0: Useful for retrieving specific context.\n",
      "Selecting query engine 0: Useful for retrieving specific context.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d403f6b018d430c8586232003fc392b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = router_query_engine.query(question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8826a0c-0f62-409b-b7b1-67b01e517529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The key features of llama-agents are:\n",
       "\n",
       "1. Distributed Service Oriented Architecture: Every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\n",
       "2. Communication via standardized API interfaces: Interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.\n",
       "3. Define agentic and explicit orchestration flows: Developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an “agentic orchestrator” that decides which agents are relevant to the task.\n",
       "4. Ease of deployment: Launch, scale, and monitor each agent and your control plane independently.\n",
       "5. Scalability and resource management: Use built-in observability tools to monitor the quality and performance of the system and each individual agent service."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "180594fd-888b-412a-a1fe-9316a623a96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'05e03730-b3db-464d-831f-b87fd9c5e3b7': {'file_path': '/workspace/projects/LlamindexHelper/data/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html',\n",
       "  'file_name': 'introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems.html',\n",
       "  'file_type': 'text/html',\n",
       "  'file_size': 18790,\n",
       "  'creation_date': '2024-07-21',\n",
       "  'last_modified_date': '2024-07-21'},\n",
       " 'selector_result': MultiSelection(selections=[SingleSelection(index=0, reason='Useful for retrieving specific context')])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a46bc278-3fa3-485a-8e5b-f0478204268b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.query_engine.router_query_engine:Selecting query engine 0: The question is asking for information about the critical areas of RAG system performance assessed in a specific section of the OpenAI Cookbook, which suggests that context is relevant..\n",
      "Selecting query engine 0: The question is asking for information about the critical areas of RAG system performance assessed in a specific section of the OpenAI Cookbook, which suggests that context is relevant..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eda3838594948bfbec134a5b1aa1d9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = router_query_engine.query(question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff54ffb6-a666-460c-acb7-39a70f3d5b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Based on the context information provided in the OpenAI Cookbook, the two critical areas of RAG system performance that are assessed in the \"Evaluating RAG with LlamaIndex\" section are:\n",
       "\n",
       "1. Retrieval System: This area assesses the ability of the RAG system to retrieve relevant and high-quality contexts for the given prompt.\n",
       "2. Response Generation: This area assesses the ability of the RAG system to generate high-quality responses to the given prompt."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0434921b-ee41-4681-83d2-8f9c1ef5fcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = router_query_engine.query(question3)\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d87ac-c836-4886-8133-e48e23eb05ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_summary_question = \"What is the summmarization of bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot document?\"\n",
    "response = router_query_engine.query(test_summary_question)\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61953756-3702-48d4-a831-087dea86b9f3",
   "metadata": {},
   "source": [
    "### MultiSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68db9dde-4d4f-45cb-b68f-f48374c0f82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_selector_query_engine = RouterQueryEngine(\n",
    "    selector=LLMMultiSelector.from_defaults(),\n",
    "    query_engine_tools=[vector_tool, summary_tool],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32db4d0-e601-4d51-a9f1-877f92262f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(question1)\n",
    "response = multi_selector_query_engine.query(question1)\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56f4496-9004-47c6-a9ed-dde0ad77f146",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(question2)\n",
    "response = multi_selector_query_engine.query(question2)\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e7d5d1-c111-4875-9333-42f53721f7f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(question3)\n",
    "response = multi_selector_query_engine.query(question3)\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5effa0b8-7372-4258-a2c4-f009e22b0441",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(question3)\n",
    "response = multi_selector_query_engine.query(question3)\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8616286-409f-4771-80e3-ad9cc3c6ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = multi_selector_query_engine.query(\"What are diffusion models?\")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f046b42-6c6a-48f7-9f60-ff89416c62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6ce9c6-8d9f-4291-b460-71cdef443159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_torch230_py310",
   "language": "python",
   "name": "llm_torch230_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
