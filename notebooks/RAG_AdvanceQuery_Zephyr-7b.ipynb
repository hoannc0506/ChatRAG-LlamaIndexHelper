{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "900c4d38-4f3a-48ce-826b-98bcd5a44744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d75825-c192-419a-a7f1-6819aac18aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/dev/lib/python3.10/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.3.0+cu118 available.\n",
      "PyTorch version 2.3.0+cu118 available.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext, Settings\n",
    "from llama_index.core.indices import load_index_from_storage\n",
    "from llama_index.core import VectorStoreIndex, DocumentSummaryIndex\n",
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.response.notebook_utils import display_response\n",
    "import chromadb\n",
    "import torch\n",
    "import model_utils\n",
    "import prompt_utils\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13d1d2e1-ccdf-415e-ae2b-aee2b508bc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: models/bge-small-en-v1.5\n",
      "Load pretrained SentenceTransformer: models/bge-small-en-v1.5\n",
      "INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2 prompts are loaded, with the keys: ['query', 'text']\n"
     ]
    }
   ],
   "source": [
    "# loads BAAI/bge-small-en-v1.5\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"models/bge-small-en-v1.5\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "065ec354-2e3f-4c83-8df6-f57f57a2af25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model with quantization config from: models/zephyr-7b-beta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165ea8ef99ac4c7ba0fad8e56a747ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load local llm zephyr\n",
    "model_name = \"models/zephyr-7b-beta\"\n",
    "model, tokenizer = model_utils.load_quantized_model(\n",
    "    model_name_or_path=model_name,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "# Set `pad_token_id` to `eos_token_id`\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "# config llm and embed_model to llamaindex\n",
    "llm_hf = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=512,\n",
    "    query_wrapper_prompt=PromptTemplate(\"<|system|>\\n</s>\\n<|user|>\\n{query_str}</s>\\n<|assistant|>\\n\"),\n",
    "    generate_kwargs={\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_k\": 50, \n",
    "        \"top_p\": 0.95,\n",
    "        \"do_sample\": True\n",
    "    },\n",
    "    device_map=\"cuda\",\n",
    "    model_name=model_name,\n",
    "    model=model,\n",
    "    messages_to_prompt=prompt_utils.zephyr_messages_to_prompt,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dbd071-6987-44ea-a43c-853c4d1878e2",
   "metadata": {},
   "source": [
    "## Load vector index and document summary index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f9a2bad-01a8-4ed0-aa4d-fb9f103db78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "# Creates a persistent instance of Chroma that saves to disk\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0174fe7b-6ac2-412d-a2bf-4022824784a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(id=3186f0ec-26e5-46fa-b687-281a3a26066f, name=llama_index_blogs),\n",
       " Collection(id=46aee9cc-50b0-4474-90d9-61e1e160c15d, name=blogs_vector_index),\n",
       " Collection(id=cf744ea1-c23d-41c8-8204-363adcd4b3fe, name=llma_blogs_summary),\n",
       " Collection(id=e2bc79ee-49dd-45e4-85f3-6acb87185f7a, name=blogs_summary)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5ae16af-4694-462c-9264-84f64ea9d500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageContext(docstore=<llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore object at 0x7f8421da2320>, index_store=<llama_index.core.storage.index_store.simple_index_store.SimpleIndexStore object at 0x7f84212dfbb0>, vector_stores={'default': ChromaVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, collection_name=None, host=None, port=None, ssl=False, headers=None, persist_dir=None, collection_kwargs={}), 'image': SimpleVectorStore(stores_text=False, is_embedding_query=True, data=SimpleVectorStoreData(embedding_dict={}, text_id_to_ref_doc_id={}, metadata_dict={}))}, graph_store=<llama_index.core.graph_stores.simple.SimpleGraphStore object at 0x7f84212df8e0>, property_graph_store=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get or create a collection with the given name and metadata.\n",
    "vector_collection = chroma_client.get_or_create_collection(\"blogs_vector_index\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=vector_collection)\n",
    "vector_storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "vector_storage_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b88d17f-4832-4fa8-aeee-139afb439670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x7f84213055a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load your index from stored vectors\n",
    "vector_index = VectorStoreIndex.from_vector_store(\n",
    "    llm=llm_hf,\n",
    "    embed_model=embed_model,\n",
    "    vector_store=vector_store, \n",
    "    storage_context=vector_storage_context\n",
    ")\n",
    "vector_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5d00da3-837e-4718-9fe9-565b97e4d9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageContext(docstore=<llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore object at 0x7f84213050c0>, index_store=<llama_index.core.storage.index_store.simple_index_store.SimpleIndexStore object at 0x7f8402ab78b0>, vector_stores={'default': ChromaVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, collection_name=None, host=None, port=None, ssl=False, headers=None, persist_dir=None, collection_kwargs={})}, graph_store=<llama_index.core.graph_stores.simple.SimpleGraphStore object at 0x7f8402ab76a0>, property_graph_store=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get or create a collection with the given name and metadata.\n",
    "doc_sum_collection = chroma_client.get_or_create_collection(\"blogs_summary\")\n",
    "doc_sum_vector_store = ChromaVectorStore(\n",
    "    chroma_collection=doc_sum_collection\n",
    ")\n",
    "\n",
    "doc_sum_storage_context = StorageContext.from_defaults(\n",
    "    vector_store=doc_sum_vector_store,\n",
    "    persist_dir=\"./database/blogs_summary_index/\"\n",
    ")\n",
    "doc_sum_storage_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48cd9948-aed2-43d5-bc9f-fef2e475a224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.document_summary.base.DocumentSummaryIndex at 0x7f8421304790>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_summary_index = load_index_from_storage(\n",
    "    llm=llm_hf,\n",
    "    embed_model=embed_model,\n",
    "    storage_context=doc_sum_storage_context\n",
    ")\n",
    "doc_summary_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d2ec04-be0d-43da-8c94-451f6d97d21a",
   "metadata": {},
   "source": [
    "## Advanced Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c2b3add-bc1b-45b9-8853-f70d064d8d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"What are key features of llama-agents?\"\n",
    "question2 = '''\n",
    "What are the two critical areas of RAG system performance that are assessed in the \"Evaluating RAG with LlamaIndex\" section of the OpenAI Cookbook?\n",
    "'''\n",
    "question3 = '''\n",
    "What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9878ad79-6471-4ca0-88e3-9bae6b6e9d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ref_docs(resp_metadata):\n",
    "    print(\"References:\")\n",
    "    base_url = \"https://www.llamaindex.ai/blog/\"\n",
    "    for idx, (_, doc_metatada) in enumerate(resp_metadata.items()):\n",
    "        ref_url = base_url + doc_metatada['file_name'].split(\".\")[0]\n",
    "        print(f\"{idx+1}.\", ref_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d3e20a2-9f26-4b0b-aabe-77412cb7b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm_hf\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "454cd317-734e-4e83-b40f-dc2fcb6a8b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracing log\n",
    "import llama_index.core\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "llama_index.core.set_global_handler(\"simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0798e7b-0335-48dd-b1e0-7f0d0ce73021",
   "metadata": {},
   "source": [
    "### VectorIndex as query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df6e2f68-c30f-4e93-8907-b6a4a3a6a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_query_engine = vector_index.as_query_engine(\n",
    "    response_mode=\"compact\", \n",
    "    use_async=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2705ecb5-9c2e-4ab9-9783-ddd88815e78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are key features of llama-agents?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9ec334cb054863aaf431517502d8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Some key features of llama-agents, as mentioned in the context information, are:\n",
       "\n",
       "1. Distributed Service Oriented Architecture: Each agent in LlamaIndex can be its own independently running microservice, orchestrated by a control plane.\n",
       "2. Communication via standardized API interfaces: Agents can communicate with each other using a central control plane orchestrator or a message queue.\n",
       "3. Define agentic and explicit orchestration flows: Developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an \"agentic orchestrator\" that decides which agents are relevant to the task.\n",
       "4. Ease of deployment: Launch, scale, and monitor each agent and the control plane independently.\n",
       "5. Scalability and resource management: Use built-in observability tools to monitor the quality and performance of the system and each individual agent service.\n",
       "\n",
       "More information and resources for getting started with llama-agents can be found in the context information provided."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "References:\n",
      "1. https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\n"
     ]
    }
   ],
   "source": [
    "print(\"Question:\", question1)\n",
    "response1 = vector_query_engine.query(question1)\n",
    "display_response(response1)\n",
    "print_ref_docs(response1.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca3bb3b-edf3-415d-ad13-b240e1110a09",
   "metadata": {},
   "source": [
    "### DocumentSummaryIndex as query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f8a6d6c-d8d1-4a62-a018-295db831d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_query_engine = doc_summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\", \n",
    "    use_async=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25a1edea-2b81-4c01-b8cd-44b03d345a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are key features of llama-agents?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68618e992f134313b63888666c927bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Question:\", question1)\n",
    "response = summary_query_engine.query(question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c406b85-5e5f-4eaf-8ffa-03e433f62daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Llama-agents are a type of language model (LLM) agent developed by Hugging Face with the following key features:\n",
       "\n",
       "1. Based on large LLMs such as GPT-3, which have been trained on vast amounts of text data.\n",
       "2. Use the transformers architecture for processing large amounts of text data efficiently and accurately.\n",
       "3. Can be customized with various tools and libraries such as LlamaIndex, a tool for vector databases that can suggest better prompts when generating images.\n",
       "4. Open-source, allowing developers to modify and extend the agents to fit their specific needs.\n",
       "5. Capable of performing multiple tasks simultaneously, making them versatile and useful for various applications.\n",
       "6. Provide real-time feedback on their performance, allowing developers to fine-tune and improve the agents as needed.\n",
       "7. Integration with popular frameworks such as TensorFlow, PyTorch, and Hugging Face Transformers, making it easy to train, test, and deploy Llama-agents in various environments.\n",
       "8. Scalable and capable of handling large datasets, making them suitable for handling complex tasks and large volumes of data.\n",
       "9. Capable of achieving high accuracy on various tasks, making them a reliable choice for many applications.\n",
       "10. Developed collaboratively by a community of developers, ensuring that they are continually improved and updated with the latest advancements."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "References:\n",
      "1. https://www.llamaindex.ai/blog/llamaindex-and-transformers-agents-67042ee1d8d6\n"
     ]
    }
   ],
   "source": [
    "display_response(response)\n",
    "print_ref_docs(response.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80060b08-2593-4ab0-98c9-8705aa91c504",
   "metadata": {},
   "source": [
    "### Router Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d33419a7-905b-432f-a229-239141005973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector, LLMMultiSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cdf40ce-8618-40c8-8339-af673f5f8ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_tool = QueryEngineTool(\n",
    "    vector_query_engine,\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"vector_search\",\n",
    "        description=\"Useful for retrieving specific context\"\n",
    "    )\n",
    ")\n",
    "\n",
    "summary_tool = QueryEngineTool(\n",
    "    summary_query_engine,\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"summary\",\n",
    "        description=\"Useful for summarization questions related to document content\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467dd416-3d56-4e3a-90b4-5603bdacbfbe",
   "metadata": {},
   "source": [
    "#### Single selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdd69932-6246-4a34-80fe-204cf685ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_query_engine = RouterQueryEngine.from_defaults(\n",
    "    [vector_tool, summary_tool],\n",
    "    selector=LLMSingleSelector.from_defaults()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3241687-4f71-4f9c-b1e1-9b28d02860da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.query_engine.router_query_engine:Selecting query engine 1: The summary provided for choice 2 suggests that it would be useful for summarization questions related to document content, and as 'key features of llama-agents' would likely be discussed in the context of the document, choice 2 is more relevant to the question..\n",
      "Selecting query engine 1: The summary provided for choice 2 suggests that it would be useful for summarization questions related to document content, and as 'key features of llama-agents' would likely be discussed in the context of the document, choice 2 is more relevant to the question..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b45788e9174ecdbae4ec9b97bd20a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = router_query_engine.query(question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8826a0c-0f62-409b-b7b1-67b01e517529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Llama-agents, which are intelligent software agents built using LLM agents, have several key features. These include the ability to interact with external services, perform specific tasks, integrate with external tools, learn and adapt, work collaboratively, handle large amounts of data, handle natural language and multimodal data, provide explanations, and work in real-time. These features make llama-agents versatile and powerful, allowing them to be useful in various applications such as search engines, recommendation engines, knowledge graphs, chatbots, virtual assistants, customer support, decision making, diagnosis, and analysis."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "180594fd-888b-412a-a1fe-9316a623a96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c71a83e3-f32a-4445-9ee3-cad21e9a0778': {'file_path': '/workspace/projects/LlamindexHelper/data/llamaindex-and-transformers-agents-67042ee1d8d6.html',\n",
       "  'file_name': 'llamaindex-and-transformers-agents-67042ee1d8d6.html',\n",
       "  'file_type': 'text/html',\n",
       "  'file_size': 14762,\n",
       "  'creation_date': '2024-07-21',\n",
       "  'last_modified_date': '2024-07-21'},\n",
       " 'selector_result': MultiSelection(selections=[SingleSelection(index=1, reason=\"The summary provided for choice 2 suggests that it would be useful for summarization questions related to document content, and as 'key features of llama-agents' would likely be discussed in the context of the document, choice 2 is more relevant to the question.\")])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a46bc278-3fa3-485a-8e5b-f0478204268b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.query_engine.router_query_engine:Selecting query engine 1: The question asks about summarization questions related to document content, which suggests that choice (2) is most relevant as it relates to document content and summarization..\n",
      "Selecting query engine 1: The question asks about summarization questions related to document content, which suggests that choice (2) is most relevant as it relates to document content and summarization..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae09ad66fd44428faff49110ce17e7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = router_query_engine.query(question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff54ffb6-a666-460c-acb7-39a70f3d5b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The two critical areas of RAG system performance that are assessed in the \"Evaluating RAG with LlamaIndex\" section of the OpenAI Cookbook are \"the Retrieval System\" and \"Response Generation.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0434921b-ee41-4681-83d2-8f9c1ef5fcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.query_engine.router_query_engine:Selecting query engine 1: The question asks about metrics for evaluating rerankers in the RAG system, which suggests that it relates to questions about document content. Choice 2 indicates that this option is useful for summarization questions related to document content, which aligns with the nature of the question..\n",
      "Selecting query engine 1: The question asks about metrics for evaluating rerankers in the RAG system, which suggests that it relates to questions about document content. Choice 2 indicates that this option is useful for summarization questions related to document content, which aligns with the nature of the question..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "595e6580e5634ad89c9db69900808f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The two main metrics used to evaluate the performance of the different rerankers in the RAG system are Hit Rate and Mean Reciprocal Rank (MRR)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = router_query_engine.query(question3)\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c78d87ac-c836-4886-8133-e48e23eb05ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.query_engine.router_query_engine:Selecting query engine 1: The question asks for a summary of the document, which suggests that choice 2 is more relevant as it is specifically related to summarization questions. Choice 1, while useful for retrieving specific context, may not necessarily provide a summary of the document as a whole..\n",
      "Selecting query engine 1: The question asks for a summary of the document, which suggests that choice 2 is more relevant as it is specifically related to summarization questions. Choice 1, while useful for retrieving specific context, may not necessarily provide a summary of the document as a whole..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f220282cb414b0b872b060490b78395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The document describes a winning solution, Counselor Copilot, from a hackathon focused on bridging the gap between demand and supply of crisis services. The AI copilot automates administrative tasks for crisis counselors, allowing them to focus on providing care. It extracts contact data from complex PDFs, suggests appropriate replies based on The Trevor Project's guidelines, searches for location-specific resources, and completes case forms. The solution also includes a ReAct Agent that deploys the right tool based on the chat history and contact context. The document highlights the potential for others to build on this work and suggests possible extensions, such as reducing costs and improving the quality of suggested responses and adding a tool for assessing the stage of the conversation. The document concludes by stating that the AI copilot represents a significant step toward more efficient and effective crisis care, enhancing the quality of care provided and addressing the pressing issue of counselor shortage by maximizing the impact of existing resources."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_summary_question = \"What is the summmarization of bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot document?\"\n",
    "response = router_query_engine.query(test_summary_question)\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61953756-3702-48d4-a831-087dea86b9f3",
   "metadata": {},
   "source": [
    "### MultiSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68db9dde-4d4f-45cb-b68f-f48374c0f82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_selector_query_engine = RouterQueryEngine(\n",
    "    selector=LLMMultiSelector.from_defaults(),\n",
    "    query_engine_tools=[vector_tool, summary_tool],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f32db4d0-e601-4d51-a9f1-877f92262f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are key features of llama-agents?\n",
      "INFO:llama_index.core.query_engine.router_query_engine:Selecting query engine 0: llama-agents have specific contexts that can be retrieved for analysis, making choice 1 the most relevant for this question..\n",
      "Selecting query engine 0: llama-agents have specific contexts that can be retrieved for analysis, making choice 1 the most relevant for this question..\n",
      "\u001b[1;3;38;5;200mSelecting query engine 0: llama-agents have specific contexts that can be retrieved for analysis, making choice 1 the most relevant for this question..\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ff3bbd2fb84087bac309f1cb34b386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** 1. Distributed Service Oriented Architecture: Every agent in LlamaIndex can be its own independently running microservice, orchestrated by a control plane.\n",
       "2. Communication via standardized API interfaces: Agents communicate using a central control plane orchestrator or a message queue.\n",
       "3. Define agentic and explicit orchestration flows: Developers have the flexibility to directly define the sequence of interactions between agents or leave it up to an “agentic orchestrator”.\n",
       "4. Ease of deployment: Launch, scale, and monitor each agent and the control plane independently.\n",
       "5. Scalability and resource management: Use built-in observability tools to monitor the quality and performance of the system and each individual agent service.\n",
       "\n",
       "These features simplify the process of building, iterating, and deploying multi-agent AI systems for complex question-answering systems, collaborative AI assistants, and distributed AI workflows. With llama-agents, developers can orchestrate tasks using a LLM-powered control plane and pass messages between agents using standardized API interfaces. Additionally, developers have the flexibility to directly define the sequence of interactions between agents or leave it up to an “agentic orchestrator”. The framework also enables ease of deployment and scalability through independent launch, scaling, and monitoring of each agent and the control plane. Built-in observability tools are provided for monitoring the quality and performance of the system and each individual agent service."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(question1)\n",
    "response = multi_selector_query_engine.query(question1)\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d56f4496-9004-47c6-a9ed-dde0ad77f146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What are the two critical areas of RAG system performance that are assessed in the \"Evaluating RAG with LlamaIndex\" section of the OpenAI Cookbook?\n",
      "\n",
      "INFO:llama_index.core.query_engine.router_query_engine:Selecting query engine 0: Choice 1 is most relevant as it suggests that the RAG system's performance in retrieving specific context is assessed in the section 'Evaluating RAG with LlamaIndex' of the OpenAI Cookbook..\n",
      "Selecting query engine 0: Choice 1 is most relevant as it suggests that the RAG system's performance in retrieving specific context is assessed in the section 'Evaluating RAG with LlamaIndex' of the OpenAI Cookbook..\n",
      "\u001b[1;3;38;5;200mSelecting query engine 0: Choice 1 is most relevant as it suggests that the RAG system's performance in retrieving specific context is assessed in the section 'Evaluating RAG with LlamaIndex' of the OpenAI Cookbook..\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea296474513d4418a80cf5329a25a886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The two critical areas of RAG system performance that are assessed in the \"Evaluating RAG with LlamaIndex\" section of the OpenAI Cookbook are \"the Retrieval System\" and \"Response Generation.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(question2)\n",
    "response = multi_selector_query_engine.query(question2)\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e8e7d5d1-c111-4875-9333-42f53721f7f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?\n",
      "\n",
      "INFO:llama_index.core.query_engine.router_query_engine:Selecting query engine 1: The first choice may be relevant to locating specific context, but it is not directly related to summarization questions related to document content. The second choice is more appropriate as it pertains to metrics used to evaluate the performance of rerankers in the RAG system, which is directly related to summarization questions..\n",
      "Selecting query engine 1: The first choice may be relevant to locating specific context, but it is not directly related to summarization questions related to document content. The second choice is more appropriate as it pertains to metrics used to evaluate the performance of rerankers in the RAG system, which is directly related to summarization questions..\n",
      "\u001b[1;3;38;5;200mSelecting query engine 1: The first choice may be relevant to locating specific context, but it is not directly related to summarization questions related to document content. The second choice is more appropriate as it pertains to metrics used to evaluate the performance of rerankers in the RAG system, which is directly related to summarization questions..\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb0a72551d944b9acef72c7bd30d489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The two main metrics used to evaluate the performance of the different rerankers in the RAG system are Hit Rate and Mean Reciprocal Rank (MRR)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(question3)\n",
    "response = multi_selector_query_engine.query(question3)\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5effa0b8-7372-4258-a2c4-f009e22b0441",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?\n",
      "\n"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Got invalid JSON object. Error: Extra data: line 12 column 1 (char 514) expected '<document start>', but found '<scalar>'\n  in \"<unicode string>\", line 12, column 1:\n    The output should be ONLY JSON f ... \n    ^. Got JSON string: [\n  {\n    \"choice\": 1,\n    \"reason\": \"These metrics are likely specific to the context of the RAG system, making choice 1 the most relevant for retrieving this information.\"\n  },\n  {\n    \"choice\": 2,\n    \"reason\": \"However, it's also possible that the question is asking for a summary of the document content, in which case choice 2 may be more relevant. In this case, both choices could potentially provide useful information, but it's best to choose only what's needed to answer the question accurately.\"\n  }\n]\n\nThe output should be ONLY JSON formatted as a JSON instance.\n\nHere is an example:\n[\n  {\n    \"choice\": 1,\n    \"reason\": \"These metrics are likely specific to the context of the RAG system, making choice 1 the most relevant for retrieving this information.\"\n  },\n  {\n    \"choice\": 2,\n    \"reason\": \"However, it's also possible that the question is asking for a summary of the document content, in which case choice 2 may be more relevant. In this case, both choices could potentially provide useful information, but it's best to choose only what's needed to answer the question accurately.\"\n  }\n]\n\nThe output should be ONLY JSON formatted as a JSON instance.\n\nHere is an example:\n[\n  { \"choice\": 1, \"reason\": \"These metrics are likely specific to the context of the RAG system, making choice 1 the most relevant for retrieving this information.\" },\n  { \"choice\": 2, \"reason\": \"However, it's also possible that the question is asking for a summary of the document content, in which case choice 2 may be more relevant. In this case, both choices could potentially provide useful information, but it's best to choose only what's needed to answer the question accurately.\" }\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.10/site-packages/llama_index/core/output_parsers/selection.py:75\u001b[0m, in \u001b[0;36mSelectionOutputParser.parse\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     json_obj \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e_json:\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.10/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 12 column 1 (char 514)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.10/site-packages/llama_index/core/output_parsers/selection.py:84\u001b[0m, in \u001b[0;36mSelectionOutputParser.parse\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# NOTE: parsing again with pyyaml\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m#       pyyaml is less strict, and allows for trailing commas\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m#       right now we rely on this since guidance program generates\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m#       trailing commas\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     json_obj \u001b[38;5;241m=\u001b[39m \u001b[43myaml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m yaml\u001b[38;5;241m.\u001b[39mYAMLError \u001b[38;5;28;01mas\u001b[39;00m e_yaml:\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.10/site-packages/yaml/__init__.py:125\u001b[0m, in \u001b[0;36msafe_load\u001b[0;34m(stream)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03mParse the first YAML document in a stream\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03mand produce the corresponding Python object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03mto be safe for untrusted input.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSafeLoader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.10/site-packages/yaml/__init__.py:81\u001b[0m, in \u001b[0;36mload\u001b[0;34m(stream, Loader)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_single_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.10/site-packages/yaml/constructor.py:49\u001b[0m, in \u001b[0;36mBaseConstructor.get_single_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_single_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# Ensure that the stream contains a single document and construct it.\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_single_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.10/site-packages/yaml/composer.py:39\u001b[0m, in \u001b[0;36mComposer.get_single_node\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Ensure that the stream contains no more documents.\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mStreamEndEvent\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     40\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_event()\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.10/site-packages/yaml/parser.py:98\u001b[0m, in \u001b[0;36mParser.check_event\u001b[0;34m(self, *choices)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate:\n\u001b[0;32m---> 98\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_event \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.10/site-packages/yaml/parser.py:171\u001b[0m, in \u001b[0;36mParser.parse_document_start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_token(DocumentStartToken):\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParserError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    172\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<document start>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, but found \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m             \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeek_token()\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m    174\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeek_token()\u001b[38;5;241m.\u001b[39mstart_mark)\n\u001b[1;32m    175\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_token()\n",
      "\u001b[0;31mParserError\u001b[0m: expected '<document start>', but found '<scalar>'\n  in \"<unicode string>\", line 12, column 1:\n    The output should be ONLY JSON f ... \n    ^",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(question3)\n\u001b[0;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_selector_query_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m display_response(response)\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    228\u001b[0m )\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.10/site-packages/llama_index/core/base/base_query_engine.py:52\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     51\u001b[0m         str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 52\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m     54\u001b[0m     QueryEndEvent(query\u001b[38;5;241m=\u001b[39mstr_or_query_bundle, response\u001b[38;5;241m=\u001b[39mquery_result)\n\u001b[1;32m     55\u001b[0m )\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    228\u001b[0m )\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.10/site-packages/llama_index/core/query_engine/router_query_engine.py:170\u001b[0m, in \u001b[0;36mRouterQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RESPONSE_TYPE:\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    168\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[1;32m    169\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[0;32m--> 170\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result\u001b[38;5;241m.\u001b[39minds) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    173\u001b[0m             responses \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.10/site-packages/llama_index/core/base/base_selector.py:88\u001b[0m, in \u001b[0;36mBaseSelector.select\u001b[0;34m(self, choices, query)\u001b[0m\n\u001b[1;32m     86\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [_wrap_choice(choice) \u001b[38;5;28;01mfor\u001b[39;00m choice \u001b[38;5;129;01min\u001b[39;00m choices]\n\u001b[1;32m     87\u001b[0m query_bundle \u001b[38;5;241m=\u001b[39m _wrap_query(query)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchoices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    228\u001b[0m )\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.10/site-packages/llama_index/core/selectors/llm_selectors.py:215\u001b[0m, in \u001b[0;36mLLMMultiSelector._select\u001b[0;34m(self, choices, query)\u001b[0m\n\u001b[1;32m    206\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m    207\u001b[0m     prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt,\n\u001b[1;32m    208\u001b[0m     num_choices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(choices),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m     query_str\u001b[38;5;241m=\u001b[39mquery\u001b[38;5;241m.\u001b[39mquery_str,\n\u001b[1;32m    212\u001b[0m )\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt\u001b[38;5;241m.\u001b[39moutput_parser \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m parsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _structured_output_to_selector_result(parsed)\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    228\u001b[0m )\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.10/site-packages/llama_index/core/output_parsers/selection.py:86\u001b[0m, in \u001b[0;36mSelectionOutputParser.parse\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m     84\u001b[0m     json_obj \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(json_string)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m yaml\u001b[38;5;241m.\u001b[39mYAMLError \u001b[38;5;28;01mas\u001b[39;00m e_yaml:\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot invalid JSON object. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me_json\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me_yaml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot JSON string: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m     )\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease pip install PyYAML.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Got invalid JSON object. Error: Extra data: line 12 column 1 (char 514) expected '<document start>', but found '<scalar>'\n  in \"<unicode string>\", line 12, column 1:\n    The output should be ONLY JSON f ... \n    ^. Got JSON string: [\n  {\n    \"choice\": 1,\n    \"reason\": \"These metrics are likely specific to the context of the RAG system, making choice 1 the most relevant for retrieving this information.\"\n  },\n  {\n    \"choice\": 2,\n    \"reason\": \"However, it's also possible that the question is asking for a summary of the document content, in which case choice 2 may be more relevant. In this case, both choices could potentially provide useful information, but it's best to choose only what's needed to answer the question accurately.\"\n  }\n]\n\nThe output should be ONLY JSON formatted as a JSON instance.\n\nHere is an example:\n[\n  {\n    \"choice\": 1,\n    \"reason\": \"These metrics are likely specific to the context of the RAG system, making choice 1 the most relevant for retrieving this information.\"\n  },\n  {\n    \"choice\": 2,\n    \"reason\": \"However, it's also possible that the question is asking for a summary of the document content, in which case choice 2 may be more relevant. In this case, both choices could potentially provide useful information, but it's best to choose only what's needed to answer the question accurately.\"\n  }\n]\n\nThe output should be ONLY JSON formatted as a JSON instance.\n\nHere is an example:\n[\n  { \"choice\": 1, \"reason\": \"These metrics are likely specific to the context of the RAG system, making choice 1 the most relevant for retrieving this information.\" },\n  { \"choice\": 2, \"reason\": \"However, it's also possible that the question is asking for a summary of the document content, in which case choice 2 may be more relevant. In this case, both choices could potentially provide useful information, but it's best to choose only what's needed to answer the question accurately.\" }\n]"
     ]
    }
   ],
   "source": [
    "print(question3)\n",
    "response = multi_selector_query_engine.query(question3)\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6ce9c6-8d9f-4291-b460-71cdef443159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_torch230_py310",
   "language": "python",
   "name": "llm_torch230_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
